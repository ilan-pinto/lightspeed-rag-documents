<!DOCTYPE html>
<html lang="en" dir="ltr" prefix="og: https://ogp.me/ns#">
  <head>
    <meta charset="utf-8" />
<link rel="canonical" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/index" />
<link rel="shortlink" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/index" />
<meta property="og:site_name" content="Red Hat Customer Portal" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/index" />
<meta property="og:title" content="Support OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<meta property="og:description" content="This document provides information on getting support from Red Hat for OpenShift Container Platform. It also contains information about remote health monitoring through Telemetry and the Insights Operator. The document also details the benefits that remote health monitoring provides." />
<meta property="og:image" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:description" content="This document provides information on getting support from Red Hat for OpenShift Container Platform. It also contains information about remote health monitoring through Telemetry and the Insights Operator. The document also details the benefits that remote health monitoring provides." />
<meta name="twitter:title" content="Support OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<meta name="twitter:url" content="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/index" />
<meta name="twitter:image" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
<meta name="title" content="Support OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<link rel="alternate" hreflang="en" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/index" />
<link rel="alternate" hreflang="ko" href="https://access.redhat.com/documentation/ko-kr/openshift_container_platform/4.13/html-single/support/index" />
<link rel="alternate" hreflang="zh-hans" href="https://access.redhat.com/documentation/zh-cn/openshift_container_platform/4.13/html-single/support/index" />
<link rel="alternate" hreflang="ja" href="https://access.redhat.com/documentation/ja-jp/openshift_container_platform/4.13/html-single/support/index" />
<link rel="alternate" hreflang="es" href="https://access.redhat.com/documentation/es-es/openshift_container_platform/4.13/html-single/support/index" />
<link rel="alternate" hreflang="ru" href="https://access.redhat.com/documentation/ru-ru/openshift_container_platform/4.13/html-single/support/index" />
<link rel="alternate" hreflang="pt-br" href="https://access.redhat.com/documentation/pt-br/openshift_container_platform/4.13/html-single/support/index" />
<link rel="alternate" hreflang="it" href="https://access.redhat.com/documentation/it-it/openshift_container_platform/4.13/html-single/support/index" />
<link rel="alternate" hreflang="de" href="https://access.redhat.com/documentation/de-de/openshift_container_platform/4.13/html-single/support/index" />
<link rel="alternate" hreflang="fr" href="https://access.redhat.com/documentation/fr-fr/openshift_container_platform/4.13/html-single/support/index" />
<link rel="alternate" hreflang="zh-hant" href="https://access.redhat.com/documentation/zh-tw/openshift_container_platform/4.13/html-single/support/index" />
<link rel="alternate" hreflang="id" href="https://access.redhat.com/documentation/id-id/openshift_container_platform/4.13/html-single/support/index" />
<link rel="alternate" hreflang="th" href="https://access.redhat.com/documentation/th-th/openshift_container_platform/4.13/html-single/support/index" />
<link rel="alternate" hreflang="vi" href="https://access.redhat.com/documentation/vi-vn/openshift_container_platform/4.13/html-single/support/index" />
<meta name="Generator" content="Drupal 9 (https://www.drupal.org)" />
<meta name="MobileOptimized" content="width" />
<meta name="HandheldFriendly" content="true" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="revision" product="b0738f19-59ac-47eb-9512-8a439cd6dfb0" title="fc5203f9-e108-41aa-993e-1871fe9fabfe" page="c4901769-f73e-4770-b554-ea8bdf9787dc" revision="dccd48b1f252cc22dad43e9ae6252b9287d98ca1:en-us" body="fec2b6f6012af540d114854906b6dd4d.html" toc="30ea65050f62dd4192b38423a99fde30.json" />

    <title>Support OpenShift Container Platform 4.13 | Red Hat Customer Portal</title>
    <link rel="stylesheet" media="all" href="/sites/dxp-docs/files/css/css_87GMcmxT1ib8ziQiU2KUAnTDFtZQV6iP-KGslA9LigM.css" />
<link rel="stylesheet" media="all" href="/sites/dxp-docs/files/css/css__Xq4GfgPDJw9K_yYJFmlRZGJeCENu3R3r4s0K7Tr_9g.css" />

    
    <script type="application/json" data-drupal-selector="drupal-settings-json">{"path":{"baseUrl":"\/","scriptPath":null,"pathPrefix":"","currentPath":"documentation\/en-us\/openshift_container_platform\/4.13\/html-single\/support\/index","currentPathIsAdmin":false,"isFront":false,"currentLanguage":"en"},"pluralDelimiter":"\u0003","suppressDeprecationErrors":true,"red_hat_jwt":{"client_id":"customer-portal","cookie_name":"rh_jwt","leeway":"0","realm":"redhat-external","sso_host":"https:\/\/sso.redhat.com\/","user_integration":1,"user_plugin":"drupal_user_auth","use_external_js":0,"use_internal_js":0,"use_in_admin":0},"user":{"uid":0,"permissionsHash":"d8ea0bce2d740dacbdfe0257cf55baa0e33f7fb8468a26d055ce75daaaa2d315"}}</script>
<script src="/sites/dxp-docs/files/js/js_EQWKo9EokWkWS99x_e1oM-NEM0zlKyTkp_83mGdm5Ks.js"></script>

    <!-- CP_PRIMER_HEAD -->  <!-- TrustArc & DTM -->
  <script src="//static.redhat.com/libs/redhat/marketing/latest/trustarc/trustarc.js"></script>
  <script src="//www.redhat.com/dtm.js"></script><meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<!--[if IEMobile]><meta http-equiv="cleartype" content="on"><![endif]-->

<!-- metaInclude -->
<meta name="avalon-host-info" content="dxp-kbase-prod-139-77b4fb8768-25dr9" />
<meta name="avalon-version" content="27861f77" />
<meta name="cp-chrome-build-date" content="2023-10-06T19:17:59.039Z" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<!-- Chrome, Firefox OS and Opera -->
<meta name="theme-color" content="#000000">
<!-- Windows Phone -->
<meta name="msapplication-navbutton-color" content="#000000">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-status-bar-style" content="#000000">
<link rel="manifest" href="https://access.redhat.com/webassets/avalon/j/manifest.json">
<!-- Open Search - Tap to search -->
<link rel="search" type="application/opensearchdescription+xml" title="Red Hat Customer Portal" href="https://access.redhat.com/webassets/avalon/j/opensearch.xml" />
<!-- title -->
<title>Red Hat Customer Portal - Access to 24x7 support and knowledge</title>
<!-- /title -->
<script type="text/javascript">
    window.portal = {
        analytics : {},
        host      : "https://access.redhat.com",
        idp_url   : "https://sso.redhat.com",
        lang      : "en", 
        version   : "27861f77",
        builddate : "2023-10-06T19:17:59.039Z",        fetchdate : "2023-10-10T17:45:08-0400",        nrid      : "NOLONGERSUPPORTED",
        nrlk      : "NOLONGERSUPPORTED"
    };
</script>
<script type="text/javascript">
    if (!/\/logout.*/.test(location.pathname) && portal.host === location.origin && document.cookie.indexOf('rh_sso_session') >= 0 && !(document.cookie.indexOf('rh_jwt') >= 0)) window.location = '/login?redirectTo=' + encodeURIComponent(window.location.href);
</script>
<!-- cssInclude -->

<link rel="shortcut icon" href="https://access.redhat.com/webassets/avalon/g/favicon.ico" /><link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/bootstrap.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/bootstrap-grid.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/main.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/components.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/pages.css?v=27861f77" />

<link href="https://access.redhat.com/webassets/avalon/s/chosen.css?v=27861f77" rel="stylesheet" type="text/css" />

<!--[if lte IE 9]>
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/ie.css" />
<![endif]--><noscript>
    <style type="text/css" media="screen"> .primary-nav { display: block; } </style>
</noscript>
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/webassets/avalon/j/public_modules/node_modules/@cpelements/pfe-navigation/dist/pfe-navigation--lightdom.min.css" />
<!-- /cssInclude -->
<script src="https://access.redhat.com/webassets/avalon/j/public_modules/node_modules/@cpelements/pfe-navigation/dist/ie-polyfills.js?v=27861f77"></script>

<script async>
  if (!HTMLScriptElement.supports || !HTMLScriptElement.supports('importmap')) {
    import("https://www.redhatstatic.com/dx/v1-alpha/es-module-shims@1.7.3.js");
  }
</script>
<script type="importmap">
{
  "imports": {
    "@patternfly/elements/" : "https://www.redhatstatic.com/dx/v1-alpha/@patternfly/elements@2.2.2/",
    "@rhds/elements/":"https://www.redhatstatic.com/dx/v1-alpha/@rhds/elements@1.1.0/elements/",
    "@rhds/elements/lib/":"https://www.redhatstatic.com/dx/v1-alpha/@rhds/elements@1.1.0/lib/",
    "@cpelements/elements/":"https://www.redhatstatic.com/dx/v1-alpha/@cpelements/elements@2.0.0-alpha.7/elements/"
  }
}
</script><script type="text/javascript" src="https://access.redhat.com/webassets/avalon/j/lib/require.js?v=27861f77" data-main="/webassets/avalon/j/"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<script src="https://access.redhat.com/chrome_themes/nimbus/js/ie8.js"></script>
<![endif]-->
<script type="text/javascript" src="https://access.redhat.com/chrome_themes/nimbus/js/new-nav.js?v=27861f77" ></script>
<!-- /CP_PRIMER_HEAD -->

  </head>
  <body>
    
      <div class="dialog-off-canvas-main-canvas" data-off-canvas-main-canvas>
      <!-- CP_PRIMER_HEADER -->
<div id="page-wrap" class="page-wrap">
    <div id="pers-top-page-wrap" class="top-page-wrap pers-loader-bg">

      <div id="hero-bg-top-left" class="summit-bg-shapes"></div>
      <div id="hero-bg-top-right" class="summit-bg-shapes"></div>

        <!--googleoff: all-->
        <header class="masthead" id="masthead">

            <a href="#pfe-navigation" id="global-skip-to-nav" class="skip-link visually-hidden">Skip to navigation</a>
            <a href="#cp-main" class="skip-link visually-hidden">Skip to main content</a>            <nav id="portal-utility-nav" class="utility-navigation utility-navigation--bar hidden-at-mobile" data-analytics-region="utility" aria-labelledby="nav__utility-nav--desktop">
                <h3 id="nav__utility-nav--desktop" class="element-invisible">Utilities
                </h3>
                <ul aria-labelledby="nav__utility-nav--desktop">
                    <li id="nav-subscription" data-portal-tour-1="1">
                        <a class="top-nav-subscriptions" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Subscription" href="https://access.redhat.com/management/" >Subscriptions
                        </a>
                    </li>
                    <li id="nav-downloads" data-portal-tour-1="2">
                        <a class="top-nav-downloads" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Downloads" href="https://access.redhat.com/downloads/" >Downloads
                        </a>
                    </li>
                    <li id="nav-containers">
                        <a class="top-nav-containers" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Containers" href="https://catalog.redhat.com/software/containers/explore/" >Containers
                        </a>
                    </li>
                    <li id="nav-support" data-portal-tour-1="3">
                        <a class="top-nav-support-cases" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Support Cases" href="https://access.redhat.com/support/cases/" >Support Cases
                        </a>
                    </li>
                </ul>
            </nav>

            <pfe-navigation id="pfe-navigation" data-analytics-region="mega menu">
                <div class="pfe-navigation__logo-wrapper" id="pfe-navigation__logo-wrapper">
                    <a href="https://access.redhat.com/" class="pfe-navigation__logo-link" data-analytics-text="logo" data-analytics-category="MM|logo">
                        <img class="pfe-navigation__logo-image" alt="Red Hat Customer Portal" src="https://access.redhat.com/chrome_themes/nimbus/img/red-hat-customer-portal.svg" />
                    </a>
                </div>

                <nav class="pfe-navigation" aria-label="Main Navigation" data-analytics-region="main nav">
                    <ul class="pfe-navigation__menu" id="pfe-navigation__menu">                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-subscription--mobile" data-portal-tour-1="1">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Subscription" href="https://access.redhat.com/management/" >Subscriptions
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-downloads--mobile" data-portal-tour-1="2">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Downloads" href="https://access.redhat.com/downloads/" >Downloads
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-containers--mobile">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Containers" href="https://catalog.redhat.com/software/containers/explore/" >Containers
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-support--mobile" data-portal-tour-1="3">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Support Cases" href="https://access.redhat.com/support/cases/" >Support Cases
                            </a>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a href="https://access.redhat.com/products/" class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Products and Services">Products &amp; Services
                            </a>
                            <div class="pfe-navigation__dropdown has-primary-detail">                                <div class="desktop-col-span-2 tablet-col-span-all">
                                    <h3>
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Products" href="https://access.redhat.com/products/">Products
                                        </a>
                                    </h3>
                                    <slot name="main-menu__dropdown--product__product-listing"></slot>
                                </div>                                <div>
                                    <h3 id="nav__products__support">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Support" href="https://access.redhat.com/support">Support
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__support">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Production Support" href="https://access.redhat.com/support/offerings/production/">Production Support
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Development Support" href="https://access.redhat.com/support/offerings/developer/">Development Support
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Product Life Cycles" href="https://access.redhat.com/product-life-cycles/">Product Life Cycles
                                                    </a></li>
                                    </ul>

                                    <h3 id="nav__products__services">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Services" href="https://www.redhat.com/en/services">Services
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__services">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Consulting" href="https://www.redhat.com/en/services/consulting">Consulting
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Technical Account Management" href="https://access.redhat.com/support/offerings/tam/">Technical Account Management
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Training and Certifications" href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications
                                                    </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__products__documentation">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Documentation" href="https://access.redhat.com/documentation">Documentation
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__documentation">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat Enterprise Linux" href="https://access.redhat.com/documentation/en/red_hat_enterprise_linux">Red Hat Enterprise Linux
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat JBoss Enterprise Application Platform" href="https://access.redhat.com/documentation/en/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat OpenStack Platform" href="https://access.redhat.com/documentation/en/red_hat_openstack_platform">Red Hat OpenStack Platform
                                                    </a></li>
                                                    <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat OpenShift Container Platform" href="https://access.redhat.com/documentation/en/openshift_container_platform">Red Hat OpenShift Container Platform
                                                        </a></li>
                                    </ul>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="All Documentation" data-analytics-linkType="cta" href="https://access.redhat.com/documentation">All Documentation
                                        </a>
                                    </pfe-cta>

                                    <h3 id="nav__products__catalog"><a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Ecosystem Catalog" href="https://catalog.redhat.com/">Ecosystem Catalog
                                        </a></h3>
                                        <ul aria-labelledby="nav__products__catalog">
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Ecosystem Catalog" data-analytics-text="Red Hat Partner Ecosystem" href="https://access.redhat.com/ecosystem/">Red Hat Partner Ecosystem
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Ecosystem Catalog" data-analytics-text="Partner Resources" href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources
                                                    </a></li>
                                        </ul>
                                </div>
                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Tools" href="https://access.redhat.com/labs/">Tools
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="nav__tools__tools" data-analytics-level="2" data-analytics-text="Tools" data-analytics-category="Tools">Tools
                                    </h3>
                                    <ul aria-labelledby="nav__tools__tools">
                                        <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Solution Engine" href="https://access.redhat.com/support/cases/#/troubleshoot">Troubleshoot a product issue
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Packages" href="https://access.redhat.com/downloads/content/package-browser">Packages
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Errata" href="https://access.redhat.com/errata/">Errata
                                                    </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__tools__labs">
                                        <a data-analytics-level="2" data-analytics-category="Tools" data-analytics-text="Customer Portal Labs" href="https://access.redhat.com/labs/">Customer Portal Labs
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__tools__labs">
                                        <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Configuration" href="https://access.redhat.com/labs/#!?type=config">Configuration
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Deployment" href="https://access.redhat.com/labs/#!?type=deploy">Deployment
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Security" href="https://access.redhat.com/labs/#!?type=security">Security
                                                    </a></li>                                                    <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Troubleshooting" href="https://access.redhat.com/labs/#!?type=troubleshoot">Troubleshoot
                                                        </a></li>
                                    </ul>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="All Labs" data-analytics-linkType="cta" href="https://access.redhat.com/labs/">All labs
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h4 id="nav__tools__red-hat-insights">
                                        <a data-analytics-level="2" data-analytics-category="Tools" data-analytics-text="Red Hat Insights" href="//www.redhat.com/en/technologies/management/insights">Red Hat Insights
                                        </a>
                                    </h4>
                                    <p>Increase visibility into IT operations to detect and resolve technical issues before they impact your business.</p>
                                    <a data-analytics-level="3" data-analytics-category="Tools|Red Hat Insights" data-analytics-text="Learn more" href="https://www.redhat.com/en/technologies/management/insights">Learn More
                                    </a>
                                    <br>
                                    <a data-analytics-level="3" data-analytics-category="Tools|Red Hat Insights" data-analytics-text="Go to Insights" href="https://cloud.redhat.com/insights">Go to Insights
                                    </a>
                                </div>
                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Security" href="https://access.redhat.com/security/">Security
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="security__security-center">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Red Hat Product Security Center" href="https://access.redhat.com/security">Red Hat Product Security Center
                                        </a>
                                    </h3>
                                    <p>Engage with our Red Hat Product Security team, access security updates, and ensure your environments are not exposed to any known security vulnerabilities.
                                    </p>
                                    <pfe-cta pfe-priority="primary">
                                        <a data-analytics-level="3" data-analytics-category="Security|Red Hat Product Security Center" data-analytics-text="Product Security Center" data-analytics-linkType="cta" href="https://access.redhat.com/security/">Product Security Center
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__security__updates" data-analytics-level="2" data-analytics-text="Security Updates" data-analytics-category="Security">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Security Updates" href="/security">Security Updates
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__security__updates">
                                        <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Security Advisories" href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Red Hat CVE Database" href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Security Labs" href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs
                                                    </a></li>
                                    </ul>
                                    <p class="margin-top-xl">Keep your systems secure with Red Hat&#039;s specialized responses to security vulnerabilities.
                                    </p>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="View Responses" data-analytics-linkType="cta" href="https://access.redhat.com/security/vulnerability">View Responses
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__security__resources">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Resources" href="https://access.redhat.com/security/overview">Resources
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__security__resources">                                            <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Security Blog" href="//redhat.com/en/blog/channel/security">Security Blog
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Security Measurement" href="https://www.redhat.com/security/data/metrics/">Security Measurement
                                                    </a></li>
                                                    <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Severity Ratings" href="https://access.redhat.com/security/updates/classification/">Severity Ratings
                                                        </a></li>
                                                        <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Backporting Policies" href="https://access.redhat.com/security/updates/backporting/">Backporting Policies
                                                            </a></li>
                                                            <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Product Signing (GPG) Keys" href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys
                                                                </a></li>
                                    </ul>
                                </div>

                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a href="https://access.redhat.com/community/" class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Community">Community
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="nav__community__cp-community">
                                        <a href="https://access.redhat.com/community" data-analytics-level="2" data-analytics-text="Customer Portal Community" data-analytics-text="Customer Portal Community" data-analytics-category="Community">Customer Portal Community
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__community__cp-community">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Discussions" href="https://access.redhat.com/discussions">Discussions
                                            </a></li>                                                <li><a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Private Groups" href="https://access.redhat.com/groups/">Private Groups
                                                    </a></li>
                                    </ul>

                                    <pfe-cta pfe-priority="primary">
                                        <a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Community Activity" data-analytics-linkType="cta" href="https://access.redhat.com/community/">Community Activity
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__community__events" data-analytics-level="2" data-analytics-text="Customer Events" data-analytics-category="Community">Customer Events
                                    </h3>
                                    <ul aria-labelledby="nav__community__events">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Customer Events" data-analytics-text="Red Hat Convergence" href="https://access.redhat.com/convergence/">Red Hat Convergence
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Community|Customer Events" data-analytics-text="Red Hat Summit" href="http://www.redhat.com/summit/">Red Hat Summit
                                                </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__community__stories" data-analytics-level="2" data-analytics-text="Stories" data-analytics-category="Community">Stories
                                    </h3>
                                    <ul aria-labelledby="nav__community__stories">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Stories" data-analytics-text="Red Hat Subscription Value" href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-text="You Asked. We Acted." data-analytics-category="Community|Stories" href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Community|Stories" data-analytics-text="Open Source Communities" href="http://www.redhat.com/en/open-source">Open Source Communities
                                                    </a></li>
                                    </ul>
                                </div>
                            </div>
                        </li>
                    </ul>                </nav>                <div id="site-search" slot="search" class="utility-link site-search">
                    <div class="content">
                        <form class="ng-pristine ng-valid topSearchForm" id="topSearchForm" name="topSearchForm" action="/search/browse/search/" method="get" enctype="application/x-www-form-urlencoded">
                            <cp-search-autocomplete class="push-bottom" path="/webassets/avalon/j/data.json"></cp-search-autocomplete>                            <div>Or <a href="/support/cases/#/troubleshoot">troubleshoot an issue</a>.
                            </div>
                        </form>
                    </div>
                </div>


                <div slot="secondary-links" id="localesMenu">
                    <button class="pfe-navigation__secondary-link">
                        <pfe-icon icon="web-icon-globe" size="sm" aria-hidden="true"></pfe-icon>English
                    </button>

                    <pfe-navigation-dropdown dropdown-width="single">
                        <h2 class="utility-header">Select Your Language
                        </h2>
                        <ul class="reset">
                            <li><a href="https://access.redhat.com/changeLanguage?language=en" data-lang="en" id="en" data-analytics-text="English">English</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=ko" data-lang="ko" id="ko" data-analytics-text="Korean">한국어</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=ja"    data-lang="ja"    id="ja" data-analytics-text="Japanese">日本語</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" data-lang="zh_CN" id="zh_CN" data-analytics-text="Chinese">中文 (中国)</a></li>
                        </ul>

                    </pfe-navigation-dropdown>
                </div>                <rh-account-dropdown slot="account"></rh-account-dropdown>                <pfe-primary-detail breakpoint-width="600" class="main-menu__dropdown--product__product-listing" slot="main-menu__dropdown--product__product-listing" consistent-height>
                    <h3 slot="details-nav">Infrastructure and Management                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Enterprise Linux" href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Satellite" href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Subscription Management" href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Insights" href="https://access.redhat.com/products/red-hat-insights/">Red Hat Insights
                                </a>
                            </li>
                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Ansible Automation Platform" href="https://access.redhat.com/products/red-hat-ansible-automation-platform/">Red Hat Ansible Automation Platform
                                </a></li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Cloud Computing                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift" href="https://access.redhat.com/products/openshift">Red Hat OpenShift
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenStack Platform" href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat OpenShift Container Platform" href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat OpenShift Data Science" href="https://access.redhat.com/products/red-hat-openshift-data-science/">Red Hat OpenShift Data Science
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift Dedicated" href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat Advanced Cluster Security for Kubernetes" href="https://access.redhat.com/products/red-hat-advanced-cluster-security-for-kubernetes/">Red Hat Advanced Cluster Security for Kubernetes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat Advanced Cluster Management for Kubernetes" href="https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes/">Red Hat Advanced Cluster Management for Kubernetes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat Quay" href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat CodeReady Workspaces" href="https://access.redhat.com/products/red-hat-codeready-workspaces/">OpenShift Dev Spaces
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift Service on AWS" href="https://access.redhat.com/products/red-hat-openshift-service-aws">Red Hat OpenShift Service on AWS
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Storage                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Gluster Storage" href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Hyperconverged Infrastructure" href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Ceph Storage" href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Openshift Container Storage" href="https://access.redhat.com/products/red-hat-openshift-data-foundation">Red Hat OpenShift Data Foundation
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Runtimes                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Runtimes" href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat JBoss Enterprise Application Platform" href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Data Grid" href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat JBoss Web Server" href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Single Sign On" href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat support for Spring Boot" href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat build of Node.js" href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js
                                </a>
                            </li>                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat build of Quarkus" href="https://access.redhat.com/products/quarkus/">Red Hat build of Quarkus
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Integration and Automation                    </h3>
                    <div slot="details">
                        <ul class="border-bottom" id="portal-menu-border-bottom">
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat Application Foundations" href="https://access.redhat.com/products/red-hat-application-foundations/">Red Hat Application Foundations
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat Fuse" href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat AMQ" href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat 3scale API Management" href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management
                                </a>
                            </li>
                        </ul>
                    </div>
                    <div slot="details-nav--footer">
                        <pfe-cta pfe-priority="primary">
                            <a href="https://access.redhat.com/products/" class="pfe-navigation__menu-link" data-analytics-level="2" data-analytics-text="All Products" data-analytics-category="Products and Services|Products:" data-analytics-linkType="cta">All Products
                            </a>
                        </pfe-cta>
                    </div>
                </pfe-primary-detail>

            </pfe-navigation>

            <div id="scroll-anchor"></div>

            <!--[if IE 8]>
                <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                <span class="icon-warning alert-icon" aria-hidden="true"></span>
                You are using an unsupported web browser. Update to a supported browser for the best experience. <a href="/announcements/2120951">Read the announcement</a>.
                </div>
                </div>
            <![endif]-->
            <!--[if IE 9]>
                <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                <span class="icon-warning alert-icon" aria-hidden="true"></span>As of March 1, 2016, the Red Hat Customer Portal will no longer support Internet Explorer 9. See our new <a href="/help/browsers">browser support policy</a> for more information.
                </div>
                </div>
            <![endif]-->
            <div id="site-section"></div>
        </header>
        <!--googleon: all-->

        <main id="cp-main" class="portal-content-area">
            <div id="cp-content" class="main-content">                            <!-- /CP_PRIMER_HEADER -->

      <div class="container">
        

                                                                                                        <script>breadcrumbs = [["Products & Services","\/products\/"],["Product Documentation","\/documentation"],["OpenShift Container Platform","\/documentation\/en-us\/openshift_container_platform"],["4.13","\/documentation\/en-us\/openshift_container_platform\/4.13"],["Support","\/documentation\/en-us\/openshift_container_platform\/4.13\/html\/support"],["Support","\/documentation\/en-us\/openshift_container_platform\/4.13\/html\/support\/--single-page-document--"]]</script>

<div data-drupal-messages-fallback class="hidden"></div>


    </div>
        <div class="container">
        

  

  


  <article class="pvof-doc__content-wrapper__outer pvof-doc__content-wrapper__outer--css-not-removed">
    <script>
      'use strict';

            var $outerWrapper = document.querySelector('.pvof-doc__content-wrapper__outer');
      if ($outerWrapper && $outerWrapper.closest) {
        var $containerWrapper = $outerWrapper.closest('.container');
        if ($containerWrapper) {
          $containerWrapper.classList.remove('container');
          $containerWrapper.classList.add('j-chrome-content-container');
        }
      }

            var cssRemoved = false;
      try {
        var $crapCss = document.querySelectorAll(
          'link[href*="/chrome_themes/nimbus/css/pages.css"], link[href*="/chrome_themes/nimbus/css/components.css"]'
        );
        if ($crapCss.length) {
          for (let index = 0; index < $crapCss.length; index++) {
            const $stylesheet = $crapCss[index];
            $stylesheet.remove();
          }
        }
        cssRemoved = true;
      }
      catch (error) {
        console.error('Ran into an issue while trying to retheme page', error);
        cssRemoved = false;
      }

            if (cssRemoved) {
        var $pvofOuterWrapper = document.querySelector('.pvof-doc__content-wrapper__outer--css-not-removed');
        if ($pvofOuterWrapper) {
          $pvofOuterWrapper.classList.remove('pvof-doc__content-wrapper__outer--css-not-removed');
        }
      }
    </script>
    <div itemscope="" itemtype="https://schema.org/TechArticle" itemref="techArticle-md1 techArticle-md2 techArticle-md3"></div>
    <div itemscope="" itemtype="https://schema.org/SoftwareApplication" itemref="softwareApplication-md1 softwareApplication-md2 softwareApplication-md3 softwareApplication-md4"></div>
    <div class="pvof-doc__content-wrapper pvof-doc__content-wrapper--has-sidebar">
                                <div class="pvof-doc__content-wrapper__inner j-superdoc j-superdoc--has-nav">
                            <div class="pvof-sidebar__wrapper j-doc-nav j-superdoc__nav">
            <div class="j-sidebar__menu-container">
              <button class="j-sidebar__menu-trigger content-expander__trigger">
                <span class="j-sidebar__menu-trigger__open-text">Jump To</span>
                <span class="j-sidebar__menu-trigger__close-text">Close</span>
              </button>

              <div class="pvof-sidebar__inner-wrapper j-doc-nav__wrapper content-expander">
                <div class="j-sidebar__menu-details-container">
                  <button class="j-sidebar__menu-details-button j-sidebar__menu-details-button--expand">
                    Expand all
                  </button>
                  <button class="j-sidebar__menu-details-button j-sidebar__menu-details-button--collapse">
                    Collapse all
                  </button>
                </div>
                

  <nav id="pvof-doc__toc" class="pvof-doc__toc">
  <h2 class="j-doc-nav__title" id="j-doc-nav__title">
    Table of contents
  </h2>
  <div class="pvof-doc__toc-inner">
              <ol class="j-doc-nav__list" aria-labelledby="j-doc-nav__title">
                  <li class="j-doc-nav__list-item">
                                    

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support" class="j-doc-nav__link ">
    Support
  </a>
  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#support-overview" class="j-doc-nav__link j-doc-nav__link--has-children">
    1. Support overview
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "1. Support overview"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "1. Support overview"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#support-overview-get-support" class="j-doc-nav__link ">
    1.1. Get support
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#support-overview-remote-health-monitoring" class="j-doc-nav__link ">
    1.2. Remote health monitoring issues
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#support-overview-gather-data-cluster" class="j-doc-nav__link ">
    1.3. Gather data about your cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#support-overview-troubleshooting-issues" class="j-doc-nav__link ">
    1.4. Troubleshooting issues
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#managing-cluster-resources" class="j-doc-nav__link j-doc-nav__link--has-children">
    2. Managing your cluster resources
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2. Managing your cluster resources"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2. Managing your cluster resources"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#support-cluster-resources_managing-cluster-resources" class="j-doc-nav__link ">
    2.1. Interacting with your cluster resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#getting-support" class="j-doc-nav__link j-doc-nav__link--has-children">
    3. Getting support
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3. Getting support"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3. Getting support"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#support_getting-support" class="j-doc-nav__link ">
    3.1. Getting support
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#support-knowledgebase-about_getting-support" class="j-doc-nav__link ">
    3.2. About the Red Hat Knowledgebase
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#support-knowledgebase-search_getting-support" class="j-doc-nav__link ">
    3.3. Searching the Red Hat Knowledgebase
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#support-submitting-a-case_getting-support" class="j-doc-nav__link ">
    3.4. Submitting a support case
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#getting-support-additional-resources" class="j-doc-nav__link ">
    3.5. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#remote-health-monitoring-with-connected-clusters" class="j-doc-nav__link j-doc-nav__link--has-children">
    4. Remote health monitoring with connected clusters
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4. Remote health monitoring with connected clusters"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4. Remote health monitoring with connected clusters"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#about-remote-health-monitoring" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.1. About remote health monitoring
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.1. About remote health monitoring"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.1. About remote health monitoring"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#telemetry-about-telemetry_about-remote-health-monitoring" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.1.1. About Telemetry
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.1.1. About Telemetry"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.1.1. About Telemetry"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#what-information-is-collected_about-remote-health-monitoring" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.1.1.1. Information collected by Telemetry
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.1.1.1. Information collected by Telemetry"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.1.1.1. Information collected by Telemetry"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#system-information_about-remote-health-monitoring" class="j-doc-nav__link ">
    4.1.1.1.1. System information
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#sizing-information_about-remote-health-monitoring" class="j-doc-nav__link ">
    4.1.1.1.2. Sizing Information
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#usage-information_about-remote-health-monitoring" class="j-doc-nav__link ">
    4.1.1.1.3. Usage information
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#insights-operator-about_about-remote-health-monitoring" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.1.2. About the Insights Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.1.2. About the Insights Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.1.2. About the Insights Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#insights-operator-what-information-is-collected_about-remote-health-monitoring" class="j-doc-nav__link ">
    4.1.2.1. Information collected by the Insights Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#understanding-telemetry-and-insights-operator-data-flow_about-remote-health-monitoring" class="j-doc-nav__link ">
    4.1.3. Understanding Telemetry and Insights Operator data flow
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#additional-details-about-how-remote-health-monitoring-data-is-used" class="j-doc-nav__link ">
    4.1.4. Additional details about how remote health monitoring data is used
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#showing-data-collected-by-remote-health-monitoring" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.2. Showing data collected by remote health monitoring
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.2. Showing data collected by remote health monitoring"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.2. Showing data collected by remote health monitoring"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#showing-data-collected-from-the-cluster_showing-data-collected-by-remote-health-monitoring" class="j-doc-nav__link ">
    4.2.1. Showing data collected by Telemetry
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#insights-operator-showing-data-collected-from-the-cluster_showing-data-collected-by-remote-health-monitoring" class="j-doc-nav__link ">
    4.2.2. Showing data collected by the Insights Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#opting-out-remote-health-reporting" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.3. Opting out of remote health reporting
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.3. Opting out of remote health reporting"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.3. Opting out of remote health reporting"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#telemetry-consequences-of-disabling-telemetry_opting-out-remote-health-reporting" class="j-doc-nav__link ">
    4.3.1. Consequences of disabling remote health reporting
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#insights-operator-new-pull-secret_opting-out-remote-health-reporting" class="j-doc-nav__link ">
    4.3.2. Modifying the global cluster pull secret to disable remote health reporting
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#insights-operator-register-disconnected-cluster_opting-out-remote-health-reporting" class="j-doc-nav__link ">
    4.3.3. Registering your disconnected cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#images-update-global-pull-secret_opting-out-remote-health-reporting" class="j-doc-nav__link ">
    4.3.4. Updating the global cluster pull secret
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#enabling-remote-health-reporting" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.4. Enabling remote health reporting
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.4. Enabling remote health reporting"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.4. Enabling remote health reporting"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#insights-operator-new-pull-secret-enable_enabling-remote-health-reporting" class="j-doc-nav__link ">
    4.4.1. Modifying your global cluster pull secret to enable remote health reporting
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#using-insights-to-identify-issues-with-your-cluster" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.5. Using Insights to identify issues with your cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.5. Using Insights to identify issues with your cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.5. Using Insights to identify issues with your cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#insights-operator-advisor-overview_using-insights-to-identify-issues-with-your-cluster" class="j-doc-nav__link ">
    4.5.1. About Red Hat Insights Advisor for OpenShift Container Platform
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#insights-operator-advisor-recommendations_using-insights-to-identify-issues-with-your-cluster" class="j-doc-nav__link ">
    4.5.2. Understanding Insights Advisor recommendations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#displaying-potential-issues-with-your-cluster_using-insights-to-identify-issues-with-your-cluster" class="j-doc-nav__link ">
    4.5.3. Displaying potential issues with your cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#displaying-all-insights-advisor-recommendations_using-insights-to-identify-issues-with-your-cluster" class="j-doc-nav__link ">
    4.5.4. Displaying all Insights Advisor recommendations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#disabling-insights-advisor-recommendations_using-insights-to-identify-issues-with-your-cluster" class="j-doc-nav__link ">
    4.5.5. Disabling Insights Advisor recommendations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#enabling-insights-advisor-recommendations_using-insights-to-identify-issues-with-your-cluster" class="j-doc-nav__link ">
    4.5.6. Enabling a previously disabled Insights Advisor recommendation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#displaying-the-insights-status-in-the-web-console_using-insights-to-identify-issues-with-your-cluster" class="j-doc-nav__link ">
    4.5.7. Displaying the Insights status in the web console
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#using-insights-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.6. Using Insights Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.6. Using Insights Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.6. Using Insights Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#understanding-insights-operator-alerts_using-insights-operator" class="j-doc-nav__link ">
    4.6.1. Understanding Insights Operator alerts
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#disabling-insights-operator-alerts_using-insights-operator" class="j-doc-nav__link ">
    4.6.2. Disabling Insights Operator alerts
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#insights-operator-downloading-archive_using-insights-operator" class="j-doc-nav__link ">
    4.6.3. Downloading your Insights Operator archive
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#insights-operator-gather-duration_using-insights-operator" class="j-doc-nav__link ">
    4.6.4. Viewing Insights Operator gather durations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#disabling-insights-operator-gather_using-insights-operator" class="j-doc-nav__link ">
    4.6.5. Disabling the Insights Operator gather operations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#insights-operator-configuring-sca_using-insights-operator" class="j-doc-nav__link ">
    4.6.6. Configuring Insights Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#remote-health-reporting-from-restricted-network" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.7. Using remote health reporting in a restricted network
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.7. Using remote health reporting in a restricted network"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.7. Using remote health reporting in a restricted network"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#insights-operator-one-time-gather_remote-health-reporting-from-restricted-network" class="j-doc-nav__link ">
    4.7.1. Running an Insights Operator gather operation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#insights-operator-manual-upload_remote-health-reporting-from-restricted-network" class="j-doc-nav__link ">
    4.7.2. Uploading an Insights Operator archive
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#insights-operator-enable-obfuscation_remote-health-reporting-from-restricted-network" class="j-doc-nav__link ">
    4.7.3. Enabling Insights Operator data obfuscation
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#insights-operator-simple-access" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.8. Importing simple content access entitlements with Insights Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.8. Importing simple content access entitlements with Insights Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.8. Importing simple content access entitlements with Insights Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#insights-operator-configuring-sca_remote-health-reporting-from-restricted-network" class="j-doc-nav__link ">
    4.8.1. Configuring simple content access import interval
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#insights-operator-disabling-sca_remote-health-reporting-from-restricted-network" class="j-doc-nav__link ">
    4.8.2. Disabling simple content access import
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#gathering-cluster-data" class="j-doc-nav__link j-doc-nav__link--has-children">
    5. Gathering data about your cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "5. Gathering data about your cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "5. Gathering data about your cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#about-must-gather_gathering-cluster-data" class="j-doc-nav__link j-doc-nav__link--has-children">
    5.1. About the must-gather tool
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "5.1. About the must-gather tool"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "5.1. About the must-gather tool"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#support_gathering_data_gathering-cluster-data" class="j-doc-nav__link ">
    5.1.1. Gathering data about your cluster for Red Hat Support
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#gathering-data-specific-features_gathering-cluster-data" class="j-doc-nav__link ">
    5.1.2. Gathering data about specific features
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#additional-resources" class="j-doc-nav__link j-doc-nav__link--has-children">
    5.2. Additional resources
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "5.2. Additional resources"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "5.2. Additional resources"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#gathering-data-network-logs_gathering-cluster-data" class="j-doc-nav__link ">
    5.2.1. Gathering network logs
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#support-get-cluster-id_gathering-cluster-data" class="j-doc-nav__link ">
    5.3. Obtaining your cluster ID
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#about-sosreport_gathering-cluster-data" class="j-doc-nav__link ">
    5.4. About sosreport
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#support-generating-a-sosreport-archive_gathering-cluster-data" class="j-doc-nav__link ">
    5.5. Generating a sosreport archive for an OpenShift Container Platform cluster node
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#querying-bootstrap-node-journal-logs_gathering-cluster-data" class="j-doc-nav__link ">
    5.6. Querying bootstrap node journal logs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#querying-cluster-node-journal-logs_gathering-cluster-data" class="j-doc-nav__link ">
    5.7. Querying cluster node journal logs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#support-network-trace-methods_gathering-cluster-data" class="j-doc-nav__link ">
    5.8. Network trace methods
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#support-collecting-host-network-trace_gathering-cluster-data" class="j-doc-nav__link ">
    5.9. Collecting a host network trace
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#support-collecting-network-trace_gathering-cluster-data" class="j-doc-nav__link ">
    5.10. Collecting a network trace from an OpenShift Container Platform node or container
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#support-providing-diagnostic-data-to-red-hat_gathering-cluster-data" class="j-doc-nav__link ">
    5.11. Providing diagnostic data to Red Hat Support
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#about-toolbox_gathering-cluster-data" class="j-doc-nav__link ">
    5.12. About toolbox
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#summarizing-cluster-specifications" class="j-doc-nav__link j-doc-nav__link--has-children">
    6. Summarizing cluster specifications
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6. Summarizing cluster specifications"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6. Summarizing cluster specifications"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#summarizing-cluster-specifications-through-clusterversion_summarizing-cluster-specifications" class="j-doc-nav__link ">
    6.1. Summarizing cluster specifications through clusterversion
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#troubleshooting" class="j-doc-nav__link j-doc-nav__link--has-children">
    7. Troubleshooting
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7. Troubleshooting"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7. Troubleshooting"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#troubleshooting-installations" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.1. Troubleshooting installations
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.1. Troubleshooting installations"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.1. Troubleshooting installations"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#determining-where-installation-issues-occur_troubleshooting-installations" class="j-doc-nav__link ">
    7.1.1. Determining where installation issues occur
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#upi-installation-considerations_troubleshooting-installations" class="j-doc-nav__link ">
    7.1.2. User-provisioned infrastructure installation considerations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#checking-load-balancer-configuration_troubleshooting-installations" class="j-doc-nav__link ">
    7.1.3. Checking a load balancer configuration before OpenShift Container Platform installation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#specifying-openshift-installer-log-levels_troubleshooting-installations" class="j-doc-nav__link ">
    7.1.4. Specifying OpenShift Container Platform installer log levels
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#troubleshooting-openshift-install-command-issues_troubleshooting-installations" class="j-doc-nav__link ">
    7.1.5. Troubleshooting openshift-install command issues
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#monitoring-installation-progress_troubleshooting-installations" class="j-doc-nav__link ">
    7.1.6. Monitoring installation progress
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#gathering-bootstrap-diagnostic-data_troubleshooting-installations" class="j-doc-nav__link ">
    7.1.7. Gathering bootstrap node diagnostic data
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#investigating-master-node-installation-issues_troubleshooting-installations" class="j-doc-nav__link ">
    7.1.8. Investigating control plane node installation issues
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#investigating-etcd-installation-issues_troubleshooting-installations" class="j-doc-nav__link ">
    7.1.9. Investigating etcd installation issues
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#investigating-kubelet-api-installation-issues_troubleshooting-installations" class="j-doc-nav__link ">
    7.1.10. Investigating control plane node kubelet and API server issues
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#investigating-worker-node-installation-issues_troubleshooting-installations" class="j-doc-nav__link ">
    7.1.11. Investigating worker node installation issues
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#querying-operator-status-after-installation_troubleshooting-installations" class="j-doc-nav__link ">
    7.1.12. Querying Operator status after installation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#installation-bootstrap-gather_troubleshooting-installations" class="j-doc-nav__link ">
    7.1.13. Gathering logs from a failed installation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#additional-resources-2" class="j-doc-nav__link ">
    7.1.14. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#verifying-node-health" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.2. Verifying node health
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.2. Verifying node health"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.2. Verifying node health"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#reviewing-node-status-use-and-configuration_verifying-node-health" class="j-doc-nav__link ">
    7.2.1. Reviewing node status, resource usage, and configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#querying-kubelet-status-on-a-node_verifying-node-health" class="j-doc-nav__link ">
    7.2.2. Querying the kubelet’s status on a node
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#querying-cluster-node-journal-logs_verifying-node-health" class="j-doc-nav__link ">
    7.2.3. Querying cluster node journal logs
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#troubleshooting-crio-issues" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.3. Troubleshooting CRI-O container runtime issues
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.3. Troubleshooting CRI-O container runtime issues"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.3. Troubleshooting CRI-O container runtime issues"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#about-crio_troubleshooting-crio-issues" class="j-doc-nav__link ">
    7.3.1. About CRI-O container runtime engine
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#verifying-crio-status_troubleshooting-crio-issues" class="j-doc-nav__link ">
    7.3.2. Verifying CRI-O runtime engine status
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#gathering-crio-logs_troubleshooting-crio-issues" class="j-doc-nav__link ">
    7.3.3. Gathering CRI-O journald unit logs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#cleaning-crio-storage_troubleshooting-crio-issues" class="j-doc-nav__link ">
    7.3.4. Cleaning CRI-O storage
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#troubleshooting-operating-system-issues" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.4. Troubleshooting operating system issues
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.4. Troubleshooting operating system issues"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.4. Troubleshooting operating system issues"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#investigating-kernel-crashes" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.4.1. Investigating kernel crashes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.4.1. Investigating kernel crashes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.4.1. Investigating kernel crashes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#enabling-kdump" class="j-doc-nav__link ">
    7.4.1.1. Enabling kdump
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#enabling-kdump-day-one" class="j-doc-nav__link ">
    7.4.1.2. Enabling kdump on day-1
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#testing-kdump-configuration" class="j-doc-nav__link ">
    7.4.1.3. Testing the kdump configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#analyzing-core-dumps" class="j-doc-nav__link ">
    7.4.1.4. Analyzing a core dump
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#debugging-ignition_troubleshooting-operating-system-issues" class="j-doc-nav__link ">
    7.4.2. Debugging Ignition failures
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#troubleshooting-network-issues" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.5. Troubleshooting network issues
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.5. Troubleshooting network issues"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.5. Troubleshooting network issues"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#nw-how-nw-iface-selected_troubleshooting-network-issues" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.5.1. How the network interface is selected
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.5.1. How the network interface is selected"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.5.1. How the network interface is selected"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#overriding-default-node-ip-selection-logic_troubleshooting-network-issues" class="j-doc-nav__link ">
    7.5.1.1. Optional: Overriding the default node IP selection logic
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#nw-troubleshoot-ovs_troubleshooting-network-issues" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.5.2. Troubleshooting Open vSwitch issues
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.5.2. Troubleshooting Open vSwitch issues"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.5.2. Troubleshooting Open vSwitch issues"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#configuring-ovs-log-level-temp_troubleshooting-network-issues" class="j-doc-nav__link ">
    7.5.2.1. Configuring the Open vSwitch log level temporarily
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#configuring-ovs-log-level-permanently_troubleshooting-network-issues" class="j-doc-nav__link ">
    7.5.2.2. Configuring the Open vSwitch log level permanently
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#displaying-ovs-logs_troubleshooting-network-issues" class="j-doc-nav__link ">
    7.5.2.3. Displaying Open vSwitch logs
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#troubleshooting-operator-issues" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.6. Troubleshooting Operator issues
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.6. Troubleshooting Operator issues"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.6. Troubleshooting Operator issues"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#olm-status-conditions_troubleshooting-operator-issues" class="j-doc-nav__link ">
    7.6.1. Operator subscription condition types
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#olm-status-viewing-cli_troubleshooting-operator-issues" class="j-doc-nav__link ">
    7.6.2. Viewing Operator subscription status by using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#olm-cs-status-cli_troubleshooting-operator-issues" class="j-doc-nav__link ">
    7.6.3. Viewing Operator catalog source status by using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#querying-operator-pod-status_troubleshooting-operator-issues" class="j-doc-nav__link ">
    7.6.4. Querying Operator pod status
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#gathering-operator-logs_troubleshooting-operator-issues" class="j-doc-nav__link ">
    7.6.5. Gathering Operator logs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#troubleshooting-disabling-autoreboot-mco_troubleshooting-operator-issues" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.6.6. Disabling the Machine Config Operator from automatically rebooting
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.6.6. Disabling the Machine Config Operator from automatically rebooting"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.6.6. Disabling the Machine Config Operator from automatically rebooting"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#troubleshooting-disabling-autoreboot-mco-console_troubleshooting-operator-issues" class="j-doc-nav__link ">
    7.6.6.1. Disabling the Machine Config Operator from automatically rebooting by using the console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#troubleshooting-disabling-autoreboot-mco-cli_troubleshooting-operator-issues" class="j-doc-nav__link ">
    7.6.6.2. Disabling the Machine Config Operator from automatically rebooting by using the CLI
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#olm-refresh-subs_troubleshooting-operator-issues" class="j-doc-nav__link ">
    7.6.7. Refreshing failing subscriptions
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#olm-reinstall_troubleshooting-operator-issues" class="j-doc-nav__link ">
    7.6.8. Reinstalling Operators after failed uninstallation
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#investigating-pod-issues" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.7. Investigating pod issues
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.7. Investigating pod issues"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.7. Investigating pod issues"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#understanding-pod-error-states_investigating-pod-issues" class="j-doc-nav__link ">
    7.7.1. Understanding pod error states
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#reviewing-pod-status_investigating-pod-issues" class="j-doc-nav__link ">
    7.7.2. Reviewing pod status
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#inspecting-pod-and-container-logs_investigating-pod-issues" class="j-doc-nav__link ">
    7.7.3. Inspecting pod and container logs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#accessing-running-pods_investigating-pod-issues" class="j-doc-nav__link ">
    7.7.4. Accessing running pods
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#starting-debug-pods-with-root-access_investigating-pod-issues" class="j-doc-nav__link ">
    7.7.5. Starting debug pods with root access
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#copying-files-pods-and-containers_investigating-pod-issues" class="j-doc-nav__link ">
    7.7.6. Copying files to and from pods and containers
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#troubleshooting-s2i" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.8. Troubleshooting the Source-to-Image process
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.8. Troubleshooting the Source-to-Image process"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.8. Troubleshooting the Source-to-Image process"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#strategies-for-s2i-troubleshooting_troubleshooting-s2i" class="j-doc-nav__link ">
    7.8.1. Strategies for Source-to-Image troubleshooting
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#gathering-s2i-diagnostic-data_troubleshooting-s2i" class="j-doc-nav__link ">
    7.8.2. Gathering Source-to-Image diagnostic data
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#gathering-application-diagnostic-data_troubleshooting-s2i" class="j-doc-nav__link ">
    7.8.3. Gathering application diagnostic data to investigate application failures
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#additional-resources-3" class="j-doc-nav__link ">
    7.8.4. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#troubleshooting-storage-issues" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.9. Troubleshooting storage issues
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.9. Troubleshooting storage issues"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.9. Troubleshooting storage issues"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#storage-multi-attach-error_troubleshooting-storage-issues" class="j-doc-nav__link ">
    7.9.1. Resolving multi-attach errors
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#troubleshooting-windows-container-workload-issues" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.10. Troubleshooting Windows container workload issues
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.10. Troubleshooting Windows container workload issues"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.10. Troubleshooting Windows container workload issues"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#wmco-does-not-install_troubleshooting-windows-container-workload-issues" class="j-doc-nav__link ">
    7.10.1. Windows Machine Config Operator does not install
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#investigating-why-windows-machine-compute-node_troubleshooting-windows-container-workload-issues" class="j-doc-nav__link ">
    7.10.2. Investigating why Windows Machine does not become compute node
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#accessing-windows-node" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.10.3. Accessing a Windows node
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.10.3. Accessing a Windows node"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.10.3. Accessing a Windows node"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#accessing-windows-node-using-ssh_troubleshooting-windows-container-workload-issues" class="j-doc-nav__link ">
    7.10.3.1. Accessing a Windows node using SSH
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#accessing-windows-node-using-rdp_troubleshooting-windows-container-workload-issues" class="j-doc-nav__link ">
    7.10.3.2. Accessing a Windows node using RDP
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#collecting-kube-node-logs-windows_troubleshooting-windows-container-workload-issues" class="j-doc-nav__link ">
    7.10.4. Collecting Kubernetes node logs for Windows containers
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#collecting-windows-application-event-logs_troubleshooting-windows-container-workload-issues" class="j-doc-nav__link ">
    7.10.5. Collecting Windows application event logs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#collecting-docker-logs-windows_troubleshooting-windows-container-workload-issues" class="j-doc-nav__link ">
    7.10.6. Collecting Docker logs for Windows containers
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#additional-resources-4" class="j-doc-nav__link ">
    7.10.7. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#investigating-monitoring-issues" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.11. Investigating monitoring issues
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.11. Investigating monitoring issues"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.11. Investigating monitoring issues"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#investigating-why-user-defined-metrics-are-unavailable_investigating-monitoring-issues" class="j-doc-nav__link ">
    7.11.1. Investigating why user-defined project metrics are unavailable
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#determining-why-prometheus-is-consuming-disk-space_investigating-monitoring-issues" class="j-doc-nav__link ">
    7.11.2. Determining why Prometheus is consuming a lot of disk space
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#diagnosing-oc-issues" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.12. Diagnosing OpenShift CLI (oc) issues
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.12. Diagnosing OpenShift CLI (oc) issues"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.12. Diagnosing OpenShift CLI (oc) issues"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#understanding-oc-log-levels_diagnosing-oc-issues" class="j-doc-nav__link ">
    7.12.1. Understanding OpenShift CLI (oc) log levels
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#specifying-oc-log-levels_diagnosing-oc-issues" class="j-doc-nav__link ">
    7.12.2. Specifying OpenShift CLI (oc) log levels
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                    

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support#idm140604671788160" class="j-doc-nav__link ">
    Legal Notice
  </a>
  
          </li>
              </ol>
    
  </div>
</nav>


              </div>
            </div>
            <div class="j-options-container j-options-container--mobile">
              <button class="j-sidebar__menu-trigger j-sidebar__menu-trigger--options content-expander__trigger">
                <span class="j-sidebar__menu-trigger__open-headline">
                  Settings
                </span>
                <span class="j-sidebar__menu-trigger__close-text">Close</span>
              </button>
              

  <ul class="j-doc-options__list content-expander">
    <li class="j-doc-options__item">
          <label class="j-doc-option__label j-doc-option__label--language" for="j-doc-language">
        Language:
      </label>
      <select id="j-doc-language" class="j-doc-option__select">
                  <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/support" selected=''>
            English
          </option>
                  <option value="/documentation/ja-jp/openshift_container_platform/4.13/html-single/support" >
            日本語
          </option>
                  <option value="/documentation/zh-cn/openshift_container_platform/4.13/html-single/support" >
            简体中文
          </option>
                  <option value="/documentation/ko-kr/openshift_container_platform/4.13/html-single/support" >
            한국어
          </option>
              </select>

            <noscript>
        <div class="j-doc-option__label j-doc-option__label--language" id="j-doc-option__label--language--nojs">
          Language:
        </div>
        <ul aria-labelledby="j-doc-option__label--language--nojs" class="j-doc-option__languages-list">
                      <li>
              <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support">English</a>
            </li>
                      <li>
              <a href="/documentation/ja-jp/openshift_container_platform/4.13/html-single/support">日本語</a>
            </li>
                      <li>
              <a href="/documentation/zh-cn/openshift_container_platform/4.13/html-single/support">简体中文</a>
            </li>
                      <li>
              <a href="/documentation/ko-kr/openshift_container_platform/4.13/html-single/support">한국어</a>
            </li>
                  </ul>
      </noscript>

      </li>

    <li class="j-doc-options__item">
    <label for="j-doc-mode" class="j-doc-option__label j-doc-option__label--format">
      Format:
    </label>
    <select id="j-doc-mode" class="j-doc-option__select">
              <option value="/documentation/en-us/openshift_container_platform/4.13/html/support"  class="j-doc-options__option j-doc-options__option--multi-page">
          Multi-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/support" selected='' class="j-doc-options__option j-doc-options__option--single-page">
          Single-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/pdf/support/OpenShift_Container_Platform-4.13-Support-en-US.pdf"  class="j-doc-options__option j-doc-options__option--pdf">
          PDF
        </option>
          </select>

        <noscript>
      <div class="j-doc-option__label j-doc-option__label--format" id="j-doc-option__label--format--nojs">
        Format:
      </div>
      <ul class="j-doc-option__format-list" aria-labelledby="j-doc-option__label--format--nojs">
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--multi-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html/support">Multi-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--single-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support">Single-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--pdf"><a href="/documentation/en-us/openshift_container_platform/4.13/pdf/support/OpenShift_Container_Platform-4.13-Support-en-US.pdf">PDF</a></li>
              </ul>
    </noscript>
  </li>
</ul>


              </div>
          </div>
                <div class="pvof-doc__tertiary-sidebar j-doc__tertiary-sidebar">
          <div class="pvof-doc__tertiary-sidebar__inner j-doc__tertiary-sidebar__inner">
            <div class="j-doc__doc-options">
              <div class="j-options-container j-options-container--desktop">
                <button class="j-sidebar__menu-trigger j-sidebar__menu-trigger--tablet content-expander__trigger">
                  <span class="j-sidebar__menu-trigger-icon"></span>
                  <h2 class="visually-hidden">Language and Page Formatting Options</h2>
                </button>
                  

  <ul class="j-doc-options__list content-expander">
    <li class="j-doc-options__item">
          <label class="j-doc-option__label j-doc-option__label--language" for="j-doc-language--2">
        Language:
      </label>
      <select id="j-doc-language--2" class="j-doc-option__select">
                  <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/support" selected=''>
            English
          </option>
                  <option value="/documentation/ja-jp/openshift_container_platform/4.13/html-single/support" >
            日本語
          </option>
                  <option value="/documentation/zh-cn/openshift_container_platform/4.13/html-single/support" >
            简体中文
          </option>
                  <option value="/documentation/ko-kr/openshift_container_platform/4.13/html-single/support" >
            한국어
          </option>
              </select>

            <noscript>
        <div class="j-doc-option__label j-doc-option__label--language" id="j-doc-option__label--language--nojs">
          Language:
        </div>
        <ul aria-labelledby="j-doc-option__label--language--nojs" class="j-doc-option__languages-list">
                      <li>
              <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support">English</a>
            </li>
                      <li>
              <a href="/documentation/ja-jp/openshift_container_platform/4.13/html-single/support">日本語</a>
            </li>
                      <li>
              <a href="/documentation/zh-cn/openshift_container_platform/4.13/html-single/support">简体中文</a>
            </li>
                      <li>
              <a href="/documentation/ko-kr/openshift_container_platform/4.13/html-single/support">한국어</a>
            </li>
                  </ul>
      </noscript>

      </li>

    <li class="j-doc-options__item">
    <label for="j-doc-mode--2" class="j-doc-option__label j-doc-option__label--format">
      Format:
    </label>
    <select id="j-doc-mode--2" class="j-doc-option__select">
              <option value="/documentation/en-us/openshift_container_platform/4.13/html/support"  class="j-doc-options__option j-doc-options__option--multi-page">
          Multi-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/support" selected='' class="j-doc-options__option j-doc-options__option--single-page">
          Single-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/pdf/support/OpenShift_Container_Platform-4.13-Support-en-US.pdf"  class="j-doc-options__option j-doc-options__option--pdf">
          PDF
        </option>
          </select>

        <noscript>
      <div class="j-doc-option__label j-doc-option__label--format" id="j-doc-option__label--format--nojs">
        Format:
      </div>
      <ul class="j-doc-option__format-list" aria-labelledby="j-doc-option__label--format--nojs">
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--multi-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html/support">Multi-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--single-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html-single/support">Single-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--pdf"><a href="/documentation/en-us/openshift_container_platform/4.13/pdf/support/OpenShift_Container_Platform-4.13-Support-en-US.pdf">PDF</a></li>
              </ul>
    </noscript>
  </li>
</ul>


                </div>
              </div>
          </div>
        </div>

                  <div class="doc-wrapper pvof-doc__wrapper j-superdoc__content-wrapper" id="doc-wrapper">
            

  <div class="pane-page-title">
    <h1 class="title" itemprop="name">Support</h1>
  </div>


  <div xml:lang="en-US" class="book" id="idm140604663440944"><div class="titlepage"><div><div class="producttitle"><span class="productname">OpenShift Container Platform</span> <span class="productnumber">4.13</span></div><div><h2 class="subtitle">Getting support for OpenShift Container Platform </h2></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat OpenShift Documentation Team</span></div></div><div><a href="#idm140604671788160">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				This document provides information on getting support from Red Hat for OpenShift Container Platform. It also contains information about remote health monitoring through Telemetry and the Insights Operator. The document also details the benefits that remote health monitoring provides.
			</div></div></div></div><hr/></div><section class="chapter" id="support-overview"><div class="titlepage"><div><div><h1 class="title">Chapter 1. Support overview</h1></div></div></div><p>
			Red Hat offers cluster administrators tools for gathering data for your cluster, monitoring, and troubleshooting.
		</p><section class="section" id="support-overview-get-support"><div class="titlepage"><div><div><h2 class="title">1.1. Get support</h2></div></div></div><p>
				<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#getting-support">Get support</a>: Visit the Red Hat Customer Portal to review knowledge base articles, submit a support case, and review additional product documentation and resources.
			</p></section><section class="section" id="support-overview-remote-health-monitoring"><div class="titlepage"><div><div><h2 class="title">1.2. Remote health monitoring issues</h2></div></div></div><p>
				<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#about-remote-health-monitoring">Remote health monitoring issues</a>: OpenShift Container Platform collects telemetry and configuration data about your cluster and reports it to Red Hat by using the Telemeter Client and the Insights Operator. Red Hat uses this data to understand and resolve issues in <span class="emphasis"><em>connected cluster</em></span>. Similar to connected clusters, you can <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#remote-health-reporting-from-restricted-network">Use remote health monitoring in a restricted network</a>. OpenShift Container Platform collects data and monitors health using the following:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						<span class="strong strong"><strong>Telemetry</strong></span>: The Telemetry Client gathers and uploads the metrics values to Red Hat every four minutes and thirty seconds. Red Hat uses this data to:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Monitor the clusters.
							</li><li class="listitem">
								Roll out OpenShift Container Platform upgrades.
							</li><li class="listitem">
								Improve the upgrade experience.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<span class="strong strong"><strong>Insight Operator</strong></span>: By default, OpenShift Container Platform installs and enables the Insight Operator, which reports configuration and component failure status every two hours. The Insight Operator helps to:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Identify potential cluster issues proactively.
							</li><li class="listitem">
								Provide a solution and preventive action in Red Hat OpenShift Cluster Manager.
							</li></ul></div></li></ul></div><p>
				You can <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#showing-data-collected-by-remote-health-monitoring">Review telemetry information</a>.
			</p><p>
				If you have enabled remote health reporting, <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#using-insights-to-identify-issues-with-your-cluster">Use Insights to identify issues</a>. You can optionally disable remote health reporting.
			</p></section><section class="section" id="support-overview-gather-data-cluster"><div class="titlepage"><div><div><h2 class="title">1.3. Gather data about your cluster</h2></div></div></div><p>
				<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#gathering-cluster-data">Gather data about your cluster</a>: Red Hat recommends gathering your debugging information when opening a support case. This helps Red Hat Support to perform a root cause analysis. A cluster administrator can use the following to gather data about your cluster:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<span class="strong strong"><strong>The must-gather tool</strong></span>: Use the <code class="literal">must-gather</code> tool to collect information about your cluster and to debug the issues.
					</li><li class="listitem">
						<span class="strong strong"><strong>sosreport</strong></span>: Use the <code class="literal">sosreport</code> tool to collect configuration details, system information, and diagnostic data for debugging purposes.
					</li><li class="listitem">
						<span class="strong strong"><strong>Cluster ID</strong></span>: Obtain the unique identifier for your cluster, when providing information to Red Hat Support.
					</li><li class="listitem">
						<span class="strong strong"><strong>Bootstrap node journal logs</strong></span>: Gather <code class="literal">bootkube.service</code> <code class="literal">journald</code> unit logs and container logs from the bootstrap node to troubleshoot bootstrap-related issues.
					</li><li class="listitem">
						<span class="strong strong"><strong>Cluster node journal logs</strong></span>: Gather <code class="literal">journald</code> unit logs and logs within <code class="literal">/var/log</code> on individual cluster nodes to troubleshoot node-related issues.
					</li><li class="listitem">
						<span class="strong strong"><strong>A network trace</strong></span>: Provide a network packet trace from a specific OpenShift Container Platform cluster node or a container to Red Hat Support to help troubleshoot network-related issues.
					</li><li class="listitem">
						<span class="strong strong"><strong>Diagnostic data</strong></span>: Use the <code class="literal">redhat-support-tool</code> command to gather(?) diagnostic data about your cluster.
					</li></ul></div></section><section class="section" id="support-overview-troubleshooting-issues"><div class="titlepage"><div><div><h2 class="title">1.4. Troubleshooting issues</h2></div></div></div><p>
				A cluster administrator can monitor and troubleshoot the following OpenShift Container Platform component issues:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#troubleshooting-installations">Installation issues</a>: OpenShift Container Platform installation proceeds through various stages. You can perform the following:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Monitor the installation stages.
							</li><li class="listitem">
								Determine at which stage installation issues occur.
							</li><li class="listitem">
								Investigate multiple installation issues.
							</li><li class="listitem">
								Gather logs from a failed installation.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#verifying-node-health">Node issues</a>: A cluster administrator can verify and troubleshoot node-related issues by reviewing the status, resource usage, and configuration of a node. You can query the following:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Kubelet’s status on a node.
							</li><li class="listitem">
								Cluster node journal logs.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#troubleshooting-crio-issues">Crio issues</a>: A cluster administrator can verify CRI-O container runtime engine status on each cluster node. If you experience container runtime issues, perform the following:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Gather CRI-O journald unit logs.
							</li><li class="listitem">
								Cleaning CRI-O storage.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#troubleshooting-operating-system-issues">Operating system issues</a>: OpenShift Container Platform runs on Red Hat Enterprise Linux CoreOS. If you experience operating system issues, you can investigate kernel crash procedures. Ensure the following:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Enable kdump.
							</li><li class="listitem">
								Test the kdump configuration.
							</li><li class="listitem">
								Analyze a core dump.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#troubleshooting-network-issues">Network issues</a>: To troubleshoot Open vSwitch issues, a cluster administrator can perform the following:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Configure the Open vSwitch log level temporarily.
							</li><li class="listitem">
								Configure the Open vSwitch log level permanently.
							</li><li class="listitem">
								Display Open vSwitch logs.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#troubleshooting-operator-issues">Operator issues</a>: A cluster administrator can do the following to resolve Operator issues:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Verify Operator subscription status.
							</li><li class="listitem">
								Check Operator pod health.
							</li><li class="listitem">
								Gather Operator logs.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#investigating-pod-issues">Pod issues</a>: A cluster administrator can troubleshoot pod-related issues by reviewing the status of a pod and completing the following:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Review pod and container logs.
							</li><li class="listitem">
								Start debug pods with root access.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#troubleshooting-s2i">Source-to-image issues</a>: A cluster administrator can observe the S2I stages to determine where in the S2I process a failure occurred. Gather the following to resolve Source-to-Image (S2I) issues:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Source-to-Image diagnostic data.
							</li><li class="listitem">
								Application diagnostic data to investigate application failure.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#troubleshooting-storage-issues">Storage issues</a>: A multi-attach storage error occurs when the mounting volume on a new node is not possible because the failed node cannot unmount the attached volume. A cluster administrator can do the following to resolve multi-attach storage issues:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Enable multiple attachments by using RWX volumes.
							</li><li class="listitem">
								Recover or delete the failed node when using an RWO volume.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#investigating-monitoring-issues">Monitoring issues</a>: A cluster administrator can follow the procedures on the troubleshooting page for monitoring. If the metrics for your user-defined projects are unavailable or if Prometheus is consuming a lot of disk space, check the following:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Investigate why user-defined metrics are unavailable.
							</li><li class="listitem">
								Determine why Prometheus is consuming a lot of disk space.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/logging/#cluster-logging">Logging issues</a>: A cluster administrator can follow the procedures on the troubleshooting page for OpenShift Logging issues. Check the following to resolve logging issues:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/logging/#cluster-logging-cluster-status">Status of the Logging Operator</a>.
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/logging/#cluster-logging-cluster-status">Status of the Log store</a>.
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/logging/#cluster-logging-alerts">OpenShift Logging alerts</a>.
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/logging/#cluster-logging-support-must-gather_cluster-logging-support">Information about your OpenShift logging environment using <code class="literal">oc adm must-gather</code> command</a>.
							</li></ul></div></li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#diagnosing-oc-issues">OpenShift CLI (oc) issues</a>: Investigate OpenShift CLI (oc) issues by increasing the log level.
					</li></ul></div></section></section><section class="chapter" id="managing-cluster-resources"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Managing your cluster resources</h1></div></div></div><p>
			You can apply global configuration options in OpenShift Container Platform. Operators apply these configuration settings across the cluster.
		</p><section class="section" id="support-cluster-resources_managing-cluster-resources"><div class="titlepage"><div><div><h2 class="title">2.1. Interacting with your cluster resources</h2></div></div></div><p>
				You can interact with cluster resources by using the OpenShift CLI (<code class="literal">oc</code>) tool in OpenShift Container Platform. The cluster resources that you see after running the <code class="literal">oc api-resources</code> command can be edited.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li><li class="listitem">
						You have access to the web console or you have installed the <code class="literal">oc</code> CLI tool.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To see which configuration Operators have been applied, run the following command:
					</p><pre class="programlisting language-terminal">$ oc api-resources -o name | grep config.openshift.io</pre></li><li class="listitem"><p class="simpara">
						To see what cluster resources you can configure, run the following command:
					</p><pre class="programlisting language-terminal">$ oc explain &lt;resource_name&gt;.config.openshift.io</pre></li><li class="listitem"><p class="simpara">
						To see the configuration of custom resource definition (CRD) objects in the cluster, run the following command:
					</p><pre class="programlisting language-terminal">$ oc get &lt;resource_name&gt;.config -o yaml</pre></li><li class="listitem"><p class="simpara">
						To edit the cluster resource configuration, run the following command:
					</p><pre class="programlisting language-terminal">$ oc edit &lt;resource_name&gt;.config -o yaml</pre></li></ol></div></section></section><section class="chapter" id="getting-support"><div class="titlepage"><div><div><h1 class="title">Chapter 3. Getting support</h1></div></div></div><section class="section" id="support_getting-support"><div class="titlepage"><div><div><h2 class="title">3.1. Getting support</h2></div></div></div><p>
				If you experience difficulty with a procedure described in this documentation, or with OpenShift Container Platform in general, visit the <a class="link" href="http://access.redhat.com">Red Hat Customer Portal</a>. From the Customer Portal, you can:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Search or browse through the Red Hat Knowledgebase of articles and solutions relating to Red Hat products.
					</li><li class="listitem">
						Submit a support case to Red Hat Support.
					</li><li class="listitem">
						Access other product documentation.
					</li></ul></div><p>
				To identify issues with your cluster, you can use Insights in <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a>. Insights provides details about issues and, if available, information on how to solve a problem.
			</p><p>
				If you have a suggestion for improving this documentation or have found an error, submit a <a class="link" href="https://issues.redhat.com/secure/CreateIssueDetails!init.jspa?pid=12332330&amp;summary=Documentation_issue&amp;issuetype=1&amp;components=12367614&amp;priority=10200&amp;versions=12391126">Jira issue</a> for the most relevant documentation component. Please provide specific details, such as the section name and OpenShift Container Platform version.
			</p></section><section class="section" id="support-knowledgebase-about_getting-support"><div class="titlepage"><div><div><h2 class="title">3.2. About the Red Hat Knowledgebase</h2></div></div></div><p>
				The <a class="link" href="https://access.redhat.com/knowledgebase">Red Hat Knowledgebase</a> provides rich content aimed at helping you make the most of Red Hat’s products and technologies. The Red Hat Knowledgebase consists of articles, product documentation, and videos outlining best practices on installing, configuring, and using Red Hat products. In addition, you can search for solutions to known issues, each providing concise root cause descriptions and remedial steps.
			</p></section><section class="section" id="support-knowledgebase-search_getting-support"><div class="titlepage"><div><div><h2 class="title">3.3. Searching the Red Hat Knowledgebase</h2></div></div></div><p>
				In the event of an OpenShift Container Platform issue, you can perform an initial search to determine if a solution already exists within the Red Hat Knowledgebase.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have a Red Hat Customer Portal account.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to the <a class="link" href="http://access.redhat.com">Red Hat Customer Portal</a>.
					</li><li class="listitem"><p class="simpara">
						In the main Red Hat Customer Portal search field, input keywords and strings relating to the problem, including:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								OpenShift Container Platform components (such as <span class="strong strong"><strong>etcd</strong></span>)
							</li><li class="listitem">
								Related procedure (such as <span class="strong strong"><strong>installation</strong></span>)
							</li><li class="listitem">
								Warnings, error messages, and other outputs related to explicit failures
							</li></ul></div></li><li class="listitem">
						Click <span class="strong strong"><strong>Search</strong></span>.
					</li><li class="listitem">
						Select the <span class="strong strong"><strong>OpenShift Container Platform</strong></span> product filter.
					</li><li class="listitem">
						Select the <span class="strong strong"><strong>Knowledgebase</strong></span> content type filter.
					</li></ol></div></section><section class="section" id="support-submitting-a-case_getting-support"><div class="titlepage"><div><div><h2 class="title">3.4. Submitting a support case</h2></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You have a Red Hat Customer Portal account.
					</li><li class="listitem">
						You have a Red Hat standard or premium Subscription.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to the <a class="link" href="http://access.redhat.com">Red Hat Customer Portal</a> and select <span class="strong strong"><strong>SUPPORT CASES</strong></span> → <span class="strong strong"><strong>Open a case</strong></span>.
					</li><li class="listitem">
						Select the appropriate category for your issue (such as <span class="strong strong"><strong>Defect / Bug</strong></span>), product (<span class="strong strong"><strong>OpenShift Container Platform</strong></span>), and product version (<span class="strong strong"><strong>4.13</strong></span>, if this is not already autofilled).
					</li><li class="listitem">
						Review the list of suggested Red Hat Knowledgebase solutions for a potential match against the problem that is being reported. If the suggested articles do not address the issue, click <span class="strong strong"><strong>Continue</strong></span>.
					</li><li class="listitem">
						Enter a concise but descriptive problem summary and further details about the symptoms being experienced, as well as your expectations.
					</li><li class="listitem">
						Review the updated list of suggested Red Hat Knowledgebase solutions for a potential match against the problem that is being reported. The list is refined as you provide more information during the case creation process. If the suggested articles do not address the issue, click <span class="strong strong"><strong>Continue</strong></span>.
					</li><li class="listitem">
						Ensure that the account information presented is as expected, and if not, amend accordingly.
					</li><li class="listitem"><p class="simpara">
						Check that the autofilled OpenShift Container Platform Cluster ID is correct. If it is not, manually obtain your cluster ID.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								To manually obtain your cluster ID using the OpenShift Container Platform web console:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Navigate to <span class="strong strong"><strong>Home</strong></span> → <span class="strong strong"><strong>Dashboards</strong></span> → <span class="strong strong"><strong>Overview</strong></span>.
									</li><li class="listitem">
										Find the value in the <span class="strong strong"><strong>Cluster ID</strong></span> field of the <span class="strong strong"><strong>Details</strong></span> section.
									</li></ol></div></li><li class="listitem"><p class="simpara">
								Alternatively, it is possible to open a new support case through the OpenShift Container Platform web console and have your cluster ID autofilled.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										From the toolbar, navigate to <span class="strong strong"><strong>(?) Help</strong></span> → <span class="strong strong"><strong>Open Support Case</strong></span>.
									</li><li class="listitem">
										The <span class="strong strong"><strong>Cluster ID</strong></span> value is autofilled.
									</li></ol></div></li><li class="listitem"><p class="simpara">
								To obtain your cluster ID using the OpenShift CLI (<code class="literal">oc</code>), run the following command:
							</p><pre class="programlisting language-terminal">$ oc get clusterversion -o jsonpath='{.items[].spec.clusterID}{"\n"}'</pre></li></ul></div></li><li class="listitem"><p class="simpara">
						Complete the following questions where prompted and then click <span class="strong strong"><strong>Continue</strong></span>:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Where are you experiencing the behavior? What environment?
							</li><li class="listitem">
								When does the behavior occur? Frequency? Repeatedly? At certain times?
							</li><li class="listitem">
								What information can you provide around time-frames and the business impact?
							</li></ul></div></li><li class="listitem">
						Upload relevant diagnostic data files and click <span class="strong strong"><strong>Continue</strong></span>. It is recommended to include data gathered using the <code class="literal">oc adm must-gather</code> command as a starting point, plus any issue specific data that is not collected by that command.
					</li><li class="listitem">
						Input relevant case management details and click <span class="strong strong"><strong>Continue</strong></span>.
					</li><li class="listitem">
						Preview the case details and click <span class="strong strong"><strong>Submit</strong></span>.
					</li></ol></div></section><section class="section _additional-resources" id="getting-support-additional-resources"><div class="titlepage"><div><div><h2 class="title">3.5. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						For details about identifying issues with your cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#using-insights-to-identify-issues-with-your-cluster">Using Insights to identify issues with your cluster</a>.
					</li></ul></div></section></section><section class="chapter" id="remote-health-monitoring-with-connected-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Remote health monitoring with connected clusters</h1></div></div></div><section class="section" id="about-remote-health-monitoring"><div class="titlepage"><div><div><h2 class="title">4.1. About remote health monitoring</h2></div></div></div><p>
				OpenShift Container Platform collects telemetry and configuration data about your cluster and reports it to Red Hat by using the Telemeter Client and the Insights Operator. The data that is provided to Red Hat enables the benefits outlined in this document.
			</p><p>
				A cluster that reports data to Red Hat through Telemetry and the Insights Operator is considered a <span class="emphasis"><em>connected cluster</em></span>.
			</p><p>
				<span class="strong strong"><strong>Telemetry</strong></span> is the term that Red Hat uses to describe the information being sent to Red Hat by the OpenShift Container Platform Telemeter Client. Lightweight attributes are sent from connected clusters to Red Hat to enable subscription management automation, monitor the health of clusters, assist with support, and improve customer experience.
			</p><p>
				The <span class="strong strong"><strong>Insights Operator</strong></span> gathers OpenShift Container Platform configuration data and sends it to Red Hat. The data is used to produce insights about potential issues that a cluster might be exposed to. These insights are communicated to cluster administrators on <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a>.
			</p><p>
				More information is provided in this document about these two processes.
			</p><div class="formalpara"><p class="title"><strong>Telemetry and Insights Operator benefits</strong></p><p>
					Telemetry and the Insights Operator enable the following benefits for end-users:
				</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<span class="strong strong"><strong>Enhanced identification and resolution of issues</strong></span>. Events that might seem normal to an end-user can be observed by Red Hat from a broader perspective across a fleet of clusters. Some issues can be more rapidly identified from this point of view and resolved without an end-user needing to open a support case or file a <a class="link" href="https://issues.redhat.com/secure/CreateIssueDetails!init.jspa?pid=12332330&amp;summary=Summary&amp;issuetype=1&amp;priority=10200&amp;versions=12391126">Jira issue</a>.
					</li><li class="listitem">
						<span class="strong strong"><strong>Advanced release management</strong></span>. OpenShift Container Platform offers the <code class="literal">candidate</code>, <code class="literal">fast</code>, and <code class="literal">stable</code> release channels, which enable you to choose an update strategy. The graduation of a release from <code class="literal">fast</code> to <code class="literal">stable</code> is dependent on the success rate of updates and on the events seen during upgrades. With the information provided by connected clusters, Red Hat can improve the quality of releases to <code class="literal">stable</code> channels and react more rapidly to issues found in the <code class="literal">fast</code> channels.
					</li><li class="listitem">
						<span class="strong strong"><strong>Targeted prioritization of new features and functionality</strong></span>. The data collected provides insights about which areas of OpenShift Container Platform are used most. With this information, Red Hat can focus on developing the new features and functionality that have the greatest impact for our customers.
					</li><li class="listitem">
						<span class="strong strong"><strong>A streamlined support experience</strong></span>. You can provide a cluster ID for a connected cluster when creating a support ticket on the <a class="link" href="https://access.redhat.com/support/">Red Hat Customer Portal</a>. This enables Red Hat to deliver a streamlined support experience that is specific to your cluster, by using the connected information. This document provides more information about that enhanced support experience.
					</li><li class="listitem">
						<span class="strong strong"><strong>Predictive analytics</strong></span>. The insights displayed for your cluster on <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a> are enabled by the information collected from connected clusters. Red Hat is investing in applying deep learning, machine learning, and artificial intelligence automation to help identify issues that OpenShift Container Platform clusters are exposed to.
					</li></ul></div><section class="section" id="telemetry-about-telemetry_about-remote-health-monitoring"><div class="titlepage"><div><div><h3 class="title">4.1.1. About Telemetry</h3></div></div></div><p>
					Telemetry sends a carefully chosen subset of the cluster monitoring metrics to Red Hat. The Telemeter Client fetches the metrics values every four minutes and thirty seconds and uploads the data to Red Hat. These metrics are described in this document.
				</p><p>
					This stream of data is used by Red Hat to monitor the clusters in real-time and to react as necessary to problems that impact our customers. It also allows Red Hat to roll out OpenShift Container Platform upgrades to customers to minimize service impact and continuously improve the upgrade experience.
				</p><p>
					This debugging information is available to Red Hat Support and Engineering teams with the same restrictions as accessing data reported through support cases. All connected cluster information is used by Red Hat to help make OpenShift Container Platform better and more intuitive to use.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							See the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-cluster-within-minor">OpenShift Container Platform update documentation</a> for more information about updating or upgrading a cluster.
						</li></ul></div><section class="section" id="what-information-is-collected_about-remote-health-monitoring"><div class="titlepage"><div><div><h4 class="title">4.1.1.1. Information collected by Telemetry</h4></div></div></div><p>
						The following information is collected by Telemetry:
					</p><section class="section" id="system-information_about-remote-health-monitoring"><div class="titlepage"><div><div><h5 class="title">4.1.1.1.1. System information</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Version information, including the OpenShift Container Platform cluster version and installed update details that are used to determine update version availability
								</li><li class="listitem">
									Update information, including the number of updates available per cluster, the channel and image repository used for an update, update progress information, and the number of errors that occur in an update
								</li><li class="listitem">
									The unique random identifier that is generated during an installation
								</li><li class="listitem">
									Configuration details that help Red Hat Support to provide beneficial support for customers, including node configuration at the cloud infrastructure level, hostnames, IP addresses, Kubernetes pod names, namespaces, and services
								</li><li class="listitem">
									The OpenShift Container Platform framework components installed in a cluster and their condition and status
								</li><li class="listitem">
									Events for all namespaces listed as "related objects" for a degraded Operator
								</li><li class="listitem">
									Information about degraded software
								</li><li class="listitem">
									Information about the validity of certificates
								</li><li class="listitem">
									The name of the provider platform that OpenShift Container Platform is deployed on and the data center location
								</li></ul></div></section><section class="section" id="sizing-information_about-remote-health-monitoring"><div class="titlepage"><div><div><h5 class="title">4.1.1.1.2. Sizing Information</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Sizing information about clusters, machine types, and machines, including the number of CPU cores and the amount of RAM used for each
								</li><li class="listitem">
									The number of etcd members and the number of objects stored in the etcd cluster
								</li><li class="listitem">
									Number of application builds by build strategy type
								</li></ul></div></section><section class="section" id="usage-information_about-remote-health-monitoring"><div class="titlepage"><div><div><h5 class="title">4.1.1.1.3. Usage information</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Usage information about components, features, and extensions
								</li><li class="listitem">
									Usage details about Technology Previews and unsupported configurations
								</li></ul></div><p>
							Telemetry does not collect identifying information such as usernames or passwords. Red Hat does not intend to collect personal information. If Red Hat discovers that personal information has been inadvertently received, Red Hat will delete such information. To the extent that any telemetry data constitutes personal data, please refer to the <a class="link" href="https://www.redhat.com/en/about/privacy-policy">Red Hat Privacy Statement</a> for more information about Red Hat’s privacy practices.
						</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
									See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#showing-data-collected-from-the-cluster_showing-data-collected-by-remote-health-monitoring">Showing data collected by Telemetry</a> for details about how to list the attributes that Telemetry gathers from Prometheus in OpenShift Container Platform.
								</li><li class="listitem">
									See the <a class="link" href="https://github.com/openshift/cluster-monitoring-operator/blob/master/manifests/0000_50_cluster-monitoring-operator_04-config.yaml">upstream cluster-monitoring-operator source code</a> for a list of the attributes that Telemetry gathers from Prometheus.
								</li><li class="listitem">
									Telemetry is installed and enabled by default. If you need to opt out of remote health reporting, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#opting-out-remote-health-reporting">Opting out of remote health reporting</a>.
								</li></ul></div></section></section></section><section class="section" id="insights-operator-about_about-remote-health-monitoring"><div class="titlepage"><div><div><h3 class="title">4.1.2. About the Insights Operator</h3></div></div></div><p>
					The Insights Operator periodically gathers configuration and component failure status and, by default, reports that data every two hours to Red Hat. This information enables Red Hat to assess configuration and deeper failure data than is reported through Telemetry.
				</p><p>
					Users of OpenShift Container Platform can display the report of each cluster in the <a class="link" href="https://console.redhat.com/openshift/insights/advisor/">Insights Advisor</a> service on Red Hat Hybrid Cloud Console. If any issues have been identified, Insights provides further details and, if available, steps on how to solve a problem.
				</p><p>
					The Insights Operator does not collect identifying information, such as user names, passwords, or certificates. See <a class="link" href="https://console.redhat.com/security/insights">Red Hat Insights Data &amp; Application Security</a> for information about Red Hat Insights data collection and controls.
				</p><p>
					Red Hat uses all connected cluster information to:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Identify potential cluster issues and provide a solution and preventive actions in the <a class="link" href="https://console.redhat.com/openshift/insights/advisor/">Insights Advisor</a> service on Red Hat Hybrid Cloud Console
						</li><li class="listitem">
							Improve OpenShift Container Platform by providing aggregated and critical information to product and support teams
						</li><li class="listitem">
							Make OpenShift Container Platform more intuitive
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							The Insights Operator is installed and enabled by default. If you need to opt out of remote health reporting, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#opting-out-remote-health-reporting">Opting out of remote health reporting</a>.
						</li></ul></div><section class="section" id="insights-operator-what-information-is-collected_about-remote-health-monitoring"><div class="titlepage"><div><div><h4 class="title">4.1.2.1. Information collected by the Insights Operator</h4></div></div></div><p>
						The following information is collected by the Insights Operator:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								General information about your cluster and its components to identify issues that are specific to your OpenShift Container Platform version and environment
							</li><li class="listitem">
								Configuration files, such as the image registry configuration, of your cluster to determine incorrect settings and issues that are specific to parameters you set
							</li><li class="listitem">
								Errors that occur in the cluster components
							</li><li class="listitem">
								Progress information of running updates, and the status of any component upgrades
							</li><li class="listitem">
								Details of the platform that OpenShift Container Platform is deployed on, such as Amazon Web Services, and the region that the cluster is located in
							</li><li class="listitem">
								Cluster workload information transformed into discreet Secure Hash Algorithm (SHA) values, which allows Red Hat to assess workloads for security and version vulnerabilities without disclosing sensitive details
							</li><li class="listitem">
								If an Operator reports an issue, information is collected about core OpenShift Container Platform pods in the <code class="literal">openshift-*</code> and <code class="literal">kube-*</code> projects. This includes state, resource, security context, volume information, and more.
							</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#insights-operator-showing-data-collected-from-the-cluster_showing-data-collected-by-remote-health-monitoring">Showing data collected by the Insights Operator</a> for details about how to review the data that is collected by the Insights Operator.
							</li><li class="listitem">
								The Insights Operator source code is available for review and contribution. See the <a class="link" href="https://github.com/openshift/insights-operator/blob/master/docs/gathered-data.md">Insights Operator upstream project</a> for a list of the items collected by the Insights Operator.
							</li></ul></div></section></section><section class="section" id="understanding-telemetry-and-insights-operator-data-flow_about-remote-health-monitoring"><div class="titlepage"><div><div><h3 class="title">4.1.3. Understanding Telemetry and Insights Operator data flow</h3></div></div></div><p>
					The Telemeter Client collects selected time series data from the Prometheus API. The time series data is uploaded to api.openshift.com every four minutes and thirty seconds for processing.
				</p><p>
					The Insights Operator gathers selected data from the Kubernetes API and the Prometheus API into an archive. The archive is uploaded to <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a> every two hours for processing. The Insights Operator also downloads the latest Insights analysis from <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a>. This is used to populate the <span class="strong strong"><strong>Insights status</strong></span> pop-up that is included in the <span class="strong strong"><strong>Overview</strong></span> page in the OpenShift Container Platform web console.
				</p><p>
					All of the communication with Red Hat occurs over encrypted channels by using Transport Layer Security (TLS) and mutual certificate authentication. All of the data is encrypted in transit and at rest.
				</p><p>
					Access to the systems that handle customer data is controlled through multi-factor authentication and strict authorization controls. Access is granted on a need-to-know basis and is limited to required operations.
				</p><div class="formalpara"><p class="title"><strong>Telemetry and Insights Operator data flow</strong></p><p>
						<span class="inlinemediaobject"><object type="image/svg+xml" class="svg-img" data="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Support-en-US/images/8a36b12e1a24c1e230354546bb644350/telmetry-and-insights-operator-data-flow.svg"><embed type="image/svg+xml" src="images/telmetry-and-insights-operator-data-flow.svg"><!--Empty--></embed></object></span>

					</p></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/monitoring/#monitoring-overview_monitoring-overview">Monitoring overview</a> for more information about the OpenShift Container Platform monitoring stack.
						</li><li class="listitem">
							See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#configuring-firewall">Configuring your firewall</a> for details about configuring a firewall and enabling endpoints for Telemetry and Insights
						</li></ul></div></section><section class="section" id="additional-details-about-how-remote-health-monitoring-data-is-used"><div class="titlepage"><div><div><h3 class="title">4.1.4. Additional details about how remote health monitoring data is used</h3></div></div></div><p>
					The information collected to enable remote health monitoring is detailed in <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#what-information-is-collected_about-remote-health-monitoring">Information collected by Telemetry</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#insights-operator-what-information-is-collected_about-remote-health-monitoring">Information collected by the Insights Operator</a>.
				</p><p>
					As further described in the preceding sections of this document, Red Hat collects data about your use of the Red Hat Product(s) for purposes such as providing support and upgrades, optimizing performance or configuration, minimizing service impacts, identifying and remediating threats, troubleshooting, improving the offerings and user experience, responding to issues, and for billing purposes if applicable.
				</p><div class="formalpara"><p class="title"><strong>Collection safeguards</strong></p><p>
						Red Hat employs technical and organizational measures designed to protect the telemetry and configuration data.
					</p></div><div class="formalpara"><p class="title"><strong>Sharing</strong></p><p>
						Red Hat may share the data collected through Telemetry and the Insights Operator internally within Red Hat to improve your user experience. Red Hat may share telemetry and configuration data with its business partners in an aggregated form that does not identify customers to help the partners better understand their markets and their customers’ use of Red Hat offerings or to ensure the successful integration of products jointly supported by those partners.
					</p></div><div class="formalpara"><p class="title"><strong>Third parties</strong></p><p>
						Red Hat may engage certain third parties to assist in the collection, analysis, and storage of the Telemetry and configuration data.
					</p></div><div class="formalpara"><p class="title"><strong>User control / enabling and disabling telemetry and configuration data collection</strong></p><p>
						You may disable OpenShift Container Platform Telemetry and the Insights Operator by following the instructions in <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#opting-out-remote-health-reporting">Opting out of remote health reporting</a>.
					</p></div></section></section><section class="section" id="showing-data-collected-by-remote-health-monitoring"><div class="titlepage"><div><div><h2 class="title">4.2. Showing data collected by remote health monitoring</h2></div></div></div><p>
				As an administrator, you can review the metrics collected by Telemetry and the Insights Operator.
			</p><section class="section" id="showing-data-collected-from-the-cluster_showing-data-collected-by-remote-health-monitoring"><div class="titlepage"><div><div><h3 class="title">4.2.1. Showing data collected by Telemetry</h3></div></div></div><p>
					You can view the cluster and components time series data captured by Telemetry.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift Container Platform CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role or the <code class="literal">cluster-monitoring-view</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to a cluster.
						</li><li class="listitem"><p class="simpara">
							Run the following command, which queries a cluster’s Prometheus service and returns the full set of time series data captured by Telemetry:
						</p><pre class="programlisting language-terminal">$ curl -G -k -H "Authorization: Bearer $(oc whoami -t)" \
https://$(oc get route prometheus-k8s-federate -n \
openshift-monitoring -o jsonpath="{.spec.host}")/federate \
--data-urlencode 'match[]={__name__=~"cluster:usage:.*"}' \
--data-urlencode 'match[]={__name__="count:up0"}' \
--data-urlencode 'match[]={__name__="count:up1"}' \
--data-urlencode 'match[]={__name__="cluster_version"}' \
--data-urlencode 'match[]={__name__="cluster_version_available_updates"}' \
--data-urlencode 'match[]={__name__="cluster_version_capability"}' \
--data-urlencode 'match[]={__name__="cluster_operator_up"}' \
--data-urlencode 'match[]={__name__="cluster_operator_conditions"}' \
--data-urlencode 'match[]={__name__="cluster_version_payload"}' \
--data-urlencode 'match[]={__name__="cluster_installer"}' \
--data-urlencode 'match[]={__name__="cluster_infrastructure_provider"}' \
--data-urlencode 'match[]={__name__="cluster_feature_set"}' \
--data-urlencode 'match[]={__name__="instance:etcd_object_counts:sum"}' \
--data-urlencode 'match[]={__name__="ALERTS",alertstate="firing"}' \
--data-urlencode 'match[]={__name__="code:apiserver_request_total:rate:sum"}' \
--data-urlencode 'match[]={__name__="cluster:capacity_cpu_cores:sum"}' \
--data-urlencode 'match[]={__name__="cluster:capacity_memory_bytes:sum"}' \
--data-urlencode 'match[]={__name__="cluster:cpu_usage_cores:sum"}' \
--data-urlencode 'match[]={__name__="cluster:memory_usage_bytes:sum"}' \
--data-urlencode 'match[]={__name__="openshift:cpu_usage_cores:sum"}' \
--data-urlencode 'match[]={__name__="openshift:memory_usage_bytes:sum"}' \
--data-urlencode 'match[]={__name__="workload:cpu_usage_cores:sum"}' \
--data-urlencode 'match[]={__name__="workload:memory_usage_bytes:sum"}' \
--data-urlencode 'match[]={__name__="cluster:virt_platform_nodes:sum"}' \
--data-urlencode 'match[]={__name__="cluster:node_instance_type_count:sum"}' \
--data-urlencode 'match[]={__name__="cnv:vmi_status_running:count"}' \
--data-urlencode 'match[]={__name__="cluster:vmi_request_cpu_cores:sum"}' \
--data-urlencode 'match[]={__name__="node_role_os_version_machine:cpu_capacity_cores:sum"}' \
--data-urlencode 'match[]={__name__="node_role_os_version_machine:cpu_capacity_sockets:sum"}' \
--data-urlencode 'match[]={__name__="subscription_sync_total"}' \
--data-urlencode 'match[]={__name__="olm_resolution_duration_seconds"}' \
--data-urlencode 'match[]={__name__="csv_succeeded"}' \
--data-urlencode 'match[]={__name__="csv_abnormal"}' \
--data-urlencode 'match[]={__name__="cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum"}' \
--data-urlencode 'match[]={__name__="cluster:kubelet_volume_stats_used_bytes:provisioner:sum"}' \
--data-urlencode 'match[]={__name__="ceph_cluster_total_bytes"}' \
--data-urlencode 'match[]={__name__="ceph_cluster_total_used_raw_bytes"}' \
--data-urlencode 'match[]={__name__="ceph_health_status"}' \
--data-urlencode 'match[]={__name__="odf_system_raw_capacity_total_bytes"}' \
--data-urlencode 'match[]={__name__="odf_system_raw_capacity_used_bytes"}' \
--data-urlencode 'match[]={__name__="odf_system_health_status"}' \
--data-urlencode 'match[]={__name__="job:ceph_osd_metadata:count"}' \
--data-urlencode 'match[]={__name__="job:kube_pv:count"}' \
--data-urlencode 'match[]={__name__="job:odf_system_pvs:count"}' \
--data-urlencode 'match[]={__name__="job:ceph_pools_iops:total"}' \
--data-urlencode 'match[]={__name__="job:ceph_pools_iops_bytes:total"}' \
--data-urlencode 'match[]={__name__="job:ceph_versions_running:count"}' \
--data-urlencode 'match[]={__name__="job:noobaa_total_unhealthy_buckets:sum"}' \
--data-urlencode 'match[]={__name__="job:noobaa_bucket_count:sum"}' \
--data-urlencode 'match[]={__name__="job:noobaa_total_object_count:sum"}' \
--data-urlencode 'match[]={__name__="odf_system_bucket_count", system_type="OCS", system_vendor="Red Hat"}' \
--data-urlencode 'match[]={__name__="odf_system_objects_total", system_type="OCS", system_vendor="Red Hat"}' \
--data-urlencode 'match[]={__name__="noobaa_accounts_num"}' \
--data-urlencode 'match[]={__name__="noobaa_total_usage"}' \
--data-urlencode 'match[]={__name__="console_url"}' \
--data-urlencode 'match[]={__name__="cluster:ovnkube_master_egress_routing_via_host:max"}' \
--data-urlencode 'match[]={__name__="cluster:network_attachment_definition_instances:max"}' \
--data-urlencode 'match[]={__name__="cluster:network_attachment_definition_enabled_instance_up:max"}' \
--data-urlencode 'match[]={__name__="cluster:ingress_controller_aws_nlb_active:sum"}' \
--data-urlencode 'match[]={__name__="cluster:route_metrics_controller_routes_per_shard:min"}' \
--data-urlencode 'match[]={__name__="cluster:route_metrics_controller_routes_per_shard:max"}' \
--data-urlencode 'match[]={__name__="cluster:route_metrics_controller_routes_per_shard:avg"}' \
--data-urlencode 'match[]={__name__="cluster:route_metrics_controller_routes_per_shard:median"}' \
--data-urlencode 'match[]={__name__="cluster:openshift_route_info:tls_termination:sum"}' \
--data-urlencode 'match[]={__name__="insightsclient_request_send_total"}' \
--data-urlencode 'match[]={__name__="cam_app_workload_migrations"}' \
--data-urlencode 'match[]={__name__="cluster:apiserver_current_inflight_requests:sum:max_over_time:2m"}' \
--data-urlencode 'match[]={__name__="cluster:alertmanager_integrations:max"}' \
--data-urlencode 'match[]={__name__="cluster:telemetry_selected_series:count"}' \
--data-urlencode 'match[]={__name__="openshift:prometheus_tsdb_head_series:sum"}' \
--data-urlencode 'match[]={__name__="openshift:prometheus_tsdb_head_samples_appended_total:sum"}' \
--data-urlencode 'match[]={__name__="monitoring:container_memory_working_set_bytes:sum"}' \
--data-urlencode 'match[]={__name__="namespace_job:scrape_series_added:topk3_sum1h"}' \
--data-urlencode 'match[]={__name__="namespace_job:scrape_samples_post_metric_relabeling:topk3"}' \
--data-urlencode 'match[]={__name__="monitoring:haproxy_server_http_responses_total:sum"}' \
--data-urlencode 'match[]={__name__="rhmi_status"}' \
--data-urlencode 'match[]={__name__="status:upgrading:version:rhoam_state:max"}' \
--data-urlencode 'match[]={__name__="state:rhoam_critical_alerts:max"}' \
--data-urlencode 'match[]={__name__="state:rhoam_warning_alerts:max"}' \
--data-urlencode 'match[]={__name__="rhoam_7d_slo_percentile:max"}' \
--data-urlencode 'match[]={__name__="rhoam_7d_slo_remaining_error_budget:max"}' \
--data-urlencode 'match[]={__name__="cluster_legacy_scheduler_policy"}' \
--data-urlencode 'match[]={__name__="cluster_master_schedulable"}' \
--data-urlencode 'match[]={__name__="che_workspace_status"}' \
--data-urlencode 'match[]={__name__="che_workspace_started_total"}' \
--data-urlencode 'match[]={__name__="che_workspace_failure_total"}' \
--data-urlencode 'match[]={__name__="che_workspace_start_time_seconds_sum"}' \
--data-urlencode 'match[]={__name__="che_workspace_start_time_seconds_count"}' \
--data-urlencode 'match[]={__name__="cco_credentials_mode"}' \
--data-urlencode 'match[]={__name__="cluster:kube_persistentvolume_plugin_type_counts:sum"}' \
--data-urlencode 'match[]={__name__="visual_web_terminal_sessions_total"}' \
--data-urlencode 'match[]={__name__="acm_managed_cluster_info"}' \
--data-urlencode 'match[]={__name__="cluster:vsphere_vcenter_info:sum"}' \
--data-urlencode 'match[]={__name__="cluster:vsphere_esxi_version_total:sum"}' \
--data-urlencode 'match[]={__name__="cluster:vsphere_node_hw_version_total:sum"}' \
--data-urlencode 'match[]={__name__="openshift:build_by_strategy:sum"}' \
--data-urlencode 'match[]={__name__="rhods_aggregate_availability"}' \
--data-urlencode 'match[]={__name__="rhods_total_users"}' \
--data-urlencode 'match[]={__name__="instance:etcd_disk_wal_fsync_duration_seconds:histogram_quantile",quantile="0.99"}' \
--data-urlencode 'match[]={__name__="instance:etcd_mvcc_db_total_size_in_bytes:sum"}' \
--data-urlencode 'match[]={__name__="instance:etcd_network_peer_round_trip_time_seconds:histogram_quantile",quantile="0.99"}' \
--data-urlencode 'match[]={__name__="instance:etcd_mvcc_db_total_size_in_use_in_bytes:sum"}' \
--data-urlencode 'match[]={__name__="instance:etcd_disk_backend_commit_duration_seconds:histogram_quantile",quantile="0.99"}' \
--data-urlencode 'match[]={__name__="jaeger_operator_instances_storage_types"}' \
--data-urlencode 'match[]={__name__="jaeger_operator_instances_strategies"}' \
--data-urlencode 'match[]={__name__="jaeger_operator_instances_agent_strategies"}' \
--data-urlencode 'match[]={__name__="appsvcs:cores_by_product:sum"}' \
--data-urlencode 'match[]={__name__="nto_custom_profiles:count"}' \
--data-urlencode 'match[]={__name__="openshift_csi_share_configmap"}' \
--data-urlencode 'match[]={__name__="openshift_csi_share_secret"}' \
--data-urlencode 'match[]={__name__="openshift_csi_share_mount_failures_total"}' \
--data-urlencode 'match[]={__name__="openshift_csi_share_mount_requests_total"}' \
--data-urlencode 'match[]={__name__="cluster:velero_backup_total:max"}' \
--data-urlencode 'match[]={__name__="cluster:velero_restore_total:max"}' \
--data-urlencode 'match[]={__name__="eo_es_storage_info"}' \
--data-urlencode 'match[]={__name__="eo_es_redundancy_policy_info"}' \
--data-urlencode 'match[]={__name__="eo_es_defined_delete_namespaces_total"}' \
--data-urlencode 'match[]={__name__="eo_es_misconfigured_memory_resources_info"}' \
--data-urlencode 'match[]={__name__="cluster:eo_es_data_nodes_total:max"}' \
--data-urlencode 'match[]={__name__="cluster:eo_es_documents_created_total:sum"}' \
--data-urlencode 'match[]={__name__="cluster:eo_es_documents_deleted_total:sum"}' \
--data-urlencode 'match[]={__name__="pod:eo_es_shards_total:max"}' \
--data-urlencode 'match[]={__name__="eo_es_cluster_management_state_info"}' \
--data-urlencode 'match[]={__name__="imageregistry:imagestreamtags_count:sum"}' \
--data-urlencode 'match[]={__name__="imageregistry:operations_count:sum"}' \
--data-urlencode 'match[]={__name__="log_logging_info"}' \
--data-urlencode 'match[]={__name__="log_collector_error_count_total"}' \
--data-urlencode 'match[]={__name__="log_forwarder_pipeline_info"}' \
--data-urlencode 'match[]={__name__="log_forwarder_input_info"}' \
--data-urlencode 'match[]={__name__="log_forwarder_output_info"}' \
--data-urlencode 'match[]={__name__="cluster:log_collected_bytes_total:sum"}' \
--data-urlencode 'match[]={__name__="cluster:log_logged_bytes_total:sum"}' \
--data-urlencode 'match[]={__name__="cluster:kata_monitor_running_shim_count:sum"}' \
--data-urlencode 'match[]={__name__="platform:hypershift_hostedclusters:max"}' \
--data-urlencode 'match[]={__name__="platform:hypershift_nodepools:max"}' \
--data-urlencode 'match[]={__name__="namespace:noobaa_unhealthy_bucket_claims:max"}' \
--data-urlencode 'match[]={__name__="namespace:noobaa_buckets_claims:max"}' \
--data-urlencode 'match[]={__name__="namespace:noobaa_unhealthy_namespace_resources:max"}' \
--data-urlencode 'match[]={__name__="namespace:noobaa_namespace_resources:max"}' \
--data-urlencode 'match[]={__name__="namespace:noobaa_unhealthy_namespace_buckets:max"}' \
--data-urlencode 'match[]={__name__="namespace:noobaa_namespace_buckets:max"}' \
--data-urlencode 'match[]={__name__="namespace:noobaa_accounts:max"}' \
--data-urlencode 'match[]={__name__="namespace:noobaa_usage:max"}' \
--data-urlencode 'match[]={__name__="namespace:noobaa_system_health_status:max"}' \
--data-urlencode 'match[]={__name__="ocs_advanced_feature_usage"}' \
--data-urlencode 'match[]={__name__="os_image_url_override:sum"}'</pre></li></ol></div></section><section class="section" id="insights-operator-showing-data-collected-from-the-cluster_showing-data-collected-by-remote-health-monitoring"><div class="titlepage"><div><div><h3 class="title">4.2.2. Showing data collected by the Insights Operator</h3></div></div></div><p>
					You can review the data that is collected by the Insights Operator.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Find the name of the currently running pod for the Insights Operator:
						</p><pre class="programlisting language-terminal">$ INSIGHTS_OPERATOR_POD=$(oc get pods --namespace=openshift-insights -o custom-columns=:metadata.name --no-headers  --field-selector=status.phase=Running)</pre></li><li class="listitem"><p class="simpara">
							Copy the recent data archives collected by the Insights Operator:
						</p><pre class="programlisting language-terminal">$ oc cp openshift-insights/$INSIGHTS_OPERATOR_POD:/var/lib/insights-operator ./insights-data</pre></li></ol></div><p>
					The recent Insights Operator archives are now available in the <code class="literal">insights-data</code> directory.
				</p></section></section><section class="section" id="opting-out-remote-health-reporting"><div class="titlepage"><div><div><h2 class="title">4.3. Opting out of remote health reporting</h2></div></div></div><p>
				You may choose to opt out of reporting health and usage data for your cluster.
			</p><p>
				To opt out of remote health reporting, you must:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#insights-operator-new-pull-secret_opting-out-remote-health-reporting">Modify the global cluster pull secret</a> to disable remote health reporting.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#images-update-global-pull-secret_opting-out-remote-health-reporting">Update the cluster</a> to use this modified pull secret.
					</li></ol></div><section class="section" id="telemetry-consequences-of-disabling-telemetry_opting-out-remote-health-reporting"><div class="titlepage"><div><div><h3 class="title">4.3.1. Consequences of disabling remote health reporting</h3></div></div></div><p>
					In OpenShift Container Platform, customers can opt out of reporting usage information. However, connected clusters allow Red Hat to react more quickly to problems and better support our customers, as well as better understand how product upgrades impact clusters. Connected clusters also help to simplify the subscription and entitlement process and enable the OpenShift Cluster Manager service to provide an overview of your clusters and their subscription status.
				</p><p>
					Red Hat strongly recommends leaving health and usage reporting enabled for pre-production and test clusters even if it is necessary to opt out for production clusters. This allows Red Hat to be a participant in qualifying OpenShift Container Platform in your environments and react more rapidly to product issues.
				</p><p>
					Some of the consequences of opting out of having a connected cluster are:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Red Hat will not be able to monitor the success of product upgrades or the health of your clusters without a support case being opened.
						</li><li class="listitem">
							Red Hat will not be able to use configuration data to better triage customer support cases and identify which configurations our customers find important.
						</li><li class="listitem">
							The OpenShift Cluster Manager will not show data about your clusters including health and usage information.
						</li><li class="listitem">
							Your subscription entitlement information must be manually entered via console.redhat.com without the benefit of automatic usage reporting.
						</li></ul></div><p>
					In restricted networks, Telemetry and Insights data can still be reported through appropriate configuration of your proxy.
				</p></section><section class="section" id="insights-operator-new-pull-secret_opting-out-remote-health-reporting"><div class="titlepage"><div><div><h3 class="title">4.3.2. Modifying the global cluster pull secret to disable remote health reporting</h3></div></div></div><p>
					You can modify your existing global cluster pull secret to disable remote health reporting. This disables both Telemetry and the Insights Operator.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Download the global cluster pull secret to your local file system.
						</p><pre class="programlisting language-terminal">$ oc extract secret/pull-secret -n openshift-config --to=.</pre></li><li class="listitem">
							In a text editor, edit the <code class="literal">.dockerconfigjson</code> file that was downloaded.
						</li><li class="listitem"><p class="simpara">
							Remove the <code class="literal">cloud.openshift.com</code> JSON entry, for example:
						</p><pre class="programlisting language-json">"cloud.openshift.com":{"auth":"&lt;hash&gt;","email":"&lt;email_address&gt;"}</pre></li><li class="listitem">
							Save the file.
						</li></ol></div><p>
					You can now update your cluster to use this modified pull secret.
				</p></section><section class="section" id="insights-operator-register-disconnected-cluster_opting-out-remote-health-reporting"><div class="titlepage"><div><div><h3 class="title">4.3.3. Registering your disconnected cluster</h3></div></div></div><p>
					Register your disconnected OpenShift Container Platform cluster on the Red Hat Hybrid Cloud Console so that your cluster is not impacted by the consequences listed in the section named "Consequences of disabling remote health reporting".
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						By registering your disconnected cluster, you can continue to report your subscription usage to Red Hat. In turn, Red Hat can return accurate usage and capacity trends associated with your subscription, so that you can use the returned information to better organize subscription allocations across all of your resources.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You are logged in to the OpenShift Container Platform web console as <code class="literal">cluster-admin</code>.
						</li><li class="listitem">
							You can log in to the Red Hat Hybrid Cloud Console.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Go to the <a class="link" href="https://console.redhat.com/openshift/register"><span class="strong strong"><strong>Register disconnected cluster</strong></span></a> web page on the Red Hat Hybrid Cloud Console.
						</li><li class="listitem">
							Optional: To access the <span class="strong strong"><strong>Register disconnected cluster</strong></span> web page from the home page of the Red Hat Hybrid Cloud Console, go to the <span class="strong strong"><strong>Clusters</strong></span> navigation menu item and then select the <span class="strong strong"><strong>Register cluster</strong></span> button.
						</li><li class="listitem">
							Enter your cluster’s details in the provided fields on the <span class="strong strong"><strong>Register disconnected cluster</strong></span> page.
						</li><li class="listitem">
							From the <span class="strong strong"><strong>Subscription settings</strong></span> section of the page, select the subcription settings that apply to your Red Hat subscription offering.
						</li><li class="listitem">
							To register your disconnected cluster, select the <span class="strong strong"><strong>Register cluster</strong></span> button.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#telemetry-consequences-of-disabling-telemetry_opting-out-remote-health-reporting">Consequences of disabling remote health reporting</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/subscription_central/2023/html/getting_started_with_the_subscriptions_service/con-how-does-subscriptionwatch-show-data_assembly-viewing-understanding-subscriptionwatch-data-ctxt">How does the subscriptions service show my subscription data?</a>(Getting Started with the Subscription Service)
						</li></ul></div></section><section class="section" id="images-update-global-pull-secret_opting-out-remote-health-reporting"><div class="titlepage"><div><div><h3 class="title">4.3.4. Updating the global cluster pull secret</h3></div></div></div><p>
					You can update the global pull secret for your cluster by either replacing the current pull secret or appending a new pull secret.
				</p><p>
					The procedure is required when users use a separate registry to store images than the registry used during installation.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Optional: To append a new pull secret to the existing pull secret, complete the following steps:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Enter the following command to download the pull secret:
								</p><pre class="programlisting language-terminal">$ oc get secret/pull-secret -n openshift-config --template='{{index .data ".dockerconfigjson" | base64decode}}' &gt;&lt;pull_secret_location&gt; <span id="CO1-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO1-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Provide the path to the pull secret file.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Enter the following command to add the new pull secret:
								</p><pre class="programlisting language-terminal">$ oc registry login --registry="&lt;registry&gt;" \ <span id="CO2-1"><!--Empty--></span><span class="callout">1</span>
--auth-basic="&lt;username&gt;:&lt;password&gt;" \ <span id="CO2-2"><!--Empty--></span><span class="callout">2</span>
--to=&lt;pull_secret_location&gt; <span id="CO2-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO2-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Provide the new registry. You can include multiple repositories within the same registry, for example: <code class="literal">--registry="&lt;registry/my-namespace/my-repository&gt;"</code>.
										</div></dd><dt><a href="#CO2-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Provide the credentials of the new registry.
										</div></dd><dt><a href="#CO2-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Provide the path to the pull secret file.
										</div></dd></dl></div><p class="simpara">
									Alternatively, you can perform a manual update to the pull secret file.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Enter the following command to update the global pull secret for your cluster:
						</p><pre class="programlisting language-terminal">$ oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=&lt;pull_secret_location&gt; <span id="CO3-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO3-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Provide the path to the new pull secret file.
								</div></dd></dl></div><p class="simpara">
							This update is rolled out to all nodes, which can take some time depending on the size of your cluster.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								As of OpenShift Container Platform 4.7.4, changes to the global pull secret no longer trigger a node drain or reboot.
							</p></div></div></li></ol></div></section></section><section class="section" id="enabling-remote-health-reporting"><div class="titlepage"><div><div><h2 class="title">4.4. Enabling remote health reporting</h2></div></div></div><p>
				If you or your organization have disabled remote health reporting, you can enable this feature again. You can see that remote health reporting is disabled from the message "Insights not available" in the <span class="strong strong"><strong>Status</strong></span> tile on the OpenShift Container Platform Web Console Overview page.
			</p><p>
				To enable remote health reporting, you must <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#insights-operator-new-pull-secret-enable_enabling-remote-health-reporting">Modify the global cluster pull secret</a> with a new authorization token.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Enabling remote health reporting enables both Insights Operator and Telemetry.
				</p></div></div><section class="section" id="insights-operator-new-pull-secret-enable_enabling-remote-health-reporting"><div class="titlepage"><div><div><h3 class="title">4.4.1. Modifying your global cluster pull secret to enable remote health reporting</h3></div></div></div><p>
					You can modify your existing global cluster pull secret to enable remote health reporting. If you have previously disabled remote health monitoring, you must first download a new pull secret with your <code class="literal">console.openshift.com</code> access token from Red Hat OpenShift Cluster Manager.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Access to OpenShift Cluster Manager.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to <a class="link" href="https://console.redhat.com/openshift/downloads">https://console.redhat.com/openshift/downloads</a>.
						</li><li class="listitem"><p class="simpara">
							From <span class="strong strong"><strong>Tokens</strong></span> → <span class="strong strong"><strong>Pull Secret</strong></span>, click <span class="strong strong"><strong>Download</strong></span>.
						</p><p class="simpara">
							The file <code class="literal">pull-secret.txt</code> containing your <code class="literal">cloud.openshift.com</code> access token in JSON format downloads:
						</p><pre class="programlisting language-json">{
  "auths": {
    "cloud.openshift.com": {
      "auth": "<span class="emphasis"><em>&lt;your_token&gt;</em></span>",
      "email": "<span class="emphasis"><em>&lt;email_address&gt;</em></span>"
    }
  }
}</pre></li><li class="listitem"><p class="simpara">
							Download the global cluster pull secret to your local file system.
						</p><pre class="programlisting language-terminal">$ oc get secret/pull-secret -n openshift-config --template='{{index .data ".dockerconfigjson" | base64decode}}' &gt; pull-secret</pre></li><li class="listitem"><p class="simpara">
							Make a backup copy of your pull secret.
						</p><pre class="programlisting language-terminal">$ cp pull-secret pull-secret-backup</pre></li><li class="listitem">
							Open the <code class="literal">pull-secret</code> file in a text editor.
						</li><li class="listitem">
							Append the <code class="literal">cloud.openshift.com</code> JSON entry from <code class="literal">pull-secret.txt</code> into <code class="literal">auths</code>.
						</li><li class="listitem">
							Save the file.
						</li><li class="listitem"><p class="simpara">
							Update the secret in your cluster.
						</p><pre class="programlisting language-terminal">oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=pull-secret</pre></li></ol></div><p>
					It may take several minutes for the secret to update and your cluster to begin reporting.
				</p><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to the OpenShift Container Platform Web Console Overview page.
						</li><li class="listitem">
							<span class="strong strong"><strong>Insights</strong></span> in the <span class="strong strong"><strong>Status</strong></span> tile reports the number of issues found.
						</li></ol></div></section></section><section class="section" id="using-insights-to-identify-issues-with-your-cluster"><div class="titlepage"><div><div><h2 class="title">4.5. Using Insights to identify issues with your cluster</h2></div></div></div><p>
				Insights repeatedly analyzes the data Insights Operator sends. Users of OpenShift Container Platform can display the report in the <a class="link" href="https://console.redhat.com/openshift/insights/advisor/">Insights Advisor</a> service on Red Hat Hybrid Cloud Console.
			</p><section class="section" id="insights-operator-advisor-overview_using-insights-to-identify-issues-with-your-cluster"><div class="titlepage"><div><div><h3 class="title">4.5.1. About Red Hat Insights Advisor for OpenShift Container Platform</h3></div></div></div><p>
					You can use Insights Advisor to assess and monitor the health of your OpenShift Container Platform clusters. Whether you are concerned about individual clusters, or with your whole infrastructure, it is important to be aware of your exposure to issues that can affect service availability, fault tolerance, performance, or security.
				</p><p>
					Insights repeatedly analyzes the data that Insights Operator sends using a database of <span class="emphasis"><em>recommendations</em></span>, which are sets of conditions that can leave your OpenShift Container Platform clusters at risk. Your data is then uploaded to the Insights Advisor service on Red Hat Hybrid Cloud Console where you can perform the following actions:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							See clusters impacted by a specific recommendation.
						</li><li class="listitem">
							Use robust filtering capabilities to refine your results to those recommendations.
						</li><li class="listitem">
							Learn more about individual recommendations, details about the risks they present, and get resolutions tailored to your individual clusters.
						</li><li class="listitem">
							Share results with other stakeholders.
						</li></ul></div></section><section class="section" id="insights-operator-advisor-recommendations_using-insights-to-identify-issues-with-your-cluster"><div class="titlepage"><div><div><h3 class="title">4.5.2. Understanding Insights Advisor recommendations</h3></div></div></div><p>
					Insights Advisor bundles information about various cluster states and component configurations that can negatively affect the service availability, fault tolerance, performance, or security of your clusters. This information set is called a recommendation in Insights Advisor and includes the following information:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<span class="strong strong"><strong>Name:</strong></span> A concise description of the recommendation
						</li><li class="listitem">
							<span class="strong strong"><strong>Added:</strong></span> When the recommendation was published to the Insights Advisor archive
						</li><li class="listitem">
							<span class="strong strong"><strong>Category:</strong></span> Whether the issue has the potential to negatively affect service availability, fault tolerance, performance, or security
						</li><li class="listitem">
							<span class="strong strong"><strong>Total risk:</strong></span> A value derived from the <span class="emphasis"><em>likelihood</em></span> that the condition will negatively affect your infrastructure, and the <span class="emphasis"><em>impact</em></span> on operations if that were to happen
						</li><li class="listitem">
							<span class="strong strong"><strong>Clusters:</strong></span> A list of clusters on which a recommendation is detected
						</li><li class="listitem">
							<span class="strong strong"><strong>Description:</strong></span> A brief synopsis of the issue, including how it affects your clusters
						</li><li class="listitem">
							<span class="strong strong"><strong>Link to associated topics:</strong></span> More information from Red Hat about the issue
						</li></ul></div></section><section class="section" id="displaying-potential-issues-with-your-cluster_using-insights-to-identify-issues-with-your-cluster"><div class="titlepage"><div><div><h3 class="title">4.5.3. Displaying potential issues with your cluster</h3></div></div></div><p>
					This section describes how to display the Insights report in <span class="strong strong"><strong>Insights Advisor</strong></span> on <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a>.
				</p><p>
					Note that Insights repeatedly analyzes your cluster and shows the latest results. These results can change, for example, if you fix an issue or a new issue has been detected.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Your cluster is registered on <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a>.
						</li><li class="listitem">
							Remote health reporting is enabled, which is the default.
						</li><li class="listitem">
							You are logged in to <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Navigate to <span class="strong strong"><strong>Advisor</strong></span> → <span class="strong strong"><strong>Recommendations</strong></span> on <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a>.
						</p><p class="simpara">
							Depending on the result, Insights Advisor displays one of the following:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<span class="strong strong"><strong>No matching recommendations found</strong></span>, if Insights did not identify any issues.
								</li><li class="listitem">
									A list of issues Insights has detected, grouped by risk (low, moderate, important, and critical).
								</li><li class="listitem">
									<span class="strong strong"><strong>No clusters yet</strong></span>, if Insights has not yet analyzed the cluster. The analysis starts shortly after the cluster has been installed, registered, and connected to the internet.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							If any issues are displayed, click the <span class="strong strong"><strong>&gt;</strong></span> icon in front of the entry for more details.
						</p><p class="simpara">
							Depending on the issue, the details can also contain a link to more information from Red Hat about the issue.
						</p></li></ol></div></section><section class="section" id="displaying-all-insights-advisor-recommendations_using-insights-to-identify-issues-with-your-cluster"><div class="titlepage"><div><div><h3 class="title">4.5.4. Displaying all Insights Advisor recommendations</h3></div></div></div><p>
					The Recommendations view, by default, only displays the recommendations that are detected on your clusters. However, you can view all of the recommendations in the advisor archive.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Remote health reporting is enabled, which is the default.
						</li><li class="listitem">
							Your cluster is <a class="link" href="https://console.redhat.com/openshift/register">registered</a> on Red Hat Hybrid Cloud Console.
						</li><li class="listitem">
							You are logged in to <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to <span class="strong strong"><strong>Advisor</strong></span> → <span class="strong strong"><strong>Recommendations</strong></span> on <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a>.
						</li><li class="listitem"><p class="simpara">
							Click the <span class="strong strong"><strong>X</strong></span> icons next to the <span class="strong strong"><strong>Clusters Impacted</strong></span> and <span class="strong strong"><strong>Status</strong></span> filters.
						</p><p class="simpara">
							You can now browse through all of the potential recommendations for your cluster.
						</p></li></ol></div></section><section class="section" id="disabling-insights-advisor-recommendations_using-insights-to-identify-issues-with-your-cluster"><div class="titlepage"><div><div><h3 class="title">4.5.5. Disabling Insights Advisor recommendations</h3></div></div></div><p>
					You can disable specific recommendations that affect your clusters, so that they no longer appear in your reports. It is possible to disable a recommendation for a single cluster or all of your clusters.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Disabling a recommendation for all of your clusters also applies to any future clusters.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Remote health reporting is enabled, which is the default.
						</li><li class="listitem">
							Your cluster is registered on <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a>.
						</li><li class="listitem">
							You are logged in to <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to <span class="strong strong"><strong>Advisor</strong></span> → <span class="strong strong"><strong>Recommendations</strong></span> on <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a>.
						</li><li class="listitem">
							Click the name of the recommendation to disable. You are directed to the single recommendation page.
						</li><li class="listitem"><p class="simpara">
							To disable the recommendation for a single cluster:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Click the <span class="strong strong"><strong>Options</strong></span> menu 
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Support-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
									 for that cluster, and then click <span class="strong strong"><strong>Disable recommendation for cluster</strong></span>.
								</li><li class="listitem">
									Enter a justification note and click <span class="strong strong"><strong>Save</strong></span>.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							To disable the recommendation for all of your clusters:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Click <span class="strong strong"><strong>Actions</strong></span> → <span class="strong strong"><strong>Disable recommendation</strong></span>.
								</li><li class="listitem">
									Enter a justification note and click <span class="strong strong"><strong>Save</strong></span>.
								</li></ol></div></li></ol></div></section><section class="section" id="enabling-insights-advisor-recommendations_using-insights-to-identify-issues-with-your-cluster"><div class="titlepage"><div><div><h3 class="title">4.5.6. Enabling a previously disabled Insights Advisor recommendation</h3></div></div></div><p>
					When a recommendation is disabled for all clusters, you will no longer see the recommendation in Insights Advisor. You can change this behavior.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Remote health reporting is enabled, which is the default.
						</li><li class="listitem">
							Your cluster is registered on <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a>.
						</li><li class="listitem">
							You are logged in to <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to <span class="strong strong"><strong>Advisor</strong></span> → <span class="strong strong"><strong>Recommendations</strong></span> on <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a>.
						</li><li class="listitem">
							Filter the recommendations by <span class="strong strong"><strong>Status</strong></span> → <span class="strong strong"><strong>Disabled</strong></span>.
						</li><li class="listitem">
							Locate the recommendation to enable.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>Options</strong></span> menu 
							<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Support-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
							 , and then click <span class="strong strong"><strong>Enable recommendation</strong></span>.
						</li></ol></div></section><section class="section" id="displaying-the-insights-status-in-the-web-console_using-insights-to-identify-issues-with-your-cluster"><div class="titlepage"><div><div><h3 class="title">4.5.7. Displaying the Insights status in the web console</h3></div></div></div><p>
					Insights repeatedly analyzes your cluster and you can display the status of identified potential issues of your cluster in the OpenShift Container Platform web console. This status shows the number of issues in the different categories and, for further details, links to the reports in <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a>.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Your cluster is registered in <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a>.
						</li><li class="listitem">
							Remote health reporting is enabled, which is the default.
						</li><li class="listitem">
							You are logged in to the OpenShift Container Platform web console.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to <span class="strong strong"><strong>Home</strong></span> → <span class="strong strong"><strong>Overview</strong></span> in the OpenShift Container Platform web console.
						</li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Insights</strong></span> on the <span class="strong strong"><strong>Status</strong></span> card.
						</p><p class="simpara">
							The pop-up window lists potential issues grouped by risk. Click the individual categories or <span class="strong strong"><strong>View all recommendations in Insights Advisor</strong></span> to display more details.
						</p></li></ol></div></section></section><section class="section" id="using-insights-operator"><div class="titlepage"><div><div><h2 class="title">4.6. Using Insights Operator</h2></div></div></div><p>
				The Insights Operator periodically gathers configuration and component failure status and, by default, reports that data every two hours to Red Hat. This information enables Red Hat to assess configuration and deeper failure data than is reported through Telemetry. Users of OpenShift Container Platform can display the report in the <a class="link" href="https://console.redhat.com/openshift/insights/advisor/">Insights Advisor</a> service on Red Hat Hybrid Cloud Console.
			</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						The Insights Operator is installed and enabled by default. If you need to opt out of remote health reporting, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#opting-out-remote-health-reporting">Opting out of remote health reporting</a>.
					</li><li class="listitem">
						For more information on using Insights Advisor to identify issues with your cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#using-insights-to-identify-issues-with-your-cluster">Using Insights to identify issues with your cluster</a>.
					</li></ul></div><section class="section" id="understanding-insights-operator-alerts_using-insights-operator"><div class="titlepage"><div><div><h3 class="title">4.6.1. Understanding Insights Operator alerts</h3></div></div></div><p>
					Insights Operator declares alerts through the Prometheus monitoring system to Alertmanager. You can view these alerts in the Alerting UI accessible through the <span class="strong strong"><strong>Administrator</strong></span> perspective and the <span class="strong strong"><strong>Developer</strong></span> perspective in the OpenShift Container Platform web console.
				</p><p>
					Currently, Insights Operator sends the following alerts when the conditions are met:
				</p><div class="table" id="idm140604676065744"><p class="title"><strong>Table 4.1. Insights Operator alerts</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140604675806288" scope="col">Alert</th><th align="left" valign="top" id="idm140604675805200" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140604675806288"> <p>
									<code class="literal">InsightsDisabled</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604675805200"> <p>
									Insights Operator is disabled.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604675806288"> <p>
									<code class="literal">SimpleContentAccessNotAvailable</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604675805200"> <p>
									Simple content access is not enabled in Red Hat Subscription Management.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604675806288"> <p>
									<code class="literal">InsightsRecommendationActive</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604675805200"> <p>
									Insights has an active recommendation for the cluster.
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section" id="disabling-insights-operator-alerts_using-insights-operator"><div class="titlepage"><div><div><h3 class="title">4.6.2. Disabling Insights Operator alerts</h3></div></div></div><p>
					You can stop Insights Operator from firing alerts to the cluster Prometheus instance.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Secrets</strong></span>.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>Secrets</strong></span> page, select <span class="strong strong"><strong>All Projects</strong></span> from the <span class="strong strong"><strong>Project</strong></span> list, and then set <span class="strong strong"><strong>Show default projects</strong></span> to on.
						</li><li class="listitem">
							Select the <span class="strong strong"><strong>openshift-config</strong></span> project from the <span class="strong strong"><strong>Projects</strong></span> list.
						</li><li class="listitem">
							Search for the <span class="strong strong"><strong>support</strong></span> secret using the <span class="strong strong"><strong>Search by name</strong></span> field. If the secret does not exist, click <span class="strong strong"><strong>Create</strong></span> → <span class="strong strong"><strong>Key/value secret</strong></span> to create it.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>Options</strong></span> menu 
							<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Support-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
							 , and then click <span class="strong strong"><strong>Edit Secret</strong></span>.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Add Key/Value</strong></span>.
						</li><li class="listitem">
							Enter <code class="literal">disableInsightsAlerts</code> as the key with the value <code class="literal">True</code>, and click <span class="strong strong"><strong>Save</strong></span>.
						</li></ol></div><p>
					After you save the changes, Insights Operator will no longer send alerts to the cluster Prometheus instance.
				</p></section><section class="section" id="insights-operator-downloading-archive_using-insights-operator"><div class="titlepage"><div><div><h3 class="title">4.6.3. Downloading your Insights Operator archive</h3></div></div></div><p>
					Insights Operator stores gathered data in an archive located in the <code class="literal">openshift-insights</code> namespace of your cluster. You can download and review the data that is gathered by the Insights Operator.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Find the name of the running pod for the Insights Operator:
						</p><pre class="programlisting language-terminal">$ oc get pods --namespace=openshift-insights -o custom-columns=:metadata.name --no-headers  --field-selector=status.phase=Running</pre></li><li class="listitem"><p class="simpara">
							Copy the recent data archives collected by the Insights Operator:
						</p><pre class="programlisting language-terminal">$ oc cp openshift-insights/&lt;insights_operator_pod_name&gt;:/var/lib/insights-operator ./insights-data <span id="CO4-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO4-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal">&lt;insights_operator_pod_name&gt;</code> with the pod name output from the preceding command.
								</div></dd></dl></div></li></ol></div><p>
					The recent Insights Operator archives are now available in the <code class="literal">insights-data</code> directory.
				</p></section><section class="section" id="insights-operator-gather-duration_using-insights-operator"><div class="titlepage"><div><div><h3 class="title">4.6.4. Viewing Insights Operator gather durations</h3></div></div></div><p>
					You can view the time it takes for the Insights Operator to gather the information contained in the archive. This helps you to understand Insights Operator resource usage and issues with Insights Advisor.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							A recent copy of your Insights Operator archive.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							From your archive, open <code class="literal">/insights-operator/gathers.json</code>.
						</p><p class="simpara">
							The file contains a list of Insights Operator gather operations:
						</p><pre class="programlisting language-json">    {
      "name": "clusterconfig/authentication",
      "duration_in_ms": 730, <span id="CO5-1"><!--Empty--></span><span class="callout">1</span>
      "records_count": 1,
      "errors": null,
      "panic": null
    }</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO5-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal">duration_in_ms</code> is the amount of time in milliseconds for each gather operation.
								</div></dd></dl></div></li><li class="listitem">
							Inspect each gather operation for abnormalities.
						</li></ol></div></section><section class="section" id="disabling-insights-operator-gather_using-insights-operator"><div class="titlepage"><div><div><h3 class="title">4.6.5. Disabling the Insights Operator gather operations</h3></div></div></div><p>
					You can disable the Insights Operator gather operations. Disabling the gather operations gives you the ability to increase privacy for your organization as Insights Operator will no longer gather and send Insights cluster reports to Red Hat. This will disable Insights analysis and recommendations for your cluster without affecting other core functions that require communication with Red Hat such as cluster transfers. You can view a list of attempted gather operations for your cluster from the <code class="literal">/insights-operator/gathers.json</code> file in your Insights Operator archive. Be aware that some gather operations only occur when certain conditions are met and might not appear in your most recent archive.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The <code class="literal">InsightsDataGather</code> custom resource is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
					</p><p>
						For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You are logged in to the OpenShift Container Platform web console as a user with <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>CustomResourceDefinitions</strong></span>.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>CustomResourceDefinitions</strong></span> page, use the <span class="strong strong"><strong>Search by name</strong></span> field to find the <span class="strong strong"><strong>InsightsDataGather</strong></span> resource definition and click it.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>CustomResourceDefinition details</strong></span> page, click the <span class="strong strong"><strong>Instances</strong></span> tab.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>cluster</strong></span>, and then click the <span class="strong strong"><strong>YAML</strong></span> tab.
						</li><li class="listitem"><p class="simpara">
							To disable all the gather operations, edit the <code class="literal">InsightsDataGather</code> configuration file:
						</p><pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1alpha1
kind: InsightsDataGather
metadata:
....

spec: <span id="CO6-1"><!--Empty--></span><span class="callout">1</span>
  gatherConfig:
    disabledGatherers:
      - all <span id="CO6-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO6-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal">spec</code> parameter specifies gather configurations.
								</div></dd><dt><a href="#CO6-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The <code class="literal">all</code> value disables all gather operations.
								</div></dd></dl></div><p class="simpara">
							To disable individual gather operations, enter their values under the <code class="literal">disabledGatherers</code> key:
						</p><pre class="programlisting language-yaml">spec:
  gatherConfig:
    disabledGatherers:
      - clusterconfig/container_images <span id="CO7-1"><!--Empty--></span><span class="callout">1</span>
      - clusterconfig/host_subnets
      - workloads/workload_info</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO7-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Example individual gather operation
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Save</strong></span>.
						</p><p class="simpara">
							After you save the changes, the Insights Operator gather configurations are updated and the operations will no longer occur.
						</p></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Disabling gather operations degrades Insights Advisor’s ability to offer effective recommendations for your cluster.
					</p></div></div></section><section class="section" id="insights-operator-configuring-sca_using-insights-operator"><div class="titlepage"><div><div><h3 class="title">4.6.6. Configuring Insights Operator</h3></div></div></div><p>
					You can configure Insights Operator to meet the needs of your organization. The Insights Operator is configured using a combination of the default configurations in the <code class="literal">pod.yaml</code> file in the Insights Operator <code class="literal">Config</code> directory and the configurations stored in the <code class="literal">support</code> secret in the <code class="literal">openshift-config</code> namespace. The <code class="literal">support</code> secret does not exist by default and must be created when adding custom configurations for the first time. Configurations in the <code class="literal">support</code> secret override the defaults set in the <code class="literal">pod.yaml</code> file.
				</p><p>
					The table below describes the available configuration attributes:
				</p><div class="table" id="idm140604675428640"><p class="title"><strong>Table 4.2. Insights Operator configurable attributes</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140604675176112" scope="col">Attribute name</th><th align="left" valign="top" id="idm140604675175024" scope="col">Description</th><th align="left" valign="top" id="idm140604675173936" scope="col">Value type</th><th align="left" valign="top" id="idm140604675172848" scope="col">Default value</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140604675176112"> <p>
									<code class="literal">username</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604675175024"> <p>
									Specifies username for basic authentication with <code class="literal">console.redhat.com</code> (overrides the default <code class="literal">pull-secret</code> token authentication when set)
								</p>
								 </td><td align="left" valign="top" headers="idm140604675173936"> <p>
									String
								</p>
								 </td><td align="left" valign="top" headers="idm140604675172848"> <p>
									Not set
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604675176112"> <p>
									<code class="literal">password</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604675175024"> <p>
									Specifies password for basic authentication with <code class="literal">console.redhat.com</code> (overrides the default <code class="literal">pull-secret</code> token authentication when set)
								</p>
								 </td><td align="left" valign="top" headers="idm140604675173936"> <p>
									String
								</p>
								 </td><td align="left" valign="top" headers="idm140604675172848"> <p>
									Not set
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604675176112"> <p>
									<code class="literal">enableGlobalObfuscation</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604675175024"> <p>
									Enables the global obfuscation of IP addresses and the cluster domain name
								</p>
								 </td><td align="left" valign="top" headers="idm140604675173936"> <p>
									Boolean
								</p>
								 </td><td align="left" valign="top" headers="idm140604675172848"> <p>
									<code class="literal">false</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604675176112"> <p>
									<code class="literal">scaInterval</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604675175024"> <p>
									Specifies the frequency of the simple content access entitlements download
								</p>
								 </td><td align="left" valign="top" headers="idm140604675173936"> <p>
									Time interval
								</p>
								 </td><td align="left" valign="top" headers="idm140604675172848"> <p>
									<code class="literal">8h</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604675176112"> <p>
									<code class="literal">scaPullDisabled</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604675175024"> <p>
									Disables the simple content access entitlements download
								</p>
								 </td><td align="left" valign="top" headers="idm140604675173936"> <p>
									Boolean
								</p>
								 </td><td align="left" valign="top" headers="idm140604675172848"> <p>
									<code class="literal">false</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604675176112"> <p>
									<code class="literal">clusterTransferInterval</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604675175024"> <p>
									Specifies how often Insights Operator checks OpenShift Cluster Manager for available cluster transfers
								</p>
								 </td><td align="left" valign="top" headers="idm140604675173936"> <p>
									Time interval
								</p>
								 </td><td align="left" valign="top" headers="idm140604675172848"> <p>
									<code class="literal">24h</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604675176112"> <p>
									<code class="literal">disableInsightsAlerts</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604675175024"> <p>
									Disables Insights Operator alerts to the cluster Prometheus instance
								</p>
								 </td><td align="left" valign="top" headers="idm140604675173936"> <p>
									Boolean
								</p>
								 </td><td align="left" valign="top" headers="idm140604675172848"> <p>
									<code class="literal">False</code>
								</p>
								 </td></tr></tbody></table></div></div><p>
					This procedure describes how to set custom Insights Operator configurations.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Red Hat recommends you consult Red Hat Support before making changes to the default Insights Operator configuration.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You are logged in to the OpenShift Container Platform web console as a user with <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Secrets</strong></span>.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>Secrets</strong></span> page, select <span class="strong strong"><strong>All Projects</strong></span> from the <span class="strong strong"><strong>Project</strong></span> list, and then set <span class="strong strong"><strong>Show default projects</strong></span> to on.
						</li><li class="listitem">
							Select the <span class="strong strong"><strong>openshift-config</strong></span> project from the <span class="strong strong"><strong>Project</strong></span> list.
						</li><li class="listitem">
							Search for the <span class="strong strong"><strong>support</strong></span> secret using the <span class="strong strong"><strong>Search by name</strong></span> field. If it does not exist, click <span class="strong strong"><strong>Create</strong></span> → <span class="strong strong"><strong>Key/value secret</strong></span> to create it.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>Options</strong></span> menu 
							<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Support-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
							 for the secret, and then click <span class="strong strong"><strong>Edit Secret</strong></span>.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Add Key/Value</strong></span>.
						</li><li class="listitem">
							Enter an attribute name with an appropriate value (see table above), and click <span class="strong strong"><strong>Save</strong></span>.
						</li><li class="listitem">
							Repeat the above steps for any additional configurations.
						</li></ol></div></section></section><section class="section" id="remote-health-reporting-from-restricted-network"><div class="titlepage"><div><div><h2 class="title">4.7. Using remote health reporting in a restricted network</h2></div></div></div><p>
				You can manually gather and upload Insights Operator archives to diagnose issues from a restricted network.
			</p><p>
				To use the Insights Operator in a restricted network, you must:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Create a copy of your Insights Operator archive.
					</li><li class="listitem">
						Upload the Insights Operator archive to <a class="link" href="https://console.redhat.com">console.redhat.com</a>.
					</li></ul></div><p>
				Additionally, you can choose to <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#insights-operator-enable-obfuscation_remote-health-reporting-from-restricted-network">obfuscate</a> the Insights Operator data before upload.
			</p><section class="section" id="insights-operator-one-time-gather_remote-health-reporting-from-restricted-network"><div class="titlepage"><div><div><h3 class="title">4.7.1. Running an Insights Operator gather operation</h3></div></div></div><p>
					You must run a gather operation to create an Insights Operator archive.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You are logged in to OpenShift Container Platform as <code class="literal">cluster-admin</code>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a file named <code class="literal">gather-job.yaml</code> using this template:
						</p><pre class="programlisting language-yaml">apiVersion: batch/v1
kind: Job
metadata:
  name: insights-operator-job
  annotations:
    config.openshift.io/inject-proxy: insights-operator
spec:
  backoffLimit: 6
  ttlSecondsAfterFinished: 600
  template:
    spec:
      restartPolicy: OnFailure
      serviceAccountName: operator
      nodeSelector:
        beta.kubernetes.io/os: linux
        node-role.kubernetes.io/master: ""
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
        operator: Exists
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 900
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 900
      volumes:
      - name: snapshots
        emptyDir: {}
      - name: service-ca-bundle
        configMap:
          name: service-ca-bundle
          optional: true
      initContainers:
      - name: insights-operator
        image: quay.io/openshift/origin-insights-operator:latest
        terminationMessagePolicy: FallbackToLogsOnError
        volumeMounts:
        - name: snapshots
          mountPath: /var/lib/insights-operator
        - name: service-ca-bundle
          mountPath: /var/run/configmaps/service-ca-bundle
          readOnly: true
        ports:
        - containerPort: 8443
          name: https
        resources:
          requests:
            cpu: 10m
            memory: 70Mi
        args:
        - gather
        - -v=4
        - --config=/etc/insights-operator/server.yaml
      containers:
        - name: sleepy
          image: quay.io/openshift/origin-base:latest
          args:
            - /bin/sh
            - -c
            - sleep 10m
          volumeMounts: [{name: snapshots, mountPath: /var/lib/insights-operator}]</pre></li><li class="listitem"><p class="simpara">
							Copy your <code class="literal">insights-operator</code> image version:
						</p><pre class="programlisting language-terminal">$ oc get -n openshift-insights deployment insights-operator -o yaml</pre></li><li class="listitem"><p class="simpara">
							Paste your image version in <code class="literal">gather-job.yaml</code>:
						</p><pre class="programlisting language-yaml">initContainers:
      - name: insights-operator
        image: <span class="emphasis"><em>&lt;your_insights_operator_image_version&gt;</em></span>
        terminationMessagePolicy: FallbackToLogsOnError
        volumeMounts:</pre></li><li class="listitem"><p class="simpara">
							Create the gather job:
						</p><pre class="programlisting language-terminal">$ oc apply -n openshift-insights -f gather-job.yaml</pre></li><li class="listitem"><p class="simpara">
							Find the name of the job pod:
						</p><pre class="programlisting language-terminal">$ oc describe -n openshift-insights job/insights-operator-job</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Events:
  Type    Reason            Age    From            Message
  ----    ------            ----   ----            -------
  Normal  SuccessfulCreate  7m18s  job-controller  Created pod: insights-operator-job-<span class="emphasis"><em>&lt;your_job&gt;</em></span></pre>

							</p></div><p class="simpara">
							where <code class="literal">insights-operator-job-<span class="emphasis"><em>&lt;your_job&gt;</em></span></code> is the name of the pod.
						</p></li><li class="listitem"><p class="simpara">
							Verify that the operation has finished:
						</p><pre class="programlisting language-terminal">$ oc logs -n openshift-insights insights-operator-job-<span class="emphasis"><em>&lt;your_job&gt;</em></span> insights-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">I0407 11:55:38.192084       1 diskrecorder.go:34] Wrote 108 records to disk in 33ms</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Save the created archive:
						</p><pre class="programlisting language-terminal">$ oc cp openshift-insights/insights-operator-job-<span class="emphasis"><em>&lt;your_job&gt;</em></span>:/var/lib/insights-operator ./insights-data</pre></li><li class="listitem"><p class="simpara">
							Clean up the job:
						</p><pre class="programlisting language-terminal">$ oc delete -n openshift-insights job insights-operator-job</pre></li></ol></div></section><section class="section" id="insights-operator-manual-upload_remote-health-reporting-from-restricted-network"><div class="titlepage"><div><div><h3 class="title">4.7.2. Uploading an Insights Operator archive</h3></div></div></div><p>
					You can manually upload an Insights Operator archive to <a class="link" href="https://console.redhat.com">console.redhat.com</a> to diagnose potential issues.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You are logged in to OpenShift Container Platform as <code class="literal">cluster-admin</code>.
						</li><li class="listitem">
							You have a workstation with unrestricted internet access.
						</li><li class="listitem">
							You have created a copy of the Insights Operator archive.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Download the <code class="literal">dockerconfig.json</code> file:
						</p><pre class="programlisting language-terminal">$ oc extract secret/pull-secret -n openshift-config --to=.</pre></li><li class="listitem"><p class="simpara">
							Copy your <code class="literal">"cloud.openshift.com"</code> <code class="literal">"auth"</code> token from the <code class="literal">dockerconfig.json</code> file:
						</p><pre class="programlisting language-json">{
  "auths": {
    "cloud.openshift.com": {
      "auth": "<span class="emphasis"><em>&lt;your_token&gt;</em></span>",
      "email": "asd@redhat.com"
    }
}</pre></li><li class="listitem"><p class="simpara">
							Upload the archive to <a class="link" href="https://console.redhat.com">console.redhat.com</a>:
						</p><pre class="programlisting language-terminal">$ curl -v -H "User-Agent: insights-operator/one10time200gather184a34f6a168926d93c330 cluster/<span class="emphasis"><em>&lt;cluster_id&gt;</em></span>" -H "Authorization: Bearer <span class="emphasis"><em>&lt;your_token&gt;</em></span>" -F "upload=@<span class="emphasis"><em>&lt;path_to_archive&gt;</em></span>; type=application/vnd.redhat.openshift.periodic+tar" https://console.redhat.com/api/ingress/v1/upload</pre><p class="simpara">
							where <code class="literal"><span class="emphasis"><em>&lt;cluster_id&gt;</em></span></code> is your cluster ID, <code class="literal"><span class="emphasis"><em>&lt;your_token&gt;</em></span></code> is the token from your pull secret, and <code class="literal"><span class="emphasis"><em>&lt;path_to_archive&gt;</em></span></code> is the path to the Insights Operator archive.
						</p><p class="simpara">
							If the operation is successful, the command returns a <code class="literal">"request_id"</code> and <code class="literal">"account_number"</code>:
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">* Connection #0 to host console.redhat.com left intact
{"request_id":"393a7cf1093e434ea8dd4ab3eb28884c","upload":{"account_number":"6274079"}}%</pre>

							</p></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification steps</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to <a class="link" href="https://console.redhat.com/openshift">https://console.redhat.com/openshift</a>.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>Clusters</strong></span> menu in the left pane.
						</li><li class="listitem">
							To display the details of the cluster, click the cluster name.
						</li><li class="listitem"><p class="simpara">
							Open the <span class="strong strong"><strong>Insights Advisor</strong></span> tab of the cluster.
						</p><p class="simpara">
							If the upload was successful, the tab displays one of the following:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<span class="strong strong"><strong>Your cluster passed all recommendations</strong></span>, if Insights Advisor did not identify any issues.
								</li><li class="listitem">
									A list of issues that Insights Advisor has detected, prioritized by risk (low, moderate, important, and critical).
								</li></ul></div></li></ol></div></section><section class="section" id="insights-operator-enable-obfuscation_remote-health-reporting-from-restricted-network"><div class="titlepage"><div><div><h3 class="title">4.7.3. Enabling Insights Operator data obfuscation</h3></div></div></div><p>
					You can enable obfuscation to mask sensitive and identifiable IPv4 addresses and cluster base domains that the Insights Operator sends to <a class="link" href="https://console.redhat.com">console.redhat.com</a>.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Although this feature is available, Red Hat recommends keeping obfuscation disabled for a more effective support experience.
					</p></div></div><p>
					Obfuscation assigns non-identifying values to cluster IPv4 addresses, and uses a translation table that is retained in memory to change IP addresses to their obfuscated versions throughout the Insights Operator archive before uploading the data to <a class="link" href="https://console.redhat.com">console.redhat.com</a>.
				</p><p>
					For cluster base domains, obfuscation changes the base domain to a hardcoded substring. For example, <code class="literal">cluster-api.openshift.example.com</code> becomes <code class="literal">cluster-api.&lt;CLUSTER_BASE_DOMAIN&gt;</code>.
				</p><p>
					The following procedure enables obfuscation using the <code class="literal">support</code> secret in the <code class="literal">openshift-config</code> namespace.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You are logged in to the OpenShift Container Platform web console as <code class="literal">cluster-admin</code>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Secrets</strong></span>.
						</li><li class="listitem">
							Select the <span class="strong strong"><strong>openshift-config</strong></span> project.
						</li><li class="listitem">
							Search for the <span class="strong strong"><strong>support</strong></span> secret using the <span class="strong strong"><strong>Search by name</strong></span> field. If it does not exist, click <span class="strong strong"><strong>Create</strong></span> → <span class="strong strong"><strong>Key/value secret</strong></span> to create it.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>Options</strong></span> menu 
							<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Support-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
							 , and then click <span class="strong strong"><strong>Edit Secret</strong></span>.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Add Key/Value</strong></span>.
						</li><li class="listitem">
							Create a key named <code class="literal">enableGlobalObfuscation</code> with a value of <code class="literal">true</code>, and click <span class="strong strong"><strong>Save</strong></span>.
						</li><li class="listitem">
							Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Pods</strong></span>
						</li><li class="listitem">
							Select the <code class="literal">openshift-insights</code> project.
						</li><li class="listitem">
							Find the <code class="literal">insights-operator</code> pod.
						</li><li class="listitem">
							To restart the <code class="literal">insights-operator</code> pod, click the <span class="strong strong"><strong>Options</strong></span> menu 
							<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Support-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
							 , and then click <span class="strong strong"><strong>Delete Pod</strong></span>.
						</li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Secrets</strong></span>.
						</li><li class="listitem">
							Select the <span class="strong strong"><strong>openshift-insights</strong></span> project.
						</li><li class="listitem">
							Search for the <span class="strong strong"><strong>obfuscation-translation-table</strong></span> secret using the <span class="strong strong"><strong>Search by name</strong></span> field.
						</li></ol></div><p>
					If the <code class="literal">obfuscation-translation-table</code> secret exists, then obfuscation is enabled and working.
				</p><p>
					Alternatively, you can inspect <code class="literal">/insights-operator/gathers.json</code> in your Insights Operator archive for the value <code class="literal">"is_global_obfuscation_enabled": true</code>.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For more information on how to download your Insights Operator archive, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#insights-operator-showing-data-collected-from-the-cluster_showing-data-collected-by-remote-health-monitoring">Showing data collected by the Insights Operator</a>.
						</li></ul></div></section></section><section class="section" id="insights-operator-simple-access"><div class="titlepage"><div><div><h2 class="title">4.8. Importing simple content access entitlements with Insights Operator</h2></div></div></div><p>
				Insights Operator periodically imports your simple content access entitlements from <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a> and stores them in the <code class="literal">etc-pki-entitlement</code> secret in the <code class="literal">openshift-config-managed</code> namespace. Simple content access is a capability in Red Hat subscription tools which simplifies the behavior of the entitlement tooling. This feature makes it easier to consume the content provided by your Red Hat subscriptions without the complexity of configuring subscription tooling.
			</p><p>
				Insights Operator imports simple content access entitlements every eight hours, but can be configured or disabled using the <code class="literal">support</code> secret in the <code class="literal">openshift-config</code> namespace.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Simple content access must be enabled in Red Hat Subscription Management for the importing to function.
				</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						See <a class="link" href="https://access.redhat.com/documentation/en-us/subscription_central/2021/html-single/getting_started_with_simple_content_access/index#assembly-about-simplecontent">About simple content access</a> in the Red Hat Subscription Central documentation, for more information about simple content access.
					</li><li class="listitem">
						See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/cicd/#using-red-hat-subscriptions-in-builds">Using Red Hat subscriptions in builds</a> for more information about using simple content access entitlements in OpenShift Container Platform builds.
					</li></ul></div><section class="section" id="insights-operator-configuring-sca_remote-health-reporting-from-restricted-network"><div class="titlepage"><div><div><h3 class="title">4.8.1. Configuring simple content access import interval</h3></div></div></div><p>
					You can configure how often the Insights Operator imports the simple content access entitlements using the <code class="literal">support</code> secret in the <code class="literal">openshift-config</code> namespace. The entitlement import normally occurs every eight hours, but you can shorten this interval if you update your simple content access configuration in Red Hat Subscription Management.
				</p><p>
					This procedure describes how to update the import interval to one hour.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You are logged in to the OpenShift Container Platform web console as <code class="literal">cluster-admin</code>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Secrets</strong></span>.
						</li><li class="listitem">
							Select the <span class="strong strong"><strong>openshift-config</strong></span> project.
						</li><li class="listitem">
							Search for the <span class="strong strong"><strong>support</strong></span> secret using the <span class="strong strong"><strong>Search by name</strong></span> field. If it does not exist, click <span class="strong strong"><strong>Create</strong></span> → <span class="strong strong"><strong>Key/value secret</strong></span> to create it.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>Options</strong></span> menu 
							<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Support-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
							 , and then click <span class="strong strong"><strong>Edit Secret</strong></span>.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Add Key/Value</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Create a key named <code class="literal">scaInterval</code> with a value of <code class="literal">1h</code>, and click <span class="strong strong"><strong>Save</strong></span>.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The interval <code class="literal">1h</code> can also be entered as <code class="literal">60m</code> for 60 minutes.
							</p></div></div></li></ol></div></section><section class="section" id="insights-operator-disabling-sca_remote-health-reporting-from-restricted-network"><div class="titlepage"><div><div><h3 class="title">4.8.2. Disabling simple content access import</h3></div></div></div><p>
					You can disable the importing of simple content access entitlements using the <code class="literal">support</code> secret in the <code class="literal">openshift-config</code> namespace.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You are logged in to the OpenShift Container Platform web console as <code class="literal">cluster-admin</code>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Secrets</strong></span>.
						</li><li class="listitem">
							Select the <span class="strong strong"><strong>openshift-config</strong></span> project.
						</li><li class="listitem">
							Search for the <span class="strong strong"><strong>support</strong></span> secret using the <span class="strong strong"><strong>Search by name</strong></span> field. If it does not exist, click <span class="strong strong"><strong>Create</strong></span> → <span class="strong strong"><strong>Key/value secret</strong></span> to create it.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>Options</strong></span> menu 
							<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Support-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
							 , and then click <span class="strong strong"><strong>Edit Secret</strong></span>.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Add Key/Value</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Create a key named <code class="literal">scaPullDisabled</code> with a value of <code class="literal">true</code>, and click <span class="strong strong"><strong>Save</strong></span>.
						</p><p class="simpara">
							The simple content access entitlement import is now disabled.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								To enable the simple content access import again, edit the <code class="literal">support</code> secret and delete the <code class="literal">scaPullDisabled</code> key.
							</p></div></div></li></ol></div></section></section></section><section class="chapter" id="gathering-cluster-data"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Gathering data about your cluster</h1></div></div></div><p>
			When opening a support case, it is helpful to provide debugging information about your cluster to Red Hat Support.
		</p><p>
			It is recommended to provide:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#support_gathering_data_gathering-cluster-data">Data gathered using the <code class="literal">oc adm must-gather</code> command</a>
				</li><li class="listitem">
					The <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#support-get-cluster-id_gathering-cluster-data">unique cluster ID</a>
				</li></ul></div><section class="section" id="about-must-gather_gathering-cluster-data"><div class="titlepage"><div><div><h2 class="title">5.1. About the must-gather tool</h2></div></div></div><p>
				The <code class="literal">oc adm must-gather</code> CLI command collects the information from your cluster that is most likely needed for debugging issues, including:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Resource definitions
					</li><li class="listitem">
						Service logs
					</li></ul></div><p>
				By default, the <code class="literal">oc adm must-gather</code> command uses the default plugin image and writes into <code class="literal">./must-gather.local</code>.
			</p><p>
				Alternatively, you can collect specific information by running the command with the appropriate arguments as described in the following sections:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						To collect data related to one or more specific features, use the <code class="literal">--image</code> argument with an image, as listed in a following section.
					</p><p class="simpara">
						For example:
					</p><pre class="programlisting language-terminal">$ oc adm must-gather \
  --image=registry.redhat.io/container-native-virtualization/cnv-must-gather-rhel9:v4.13.4</pre></li><li class="listitem"><p class="simpara">
						To collect the audit logs, use the <code class="literal">-- /usr/bin/gather_audit_logs</code> argument, as described in a following section.
					</p><p class="simpara">
						For example:
					</p><pre class="programlisting language-terminal">$ oc adm must-gather -- /usr/bin/gather_audit_logs</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Audit logs are not collected as part of the default set of information to reduce the size of the files.
						</p></div></div></li></ul></div><p>
				When you run <code class="literal">oc adm must-gather</code>, a new pod with a random name is created in a new project on the cluster. The data is collected on that pod and saved in a new directory that starts with <code class="literal">must-gather.local</code>. This directory is created in the current working directory.
			</p><p>
				For example:
			</p><pre class="programlisting language-terminal">NAMESPACE                      NAME                 READY   STATUS      RESTARTS      AGE
...
openshift-must-gather-5drcj    must-gather-bklx4    2/2     Running     0             72s
openshift-must-gather-5drcj    must-gather-s8sdh    2/2     Running     0             72s
...</pre><p>
				Optionally, you can run the <code class="literal">oc adm must-gather</code> command in a specific namespace by using the <code class="literal">--run-namespace</code> option.
			</p><p>
				For example:
			</p><pre class="programlisting language-terminal">$ oc adm must-gather --run-namespace &lt;namespace&gt; \
  --image=registry.redhat.io/container-native-virtualization/cnv-must-gather-rhel9:v4.13.4</pre><section class="section" id="support_gathering_data_gathering-cluster-data"><div class="titlepage"><div><div><h3 class="title">5.1.1. Gathering data about your cluster for Red Hat Support</h3></div></div></div><p>
					You can gather debugging information about your cluster by using the <code class="literal">oc adm must-gather</code> CLI command.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							The OpenShift Container Platform CLI (<code class="literal">oc</code>) installed.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Navigate to the directory where you want to store the <code class="literal">must-gather</code> data.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If your cluster is in a disconnected environment, you must take additional steps. If your mirror registry has a trusted CA, you must first add the trusted CA to the cluster. For all clusters in disconnected environments, you must import the default <code class="literal">must-gather</code> image as an image stream.
							</p><pre class="programlisting language-terminal">$ oc import-image is/must-gather -n openshift</pre></div></div></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">oc adm must-gather</code> command:
						</p><pre class="programlisting language-terminal">$ oc adm must-gather</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								If you are in a disconnected environment, use the <code class="literal">--image</code> flag as part of must-gather and point to the payload image.
							</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Because this command picks a random control plane node by default, the pod might be scheduled to a control plane node that is in the <code class="literal">NotReady</code> and <code class="literal">SchedulingDisabled</code> state.
							</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									If this command fails, for example, if you cannot schedule a pod on your cluster, then use the <code class="literal">oc adm inspect</code> command to gather information for particular resources.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Contact Red Hat Support for the recommended resources to gather.
									</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a compressed file from the <code class="literal">must-gather</code> directory that was just created in your working directory. For example, on a computer that uses a Linux operating system, run the following command:
						</p><pre class="programlisting language-terminal">$ tar cvaf must-gather.tar.gz must-gather.local.5421342344627712289/ <span id="CO8-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO8-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Make sure to replace <code class="literal">must-gather-local.5421342344627712289/</code> with the actual directory name.
								</div></dd></dl></div></li><li class="listitem">
							Attach the compressed file to your support case on the <a class="link" href="https://access.redhat.com">Red Hat Customer Portal</a>.
						</li></ol></div></section><section class="section" id="gathering-data-specific-features_gathering-cluster-data"><div class="titlepage"><div><div><h3 class="title">5.1.2. Gathering data about specific features</h3></div></div></div><p>
					You can gather debugging information about specific features by using the <code class="literal">oc adm must-gather</code> CLI command with the <code class="literal">--image</code> or <code class="literal">--image-stream</code> argument. The <code class="literal">must-gather</code> tool supports multiple images, so you can gather data about more than one feature by running a single command.
				</p><div class="table" id="idm140604674552240"><p class="title"><strong>Table 5.1. Supported must-gather images</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140604674547392" scope="col">Image</th><th align="left" valign="top" id="idm140604674546304" scope="col">Purpose</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140604674547392"> <p>
									<code class="literal">registry.redhat.io/container-native-virtualization/cnv-must-gather-rhel9:v4.13.4</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604674546304"> <p>
									Data collection for OpenShift Virtualization.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604674547392"> <p>
									<code class="literal">registry.redhat.io/openshift-serverless-1/svls-must-gather-rhel8</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604674546304"> <p>
									Data collection for OpenShift Serverless.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604674547392"> <p>
									<code class="literal">registry.redhat.io/openshift-service-mesh/istio-must-gather-rhel8:v&lt;installed_version_service_mesh&gt;</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604674546304"> <p>
									Data collection for Red Hat OpenShift Service Mesh.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604674547392"> <p>
									<code class="literal">registry.redhat.io/rhmtc/openshift-migration-must-gather-rhel8:v&lt;installed_version_migration_toolkit&gt;</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604674546304"> <p>
									Data collection for the Migration Toolkit for Containers.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604674547392"> <p>
									<code class="literal">registry.redhat.io/odf4/ocs-must-gather-rhel8:v&lt;installed_version_ODF&gt;</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604674546304"> <p>
									Data collection for Red Hat OpenShift Data Foundation.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604674547392"> <p>
									<code class="literal">registry.redhat.io/openshift-logging/cluster-logging-rhel8-operator</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604674546304"> <p>
									Data collection for OpenShift Logging.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604674547392"> <p>
									<code class="literal">registry.redhat.io/openshift4/ose-csi-driver-shared-resource-mustgather-rhel8</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604674546304"> <p>
									Data collection for OpenShift Shared Resource CSI Driver.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604674547392"> <p>
									<code class="literal">registry.redhat.io/openshift4/ose-local-storage-mustgather-rhel8:v&lt;installed_version_LSO&gt;</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604674546304"> <p>
									Data collection for Local Storage Operator.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604674547392"> <p>
									<code class="literal">registry.redhat.io/openshift-sandboxed-containers/osc-must-gather-rhel8:v&lt;installed_version_sandboxed_containers&gt;</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604674546304"> <p>
									Data collection for OpenShift sandboxed containers.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604674547392"> <p>
									<code class="literal">registry.redhat.io/workload-availability/self-node-remediation-must-gather-rhel8:v&lt;installed-version-SNR&gt;</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604674546304"> <p>
									Data collection for the Self Node Remediation (SNR) Operator and the Node Health Check (NHC) Operator.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604674547392"> <p>
									<code class="literal">registry.redhat.io/openshift4/ptp-must-gather-rhel8:v&lt;installed-version-ptp&gt;</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604674546304"> <p>
									Data collection for the PTP Operator.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604674547392"> <p>
									<code class="literal">registry.redhat.io/workload-availability/node-maintenance-must-gather-rhel8:v&lt;installed-version-NMO&gt;</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604674546304"> <p>
									Data collection for the Node Maintenance Operator (NMO).
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604674547392"> <p>
									<code class="literal">registry.redhat.io/openshift-gitops-1/must-gather-rhel8:v&lt;installed_version_GitOps&gt;</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604674546304"> <p>
									Data collection for Red Hat OpenShift GitOps.
								</p>
								 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						To determine the latest version for an OpenShift Container Platform component’s image, see the <a class="link" href="https://access.redhat.com/support/policy/updates/openshift">Red Hat OpenShift Container Platform Life Cycle Policy</a> web page on the Red Hat Customer Portal.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							The OpenShift Container Platform CLI (<code class="literal">oc</code>) installed.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to the directory where you want to store the <code class="literal">must-gather</code> data.
						</li><li class="listitem"><p class="simpara">
							Run the <code class="literal">oc adm must-gather</code> command with one or more <code class="literal">--image</code> or <code class="literal">--image-stream</code> arguments.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										To collect the default <code class="literal">must-gather</code> data in addition to specific feature data, add the <code class="literal">--image-stream=openshift/must-gather</code> argument.
									</li><li class="listitem">
										For information on gathering data about the Custom Metrics Autoscaler, see the Additional resources section that follows.
									</li></ul></div></div></div><p class="simpara">
							For example, the following command gathers both the default cluster data and information specific to OpenShift Virtualization:
						</p><pre class="programlisting language-terminal">$ oc adm must-gather \
  --image-stream=openshift/must-gather \ <span id="CO9-1"><!--Empty--></span><span class="callout">1</span>
  --image=registry.redhat.io/container-native-virtualization/cnv-must-gather-rhel9:v4.13.4 <span id="CO9-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO9-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The default OpenShift Container Platform <code class="literal">must-gather</code> image
								</div></dd><dt><a href="#CO9-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The must-gather image for OpenShift Virtualization
								</div></dd></dl></div><p class="simpara">
							You can use the <code class="literal">must-gather</code> tool with additional arguments to gather data that is specifically related to OpenShift Logging and the Red Hat OpenShift Logging Operator in your cluster. For OpenShift Logging, run the following command:
						</p><pre class="programlisting language-terminal">$ oc adm must-gather --image=$(oc -n openshift-logging get deployment.apps/cluster-logging-operator \
  -o jsonpath='{.spec.template.spec.containers[?(@.name == "cluster-logging-operator")].image}')</pre><div class="example" id="idm140604675052288"><p class="title"><strong>Example 5.1. Example <code class="literal">must-gather</code> output for OpenShift Logging</strong></p><div class="example-contents"><pre class="programlisting language-terminal">├── cluster-logging
│  ├── clo
│  │  ├── cluster-logging-operator-74dd5994f-6ttgt
│  │  ├── clusterlogforwarder_cr
│  │  ├── cr
│  │  ├── csv
│  │  ├── deployment
│  │  └── logforwarding_cr
│  ├── collector
│  │  ├── fluentd-2tr64
│  ├── eo
│  │  ├── csv
│  │  ├── deployment
│  │  └── elasticsearch-operator-7dc7d97b9d-jb4r4
│  ├── es
│  │  ├── cluster-elasticsearch
│  │  │  ├── aliases
│  │  │  ├── health
│  │  │  ├── indices
│  │  │  ├── latest_documents.json
│  │  │  ├── nodes
│  │  │  ├── nodes_stats.json
│  │  │  └── thread_pool
│  │  ├── cr
│  │  ├── elasticsearch-cdm-lp8l38m0-1-794d6dd989-4jxms
│  │  └── logs
│  │     ├── elasticsearch-cdm-lp8l38m0-1-794d6dd989-4jxms
│  ├── install
│  │  ├── co_logs
│  │  ├── install_plan
│  │  ├── olmo_logs
│  │  └── subscription
│  └── kibana
│     ├── cr
│     ├── kibana-9d69668d4-2rkvz
├── cluster-scoped-resources
│  └── core
│     ├── nodes
│     │  ├── ip-10-0-146-180.eu-west-1.compute.internal.yaml
│     └── persistentvolumes
│        ├── pvc-0a8d65d9-54aa-4c44-9ecc-33d9381e41c1.yaml
├── event-filter.html
├── gather-debug.log
└── namespaces
   ├── openshift-logging
   │  ├── apps
   │  │  ├── daemonsets.yaml
   │  │  ├── deployments.yaml
   │  │  ├── replicasets.yaml
   │  │  └── statefulsets.yaml
   │  ├── batch
   │  │  ├── cronjobs.yaml
   │  │  └── jobs.yaml
   │  ├── core
   │  │  ├── configmaps.yaml
   │  │  ├── endpoints.yaml
   │  │  ├── events
   │  │  │  ├── elasticsearch-im-app-1596020400-gm6nl.1626341a296c16a1.yaml
   │  │  │  ├── elasticsearch-im-audit-1596020400-9l9n4.1626341a2af81bbd.yaml
   │  │  │  ├── elasticsearch-im-infra-1596020400-v98tk.1626341a2d821069.yaml
   │  │  │  ├── elasticsearch-im-app-1596020400-cc5vc.1626341a3019b238.yaml
   │  │  │  ├── elasticsearch-im-audit-1596020400-s8d5s.1626341a31f7b315.yaml
   │  │  │  ├── elasticsearch-im-infra-1596020400-7mgv8.1626341a35ea59ed.yaml
   │  │  ├── events.yaml
   │  │  ├── persistentvolumeclaims.yaml
   │  │  ├── pods.yaml
   │  │  ├── replicationcontrollers.yaml
   │  │  ├── secrets.yaml
   │  │  └── services.yaml
   │  ├── openshift-logging.yaml
   │  ├── pods
   │  │  ├── cluster-logging-operator-74dd5994f-6ttgt
   │  │  │  ├── cluster-logging-operator
   │  │  │  │  └── cluster-logging-operator
   │  │  │  │     └── logs
   │  │  │  │        ├── current.log
   │  │  │  │        ├── previous.insecure.log
   │  │  │  │        └── previous.log
   │  │  │  └── cluster-logging-operator-74dd5994f-6ttgt.yaml
   │  │  ├── cluster-logging-operator-registry-6df49d7d4-mxxff
   │  │  │  ├── cluster-logging-operator-registry
   │  │  │  │  └── cluster-logging-operator-registry
   │  │  │  │     └── logs
   │  │  │  │        ├── current.log
   │  │  │  │        ├── previous.insecure.log
   │  │  │  │        └── previous.log
   │  │  │  ├── cluster-logging-operator-registry-6df49d7d4-mxxff.yaml
   │  │  │  └── mutate-csv-and-generate-sqlite-db
   │  │  │     └── mutate-csv-and-generate-sqlite-db
   │  │  │        └── logs
   │  │  │           ├── current.log
   │  │  │           ├── previous.insecure.log
   │  │  │           └── previous.log
   │  │  ├── elasticsearch-cdm-lp8l38m0-1-794d6dd989-4jxms
   │  │  ├── elasticsearch-im-app-1596030300-bpgcx
   │  │  │  ├── elasticsearch-im-app-1596030300-bpgcx.yaml
   │  │  │  └── indexmanagement
   │  │  │     └── indexmanagement
   │  │  │        └── logs
   │  │  │           ├── current.log
   │  │  │           ├── previous.insecure.log
   │  │  │           └── previous.log
   │  │  ├── fluentd-2tr64
   │  │  │  ├── fluentd
   │  │  │  │  └── fluentd
   │  │  │  │     └── logs
   │  │  │  │        ├── current.log
   │  │  │  │        ├── previous.insecure.log
   │  │  │  │        └── previous.log
   │  │  │  ├── fluentd-2tr64.yaml
   │  │  │  └── fluentd-init
   │  │  │     └── fluentd-init
   │  │  │        └── logs
   │  │  │           ├── current.log
   │  │  │           ├── previous.insecure.log
   │  │  │           └── previous.log
   │  │  ├── kibana-9d69668d4-2rkvz
   │  │  │  ├── kibana
   │  │  │  │  └── kibana
   │  │  │  │     └── logs
   │  │  │  │        ├── current.log
   │  │  │  │        ├── previous.insecure.log
   │  │  │  │        └── previous.log
   │  │  │  ├── kibana-9d69668d4-2rkvz.yaml
   │  │  │  └── kibana-proxy
   │  │  │     └── kibana-proxy
   │  │  │        └── logs
   │  │  │           ├── current.log
   │  │  │           ├── previous.insecure.log
   │  │  │           └── previous.log
   │  └── route.openshift.io
   │     └── routes.yaml
   └── openshift-operators-redhat
      ├── ...</pre></div></div></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">oc adm must-gather</code> command with one or more <code class="literal">--image</code> or <code class="literal">--image-stream</code> arguments. For example, the following command gathers both the default cluster data and information specific to KubeVirt:
						</p><pre class="programlisting language-terminal">$ oc adm must-gather \
 --image-stream=openshift/must-gather \ <span id="CO10-1"><!--Empty--></span><span class="callout">1</span>
 --image=quay.io/kubevirt/must-gather <span id="CO10-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO10-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The default OpenShift Container Platform <code class="literal">must-gather</code> image
								</div></dd><dt><a href="#CO10-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The must-gather image for KubeVirt
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create a compressed file from the <code class="literal">must-gather</code> directory that was just created in your working directory. For example, on a computer that uses a Linux operating system, run the following command:
						</p><pre class="programlisting language-terminal">$ tar cvaf must-gather.tar.gz must-gather.local.5421342344627712289/ <span id="CO11-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO11-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Make sure to replace <code class="literal">must-gather-local.5421342344627712289/</code> with the actual directory name.
								</div></dd></dl></div></li><li class="listitem">
							Attach the compressed file to your support case on the <a class="link" href="https://access.redhat.com">Red Hat Customer Portal</a>.
						</li></ol></div></section></section><section class="section" id="additional-resources"><div class="titlepage"><div><div><h2 class="title">5.2. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-cma-autoscaling-custom-gather">Gathering debugging data</a> for the Custom Metrics Autoscaler.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/support/policy/updates/openshift">Red Hat OpenShift Container Platform Life Cycle Policy</a>
					</li></ul></div><section class="section" id="gathering-data-network-logs_gathering-cluster-data"><div class="titlepage"><div><div><h3 class="title">5.2.1. Gathering network logs</h3></div></div></div><p>
					You can gather network logs on all nodes in a cluster.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Run the <code class="literal">oc adm must-gather</code> command with <code class="literal">-- gather_network_logs</code>:
						</p><pre class="programlisting language-terminal">$ oc adm must-gather -- gather_network_logs</pre></li><li class="listitem"><p class="simpara">
							Create a compressed file from the <code class="literal">must-gather</code> directory that was just created in your working directory. For example, on a computer that uses a Linux operating system, run the following command:
						</p><pre class="programlisting language-terminal">$ tar cvaf must-gather.tar.gz must-gather.local.472290403699006248 <span id="CO12-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO12-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal">must-gather-local.472290403699006248</code> with the actual directory name.
								</div></dd></dl></div></li><li class="listitem">
							Attach the compressed file to your support case on the <a class="link" href="https://access.redhat.com">Red Hat Customer Portal</a>.
						</li></ol></div></section></section><section class="section" id="support-get-cluster-id_gathering-cluster-data"><div class="titlepage"><div><div><h2 class="title">5.3. Obtaining your cluster ID</h2></div></div></div><p>
				When providing information to Red Hat Support, it is helpful to provide the unique identifier for your cluster. You can have your cluster ID autofilled by using the OpenShift Container Platform web console. You can also manually obtain your cluster ID by using the web console or the OpenShift CLI (<code class="literal">oc</code>).
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li><li class="listitem">
						Access to the web console or the OpenShift CLI (<code class="literal">oc</code>) installed.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						To open a support case and have your cluster ID autofilled using the web console:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								From the toolbar, navigate to <span class="strong strong"><strong>(?) Help</strong></span> → <span class="strong strong"><strong>Open Support Case</strong></span>.
							</li><li class="listitem">
								The <span class="strong strong"><strong>Cluster ID</strong></span> value is autofilled.
							</li></ol></div></li><li class="listitem"><p class="simpara">
						To manually obtain your cluster ID using the web console:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Navigate to <span class="strong strong"><strong>Home</strong></span> → <span class="strong strong"><strong>Dashboards</strong></span> → <span class="strong strong"><strong>Overview</strong></span>.
							</li><li class="listitem">
								The value is available in the <span class="strong strong"><strong>Cluster ID</strong></span> field of the <span class="strong strong"><strong>Details</strong></span> section.
							</li></ol></div></li><li class="listitem"><p class="simpara">
						To obtain your cluster ID using the OpenShift CLI (<code class="literal">oc</code>), run the following command:
					</p><pre class="programlisting language-terminal">$ oc get clusterversion -o jsonpath='{.items[].spec.clusterID}{"\n"}'</pre></li></ul></div></section><section class="section" id="about-sosreport_gathering-cluster-data"><div class="titlepage"><div><div><h2 class="title">5.4. About sosreport</h2></div></div></div><p>
				<code class="literal">sosreport</code> is a tool that collects configuration details, system information, and diagnostic data from Red Hat Enterprise Linux (RHEL) and Red Hat Enterprise Linux CoreOS (RHCOS) systems. <code class="literal">sosreport</code> provides a standardized way to collect diagnostic information relating to a node, which can then be provided to Red Hat Support for issue diagnosis.
			</p><p>
				In some support interactions, Red Hat Support may ask you to collect a <code class="literal">sosreport</code> archive for a specific OpenShift Container Platform node. For example, it might sometimes be necessary to review system logs or other node-specific data that is not included within the output of <code class="literal">oc adm must-gather</code>.
			</p></section><section class="section" id="support-generating-a-sosreport-archive_gathering-cluster-data"><div class="titlepage"><div><div><h2 class="title">5.5. Generating a sosreport archive for an OpenShift Container Platform cluster node</h2></div></div></div><p>
				The recommended way to generate a <code class="literal">sosreport</code> for an OpenShift Container Platform 4.13 cluster node is through a debug pod.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li><li class="listitem">
						You have SSH access to your hosts.
					</li><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You have a Red Hat standard or premium Subscription.
					</li><li class="listitem">
						You have a Red Hat Customer Portal account.
					</li><li class="listitem">
						You have an existing Red Hat Support case ID.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Obtain a list of cluster nodes:
					</p><pre class="programlisting language-terminal">$ oc get nodes</pre></li><li class="listitem"><p class="simpara">
						Enter into a debug session on the target node. This step instantiates a debug pod called <code class="literal">&lt;node_name&gt;-debug</code>:
					</p><pre class="programlisting language-terminal">$ oc debug node/my-cluster-node</pre><p class="simpara">
						To enter into a debug session on the target node that is tainted with the <code class="literal">NoExecute</code> effect, add a toleration to a dummy namespace, and start the debug pod in the dummy namespace:
					</p><pre class="programlisting language-terminal">$ oc new-project dummy</pre><pre class="programlisting language-terminal">$ oc patch namespace dummy --type=merge -p '{"metadata": {"annotations": { "scheduler.alpha.kubernetes.io/defaultTolerations": "[{\"operator\": \"Exists\"}]"}}}'</pre><pre class="programlisting language-terminal">$ oc debug node/my-cluster-node</pre></li><li class="listitem"><p class="simpara">
						Set <code class="literal">/host</code> as the root directory within the debug shell. The debug pod mounts the host’s root file system in <code class="literal">/host</code> within the pod. By changing the root directory to <code class="literal">/host</code>, you can run binaries contained in the host’s executable paths:
					</p><pre class="programlisting language-terminal"># chroot /host</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							OpenShift Container Platform 4.13 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes by using SSH is not recommended. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code class="literal">oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code class="literal">ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> instead.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Start a <code class="literal">toolbox</code> container, which includes the required binaries and plugins to run <code class="literal">sosreport</code>:
					</p><pre class="programlisting language-terminal"># toolbox</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If an existing <code class="literal">toolbox</code> pod is already running, the <code class="literal">toolbox</code> command outputs <code class="literal">'toolbox-' already exists. Trying to start…​</code>. Remove the running toolbox container with <code class="literal">podman rm toolbox-</code> and spawn a new toolbox container, to avoid issues with <code class="literal">sosreport</code> plugins.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Collect a <code class="literal">sosreport</code> archive.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Run the <code class="literal">sosreport</code> command and enable the <code class="literal">crio.all</code> and <code class="literal">crio.logs</code> CRI-O container engine <code class="literal">sosreport</code> plugins:
							</p><pre class="programlisting language-terminal"># sosreport -k crio.all=on -k crio.logs=on <span id="CO13-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO13-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										<code class="literal">-k</code> enables you to define <code class="literal">sosreport</code> plugin parameters outside of the defaults.
									</div></dd></dl></div></li><li class="listitem">
								Press <span class="strong strong"><strong>Enter</strong></span> when prompted, to continue.
							</li><li class="listitem">
								Provide the Red Hat Support case ID. <code class="literal">sosreport</code> adds the ID to the archive’s file name.
							</li><li class="listitem"><p class="simpara">
								The <code class="literal">sosreport</code> output provides the archive’s location and checksum. The following sample output references support case ID <code class="literal">01234567</code>:
							</p><pre class="programlisting language-terminal">Your sosreport has been generated and saved in:
  /host/var/tmp/sosreport-my-cluster-node-01234567-2020-05-28-eyjknxt.tar.xz <span id="CO14-1"><!--Empty--></span><span class="callout">1</span>

The checksum is: 382ffc167510fd71b4f12a4f40b97a4e</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO14-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The <code class="literal">sosreport</code> archive’s file path is outside of the <code class="literal">chroot</code> environment because the toolbox container mounts the host’s root directory at <code class="literal">/host</code>.
									</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
						Provide the <code class="literal">sosreport</code> archive to Red Hat Support for analysis, using one of the following methods.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Upload the file to an existing Red Hat support case directly from an OpenShift Container Platform cluster.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										From within the toolbox container, run <code class="literal">redhat-support-tool</code> to attach the archive directly to an existing Red Hat support case. This example uses support case ID <code class="literal">01234567</code>:
									</p><pre class="programlisting language-terminal"># redhat-support-tool addattachment -c 01234567 /host/var/tmp/my-sosreport.tar.xz <span id="CO15-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO15-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												The toolbox container mounts the host’s root directory at <code class="literal">/host</code>. Reference the absolute path from the toolbox container’s root directory, including <code class="literal">/host/</code>, when specifying files to upload through the <code class="literal">redhat-support-tool</code> command.
											</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
								Upload the file to an existing Red Hat support case.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Concatenate the <code class="literal">sosreport</code> archive by running the <code class="literal">oc debug node/&lt;node_name&gt;</code> command and redirect the output to a file. This command assumes you have exited the previous <code class="literal">oc debug</code> session:
									</p><pre class="programlisting language-terminal">$ oc debug node/my-cluster-node -- bash -c 'cat /host/var/tmp/sosreport-my-cluster-node-01234567-2020-05-28-eyjknxt.tar.xz' &gt; /tmp/sosreport-my-cluster-node-01234567-2020-05-28-eyjknxt.tar.xz <span id="CO16-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO16-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												The debug container mounts the host’s root directory at <code class="literal">/host</code>. Reference the absolute path from the debug container’s root directory, including <code class="literal">/host</code>, when specifying target files for concatenation.
											</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											OpenShift Container Platform 4.13 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Transferring a <code class="literal">sosreport</code> archive from a cluster node by using <code class="literal">scp</code> is not recommended. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code class="literal">oc</code> operations will be impacted. In such situations, it is possible to copy a <code class="literal">sosreport</code> archive from a node by running <code class="literal">scp core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;:&lt;file_path&gt; &lt;local_path&gt;</code>.
										</p></div></div></li><li class="listitem">
										Navigate to an existing support case within <a class="link" href="https://access.redhat.com/support/cases/">https://access.redhat.com/support/cases/</a>.
									</li><li class="listitem">
										Select <span class="strong strong"><strong>Attach files</strong></span> and follow the prompts to upload the file.
									</li></ol></div></li></ul></div></li></ol></div></section><section class="section" id="querying-bootstrap-node-journal-logs_gathering-cluster-data"><div class="titlepage"><div><div><h2 class="title">5.6. Querying bootstrap node journal logs</h2></div></div></div><p>
				If you experience bootstrap-related issues, you can gather <code class="literal">bootkube.service</code> <code class="literal">journald</code> unit logs and container logs from the bootstrap node.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have SSH access to your bootstrap node.
					</li><li class="listitem">
						You have the fully qualified domain name of the bootstrap node.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Query <code class="literal">bootkube.service</code> <code class="literal">journald</code> unit logs from a bootstrap node during OpenShift Container Platform installation. Replace <code class="literal">&lt;bootstrap_fqdn&gt;</code> with the bootstrap node’s fully qualified domain name:
					</p><pre class="programlisting language-terminal">$ ssh core@&lt;bootstrap_fqdn&gt; journalctl -b -f -u bootkube.service</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The <code class="literal">bootkube.service</code> log on the bootstrap node outputs etcd <code class="literal">connection refused</code> errors, indicating that the bootstrap server is unable to connect to etcd on control plane nodes. After etcd has started on each control plane node and the nodes have joined the cluster, the errors should stop.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Collect logs from the bootstrap node containers using <code class="literal">podman</code> on the bootstrap node. Replace <code class="literal">&lt;bootstrap_fqdn&gt;</code> with the bootstrap node’s fully qualified domain name:
					</p><pre class="programlisting language-terminal">$ ssh core@&lt;bootstrap_fqdn&gt; 'for pod in $(sudo podman ps -a -q); do sudo podman logs $pod; done'</pre></li></ol></div></section><section class="section" id="querying-cluster-node-journal-logs_gathering-cluster-data"><div class="titlepage"><div><div><h2 class="title">5.7. Querying cluster node journal logs</h2></div></div></div><p>
				You can gather <code class="literal">journald</code> unit logs and other logs within <code class="literal">/var/log</code> on individual cluster nodes.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li><li class="listitem">
						Your API service is still functional.
					</li><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You have SSH access to your hosts.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Query <code class="literal">kubelet</code> <code class="literal">journald</code> unit logs from OpenShift Container Platform cluster nodes. The following example queries control plane nodes only:
					</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=master -u kubelet  <span id="CO17-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO17-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Replace <code class="literal">kubelet</code> as appropriate to query other unit logs.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Collect logs from specific subdirectories under <code class="literal">/var/log/</code> on cluster nodes.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Retrieve a list of logs contained within a <code class="literal">/var/log/</code> subdirectory. The following example lists files in <code class="literal">/var/log/openshift-apiserver/</code> on all control plane nodes:
							</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=master --path=openshift-apiserver</pre></li><li class="listitem"><p class="simpara">
								Inspect a specific log within a <code class="literal">/var/log/</code> subdirectory. The following example outputs <code class="literal">/var/log/openshift-apiserver/audit.log</code> contents from all control plane nodes:
							</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=master --path=openshift-apiserver/audit.log</pre></li><li class="listitem"><p class="simpara">
								If the API is not functional, review the logs on each node using SSH instead. The following example tails <code class="literal">/var/log/openshift-apiserver/audit.log</code>:
							</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; sudo tail -f /var/log/openshift-apiserver/audit.log</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									OpenShift Container Platform 4.13 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes by using SSH is not recommended. Before attempting to collect diagnostic data over SSH, review whether the data collected by running <code class="literal">oc adm must gather</code> and other <code class="literal">oc</code> commands is sufficient instead. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code class="literal">oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code class="literal">ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>.
								</p></div></div></li></ol></div></li></ol></div></section><section class="section" id="support-network-trace-methods_gathering-cluster-data"><div class="titlepage"><div><div><h2 class="title">5.8. Network trace methods</h2></div></div></div><p>
				Collecting network traces, in the form of packet capture records, can assist Red Hat Support with troubleshooting network issues.
			</p><p>
				OpenShift Container Platform supports two ways of performing a network trace. Review the following table and choose the method that meets your needs.
			</p><div class="table" id="idm140604668820352"><p class="title"><strong>Table 5.2. Supported methods of collecting a network trace</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140604663936224" scope="col">Method</th><th align="left" valign="top" id="idm140604663935136" scope="col">Benefits and capabilities</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140604663936224"> <p>
								Collecting a host network trace
							</p>
							 </td><td align="left" valign="top" headers="idm140604663935136"> <p>
								You perform a packet capture for a duration that you specify on one or more nodes at the same time. The packet capture files are transferred from nodes to the client machine when the specified duration is met.
							</p>
							 <p>
								You can troubleshoot why a specific action triggers network communication issues. Run the packet capture, perform the action that triggers the issue, and use the logs to diagnose the issue.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140604663936224"> <p>
								Collecting a network trace from an OpenShift Container Platform node or container
							</p>
							 </td><td align="left" valign="top" headers="idm140604663935136"> <p>
								You perform a packet capture on one node or one container. You run the <code class="literal">tcpdump</code> command interactively, so you can control the duration of the packet capture.
							</p>
							 <p>
								You can start the packet capture manually, trigger the network communication issue, and then stop the packet capture manually.
							</p>
							 <p>
								This method uses the <code class="literal">cat</code> command and shell redirection to copy the packet capture data from the node or container to the client machine.
							</p>
							 </td></tr></tbody></table></div></div></section><section class="section" id="support-collecting-host-network-trace_gathering-cluster-data"><div class="titlepage"><div><div><h2 class="title">5.9. Collecting a host network trace</h2></div></div></div><p>
				Sometimes, troubleshooting a network-related issue is simplified by tracing network communication and capturing packets on multiple nodes at the same time.
			</p><p>
				You can use a combination of the <code class="literal">oc adm must-gather</code> command and the <code class="literal">registry.redhat.io/openshift4/network-tools-rhel8</code> container image to gather packet captures from nodes. Analyzing packet captures can help you troubleshoot network communication issues.
			</p><p>
				The <code class="literal">oc adm must-gather</code> command is used to run the <code class="literal">tcpdump</code> command in pods on specific nodes. The <code class="literal">tcpdump</code> command records the packet captures in the pods. When the <code class="literal">tcpdump</code> command exits, the <code class="literal">oc adm must-gather</code> command transfers the files with the packet captures from the pods to your client machine.
			</p><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
				The sample command in the following procedure demonstrates performing a packet capture with the <code class="literal">tcpdump</code> command. However, you can run any command in the container image that is specified in the <code class="literal">--image</code> argument to gather troubleshooting information from multiple nodes at the same time.
			</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Run a packet capture from the host network on some nodes by running the following command:
					</p><pre class="programlisting language-terminal">$ oc adm must-gather \
    --dest-dir /tmp/captures \  &lt;.&gt;
    --source-dir '/tmp/tcpdump/' \  &lt;.&gt;
    --image registry.redhat.io/openshift4/network-tools-rhel8:latest \  &lt;.&gt;
    --node-selector 'node-role.kubernetes.io/worker' \  &lt;.&gt;
    --host-network=true \  &lt;.&gt;
    --timeout 30s \  &lt;.&gt;
    -- \
    tcpdump -i any \  &lt;.&gt;
    -w /tmp/tcpdump/%Y-%m-%dT%H:%M:%S.pcap -W 1 -G 300</pre><p class="simpara">
						&lt;.&gt; The <code class="literal">--dest-dir</code> argument specifies that <code class="literal">oc adm must-gather</code> stores the packet captures in directories that are relative to <code class="literal">/tmp/captures</code> on the client machine. You can specify any writable directory. &lt;.&gt; When <code class="literal">tcpdump</code> is run in the debug pod that <code class="literal">oc adm must-gather</code> starts, the <code class="literal">--source-dir</code> argument specifies that the packet captures are temporarily stored in the <code class="literal">/tmp/tcpdump</code> directory on the pod. &lt;.&gt; The <code class="literal">--image</code> argument specifies a container image that includes the <code class="literal">tcpdump</code> command. &lt;.&gt; The <code class="literal">--node-selector</code> argument and example value specifies to perform the packet captures on the worker nodes. As an alternative, you can specify the <code class="literal">--node-name</code> argument instead to run the packet capture on a single node. If you omit both the <code class="literal">--node-selector</code> and the <code class="literal">--node-name</code> argument, the packet captures are performed on all nodes. &lt;.&gt; The <code class="literal">--host-network=true</code> argument is required so that the packet captures are performed on the network interfaces of the node. &lt;.&gt; The <code class="literal">--timeout</code> argument and value specify to run the debug pod for 30 seconds. If you do not specify the <code class="literal">--timeout</code> argument and a duration, the debug pod runs for 10 minutes. &lt;.&gt; The <code class="literal">-i any</code> argument for the <code class="literal">tcpdump</code> command specifies to capture packets on all network interfaces. As an alternative, you can specify a network interface name.
					</p></li><li class="listitem">
						Perform the action, such as accessing a web application, that triggers the network communication issue while the network trace captures packets.
					</li><li class="listitem"><p class="simpara">
						Review the packet capture files that <code class="literal">oc adm must-gather</code> transferred from the pods to your client machine:
					</p><pre class="programlisting language-text">tmp/captures
├── event-filter.html
├── ip-10-0-192-217-ec2-internal  <span id="CO18-1"><!--Empty--></span><span class="callout">1</span>
│   └── registry-redhat-io-openshift4-network-tools-rhel8-sha256-bca...
│       └── 2022-01-13T19:31:31.pcap
├── ip-10-0-201-178-ec2-internal  <span id="CO18-2"><!--Empty--></span><span class="callout">2</span>
│   └── registry-redhat-io-openshift4-network-tools-rhel8-sha256-bca...
│       └── 2022-01-13T19:31:30.pcap
├── ip-...
└── timestamp</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO18-1"><span class="callout">1</span></a> <a href="#CO18-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The packet captures are stored in directories that identify the hostname, container, and file name. If you did not specify the <code class="literal">--node-selector</code> argument, then the directory level for the hostname is not present.
							</div></dd></dl></div></li></ol></div></section><section class="section" id="support-collecting-network-trace_gathering-cluster-data"><div class="titlepage"><div><div><h2 class="title">5.10. Collecting a network trace from an OpenShift Container Platform node or container</h2></div></div></div><p>
				When investigating potential network-related OpenShift Container Platform issues, Red Hat Support might request a network packet trace from a specific OpenShift Container Platform cluster node or from a specific container. The recommended method to capture a network trace in OpenShift Container Platform is through a debug pod.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You have a Red Hat standard or premium Subscription.
					</li><li class="listitem">
						You have a Red Hat Customer Portal account.
					</li><li class="listitem">
						You have an existing Red Hat Support case ID.
					</li><li class="listitem">
						You have SSH access to your hosts.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Obtain a list of cluster nodes:
					</p><pre class="programlisting language-terminal">$ oc get nodes</pre></li><li class="listitem"><p class="simpara">
						Enter into a debug session on the target node. This step instantiates a debug pod called <code class="literal">&lt;node_name&gt;-debug</code>:
					</p><pre class="programlisting language-terminal">$ oc debug node/my-cluster-node</pre></li><li class="listitem"><p class="simpara">
						Set <code class="literal">/host</code> as the root directory within the debug shell. The debug pod mounts the host’s root file system in <code class="literal">/host</code> within the pod. By changing the root directory to <code class="literal">/host</code>, you can run binaries contained in the host’s executable paths:
					</p><pre class="programlisting language-terminal"># chroot /host</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							OpenShift Container Platform 4.13 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes by using SSH is not recommended. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code class="literal">oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code class="literal">ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> instead.
						</p></div></div></li><li class="listitem"><p class="simpara">
						From within the <code class="literal">chroot</code> environment console, obtain the node’s interface names:
					</p><pre class="programlisting language-terminal"># ip ad</pre></li><li class="listitem"><p class="simpara">
						Start a <code class="literal">toolbox</code> container, which includes the required binaries and plugins to run <code class="literal">sosreport</code>:
					</p><pre class="programlisting language-terminal"># toolbox</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If an existing <code class="literal">toolbox</code> pod is already running, the <code class="literal">toolbox</code> command outputs <code class="literal">'toolbox-' already exists. Trying to start…​</code>. To avoid <code class="literal">tcpdump</code> issues, remove the running toolbox container with <code class="literal">podman rm toolbox-</code> and spawn a new toolbox container.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Initiate a <code class="literal">tcpdump</code> session on the cluster node and redirect output to a capture file. This example uses <code class="literal">ens5</code> as the interface name:
					</p><pre class="programlisting language-terminal">$ tcpdump -nn -s 0 -i ens5 -w /host/var/tmp/my-cluster-node_$(date +%d_%m_%Y-%H_%M_%S-%Z).pcap  <span id="CO19-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO19-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The <code class="literal">tcpdump</code> capture file’s path is outside of the <code class="literal">chroot</code> environment because the toolbox container mounts the host’s root directory at <code class="literal">/host</code>.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						If a <code class="literal">tcpdump</code> capture is required for a specific container on the node, follow these steps.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Determine the target container ID. The <code class="literal">chroot host</code> command precedes the <code class="literal">crictl</code> command in this step because the toolbox container mounts the host’s root directory at <code class="literal">/host</code>:
							</p><pre class="programlisting language-terminal"># chroot /host crictl ps</pre></li><li class="listitem"><p class="simpara">
								Determine the container’s process ID. In this example, the container ID is <code class="literal">a7fe32346b120</code>:
							</p><pre class="programlisting language-terminal"># chroot /host crictl inspect --output yaml a7fe32346b120 | grep 'pid' | awk '{print $2}'</pre></li><li class="listitem"><p class="simpara">
								Initiate a <code class="literal">tcpdump</code> session on the container and redirect output to a capture file. This example uses <code class="literal">49628</code> as the container’s process ID and <code class="literal">ens5</code> as the interface name. The <code class="literal">nsenter</code> command enters the namespace of a target process and runs a command in its namespace. because the target process in this example is a container’s process ID, the <code class="literal">tcpdump</code> command is run in the container’s namespace from the host:
							</p><pre class="programlisting language-terminal"># nsenter -n -t 49628 -- tcpdump -nn -i ens5 -w /host/var/tmp/my-cluster-node-my-container_$(date +%d_%m_%Y-%H_%M_%S-%Z).pcap.pcap  <span id="CO20-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO20-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The <code class="literal">tcpdump</code> capture file’s path is outside of the <code class="literal">chroot</code> environment because the toolbox container mounts the host’s root directory at <code class="literal">/host</code>.
									</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
						Provide the <code class="literal">tcpdump</code> capture file to Red Hat Support for analysis, using one of the following methods.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Upload the file to an existing Red Hat support case directly from an OpenShift Container Platform cluster.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										From within the toolbox container, run <code class="literal">redhat-support-tool</code> to attach the file directly to an existing Red Hat Support case. This example uses support case ID <code class="literal">01234567</code>:
									</p><pre class="programlisting language-terminal"># redhat-support-tool addattachment -c 01234567 /host/var/tmp/my-tcpdump-capture-file.pcap <span id="CO21-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO21-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												The toolbox container mounts the host’s root directory at <code class="literal">/host</code>. Reference the absolute path from the toolbox container’s root directory, including <code class="literal">/host/</code>, when specifying files to upload through the <code class="literal">redhat-support-tool</code> command.
											</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
								Upload the file to an existing Red Hat support case.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Concatenate the <code class="literal">sosreport</code> archive by running the <code class="literal">oc debug node/&lt;node_name&gt;</code> command and redirect the output to a file. This command assumes you have exited the previous <code class="literal">oc debug</code> session:
									</p><pre class="programlisting language-terminal">$ oc debug node/my-cluster-node -- bash -c 'cat /host/var/tmp/my-tcpdump-capture-file.pcap' &gt; /tmp/my-tcpdump-capture-file.pcap <span id="CO22-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO22-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												The debug container mounts the host’s root directory at <code class="literal">/host</code>. Reference the absolute path from the debug container’s root directory, including <code class="literal">/host</code>, when specifying target files for concatenation.
											</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											OpenShift Container Platform 4.13 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Transferring a <code class="literal">tcpdump</code> capture file from a cluster node by using <code class="literal">scp</code> is not recommended. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code class="literal">oc</code> operations will be impacted. In such situations, it is possible to copy a <code class="literal">tcpdump</code> capture file from a node by running <code class="literal">scp core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;:&lt;file_path&gt; &lt;local_path&gt;</code>.
										</p></div></div></li><li class="listitem">
										Navigate to an existing support case within <a class="link" href="https://access.redhat.com/support/cases/">https://access.redhat.com/support/cases/</a>.
									</li><li class="listitem">
										Select <span class="strong strong"><strong>Attach files</strong></span> and follow the prompts to upload the file.
									</li></ol></div></li></ul></div></li></ol></div></section><section class="section" id="support-providing-diagnostic-data-to-red-hat_gathering-cluster-data"><div class="titlepage"><div><div><h2 class="title">5.11. Providing diagnostic data to Red Hat Support</h2></div></div></div><p>
				When investigating OpenShift Container Platform issues, Red Hat Support might ask you to upload diagnostic data to a support case. Files can be uploaded to a support case through the Red Hat Customer Portal, or from an OpenShift Container Platform cluster directly by using the <code class="literal">redhat-support-tool</code> command.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li><li class="listitem">
						You have SSH access to your hosts.
					</li><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You have a Red Hat standard or premium Subscription.
					</li><li class="listitem">
						You have a Red Hat Customer Portal account.
					</li><li class="listitem">
						You have an existing Red Hat Support case ID.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Upload diagnostic data to an existing Red Hat support case through the Red Hat Customer Portal.
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Concatenate a diagnostic file contained on an OpenShift Container Platform node by using the <code class="literal">oc debug node/&lt;node_name&gt;</code> command and redirect the output to a file. The following example copies <code class="literal">/host/var/tmp/my-diagnostic-data.tar.gz</code> from a debug container to <code class="literal">/var/tmp/my-diagnostic-data.tar.gz</code>:
							</p><pre class="programlisting language-terminal">$ oc debug node/my-cluster-node -- bash -c 'cat /host/var/tmp/my-diagnostic-data.tar.gz' &gt; /var/tmp/my-diagnostic-data.tar.gz <span id="CO23-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO23-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The debug container mounts the host’s root directory at <code class="literal">/host</code>. Reference the absolute path from the debug container’s root directory, including <code class="literal">/host</code>, when specifying target files for concatenation.
									</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									OpenShift Container Platform 4.13 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Transferring files from a cluster node by using <code class="literal">scp</code> is not recommended. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code class="literal">oc</code> operations will be impacted. In such situations, it is possible to copy diagnostic files from a node by running <code class="literal">scp core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;:&lt;file_path&gt; &lt;local_path&gt;</code>.
								</p></div></div></li><li class="listitem">
								Navigate to an existing support case within <a class="link" href="https://access.redhat.com/support/cases/">https://access.redhat.com/support/cases/</a>.
							</li><li class="listitem">
								Select <span class="strong strong"><strong>Attach files</strong></span> and follow the prompts to upload the file.
							</li></ol></div></li><li class="listitem"><p class="simpara">
						Upload diagnostic data to an existing Red Hat support case directly from an OpenShift Container Platform cluster.
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Obtain a list of cluster nodes:
							</p><pre class="programlisting language-terminal">$ oc get nodes</pre></li><li class="listitem"><p class="simpara">
								Enter into a debug session on the target node. This step instantiates a debug pod called <code class="literal">&lt;node_name&gt;-debug</code>:
							</p><pre class="programlisting language-terminal">$ oc debug node/my-cluster-node</pre></li><li class="listitem"><p class="simpara">
								Set <code class="literal">/host</code> as the root directory within the debug shell. The debug pod mounts the host’s root file system in <code class="literal">/host</code> within the pod. By changing the root directory to <code class="literal">/host</code>, you can run binaries contained in the host’s executable paths:
							</p><pre class="programlisting language-terminal"># chroot /host</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									OpenShift Container Platform 4.13 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes by using SSH is not recommended. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code class="literal">oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code class="literal">ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> instead.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Start a <code class="literal">toolbox</code> container, which includes the required binaries to run <code class="literal">redhat-support-tool</code>:
							</p><pre class="programlisting language-terminal"># toolbox</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									If an existing <code class="literal">toolbox</code> pod is already running, the <code class="literal">toolbox</code> command outputs <code class="literal">'toolbox-' already exists. Trying to start…​</code>. Remove the running toolbox container with <code class="literal">podman rm toolbox-</code> and spawn a new toolbox container, to avoid issues.
								</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Run <code class="literal">redhat-support-tool</code> to attach a file from the debug pod directly to an existing Red Hat Support case. This example uses support case ID '01234567' and example file path <code class="literal">/host/var/tmp/my-diagnostic-data.tar.gz</code>:
									</p><pre class="programlisting language-terminal"># redhat-support-tool addattachment -c 01234567 /host/var/tmp/my-diagnostic-data.tar.gz <span id="CO24-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO24-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												The toolbox container mounts the host’s root directory at <code class="literal">/host</code>. Reference the absolute path from the toolbox container’s root directory, including <code class="literal">/host/</code>, when specifying files to upload through the <code class="literal">redhat-support-tool</code> command.
											</div></dd></dl></div></li></ol></div></li></ol></div></li></ul></div></section><section class="section" id="about-toolbox_gathering-cluster-data"><div class="titlepage"><div><div><h2 class="title">5.12. About <code class="literal">toolbox</code></h2></div></div></div><p>
				<code class="literal">toolbox</code> is a tool that starts a container on a Red Hat Enterprise Linux CoreOS (RHCOS) system. The tool is primarily used to start a container that includes the required binaries and plugins that are needed to run commands such as <code class="literal">sosreport</code> and <code class="literal">redhat-support-tool</code>.
			</p><p>
				The primary purpose for a <code class="literal">toolbox</code> container is to gather diagnostic information and to provide it to Red Hat Support. However, if additional diagnostic tools are required, you can add RPM packages or run an image that is an alternative to the standard support tools image.
			</p><h4 id="installing-packages-to-a-toolbox-container_gathering-cluster-data">Installing packages to a <code class="literal">toolbox</code> container</h4><p>
				By default, running the <code class="literal">toolbox</code> command starts a container with the <code class="literal">registry.redhat.io/rhel8/support-tools:latest</code> image. This image contains the most frequently used support tools. If you need to collect node-specific data that requires a support tool that is not part of the image, you can install additional packages.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have accessed a node with the <code class="literal">oc debug node/&lt;node_name&gt;</code> command.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Set <code class="literal">/host</code> as the root directory within the debug shell. The debug pod mounts the host’s root file system in <code class="literal">/host</code> within the pod. By changing the root directory to <code class="literal">/host</code>, you can run binaries contained in the host’s executable paths:
					</p><pre class="programlisting language-terminal"># chroot /host</pre></li><li class="listitem"><p class="simpara">
						Start the toolbox container:
					</p><pre class="programlisting language-terminal"># toolbox</pre></li><li class="listitem"><p class="simpara">
						Install the additional package, such as <code class="literal">wget</code>:
					</p><pre class="programlisting language-terminal"># dnf install -y &lt;package_name&gt;</pre></li></ol></div><h4 id="starting-an-alternative-image-with-toolbox_gathering-cluster-data">Starting an alternative image with <code class="literal">toolbox</code></h4><p>
				By default, running the <code class="literal">toolbox</code> command starts a container with the <code class="literal">registry.redhat.io/rhel8/support-tools:latest</code> image. You can start an alternative image by creating a <code class="literal">.toolboxrc</code> file and specifying the image to run.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have accessed a node with the <code class="literal">oc debug node/&lt;node_name&gt;</code> command.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Set <code class="literal">/host</code> as the root directory within the debug shell. The debug pod mounts the host’s root file system in <code class="literal">/host</code> within the pod. By changing the root directory to <code class="literal">/host</code>, you can run binaries contained in the host’s executable paths:
					</p><pre class="programlisting language-terminal"># chroot /host</pre></li><li class="listitem"><p class="simpara">
						Create a <code class="literal">.toolboxrc</code> file in the home directory for the root user ID:
					</p><pre class="programlisting language-terminal"># vi ~/.toolboxrc</pre><pre class="programlisting language-text">REGISTRY=quay.io                <span id="CO25-1"><!--Empty--></span><span class="callout">1</span>
IMAGE=fedora/fedora:33-x86_64   <span id="CO25-2"><!--Empty--></span><span class="callout">2</span>
TOOLBOX_NAME=toolbox-fedora-33  <span id="CO25-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO25-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Optional: Specify an alternative container registry.
							</div></dd><dt><a href="#CO25-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specify an alternative image to start.
							</div></dd><dt><a href="#CO25-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Optional: Specify an alternative name for the toolbox container.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Start a toolbox container with the alternative image:
					</p><pre class="programlisting language-terminal"># toolbox</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If an existing <code class="literal">toolbox</code> pod is already running, the <code class="literal">toolbox</code> command outputs <code class="literal">'toolbox-' already exists. Trying to start…​</code>. Remove the running toolbox container with <code class="literal">podman rm toolbox-</code> and spawn a new toolbox container, to avoid issues with <code class="literal">sosreport</code> plugins.
						</p></div></div></li></ol></div></section></section><section class="chapter" id="summarizing-cluster-specifications"><div class="titlepage"><div><div><h1 class="title">Chapter 6. Summarizing cluster specifications</h1></div></div></div><section class="section" id="summarizing-cluster-specifications-through-clusterversion_summarizing-cluster-specifications"><div class="titlepage"><div><div><h2 class="title">6.1. Summarizing cluster specifications through <code class="literal">clusterversion</code></h2></div></div></div><p>
				You can obtain a summary of OpenShift Container Platform cluster specifications by querying the <code class="literal">clusterversion</code> resource.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Query cluster version, availability, uptime, and general status:
					</p><pre class="programlisting language-terminal">$ oc get clusterversion</pre></li><li class="listitem"><p class="simpara">
						Obtain a detailed summary of cluster specifications, update availability, and update history:
					</p><pre class="programlisting language-terminal">$ oc describe clusterversion</pre></li></ol></div></section></section><section class="chapter" id="troubleshooting"><div class="titlepage"><div><div><h1 class="title">Chapter 7. Troubleshooting</h1></div></div></div><section class="section" id="troubleshooting-installations"><div class="titlepage"><div><div><h2 class="title">7.1. Troubleshooting installations</h2></div></div></div><section class="section" id="determining-where-installation-issues-occur_troubleshooting-installations"><div class="titlepage"><div><div><h3 class="title">7.1.1. Determining where installation issues occur</h3></div></div></div><p>
					When troubleshooting OpenShift Container Platform installation issues, you can monitor installation logs to determine at which stage issues occur. Then, retrieve diagnostic data relevant to that stage.
				</p><p>
					OpenShift Container Platform installation proceeds through the following stages:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Ignition configuration files are created.
						</li><li class="listitem">
							The bootstrap machine boots and starts hosting the remote resources required for the control plane machines to boot.
						</li><li class="listitem">
							The control plane machines fetch the remote resources from the bootstrap machine and finish booting.
						</li><li class="listitem">
							The control plane machines use the bootstrap machine to form an etcd cluster.
						</li><li class="listitem">
							The bootstrap machine starts a temporary Kubernetes control plane using the new etcd cluster.
						</li><li class="listitem">
							The temporary control plane schedules the production control plane to the control plane machines.
						</li><li class="listitem">
							The temporary control plane shuts down and passes control to the production control plane.
						</li><li class="listitem">
							The bootstrap machine adds OpenShift Container Platform components into the production control plane.
						</li><li class="listitem">
							The installation program shuts down the bootstrap machine.
						</li><li class="listitem">
							The control plane sets up the worker nodes.
						</li><li class="listitem">
							The control plane installs additional services in the form of a set of Operators.
						</li><li class="listitem">
							The cluster downloads and configures remaining components needed for the day-to-day operation, including the creation of worker machines in supported environments.
						</li></ol></div></section><section class="section" id="upi-installation-considerations_troubleshooting-installations"><div class="titlepage"><div><div><h3 class="title">7.1.2. User-provisioned infrastructure installation considerations</h3></div></div></div><p>
					The default installation method uses installer-provisioned infrastructure. With installer-provisioned infrastructure clusters, OpenShift Container Platform manages all aspects of the cluster, including the operating system itself. If possible, use this feature to avoid having to provision and maintain the cluster infrastructure.
				</p><p>
					You can alternatively install OpenShift Container Platform 4.13 on infrastructure that you provide. If you use this installation method, follow user-provisioned infrastructure installation documentation carefully. Additionally, review the following considerations before the installation:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Check the <a class="link" href="https://access.redhat.com/ecosystem/search/#/ecosystem/Red%20Hat%20Enterprise%20Linux">Red Hat Enterprise Linux (RHEL) Ecosystem</a> to determine the level of Red Hat Enterprise Linux CoreOS (RHCOS) support provided for your chosen server hardware or virtualization technology.
						</li><li class="listitem">
							Many virtualization and cloud environments require agents to be installed on guest operating systems. Ensure that these agents are installed as a containerized workload deployed through a daemon set.
						</li><li class="listitem"><p class="simpara">
							Install cloud provider integration if you want to enable features such as dynamic storage, on-demand service routing, node hostname to Kubernetes hostname resolution, and cluster autoscaling.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								It is not possible to enable cloud provider integration in OpenShift Container Platform environments that mix resources from different cloud providers, or that span multiple physical or virtual platforms. The node life cycle controller will not allow nodes that are external to the existing provider to be added to a cluster, and it is not possible to specify more than one cloud provider integration.
							</p></div></div></li><li class="listitem">
							A provider-specific Machine API implementation is required if you want to use machine sets or autoscaling to automatically provision OpenShift Container Platform cluster nodes.
						</li><li class="listitem">
							Check whether your chosen cloud provider offers a method to inject Ignition configuration files into hosts as part of their initial deployment. If they do not, you will need to host Ignition configuration files by using an HTTP server. The steps taken to troubleshoot Ignition configuration file issues will differ depending on which of these two methods is deployed.
						</li><li class="listitem">
							Storage needs to be manually provisioned if you want to leverage optional framework components such as the embedded container registry, Elasticsearch, or Prometheus. Default storage classes are not defined in user-provisioned infrastructure installations unless explicitly configured.
						</li><li class="listitem">
							A load balancer is required to distribute API requests across all control plane nodes in highly available OpenShift Container Platform environments. You can use any TCP-based load balancing solution that meets OpenShift Container Platform DNS routing and port requirements.
						</li></ul></div></section><section class="section" id="checking-load-balancer-configuration_troubleshooting-installations"><div class="titlepage"><div><div><h3 class="title">7.1.3. Checking a load balancer configuration before OpenShift Container Platform installation</h3></div></div></div><p>
					Check your load balancer configuration prior to starting an OpenShift Container Platform installation.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have configured an external load balancer of your choosing, in preparation for an OpenShift Container Platform installation. The following example is based on a Red Hat Enterprise Linux (RHEL) host using HAProxy to provide load balancing services to a cluster.
						</li><li class="listitem">
							You have configured DNS in preparation for an OpenShift Container Platform installation.
						</li><li class="listitem">
							You have SSH access to your load balancer.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check that the <code class="literal">haproxy</code> systemd service is active:
						</p><pre class="programlisting language-terminal">$ ssh &lt;user_name&gt;@&lt;load_balancer&gt; systemctl status haproxy</pre></li><li class="listitem"><p class="simpara">
							Verify that the load balancer is listening on the required ports. The following example references ports <code class="literal">80</code>, <code class="literal">443</code>, <code class="literal">6443</code>, and <code class="literal">22623</code>.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									For HAProxy instances running on Red Hat Enterprise Linux (RHEL) 6, verify port status by using the <code class="literal">netstat</code> command:
								</p><pre class="programlisting language-terminal">$ ssh &lt;user_name&gt;@&lt;load_balancer&gt; netstat -nltupe | grep -E ':80|:443|:6443|:22623'</pre></li><li class="listitem"><p class="simpara">
									For HAProxy instances running on Red Hat Enterprise Linux (RHEL) 7 or 8, verify port status by using the <code class="literal">ss</code> command:
								</p><pre class="programlisting language-terminal">$ ssh &lt;user_name&gt;@&lt;load_balancer&gt; ss -nltupe | grep -E ':80|:443|:6443|:22623'</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Red Hat recommends the <code class="literal">ss</code> command instead of <code class="literal">netstat</code> in Red Hat Enterprise Linux (RHEL) 7 or later. <code class="literal">ss</code> is provided by the iproute package. For more information on the <code class="literal">ss</code> command, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-tool_reference-ss">Red Hat Enterprise Linux (RHEL) 7 Performance Tuning Guide</a>.
									</p></div></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Check that the wildcard DNS record resolves to the load balancer:
						</p><pre class="programlisting language-terminal">$ dig &lt;wildcard_fqdn&gt; @&lt;dns_server&gt;</pre></li></ol></div></section><section class="section" id="specifying-openshift-installer-log-levels_troubleshooting-installations"><div class="titlepage"><div><div><h3 class="title">7.1.4. Specifying OpenShift Container Platform installer log levels</h3></div></div></div><p>
					By default, the OpenShift Container Platform installer log level is set to <code class="literal">info</code>. If more detailed logging is required when diagnosing a failed OpenShift Container Platform installation, you can increase the <code class="literal">openshift-install</code> log level to <code class="literal">debug</code> when starting the installation again.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the installation host.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Set the installation log level to <code class="literal">debug</code> when initiating the installation:
						</p><pre class="programlisting language-terminal">$ ./openshift-install --dir &lt;installation_directory&gt; wait-for bootstrap-complete --log-level debug  <span id="CO26-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO26-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Possible log levels include <code class="literal">info</code>, <code class="literal">warn</code>, <code class="literal">error,</code> and <code class="literal">debug</code>.
								</div></dd></dl></div></li></ul></div></section><section class="section" id="troubleshooting-openshift-install-command-issues_troubleshooting-installations"><div class="titlepage"><div><div><h3 class="title">7.1.5. Troubleshooting openshift-install command issues</h3></div></div></div><p>
					If you experience issues running the <code class="literal">openshift-install</code> command, check the following:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							The installation has been initiated within 24 hours of Ignition configuration file creation. The Ignition files are created when the following command is run:
						</p><pre class="programlisting language-terminal">$ ./openshift-install create ignition-configs --dir=./install_dir</pre></li><li class="listitem">
							The <code class="literal">install-config.yaml</code> file is in the same directory as the installer. If an alternative installation path is declared by using the <code class="literal">./openshift-install --dir</code> option, verify that the <code class="literal">install-config.yaml</code> file exists within that directory.
						</li></ul></div></section><section class="section" id="monitoring-installation-progress_troubleshooting-installations"><div class="titlepage"><div><div><h3 class="title">7.1.6. Monitoring installation progress</h3></div></div></div><p>
					You can monitor high-level installation, bootstrap, and control plane logs as an OpenShift Container Platform installation progresses. This provides greater visibility into how an installation progresses and helps identify the stage at which an installation failure occurs.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have SSH access to your hosts.
						</li><li class="listitem"><p class="simpara">
							You have the fully qualified domain names of the bootstrap and control plane nodes.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The initial <code class="literal">kubeadmin</code> password can be found in <code class="literal">&lt;install_directory&gt;/auth/kubeadmin-password</code> on the installation host.
							</p></div></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Watch the installation log as the installation progresses:
						</p><pre class="programlisting language-terminal">$ tail -f ~/&lt;installation_directory&gt;/.openshift_install.log</pre></li><li class="listitem"><p class="simpara">
							Monitor the <code class="literal">bootkube.service</code> journald unit log on the bootstrap node, after it has booted. This provides visibility into the bootstrapping of the first control plane. Replace <code class="literal">&lt;bootstrap_fqdn&gt;</code> with the bootstrap node’s fully qualified domain name:
						</p><pre class="programlisting language-terminal">$ ssh core@&lt;bootstrap_fqdn&gt; journalctl -b -f -u bootkube.service</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The <code class="literal">bootkube.service</code> log on the bootstrap node outputs etcd <code class="literal">connection refused</code> errors, indicating that the bootstrap server is unable to connect to etcd on control plane nodes. After etcd has started on each control plane node and the nodes have joined the cluster, the errors should stop.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Monitor <code class="literal">kubelet.service</code> journald unit logs on control plane nodes, after they have booted. This provides visibility into control plane node agent activity.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Monitor the logs using <code class="literal">oc</code>:
								</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=master -u kubelet</pre></li><li class="listitem"><p class="simpara">
									If the API is not functional, review the logs using SSH instead. Replace <code class="literal">&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> with appropriate values:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; journalctl -b -f -u kubelet.service</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Monitor <code class="literal">crio.service</code> journald unit logs on control plane nodes, after they have booted. This provides visibility into control plane node CRI-O container runtime activity.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Monitor the logs using <code class="literal">oc</code>:
								</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=master -u crio</pre></li><li class="listitem"><p class="simpara">
									If the API is not functional, review the logs using SSH instead. Replace <code class="literal">&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> with appropriate values:
								</p><pre class="programlisting language-terminal">$ ssh core@master-N.cluster_name.sub_domain.domain journalctl -b -f -u crio.service</pre></li></ol></div></li></ol></div></section><section class="section" id="gathering-bootstrap-diagnostic-data_troubleshooting-installations"><div class="titlepage"><div><div><h3 class="title">7.1.7. Gathering bootstrap node diagnostic data</h3></div></div></div><p>
					When experiencing bootstrap-related issues, you can gather <code class="literal">bootkube.service</code> <code class="literal">journald</code> unit logs and container logs from the bootstrap node.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have SSH access to your bootstrap node.
						</li><li class="listitem">
							You have the fully qualified domain name of the bootstrap node.
						</li><li class="listitem">
							If you are hosting Ignition configuration files by using an HTTP server, you must have the HTTP server’s fully qualified domain name and the port number. You must also have SSH access to the HTTP host.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							If you have access to the bootstrap node’s console, monitor the console until the node reaches the login prompt.
						</li><li class="listitem"><p class="simpara">
							Verify the Ignition file configuration.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									If you are hosting Ignition configuration files by using an HTTP server.
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
											Verify the bootstrap node Ignition file URL. Replace <code class="literal">&lt;http_server_fqdn&gt;</code> with HTTP server’s fully qualified domain name:
										</p><pre class="programlisting language-terminal">$ curl -I http://&lt;http_server_fqdn&gt;:&lt;port&gt;/bootstrap.ign  <span id="CO27-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO27-1"><span class="callout">1</span></a> </dt><dd><div class="para">
													The <code class="literal">-I</code> option returns the header only. If the Ignition file is available on the specified URL, the command returns <code class="literal">200 OK</code> status. If it is not available, the command returns <code class="literal">404 file not found</code>.
												</div></dd></dl></div></li><li class="listitem"><p class="simpara">
											To verify that the Ignition file was received by the bootstrap node, query the HTTP server logs on the serving host. For example, if you are using an Apache web server to serve Ignition files, enter the following command:
										</p><pre class="programlisting language-terminal">$ grep -is 'bootstrap.ign' /var/log/httpd/access_log</pre><p class="simpara">
											If the bootstrap Ignition file is received, the associated <code class="literal">HTTP GET</code> log message will include a <code class="literal">200 OK</code> success status, indicating that the request succeeded.
										</p></li><li class="listitem">
											If the Ignition file was not received, check that the Ignition files exist and that they have the appropriate file and web server permissions on the serving host directly.
										</li></ol></div></li><li class="listitem"><p class="simpara">
									If you are using a cloud provider mechanism to inject Ignition configuration files into hosts as part of their initial deployment.
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
											Review the bootstrap node’s console to determine if the mechanism is injecting the bootstrap node Ignition file correctly.
										</li></ol></div></li></ul></div></li><li class="listitem">
							Verify the availability of the bootstrap node’s assigned storage device.
						</li><li class="listitem">
							Verify that the bootstrap node has been assigned an IP address from the DHCP server.
						</li><li class="listitem"><p class="simpara">
							Collect <code class="literal">bootkube.service</code> journald unit logs from the bootstrap node. Replace <code class="literal">&lt;bootstrap_fqdn&gt;</code> with the bootstrap node’s fully qualified domain name:
						</p><pre class="programlisting language-terminal">$ ssh core@&lt;bootstrap_fqdn&gt; journalctl -b -f -u bootkube.service</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The <code class="literal">bootkube.service</code> log on the bootstrap node outputs etcd <code class="literal">connection refused</code> errors, indicating that the bootstrap server is unable to connect to etcd on control plane nodes. After etcd has started on each control plane node and the nodes have joined the cluster, the errors should stop.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Collect logs from the bootstrap node containers.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Collect the logs using <code class="literal">podman</code> on the bootstrap node. Replace <code class="literal">&lt;bootstrap_fqdn&gt;</code> with the bootstrap node’s fully qualified domain name:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;bootstrap_fqdn&gt; 'for pod in $(sudo podman ps -a -q); do sudo podman logs $pod; done'</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							If the bootstrap process fails, verify the following.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									You can resolve <code class="literal">api.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> from the installation host.
								</li><li class="listitem">
									The load balancer proxies port 6443 connections to bootstrap and control plane nodes. Ensure that the proxy configuration meets OpenShift Container Platform installation requirements.
								</li></ul></div></li></ol></div></section><section class="section" id="investigating-master-node-installation-issues_troubleshooting-installations"><div class="titlepage"><div><div><h3 class="title">7.1.8. Investigating control plane node installation issues</h3></div></div></div><p>
					If you experience control plane node installation issues, determine the control plane node OpenShift Container Platform software defined network (SDN), and network Operator status. Collect <code class="literal">kubelet.service</code>, <code class="literal">crio.service</code> journald unit logs, and control plane node container logs for visibility into control plane node agent, CRI-O container runtime, and pod activity.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have SSH access to your hosts.
						</li><li class="listitem">
							You have the fully qualified domain names of the bootstrap and control plane nodes.
						</li><li class="listitem"><p class="simpara">
							If you are hosting Ignition configuration files by using an HTTP server, you must have the HTTP server’s fully qualified domain name and the port number. You must also have SSH access to the HTTP host.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The initial <code class="literal">kubeadmin</code> password can be found in <code class="literal">&lt;install_directory&gt;/auth/kubeadmin-password</code> on the installation host.
							</p></div></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							If you have access to the console for the control plane node, monitor the console until the node reaches the login prompt. During the installation, Ignition log messages are output to the console.
						</li><li class="listitem"><p class="simpara">
							Verify Ignition file configuration.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									If you are hosting Ignition configuration files by using an HTTP server.
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
											Verify the control plane node Ignition file URL. Replace <code class="literal">&lt;http_server_fqdn&gt;</code> with HTTP server’s fully qualified domain name:
										</p><pre class="programlisting language-terminal">$ curl -I http://&lt;http_server_fqdn&gt;:&lt;port&gt;/master.ign  <span id="CO28-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO28-1"><span class="callout">1</span></a> </dt><dd><div class="para">
													The <code class="literal">-I</code> option returns the header only. If the Ignition file is available on the specified URL, the command returns <code class="literal">200 OK</code> status. If it is not available, the command returns <code class="literal">404 file not found</code>.
												</div></dd></dl></div></li><li class="listitem"><p class="simpara">
											To verify that the Ignition file was received by the control plane node query the HTTP server logs on the serving host. For example, if you are using an Apache web server to serve Ignition files:
										</p><pre class="programlisting language-terminal">$ grep -is 'master.ign' /var/log/httpd/access_log</pre><p class="simpara">
											If the master Ignition file is received, the associated <code class="literal">HTTP GET</code> log message will include a <code class="literal">200 OK</code> success status, indicating that the request succeeded.
										</p></li><li class="listitem">
											If the Ignition file was not received, check that it exists on the serving host directly. Ensure that the appropriate file and web server permissions are in place.
										</li></ol></div></li><li class="listitem"><p class="simpara">
									If you are using a cloud provider mechanism to inject Ignition configuration files into hosts as part of their initial deployment.
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
											Review the console for the control plane node to determine if the mechanism is injecting the control plane node Ignition file correctly.
										</li></ol></div></li></ul></div></li><li class="listitem">
							Check the availability of the storage device assigned to the control plane node.
						</li><li class="listitem">
							Verify that the control plane node has been assigned an IP address from the DHCP server.
						</li><li class="listitem"><p class="simpara">
							Determine control plane node status.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Query control plane node status:
								</p><pre class="programlisting language-terminal">$ oc get nodes</pre></li><li class="listitem"><p class="simpara">
									If one of the control plane nodes does not reach a <code class="literal">Ready</code> status, retrieve a detailed node description:
								</p><pre class="programlisting language-terminal">$ oc describe node &lt;master_node&gt;</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										It is not possible to run <code class="literal">oc</code> commands if an installation issue prevents the OpenShift Container Platform API from running or if the kubelet is not running yet on each node:
									</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Determine OpenShift Container Platform SDN status.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Review <code class="literal">sdn-controller</code>, <code class="literal">sdn</code>, and <code class="literal">ovs</code> daemon set status, in the <code class="literal">openshift-sdn</code> namespace:
								</p><pre class="programlisting language-terminal">$ oc get daemonsets -n openshift-sdn</pre></li><li class="listitem"><p class="simpara">
									If those resources are listed as <code class="literal">Not found</code>, review pods in the <code class="literal">openshift-sdn</code> namespace:
								</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-sdn</pre></li><li class="listitem"><p class="simpara">
									Review logs relating to failed OpenShift Container Platform SDN pods in the <code class="literal">openshift-sdn</code> namespace:
								</p><pre class="programlisting language-terminal">$ oc logs &lt;sdn_pod&gt; -n openshift-sdn</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Determine cluster network configuration status.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Review whether the cluster’s network configuration exists:
								</p><pre class="programlisting language-terminal">$ oc get network.config.openshift.io cluster -o yaml</pre></li><li class="listitem"><p class="simpara">
									If the installer failed to create the network configuration, generate the Kubernetes manifests again and review message output:
								</p><pre class="programlisting language-terminal">$ ./openshift-install create manifests</pre></li><li class="listitem"><p class="simpara">
									Review the pod status in the <code class="literal">openshift-network-operator</code> namespace to determine whether the Cluster Network Operator (CNO) is running:
								</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-network-operator</pre></li><li class="listitem"><p class="simpara">
									Gather network Operator pod logs from the <code class="literal">openshift-network-operator</code> namespace:
								</p><pre class="programlisting language-terminal">$ oc logs pod/&lt;network_operator_pod_name&gt; -n openshift-network-operator</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Monitor <code class="literal">kubelet.service</code> journald unit logs on control plane nodes, after they have booted. This provides visibility into control plane node agent activity.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Retrieve the logs using <code class="literal">oc</code>:
								</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=master -u kubelet</pre></li><li class="listitem"><p class="simpara">
									If the API is not functional, review the logs using SSH instead. Replace <code class="literal">&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> with appropriate values:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; journalctl -b -f -u kubelet.service</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										OpenShift Container Platform 4.13 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes by using SSH is not recommended. Before attempting to collect diagnostic data over SSH, review whether the data collected by running <code class="literal">oc adm must gather</code> and other <code class="literal">oc</code> commands is sufficient instead. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code class="literal">oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code class="literal">ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>.
									</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Retrieve <code class="literal">crio.service</code> journald unit logs on control plane nodes, after they have booted. This provides visibility into control plane node CRI-O container runtime activity.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Retrieve the logs using <code class="literal">oc</code>:
								</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=master -u crio</pre></li><li class="listitem"><p class="simpara">
									If the API is not functional, review the logs using SSH instead:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; journalctl -b -f -u crio.service</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Collect logs from specific subdirectories under <code class="literal">/var/log/</code> on control plane nodes.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Retrieve a list of logs contained within a <code class="literal">/var/log/</code> subdirectory. The following example lists files in <code class="literal">/var/log/openshift-apiserver/</code> on all control plane nodes:
								</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=master --path=openshift-apiserver</pre></li><li class="listitem"><p class="simpara">
									Inspect a specific log within a <code class="literal">/var/log/</code> subdirectory. The following example outputs <code class="literal">/var/log/openshift-apiserver/audit.log</code> contents from all control plane nodes:
								</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=master --path=openshift-apiserver/audit.log</pre></li><li class="listitem"><p class="simpara">
									If the API is not functional, review the logs on each node using SSH instead. The following example tails <code class="literal">/var/log/openshift-apiserver/audit.log</code>:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; sudo tail -f /var/log/openshift-apiserver/audit.log</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Review control plane node container logs using SSH.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									List the containers:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; sudo crictl ps -a</pre></li><li class="listitem"><p class="simpara">
									Retrieve a container’s logs using <code class="literal">crictl</code>:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; sudo crictl logs -f &lt;container_id&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							If you experience control plane node configuration issues, verify that the MCO, MCO endpoint, and DNS record are functioning. The Machine Config Operator (MCO) manages operating system configuration during the installation procedure. Also verify system clock accuracy and certificate validity.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Test whether the MCO endpoint is available. Replace <code class="literal">&lt;cluster_name&gt;</code> with appropriate values:
								</p><pre class="programlisting language-terminal">$ curl https://api-int.&lt;cluster_name&gt;:22623/config/master</pre></li><li class="listitem">
									If the endpoint is unresponsive, verify load balancer configuration. Ensure that the endpoint is configured to run on port 22623.
								</li><li class="listitem"><p class="simpara">
									Verify that the MCO endpoint’s DNS record is configured and resolves to the load balancer.
								</p><div class="orderedlist"><ol class="orderedlist" type="i"><li class="listitem"><p class="simpara">
											Run a DNS lookup for the defined MCO endpoint name:
										</p><pre class="programlisting language-terminal">$ dig api-int.&lt;cluster_name&gt; @&lt;dns_server&gt;</pre></li><li class="listitem"><p class="simpara">
											Run a reverse lookup to the assigned MCO IP address on the load balancer:
										</p><pre class="programlisting language-terminal">$ dig -x &lt;load_balancer_mco_ip_address&gt; @&lt;dns_server&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
									Verify that the MCO is functioning from the bootstrap node directly. Replace <code class="literal">&lt;bootstrap_fqdn&gt;</code> with the bootstrap node’s fully qualified domain name:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;bootstrap_fqdn&gt; curl https://api-int.&lt;cluster_name&gt;:22623/config/master</pre></li><li class="listitem"><p class="simpara">
									System clock time must be synchronized between bootstrap, master, and worker nodes. Check each node’s system clock reference time and time synchronization statistics:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; chronyc tracking</pre></li><li class="listitem"><p class="simpara">
									Review certificate validity:
								</p><pre class="programlisting language-terminal">$ openssl s_client -connect api-int.&lt;cluster_name&gt;:22623 | openssl x509 -noout -text</pre></li></ol></div></li></ol></div></section><section class="section" id="investigating-etcd-installation-issues_troubleshooting-installations"><div class="titlepage"><div><div><h3 class="title">7.1.9. Investigating etcd installation issues</h3></div></div></div><p>
					If you experience etcd issues during installation, you can check etcd pod status and collect etcd pod logs. You can also verify etcd DNS records and check DNS availability on control plane nodes.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have SSH access to your hosts.
						</li><li class="listitem">
							You have the fully qualified domain names of the control plane nodes.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check the status of etcd pods.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Review the status of pods in the <code class="literal">openshift-etcd</code> namespace:
								</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-etcd</pre></li><li class="listitem"><p class="simpara">
									Review the status of pods in the <code class="literal">openshift-etcd-operator</code> namespace:
								</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-etcd-operator</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							If any of the pods listed by the previous commands are not showing a <code class="literal">Running</code> or a <code class="literal">Completed</code> status, gather diagnostic information for the pod.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Review events for the pod:
								</p><pre class="programlisting language-terminal">$ oc describe pod/&lt;pod_name&gt; -n &lt;namespace&gt;</pre></li><li class="listitem"><p class="simpara">
									Inspect the pod’s logs:
								</p><pre class="programlisting language-terminal">$ oc logs pod/&lt;pod_name&gt; -n &lt;namespace&gt;</pre></li><li class="listitem"><p class="simpara">
									If the pod has more than one container, the preceding command will create an error, and the container names will be provided in the error message. Inspect logs for each container:
								</p><pre class="programlisting language-terminal">$ oc logs pod/&lt;pod_name&gt; -c &lt;container_name&gt; -n &lt;namespace&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							If the API is not functional, review etcd pod and container logs on each control plane node by using SSH instead. Replace <code class="literal">&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> with appropriate values.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									List etcd pods on each control plane node:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; sudo crictl pods --name=etcd-</pre></li><li class="listitem"><p class="simpara">
									For any pods not showing <code class="literal">Ready</code> status, inspect pod status in detail. Replace <code class="literal">&lt;pod_id&gt;</code> with the pod’s ID listed in the output of the preceding command:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; sudo crictl inspectp &lt;pod_id&gt;</pre></li><li class="listitem"><p class="simpara">
									List containers related to a pod:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; sudo crictl ps | grep '&lt;pod_id&gt;'</pre></li><li class="listitem"><p class="simpara">
									For any containers not showing <code class="literal">Ready</code> status, inspect container status in detail. Replace <code class="literal">&lt;container_id&gt;</code> with container IDs listed in the output of the preceding command:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; sudo crictl inspect &lt;container_id&gt;</pre></li><li class="listitem"><p class="simpara">
									Review the logs for any containers not showing a <code class="literal">Ready</code> status. Replace <code class="literal">&lt;container_id&gt;</code> with the container IDs listed in the output of the preceding command:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; sudo crictl logs -f &lt;container_id&gt;</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										OpenShift Container Platform 4.13 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes by using SSH is not recommended. Before attempting to collect diagnostic data over SSH, review whether the data collected by running <code class="literal">oc adm must gather</code> and other <code class="literal">oc</code> commands is sufficient instead. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code class="literal">oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code class="literal">ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>.
									</p></div></div></li></ol></div></li><li class="listitem">
							Validate primary and secondary DNS server connectivity from control plane nodes.
						</li></ol></div></section><section class="section" id="investigating-kubelet-api-installation-issues_troubleshooting-installations"><div class="titlepage"><div><div><h3 class="title">7.1.10. Investigating control plane node kubelet and API server issues</h3></div></div></div><p>
					To investigate control plane node kubelet and API server issues during installation, check DNS, DHCP, and load balancer functionality. Also, verify that certificates have not expired.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have SSH access to your hosts.
						</li><li class="listitem">
							You have the fully qualified domain names of the control plane nodes.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Verify that the API server’s DNS record directs the kubelet on control plane nodes to <code class="literal">https://api-int.&lt;cluster_name&gt;.&lt;base_domain&gt;:6443</code>. Ensure that the record references the load balancer.
						</li><li class="listitem">
							Ensure that the load balancer’s port 6443 definition references each control plane node.
						</li><li class="listitem">
							Check that unique control plane node hostnames have been provided by DHCP.
						</li><li class="listitem"><p class="simpara">
							Inspect the <code class="literal">kubelet.service</code> journald unit logs on each control plane node.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Retrieve the logs using <code class="literal">oc</code>:
								</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=master -u kubelet</pre></li><li class="listitem"><p class="simpara">
									If the API is not functional, review the logs using SSH instead. Replace <code class="literal">&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> with appropriate values:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; journalctl -b -f -u kubelet.service</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										OpenShift Container Platform 4.13 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes by using SSH is not recommended. Before attempting to collect diagnostic data over SSH, review whether the data collected by running <code class="literal">oc adm must gather</code> and other <code class="literal">oc</code> commands is sufficient instead. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code class="literal">oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code class="literal">ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>.
									</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Check for certificate expiration messages in the control plane node kubelet logs.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Retrieve the log using <code class="literal">oc</code>:
								</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=master -u kubelet | grep -is 'x509: certificate has expired'</pre></li><li class="listitem"><p class="simpara">
									If the API is not functional, review the logs using SSH instead. Replace <code class="literal">&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> with appropriate values:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; journalctl -b -f -u kubelet.service  | grep -is 'x509: certificate has expired'</pre></li></ol></div></li></ol></div></section><section class="section" id="investigating-worker-node-installation-issues_troubleshooting-installations"><div class="titlepage"><div><div><h3 class="title">7.1.11. Investigating worker node installation issues</h3></div></div></div><p>
					If you experience worker node installation issues, you can review the worker node status. Collect <code class="literal">kubelet.service</code>, <code class="literal">crio.service</code> journald unit logs and the worker node container logs for visibility into the worker node agent, CRI-O container runtime and pod activity. Additionally, you can check the Ignition file and Machine API Operator functionality. If worker node post-installation configuration fails, check Machine Config Operator (MCO) and DNS functionality. You can also verify system clock synchronization between the bootstrap, master, and worker nodes, and validate certificates.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have SSH access to your hosts.
						</li><li class="listitem">
							You have the fully qualified domain names of the bootstrap and worker nodes.
						</li><li class="listitem"><p class="simpara">
							If you are hosting Ignition configuration files by using an HTTP server, you must have the HTTP server’s fully qualified domain name and the port number. You must also have SSH access to the HTTP host.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The initial <code class="literal">kubeadmin</code> password can be found in <code class="literal">&lt;install_directory&gt;/auth/kubeadmin-password</code> on the installation host.
							</p></div></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							If you have access to the worker node’s console, monitor the console until the node reaches the login prompt. During the installation, Ignition log messages are output to the console.
						</li><li class="listitem"><p class="simpara">
							Verify Ignition file configuration.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									If you are hosting Ignition configuration files by using an HTTP server.
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
											Verify the worker node Ignition file URL. Replace <code class="literal">&lt;http_server_fqdn&gt;</code> with HTTP server’s fully qualified domain name:
										</p><pre class="programlisting language-terminal">$ curl -I http://&lt;http_server_fqdn&gt;:&lt;port&gt;/worker.ign  <span id="CO29-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO29-1"><span class="callout">1</span></a> </dt><dd><div class="para">
													The <code class="literal">-I</code> option returns the header only. If the Ignition file is available on the specified URL, the command returns <code class="literal">200 OK</code> status. If it is not available, the command returns <code class="literal">404 file not found</code>.
												</div></dd></dl></div></li><li class="listitem"><p class="simpara">
											To verify that the Ignition file was received by the worker node, query the HTTP server logs on the HTTP host. For example, if you are using an Apache web server to serve Ignition files:
										</p><pre class="programlisting language-terminal">$ grep -is 'worker.ign' /var/log/httpd/access_log</pre><p class="simpara">
											If the worker Ignition file is received, the associated <code class="literal">HTTP GET</code> log message will include a <code class="literal">200 OK</code> success status, indicating that the request succeeded.
										</p></li><li class="listitem">
											If the Ignition file was not received, check that it exists on the serving host directly. Ensure that the appropriate file and web server permissions are in place.
										</li></ol></div></li><li class="listitem"><p class="simpara">
									If you are using a cloud provider mechanism to inject Ignition configuration files into hosts as part of their initial deployment.
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
											Review the worker node’s console to determine if the mechanism is injecting the worker node Ignition file correctly.
										</li></ol></div></li></ul></div></li><li class="listitem">
							Check the availability of the worker node’s assigned storage device.
						</li><li class="listitem">
							Verify that the worker node has been assigned an IP address from the DHCP server.
						</li><li class="listitem"><p class="simpara">
							Determine worker node status.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Query node status:
								</p><pre class="programlisting language-terminal">$ oc get nodes</pre></li><li class="listitem"><p class="simpara">
									Retrieve a detailed node description for any worker nodes not showing a <code class="literal">Ready</code> status:
								</p><pre class="programlisting language-terminal">$ oc describe node &lt;worker_node&gt;</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										It is not possible to run <code class="literal">oc</code> commands if an installation issue prevents the OpenShift Container Platform API from running or if the kubelet is not running yet on each node.
									</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Unlike control plane nodes, worker nodes are deployed and scaled using the Machine API Operator. Check the status of the Machine API Operator.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Review Machine API Operator pod status:
								</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
									If the Machine API Operator pod does not have a <code class="literal">Ready</code> status, detail the pod’s events:
								</p><pre class="programlisting language-terminal">$ oc describe pod/&lt;machine_api_operator_pod_name&gt; -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
									Inspect <code class="literal">machine-api-operator</code> container logs. The container runs within the <code class="literal">machine-api-operator</code> pod:
								</p><pre class="programlisting language-terminal">$ oc logs pod/&lt;machine_api_operator_pod_name&gt; -n openshift-machine-api -c machine-api-operator</pre></li><li class="listitem"><p class="simpara">
									Also inspect <code class="literal">kube-rbac-proxy</code> container logs. The container also runs within the <code class="literal">machine-api-operator</code> pod:
								</p><pre class="programlisting language-terminal">$ oc logs pod/&lt;machine_api_operator_pod_name&gt; -n openshift-machine-api -c kube-rbac-proxy</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Monitor <code class="literal">kubelet.service</code> journald unit logs on worker nodes, after they have booted. This provides visibility into worker node agent activity.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Retrieve the logs using <code class="literal">oc</code>:
								</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=worker -u kubelet</pre></li><li class="listitem"><p class="simpara">
									If the API is not functional, review the logs using SSH instead. Replace <code class="literal">&lt;worker-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> with appropriate values:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;worker-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; journalctl -b -f -u kubelet.service</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										OpenShift Container Platform 4.13 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes by using SSH is not recommended. Before attempting to collect diagnostic data over SSH, review whether the data collected by running <code class="literal">oc adm must gather</code> and other <code class="literal">oc</code> commands is sufficient instead. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code class="literal">oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code class="literal">ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>.
									</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Retrieve <code class="literal">crio.service</code> journald unit logs on worker nodes, after they have booted. This provides visibility into worker node CRI-O container runtime activity.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Retrieve the logs using <code class="literal">oc</code>:
								</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=worker -u crio</pre></li><li class="listitem"><p class="simpara">
									If the API is not functional, review the logs using SSH instead:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;worker-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; journalctl -b -f -u crio.service</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Collect logs from specific subdirectories under <code class="literal">/var/log/</code> on worker nodes.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Retrieve a list of logs contained within a <code class="literal">/var/log/</code> subdirectory. The following example lists files in <code class="literal">/var/log/sssd/</code> on all worker nodes:
								</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=worker --path=sssd</pre></li><li class="listitem"><p class="simpara">
									Inspect a specific log within a <code class="literal">/var/log/</code> subdirectory. The following example outputs <code class="literal">/var/log/sssd/audit.log</code> contents from all worker nodes:
								</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=worker --path=sssd/sssd.log</pre></li><li class="listitem"><p class="simpara">
									If the API is not functional, review the logs on each node using SSH instead. The following example tails <code class="literal">/var/log/sssd/sssd.log</code>:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;worker-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; sudo tail -f /var/log/sssd/sssd.log</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Review worker node container logs using SSH.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									List the containers:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;worker-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; sudo crictl ps -a</pre></li><li class="listitem"><p class="simpara">
									Retrieve a container’s logs using <code class="literal">crictl</code>:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;worker-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; sudo crictl logs -f &lt;container_id&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							If you experience worker node configuration issues, verify that the MCO, MCO endpoint, and DNS record are functioning. The Machine Config Operator (MCO) manages operating system configuration during the installation procedure. Also verify system clock accuracy and certificate validity.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Test whether the MCO endpoint is available. Replace <code class="literal">&lt;cluster_name&gt;</code> with appropriate values:
								</p><pre class="programlisting language-terminal">$ curl https://api-int.&lt;cluster_name&gt;:22623/config/worker</pre></li><li class="listitem">
									If the endpoint is unresponsive, verify load balancer configuration. Ensure that the endpoint is configured to run on port 22623.
								</li><li class="listitem"><p class="simpara">
									Verify that the MCO endpoint’s DNS record is configured and resolves to the load balancer.
								</p><div class="orderedlist"><ol class="orderedlist" type="i"><li class="listitem"><p class="simpara">
											Run a DNS lookup for the defined MCO endpoint name:
										</p><pre class="programlisting language-terminal">$ dig api-int.&lt;cluster_name&gt; @&lt;dns_server&gt;</pre></li><li class="listitem"><p class="simpara">
											Run a reverse lookup to the assigned MCO IP address on the load balancer:
										</p><pre class="programlisting language-terminal">$ dig -x &lt;load_balancer_mco_ip_address&gt; @&lt;dns_server&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
									Verify that the MCO is functioning from the bootstrap node directly. Replace <code class="literal">&lt;bootstrap_fqdn&gt;</code> with the bootstrap node’s fully qualified domain name:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;bootstrap_fqdn&gt; curl https://api-int.&lt;cluster_name&gt;:22623/config/worker</pre></li><li class="listitem"><p class="simpara">
									System clock time must be synchronized between bootstrap, master, and worker nodes. Check each node’s system clock reference time and time synchronization statistics:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; chronyc tracking</pre></li><li class="listitem"><p class="simpara">
									Review certificate validity:
								</p><pre class="programlisting language-terminal">$ openssl s_client -connect api-int.&lt;cluster_name&gt;:22623 | openssl x509 -noout -text</pre></li></ol></div></li></ol></div></section><section class="section" id="querying-operator-status-after-installation_troubleshooting-installations"><div class="titlepage"><div><div><h3 class="title">7.1.12. Querying Operator status after installation</h3></div></div></div><p>
					You can check Operator status at the end of an installation. Retrieve diagnostic data for Operators that do not become available. Review logs for any Operator pods that are listed as <code class="literal">Pending</code> or have an error status. Validate base images used by problematic pods.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check that cluster Operators are all available at the end of an installation.
						</p><pre class="programlisting language-terminal">$ oc get clusteroperators</pre></li><li class="listitem"><p class="simpara">
							Verify that all of the required certificate signing requests (CSRs) are approved. Some nodes might not move to a <code class="literal">Ready</code> status and some cluster Operators might not become available if there are pending CSRs.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Check the status of the CSRs and ensure that you see a client and server request with the <code class="literal">Pending</code> or <code class="literal">Approved</code> status for each machine that you added to the cluster:
								</p><pre class="programlisting language-terminal">$ oc get csr</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-8b2br   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending <span id="CO30-1"><!--Empty--></span><span class="callout">1</span>
csr-8vnps   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-bfd72   5m26s   system:node:ip-10-0-50-126.us-east-2.compute.internal                       Pending <span id="CO30-2"><!--Empty--></span><span class="callout">2</span>
csr-c57lv   5m26s   system:node:ip-10-0-95-157.us-east-2.compute.internal                       Pending
...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO30-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											A client request CSR.
										</div></dd><dt><a href="#CO30-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											A server request CSR.
										</div></dd></dl></div><p class="simpara">
									In this example, two machines are joining the cluster. You might see more approved CSRs in the list.
								</p></li><li class="listitem"><p class="simpara">
									If the CSRs were not approved, after all of the pending CSRs for the machines you added are in <code class="literal">Pending</code> status, approve the CSRs for your cluster machines:
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Because the CSRs rotate automatically, approve your CSRs within an hour of adding the machines to the cluster. If you do not approve them within an hour, the certificates will rotate, and more than two certificates will be present for each node. You must approve all of these certificates. After you approve the initial CSRs, the subsequent node client CSRs are automatically approved by the cluster <code class="literal">kube-controller-manager</code>.
									</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										For clusters running on platforms that are not machine API enabled, such as bare metal and other user-provisioned infrastructure, you must implement a method of automatically approving the kubelet serving certificate requests (CSRs). If a request is not approved, then the <code class="literal">oc exec</code>, <code class="literal">oc rsh</code>, and <code class="literal">oc logs</code> commands cannot succeed, because a serving certificate is required when the API server connects to the kubelet. Any operation that contacts the Kubelet endpoint requires this certificate approval to be in place. The method must watch for new CSRs, confirm that the CSR was submitted by the <code class="literal">node-bootstrapper</code> service account in the <code class="literal">system:node</code> or <code class="literal">system:admin</code> groups, and confirm the identity of the node.
									</p></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
											To approve them individually, run the following command for each valid CSR:
										</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt; <span id="CO31-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO31-1"><span class="callout">1</span></a> </dt><dd><div class="para">
													<code class="literal">&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.
												</div></dd></dl></div></li><li class="listitem"><p class="simpara">
											To approve all pending CSRs, run the following command:
										</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs oc adm certificate approve</pre></li></ul></div></li></ol></div></li><li class="listitem"><p class="simpara">
							View Operator events:
						</p><pre class="programlisting language-terminal">$ oc describe clusteroperator &lt;operator_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Review Operator pod status within the Operator’s namespace:
						</p><pre class="programlisting language-terminal">$ oc get pods -n &lt;operator_namespace&gt;</pre></li><li class="listitem"><p class="simpara">
							Obtain a detailed description for pods that do not have <code class="literal">Running</code> status:
						</p><pre class="programlisting language-terminal">$ oc describe pod/&lt;operator_pod_name&gt; -n &lt;operator_namespace&gt;</pre></li><li class="listitem"><p class="simpara">
							Inspect pod logs:
						</p><pre class="programlisting language-terminal">$ oc logs pod/&lt;operator_pod_name&gt; -n &lt;operator_namespace&gt;</pre></li><li class="listitem"><p class="simpara">
							When experiencing pod base image related issues, review base image status.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Obtain details of the base image used by a problematic pod:
								</p><pre class="programlisting language-terminal">$ oc get pod -o "jsonpath={range .status.containerStatuses[*]}{.name}{'\t'}{.state}{'\t'}{.image}{'\n'}{end}" &lt;operator_pod_name&gt; -n &lt;operator_namespace&gt;</pre></li><li class="listitem"><p class="simpara">
									List base image release information:
								</p><pre class="programlisting language-terminal">$ oc adm release info &lt;image_path&gt;:&lt;tag&gt; --commits</pre></li></ol></div></li></ol></div></section><section class="section" id="installation-bootstrap-gather_troubleshooting-installations"><div class="titlepage"><div><div><h3 class="title">7.1.13. Gathering logs from a failed installation</h3></div></div></div><p>
					If you gave an SSH key to your installation program, you can gather data about your failed installation.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You use a different command to gather logs about an unsuccessful installation than to gather logs from a running cluster. If you must gather logs from a running cluster, use the <code class="literal">oc adm must-gather</code> command.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Your OpenShift Container Platform installation failed before the bootstrap process finished. The bootstrap node is running and accessible through SSH.
						</li><li class="listitem">
							The <code class="literal">ssh-agent</code> process is active on your computer, and you provided the same SSH key to both the <code class="literal">ssh-agent</code> process and the installation program.
						</li><li class="listitem">
							If you tried to install a cluster on infrastructure that you provisioned, you must have the fully qualified domain names of the bootstrap and control plane nodes.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Generate the commands that are required to obtain the installation logs from the bootstrap and control plane machines:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									If you used installer-provisioned infrastructure, change to the directory that contains the installation program and run the following command:
								</p><pre class="programlisting language-terminal">$ ./openshift-install gather bootstrap --dir &lt;installation_directory&gt; <span id="CO32-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO32-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											<code class="literal">installation_directory</code> is the directory you specified when you ran <code class="literal">./openshift-install create cluster</code>. This directory contains the OpenShift Container Platform definition files that the installation program creates.
										</div></dd></dl></div><p class="simpara">
									For installer-provisioned infrastructure, the installation program stores information about the cluster, so you do not specify the hostnames or IP addresses.
								</p></li><li class="listitem"><p class="simpara">
									If you used infrastructure that you provisioned yourself, change to the directory that contains the installation program and run the following command:
								</p><pre class="programlisting language-terminal">$ ./openshift-install gather bootstrap --dir &lt;installation_directory&gt; \ <span id="CO33-1"><!--Empty--></span><span class="callout">1</span>
    --bootstrap &lt;bootstrap_address&gt; \ <span id="CO33-2"><!--Empty--></span><span class="callout">2</span>
    --master &lt;master_1_address&gt; \ <span id="CO33-3"><!--Empty--></span><span class="callout">3</span>
    --master &lt;master_2_address&gt; \ <span id="CO33-4"><!--Empty--></span><span class="callout">4</span>
    --master &lt;master_3_address&gt;" <span id="CO33-5"><!--Empty--></span><span class="callout">5</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO33-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											For <code class="literal">installation_directory</code>, specify the same directory you specified when you ran <code class="literal">./openshift-install create cluster</code>. This directory contains the OpenShift Container Platform definition files that the installation program creates.
										</div></dd><dt><a href="#CO33-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											<code class="literal">&lt;bootstrap_address&gt;</code> is the fully qualified domain name or IP address of the cluster’s bootstrap machine.
										</div></dd><dt><a href="#CO33-3"><span class="callout">3</span></a> <a href="#CO33-4"><span class="callout">4</span></a> <a href="#CO33-5"><span class="callout">5</span></a> </dt><dd><div class="para">
											For each control plane, or master, machine in your cluster, replace <code class="literal">&lt;master_*_address&gt;</code> with its fully qualified domain name or IP address.
										</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										A default cluster contains three control plane machines. List all of your control plane machines as shown, no matter how many your cluster uses.
									</p></div></div></li></ul></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">INFO Pulling debug logs from the bootstrap machine
INFO Bootstrap gather logs captured here "&lt;installation_directory&gt;/log-bundle-&lt;timestamp&gt;.tar.gz"</pre>

							</p></div><p class="simpara">
							If you open a Red Hat support case about your installation failure, include the compressed logs in the case.
						</p></li></ol></div></section><section class="section _additional-resources" id="additional-resources-2"><div class="titlepage"><div><div><h3 class="title">7.1.14. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/architecture/#installation-process_architecture-installation">Installation process</a> for more details on OpenShift Container Platform installation types and process.
						</li></ul></div></section></section><section class="section" id="verifying-node-health"><div class="titlepage"><div><div><h2 class="title">7.2. Verifying node health</h2></div></div></div><section class="section" id="reviewing-node-status-use-and-configuration_verifying-node-health"><div class="titlepage"><div><div><h3 class="title">7.2.1. Reviewing node status, resource usage, and configuration</h3></div></div></div><p>
					Review cluster node health status, resource consumption statistics, and node logs. Additionally, query <code class="literal">kubelet</code> status on individual nodes.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							List the name, status, and role for all nodes in the cluster:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre></li><li class="listitem"><p class="simpara">
							Summarize CPU and memory usage for each node within the cluster:
						</p><pre class="programlisting language-terminal">$ oc adm top nodes</pre></li><li class="listitem"><p class="simpara">
							Summarize CPU and memory usage for a specific node:
						</p><pre class="programlisting language-terminal">$ oc adm top node my-node</pre></li></ul></div></section><section class="section" id="querying-kubelet-status-on-a-node_verifying-node-health"><div class="titlepage"><div><div><h3 class="title">7.2.2. Querying the kubelet’s status on a node</h3></div></div></div><p>
					You can review cluster node health status, resource consumption statistics, and node logs. Additionally, you can query <code class="literal">kubelet</code> status on individual nodes.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Your API service is still functional.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							The kubelet is managed using a systemd service on each node. Review the kubelet’s status by querying the <code class="literal">kubelet</code> systemd service within a debug pod.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Start a debug pod for a node:
								</p><pre class="programlisting language-terminal">$ oc debug node/my-node</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										If you are running <code class="literal">oc debug</code> on a control plane node, you can find administrative <code class="literal">kubeconfig</code> files in the <code class="literal">/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs</code> directory.
									</p></div></div></li><li class="listitem"><p class="simpara">
									Set <code class="literal">/host</code> as the root directory within the debug shell. The debug pod mounts the host’s root file system in <code class="literal">/host</code> within the pod. By changing the root directory to <code class="literal">/host</code>, you can run binaries contained in the host’s executable paths:
								</p><pre class="programlisting language-terminal"># chroot /host</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										OpenShift Container Platform 4.13 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes by using SSH is not recommended. However, if the OpenShift Container Platform API is not available, or <code class="literal">kubelet</code> is not properly functioning on the target node, <code class="literal">oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code class="literal">ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> instead.
									</p></div></div></li><li class="listitem"><p class="simpara">
									Check whether the <code class="literal">kubelet</code> systemd service is active on the node:
								</p><pre class="programlisting language-terminal"># systemctl is-active kubelet</pre></li><li class="listitem"><p class="simpara">
									Output a more detailed <code class="literal">kubelet.service</code> status summary:
								</p><pre class="programlisting language-terminal"># systemctl status kubelet</pre></li></ol></div></li></ol></div></section><section class="section" id="querying-cluster-node-journal-logs_verifying-node-health"><div class="titlepage"><div><div><h3 class="title">7.2.3. Querying cluster node journal logs</h3></div></div></div><p>
					You can gather <code class="literal">journald</code> unit logs and other logs within <code class="literal">/var/log</code> on individual cluster nodes.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Your API service is still functional.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have SSH access to your hosts.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Query <code class="literal">kubelet</code> <code class="literal">journald</code> unit logs from OpenShift Container Platform cluster nodes. The following example queries control plane nodes only:
						</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=master -u kubelet  <span id="CO34-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO34-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal">kubelet</code> as appropriate to query other unit logs.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Collect logs from specific subdirectories under <code class="literal">/var/log/</code> on cluster nodes.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Retrieve a list of logs contained within a <code class="literal">/var/log/</code> subdirectory. The following example lists files in <code class="literal">/var/log/openshift-apiserver/</code> on all control plane nodes:
								</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=master --path=openshift-apiserver</pre></li><li class="listitem"><p class="simpara">
									Inspect a specific log within a <code class="literal">/var/log/</code> subdirectory. The following example outputs <code class="literal">/var/log/openshift-apiserver/audit.log</code> contents from all control plane nodes:
								</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=master --path=openshift-apiserver/audit.log</pre></li><li class="listitem"><p class="simpara">
									If the API is not functional, review the logs on each node using SSH instead. The following example tails <code class="literal">/var/log/openshift-apiserver/audit.log</code>:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; sudo tail -f /var/log/openshift-apiserver/audit.log</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										OpenShift Container Platform 4.13 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes by using SSH is not recommended. Before attempting to collect diagnostic data over SSH, review whether the data collected by running <code class="literal">oc adm must gather</code> and other <code class="literal">oc</code> commands is sufficient instead. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code class="literal">oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code class="literal">ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>.
									</p></div></div></li></ol></div></li></ol></div></section></section><section class="section" id="troubleshooting-crio-issues"><div class="titlepage"><div><div><h2 class="title">7.3. Troubleshooting CRI-O container runtime issues</h2></div></div></div><section class="section" id="about-crio_troubleshooting-crio-issues"><div class="titlepage"><div><div><h3 class="title">7.3.1. About CRI-O container runtime engine</h3></div></div></div><p>
					CRI-O is a Kubernetes-native container engine implementation that integrates closely with the operating system to deliver an efficient and optimized Kubernetes experience. The CRI-O container engine runs as a systemd service on each OpenShift Container Platform cluster node.
				</p><p>
					When container runtime issues occur, verify the status of the <code class="literal">crio</code> systemd service on each node. Gather CRI-O journald unit logs from nodes that have container runtime issues.
				</p></section><section class="section" id="verifying-crio-status_troubleshooting-crio-issues"><div class="titlepage"><div><div><h3 class="title">7.3.2. Verifying CRI-O runtime engine status</h3></div></div></div><p>
					You can verify CRI-O container runtime engine status on each cluster node.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Review CRI-O status by querying the <code class="literal">crio</code> systemd service on a node, within a debug pod.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Start a debug pod for a node:
								</p><pre class="programlisting language-terminal">$ oc debug node/my-node</pre></li><li class="listitem"><p class="simpara">
									Set <code class="literal">/host</code> as the root directory within the debug shell. The debug pod mounts the host’s root file system in <code class="literal">/host</code> within the pod. By changing the root directory to <code class="literal">/host</code>, you can run binaries contained in the host’s executable paths:
								</p><pre class="programlisting language-terminal"># chroot /host</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										OpenShift Container Platform 4.13 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes by using SSH is not recommended. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code class="literal">oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code class="literal">ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> instead.
									</p></div></div></li><li class="listitem"><p class="simpara">
									Check whether the <code class="literal">crio</code> systemd service is active on the node:
								</p><pre class="programlisting language-terminal"># systemctl is-active crio</pre></li><li class="listitem"><p class="simpara">
									Output a more detailed <code class="literal">crio.service</code> status summary:
								</p><pre class="programlisting language-terminal"># systemctl status crio.service</pre></li></ol></div></li></ol></div></section><section class="section" id="gathering-crio-logs_troubleshooting-crio-issues"><div class="titlepage"><div><div><h3 class="title">7.3.3. Gathering CRI-O journald unit logs</h3></div></div></div><p>
					If you experience CRI-O issues, you can obtain CRI-O journald unit logs from a node.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Your API service is still functional.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have the fully qualified domain names of the control plane or control plane machines.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Gather CRI-O journald unit logs. The following example collects logs from all control plane nodes (within the cluster:
						</p><pre class="programlisting language-terminal">$ oc adm node-logs --role=master -u crio</pre></li><li class="listitem"><p class="simpara">
							Gather CRI-O journald unit logs from a specific node:
						</p><pre class="programlisting language-terminal">$ oc adm node-logs &lt;node_name&gt; -u crio</pre></li><li class="listitem"><p class="simpara">
							If the API is not functional, review the logs using SSH instead. Replace <code class="literal">&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> with appropriate values:
						</p><pre class="programlisting language-terminal">$ ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; journalctl -b -f -u crio.service</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								OpenShift Container Platform 4.13 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes by using SSH is not recommended. Before attempting to collect diagnostic data over SSH, review whether the data collected by running <code class="literal">oc adm must gather</code> and other <code class="literal">oc</code> commands is sufficient instead. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code class="literal">oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code class="literal">ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>.
							</p></div></div></li></ol></div></section><section class="section" id="cleaning-crio-storage_troubleshooting-crio-issues"><div class="titlepage"><div><div><h3 class="title">7.3.4. Cleaning CRI-O storage</h3></div></div></div><p>
					You can manually clear the CRI-O ephemeral storage if you experience the following issues:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							A node cannot run on any pods and this error appears:
						</p><pre class="programlisting language-terminal">Failed to create pod sandbox: rpc error: code = Unknown desc = failed to mount container XXX: error recreating the missing symlinks: error reading name of symlink for XXX: open /var/lib/containers/storage/overlay/XXX/link: no such file or directory</pre></li><li class="listitem"><p class="simpara">
							You cannot create a new container on a working node and the “can’t stat lower layer” error appears:
						</p><pre class="programlisting language-terminal">can't stat lower layer ...  because it does not exist.  Going through storage to recreate the missing symlinks.</pre></li><li class="listitem">
							Your node is in the <code class="literal">NotReady</code> state after a cluster upgrade or if you attempt to reboot it.
						</li><li class="listitem">
							The container runtime implementation (<code class="literal">crio</code>) is not working properly.
						</li><li class="listitem">
							You are unable to start a debug shell on the node using <code class="literal">oc debug node/&lt;nodename&gt;</code> because the container runtime instance (<code class="literal">crio</code>) is not working.
						</li></ul></div><p>
					Follow this process to completely wipe the CRI-O storage and resolve the errors.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites:</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Use <code class="literal">cordon</code> on the node. This is to avoid any workload getting scheduled if the node gets into the <code class="literal">Ready</code> status. You will know that scheduling is disabled when <code class="literal">SchedulingDisabled</code> is in your Status section:
						</p><pre class="programlisting language-terminal">$ oc adm cordon &lt;nodename&gt;</pre></li><li class="listitem"><p class="simpara">
							Drain the node as the cluster-admin user:
						</p><pre class="programlisting language-terminal">$ oc adm drain &lt;nodename&gt; --ignore-daemonsets --delete-emptydir-data</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The <code class="literal">terminationGracePeriodSeconds</code> attribute of a pod or pod template controls the graceful termination period. This attribute defaults at 30 seconds, but can be customized per application as necessary. If set to more than 90 seconds, the pod might be marked as <code class="literal">SIGKILLed</code> and fail to terminate successfully.
							</p></div></div></li><li class="listitem"><p class="simpara">
							When the node returns, connect back to the node via SSH or Console. Then connect to the root user:
						</p><pre class="programlisting language-terminal">$ ssh core@node1.example.com
$ sudo -i</pre></li><li class="listitem"><p class="simpara">
							Manually stop the kubelet:
						</p><pre class="programlisting language-terminal"># systemctl stop kubelet</pre></li><li class="listitem"><p class="simpara">
							Stop the containers and pods:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Use the following command to stop the pods that are not in the <code class="literal">HostNetwork</code>. They must be removed first because their removal relies on the networking plugin pods, which are in the <code class="literal">HostNetwork</code>.
								</p><pre class="programlisting language-terminal">.. for pod in $(crictl pods -q); do if [[ "$(crictl inspectp $pod | jq -r .status.linux.namespaces.options.network)" != "NODE" ]]; then crictl rmp -f $pod; fi; done</pre></li><li class="listitem"><p class="simpara">
									Stop all other pods:
								</p><pre class="programlisting language-terminal"># crictl rmp -fa</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Manually stop the crio services:
						</p><pre class="programlisting language-terminal"># systemctl stop crio</pre></li><li class="listitem"><p class="simpara">
							After you run those commands, you can completely wipe the ephemeral storage:
						</p><pre class="programlisting language-terminal"># crio wipe -f</pre></li><li class="listitem"><p class="simpara">
							Start the crio and kubelet service:
						</p><pre class="programlisting language-terminal"># systemctl start crio
# systemctl start kubelet</pre></li><li class="listitem"><p class="simpara">
							You will know if the clean up worked if the crio and kubelet services are started, and the node is in the <code class="literal">Ready</code> status:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME				    STATUS	                ROLES    AGE    VERSION
ci-ln-tkbxyft-f76d1-nvwhr-master-1  Ready, SchedulingDisabled   master	 133m   v1.26.0</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Mark the node schedulable. You will know that the scheduling is enabled when <code class="literal">SchedulingDisabled</code> is no longer in status:
						</p><pre class="programlisting language-terminal">$ oc adm uncordon &lt;nodename&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME				     STATUS	      ROLES    AGE    VERSION
ci-ln-tkbxyft-f76d1-nvwhr-master-1   Ready            master   133m   v1.26.0</pre>

							</p></div></li></ol></div></section></section><section class="section" id="troubleshooting-operating-system-issues"><div class="titlepage"><div><div><h2 class="title">7.4. Troubleshooting operating system issues</h2></div></div></div><p>
				OpenShift Container Platform runs on RHCOS. You can follow these procedures to troubleshoot problems related to the operating system.
			</p><section class="section" id="investigating-kernel-crashes"><div class="titlepage"><div><div><h3 class="title">7.4.1. Investigating kernel crashes</h3></div></div></div><p>
					The <code class="literal">kdump</code> service, included in the <code class="literal">kexec-tools</code> package, provides a crash-dumping mechanism. You can use this service to save the contents of a system’s memory for later analysis.
				</p><p>
					The <code class="literal">x86_64</code> architecture supports kdump in General Availability (GA) status, whereas other architectures support kdump in Technology Preview (TP) status.
				</p><p>
					The following table provides details about the support level of kdump for different architectures.
				</p><div class="table" id="idm140604668138976"><p class="title"><strong>Table 7.1. Kdump support in RHCOS</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140604665841152" scope="col">Architecture</th><th align="center" valign="top" id="idm140604665840064" scope="col">Support level</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140604665841152"> <p>
									<code class="literal">x86_64</code>
								</p>
								 </td><td align="center" valign="top" headers="idm140604665840064">
<div class="literallayout"><p> GA</p></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665841152"> <p>
									<code class="literal">aarch64</code>
								</p>
								 </td><td align="center" valign="top" headers="idm140604665840064">
<div class="literallayout"><p> TP</p></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665841152"> <p>
									<code class="literal">s390x</code>
								</p>
								 </td><td align="center" valign="top" headers="idm140604665840064">
<div class="literallayout"><p> TP</p></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665841152"> <p>
									<code class="literal">ppc64le</code>
								</p>
								 </td><td align="center" valign="top" headers="idm140604665840064">
<div class="literallayout"><p> TP</p></div>
								 </td></tr></tbody></table></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Kdump support, for the preceding three architectures in the table, is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
					</p><p>
						For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
					</p></div></div><section class="section" id="enabling-kdump"><div class="titlepage"><div><div><h4 class="title">7.4.1.1. Enabling kdump</h4></div></div></div><p>
						RHCOS ships with the <code class="literal">kexec-tools</code> package, but manual configuration is required to enable the <code class="literal">kdump</code> service.
					</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							Perform the following steps to enable kdump on RHCOS.
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								To reserve memory for the crash kernel during the first kernel booting, provide kernel arguments by entering the following command:
							</p><pre class="programlisting language-terminal"># rpm-ostree kargs --append='crashkernel=256M'</pre></li><li class="listitem"><p class="simpara">
								Optional: To write the crash dump over the network or to some other location, rather than to the default local <code class="literal">/var/crash</code> location, edit the <code class="literal">/etc/kdump.conf</code> configuration file.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									If your node uses LUKS-encrypted devices, you must use network dumps as kdump does not support saving crash dumps to LUKS-encrypted devices.
								</p></div></div><p class="simpara">
								For details on configuring the <code class="literal">kdump</code> service, see the comments in <code class="literal">/etc/sysconfig/kdump</code>, <code class="literal">/etc/kdump.conf</code>, and the <code class="literal">kdump.conf</code> manual page. Also refer to the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_monitoring_and_updating_the_kernel/configuring-kdump-on-the-command-line_managing-monitoring-and-updating-the-kernel">RHEL kdump documentation</a> for further information on configuring the dump target.
							</p></li><li class="listitem"><p class="simpara">
								Enable the <code class="literal">kdump</code> systemd service.
							</p><pre class="programlisting language-terminal"># systemctl enable kdump.service</pre></li><li class="listitem"><p class="simpara">
								Reboot your system.
							</p><pre class="programlisting language-terminal"># systemctl reboot</pre></li><li class="listitem">
								Ensure that kdump has loaded a crash kernel by checking that the <code class="literal">kdump.service</code> systemd service has started and exited successfully and that the command, <code class="literal">cat /sys/kernel/kexec_crash_loaded</code>, prints the value <code class="literal">1</code>.
							</li></ol></div></section><section class="section" id="enabling-kdump-day-one"><div class="titlepage"><div><div><h4 class="title">7.4.1.2. Enabling kdump on day-1</h4></div></div></div><p>
						The <code class="literal">kdump</code> service is intended to be enabled per node to debug kernel problems. Because there are costs to having kdump enabled, and these costs accumulate with each additional kdump-enabled node, it is recommended that the <code class="literal">kdump</code> service only be enabled on each node as needed. Potential costs of enabling the <code class="literal">kdump</code> service on each node include:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Less available RAM due to memory being reserved for the crash kernel.
							</li><li class="listitem">
								Node unavailability while the kernel is dumping the core.
							</li><li class="listitem">
								Additional storage space being used to store the crash dumps.
							</li></ul></div><p>
						If you are aware of the downsides and trade-offs of having the <code class="literal">kdump</code> service enabled, it is possible to enable kdump in a cluster-wide fashion. Although machine-specific machine configs are not yet supported, you can use a <code class="literal">systemd</code> unit in a <code class="literal">MachineConfig</code> object as a day-1 customization and have kdump enabled on all nodes in the cluster. You can create a <code class="literal">MachineConfig</code> object and inject that object into the set of manifest files used by Ignition during cluster setup.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							See "Customizing nodes" in the <span class="emphasis"><em>Installing → Installation configuration</em></span> section for more information and examples on how to use Ignition configs.
						</p></div></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							Create a <code class="literal">MachineConfig</code> object for cluster-wide configuration:
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a Butane config file, <code class="literal">99-worker-kdump.bu</code>, that configures and enables kdump:
							</p><pre class="programlisting language-yaml">variant: openshift
version: 4.13.0
metadata:
  name: 99-worker-kdump <span id="CO35-1"><!--Empty--></span><span class="callout">1</span>
  labels:
    machineconfiguration.openshift.io/role: worker <span id="CO35-2"><!--Empty--></span><span class="callout">2</span>
openshift:
  kernel_arguments: <span id="CO35-3"><!--Empty--></span><span class="callout">3</span>
    - crashkernel=256M
storage:
  files:
    - path: /etc/kdump.conf <span id="CO35-4"><!--Empty--></span><span class="callout">4</span>
      mode: 0644
      overwrite: true
      contents:
        inline: |
          path /var/crash
          core_collector makedumpfile -l --message-level 7 -d 31

    - path: /etc/sysconfig/kdump <span id="CO35-5"><!--Empty--></span><span class="callout">5</span>
      mode: 0644
      overwrite: true
      contents:
        inline: |
          KDUMP_COMMANDLINE_REMOVE="hugepages hugepagesz slub_debug quiet log_buf_len swiotlb"
          KDUMP_COMMANDLINE_APPEND="irqpoll nr_cpus=1 reset_devices cgroup_disable=memory mce=off numa=off udev.children-max=2 panic=10 rootflags=nofail acpi_no_memhotplug transparent_hugepage=never nokaslr novmcoredd hest_disable"
          KEXEC_ARGS="-s"
          KDUMP_IMG="vmlinuz"

systemd:
  units:
    - name: kdump.service
      enabled: true</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO35-1"><span class="callout">1</span></a> <a href="#CO35-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Replace <code class="literal">worker</code> with <code class="literal">master</code> in both locations when creating a <code class="literal">MachineConfig</code> object for control plane nodes.
									</div></dd><dt><a href="#CO35-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Provide kernel arguments to reserve memory for the crash kernel. You can add other kernel arguments if necessary.
									</div></dd><dt><a href="#CO35-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										If you want to change the contents of <code class="literal">/etc/kdump.conf</code> from the default, include this section and modify the <code class="literal">inline</code> subsection accordingly.
									</div></dd><dt><a href="#CO35-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										If you want to change the contents of <code class="literal">/etc/sysconfig/kdump</code> from the default, include this section and modify the <code class="literal">inline</code> subsection accordingly.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Use Butane to generate a machine config YAML file, <code class="literal">99-worker-kdump.yaml</code>, containing the configuration to be delivered to the nodes:
							</p><pre class="programlisting language-terminal">$ butane 99-worker-kdump.bu -o 99-worker-kdump.yaml</pre></li><li class="listitem"><p class="simpara">
								Put the YAML file into the <code class="literal">&lt;installation_directory&gt;/manifests/</code> directory during cluster setup. You can also create this <code class="literal">MachineConfig</code> object after cluster setup with the YAML file:
							</p><pre class="programlisting language-terminal">$ oc create -f 99-worker-kdump.yaml</pre></li></ol></div></section><section class="section" id="testing-kdump-configuration"><div class="titlepage"><div><div><h4 class="title">7.4.1.3. Testing the kdump configuration</h4></div></div></div><p>
						See the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_monitoring_and_updating_the_kernel/configuring-kdump-on-the-command-line_managing-monitoring-and-updating-the-kernel#testing-the-kdump-configuration_configuring-kdump-on-the-command-line">Testing the kdump configuration</a> section in the RHEL documentation for kdump.
					</p></section><section class="section" id="analyzing-core-dumps"><div class="titlepage"><div><div><h4 class="title">7.4.1.4. Analyzing a core dump</h4></div></div></div><p>
						See the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_monitoring_and_updating_the_kernel/analyzing-a-core-dump_managing-monitoring-and-updating-the-kernel">Analyzing a core dump</a> section in the RHEL documentation for kdump.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							It is recommended to perform vmcore analysis on a separate RHEL system.
						</p></div></div><h5 id="additional-resources_investigating-kernel-crashes">Additional resources</h5><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_monitoring_and_updating_the_kernel/configuring-kdump-on-the-command-line_managing-monitoring-and-updating-the-kernel">Setting up kdump in RHEL</a>
							</li><li class="listitem">
								<a class="link" href="https://www.kernel.org/doc/html/latest/admin-guide/kdump/kdump.html">Linux kernel documentation for kdump</a>
							</li><li class="listitem">
								kdump.conf(5) — a manual page for the <code class="literal">/etc/kdump.conf</code> configuration file containing the full documentation of available options
							</li><li class="listitem">
								kexec(8) — a manual page for the <code class="literal">kexec</code> package
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/site/solutions/6038">Red Hat Knowledgebase article</a> regarding kexec and kdump
							</li></ul></div></section></section><section class="section" id="debugging-ignition_troubleshooting-operating-system-issues"><div class="titlepage"><div><div><h3 class="title">7.4.2. Debugging Ignition failures</h3></div></div></div><p>
					If a machine cannot be provisioned, Ignition fails and RHCOS will boot into the emergency shell. Use the following procedure to get debugging information.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Run the following command to show which service units failed:
						</p><pre class="programlisting language-terminal">$ systemctl --failed</pre></li><li class="listitem"><p class="simpara">
							Optional: Run the following command on an individual service unit to find out more information:
						</p><pre class="programlisting language-terminal">$ journalctl -u &lt;unit&gt;.service</pre></li></ol></div></section></section><section class="section" id="troubleshooting-network-issues"><div class="titlepage"><div><div><h2 class="title">7.5. Troubleshooting network issues</h2></div></div></div><section class="section" id="nw-how-nw-iface-selected_troubleshooting-network-issues"><div class="titlepage"><div><div><h3 class="title">7.5.1. How the network interface is selected</h3></div></div></div><p>
					For installations on bare metal or with virtual machines that have more than one network interface controller (NIC), the NIC that OpenShift Container Platform uses for communication with the Kubernetes API server is determined by the <code class="literal">nodeip-configuration.service</code> service unit that is run by systemd when the node boots. The <code class="literal">nodeip-configuration.service</code> selects the IP from the interface associated with the default route.
				</p><p>
					After the <code class="literal">nodeip-configuration.service</code> service determines the correct NIC, the service creates the <code class="literal">/etc/systemd/system/kubelet.service.d/20-nodenet.conf</code> file. The <code class="literal">20-nodenet.conf</code> file sets the <code class="literal">KUBELET_NODE_IP</code> environment variable to the IP address that the service selected.
				</p><p>
					When the kubelet service starts, it reads the value of the environment variable from the <code class="literal">20-nodenet.conf</code> file and sets the IP address as the value of the <code class="literal">--node-ip</code> kubelet command-line argument. As a result, the kubelet service uses the selected IP address as the node IP address.
				</p><p>
					If hardware or networking is reconfigured after installation, or if there is a networking layout where the node IP should not come from the default route interface, it is possible for the <code class="literal">nodeip-configuration.service</code> service to select a different NIC after a reboot. In some cases, you might be able to detect that a different NIC is selected by reviewing the <code class="literal">INTERNAL-IP</code> column in the output from the <code class="literal">oc get nodes -o wide</code> command.
				</p><p>
					If network communication is disrupted or misconfigured because a different NIC is selected, you might receive the following error: <code class="literal">EtcdCertSignerControllerDegraded</code>. You can create a hint file that includes the <code class="literal">NODEIP_HINT</code> variable to override the default IP selection logic. For more information, see Optional: Overriding the default node IP selection logic.
				</p><section class="section" id="overriding-default-node-ip-selection-logic_troubleshooting-network-issues"><div class="titlepage"><div><div><h4 class="title">7.5.1.1. Optional: Overriding the default node IP selection logic</h4></div></div></div><p>
						To override the default IP selection logic, you can create a hint file that includes the <code class="literal">NODEIP_HINT</code> variable to override the default IP selection logic. Creating a hint file allows you to select a specific node IP address from the interface in the subnet of the IP address specified in the <code class="literal">NODEIP_HINT</code> variable.
					</p><p>
						For example, if a node has two interfaces, <code class="literal">eth0</code> with an address of <code class="literal">10.0.0.10/24</code>, and <code class="literal">eth1</code> with an address of <code class="literal">192.0.2.5/24</code>, and the default route points to <code class="literal">eth0</code> (<code class="literal">10.0.0.10</code>),the node IP address would normally use the <code class="literal">10.0.0.10</code> IP address.
					</p><p>
						Users can configure the <code class="literal">NODEIP_HINT</code> variable to point at a known IP in the subnet, for example, a subnet gateway such as <code class="literal">192.0.2.1</code> so that the other subnet, <code class="literal">192.0.2.0/24</code>, is selected. As a result, the <code class="literal">192.0.2.5</code> IP address on <code class="literal">eth1</code> is used for the node.
					</p><p>
						The following procedure shows how to override the default node IP selection logic.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Add a hint file to your <code class="literal">/etc/default/nodeip-configuration</code> file, for example:
							</p><pre class="programlisting language-text">NODEIP_HINT=192.0.2.1</pre><div class="admonition important"><div class="admonition_header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Do not use the exact IP address of a node as a hint, for example, <code class="literal">192.0.2.5</code>. Using the exact IP address of a node causes the node using the hint IP address to fail to configure correctly.
										</li><li class="listitem">
											The IP address in the hint file is only used to determine the correct subnet. It will not receive traffic as a result of appearing in the hint file.
										</li></ul></div></div></div></li><li class="listitem"><p class="simpara">
								Generate the <code class="literal">base-64</code> encoded content by running the following command:
							</p><pre class="programlisting language-terminal">$ echo -n 'NODEIP_HINT=192.0.2.1' | base64 -w0</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Tk9ERUlQX0hJTlQ9MTkyLjAuMCxxxx==</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Activate the hint by creating a machine config manifest for both <code class="literal">master</code> and <code class="literal">worker</code> roles before deploying the cluster:
							</p><div class="formalpara"><p class="title"><strong>99-nodeip-hint-master.yaml</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 99-nodeip-hint-master
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
      - contents:
          source: data:text/plain;charset=utf-8;base64,&lt;encoded_content&gt; <span id="CO36-1"><!--Empty--></span><span class="callout">1</span>
        mode: 0644
        overwrite: true
        path: /etc/default/nodeip-configuration</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO36-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;encoded_contents&gt;</code> with the base64-encoded content of the <code class="literal">/etc/default/nodeip-configuration</code> file, for example, <code class="literal">Tk9ERUlQX0hJTlQ9MTkyLjAuMCxxxx==</code>. Note that a space is not acceptable after the comma and before the encoded content.
									</div></dd></dl></div><div class="formalpara"><p class="title"><strong>99-nodeip-hint-worker.yaml</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
 labels:
   machineconfiguration.openshift.io/role: worker
   name: 99-nodeip-hint-worker
spec:
 config:
   ignition:
     version: 3.2.0
   storage:
     files:
     - contents:
         source: data:text/plain;charset=utf-8;base64,&lt;encoded_content&gt; <span id="CO37-1"><!--Empty--></span><span class="callout">1</span>
       mode: 0644
       overwrite: true
       path: /etc/default/nodeip-configuration</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO37-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;encoded_contents&gt;</code> with the base64-encoded content of the <code class="literal">/etc/default/nodeip-configuration</code> file, for example, <code class="literal">Tk9ERUlQX0hJTlQ9MTkyLjAuMCxxxx==</code>. Note that a space is not acceptable after the comma and before the encoded content.
									</div></dd></dl></div></li><li class="listitem">
								Save the manifest to the directory where you store your cluster configuration, for example, <code class="literal">~/clusterconfigs</code>.
							</li><li class="listitem">
								Deploy the cluster.
							</li></ol></div></section></section><section class="section" id="nw-troubleshoot-ovs_troubleshooting-network-issues"><div class="titlepage"><div><div><h3 class="title">7.5.2. Troubleshooting Open vSwitch issues</h3></div></div></div><p>
					To troubleshoot some Open vSwitch (OVS) issues, you might need to configure the log level to include more information.
				</p><p>
					If you modify the log level on a node temporarily, be aware that you can receive log messages from the machine config daemon on the node like the following example:
				</p><pre class="programlisting language-terminal">E0514 12:47:17.998892    2281 daemon.go:1350] content mismatch for file /etc/systemd/system/ovs-vswitchd.service: [Unit]</pre><p>
					To avoid the log messages related to the mismatch, revert the log level change after you complete your troubleshooting.
				</p><section class="section" id="configuring-ovs-log-level-temp_troubleshooting-network-issues"><div class="titlepage"><div><div><h4 class="title">7.5.2.1. Configuring the Open vSwitch log level temporarily</h4></div></div></div><p>
						For short-term troubleshooting, you can configure the Open vSwitch (OVS) log level temporarily. The following procedure does not require rebooting the node. In addition, the configuration change does not persist whenever you reboot the node.
					</p><p>
						After you perform this procedure to change the log level, you can receive log messages from the machine config daemon that indicate a content mismatch for the <code class="literal">ovs-vswitchd.service</code>. To avoid the log messages, repeat this procedure and set the log level to the original value.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Start a debug pod for a node:
							</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
								Set <code class="literal">/host</code> as the root directory within the debug shell. The debug pod mounts the root file system from the host in <code class="literal">/host</code> within the pod. By changing the root directory to <code class="literal">/host</code>, you can run binaries from the host file system:
							</p><pre class="programlisting language-terminal"># chroot /host</pre></li><li class="listitem"><p class="simpara">
								View the current syslog level for OVS modules:
							</p><pre class="programlisting language-terminal"># ovs-appctl vlog/list</pre><p class="simpara">
								The following example output shows the log level for syslog set to <code class="literal">info</code>.
							</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">                 console    syslog    file
                 -------    ------    ------
backtrace          OFF       INFO       INFO
bfd                OFF       INFO       INFO
bond               OFF       INFO       INFO
bridge             OFF       INFO       INFO
bundle             OFF       INFO       INFO
bundles            OFF       INFO       INFO
cfm                OFF       INFO       INFO
collectors         OFF       INFO       INFO
command_line       OFF       INFO       INFO
connmgr            OFF       INFO       INFO
conntrack          OFF       INFO       INFO
conntrack_tp       OFF       INFO       INFO
coverage           OFF       INFO       INFO
ct_dpif            OFF       INFO       INFO
daemon             OFF       INFO       INFO
daemon_unix        OFF       INFO       INFO
dns_resolve        OFF       INFO       INFO
dpdk               OFF       INFO       INFO
...</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Specify the log level in the <code class="literal">/etc/systemd/system/ovs-vswitchd.service.d/10-ovs-vswitchd-restart.conf</code> file:
							</p><pre class="programlisting language-text">Restart=always
ExecStartPre=-/bin/sh -c '/usr/bin/chown -R :$${OVS_USER_ID##*:} /var/lib/openvswitch'
ExecStartPre=-/bin/sh -c '/usr/bin/chown -R :$${OVS_USER_ID##*:} /etc/openvswitch'
ExecStartPre=-/bin/sh -c '/usr/bin/chown -R :$${OVS_USER_ID##*:} /run/openvswitch'
ExecStartPost=-/usr/bin/ovs-appctl vlog/set syslog:dbg
ExecReload=-/usr/bin/ovs-appctl vlog/set syslog:dbg</pre><p class="simpara">
								In the preceding example, the log level is set to <code class="literal">dbg</code>. Change the last two lines by setting <code class="literal">syslog:&lt;log_level&gt;</code> to <code class="literal">off</code>, <code class="literal">emer</code>, <code class="literal">err</code>, <code class="literal">warn</code>, <code class="literal">info</code>, or <code class="literal">dbg</code>. The <code class="literal">off</code> log level filters out all log messages.
							</p></li><li class="listitem"><p class="simpara">
								Restart the service:
							</p><pre class="programlisting language-terminal"># systemctl daemon-reload</pre><pre class="programlisting language-terminal"># systemctl restart ovs-vswitchd</pre></li></ol></div></section><section class="section" id="configuring-ovs-log-level-permanently_troubleshooting-network-issues"><div class="titlepage"><div><div><h4 class="title">7.5.2.2. Configuring the Open vSwitch log level permanently</h4></div></div></div><p>
						For long-term changes to the Open vSwitch (OVS) log level, you can change the log level permanently.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a file, such as <code class="literal">99-change-ovs-loglevel.yaml</code>, with a <code class="literal">MachineConfig</code> object like the following example:
							</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master  <span id="CO38-1"><!--Empty--></span><span class="callout">1</span>
  name: 99-change-ovs-loglevel
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
      - dropins:
        - contents: |
            [Service]
              ExecStartPost=-/usr/bin/ovs-appctl vlog/set syslog:dbg  <span id="CO38-2"><!--Empty--></span><span class="callout">2</span>
              ExecReload=-/usr/bin/ovs-appctl vlog/set syslog:dbg
          name: 20-ovs-vswitchd-restart.conf
        name: ovs-vswitchd.service</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO38-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										After you perform this procedure to configure control plane nodes, repeat the procedure and set the role to <code class="literal">worker</code> to configure worker nodes.
									</div></dd><dt><a href="#CO38-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Set the <code class="literal">syslog:&lt;log_level&gt;</code> value. Log levels are <code class="literal">off</code>, <code class="literal">emer</code>, <code class="literal">err</code>, <code class="literal">warn</code>, <code class="literal">info</code>, or <code class="literal">dbg</code>. Setting the value to <code class="literal">off</code> filters out all log messages.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Apply the machine config:
							</p><pre class="programlisting language-terminal">$ oc apply -f 99-change-ovs-loglevel.yaml</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#understanding-the-machine-config-operator">Understanding the Machine Config Operator</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#checking-mco-status_post-install-machine-configuration-tasks">Checking machine config pool status</a>
							</li></ul></div></section><section class="section" id="displaying-ovs-logs_troubleshooting-network-issues"><div class="titlepage"><div><div><h4 class="title">7.5.2.3. Displaying Open vSwitch logs</h4></div></div></div><p>
						Use the following procedure to display Open vSwitch (OVS) logs.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Run one of the following commands:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
										Display the logs by using the <code class="literal">oc</code> command from outside the cluster:
									</p><pre class="programlisting language-terminal">$ oc adm node-logs &lt;node_name&gt; -u ovs-vswitchd</pre></li><li class="listitem"><p class="simpara">
										Display the logs after logging on to a node in the cluster:
									</p><pre class="programlisting language-terminal"># journalctl -b -f -u ovs-vswitchd.service</pre><p class="simpara">
										One way to log on to a node is by using the <code class="literal">oc debug node/&lt;node_name&gt;</code> command.
									</p></li></ul></div></li></ul></div></section></section></section><section class="section" id="troubleshooting-operator-issues"><div class="titlepage"><div><div><h2 class="title">7.6. Troubleshooting Operator issues</h2></div></div></div><p>
				Operators are a method of packaging, deploying, and managing an OpenShift Container Platform application. They act like an extension of the software vendor’s engineering team, watching over an OpenShift Container Platform environment and using its current state to make decisions in real time. Operators are designed to handle upgrades seamlessly, react to failures automatically, and not take shortcuts, such as skipping a software backup process to save time.
			</p><p>
				OpenShift Container Platform 4.13 includes a default set of Operators that are required for proper functioning of the cluster. These default Operators are managed by the Cluster Version Operator (CVO).
			</p><p>
				As a cluster administrator, you can install application Operators from the OperatorHub using the OpenShift Container Platform web console or the CLI. You can then subscribe the Operator to one or more namespaces to make it available for developers on your cluster. Application Operators are managed by Operator Lifecycle Manager (OLM).
			</p><p>
				If you experience Operator issues, verify Operator subscription status. Check Operator pod health across the cluster and gather Operator logs for diagnosis.
			</p><section class="section" id="olm-status-conditions_troubleshooting-operator-issues"><div class="titlepage"><div><div><h3 class="title">7.6.1. Operator subscription condition types</h3></div></div></div><p>
					Subscriptions can report the following condition types:
				</p><div class="table" id="idm140604664232128"><p class="title"><strong>Table 7.2. Subscription condition types</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 67%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140604664227280" scope="col">Condition</th><th align="left" valign="top" id="idm140604664226192" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140604664227280"> <p>
									<code class="literal">CatalogSourcesUnhealthy</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604664226192"> <p>
									Some or all of the catalog sources to be used in resolution are unhealthy.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604664227280"> <p>
									<code class="literal">InstallPlanMissing</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604664226192"> <p>
									An install plan for a subscription is missing.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604664227280"> <p>
									<code class="literal">InstallPlanPending</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604664226192"> <p>
									An install plan for a subscription is pending installation.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604664227280"> <p>
									<code class="literal">InstallPlanFailed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604664226192"> <p>
									An install plan for a subscription has failed.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604664227280"> <p>
									<code class="literal">ResolutionFailed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604664226192"> <p>
									The dependency resolution for a subscription has failed.
								</p>
								 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Default OpenShift Container Platform cluster Operators are managed by the Cluster Version Operator (CVO) and they do not have a <code class="literal">Subscription</code> object. Application Operators are managed by Operator Lifecycle Manager (OLM) and they have a <code class="literal">Subscription</code> object.
					</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-cs-health_olm-understanding-olm">Catalog health requirements</a>
						</li></ul></div></section><section class="section" id="olm-status-viewing-cli_troubleshooting-operator-issues"><div class="titlepage"><div><div><h3 class="title">7.6.2. Viewing Operator subscription status by using the CLI</h3></div></div></div><p>
					You can view Operator subscription status by using the CLI.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							List Operator subscriptions:
						</p><pre class="programlisting language-terminal">$ oc get subs -n &lt;operator_namespace&gt;</pre></li><li class="listitem"><p class="simpara">
							Use the <code class="literal">oc describe</code> command to inspect a <code class="literal">Subscription</code> resource:
						</p><pre class="programlisting language-terminal">$ oc describe sub &lt;subscription_name&gt; -n &lt;operator_namespace&gt;</pre></li><li class="listitem"><p class="simpara">
							In the command output, find the <code class="literal">Conditions</code> section for the status of Operator subscription condition types. In the following example, the <code class="literal">CatalogSourcesUnhealthy</code> condition type has a status of <code class="literal">false</code> because all available catalog sources are healthy:
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Conditions:
   Last Transition Time:  2019-07-29T13:42:57Z
   Message:               all available catalogsources are healthy
   Reason:                AllCatalogSourcesHealthy
   Status:                False
   Type:                  CatalogSourcesUnhealthy</pre>

							</p></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Default OpenShift Container Platform cluster Operators are managed by the Cluster Version Operator (CVO) and they do not have a <code class="literal">Subscription</code> object. Application Operators are managed by Operator Lifecycle Manager (OLM) and they have a <code class="literal">Subscription</code> object.
					</p></div></div></section><section class="section" id="olm-cs-status-cli_troubleshooting-operator-issues"><div class="titlepage"><div><div><h3 class="title">7.6.3. Viewing Operator catalog source status by using the CLI</h3></div></div></div><p>
					You can view the status of an Operator catalog source by using the CLI.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							List the catalog sources in a namespace. For example, you can check the <code class="literal">openshift-marketplace</code> namespace, which is used for cluster-wide catalog sources:
						</p><pre class="programlisting language-terminal">$ oc get catalogsources -n openshift-marketplace</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                  DISPLAY               TYPE   PUBLISHER   AGE
certified-operators   Certified Operators   grpc   Red Hat     55m
community-operators   Community Operators   grpc   Red Hat     55m
example-catalog       Example Catalog       grpc   Example Org 2m25s
redhat-marketplace    Red Hat Marketplace   grpc   Red Hat     55m
redhat-operators      Red Hat Operators     grpc   Red Hat     55m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Use the <code class="literal">oc describe</code> command to get more details and status about a catalog source:
						</p><pre class="programlisting language-terminal">$ oc describe catalogsource example-catalog -n openshift-marketplace</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Name:         example-catalog
Namespace:    openshift-marketplace
...
Status:
  Connection State:
    Address:              example-catalog.openshift-marketplace.svc:50051
    Last Connect:         2021-09-09T17:07:35Z
    Last Observed State:  TRANSIENT_FAILURE
  Registry Service:
    Created At:         2021-09-09T17:05:45Z
    Port:               50051
    Protocol:           grpc
    Service Name:       example-catalog
    Service Namespace:  openshift-marketplace</pre>

							</p></div><p class="simpara">
							In the preceding example output, the last observed state is <code class="literal">TRANSIENT_FAILURE</code>. This state indicates that there is a problem establishing a connection for the catalog source.
						</p></li><li class="listitem"><p class="simpara">
							List the pods in the namespace where your catalog source was created:
						</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-marketplace</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                    READY   STATUS             RESTARTS   AGE
certified-operators-cv9nn               1/1     Running            0          36m
community-operators-6v8lp               1/1     Running            0          36m
marketplace-operator-86bfc75f9b-jkgbc   1/1     Running            0          42m
example-catalog-bwt8z                   0/1     ImagePullBackOff   0          3m55s
redhat-marketplace-57p8c                1/1     Running            0          36m
redhat-operators-smxx8                  1/1     Running            0          36m</pre>

							</p></div><p class="simpara">
							When a catalog source is created in a namespace, a pod for the catalog source is created in that namespace. In the preceding example output, the status for the <code class="literal">example-catalog-bwt8z</code> pod is <code class="literal">ImagePullBackOff</code>. This status indicates that there is an issue pulling the catalog source’s index image.
						</p></li><li class="listitem"><p class="simpara">
							Use the <code class="literal">oc describe</code> command to inspect a pod for more detailed information:
						</p><pre class="programlisting language-terminal">$ oc describe pod example-catalog-bwt8z -n openshift-marketplace</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Name:         example-catalog-bwt8z
Namespace:    openshift-marketplace
Priority:     0
Node:         ci-ln-jyryyg2-f76d1-ggdbq-worker-b-vsxjd/10.0.128.2
...
Events:
  Type     Reason          Age                From               Message
  ----     ------          ----               ----               -------
  Normal   Scheduled       48s                default-scheduler  Successfully assigned openshift-marketplace/example-catalog-bwt8z to ci-ln-jyryyf2-f76d1-fgdbq-worker-b-vsxjd
  Normal   AddedInterface  47s                multus             Add eth0 [10.131.0.40/23] from openshift-sdn
  Normal   BackOff         20s (x2 over 46s)  kubelet            Back-off pulling image "quay.io/example-org/example-catalog:v1"
  Warning  Failed          20s (x2 over 46s)  kubelet            Error: ImagePullBackOff
  Normal   Pulling         8s (x3 over 47s)   kubelet            Pulling image "quay.io/example-org/example-catalog:v1"
  Warning  Failed          8s (x3 over 47s)   kubelet            Failed to pull image "quay.io/example-org/example-catalog:v1": rpc error: code = Unknown desc = reading manifest v1 in quay.io/example-org/example-catalog: unauthorized: access to the requested resource is not authorized
  Warning  Failed          8s (x3 over 47s)   kubelet            Error: ErrImagePull</pre>

							</p></div><p class="simpara">
							In the preceding example output, the error messages indicate that the catalog source’s index image is failing to pull successfully because of an authorization issue. For example, the index image might be stored in a registry that requires login credentials.
						</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-catalogsource_olm-understanding-olm">Operator Lifecycle Manager concepts and resources → Catalog source</a>
						</li><li class="listitem">
							gRPC documentation: <a class="link" href="https://grpc.github.io/grpc/core/md_doc_connectivity-semantics-and-api.html">States of Connectivity</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-accessing-images-private-registries_olm-managing-custom-catalogs">Accessing images for Operators from private registries</a>
						</li></ul></div></section><section class="section" id="querying-operator-pod-status_troubleshooting-operator-issues"><div class="titlepage"><div><div><h3 class="title">7.6.4. Querying Operator pod status</h3></div></div></div><p>
					You can list Operator pods within a cluster and their status. You can also collect a detailed Operator pod summary.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Your API service is still functional.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							List Operators running in the cluster. The output includes Operator version, availability, and up-time information:
						</p><pre class="programlisting language-terminal">$ oc get clusteroperators</pre></li><li class="listitem"><p class="simpara">
							List Operator pods running in the Operator’s namespace, plus pod status, restarts, and age:
						</p><pre class="programlisting language-terminal">$ oc get pod -n &lt;operator_namespace&gt;</pre></li><li class="listitem"><p class="simpara">
							Output a detailed Operator pod summary:
						</p><pre class="programlisting language-terminal">$ oc describe pod &lt;operator_pod_name&gt; -n &lt;operator_namespace&gt;</pre></li><li class="listitem"><p class="simpara">
							If an Operator issue is node-specific, query Operator container status on that node.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Start a debug pod for the node:
								</p><pre class="programlisting language-terminal">$ oc debug node/my-node</pre></li><li class="listitem"><p class="simpara">
									Set <code class="literal">/host</code> as the root directory within the debug shell. The debug pod mounts the host’s root file system in <code class="literal">/host</code> within the pod. By changing the root directory to <code class="literal">/host</code>, you can run binaries contained in the host’s executable paths:
								</p><pre class="programlisting language-terminal"># chroot /host</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										OpenShift Container Platform 4.13 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes by using SSH is not recommended. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code class="literal">oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code class="literal">ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> instead.
									</p></div></div></li><li class="listitem"><p class="simpara">
									List details about the node’s containers, including state and associated pod IDs:
								</p><pre class="programlisting language-terminal"># crictl ps</pre></li><li class="listitem"><p class="simpara">
									List information about a specific Operator container on the node. The following example lists information about the <code class="literal">network-operator</code> container:
								</p><pre class="programlisting language-terminal"># crictl ps --name network-operator</pre></li><li class="listitem">
									Exit from the debug shell.
								</li></ol></div></li></ol></div></section><section class="section" id="gathering-operator-logs_troubleshooting-operator-issues"><div class="titlepage"><div><div><h3 class="title">7.6.5. Gathering Operator logs</h3></div></div></div><p>
					If you experience Operator issues, you can gather detailed diagnostic information from Operator pod logs.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Your API service is still functional.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have the fully qualified domain names of the control plane or control plane machines.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							List the Operator pods that are running in the Operator’s namespace, plus the pod status, restarts, and age:
						</p><pre class="programlisting language-terminal">$ oc get pods -n &lt;operator_namespace&gt;</pre></li><li class="listitem"><p class="simpara">
							Review logs for an Operator pod:
						</p><pre class="programlisting language-terminal">$ oc logs pod/&lt;pod_name&gt; -n &lt;operator_namespace&gt;</pre><p class="simpara">
							If an Operator pod has multiple containers, the preceding command will produce an error that includes the name of each container. Query logs from an individual container:
						</p><pre class="programlisting language-terminal">$ oc logs pod/&lt;operator_pod_name&gt; -c &lt;container_name&gt; -n &lt;operator_namespace&gt;</pre></li><li class="listitem"><p class="simpara">
							If the API is not functional, review Operator pod and container logs on each control plane node by using SSH instead. Replace <code class="literal">&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> with appropriate values.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									List pods on each control plane node:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; sudo crictl pods</pre></li><li class="listitem"><p class="simpara">
									For any Operator pods not showing a <code class="literal">Ready</code> status, inspect the pod’s status in detail. Replace <code class="literal">&lt;operator_pod_id&gt;</code> with the Operator pod’s ID listed in the output of the preceding command:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; sudo crictl inspectp &lt;operator_pod_id&gt;</pre></li><li class="listitem"><p class="simpara">
									List containers related to an Operator pod:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; sudo crictl ps --pod=&lt;operator_pod_id&gt;</pre></li><li class="listitem"><p class="simpara">
									For any Operator container not showing a <code class="literal">Ready</code> status, inspect the container’s status in detail. Replace <code class="literal">&lt;container_id&gt;</code> with a container ID listed in the output of the preceding command:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; sudo crictl inspect &lt;container_id&gt;</pre></li><li class="listitem"><p class="simpara">
									Review the logs for any Operator containers not showing a <code class="literal">Ready</code> status. Replace <code class="literal">&lt;container_id&gt;</code> with a container ID listed in the output of the preceding command:
								</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt; sudo crictl logs -f &lt;container_id&gt;</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										OpenShift Container Platform 4.13 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes by using SSH is not recommended. Before attempting to collect diagnostic data over SSH, review whether the data collected by running <code class="literal">oc adm must gather</code> and other <code class="literal">oc</code> commands is sufficient instead. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code class="literal">oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code class="literal">ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>.
									</p></div></div></li></ol></div></li></ol></div></section><section class="section" id="troubleshooting-disabling-autoreboot-mco_troubleshooting-operator-issues"><div class="titlepage"><div><div><h3 class="title">7.6.6. Disabling the Machine Config Operator from automatically rebooting</h3></div></div></div><p>
					When configuration changes are made by the Machine Config Operator (MCO), Red Hat Enterprise Linux CoreOS (RHCOS) must reboot for the changes to take effect. Whether the configuration change is automatic or manual, an RHCOS node reboots automatically unless it is paused.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The following modifications do not trigger a node reboot:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								When the MCO detects any of the following changes, it applies the update without draining or rebooting the node:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										Changes to the SSH key in the <code class="literal">spec.config.passwd.users.sshAuthorizedKeys</code> parameter of a machine config.
									</li><li class="listitem">
										Changes to the global pull secret or pull secret in the <code class="literal">openshift-config</code> namespace.
									</li><li class="listitem">
										Automatic rotation of the <code class="literal">/etc/kubernetes/kubelet-ca.crt</code> certificate authority (CA) by the Kubernetes API Server Operator.
									</li></ul></div></li><li class="listitem"><p class="simpara">
								When the MCO detects changes to the <code class="literal">/etc/containers/registries.conf</code> file, such as adding or editing an <code class="literal">ImageDigestMirrorSet</code> or <code class="literal">ImageTagMirrorSet</code> object, it drains the corresponding nodes, applies the changes, and uncordons the nodes.The node drain does not happen for the following changes:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										The addition of a registry with the <code class="literal">pull-from-mirror = "digest-only"</code> parameter set for each mirror.
									</li><li class="listitem">
										The addition of a mirror with the <code class="literal">pull-from-mirror = "digest-only"</code> parameter set in a registry.
									</li><li class="listitem">
										The addition of items to the <code class="literal">unqualified-search-registries</code> list.
									</li></ul></div></li></ul></div></div></div><p>
					To avoid unwanted disruptions, you can modify the machine config pool (MCP) to prevent automatic rebooting after the Operator makes changes to the machine config.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Pausing an MCP prevents the MCO from applying any configuration changes on the associated nodes. Pausing an MCP also prevents any automatically rotated certificates from being pushed to the associated nodes, including the automatic rotation of the <code class="literal">kube-apiserver-to-kubelet-signer</code> CA certificate.
					</p><p>
						If the MCP is paused when the <code class="literal">kube-apiserver-to-kubelet-signer</code> CA certificate expires, and the MCO attempts to renew the certificate automatically, the MCO cannot push the newly rotated certificates to those nodes. This causes the cluster to become degraded and causes failure in multiple <code class="literal">oc</code> commands, including <code class="literal">oc debug</code>, <code class="literal">oc logs</code>, <code class="literal">oc exec</code>, and <code class="literal">oc attach</code>. You receive alerts in the Alerting UI of the OpenShift Container Platform web console if an MCP is paused when the certificates are rotated.
					</p><p>
						Pausing an MCP should be done with careful consideration about the <code class="literal">kube-apiserver-to-kubelet-signer</code> CA certificate expiration and for short periods of time only.
					</p><p>
						New CA certificates are generated at 292 days from the installation date and removed at 365 days from that date. To determine the next automatic CA certificate rotation, see the <a class="link" href="https://access.redhat.com/articles/5651701">Understand CA cert auto renewal in Red Hat OpenShift 4</a>.
					</p></div></div><section class="section" id="troubleshooting-disabling-autoreboot-mco-console_troubleshooting-operator-issues"><div class="titlepage"><div><div><h4 class="title">7.6.6.1. Disabling the Machine Config Operator from automatically rebooting by using the console</h4></div></div></div><p>
						To avoid unwanted disruptions from changes made by the Machine Config Operator (MCO), you can use the OpenShift Container Platform web console to modify the machine config pool (MCP) to prevent the MCO from making any changes to nodes in that pool. This prevents any reboots that would normally be part of the MCO update process.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							See second <code class="literal">NOTE</code> in <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#troubleshooting-disabling-autoreboot-mco_troubleshooting-operator-issues">Disabling the Machine Config Operator from automatically rebooting</a>.
						</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To pause or unpause automatic MCO update rebooting:
						</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Pause the autoreboot process:
							</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
										Log in to the OpenShift Container Platform web console as a user with the <code class="literal">cluster-admin</code> role.
									</li><li class="listitem">
										Click <span class="strong strong"><strong>Compute</strong></span> → <span class="strong strong"><strong>MachineConfigPools</strong></span>.
									</li><li class="listitem">
										On the <span class="strong strong"><strong>MachineConfigPools</strong></span> page, click either <span class="strong strong"><strong>master</strong></span> or <span class="strong strong"><strong>worker</strong></span>, depending upon which nodes you want to pause rebooting for.
									</li><li class="listitem">
										On the <span class="strong strong"><strong>master</strong></span> or <span class="strong strong"><strong>worker</strong></span> page, click <span class="strong strong"><strong>YAML</strong></span>.
									</li><li class="listitem"><p class="simpara">
										In the YAML, update the <code class="literal">spec.paused</code> field to <code class="literal">true</code>.
									</p><div class="formalpara"><p class="title"><strong>Sample MachineConfigPool object</strong></p><p>
											
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
 ...
spec:
 ...
  paused: true <span id="CO39-1"><!--Empty--></span><span class="callout">1</span></pre>

										</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO39-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Update the <code class="literal">spec.paused</code> field to <code class="literal">true</code> to pause rebooting.
											</div></dd></dl></div></li><li class="listitem"><p class="simpara">
										To verify that the MCP is paused, return to the <span class="strong strong"><strong>MachineConfigPools</strong></span> page.
									</p><p class="simpara">
										On the <span class="strong strong"><strong>MachineConfigPools</strong></span> page, the <span class="strong strong"><strong>Paused</strong></span> column reports <span class="strong strong"><strong>True</strong></span> for the MCP you modified.
									</p><p class="simpara">
										If the MCP has pending changes while paused, the <span class="strong strong"><strong>Updated</strong></span> column is <span class="strong strong"><strong>False</strong></span> and <span class="strong strong"><strong>Updating</strong></span> is <span class="strong strong"><strong>False</strong></span>. When <span class="strong strong"><strong>Updated</strong></span> is <span class="strong strong"><strong>True</strong></span> and <span class="strong strong"><strong>Updating</strong></span> is <span class="strong strong"><strong>False</strong></span>, there are no pending changes.
									</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
											If there are pending changes (where both the <span class="strong strong"><strong>Updated</strong></span> and <span class="strong strong"><strong>Updating</strong></span> columns are <span class="strong strong"><strong>False</strong></span>), it is recommended to schedule a maintenance window for a reboot as early as possible. Use the following steps for unpausing the autoreboot process to apply the changes that were queued since the last reboot.
										</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
								Unpause the autoreboot process:
							</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
										Log in to the OpenShift Container Platform web console as a user with the <code class="literal">cluster-admin</code> role.
									</li><li class="listitem">
										Click <span class="strong strong"><strong>Compute</strong></span> → <span class="strong strong"><strong>MachineConfigPools</strong></span>.
									</li><li class="listitem">
										On the <span class="strong strong"><strong>MachineConfigPools</strong></span> page, click either <span class="strong strong"><strong>master</strong></span> or <span class="strong strong"><strong>worker</strong></span>, depending upon which nodes you want to pause rebooting for.
									</li><li class="listitem">
										On the <span class="strong strong"><strong>master</strong></span> or <span class="strong strong"><strong>worker</strong></span> page, click <span class="strong strong"><strong>YAML</strong></span>.
									</li><li class="listitem"><p class="simpara">
										In the YAML, update the <code class="literal">spec.paused</code> field to <code class="literal">false</code>.
									</p><div class="formalpara"><p class="title"><strong>Sample MachineConfigPool object</strong></p><p>
											
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
 ...
spec:
 ...
  paused: false <span id="CO40-1"><!--Empty--></span><span class="callout">1</span></pre>

										</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO40-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Update the <code class="literal">spec.paused</code> field to <code class="literal">false</code> to allow rebooting.
											</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											By unpausing an MCP, the MCO applies all paused changes reboots Red Hat Enterprise Linux CoreOS (RHCOS) as needed.
										</p></div></div></li><li class="listitem"><p class="simpara">
										To verify that the MCP is paused, return to the <span class="strong strong"><strong>MachineConfigPools</strong></span> page.
									</p><p class="simpara">
										On the <span class="strong strong"><strong>MachineConfigPools</strong></span> page, the <span class="strong strong"><strong>Paused</strong></span> column reports <span class="strong strong"><strong>False</strong></span> for the MCP you modified.
									</p><p class="simpara">
										If the MCP is applying any pending changes, the <span class="strong strong"><strong>Updated</strong></span> column is <span class="strong strong"><strong>False</strong></span> and the <span class="strong strong"><strong>Updating</strong></span> column is <span class="strong strong"><strong>True</strong></span>. When <span class="strong strong"><strong>Updated</strong></span> is <span class="strong strong"><strong>True</strong></span> and <span class="strong strong"><strong>Updating</strong></span> is <span class="strong strong"><strong>False</strong></span>, there are no further changes being made.
									</p></li></ol></div></li></ul></div></section><section class="section" id="troubleshooting-disabling-autoreboot-mco-cli_troubleshooting-operator-issues"><div class="titlepage"><div><div><h4 class="title">7.6.6.2. Disabling the Machine Config Operator from automatically rebooting by using the CLI</h4></div></div></div><p>
						To avoid unwanted disruptions from changes made by the Machine Config Operator (MCO), you can modify the machine config pool (MCP) using the OpenShift CLI (oc) to prevent the MCO from making any changes to nodes in that pool. This prevents any reboots that would normally be part of the MCO update process.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							See second <code class="literal">NOTE</code> in <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#troubleshooting-disabling-autoreboot-mco_troubleshooting-operator-issues">Disabling the Machine Config Operator from automatically rebooting</a>.
						</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To pause or unpause automatic MCO update rebooting:
						</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Pause the autoreboot process:
							</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
										Update the <code class="literal">MachineConfigPool</code> custom resource to set the <code class="literal">spec.paused</code> field to <code class="literal">true</code>.
									</p><div class="formalpara"><p class="title"><strong>Control plane (master) nodes</strong></p><p>
											
<pre class="programlisting language-terminal">$ oc patch --type=merge --patch='{"spec":{"paused":true}}' machineconfigpool/master</pre>

										</p></div><div class="formalpara"><p class="title"><strong>Worker nodes</strong></p><p>
											
<pre class="programlisting language-terminal">$ oc patch --type=merge --patch='{"spec":{"paused":true}}' machineconfigpool/worker</pre>

										</p></div></li><li class="listitem"><p class="simpara">
										Verify that the MCP is paused:
									</p><div class="formalpara"><p class="title"><strong>Control plane (master) nodes</strong></p><p>
											
<pre class="programlisting language-terminal">$ oc get machineconfigpool/master --template='{{.spec.paused}}'</pre>

										</p></div><div class="formalpara"><p class="title"><strong>Worker nodes</strong></p><p>
											
<pre class="programlisting language-terminal">$ oc get machineconfigpool/worker --template='{{.spec.paused}}'</pre>

										</p></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">true</pre>

										</p></div><p class="simpara">
										The <code class="literal">spec.paused</code> field is <code class="literal">true</code> and the MCP is paused.
									</p></li><li class="listitem"><p class="simpara">
										Determine if the MCP has pending changes:
									</p><pre class="programlisting language-terminal"># oc get machineconfigpool</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="screen">NAME     CONFIG                                             UPDATED   UPDATING
master   rendered-master-33cf0a1254318755d7b48002c597bf91   True      False
worker   rendered-worker-e405a5bdb0db1295acea08bcca33fa60   False     False</pre>

										</p></div><p class="simpara">
										If the <span class="strong strong"><strong>UPDATED</strong></span> column is <span class="strong strong"><strong>False</strong></span> and <span class="strong strong"><strong>UPDATING</strong></span> is <span class="strong strong"><strong>False</strong></span>, there are pending changes. When <span class="strong strong"><strong>UPDATED</strong></span> is <span class="strong strong"><strong>True</strong></span> and <span class="strong strong"><strong>UPDATING</strong></span> is <span class="strong strong"><strong>False</strong></span>, there are no pending changes. In the previous example, the worker node has pending changes. The control plane node does not have any pending changes.
									</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
											If there are pending changes (where both the <span class="strong strong"><strong>Updated</strong></span> and <span class="strong strong"><strong>Updating</strong></span> columns are <span class="strong strong"><strong>False</strong></span>), it is recommended to schedule a maintenance window for a reboot as early as possible. Use the following steps for unpausing the autoreboot process to apply the changes that were queued since the last reboot.
										</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
								Unpause the autoreboot process:
							</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
										Update the <code class="literal">MachineConfigPool</code> custom resource to set the <code class="literal">spec.paused</code> field to <code class="literal">false</code>.
									</p><div class="formalpara"><p class="title"><strong>Control plane (master) nodes</strong></p><p>
											
<pre class="programlisting language-terminal">$ oc patch --type=merge --patch='{"spec":{"paused":false}}' machineconfigpool/master</pre>

										</p></div><div class="formalpara"><p class="title"><strong>Worker nodes</strong></p><p>
											
<pre class="programlisting language-terminal">$ oc patch --type=merge --patch='{"spec":{"paused":false}}' machineconfigpool/worker</pre>

										</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											By unpausing an MCP, the MCO applies all paused changes and reboots Red Hat Enterprise Linux CoreOS (RHCOS) as needed.
										</p></div></div></li><li class="listitem"><p class="simpara">
										Verify that the MCP is unpaused:
									</p><div class="formalpara"><p class="title"><strong>Control plane (master) nodes</strong></p><p>
											
<pre class="programlisting language-terminal">$ oc get machineconfigpool/master --template='{{.spec.paused}}'</pre>

										</p></div><div class="formalpara"><p class="title"><strong>Worker nodes</strong></p><p>
											
<pre class="programlisting language-terminal">$ oc get machineconfigpool/worker --template='{{.spec.paused}}'</pre>

										</p></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">false</pre>

										</p></div><p class="simpara">
										The <code class="literal">spec.paused</code> field is <code class="literal">false</code> and the MCP is unpaused.
									</p></li><li class="listitem"><p class="simpara">
										Determine if the MCP has pending changes:
									</p><pre class="programlisting language-terminal">$ oc get machineconfigpool</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="screen">NAME     CONFIG                                   UPDATED  UPDATING
master   rendered-master-546383f80705bd5aeaba93   True     False
worker   rendered-worker-b4c51bb33ccaae6fc4a6a5   False    True</pre>

										</p></div><p class="simpara">
										If the MCP is applying any pending changes, the <span class="strong strong"><strong>UPDATED</strong></span> column is <span class="strong strong"><strong>False</strong></span> and the <span class="strong strong"><strong>UPDATING</strong></span> column is <span class="strong strong"><strong>True</strong></span>. When <span class="strong strong"><strong>UPDATED</strong></span> is <span class="strong strong"><strong>True</strong></span> and <span class="strong strong"><strong>UPDATING</strong></span> is <span class="strong strong"><strong>False</strong></span>, there are no further changes being made. In the previous example, the MCO is updating the worker node.
									</p></li></ol></div></li></ul></div></section></section><section class="section" id="olm-refresh-subs_troubleshooting-operator-issues"><div class="titlepage"><div><div><h3 class="title">7.6.7. Refreshing failing subscriptions</h3></div></div></div><p>
					In Operator Lifecycle Manager (OLM), if you subscribe to an Operator that references images that are not accessible on your network, you can find jobs in the <code class="literal">openshift-marketplace</code> namespace that are failing with the following errors:
				</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">ImagePullBackOff for
Back-off pulling image "example.com/openshift4/ose-elasticsearch-operator-bundle@sha256:6d2587129c846ec28d384540322b40b05833e7e00b25cca584e004af9a1d292e"</pre>

					</p></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">rpc error: code = Unknown desc = error pinging docker registry example.com: Get "https://example.com/v2/": dial tcp: lookup example.com on 10.0.0.1:53: no such host</pre>

					</p></div><p>
					As a result, the subscription is stuck in this failing state and the Operator is unable to install or upgrade.
				</p><p>
					You can refresh a failing subscription by deleting the subscription, cluster service version (CSV), and other related objects. After recreating the subscription, OLM then reinstalls the correct version of the Operator.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have a failing subscription that is unable to pull an inaccessible bundle image.
						</li><li class="listitem">
							You have confirmed that the correct bundle image is accessible.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Get the names of the <code class="literal">Subscription</code> and <code class="literal">ClusterServiceVersion</code> objects from the namespace where the Operator is installed:
						</p><pre class="programlisting language-terminal">$ oc get sub,csv -n &lt;namespace&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                       PACKAGE                  SOURCE             CHANNEL
subscription.operators.coreos.com/elasticsearch-operator   elasticsearch-operator   redhat-operators   5.0

NAME                                                                         DISPLAY                            VERSION    REPLACES   PHASE
clusterserviceversion.operators.coreos.com/elasticsearch-operator.5.0.0-65   OpenShift Elasticsearch Operator   5.0.0-65              Succeeded</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Delete the subscription:
						</p><pre class="programlisting language-terminal">$ oc delete subscription &lt;subscription_name&gt; -n &lt;namespace&gt;</pre></li><li class="listitem"><p class="simpara">
							Delete the cluster service version:
						</p><pre class="programlisting language-terminal">$ oc delete csv &lt;csv_name&gt; -n &lt;namespace&gt;</pre></li><li class="listitem"><p class="simpara">
							Get the names of any failing jobs and related config maps in the <code class="literal">openshift-marketplace</code> namespace:
						</p><pre class="programlisting language-terminal">$ oc get job,configmap -n openshift-marketplace</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                                        COMPLETIONS   DURATION   AGE
job.batch/1de9443b6324e629ddf31fed0a853a121275806170e34c926d69e53a7fcbccb   1/1           26s        9m30s

NAME                                                                        DATA   AGE
configmap/1de9443b6324e629ddf31fed0a853a121275806170e34c926d69e53a7fcbccb   3      9m30s</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Delete the job:
						</p><pre class="programlisting language-terminal">$ oc delete job &lt;job_name&gt; -n openshift-marketplace</pre><p class="simpara">
							This ensures pods that try to pull the inaccessible image are not recreated.
						</p></li><li class="listitem"><p class="simpara">
							Delete the config map:
						</p><pre class="programlisting language-terminal">$ oc delete configmap &lt;configmap_name&gt; -n openshift-marketplace</pre></li><li class="listitem">
							Reinstall the Operator using OperatorHub in the web console.
						</li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Check that the Operator has been reinstalled successfully:
						</p><pre class="programlisting language-terminal">$ oc get sub,csv,installplan -n &lt;namespace&gt;</pre></li></ul></div></section><section class="section" id="olm-reinstall_troubleshooting-operator-issues"><div class="titlepage"><div><div><h3 class="title">7.6.8. Reinstalling Operators after failed uninstallation</h3></div></div></div><p>
					You must successfully and completely uninstall an Operator prior to attempting to reinstall the same Operator. Failure to fully uninstall the Operator properly can leave resources, such as a project or namespace, stuck in a "Terminating" state and cause "error resolving resource" messages. For example:
				</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">Project</code> resource description</strong></p><p>
						
<pre class="screen">...
    message: 'Failed to delete all resource types, 1 remaining: Internal error occurred:
      error resolving resource'
...</pre>

					</p></div><p>
					These types of issues can prevent an Operator from being reinstalled successfully.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Forced deletion of a namespace is not likely to resolve "Terminating" state issues and can lead to unstable or unpredictable cluster behavior, so it is better to try to find related resources that might be preventing the namespace from being deleted. For more information, see the <a class="link" href="https://access.redhat.com/solutions/4165791">Red Hat Knowledgebase Solution #4165791</a>, paying careful attention to the cautions and warnings.
					</p></div></div><p>
					The following procedure shows how to troubleshoot when an Operator cannot be reinstalled because an existing custom resource definition (CRD) from a previous installation of the Operator is preventing a related namespace from deleting successfully.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check if there are any namespaces related to the Operator that are stuck in "Terminating" state:
						</p><pre class="programlisting language-terminal">$ oc get namespaces</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">operator-ns-1                                       Terminating</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check if there are any CRDs related to the Operator that are still present after the failed uninstallation:
						</p><pre class="programlisting language-terminal">$ oc get crds</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								CRDs are global cluster definitions; the actual custom resource (CR) instances related to the CRDs could be in other namespaces or be global cluster instances.
							</p></div></div></li><li class="listitem"><p class="simpara">
							If there are any CRDs that you know were provided or managed by the Operator and that should have been deleted after uninstallation, delete the CRD:
						</p><pre class="programlisting language-terminal">$ oc delete crd &lt;crd_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Check if there are any remaining CR instances related to the Operator that are still present after uninstallation, and if so, delete the CRs:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									The type of CRs to search for can be difficult to determine after uninstallation and can require knowing what CRDs the Operator manages. For example, if you are troubleshooting an uninstallation of the etcd Operator, which provides the <code class="literal">EtcdCluster</code> CRD, you can search for remaining <code class="literal">EtcdCluster</code> CRs in a namespace:
								</p><pre class="programlisting language-terminal">$ oc get EtcdCluster -n &lt;namespace_name&gt;</pre><p class="simpara">
									Alternatively, you can search across all namespaces:
								</p><pre class="programlisting language-terminal">$ oc get EtcdCluster --all-namespaces</pre></li><li class="listitem"><p class="simpara">
									If there are any remaining CRs that should be removed, delete the instances:
								</p><pre class="programlisting language-terminal">$ oc delete &lt;cr_name&gt; &lt;cr_instance_name&gt; -n &lt;namespace_name&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Check that the namespace deletion has successfully resolved:
						</p><pre class="programlisting language-terminal">$ oc get namespace &lt;namespace_name&gt;</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								If the namespace or other Operator resources are still not uninstalled cleanly, contact Red Hat Support.
							</p></div></div></li><li class="listitem">
							Reinstall the Operator using OperatorHub in the web console.
						</li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Check that the Operator has been reinstalled successfully:
						</p><pre class="programlisting language-terminal">$ oc get sub,csv,installplan -n &lt;namespace&gt;</pre></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-deleting-operators-from-a-cluster">Deleting Operators from a cluster</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-adding-operators-to-a-cluster">Adding Operators to a cluster</a>
						</li></ul></div></section></section><section class="section" id="investigating-pod-issues"><div class="titlepage"><div><div><h2 class="title">7.7. Investigating pod issues</h2></div></div></div><p>
				OpenShift Container Platform leverages the Kubernetes concept of a pod, which is one or more containers deployed together on one host. A pod is the smallest compute unit that can be defined, deployed, and managed on OpenShift Container Platform 4.13.
			</p><p>
				After a pod is defined, it is assigned to run on a node until its containers exit, or until it is removed. Depending on policy and exit code, Pods are either removed after exiting or retained so that their logs can be accessed.
			</p><p>
				The first thing to check when pod issues arise is the pod’s status. If an explicit pod failure has occurred, observe the pod’s error state to identify specific image, container, or pod network issues. Focus diagnostic data collection according to the error state. Review pod event messages, as well as pod and container log information. Diagnose issues dynamically by accessing running Pods on the command line, or start a debug pod with root access based on a problematic pod’s deployment configuration.
			</p><section class="section" id="understanding-pod-error-states_investigating-pod-issues"><div class="titlepage"><div><div><h3 class="title">7.7.1. Understanding pod error states</h3></div></div></div><p>
					Pod failures return explicit error states that can be observed in the <code class="literal">status</code> field in the output of <code class="literal">oc get pods</code>. Pod error states cover image, container, and container network related failures.
				</p><p>
					The following table provides a list of pod error states along with their descriptions.
				</p><div class="table" id="idm140604665879696"><p class="title"><strong>Table 7.3. Pod error states</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140604665874864" scope="col">Pod error state</th><th align="left" valign="top" id="idm140604665873776" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140604665874864"> <p>
									<code class="literal">ErrImagePull</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604665873776"> <p>
									Generic image retrieval error.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665874864"> <p>
									<code class="literal">ErrImagePullBackOff</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604665873776"> <p>
									Image retrieval failed and is backed off.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665874864"> <p>
									<code class="literal">ErrInvalidImageName</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604665873776"> <p>
									The specified image name was invalid.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665874864"> <p>
									<code class="literal">ErrImageInspect</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604665873776"> <p>
									Image inspection did not succeed.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665874864"> <p>
									<code class="literal">ErrImageNeverPull</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604665873776"> <p>
									<code class="literal">PullPolicy</code> is set to <code class="literal">NeverPullImage</code> and the target image is not present locally on the host.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665874864"> <p>
									<code class="literal">ErrRegistryUnavailable</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604665873776"> <p>
									When attempting to retrieve an image from a registry, an HTTP error was encountered.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665874864"> <p>
									<code class="literal">ErrContainerNotFound</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604665873776"> <p>
									The specified container is either not present or not managed by the kubelet, within the declared pod.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665874864"> <p>
									<code class="literal">ErrRunInitContainer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604665873776"> <p>
									Container initialization failed.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665874864"> <p>
									<code class="literal">ErrRunContainer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604665873776"> <p>
									None of the pod’s containers started successfully.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665874864"> <p>
									<code class="literal">ErrKillContainer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604665873776"> <p>
									None of the pod’s containers were killed successfully.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665874864"> <p>
									<code class="literal">ErrCrashLoopBackOff</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604665873776"> <p>
									A container has terminated. The kubelet will not attempt to restart it.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665874864"> <p>
									<code class="literal">ErrVerifyNonRoot</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604665873776"> <p>
									A container or image attempted to run with root privileges.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665874864"> <p>
									<code class="literal">ErrCreatePodSandbox</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604665873776"> <p>
									Pod sandbox creation did not succeed.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665874864"> <p>
									<code class="literal">ErrConfigPodSandbox</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604665873776"> <p>
									Pod sandbox configuration was not obtained.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665874864"> <p>
									<code class="literal">ErrKillPodSandbox</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604665873776"> <p>
									A pod sandbox did not stop successfully.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665874864"> <p>
									<code class="literal">ErrSetupNetwork</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604665873776"> <p>
									Network initialization failed.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665874864"> <p>
									<code class="literal">ErrTeardownNetwork</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140604665873776"> <p>
									Network termination failed.
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section" id="reviewing-pod-status_investigating-pod-issues"><div class="titlepage"><div><div><h3 class="title">7.7.2. Reviewing pod status</h3></div></div></div><p>
					You can query pod status and error states. You can also query a pod’s associated deployment configuration and review base image availability.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							<code class="literal">skopeo</code> is installed.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Switch into a project:
						</p><pre class="programlisting language-terminal">$ oc project &lt;project_name&gt;</pre></li><li class="listitem"><p class="simpara">
							List pods running within the namespace, as well as pod status, error states, restarts, and age:
						</p><pre class="programlisting language-terminal">$ oc get pods</pre></li><li class="listitem"><p class="simpara">
							Determine whether the namespace is managed by a deployment configuration:
						</p><pre class="programlisting language-terminal">$ oc status</pre><p class="simpara">
							If the namespace is managed by a deployment configuration, the output includes the deployment configuration name and a base image reference.
						</p></li><li class="listitem"><p class="simpara">
							Inspect the base image referenced in the preceding command’s output:
						</p><pre class="programlisting language-terminal">$ skopeo inspect docker://&lt;image_reference&gt;</pre></li><li class="listitem"><p class="simpara">
							If the base image reference is not correct, update the reference in the deployment configuration:
						</p><pre class="programlisting language-terminal">$ oc edit deployment/my-deployment</pre></li><li class="listitem"><p class="simpara">
							When deployment configuration changes on exit, the configuration will automatically redeploy. Watch pod status as the deployment progresses, to determine whether the issue has been resolved:
						</p><pre class="programlisting language-terminal">$ oc get pods -w</pre></li><li class="listitem"><p class="simpara">
							Review events within the namespace for diagnostic information relating to pod failures:
						</p><pre class="programlisting language-terminal">$ oc get events</pre></li></ol></div></section><section class="section" id="inspecting-pod-and-container-logs_investigating-pod-issues"><div class="titlepage"><div><div><h3 class="title">7.7.3. Inspecting pod and container logs</h3></div></div></div><p>
					You can inspect pod and container logs for warnings and error messages related to explicit pod failures. Depending on policy and exit code, pod and container logs remain available after pods have been terminated.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Your API service is still functional.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Query logs for a specific pod:
						</p><pre class="programlisting language-terminal">$ oc logs &lt;pod_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Query logs for a specific container within a pod:
						</p><pre class="programlisting language-terminal">$ oc logs &lt;pod_name&gt; -c &lt;container_name&gt;</pre><p class="simpara">
							Logs retrieved using the preceding <code class="literal">oc logs</code> commands are composed of messages sent to stdout within pods or containers.
						</p></li><li class="listitem"><p class="simpara">
							Inspect logs contained in <code class="literal">/var/log/</code> within a pod.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									List log files and subdirectories contained in <code class="literal">/var/log</code> within a pod:
								</p><pre class="programlisting language-terminal">$ oc exec &lt;pod_name&gt; ls -alh /var/log</pre></li><li class="listitem"><p class="simpara">
									Query a specific log file contained in <code class="literal">/var/log</code> within a pod:
								</p><pre class="programlisting language-terminal">$ oc exec &lt;pod_name&gt; cat /var/log/&lt;path_to_log&gt;</pre></li><li class="listitem"><p class="simpara">
									List log files and subdirectories contained in <code class="literal">/var/log</code> within a specific container:
								</p><pre class="programlisting language-terminal">$ oc exec &lt;pod_name&gt; -c &lt;container_name&gt; ls /var/log</pre></li><li class="listitem"><p class="simpara">
									Query a specific log file contained in <code class="literal">/var/log</code> within a specific container:
								</p><pre class="programlisting language-terminal">$ oc exec &lt;pod_name&gt; -c &lt;container_name&gt; cat /var/log/&lt;path_to_log&gt;</pre></li></ol></div></li></ol></div></section><section class="section" id="accessing-running-pods_investigating-pod-issues"><div class="titlepage"><div><div><h3 class="title">7.7.4. Accessing running pods</h3></div></div></div><p>
					You can review running pods dynamically by opening a shell inside a pod or by gaining network access through port forwarding.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Your API service is still functional.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Switch into the project that contains the pod you would like to access. This is necessary because the <code class="literal">oc rsh</code> command does not accept the <code class="literal">-n</code> namespace option:
						</p><pre class="programlisting language-terminal">$ oc project &lt;namespace&gt;</pre></li><li class="listitem"><p class="simpara">
							Start a remote shell into a pod:
						</p><pre class="programlisting language-terminal">$ oc rsh &lt;pod_name&gt;  <span id="CO41-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO41-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									If a pod has multiple containers, <code class="literal">oc rsh</code> defaults to the first container unless <code class="literal">-c &lt;container_name&gt;</code> is specified.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Start a remote shell into a specific container within a pod:
						</p><pre class="programlisting language-terminal">$ oc rsh -c &lt;container_name&gt; pod/&lt;pod_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Create a port forwarding session to a port on a pod:
						</p><pre class="programlisting language-terminal">$ oc port-forward &lt;pod_name&gt; &lt;host_port&gt;:&lt;pod_port&gt;  <span id="CO42-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO42-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Enter <code class="literal">Ctrl+C</code> to cancel the port forwarding session.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="starting-debug-pods-with-root-access_investigating-pod-issues"><div class="titlepage"><div><div><h3 class="title">7.7.5. Starting debug pods with root access</h3></div></div></div><p>
					You can start a debug pod with root access, based on a problematic pod’s deployment or deployment configuration. Pod users typically run with non-root privileges, but running troubleshooting pods with temporary root privileges can be useful during issue investigation.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Your API service is still functional.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Start a debug pod with root access, based on a deployment.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Obtain a project’s deployment name:
								</p><pre class="programlisting language-terminal">$ oc get deployment -n &lt;project_name&gt;</pre></li><li class="listitem"><p class="simpara">
									Start a debug pod with root privileges, based on the deployment:
								</p><pre class="programlisting language-terminal">$ oc debug deployment/my-deployment --as-root -n &lt;project_name&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Start a debug pod with root access, based on a deployment configuration.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Obtain a project’s deployment configuration name:
								</p><pre class="programlisting language-terminal">$ oc get deploymentconfigs -n &lt;project_name&gt;</pre></li><li class="listitem"><p class="simpara">
									Start a debug pod with root privileges, based on the deployment configuration:
								</p><pre class="programlisting language-terminal">$ oc debug deploymentconfig/my-deployment-configuration --as-root -n &lt;project_name&gt;</pre></li></ol></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can append <code class="literal">-- &lt;command&gt;</code> to the preceding <code class="literal">oc debug</code> commands to run individual commands within a debug pod, instead of running an interactive shell.
					</p></div></div></section><section class="section" id="copying-files-pods-and-containers_investigating-pod-issues"><div class="titlepage"><div><div><h3 class="title">7.7.6. Copying files to and from pods and containers</h3></div></div></div><p>
					You can copy files to and from a pod to test configuration changes or gather diagnostic information.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Your API service is still functional.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Copy a file to a pod:
						</p><pre class="programlisting language-terminal">$ oc cp &lt;local_path&gt; &lt;pod_name&gt;:/&lt;path&gt; -c &lt;container_name&gt;  <span id="CO43-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO43-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The first container in a pod is selected if the <code class="literal">-c</code> option is not specified.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Copy a file from a pod:
						</p><pre class="programlisting language-terminal">$ oc cp &lt;pod_name&gt;:/&lt;path&gt;  -c &lt;container_name&gt;&lt;local_path&gt;  <span id="CO44-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO44-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The first container in a pod is selected if the <code class="literal">-c</code> option is not specified.
								</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								For <code class="literal">oc cp</code> to function, the <code class="literal">tar</code> binary must be available within the container.
							</p></div></div></li></ol></div></section></section><section class="section" id="troubleshooting-s2i"><div class="titlepage"><div><div><h2 class="title">7.8. Troubleshooting the Source-to-Image process</h2></div></div></div><section class="section" id="strategies-for-s2i-troubleshooting_troubleshooting-s2i"><div class="titlepage"><div><div><h3 class="title">7.8.1. Strategies for Source-to-Image troubleshooting</h3></div></div></div><p>
					Use Source-to-Image (S2I) to build reproducible, Docker-formatted container images. You can create ready-to-run images by injecting application source code into a container image and assembling a new image. The new image incorporates the base image (the builder) and built source.
				</p><p>
					To determine where in the S2I process a failure occurs, you can observe the state of the pods relating to each of the following S2I stages:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							<span class="strong strong"><strong>During the build configuration stage</strong></span>, a build pod is used to create an application container image from a base image and application source code.
						</li><li class="listitem">
							<span class="strong strong"><strong>During the deployment configuration stage</strong></span>, a deployment pod is used to deploy application pods from the application container image that was built in the build configuration stage. The deployment pod also deploys other resources such as services and routes. The deployment configuration begins after the build configuration succeeds.
						</li><li class="listitem">
							<span class="strong strong"><strong>After the deployment pod has started the application pods</strong></span>, application failures can occur within the running application pods. For instance, an application might not behave as expected even though the application pods are in a <code class="literal">Running</code> state. In this scenario, you can access running application pods to investigate application failures within a pod.
						</li></ol></div><p>
					When troubleshooting S2I issues, follow this strategy:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Monitor build, deployment, and application pod status
						</li><li class="listitem">
							Determine the stage of the S2I process where the problem occurred
						</li><li class="listitem">
							Review logs corresponding to the failed stage
						</li></ol></div></section><section class="section" id="gathering-s2i-diagnostic-data_troubleshooting-s2i"><div class="titlepage"><div><div><h3 class="title">7.8.2. Gathering Source-to-Image diagnostic data</h3></div></div></div><p>
					The S2I tool runs a build pod and a deployment pod in sequence. The deployment pod is responsible for deploying the application pods based on the application container image created in the build stage. Watch build, deployment and application pod status to determine where in the S2I process a failure occurs. Then, focus diagnostic data collection accordingly.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Your API service is still functional.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Watch the pod status throughout the S2I process to determine at which stage a failure occurs:
						</p><pre class="programlisting language-terminal">$ oc get pods -w  <span id="CO45-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO45-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Use <code class="literal">-w</code> to monitor pods for changes until you quit the command using <code class="literal">Ctrl+C</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Review a failed pod’s logs for errors.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>If the build pod fails</strong></span>, review the build pod’s logs:
								</p><pre class="programlisting language-terminal">$ oc logs -f pod/&lt;application_name&gt;-&lt;build_number&gt;-build</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Alternatively, you can review the build configuration’s logs using <code class="literal">oc logs -f bc/&lt;application_name&gt;</code>. The build configuration’s logs include the logs from the build pod.
									</p></div></div></li><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>If the deployment pod fails</strong></span>, review the deployment pod’s logs:
								</p><pre class="programlisting language-terminal">$ oc logs -f pod/&lt;application_name&gt;-&lt;build_number&gt;-deploy</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Alternatively, you can review the deployment configuration’s logs using <code class="literal">oc logs -f dc/&lt;application_name&gt;</code>. This outputs logs from the deployment pod until the deployment pod completes successfully. The command outputs logs from the application pods if you run it after the deployment pod has completed. After a deployment pod completes, its logs can still be accessed by running <code class="literal">oc logs -f pod/&lt;application_name&gt;-&lt;build_number&gt;-deploy</code>.
									</p></div></div></li><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>If an application pod fails, or if an application is not behaving as expected within a running application pod</strong></span>, review the application pod’s logs:
								</p><pre class="programlisting language-terminal">$ oc logs -f pod/&lt;application_name&gt;-&lt;build_number&gt;-&lt;random_string&gt;</pre></li></ul></div></li></ol></div></section><section class="section" id="gathering-application-diagnostic-data_troubleshooting-s2i"><div class="titlepage"><div><div><h3 class="title">7.8.3. Gathering application diagnostic data to investigate application failures</h3></div></div></div><p>
					Application failures can occur within running application pods. In these situations, you can retrieve diagnostic information with these strategies:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Review events relating to the application pods.
						</li><li class="listitem">
							Review the logs from the application pods, including application-specific log files that are not collected by the OpenShift Logging framework.
						</li><li class="listitem">
							Test application functionality interactively and run diagnostic tools in an application container.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							List events relating to a specific application pod. The following example retrieves events for an application pod named <code class="literal">my-app-1-akdlg</code>:
						</p><pre class="programlisting language-terminal">$ oc describe pod/my-app-1-akdlg</pre></li><li class="listitem"><p class="simpara">
							Review logs from an application pod:
						</p><pre class="programlisting language-terminal">$ oc logs -f pod/my-app-1-akdlg</pre></li><li class="listitem"><p class="simpara">
							Query specific logs within a running application pod. Logs that are sent to stdout are collected by the OpenShift Logging framework and are included in the output of the preceding command. The following query is only required for logs that are not sent to stdout.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									If an application log can be accessed without root privileges within a pod, concatenate the log file as follows:
								</p><pre class="programlisting language-terminal">$ oc exec my-app-1-akdlg -- cat /var/log/my-application.log</pre></li><li class="listitem"><p class="simpara">
									If root access is required to view an application log, you can start a debug container with root privileges and then view the log file from within the container. Start the debug container from the project’s <code class="literal">DeploymentConfig</code> object. Pod users typically run with non-root privileges, but running troubleshooting pods with temporary root privileges can be useful during issue investigation:
								</p><pre class="programlisting language-terminal">$ oc debug dc/my-deployment-configuration --as-root -- cat /var/log/my-application.log</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										You can access an interactive shell with root access within the debug pod if you run <code class="literal">oc debug dc/&lt;deployment_configuration&gt; --as-root</code> without appending <code class="literal">-- &lt;command&gt;</code>.
									</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Test application functionality interactively and run diagnostic tools, in an application container with an interactive shell.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Start an interactive shell on the application container:
								</p><pre class="programlisting language-terminal">$ oc exec -it my-app-1-akdlg /bin/bash</pre></li><li class="listitem">
									Test application functionality interactively from within the shell. For example, you can run the container’s entry point command and observe the results. Then, test changes from the command line directly, before updating the source code and rebuilding the application container through the S2I process.
								</li><li class="listitem"><p class="simpara">
									Run diagnostic binaries available within the container.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Root privileges are required to run some diagnostic binaries. In these situations you can start a debug pod with root access, based on a problematic pod’s <code class="literal">DeploymentConfig</code> object, by running <code class="literal">oc debug dc/&lt;deployment_configuration&gt; --as-root</code>. Then, you can run diagnostic binaries as root from within the debug pod.
									</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
							If diagnostic binaries are not available within a container, you can run a host’s diagnostic binaries within a container’s namespace by using <code class="literal">nsenter</code>. The following example runs <code class="literal">ip ad</code> within a container’s namespace, using the host`s <code class="literal">ip</code> binary.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Enter into a debug session on the target node. This step instantiates a debug pod called <code class="literal">&lt;node_name&gt;-debug</code>:
								</p><pre class="programlisting language-terminal">$ oc debug node/my-cluster-node</pre></li><li class="listitem"><p class="simpara">
									Set <code class="literal">/host</code> as the root directory within the debug shell. The debug pod mounts the host’s root file system in <code class="literal">/host</code> within the pod. By changing the root directory to <code class="literal">/host</code>, you can run binaries contained in the host’s executable paths:
								</p><pre class="programlisting language-terminal"># chroot /host</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										OpenShift Container Platform 4.13 cluster nodes running Red Hat Enterprise Linux CoreOS (RHCOS) are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes by using SSH is not recommended. However, if the OpenShift Container Platform API is not available, or the kubelet is not properly functioning on the target node, <code class="literal">oc</code> operations will be impacted. In such situations, it is possible to access nodes using <code class="literal">ssh core@&lt;node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> instead.
									</p></div></div></li><li class="listitem"><p class="simpara">
									Determine the target container ID:
								</p><pre class="programlisting language-terminal"># crictl ps</pre></li><li class="listitem"><p class="simpara">
									Determine the container’s process ID. In this example, the target container ID is <code class="literal">a7fe32346b120</code>:
								</p><pre class="programlisting language-terminal"># crictl inspect a7fe32346b120 --output yaml | grep 'pid:' | awk '{print $2}'</pre></li><li class="listitem"><p class="simpara">
									Run <code class="literal">ip ad</code> within the container’s namespace, using the host’s <code class="literal">ip</code> binary. This example uses <code class="literal">31150</code> as the container’s process ID. The <code class="literal">nsenter</code> command enters the namespace of a target process and runs a command in its namespace. Because the target process in this example is a container’s process ID, the <code class="literal">ip ad</code> command is run in the container’s namespace from the host:
								</p><pre class="programlisting language-terminal"># nsenter -n -t 31150 -- ip ad</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Running a host’s diagnostic binaries within a container’s namespace is only possible if you are using a privileged container such as a debug node.
									</p></div></div></li></ol></div></li></ol></div></section><section class="section _additional-resources" id="additional-resources-3"><div class="titlepage"><div><div><h3 class="title">7.8.4. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/cicd/#build-strategy-s2i_build-strategies">Source-to-Image (S2I) build</a> for more details about the S2I build strategy.
						</li></ul></div></section></section><section class="section" id="troubleshooting-storage-issues"><div class="titlepage"><div><div><h2 class="title">7.9. Troubleshooting storage issues</h2></div></div></div><section class="section" id="storage-multi-attach-error_troubleshooting-storage-issues"><div class="titlepage"><div><div><h3 class="title">7.9.1. Resolving multi-attach errors</h3></div></div></div><p>
					When a node crashes or shuts down abruptly, the attached ReadWriteOnce (RWO) volume is expected to be unmounted from the node so that it can be used by a pod scheduled on another node.
				</p><p>
					However, mounting on a new node is not possible because the failed node is unable to unmount the attached volume.
				</p><p>
					A multi-attach error is reported:
				</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">Unable to attach or mount volumes: unmounted volumes=[sso-mysql-pvol], unattached volumes=[sso-mysql-pvol default-token-x4rzc]: timed out waiting for the condition
Multi-Attach error for volume "pvc-8837384d-69d7-40b2-b2e6-5df86943eef9" Volume is already used by pod(s) sso-mysql-1-ns6b4</pre>

					</p></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To resolve the multi-attach issue, use one of the following solutions:
					</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Enable multiple attachments by using RWX volumes.
						</p><p class="simpara">
							For most storage solutions, you can use ReadWriteMany (RWX) volumes to prevent multi-attach errors.
						</p></li><li class="listitem"><p class="simpara">
							Recover or delete the failed node when using an RWO volume.
						</p><p class="simpara">
							For storage that does not support RWX, such as VMware vSphere, RWO volumes must be used instead. However, RWO volumes cannot be mounted on multiple nodes.
						</p><p class="simpara">
							If you encounter a multi-attach error message with an RWO volume, force delete the pod on a shutdown or crashed node to avoid data loss in critical workloads, such as when dynamic persistent volumes are attached.
						</p><pre class="programlisting language-terminal">$ oc delete pod &lt;old_pod&gt; --force=true --grace-period=0</pre><p class="simpara">
							This command deletes the volumes stuck on shutdown or crashed nodes after six minutes.
						</p></li></ul></div></section></section><section class="section" id="troubleshooting-windows-container-workload-issues"><div class="titlepage"><div><div><h2 class="title">7.10. Troubleshooting Windows container workload issues</h2></div></div></div><section class="section" id="wmco-does-not-install_troubleshooting-windows-container-workload-issues"><div class="titlepage"><div><div><h3 class="title">7.10.1. Windows Machine Config Operator does not install</h3></div></div></div><p>
					If you have completed the process of installing the Windows Machine Config Operator (WMCO), but the Operator is stuck in the <code class="literal">InstallWaiting</code> phase, your issue is likely caused by a networking issue.
				</p><p>
					The WMCO requires your OpenShift Container Platform cluster to be configured with hybrid networking using OVN-Kubernetes; the WMCO cannot complete the installation process without hybrid networking available. This is necessary to manage nodes on multiple operating systems (OS) and OS variants. This must be completed during the installation of your cluster.
				</p><p>
					For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-hybrid-ovnkubernetes">Configuring hybrid networking</a>.
				</p></section><section class="section" id="investigating-why-windows-machine-compute-node_troubleshooting-windows-container-workload-issues"><div class="titlepage"><div><div><h3 class="title">7.10.2. Investigating why Windows Machine does not become compute node</h3></div></div></div><p>
					There are various reasons why a Windows Machine does not become a compute node. The best way to investigate this problem is to collect the Windows Machine Config Operator (WMCO) logs.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You installed the Windows Machine Config Operator (WMCO) using Operator Lifecycle Manager (OLM).
						</li><li class="listitem">
							You have created a Windows compute machine set.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Run the following command to collect the WMCO logs:
						</p><pre class="programlisting language-terminal">$ oc logs -f deployment/windows-machine-config-operator -n openshift-windows-machine-config-operator</pre></li></ul></div></section><section class="section" id="accessing-windows-node"><div class="titlepage"><div><div><h3 class="title">7.10.3. Accessing a Windows node</h3></div></div></div><p>
					Windows nodes cannot be accessed using the <code class="literal">oc debug node</code> command; the command requires running a privileged pod on the node, which is not yet supported for Windows. Instead, a Windows node can be accessed using a secure shell (SSH) or Remote Desktop Protocol (RDP). An SSH bastion is required for both methods.
				</p><section class="section" id="accessing-windows-node-using-ssh_troubleshooting-windows-container-workload-issues"><div class="titlepage"><div><div><h4 class="title">7.10.3.1. Accessing a Windows node using SSH</h4></div></div></div><p>
						You can access a Windows node by using a secure shell (SSH).
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the Windows Machine Config Operator (WMCO) using Operator Lifecycle Manager (OLM).
							</li><li class="listitem">
								You have created a Windows compute machine set.
							</li><li class="listitem">
								You have added the key used in the <code class="literal">cloud-private-key</code> secret and the key used when creating the cluster to the ssh-agent. For security reasons, remember to remove the keys from the ssh-agent after use.
							</li><li class="listitem">
								You have connected to the Windows node <a class="link" href="https://access.redhat.com/solutions/4073041">using an <code class="literal">ssh-bastion</code> pod</a>.
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Access the Windows node by running the following command:
							</p><pre class="programlisting language-terminal">$ ssh -t -o StrictHostKeyChecking=no -o ProxyCommand='ssh -A -o StrictHostKeyChecking=no \
    -o ServerAliveInterval=30 -W %h:%p core@$(oc get service --all-namespaces -l run=ssh-bastion \
    -o go-template="{{ with (index (index .items 0).status.loadBalancer.ingress 0) }}{{ or .hostname .ip }}{{end}}")' &lt;username&gt;@&lt;windows_node_internal_ip&gt; <span id="CO46-1"><!--Empty--></span><span class="callout">1</span> <span id="CO46-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO46-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify the cloud provider username, such as <code class="literal">Administrator</code> for Amazon Web Services (AWS) or <code class="literal">capi</code> for Microsoft Azure.
									</div></dd><dt><a href="#CO46-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specify the internal IP address of the node, which can be discovered by running the following command:
									</div></dd></dl></div><pre class="programlisting language-terminal">$ oc get nodes &lt;node_name&gt; -o jsonpath={.status.addresses[?\(@.type==\"InternalIP\"\)].address}</pre></li></ul></div></section><section class="section" id="accessing-windows-node-using-rdp_troubleshooting-windows-container-workload-issues"><div class="titlepage"><div><div><h4 class="title">7.10.3.2. Accessing a Windows node using RDP</h4></div></div></div><p>
						You can access a Windows node by using a Remote Desktop Protocol (RDP).
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You installed the Windows Machine Config Operator (WMCO) using Operator Lifecycle Manager (OLM).
							</li><li class="listitem">
								You have created a Windows compute machine set.
							</li><li class="listitem">
								You have added the key used in the <code class="literal">cloud-private-key</code> secret and the key used when creating the cluster to the ssh-agent. For security reasons, remember to remove the keys from the ssh-agent after use.
							</li><li class="listitem">
								You have connected to the Windows node <a class="link" href="https://access.redhat.com/solutions/4073041">using an <code class="literal">ssh-bastion</code> pod</a>.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Run the following command to set up an SSH tunnel:
							</p><pre class="programlisting language-terminal">$ ssh -L 2020:&lt;windows_node_internal_ip&gt;:3389 \ <span id="CO47-1"><!--Empty--></span><span class="callout">1</span>
    core@$(oc get service --all-namespaces -l run=ssh-bastion -o go-template="{{ with (index (index .items 0).status.loadBalancer.ingress 0) }}{{ or .hostname .ip }}{{end}}")</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO47-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify the internal IP address of the node, which can be discovered by running the following command:
									</div></dd></dl></div><pre class="programlisting language-terminal">$ oc get nodes &lt;node_name&gt; -o jsonpath={.status.addresses[?\(@.type==\"InternalIP\"\)].address}</pre></li><li class="listitem"><p class="simpara">
								From within the resulting shell, SSH into the Windows node and run the following command to create a password for the user:
							</p><pre class="programlisting language-terminal">C:\&gt; net user &lt;username&gt; * <span id="CO48-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO48-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify the cloud provider user name, such as <code class="literal">Administrator</code> for AWS or <code class="literal">capi</code> for Azure.
									</div></dd></dl></div></li></ol></div><p>
						You can now remotely access the Windows node at <code class="literal">localhost:2020</code> using an RDP client.
					</p></section></section><section class="section" id="collecting-kube-node-logs-windows_troubleshooting-windows-container-workload-issues"><div class="titlepage"><div><div><h3 class="title">7.10.4. Collecting Kubernetes node logs for Windows containers</h3></div></div></div><p>
					Windows container logging works differently from Linux container logging; the Kubernetes node logs for Windows workloads are streamed to the <code class="literal">C:\var\logs</code> directory by default. Therefore, you must gather the Windows node logs from that directory.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You installed the Windows Machine Config Operator (WMCO) using Operator Lifecycle Manager (OLM).
						</li><li class="listitem">
							You have created a Windows compute machine set.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To view the logs under all directories in <code class="literal">C:\var\logs</code>, run the following command:
						</p><pre class="programlisting language-terminal">$ oc adm node-logs -l kubernetes.io/os=windows --path= \
    /ip-10-0-138-252.us-east-2.compute.internal containers \
    /ip-10-0-138-252.us-east-2.compute.internal hybrid-overlay \
    /ip-10-0-138-252.us-east-2.compute.internal kube-proxy \
    /ip-10-0-138-252.us-east-2.compute.internal kubelet \
    /ip-10-0-138-252.us-east-2.compute.internal pods</pre></li><li class="listitem"><p class="simpara">
							You can now list files in the directories using the same command and view the individual log files. For example, to view the kubelet logs, run the following command:
						</p><pre class="programlisting language-terminal">$ oc adm node-logs -l kubernetes.io/os=windows --path=/kubelet/kubelet.log</pre></li></ol></div></section><section class="section" id="collecting-windows-application-event-logs_troubleshooting-windows-container-workload-issues"><div class="titlepage"><div><div><h3 class="title">7.10.5. Collecting Windows application event logs</h3></div></div></div><p>
					The <code class="literal">Get-WinEvent</code> shim on the kubelet <code class="literal">logs</code> endpoint can be used to collect application event logs from Windows machines.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You installed the Windows Machine Config Operator (WMCO) using Operator Lifecycle Manager (OLM).
						</li><li class="listitem">
							You have created a Windows compute machine set.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To view logs from all applications logging to the event logs on the Windows machine, run:
						</p><pre class="programlisting language-terminal">$ oc adm node-logs -l kubernetes.io/os=windows --path=journal</pre><p class="simpara">
							The same command is executed when collecting logs with <code class="literal">oc adm must-gather</code>.
						</p><p class="simpara">
							Other Windows application logs from the event log can also be collected by specifying the respective service with a <code class="literal">-u</code> flag. For example, you can run the following command to collect logs for the docker runtime service:
						</p><pre class="programlisting language-terminal">$ oc adm node-logs -l kubernetes.io/os=windows --path=journal -u docker</pre></li></ul></div></section><section class="section" id="collecting-docker-logs-windows_troubleshooting-windows-container-workload-issues"><div class="titlepage"><div><div><h3 class="title">7.10.6. Collecting Docker logs for Windows containers</h3></div></div></div><p>
					The Windows Docker service does not stream its logs to stdout, but instead, logs to the event log for Windows. You can view the Docker event logs to investigate issues you think might be caused by the Windows Docker service.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You installed the Windows Machine Config Operator (WMCO) using Operator Lifecycle Manager (OLM).
						</li><li class="listitem">
							You have created a Windows compute machine set.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							SSH into the Windows node and enter PowerShell:
						</p><pre class="programlisting language-terminal">C:\&gt; powershell</pre></li><li class="listitem"><p class="simpara">
							View the Docker logs by running the following command:
						</p><pre class="programlisting language-terminal">C:\&gt; Get-EventLog -LogName Application -Source Docker</pre></li></ol></div></section><section class="section _additional-resources" id="additional-resources-4"><div class="titlepage"><div><div><h3 class="title">7.10.7. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/troubleshooting">Containers on Windows troubleshooting</a>
						</li><li class="listitem">
							<a class="link" href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/update-containers#troubleshoot-host-and-container-image-mismatches">Troubleshoot host and container image mismatches</a>
						</li><li class="listitem">
							<a class="link" href="https://docs.docker.com/docker-for-windows/troubleshoot/">Docker for Windows troubleshooting</a>
						</li><li class="listitem">
							<a class="link" href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/common-problems">Common Kubernetes problems with Windows</a>
						</li></ul></div></section></section><section class="section" id="investigating-monitoring-issues"><div class="titlepage"><div><div><h2 class="title">7.11. Investigating monitoring issues</h2></div></div></div><p>
				OpenShift Container Platform includes a pre-configured, pre-installed, and self-updating monitoring stack that provides monitoring for core platform components. In OpenShift Container Platform 4.13, cluster administrators can optionally enable monitoring for user-defined projects.
			</p><p>
				You can follow these procedures if your own metrics are unavailable or if Prometheus is consuming a lot of disk space.
			</p><section class="section" id="investigating-why-user-defined-metrics-are-unavailable_investigating-monitoring-issues"><div class="titlepage"><div><div><h3 class="title">7.11.1. Investigating why user-defined project metrics are unavailable</h3></div></div></div><p>
					<code class="literal">ServiceMonitor</code> resources enable you to determine how to use the metrics exposed by a service in user-defined projects. Follow the steps outlined in this procedure if you have created a <code class="literal">ServiceMonitor</code> resource but cannot see any corresponding metrics in the Metrics UI.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have enabled and configured monitoring for user-defined workloads.
						</li><li class="listitem">
							You have created the <code class="literal">user-workload-monitoring-config</code> <code class="literal">ConfigMap</code> object.
						</li><li class="listitem">
							You have created a <code class="literal">ServiceMonitor</code> resource.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							<span class="strong strong"><strong>Check that the corresponding labels match</strong></span> in the service and <code class="literal">ServiceMonitor</code> resource configurations.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Obtain the label defined in the service. The following example queries the <code class="literal">prometheus-example-app</code> service in the <code class="literal">ns1</code> project:
								</p><pre class="programlisting language-terminal">$ oc -n ns1 get service prometheus-example-app -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">  labels:
    app: prometheus-example-app</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Check that the <code class="literal">matchLabels</code> <code class="literal">app</code> label in the <code class="literal">ServiceMonitor</code> resource configuration matches the label output in the preceding step:
								</p><pre class="programlisting language-terminal">$ oc -n ns1 get servicemonitor prometheus-example-monitor -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="screen">spec:
  endpoints:
  - interval: 30s
    port: web
    scheme: http
  selector:
    matchLabels:
      app: prometheus-example-app</pre>

									</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										You can check service and <code class="literal">ServiceMonitor</code> resource labels as a developer with view permissions for the project.
									</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
							<span class="strong strong"><strong>Inspect the logs for the Prometheus Operator</strong></span> in the <code class="literal">openshift-user-workload-monitoring</code> project.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									List the pods in the <code class="literal">openshift-user-workload-monitoring</code> project:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring get pods</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                   READY   STATUS    RESTARTS   AGE
prometheus-operator-776fcbbd56-2nbfm   2/2     Running   0          132m
prometheus-user-workload-0             5/5     Running   1          132m
prometheus-user-workload-1             5/5     Running   1          132m
thanos-ruler-user-workload-0           3/3     Running   0          132m
thanos-ruler-user-workload-1           3/3     Running   0          132m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Obtain the logs from the <code class="literal">prometheus-operator</code> container in the <code class="literal">prometheus-operator</code> pod. In the following example, the pod is called <code class="literal">prometheus-operator-776fcbbd56-2nbfm</code>:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring logs prometheus-operator-776fcbbd56-2nbfm -c prometheus-operator</pre><p class="simpara">
									If there is a issue with the service monitor, the logs might include an error similar to this example:
								</p><pre class="programlisting language-terminal">level=warn ts=2020-08-10T11:48:20.906739623Z caller=operator.go:1829 component=prometheusoperator msg="skipping servicemonitor" error="it accesses file system via bearer token file which Prometheus specification prohibits" servicemonitor=eagle/eagle namespace=openshift-user-workload-monitoring prometheus=user-workload</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							<span class="strong strong"><strong>Review the target status for your endpoint</strong></span> on the <span class="strong strong"><strong>Metrics targets</strong></span> page in the OpenShift Container Platform web console UI.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Log in to the OpenShift Container Platform web console and navigate to <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Targets</strong></span> in the <span class="strong strong"><strong>Administrator</strong></span> perspective.
								</li><li class="listitem">
									Locate the metrics endpoint in the list, and review the status of the target in the <span class="strong strong"><strong>Status</strong></span> column.
								</li><li class="listitem">
									If the <span class="strong strong"><strong>Status</strong></span> is <span class="strong strong"><strong>Down</strong></span>, click the URL for the endpoint to view more information on the <span class="strong strong"><strong>Target Details</strong></span> page for that metrics target.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							<span class="strong strong"><strong>Configure debug level logging for the Prometheus Operator</strong></span> in the <code class="literal">openshift-user-workload-monitoring</code> project.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Edit the <code class="literal">user-workload-monitoring-config</code> <code class="literal">ConfigMap</code> object in the <code class="literal">openshift-user-workload-monitoring</code> project:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
									Add <code class="literal">logLevel: debug</code> for <code class="literal">prometheusOperator</code> under <code class="literal">data/config.yaml</code> to set the log level to <code class="literal">debug</code>:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheusOperator:
      logLevel: debug</pre></li><li class="listitem"><p class="simpara">
									Save the file to apply the changes.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										The <code class="literal">prometheus-operator</code> in the <code class="literal">openshift-user-workload-monitoring</code> project restarts automatically when you apply the log-level change.
									</p></div></div></li><li class="listitem"><p class="simpara">
									Confirm that the <code class="literal">debug</code> log-level has been applied to the <code class="literal">prometheus-operator</code> deployment in the <code class="literal">openshift-user-workload-monitoring</code> project:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring get deploy prometheus-operator -o yaml |  grep "log-level"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">        - --log-level=debug</pre>

									</p></div><p class="simpara">
									Debug level logging will show all calls made by the Prometheus Operator.
								</p></li><li class="listitem"><p class="simpara">
									Check that the <code class="literal">prometheus-operator</code> pod is running:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring get pods</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										If an unrecognized Prometheus Operator <code class="literal">loglevel</code> value is included in the config map, the <code class="literal">prometheus-operator</code> pod might not restart successfully.
									</p></div></div></li><li class="listitem">
									Review the debug logs to see if the Prometheus Operator is using the <code class="literal">ServiceMonitor</code> resource. Review the logs for other related errors.
								</li></ol></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/monitoring/#creating-user-defined-workload-monitoring-configmap_configuring-the-monitoring-stack">Creating a user-defined workload monitoring config map</a>
						</li><li class="listitem">
							See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/monitoring/#specifying-how-a-service-is-monitored_managing-metrics">Specifying how a service is monitored</a> for details on how to create a service monitor or pod monitor
						</li><li class="listitem">
							See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/monitoring/#getting-detailed-information-about-a-target_managing-metrics">Getting detailed information about a metrics target</a>
						</li></ul></div></section><section class="section" id="determining-why-prometheus-is-consuming-disk-space_investigating-monitoring-issues"><div class="titlepage"><div><div><h3 class="title">7.11.2. Determining why Prometheus is consuming a lot of disk space</h3></div></div></div><p>
					Developers can create labels to define attributes for metrics in the form of key-value pairs. The number of potential key-value pairs corresponds to the number of possible values for an attribute. An attribute that has an unlimited number of potential values is called an unbound attribute. For example, a <code class="literal">customer_id</code> attribute is unbound because it has an infinite number of possible values.
				</p><p>
					Every assigned key-value pair has a unique time series. The use of many unbound attributes in labels can result in an exponential increase in the number of time series created. This can impact Prometheus performance and can consume a lot of disk space.
				</p><p>
					You can use the following measures when Prometheus consumes a lot of disk:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<span class="strong strong"><strong>Check the number of scrape samples</strong></span> that are being collected.
						</li><li class="listitem">
							<span class="strong strong"><strong>Check the time series database (TSDB) status using the Prometheus HTTP API</strong></span> for more information about which labels are creating the most time series. Doing so requires cluster administrator privileges.
						</li><li class="listitem"><p class="simpara">
							<span class="strong strong"><strong>Reduce the number of unique time series that are created</strong></span> by reducing the number of unbound attributes that are assigned to user-defined metrics.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Using attributes that are bound to a limited set of possible values reduces the number of potential key-value pair combinations.
							</p></div></div></li><li class="listitem">
							<span class="strong strong"><strong>Enforce limits on the number of samples that can be scraped</strong></span> across user-defined projects. This requires cluster administrator privileges.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the <span class="strong strong"><strong>Administrator</strong></span> perspective, navigate to <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Metrics</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Run the following Prometheus Query Language (PromQL) query in the <span class="strong strong"><strong>Expression</strong></span> field. This returns the ten metrics that have the highest number of scrape samples:
						</p><pre class="programlisting language-terminal">topk(10,count by (job)({__name__=~".+"}))</pre></li><li class="listitem"><p class="simpara">
							Investigate the number of unbound label values assigned to metrics with higher than expected scrape sample counts.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<span class="strong strong"><strong>If the metrics relate to a user-defined project</strong></span>, review the metrics key-value pairs assigned to your workload. These are implemented through Prometheus client libraries at the application level. Try to limit the number of unbound attributes referenced in your labels.
								</li><li class="listitem">
									<span class="strong strong"><strong>If the metrics relate to a core OpenShift Container Platform project</strong></span>, create a Red Hat support case on the <a class="link" href="https://access.redhat.com/">Red Hat Customer Portal</a>.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							Review the TSDB status using the Prometheus HTTP API by running the following commands as a cluster administrator:
						</p><pre class="programlisting language-terminal">$ oc login -u &lt;username&gt; -p &lt;password&gt;</pre><pre class="programlisting language-terminal">$ host=$(oc -n openshift-monitoring get route prometheus-k8s -ojsonpath={.spec.host})</pre><pre class="programlisting language-terminal">$ token=$(oc whoami -t)</pre><pre class="programlisting language-terminal">$ curl -H "Authorization: Bearer $token" -k "https://$host/api/v1/status/tsdb"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">"status": "success",</pre>

							</p></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/monitoring/#setting-scrape-sample-and-label-limits-for-user-defined-projects_configuring-the-monitoring-stack">Setting a scrape sample limit for user-defined projects</a> for details on how to set a scrape sample limit and create related alerting rules
						</li></ul></div></section></section><section class="section" id="diagnosing-oc-issues"><div class="titlepage"><div><div><h2 class="title">7.12. Diagnosing OpenShift CLI (<code class="literal">oc</code>) issues</h2></div></div></div><section class="section" id="understanding-oc-log-levels_diagnosing-oc-issues"><div class="titlepage"><div><div><h3 class="title">7.12.1. Understanding OpenShift CLI (<code class="literal">oc</code>) log levels</h3></div></div></div><p>
					With the OpenShift CLI (<code class="literal">oc</code>), you can create applications and manage OpenShift Container Platform projects from a terminal.
				</p><p>
					If <code class="literal">oc</code> command-specific issues arise, increase the <code class="literal">oc</code> log level to output API request, API response, and <code class="literal">curl</code> request details generated by the command. This provides a granular view of a particular <code class="literal">oc</code> command’s underlying operation, which in turn might provide insight into the nature of a failure.
				</p><p>
					<code class="literal">oc</code> log levels range from 1 to 10. The following table provides a list of <code class="literal">oc</code> log levels, along with their descriptions.
				</p><div class="table" id="idm140604667143808"><p class="title"><strong>Table 7.4. OpenShift CLI (<code class="literal">oc</code>) log levels</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140604665272048" scope="col">Log level</th><th align="left" valign="top" id="idm140604665270960" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140604665272048"> <p>
									1 to 5
								</p>
								 </td><td align="left" valign="top" headers="idm140604665270960"> <p>
									No additional logging to stderr.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665272048"> <p>
									6
								</p>
								 </td><td align="left" valign="top" headers="idm140604665270960"> <p>
									Log API requests to stderr.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665272048"> <p>
									7
								</p>
								 </td><td align="left" valign="top" headers="idm140604665270960"> <p>
									Log API requests and headers to stderr.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665272048"> <p>
									8
								</p>
								 </td><td align="left" valign="top" headers="idm140604665270960"> <p>
									Log API requests, headers, and body, plus API response headers and body to stderr.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665272048"> <p>
									9
								</p>
								 </td><td align="left" valign="top" headers="idm140604665270960"> <p>
									Log API requests, headers, and body, API response headers and body, plus <code class="literal">curl</code> requests to stderr.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140604665272048"> <p>
									10
								</p>
								 </td><td align="left" valign="top" headers="idm140604665270960"> <p>
									Log API requests, headers, and body, API response headers and body, plus <code class="literal">curl</code> requests to stderr, in verbose detail.
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section" id="specifying-oc-log-levels_diagnosing-oc-issues"><div class="titlepage"><div><div><h3 class="title">7.12.2. Specifying OpenShift CLI (<code class="literal">oc</code>) log levels</h3></div></div></div><p>
					You can investigate OpenShift CLI (<code class="literal">oc</code>) issues by increasing the command’s log level.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Specify the <code class="literal">oc</code> log level when running an <code class="literal">oc</code> command:
						</p><pre class="programlisting language-terminal">$ oc &lt;options&gt; --loglevel &lt;log_level&gt;</pre></li><li class="listitem"><p class="simpara">
							The OpenShift Container Platform user’s current session token is typically included in logged <code class="literal">curl</code> requests where required. You can also obtain the current user’s session token manually, for use when testing aspects of an <code class="literal">oc</code> command’s underlying process step by step:
						</p><pre class="programlisting language-terminal">$ oc whoami -t</pre></li></ol></div></section></section></section><div><div xml:lang="en-US" class="legalnotice" id="idm140604671788160"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"><!--Empty--></span>© 2023 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div>


  <nav class="pvof-doc__book-nav">
  <ol class="book-nav__list">
              </ol>
</nav>


          </div>
              </div>
              <div id="comments-footer" class="book-comments">
          

  

        </div>
          </div>
  </article>
<meta itemscope="" itemref="md1">



    </div>
      <!-- CP_PRIMER_FOOTER -->            </div>
        </main>
    </div>
    <!--googleoff: all-->
    <div id="to-top"><a class="btn_slideto" href="#masthead" aria-label="Back to Top"><span class="web-icon-upload"></span></a></div>
    <footer class="footer-main">
        <div class="footer-top">
            <div class="container">

              <div class="brand">
                <a href="https://redhat.com">
                  <svg class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 613 145">
                    <defs>
                      <style>
                        .rh-logo-hat {
                          fill: #e00;
                        }
                        .rh-logo-type {
                          fill: #fff;
                        }
                      </style>
                    </defs>
                    <title>Red Hat</title>
                    <path
                      class="rh-logo-hat"
                      d="M127.47,83.49c12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42l-7.45-32.36c-1.72-7.12-3.23-10.35-15.73-16.6C124.89,8.69,103.76.5,97.51.5,91.69.5,90,8,83.06,8c-6.68,0-11.64-5.6-17.89-5.6-6,0-9.91,4.09-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33C22.27,52,.5,55,.5,74.22c0,31.48,74.59,70.28,133.65,70.28,45.28,0,56.7-20.48,56.7-36.65,0-12.72-11-27.16-30.83-35.78"/>
                      <path class="rh-logo-band"
                      d="M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45,12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/>
                      <path
                      class="rh-logo-type"
                      d="M579.74,92.8c0,11.89,7.15,17.67,20.19,17.67a52.11,52.11,0,0,0,11.89-1.68V95a24.84,24.84,0,0,1-7.68,1.16c-5.37,0-7.36-1.68-7.36-6.73V68.3h15.56V54.1H596.78v-18l-17,3.68V54.1H568.49V68.3h11.25Zm-53,.32c0-3.68,3.69-5.47,9.26-5.47a43.12,43.12,0,0,1,10.1,1.26v7.15a21.51,21.51,0,0,1-10.63,2.63c-5.46,0-8.73-2.1-8.73-5.57m5.2,17.56c6,0,10.84-1.26,15.36-4.31v3.37h16.82V74.08c0-13.56-9.14-21-24.39-21-8.52,0-16.94,2-26,6.1l6.1,12.52c6.52-2.74,12-4.42,16.83-4.42,7,0,10.62,2.73,10.62,8.31v2.73a49.53,49.53,0,0,0-12.62-1.58c-14.31,0-22.93,6-22.93,16.73,0,9.78,7.78,17.24,20.19,17.24m-92.44-.94h18.09V80.92h30.29v28.82H506V36.12H487.93V64.41H457.64V36.12H439.55ZM370.62,81.87c0-8,6.31-14.1,14.62-14.1A17.22,17.22,0,0,1,397,72.09V91.54A16.36,16.36,0,0,1,385.24,96c-8.2,0-14.62-6.1-14.62-14.09m26.61,27.87h16.83V32.44l-17,3.68V57.05a28.3,28.3,0,0,0-14.2-3.68c-16.19,0-28.92,12.51-28.92,28.5a28.25,28.25,0,0,0,28.4,28.6,25.12,25.12,0,0,0,14.93-4.83ZM320,67c5.36,0,9.88,3.47,11.67,8.83H308.47C310.15,70.3,314.36,67,320,67M291.33,82c0,16.2,13.25,28.82,30.28,28.82,9.36,0,16.2-2.53,23.25-8.42l-11.26-10c-2.63,2.74-6.52,4.21-11.14,4.21a14.39,14.39,0,0,1-13.68-8.83h39.65V83.55c0-17.67-11.88-30.39-28.08-30.39a28.57,28.57,0,0,0-29,28.81M262,51.58c6,0,9.36,3.78,9.36,8.31S268,68.2,262,68.2H244.11V51.58Zm-36,58.16h18.09V82.92h13.77l13.89,26.82H292l-16.2-29.45a22.27,22.27,0,0,0,13.88-20.72c0-13.25-10.41-23.45-26-23.45H226Z"/>
                  </svg>
                </a>
              </div>

              <div role="navigation" aria-label="quick">
                  <h3>Quick Links</h3>
                  <ul>
                      <li><a class="download-software" href="https://access.redhat.com/downloads/">Downloads</a></li>
                      <li><a class="manage-subscriptions" href="https://access.redhat.com/management">Subscriptions</a></li>
                      <li><a class="support-cases" href="https://access.redhat.com/support">Support Cases</a></li>
                      <li><a class="customer-service" href="https://access.redhat.com/support/customer-service">Customer Service</a></li>
                      <li><a class="quick-docs" href="https://access.redhat.com/documentation">Product Documentation</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="help">
                  <h3>Help</h3>
                  <ul>
                      <li><a class="contact-us" href="https://access.redhat.com/support/contact/">Contact Us</a></li>
                      <li><a class="cp-faqs" href="https://access.redhat.com/articles/33844">Customer Portal FAQ</a></li>
                      <li><a class="login-problems" href="https://access.redhat.com/help/login_assistance">Log-in Assistance</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="site">
                  <h3>Site Info</h3>
                  <ul>
                      <li><a class="trust-red-hat" href="https://www.redhat.com/en/trust">Trust Red Hat</a></li>
                      <li><a class="browser-support-policy" href="https://www.redhat.com/en/about/browser-support">Browser Support Policy</a></li>
                      <li><a class="accessibility" href="https://www.redhat.com/en/about/digital-accessibility">Accessibility</a></li>
                      <li><a class="recognition" href="https://access.redhat.com/recognition/">Awards and Recognition</a></li>
                      <li><a class="colophon" href="https://access.redhat.com/help/colophon/">Colophon</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="other">
                  <h3>Related Sites</h3>
                  <ul>
                      <li><a href="https://www.redhat.com/" class="red-hat-com">redhat.com</a></li>
                      <li><a href="http://developers.redhat.com/" class="red-hat-developers">developers.redhat.com</a></li>
                      <li><a href="https://connect.redhat.com/" class="partner-connect">connect.redhat.com</a></li>
                      <li><a href="https://cloud.redhat.com/" class="cloud-com">cloud.redhat.com</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="about">
                  <h3>About</h3>
                  <ul>
                      <li><a href="https://access.redhat.com/subscription-value" class="subscription-value">Red Hat Subscription Value</a></li>
                      <li><a href="https://www.redhat.com/about/" class="about-red-hat">About Red Hat</a></li>
                      <li><a href="http://jobs.redhat.com" class="about-jobs">Red Hat Jobs</a></li>
                  </ul>
              </div>

            </div>
        </div>

        <div class="anchor">
            <div class="container">
                <div class="status-legal">
                    <a hidden href="https://status.redhat.com" class="status-page-widget">
                        <span class="status-description"></span>
                        <span class="status-dot shape-circle"></span>
                    </a>
                    <div class="legal-copyright">
                        <div class="copyright">Copyright © 2023 Red Hat, Inc.</div>
                        <div role="navigation" aria-label="legal" class="legal">
                            <ul>
                                <li><a href="http://www.redhat.com/en/about/privacy-policy" class="privacy-policy">Privacy Statement</a></li>
                                <li><a href="https://www.redhat.com/en/about/terms-use" class="terms-of-use">Terms of Use</a></li>
                                <li><a href="http://www.redhat.com/en/about/all-policies-guidelines" class="all-policies">All Policies and Guidelines</a></li>
                                <li><a id="teconsent"></a></li>
                            </ul>
                            <div id="privacy_policy">We've updated our <a href='http://www.redhat.com/en/about/privacy-policy' class='privacy-policy'>Privacy Statement</a> effective September 15, 2023.
                            </div>
                          </div>
                        </div>
                </div>
                <div class="social">
                    <a href="http://www.redhat.com/summit/" class="summit">
                        <img src="https://access.redhat.com/chrome_themes/nimbus/img/rh-summit-red-a.svg" alt="Red Hat Summit" /> <span class="offscreen">Red Hat Summit</span>
                    </a>

                    <div class="social-media">
                        <a href="https://twitter.com/RedHat" class="sm-icon twitter"><span class="nicon-twitter"></span><span class="offscreen">Twitter</span></a>                        
                    </div>
                </div>
            </div>
        </div>
    </footer>
    <!-- TrustArc -->
    <div id="consent_blackbar"></div> 
    <!--googleon: all-->
</div>
<!-- /CP_PRIMER_FOOTER -->


  </div>

    
  </body>
</html>
