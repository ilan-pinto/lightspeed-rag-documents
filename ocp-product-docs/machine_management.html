<!DOCTYPE html>
<html lang="en" dir="ltr" prefix="og: https://ogp.me/ns#">
  <head>
    <meta charset="utf-8" />
<link rel="canonical" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/index" />
<link rel="shortlink" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/index" />
<meta property="og:site_name" content="Red Hat Customer Portal" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/index" />
<meta property="og:title" content="Machine management OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<meta property="og:description" content="This document provides instructions for managing the machines that make up an OpenShift Container Platform cluster. Some tasks make use of the enhanced automatic machine management functions of an OpenShift Container Platform cluster and some tasks are manual. Not all tasks that are described in this document are available in all installation types." />
<meta property="og:image" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:description" content="This document provides instructions for managing the machines that make up an OpenShift Container Platform cluster. Some tasks make use of the enhanced automatic machine management functions of an OpenShift Container Platform cluster and some tasks are manual. Not all tasks that are described in this document are available in all installation types." />
<meta name="twitter:title" content="Machine management OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<meta name="twitter:url" content="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/index" />
<meta name="twitter:image" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
<meta name="title" content="Machine management OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<link rel="alternate" hreflang="en" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/index" />
<link rel="alternate" hreflang="ko" href="https://access.redhat.com/documentation/ko-kr/openshift_container_platform/4.13/html-single/machine_management/index" />
<link rel="alternate" hreflang="zh-hans" href="https://access.redhat.com/documentation/zh-cn/openshift_container_platform/4.13/html-single/machine_management/index" />
<link rel="alternate" hreflang="ja" href="https://access.redhat.com/documentation/ja-jp/openshift_container_platform/4.13/html-single/machine_management/index" />
<link rel="alternate" hreflang="es" href="https://access.redhat.com/documentation/es-es/openshift_container_platform/4.13/html-single/machine_management/index" />
<link rel="alternate" hreflang="ru" href="https://access.redhat.com/documentation/ru-ru/openshift_container_platform/4.13/html-single/machine_management/index" />
<link rel="alternate" hreflang="pt-br" href="https://access.redhat.com/documentation/pt-br/openshift_container_platform/4.13/html-single/machine_management/index" />
<link rel="alternate" hreflang="it" href="https://access.redhat.com/documentation/it-it/openshift_container_platform/4.13/html-single/machine_management/index" />
<link rel="alternate" hreflang="de" href="https://access.redhat.com/documentation/de-de/openshift_container_platform/4.13/html-single/machine_management/index" />
<link rel="alternate" hreflang="fr" href="https://access.redhat.com/documentation/fr-fr/openshift_container_platform/4.13/html-single/machine_management/index" />
<link rel="alternate" hreflang="zh-hant" href="https://access.redhat.com/documentation/zh-tw/openshift_container_platform/4.13/html-single/machine_management/index" />
<link rel="alternate" hreflang="id" href="https://access.redhat.com/documentation/id-id/openshift_container_platform/4.13/html-single/machine_management/index" />
<link rel="alternate" hreflang="th" href="https://access.redhat.com/documentation/th-th/openshift_container_platform/4.13/html-single/machine_management/index" />
<link rel="alternate" hreflang="vi" href="https://access.redhat.com/documentation/vi-vn/openshift_container_platform/4.13/html-single/machine_management/index" />
<meta name="Generator" content="Drupal 9 (https://www.drupal.org)" />
<meta name="MobileOptimized" content="width" />
<meta name="HandheldFriendly" content="true" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="revision" product="b0738f19-59ac-47eb-9512-8a439cd6dfb0" title="27029d0b-d85e-4a00-843c-108a37d7f571" page="39180644-e2c0-460d-b89d-b8d58313ef9f" revision="a9b2f940183f7b22b049f578bab422b40265d849:en-us" body="020e976309abc89461d2510bee5a6224.html" toc="371c41002bf72d60941f9c8899e77e50.json" />

    <title>Machine management OpenShift Container Platform 4.13 | Red Hat Customer Portal</title>
    <link rel="stylesheet" media="all" href="/sites/dxp-docs/files/css/css_87GMcmxT1ib8ziQiU2KUAnTDFtZQV6iP-KGslA9LigM.css" />
<link rel="stylesheet" media="all" href="/sites/dxp-docs/files/css/css__Xq4GfgPDJw9K_yYJFmlRZGJeCENu3R3r4s0K7Tr_9g.css" />

    
    <script type="application/json" data-drupal-selector="drupal-settings-json">{"path":{"baseUrl":"\/","scriptPath":null,"pathPrefix":"","currentPath":"documentation\/en-us\/openshift_container_platform\/4.13\/html-single\/machine_management\/index","currentPathIsAdmin":false,"isFront":false,"currentLanguage":"en"},"pluralDelimiter":"\u0003","suppressDeprecationErrors":true,"red_hat_jwt":{"client_id":"customer-portal","cookie_name":"rh_jwt","leeway":"0","realm":"redhat-external","sso_host":"https:\/\/sso.redhat.com\/","user_integration":1,"user_plugin":"drupal_user_auth","use_external_js":0,"use_internal_js":0,"use_in_admin":0},"user":{"uid":0,"permissionsHash":"d8ea0bce2d740dacbdfe0257cf55baa0e33f7fb8468a26d055ce75daaaa2d315"}}</script>
<script src="/sites/dxp-docs/files/js/js_EQWKo9EokWkWS99x_e1oM-NEM0zlKyTkp_83mGdm5Ks.js"></script>

    <!-- CP_PRIMER_HEAD -->  <!-- TrustArc & DTM -->
  <script src="//static.redhat.com/libs/redhat/marketing/latest/trustarc/trustarc.js"></script>
  <script src="//www.redhat.com/dtm.js"></script><meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<!--[if IEMobile]><meta http-equiv="cleartype" content="on"><![endif]-->

<!-- metaInclude -->
<meta name="avalon-host-info" content="dxp-kbase-prod-139-77b4fb8768-25dr9" />
<meta name="avalon-version" content="27861f77" />
<meta name="cp-chrome-build-date" content="2023-10-06T19:17:59.039Z" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<!-- Chrome, Firefox OS and Opera -->
<meta name="theme-color" content="#000000">
<!-- Windows Phone -->
<meta name="msapplication-navbutton-color" content="#000000">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-status-bar-style" content="#000000">
<link rel="manifest" href="https://access.redhat.com/webassets/avalon/j/manifest.json">
<!-- Open Search - Tap to search -->
<link rel="search" type="application/opensearchdescription+xml" title="Red Hat Customer Portal" href="https://access.redhat.com/webassets/avalon/j/opensearch.xml" />
<!-- title -->
<title>Red Hat Customer Portal - Access to 24x7 support and knowledge</title>
<!-- /title -->
<script type="text/javascript">
    window.portal = {
        analytics : {},
        host      : "https://access.redhat.com",
        idp_url   : "https://sso.redhat.com",
        lang      : "en", 
        version   : "27861f77",
        builddate : "2023-10-06T19:17:59.039Z",        fetchdate : "2023-10-10T17:45:08-0400",        nrid      : "NOLONGERSUPPORTED",
        nrlk      : "NOLONGERSUPPORTED"
    };
</script>
<script type="text/javascript">
    if (!/\/logout.*/.test(location.pathname) && portal.host === location.origin && document.cookie.indexOf('rh_sso_session') >= 0 && !(document.cookie.indexOf('rh_jwt') >= 0)) window.location = '/login?redirectTo=' + encodeURIComponent(window.location.href);
</script>
<!-- cssInclude -->

<link rel="shortcut icon" href="https://access.redhat.com/webassets/avalon/g/favicon.ico" /><link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/bootstrap.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/bootstrap-grid.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/main.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/components.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/pages.css?v=27861f77" />

<link href="https://access.redhat.com/webassets/avalon/s/chosen.css?v=27861f77" rel="stylesheet" type="text/css" />

<!--[if lte IE 9]>
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/ie.css" />
<![endif]--><noscript>
    <style type="text/css" media="screen"> .primary-nav { display: block; } </style>
</noscript>
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/webassets/avalon/j/public_modules/node_modules/@cpelements/pfe-navigation/dist/pfe-navigation--lightdom.min.css" />
<!-- /cssInclude -->
<script src="https://access.redhat.com/webassets/avalon/j/public_modules/node_modules/@cpelements/pfe-navigation/dist/ie-polyfills.js?v=27861f77"></script>

<script async>
  if (!HTMLScriptElement.supports || !HTMLScriptElement.supports('importmap')) {
    import("https://www.redhatstatic.com/dx/v1-alpha/es-module-shims@1.7.3.js");
  }
</script>
<script type="importmap">
{
  "imports": {
    "@patternfly/elements/" : "https://www.redhatstatic.com/dx/v1-alpha/@patternfly/elements@2.2.2/",
    "@rhds/elements/":"https://www.redhatstatic.com/dx/v1-alpha/@rhds/elements@1.1.0/elements/",
    "@rhds/elements/lib/":"https://www.redhatstatic.com/dx/v1-alpha/@rhds/elements@1.1.0/lib/",
    "@cpelements/elements/":"https://www.redhatstatic.com/dx/v1-alpha/@cpelements/elements@2.0.0-alpha.7/elements/"
  }
}
</script><script type="text/javascript" src="https://access.redhat.com/webassets/avalon/j/lib/require.js?v=27861f77" data-main="/webassets/avalon/j/"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<script src="https://access.redhat.com/chrome_themes/nimbus/js/ie8.js"></script>
<![endif]-->
<script type="text/javascript" src="https://access.redhat.com/chrome_themes/nimbus/js/new-nav.js?v=27861f77" ></script>
<!-- /CP_PRIMER_HEAD -->

  </head>
  <body>
    
      <div class="dialog-off-canvas-main-canvas" data-off-canvas-main-canvas>
      <!-- CP_PRIMER_HEADER -->
<div id="page-wrap" class="page-wrap">
    <div id="pers-top-page-wrap" class="top-page-wrap pers-loader-bg">

      <div id="hero-bg-top-left" class="summit-bg-shapes"></div>
      <div id="hero-bg-top-right" class="summit-bg-shapes"></div>

        <!--googleoff: all-->
        <header class="masthead" id="masthead">

            <a href="#pfe-navigation" id="global-skip-to-nav" class="skip-link visually-hidden">Skip to navigation</a>
            <a href="#cp-main" class="skip-link visually-hidden">Skip to main content</a>            <nav id="portal-utility-nav" class="utility-navigation utility-navigation--bar hidden-at-mobile" data-analytics-region="utility" aria-labelledby="nav__utility-nav--desktop">
                <h3 id="nav__utility-nav--desktop" class="element-invisible">Utilities
                </h3>
                <ul aria-labelledby="nav__utility-nav--desktop">
                    <li id="nav-subscription" data-portal-tour-1="1">
                        <a class="top-nav-subscriptions" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Subscription" href="https://access.redhat.com/management/" >Subscriptions
                        </a>
                    </li>
                    <li id="nav-downloads" data-portal-tour-1="2">
                        <a class="top-nav-downloads" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Downloads" href="https://access.redhat.com/downloads/" >Downloads
                        </a>
                    </li>
                    <li id="nav-containers">
                        <a class="top-nav-containers" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Containers" href="https://catalog.redhat.com/software/containers/explore/" >Containers
                        </a>
                    </li>
                    <li id="nav-support" data-portal-tour-1="3">
                        <a class="top-nav-support-cases" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Support Cases" href="https://access.redhat.com/support/cases/" >Support Cases
                        </a>
                    </li>
                </ul>
            </nav>

            <pfe-navigation id="pfe-navigation" data-analytics-region="mega menu">
                <div class="pfe-navigation__logo-wrapper" id="pfe-navigation__logo-wrapper">
                    <a href="https://access.redhat.com/" class="pfe-navigation__logo-link" data-analytics-text="logo" data-analytics-category="MM|logo">
                        <img class="pfe-navigation__logo-image" alt="Red Hat Customer Portal" src="https://access.redhat.com/chrome_themes/nimbus/img/red-hat-customer-portal.svg" />
                    </a>
                </div>

                <nav class="pfe-navigation" aria-label="Main Navigation" data-analytics-region="main nav">
                    <ul class="pfe-navigation__menu" id="pfe-navigation__menu">                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-subscription--mobile" data-portal-tour-1="1">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Subscription" href="https://access.redhat.com/management/" >Subscriptions
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-downloads--mobile" data-portal-tour-1="2">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Downloads" href="https://access.redhat.com/downloads/" >Downloads
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-containers--mobile">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Containers" href="https://catalog.redhat.com/software/containers/explore/" >Containers
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-support--mobile" data-portal-tour-1="3">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Support Cases" href="https://access.redhat.com/support/cases/" >Support Cases
                            </a>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a href="https://access.redhat.com/products/" class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Products and Services">Products &amp; Services
                            </a>
                            <div class="pfe-navigation__dropdown has-primary-detail">                                <div class="desktop-col-span-2 tablet-col-span-all">
                                    <h3>
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Products" href="https://access.redhat.com/products/">Products
                                        </a>
                                    </h3>
                                    <slot name="main-menu__dropdown--product__product-listing"></slot>
                                </div>                                <div>
                                    <h3 id="nav__products__support">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Support" href="https://access.redhat.com/support">Support
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__support">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Production Support" href="https://access.redhat.com/support/offerings/production/">Production Support
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Development Support" href="https://access.redhat.com/support/offerings/developer/">Development Support
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Product Life Cycles" href="https://access.redhat.com/product-life-cycles/">Product Life Cycles
                                                    </a></li>
                                    </ul>

                                    <h3 id="nav__products__services">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Services" href="https://www.redhat.com/en/services">Services
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__services">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Consulting" href="https://www.redhat.com/en/services/consulting">Consulting
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Technical Account Management" href="https://access.redhat.com/support/offerings/tam/">Technical Account Management
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Training and Certifications" href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications
                                                    </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__products__documentation">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Documentation" href="https://access.redhat.com/documentation">Documentation
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__documentation">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat Enterprise Linux" href="https://access.redhat.com/documentation/en/red_hat_enterprise_linux">Red Hat Enterprise Linux
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat JBoss Enterprise Application Platform" href="https://access.redhat.com/documentation/en/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat OpenStack Platform" href="https://access.redhat.com/documentation/en/red_hat_openstack_platform">Red Hat OpenStack Platform
                                                    </a></li>
                                                    <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat OpenShift Container Platform" href="https://access.redhat.com/documentation/en/openshift_container_platform">Red Hat OpenShift Container Platform
                                                        </a></li>
                                    </ul>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="All Documentation" data-analytics-linkType="cta" href="https://access.redhat.com/documentation">All Documentation
                                        </a>
                                    </pfe-cta>

                                    <h3 id="nav__products__catalog"><a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Ecosystem Catalog" href="https://catalog.redhat.com/">Ecosystem Catalog
                                        </a></h3>
                                        <ul aria-labelledby="nav__products__catalog">
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Ecosystem Catalog" data-analytics-text="Red Hat Partner Ecosystem" href="https://access.redhat.com/ecosystem/">Red Hat Partner Ecosystem
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Ecosystem Catalog" data-analytics-text="Partner Resources" href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources
                                                    </a></li>
                                        </ul>
                                </div>
                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Tools" href="https://access.redhat.com/labs/">Tools
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="nav__tools__tools" data-analytics-level="2" data-analytics-text="Tools" data-analytics-category="Tools">Tools
                                    </h3>
                                    <ul aria-labelledby="nav__tools__tools">
                                        <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Solution Engine" href="https://access.redhat.com/support/cases/#/troubleshoot">Troubleshoot a product issue
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Packages" href="https://access.redhat.com/downloads/content/package-browser">Packages
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Errata" href="https://access.redhat.com/errata/">Errata
                                                    </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__tools__labs">
                                        <a data-analytics-level="2" data-analytics-category="Tools" data-analytics-text="Customer Portal Labs" href="https://access.redhat.com/labs/">Customer Portal Labs
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__tools__labs">
                                        <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Configuration" href="https://access.redhat.com/labs/#!?type=config">Configuration
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Deployment" href="https://access.redhat.com/labs/#!?type=deploy">Deployment
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Security" href="https://access.redhat.com/labs/#!?type=security">Security
                                                    </a></li>                                                    <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Troubleshooting" href="https://access.redhat.com/labs/#!?type=troubleshoot">Troubleshoot
                                                        </a></li>
                                    </ul>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="All Labs" data-analytics-linkType="cta" href="https://access.redhat.com/labs/">All labs
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h4 id="nav__tools__red-hat-insights">
                                        <a data-analytics-level="2" data-analytics-category="Tools" data-analytics-text="Red Hat Insights" href="//www.redhat.com/en/technologies/management/insights">Red Hat Insights
                                        </a>
                                    </h4>
                                    <p>Increase visibility into IT operations to detect and resolve technical issues before they impact your business.</p>
                                    <a data-analytics-level="3" data-analytics-category="Tools|Red Hat Insights" data-analytics-text="Learn more" href="https://www.redhat.com/en/technologies/management/insights">Learn More
                                    </a>
                                    <br>
                                    <a data-analytics-level="3" data-analytics-category="Tools|Red Hat Insights" data-analytics-text="Go to Insights" href="https://cloud.redhat.com/insights">Go to Insights
                                    </a>
                                </div>
                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Security" href="https://access.redhat.com/security/">Security
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="security__security-center">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Red Hat Product Security Center" href="https://access.redhat.com/security">Red Hat Product Security Center
                                        </a>
                                    </h3>
                                    <p>Engage with our Red Hat Product Security team, access security updates, and ensure your environments are not exposed to any known security vulnerabilities.
                                    </p>
                                    <pfe-cta pfe-priority="primary">
                                        <a data-analytics-level="3" data-analytics-category="Security|Red Hat Product Security Center" data-analytics-text="Product Security Center" data-analytics-linkType="cta" href="https://access.redhat.com/security/">Product Security Center
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__security__updates" data-analytics-level="2" data-analytics-text="Security Updates" data-analytics-category="Security">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Security Updates" href="/security">Security Updates
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__security__updates">
                                        <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Security Advisories" href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Red Hat CVE Database" href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Security Labs" href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs
                                                    </a></li>
                                    </ul>
                                    <p class="margin-top-xl">Keep your systems secure with Red Hat&#039;s specialized responses to security vulnerabilities.
                                    </p>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="View Responses" data-analytics-linkType="cta" href="https://access.redhat.com/security/vulnerability">View Responses
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__security__resources">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Resources" href="https://access.redhat.com/security/overview">Resources
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__security__resources">                                            <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Security Blog" href="//redhat.com/en/blog/channel/security">Security Blog
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Security Measurement" href="https://www.redhat.com/security/data/metrics/">Security Measurement
                                                    </a></li>
                                                    <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Severity Ratings" href="https://access.redhat.com/security/updates/classification/">Severity Ratings
                                                        </a></li>
                                                        <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Backporting Policies" href="https://access.redhat.com/security/updates/backporting/">Backporting Policies
                                                            </a></li>
                                                            <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Product Signing (GPG) Keys" href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys
                                                                </a></li>
                                    </ul>
                                </div>

                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a href="https://access.redhat.com/community/" class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Community">Community
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="nav__community__cp-community">
                                        <a href="https://access.redhat.com/community" data-analytics-level="2" data-analytics-text="Customer Portal Community" data-analytics-text="Customer Portal Community" data-analytics-category="Community">Customer Portal Community
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__community__cp-community">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Discussions" href="https://access.redhat.com/discussions">Discussions
                                            </a></li>                                                <li><a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Private Groups" href="https://access.redhat.com/groups/">Private Groups
                                                    </a></li>
                                    </ul>

                                    <pfe-cta pfe-priority="primary">
                                        <a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Community Activity" data-analytics-linkType="cta" href="https://access.redhat.com/community/">Community Activity
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__community__events" data-analytics-level="2" data-analytics-text="Customer Events" data-analytics-category="Community">Customer Events
                                    </h3>
                                    <ul aria-labelledby="nav__community__events">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Customer Events" data-analytics-text="Red Hat Convergence" href="https://access.redhat.com/convergence/">Red Hat Convergence
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Community|Customer Events" data-analytics-text="Red Hat Summit" href="http://www.redhat.com/summit/">Red Hat Summit
                                                </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__community__stories" data-analytics-level="2" data-analytics-text="Stories" data-analytics-category="Community">Stories
                                    </h3>
                                    <ul aria-labelledby="nav__community__stories">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Stories" data-analytics-text="Red Hat Subscription Value" href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-text="You Asked. We Acted." data-analytics-category="Community|Stories" href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Community|Stories" data-analytics-text="Open Source Communities" href="http://www.redhat.com/en/open-source">Open Source Communities
                                                    </a></li>
                                    </ul>
                                </div>
                            </div>
                        </li>
                    </ul>                </nav>                <div id="site-search" slot="search" class="utility-link site-search">
                    <div class="content">
                        <form class="ng-pristine ng-valid topSearchForm" id="topSearchForm" name="topSearchForm" action="/search/browse/search/" method="get" enctype="application/x-www-form-urlencoded">
                            <cp-search-autocomplete class="push-bottom" path="/webassets/avalon/j/data.json"></cp-search-autocomplete>                            <div>Or <a href="/support/cases/#/troubleshoot">troubleshoot an issue</a>.
                            </div>
                        </form>
                    </div>
                </div>


                <div slot="secondary-links" id="localesMenu">
                    <button class="pfe-navigation__secondary-link">
                        <pfe-icon icon="web-icon-globe" size="sm" aria-hidden="true"></pfe-icon>English
                    </button>

                    <pfe-navigation-dropdown dropdown-width="single">
                        <h2 class="utility-header">Select Your Language
                        </h2>
                        <ul class="reset">
                            <li><a href="https://access.redhat.com/changeLanguage?language=en" data-lang="en" id="en" data-analytics-text="English">English</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=ko" data-lang="ko" id="ko" data-analytics-text="Korean">한국어</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=ja"    data-lang="ja"    id="ja" data-analytics-text="Japanese">日本語</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" data-lang="zh_CN" id="zh_CN" data-analytics-text="Chinese">中文 (中国)</a></li>
                        </ul>

                    </pfe-navigation-dropdown>
                </div>                <rh-account-dropdown slot="account"></rh-account-dropdown>                <pfe-primary-detail breakpoint-width="600" class="main-menu__dropdown--product__product-listing" slot="main-menu__dropdown--product__product-listing" consistent-height>
                    <h3 slot="details-nav">Infrastructure and Management                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Enterprise Linux" href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Satellite" href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Subscription Management" href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Insights" href="https://access.redhat.com/products/red-hat-insights/">Red Hat Insights
                                </a>
                            </li>
                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Ansible Automation Platform" href="https://access.redhat.com/products/red-hat-ansible-automation-platform/">Red Hat Ansible Automation Platform
                                </a></li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Cloud Computing                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift" href="https://access.redhat.com/products/openshift">Red Hat OpenShift
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenStack Platform" href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat OpenShift Container Platform" href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat OpenShift Data Science" href="https://access.redhat.com/products/red-hat-openshift-data-science/">Red Hat OpenShift Data Science
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift Dedicated" href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat Advanced Cluster Security for Kubernetes" href="https://access.redhat.com/products/red-hat-advanced-cluster-security-for-kubernetes/">Red Hat Advanced Cluster Security for Kubernetes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat Advanced Cluster Management for Kubernetes" href="https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes/">Red Hat Advanced Cluster Management for Kubernetes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat Quay" href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat CodeReady Workspaces" href="https://access.redhat.com/products/red-hat-codeready-workspaces/">OpenShift Dev Spaces
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift Service on AWS" href="https://access.redhat.com/products/red-hat-openshift-service-aws">Red Hat OpenShift Service on AWS
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Storage                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Gluster Storage" href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Hyperconverged Infrastructure" href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Ceph Storage" href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Openshift Container Storage" href="https://access.redhat.com/products/red-hat-openshift-data-foundation">Red Hat OpenShift Data Foundation
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Runtimes                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Runtimes" href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat JBoss Enterprise Application Platform" href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Data Grid" href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat JBoss Web Server" href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Single Sign On" href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat support for Spring Boot" href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat build of Node.js" href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js
                                </a>
                            </li>                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat build of Quarkus" href="https://access.redhat.com/products/quarkus/">Red Hat build of Quarkus
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Integration and Automation                    </h3>
                    <div slot="details">
                        <ul class="border-bottom" id="portal-menu-border-bottom">
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat Application Foundations" href="https://access.redhat.com/products/red-hat-application-foundations/">Red Hat Application Foundations
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat Fuse" href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat AMQ" href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat 3scale API Management" href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management
                                </a>
                            </li>
                        </ul>
                    </div>
                    <div slot="details-nav--footer">
                        <pfe-cta pfe-priority="primary">
                            <a href="https://access.redhat.com/products/" class="pfe-navigation__menu-link" data-analytics-level="2" data-analytics-text="All Products" data-analytics-category="Products and Services|Products:" data-analytics-linkType="cta">All Products
                            </a>
                        </pfe-cta>
                    </div>
                </pfe-primary-detail>

            </pfe-navigation>

            <div id="scroll-anchor"></div>

            <!--[if IE 8]>
                <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                <span class="icon-warning alert-icon" aria-hidden="true"></span>
                You are using an unsupported web browser. Update to a supported browser for the best experience. <a href="/announcements/2120951">Read the announcement</a>.
                </div>
                </div>
            <![endif]-->
            <!--[if IE 9]>
                <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                <span class="icon-warning alert-icon" aria-hidden="true"></span>As of March 1, 2016, the Red Hat Customer Portal will no longer support Internet Explorer 9. See our new <a href="/help/browsers">browser support policy</a> for more information.
                </div>
                </div>
            <![endif]-->
            <div id="site-section"></div>
        </header>
        <!--googleon: all-->

        <main id="cp-main" class="portal-content-area">
            <div id="cp-content" class="main-content">                            <!-- /CP_PRIMER_HEADER -->

      <div class="container">
        

                                                                                                        <script>breadcrumbs = [["Products & Services","\/products\/"],["Product Documentation","\/documentation"],["OpenShift Container Platform","\/documentation\/en-us\/openshift_container_platform"],["4.13","\/documentation\/en-us\/openshift_container_platform\/4.13"],["Machine management","\/documentation\/en-us\/openshift_container_platform\/4.13\/html\/machine_management"],["Machine management","\/documentation\/en-us\/openshift_container_platform\/4.13\/html\/machine_management\/--single-page-document--"]]</script>

<div data-drupal-messages-fallback class="hidden"></div>


    </div>
        <div class="container">
        

  

  


  <article class="pvof-doc__content-wrapper__outer pvof-doc__content-wrapper__outer--css-not-removed">
    <script>
      'use strict';

            var $outerWrapper = document.querySelector('.pvof-doc__content-wrapper__outer');
      if ($outerWrapper && $outerWrapper.closest) {
        var $containerWrapper = $outerWrapper.closest('.container');
        if ($containerWrapper) {
          $containerWrapper.classList.remove('container');
          $containerWrapper.classList.add('j-chrome-content-container');
        }
      }

            var cssRemoved = false;
      try {
        var $crapCss = document.querySelectorAll(
          'link[href*="/chrome_themes/nimbus/css/pages.css"], link[href*="/chrome_themes/nimbus/css/components.css"]'
        );
        if ($crapCss.length) {
          for (let index = 0; index < $crapCss.length; index++) {
            const $stylesheet = $crapCss[index];
            $stylesheet.remove();
          }
        }
        cssRemoved = true;
      }
      catch (error) {
        console.error('Ran into an issue while trying to retheme page', error);
        cssRemoved = false;
      }

            if (cssRemoved) {
        var $pvofOuterWrapper = document.querySelector('.pvof-doc__content-wrapper__outer--css-not-removed');
        if ($pvofOuterWrapper) {
          $pvofOuterWrapper.classList.remove('pvof-doc__content-wrapper__outer--css-not-removed');
        }
      }
    </script>
    <div itemscope="" itemtype="https://schema.org/TechArticle" itemref="techArticle-md1 techArticle-md2 techArticle-md3"></div>
    <div itemscope="" itemtype="https://schema.org/SoftwareApplication" itemref="softwareApplication-md1 softwareApplication-md2 softwareApplication-md3 softwareApplication-md4"></div>
    <div class="pvof-doc__content-wrapper pvof-doc__content-wrapper--has-sidebar">
                                <div class="pvof-doc__content-wrapper__inner j-superdoc j-superdoc--has-nav">
                            <div class="pvof-sidebar__wrapper j-doc-nav j-superdoc__nav">
            <div class="j-sidebar__menu-container">
              <button class="j-sidebar__menu-trigger content-expander__trigger">
                <span class="j-sidebar__menu-trigger__open-text">Jump To</span>
                <span class="j-sidebar__menu-trigger__close-text">Close</span>
              </button>

              <div class="pvof-sidebar__inner-wrapper j-doc-nav__wrapper content-expander">
                <div class="j-sidebar__menu-details-container">
                  <button class="j-sidebar__menu-details-button j-sidebar__menu-details-button--expand">
                    Expand all
                  </button>
                  <button class="j-sidebar__menu-details-button j-sidebar__menu-details-button--collapse">
                    Collapse all
                  </button>
                </div>
                

  <nav id="pvof-doc__toc" class="pvof-doc__toc">
  <h2 class="j-doc-nav__title" id="j-doc-nav__title">
    Table of contents
  </h2>
  <div class="pvof-doc__toc-inner">
              <ol class="j-doc-nav__list" aria-labelledby="j-doc-nav__title">
                  <li class="j-doc-nav__list-item">
                                    

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management" class="j-doc-nav__link ">
    Machine management
  </a>
  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#overview-of-machine-management" class="j-doc-nav__link j-doc-nav__link--has-children">
    1. Overview of machine management
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "1. Overview of machine management"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "1. Overview of machine management"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-api-overview_overview-of-machine-management" class="j-doc-nav__link ">
    1.1. Machine API overview
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-mgmt-intro-managing-compute_overview-of-machine-management" class="j-doc-nav__link ">
    1.2. Managing compute machines
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-mgmt-intro-managing-control-plane_overview-of-machine-management" class="j-doc-nav__link ">
    1.3. Managing control plane machines
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-mgmt-intro-autoscaling_overview-of-machine-management" class="j-doc-nav__link ">
    1.4. Applying autoscaling to an OpenShift Container Platform cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-mgmt-intro-add-for-upi_overview-of-machine-management" class="j-doc-nav__link ">
    1.5. Adding compute machines on user-provisioned infrastructure
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-mgmt-intro-add-rhel_overview-of-machine-management" class="j-doc-nav__link ">
    1.6. Adding RHEL compute machines to your cluster
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#managing-compute-machines-with-the-machine-api" class="j-doc-nav__link j-doc-nav__link--has-children">
    2. Managing compute machines with the Machine API
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2. Managing compute machines with the Machine API"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2. Managing compute machines with the Machine API"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#creating-machineset-alibaba" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.1. Creating a compute machine set on Alibaba Cloud
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.1. Creating a compute machine set on Alibaba Cloud"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.1. Creating a compute machine set on Alibaba Cloud"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-alibaba_creating-machineset-alibaba" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.1.1. Sample YAML for a compute machine set custom resource on Alibaba Cloud
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.1.1. Sample YAML for a compute machine set custom resource on Alibaba Cloud"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.1.1. Sample YAML for a compute machine set custom resource on Alibaba Cloud"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-alibaba-usage-stats_creating-machineset-alibaba" class="j-doc-nav__link ">
    2.1.1.1. Machine set parameters for Alibaba Cloud usage statistics
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating_creating-machineset-alibaba" class="j-doc-nav__link ">
    2.1.2. Creating a compute machine set
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#creating-machineset-aws" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.2. Creating a compute machine set on AWS
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.2. Creating a compute machine set on AWS"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.2. Creating a compute machine set on AWS"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-aws_creating-machineset-aws" class="j-doc-nav__link ">
    2.2.1. Sample YAML for a compute machine set custom resource on AWS
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating_creating-machineset-aws" class="j-doc-nav__link ">
    2.2.2. Creating a compute machine set
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-imds-options_creating-machineset-aws" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.2.3. Machine set options for the Amazon EC2 Instance Metadata Service
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.2.3. Machine set options for the Amazon EC2 Instance Metadata Service"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.2.3. Machine set options for the Amazon EC2 Instance Metadata Service"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating-imds-options_creating-machineset-aws" class="j-doc-nav__link ">
    2.2.3.1. Configuring IMDS by using machine sets
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-dedicated-instance_creating-machineset-aws" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.2.4. Machine sets that deploy machines as Dedicated Instances
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.2.4. Machine sets that deploy machines as Dedicated Instances"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.2.4. Machine sets that deploy machines as Dedicated Instances"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating-dedicated-instance_creating-machineset-aws" class="j-doc-nav__link ">
    2.2.4.1. Creating Dedicated Instances by using machine sets
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-non-guaranteed-instance_creating-machineset-aws" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.2.5. Machine sets that deploy machines as Spot Instances
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.2.5. Machine sets that deploy machines as Spot Instances"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.2.5. Machine sets that deploy machines as Spot Instances"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating-non-guaranteed-instance_creating-machineset-aws" class="j-doc-nav__link ">
    2.2.5.1. Creating Spot Instances by using compute machine sets
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#nvidia-gpu-aws-adding-a-gpu-node_creating-machineset-aws" class="j-doc-nav__link ">
    2.2.6. Adding a GPU node to an existing OpenShift Container Platform cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#nvidia-gpu-aws-deploying-the-node-feature-discovery-operator_creating-machineset-aws" class="j-doc-nav__link ">
    2.2.7. Deploying the Node Feature Discovery Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#creating-machineset-azure" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.3. Creating a compute machine set on Azure
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.3. Creating a compute machine set on Azure"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.3. Creating a compute machine set on Azure"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-azure_creating-machineset-azure" class="j-doc-nav__link ">
    2.3.1. Sample YAML for a compute machine set custom resource on Azure
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating_creating-machineset-azure" class="j-doc-nav__link ">
    2.3.2. Creating a compute machine set
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#installation-azure-marketplace-subscribe_creating-machineset-azure" class="j-doc-nav__link ">
    2.3.3. Selecting an Azure Marketplace image
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-azure-boot-diagnostics_creating-machineset-azure" class="j-doc-nav__link ">
    2.3.4. Enabling Azure boot diagnostics
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-non-guaranteed-instance_creating-machineset-azure" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.3.5. Machine sets that deploy machines as Spot VMs
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.3.5. Machine sets that deploy machines as Spot VMs"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.3.5. Machine sets that deploy machines as Spot VMs"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating-non-guaranteed-instance_creating-machineset-azure" class="j-doc-nav__link ">
    2.3.5.1. Creating Spot VMs by using compute machine sets
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-azure-ephemeral-os_creating-machineset-azure" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.3.6. Machine sets that deploy machines on Ephemeral OS disks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.3.6. Machine sets that deploy machines on Ephemeral OS disks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.3.6. Machine sets that deploy machines on Ephemeral OS disks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating-azure-ephemeral-os_creating-machineset-azure" class="j-doc-nav__link ">
    2.3.6.1. Creating machines on Ephemeral OS disks by using compute machine sets
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-azure-ultra-disk_creating-machineset-azure" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.3.7. Machine sets that deploy machines with ultra disks as data disks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.3.7. Machine sets that deploy machines with ultra disks as data disks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.3.7. Machine sets that deploy machines with ultra disks as data disks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating-azure-ultra-disk_creating-machineset-azure" class="j-doc-nav__link ">
    2.3.7.1. Creating machines with ultra disks by using machine sets
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-troubleshooting-azure-ultra-disk_creating-machineset-azure" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.3.7.2. Troubleshooting resources for machine sets that enable ultra disks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.3.7.2. Troubleshooting resources for machine sets that enable ultra disks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.3.7.2. Troubleshooting resources for machine sets that enable ultra disks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#ts-mapi-attach-misconfigure_creating-machineset-azure" class="j-doc-nav__link ">
    2.3.7.2.1. Incorrect ultra disk configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#ts-mapi-attach-unsupported_creating-machineset-azure" class="j-doc-nav__link ">
    2.3.7.2.2. Unsupported disk parameters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#ts-mapi-delete_creating-machineset-azure" class="j-doc-nav__link ">
    2.3.7.2.3. Unable to delete disks
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-enabling-customer-managed-encryption-azure_creating-machineset-azure" class="j-doc-nav__link ">
    2.3.8. Enabling customer-managed encryption keys for a machine set
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-azure-accelerated-networking_creating-machineset-azure" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.3.9. Accelerated Networking for Microsoft Azure VMs
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.3.9. Accelerated Networking for Microsoft Azure VMs"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.3.9. Accelerated Networking for Microsoft Azure VMs"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-azure-accelerated-networking-limits_creating-machineset-azure" class="j-doc-nav__link ">
    2.3.9.1. Limitations
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#nvidia-gpu-aws-adding-a-gpu-node_creating-machineset-azure" class="j-doc-nav__link ">
    2.3.10. Adding a GPU node to an existing OpenShift Container Platform cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#nvidia-gpu-aws-deploying-the-node-feature-discovery-operator_creating-machineset-azure" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.3.11. Deploying the Node Feature Discovery Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.3.11. Deploying the Node Feature Discovery Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.3.11. Deploying the Node Feature Discovery Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-azure-enabling-accelerated-networking-existing_creating-machineset-azure" class="j-doc-nav__link ">
    2.3.11.1. Enabling Accelerated Networking on an existing Microsoft Azure cluster
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#creating-machineset-azure-stack-hub" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.4. Creating a compute machine set on Azure Stack Hub
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.4. Creating a compute machine set on Azure Stack Hub"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.4. Creating a compute machine set on Azure Stack Hub"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-azure-stack-hub_creating-machineset-azure-stack-hub" class="j-doc-nav__link ">
    2.4.1. Sample YAML for a compute machine set custom resource on Azure Stack Hub
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating_creating-machineset-azure-stack-hub" class="j-doc-nav__link ">
    2.4.2. Creating a compute machine set
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-azure-boot-diagnostics_creating-machineset-azure-stack-hub" class="j-doc-nav__link ">
    2.4.3. Enabling Azure boot diagnostics
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-enabling-customer-managed-encryption-azure_creating-machineset-azure-stack-hub" class="j-doc-nav__link ">
    2.4.4. Enabling customer-managed encryption keys for a machine set
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#creating-machineset-gcp" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.5. Creating a compute machine set on GCP
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.5. Creating a compute machine set on GCP"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.5. Creating a compute machine set on GCP"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-gcp_creating-machineset-gcp" class="j-doc-nav__link ">
    2.5.1. Sample YAML for a compute machine set custom resource on GCP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating_creating-machineset-gcp" class="j-doc-nav__link ">
    2.5.2. Creating a compute machine set
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-gcp-pd-disk-types_creating-machineset-gcp" class="j-doc-nav__link ">
    2.5.3. Configuring persistent disk types by using machine sets
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-gcp-confidential-vm_creating-machineset-gcp" class="j-doc-nav__link ">
    2.5.4. Configuring Confidential VM by using machine sets
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-non-guaranteed-instance_creating-machineset-gcp" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.5.5. Machine sets that deploy machines as preemptible VM instances
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.5.5. Machine sets that deploy machines as preemptible VM instances"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.5.5. Machine sets that deploy machines as preemptible VM instances"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating-non-guaranteed-instance_creating-machineset-gcp" class="j-doc-nav__link ">
    2.5.5.1. Creating preemptible VM instances by using compute machine sets
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-gcp-shielded-vms_creating-machineset-gcp" class="j-doc-nav__link ">
    2.5.6. Configuring Shielded VM options by using machine sets
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-gcp-enabling-customer-managed-encryption_creating-machineset-gcp" class="j-doc-nav__link ">
    2.5.7. Enabling customer-managed encryption keys for a machine set
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-gcp-enabling-gpu-support_creating-machineset-gcp" class="j-doc-nav__link ">
    2.5.8. Enabling GPU support for a compute machine set
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#nvidia-gpu-gcp-adding-a-gpu-node_creating-machineset-gcp" class="j-doc-nav__link ">
    2.5.9. Adding a GPU node to an existing OpenShift Container Platform cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#nvidia-gpu-aws-deploying-the-node-feature-discovery-operator_creating-machineset-gcp" class="j-doc-nav__link ">
    2.5.10. Deploying the Node Feature Discovery Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#creating-machineset-ibm-cloud" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.6. Creating a compute machine set on IBM Cloud
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.6. Creating a compute machine set on IBM Cloud"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.6. Creating a compute machine set on IBM Cloud"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-ibm-cloud_creating-machineset-ibm-cloud" class="j-doc-nav__link ">
    2.6.1. Sample YAML for a compute machine set custom resource on IBM Cloud
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating_creating-machineset-ibm-cloud" class="j-doc-nav__link ">
    2.6.2. Creating a compute machine set
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#creating-machineset-ibm-power-vs" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.7. Creating a compute machine set on IBM Power Virtual Server
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.7. Creating a compute machine set on IBM Power Virtual Server"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.7. Creating a compute machine set on IBM Power Virtual Server"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-ibm-power-vs_creating-machineset-ibm-power-vs" class="j-doc-nav__link ">
    2.7.1. Sample YAML for a compute machine set custom resource on IBM Power Virtual Server
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating_creating-machineset-ibm-power-vs" class="j-doc-nav__link ">
    2.7.2. Creating a compute machine set
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#creating-machineset-nutanix" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.8. Creating a compute machine set on Nutanix
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.8. Creating a compute machine set on Nutanix"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.8. Creating a compute machine set on Nutanix"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-nutanix_creating-machineset-nutanix" class="j-doc-nav__link ">
    2.8.1. Sample YAML for a compute machine set custom resource on Nutanix
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating_creating-machineset-nutanix" class="j-doc-nav__link ">
    2.8.2. Creating a compute machine set
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#creating-machineset-osp" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.9. Creating a compute machine set on OpenStack
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.9. Creating a compute machine set on OpenStack"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.9. Creating a compute machine set on OpenStack"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-osp_creating-machineset-osp" class="j-doc-nav__link ">
    2.9.1. Sample YAML for a compute machine set custom resource on RHOSP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-osp-sr-iov_creating-machineset-osp" class="j-doc-nav__link ">
    2.9.2. Sample YAML for a compute machine set custom resource that uses SR-IOV on RHOSP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-osp-sr-iov-port-security_creating-machineset-osp" class="j-doc-nav__link ">
    2.9.3. Sample YAML for SR-IOV deployments where port security is disabled
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating_creating-machineset-osp" class="j-doc-nav__link ">
    2.9.4. Creating a compute machine set
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#creating-machineset-rhv" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.10. Creating a compute machine set on RHV
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.10. Creating a compute machine set on RHV"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.10. Creating a compute machine set on RHV"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-rhv_creating-machineset-rhv" class="j-doc-nav__link ">
    2.10.1. Sample YAML for a compute machine set custom resource on RHV
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating_creating-machineset-rhv" class="j-doc-nav__link ">
    2.10.2. Creating a compute machine set
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#creating-machineset-vsphere" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.11. Creating a compute machine set on vSphere
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.11. Creating a compute machine set on vSphere"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.11. Creating a compute machine set on vSphere"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-vsphere_creating-machineset-vsphere" class="j-doc-nav__link ">
    2.11.1. Sample YAML for a compute machine set custom resource on vSphere
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-vsphere-requirements-user-provisioned-machine-sets_creating-machineset-vsphere" class="j-doc-nav__link ">
    2.11.2. Minimum required vCenter privileges for compute machine set management
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#compute-machineset-upi-reqs_creating-machineset-vsphere" class="j-doc-nav__link ">
    2.11.3. Requirements for clusters with user-provisioned infrastructure to use compute machine sets
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating_creating-machineset-vsphere" class="j-doc-nav__link ">
    2.11.4. Creating a compute machine set
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#creating-machineset-bare-metal" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.12. Creating a compute machine set on bare metal
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.12. Creating a compute machine set on bare metal"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.12. Creating a compute machine set on bare metal"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-vsphere_creating-machineset-bare-metal" class="j-doc-nav__link ">
    2.12.1. Sample YAML for a compute machine set custom resource on bare metal
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating_creating-machineset-bare-metal" class="j-doc-nav__link ">
    2.12.2. Creating a compute machine set
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#manually-scaling-machineset" class="j-doc-nav__link j-doc-nav__link--has-children">
    3. Manually scaling a compute machine set
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3. Manually scaling a compute machine set"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3. Manually scaling a compute machine set"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#prerequisites" class="j-doc-nav__link ">
    3.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-manually-scaling_manually-scaling-machineset" class="j-doc-nav__link ">
    3.2. Scaling a compute machine set manually
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-delete-policy_manually-scaling-machineset" class="j-doc-nav__link ">
    3.3. The compute machine set deletion policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#additional-resources_manually-scaling-machineset" class="j-doc-nav__link ">
    3.4. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#modifying-machineset" class="j-doc-nav__link j-doc-nav__link--has-children">
    4. Modifying a compute machine set
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4. Modifying a compute machine set"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4. Modifying a compute machine set"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-modifying_modifying-machineset" class="j-doc-nav__link ">
    4.1. Modifying a compute machine set
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#migrating-nodes-to-a-different-storage-domain-rhv_modifying-machineset" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.2. Migrating nodes to a different storage domain on RHV
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.2. Migrating nodes to a different storage domain on RHV"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.2. Migrating nodes to a different storage domain on RHV"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-migrating-compute-nodes-to-diff-sd-rhv_modifying-machineset" class="j-doc-nav__link ">
    4.2.1. Migrating compute nodes to a different storage domain in RHV
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-migrating-control-plane-nodes-to-diff-sd-rhv_modifying-machineset" class="j-doc-nav__link ">
    4.2.2. Migrating control plane nodes to a different storage domain on RHV
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#deleting-machine" class="j-doc-nav__link j-doc-nav__link--has-children">
    5. Deleting a machine
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "5. Deleting a machine"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "5. Deleting a machine"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-delete_deleting-machine" class="j-doc-nav__link ">
    5.1. Deleting a specific machine
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-lifecycle-hook-deletion_deleting-machine" class="j-doc-nav__link j-doc-nav__link--has-children">
    5.2. Lifecycle hooks for the machine deletion phase
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "5.2. Lifecycle hooks for the machine deletion phase"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "5.2. Lifecycle hooks for the machine deletion phase"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-lifecycle-hook-deletion-terms_deleting-machine" class="j-doc-nav__link ">
    5.2.1. Terminology and definitions
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-lifecycle-hook-deletion-order_deleting-machine" class="j-doc-nav__link ">
    5.2.2. Machine deletion processing order
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-lifecycle-hook-deletion-format_deleting-machine" class="j-doc-nav__link ">
    5.2.3. Deletion lifecycle hook configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-lifecycle-hook-deletion-uses_deleting-machine" class="j-doc-nav__link ">
    5.2.4. Machine deletion lifecycle hook examples for Operator developers
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-lifecycle-hook-deletion-etcd_deleting-machine" class="j-doc-nav__link j-doc-nav__link--has-children">
    5.2.5. Quorum protection with machine lifecycle hooks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "5.2.5. Quorum protection with machine lifecycle hooks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "5.2.5. Quorum protection with machine lifecycle hooks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-lifecycle-hook-deletion-etcd-order_deleting-machine" class="j-doc-nav__link ">
    5.2.5.1. Control plane deletion with quorum protection processing order
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#additional-resources_unhealthy-etcd-member" class="j-doc-nav__link ">
    5.3. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#applying-autoscaling" class="j-doc-nav__link j-doc-nav__link--has-children">
    6. Applying autoscaling to an OpenShift Container Platform cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6. Applying autoscaling to an OpenShift Container Platform cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6. Applying autoscaling to an OpenShift Container Platform cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cluster-autoscaler-about_applying-autoscaling" class="j-doc-nav__link ">
    6.1. About the cluster autoscaler
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#configuring-clusterautoscaler_applying-autoscaling" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.2. Configuring the cluster autoscaler
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.2. Configuring the cluster autoscaler"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.2. Configuring the cluster autoscaler"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cluster-autoscaler-cr_applying-autoscaling" class="j-doc-nav__link ">
    6.2.1. Cluster autoscaler resource definition
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#ClusterAutoscaler-deploying_applying-autoscaling" class="j-doc-nav__link ">
    6.2.2. Deploying a cluster autoscaler
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-autoscaler-about_applying-autoscaling" class="j-doc-nav__link ">
    6.3. About the machine autoscaler
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#configuring-machineautoscaler_applying-autoscaling" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.4. Configuring machine autoscalers
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.4. Configuring machine autoscalers"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.4. Configuring machine autoscalers"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-autoscaler-cr_applying-autoscaling" class="j-doc-nav__link ">
    6.4.1. Machine autoscaler resource definition
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#MachineAutoscaler-deploying_applying-autoscaling" class="j-doc-nav__link ">
    6.4.2. Deploying a machine autoscaler
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#disabling-autoscaling_applying-autoscaling" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.5. Disabling autoscaling
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.5. Disabling autoscaling"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.5. Disabling autoscaling"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#deleting-machine-autoscaler_applying-autoscaling" class="j-doc-nav__link ">
    6.5.1. Disabling a machine autoscaler
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#deleting-cluster-autoscaler_applying-autoscaling" class="j-doc-nav__link ">
    6.5.2. Disabling the cluster autoscaler
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#additional-resources" class="j-doc-nav__link ">
    6.6. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#creating-infrastructure-machinesets" class="j-doc-nav__link j-doc-nav__link--has-children">
    7. Creating infrastructure machine sets
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7. Creating infrastructure machine sets"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7. Creating infrastructure machine sets"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#infrastructure-components_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.1. OpenShift Container Platform infrastructure components
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#creating-infrastructure-machinesets-production" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.2. Creating infrastructure machine sets for production environments
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.2. Creating infrastructure machine sets for production environments"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.2. Creating infrastructure machine sets for production environments"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#creating-infrastructure-machinesets-clouds" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.2.1. Creating infrastructure machine sets for different clouds
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.2.1. Creating infrastructure machine sets for different clouds"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.2.1. Creating infrastructure machine sets for different clouds"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-alibaba_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.2.1.1. Sample YAML for a compute machine set custom resource on Alibaba Cloud
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-aws_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.2.1.2. Sample YAML for a compute machine set custom resource on AWS
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-azure_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.2.1.3. Sample YAML for a compute machine set custom resource on Azure
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-azure-stack-hub_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.2.1.4. Sample YAML for a compute machine set custom resource on Azure Stack Hub
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-ibm-cloud_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.2.1.5. Sample YAML for a compute machine set custom resource on IBM Cloud
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-gcp_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.2.1.6. Sample YAML for a compute machine set custom resource on GCP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-nutanix_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.2.1.7. Sample YAML for a compute machine set custom resource on Nutanix
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-osp_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.2.1.8. Sample YAML for a compute machine set custom resource on RHOSP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-rhv_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.2.1.9. Sample YAML for a compute machine set custom resource on RHV
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-yaml-vsphere_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.2.1.10. Sample YAML for a compute machine set custom resource on vSphere
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.2.2. Creating a compute machine set
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#creating-an-infra-node_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.2.3. Creating an infrastructure node
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#creating-infra-machines_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.2.4. Creating a machine config pool for infrastructure machines
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#assigning-machineset-resources-to-infra-nodes" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.3. Assigning machine set resources to infrastructure nodes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.3. Assigning machine set resources to infrastructure nodes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.3. Assigning machine set resources to infrastructure nodes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#binding-infra-node-workloads-using-taints-tolerations_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.3.1. Binding infrastructure node workloads using taints and tolerations
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#moving-resources-to-infrastructure-machinesets" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.4. Moving resources to infrastructure machine sets
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.4. Moving resources to infrastructure machine sets"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.4. Moving resources to infrastructure machine sets"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#infrastructure-moving-router_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.4.1. Moving the router
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#infrastructure-moving-registry_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.4.2. Moving the default registry
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#infrastructure-moving-monitoring_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.4.3. Moving the monitoring solution
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#infrastructure-moving-logging_creating-infrastructure-machinesets" class="j-doc-nav__link ">
    7.4.4. Moving OpenShift Logging resources
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#adding-rhel-compute" class="j-doc-nav__link j-doc-nav__link--has-children">
    8. Adding RHEL compute machines to an OpenShift Container Platform cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8. Adding RHEL compute machines to an OpenShift Container Platform cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8. Adding RHEL compute machines to an OpenShift Container Platform cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#rhel-compute-overview_adding-rhel-compute" class="j-doc-nav__link ">
    8.1. About adding RHEL compute nodes to a cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#rhel-compute-requirements_adding-rhel-compute" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.2. System requirements for RHEL compute nodes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.2. System requirements for RHEL compute nodes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.2. System requirements for RHEL compute nodes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#csr-management_adding-rhel-compute" class="j-doc-nav__link ">
    8.2.1. Certificate signing requests management
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#adding-rhel-compute-preparing-image-cloud" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.3. Preparing an image for your cloud
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.3. Preparing an image for your cloud"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.3. Preparing an image for your cloud"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#rhel-images-aws_adding-rhel-compute" class="j-doc-nav__link ">
    8.3.1. Listing latest available RHEL images on AWS
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#rhel-preparing-playbook-machine_adding-rhel-compute" class="j-doc-nav__link ">
    8.4. Preparing the machine to run the playbook
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#rhel-preparing-node_adding-rhel-compute" class="j-doc-nav__link ">
    8.5. Preparing a RHEL compute node
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#rhel-attaching-instance-aws_adding-rhel-compute" class="j-doc-nav__link ">
    8.6. Attaching the role permissions to RHEL instance in AWS
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#rhel-worker-tag_adding-rhel-compute" class="j-doc-nav__link ">
    8.7. Tagging a RHEL worker node as owned or shared
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#rhel-adding-node_adding-rhel-compute" class="j-doc-nav__link ">
    8.8. Adding a RHEL compute machine to your cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#installation-approve-csrs_adding-rhel-compute" class="j-doc-nav__link ">
    8.9. Approving the certificate signing requests for your machines
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#rhel-ansible-parameters_adding-rhel-compute" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.10. Required parameters for the Ansible hosts file
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.10. Required parameters for the Ansible hosts file"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.10. Required parameters for the Ansible hosts file"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#rhel-removing-rhcos_adding-rhel-compute" class="j-doc-nav__link ">
    8.10.1. Optional: Removing RHCOS compute machines from a cluster
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#more-rhel-compute" class="j-doc-nav__link j-doc-nav__link--has-children">
    9. Adding more RHEL compute machines to an OpenShift Container Platform cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9. Adding more RHEL compute machines to an OpenShift Container Platform cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9. Adding more RHEL compute machines to an OpenShift Container Platform cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#rhel-compute-overview_more-rhel-compute" class="j-doc-nav__link ">
    9.1. About adding RHEL compute nodes to a cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#rhel-compute-requirements_more-rhel-compute" class="j-doc-nav__link j-doc-nav__link--has-children">
    9.2. System requirements for RHEL compute nodes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9.2. System requirements for RHEL compute nodes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9.2. System requirements for RHEL compute nodes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#csr-management_more-rhel-compute" class="j-doc-nav__link ">
    9.2.1. Certificate signing requests management
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#more-rhel-compute-preparing-image-cloud" class="j-doc-nav__link j-doc-nav__link--has-children">
    9.3. Preparing an image for your cloud
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9.3. Preparing an image for your cloud"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9.3. Preparing an image for your cloud"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#rhel-images-aws_more-rhel-compute" class="j-doc-nav__link ">
    9.3.1. Listing latest available RHEL images on AWS
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#rhel-preparing-node_more-rhel-compute" class="j-doc-nav__link ">
    9.4. Preparing a RHEL compute node
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#rhel-attaching-instance-aws_more-rhel-compute" class="j-doc-nav__link ">
    9.5. Attaching the role permissions to RHEL instance in AWS
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#rhel-worker-tag_more-rhel-compute" class="j-doc-nav__link ">
    9.6. Tagging a RHEL worker node as owned or shared
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#rhel-adding-more-nodes_more-rhel-compute" class="j-doc-nav__link ">
    9.7. Adding more RHEL compute machines to your cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#installation-approve-csrs_more-rhel-compute" class="j-doc-nav__link ">
    9.8. Approving the certificate signing requests for your machines
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#rhel-ansible-parameters_more-rhel-compute" class="j-doc-nav__link ">
    9.9. Required parameters for the Ansible hosts file
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#managing-user-provisioned-infrastructure-manually" class="j-doc-nav__link j-doc-nav__link--has-children">
    10. Managing user-provisioned infrastructure manually
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10. Managing user-provisioned infrastructure manually"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10. Managing user-provisioned infrastructure manually"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#adding-compute-user-infra-general" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.1. Adding compute machines to clusters with user-provisioned infrastructure manually
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.1. Adding compute machines to clusters with user-provisioned infrastructure manually"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.1. Adding compute machines to clusters with user-provisioned infrastructure manually"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#upi-adding-compute-aws" class="j-doc-nav__link ">
    10.1.1. Adding compute machines to Amazon Web Services
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#upi-adding-compute-azure" class="j-doc-nav__link ">
    10.1.2. Adding compute machines to Microsoft Azure
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#upi-adding-compute-ash" class="j-doc-nav__link ">
    10.1.3. Adding compute machines to Azure Stack Hub
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#upi-adding-compute-gcp" class="j-doc-nav__link ">
    10.1.4. Adding compute machines to Google Cloud Platform
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#upi-adding-compute-vsphere" class="j-doc-nav__link ">
    10.1.5. Adding compute machines to vSphere
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#upi-adding-compute-rhv" class="j-doc-nav__link ">
    10.1.6. Adding compute machines to RHV
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#upi-adding-compute-bare-metal" class="j-doc-nav__link ">
    10.1.7. Adding compute machines to bare metal
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#adding-aws-compute-user-infra" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.2. Adding compute machines to AWS by using CloudFormation templates
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.2. Adding compute machines to AWS by using CloudFormation templates"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.2. Adding compute machines to AWS by using CloudFormation templates"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#prerequisites_adding-aws-compute-user-infra" class="j-doc-nav__link ">
    10.2.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-adding-aws-compute-cloudformation_adding-aws-compute-user-infra" class="j-doc-nav__link ">
    10.2.2. Adding more compute machines to your AWS cluster by using CloudFormation templates
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#installation-approve-csrs_adding-aws-compute-user-infra" class="j-doc-nav__link ">
    10.2.3. Approving the certificate signing requests for your machines
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#adding-vsphere-compute-user-infra" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.3. Adding compute machines to vSphere manually
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.3. Adding compute machines to vSphere manually"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.3. Adding compute machines to vSphere manually"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#prerequisites-2" class="j-doc-nav__link ">
    10.3.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-vsphere-machines_adding-vsphere-compute-user-infra" class="j-doc-nav__link ">
    10.3.2. Adding more compute machines to a cluster in vSphere
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#installation-approve-csrs_adding-vsphere-compute-user-infra" class="j-doc-nav__link ">
    10.3.3. Approving the certificate signing requests for your machines
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#adding-rhv-compute-user-infra" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.4. Adding compute machines to a cluster on RHV
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.4. Adding compute machines to a cluster on RHV"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.4. Adding compute machines to a cluster on RHV"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-user-provisioned-rhv_adding-rhv-compute-user-infra" class="j-doc-nav__link ">
    10.4.1. Adding more compute machines to a cluster on RHV
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#adding-bare-metal-compute-user-infra" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.5. Adding compute machines to bare metal
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.5. Adding compute machines to bare metal"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.5. Adding compute machines to bare metal"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#prerequisites-3" class="j-doc-nav__link ">
    10.5.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#creating-rhcos-machines-bare-metal" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.5.2. Creating Red Hat Enterprise Linux CoreOS (RHCOS) machines
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.5.2. Creating Red Hat Enterprise Linux CoreOS (RHCOS) machines"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.5.2. Creating Red Hat Enterprise Linux CoreOS (RHCOS) machines"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-user-infra-machines-iso_adding-bare-metal-compute-user-infra" class="j-doc-nav__link ">
    10.5.2.1. Creating RHCOS machines using an ISO image
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-user-infra-machines-pxe_adding-bare-metal-compute-user-infra" class="j-doc-nav__link ">
    10.5.2.2. Creating RHCOS machines by PXE or iPXE booting
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#installation-approve-csrs_adding-bare-metal-compute-user-infra" class="j-doc-nav__link ">
    10.5.3. Approving the certificate signing requests for your machines
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#capi-machine-management" class="j-doc-nav__link j-doc-nav__link--has-children">
    11. Managing machines with the Cluster API
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11. Managing machines with the Cluster API"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11. Managing machines with the Cluster API"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cluster-api-architecture_capi-machine-management" class="j-doc-nav__link j-doc-nav__link--has-children">
    11.1. Cluster API architecture
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11.1. Cluster API architecture"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11.1. Cluster API architecture"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#capi-arch-operator" class="j-doc-nav__link ">
    11.1.1. The Cluster CAPI Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#capi-arch-resources" class="j-doc-nav__link ">
    11.1.2. Primary resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#capi-sample-yaml-files" class="j-doc-nav__link j-doc-nav__link--has-children">
    11.2. Sample YAML files
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11.2. Sample YAML files"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11.2. Sample YAML files"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#capi-yaml-cluster_capi-machine-management" class="j-doc-nav__link ">
    11.2.1. Sample YAML for a Cluster API cluster resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#capi-sample-yaml-files-aws" class="j-doc-nav__link j-doc-nav__link--has-children">
    11.2.2. Sample YAML files for configuring Amazon Web Services clusters
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11.2.2. Sample YAML files for configuring Amazon Web Services clusters"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11.2.2. Sample YAML files for configuring Amazon Web Services clusters"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#capi-yaml-infrastructure-aws_capi-machine-management" class="j-doc-nav__link ">
    11.2.2.1. Sample YAML for a Cluster API infrastructure resource on Amazon Web Services
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#capi-yaml-machine-template-aws_capi-machine-management" class="j-doc-nav__link ">
    11.2.2.2. Sample YAML for a Cluster API machine template resource on Amazon Web Services
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#capi-yaml-machine-set-aws_capi-machine-management" class="j-doc-nav__link ">
    11.2.2.3. Sample YAML for a Cluster API compute machine set resource on Amazon Web Services
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#capi-sample-yaml-files-gcp" class="j-doc-nav__link j-doc-nav__link--has-children">
    11.2.3. Sample YAML files for configuring Google Cloud Platform clusters
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11.2.3. Sample YAML files for configuring Google Cloud Platform clusters"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11.2.3. Sample YAML files for configuring Google Cloud Platform clusters"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#capi-yaml-infrastructure-gcp_capi-machine-management" class="j-doc-nav__link ">
    11.2.3.1. Sample YAML for a Cluster API infrastructure resource on Google Cloud Platform
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#capi-yaml-machine-template-gcp_capi-machine-management" class="j-doc-nav__link ">
    11.2.3.2. Sample YAML for a Cluster API machine template resource on Google Cloud Platform
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#capi-yaml-machine-set-gcp_capi-machine-management" class="j-doc-nav__link ">
    11.2.3.3. Sample YAML for a Cluster API compute machine set resource on Google Cloud Platform
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#capi-machine-set-creating_capi-machine-management" class="j-doc-nav__link ">
    11.3. Creating a Cluster API compute machine set
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#capi-troubleshooting_capi-machine-management" class="j-doc-nav__link j-doc-nav__link--has-children">
    11.4. Troubleshooting clusters that use the Cluster API
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11.4. Troubleshooting clusters that use the Cluster API"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11.4. Troubleshooting clusters that use the Cluster API"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#ts-capi-cli_capi-machine-management" class="j-doc-nav__link ">
    11.4.1. CLI commands return Cluster API machines
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#managing-control-plane-machines" class="j-doc-nav__link j-doc-nav__link--has-children">
    12. Managing control plane machines
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12. Managing control plane machines"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12. Managing control plane machines"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-about" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.1. About control plane machine sets
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.1. About control plane machine sets"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.1. About control plane machine sets"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-overview_cpmso-about" class="j-doc-nav__link ">
    12.1.1. Control Plane Machine Set Operator overview
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-limitations_cpmso-about" class="j-doc-nav__link ">
    12.1.2. Limitations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#additional-resources_cpmso-about" class="j-doc-nav__link ">
    12.1.3. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-getting-started" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.2. Getting started with control plane machine sets
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.2. Getting started with control plane machine sets"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.2. Getting started with control plane machine sets"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-platform-matrix_cpmso-getting-started" class="j-doc-nav__link ">
    12.2.1. Supported cloud providers
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-checking-status_cpmso-getting-started" class="j-doc-nav__link ">
    12.2.2. Checking the control plane machine set custom resource state
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-activating_cpmso-getting-started" class="j-doc-nav__link ">
    12.2.3. Activating the control plane machine set custom resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-creating-cr_cpmso-getting-started" class="j-doc-nav__link ">
    12.2.4. Creating a control plane machine set custom resource
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.3. Control plane machine set configuration
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.3. Control plane machine set configuration"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.3. Control plane machine set configuration"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-yaml-sample-cr_cpmso-configuration" class="j-doc-nav__link ">
    12.3.1. Sample YAML for a control plane machine set custom resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-sample-yaml-aws_cpmso-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.3.2. Sample YAML for configuring Amazon Web Services clusters
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.3.2. Sample YAML for configuring Amazon Web Services clusters"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.3.2. Sample YAML for configuring Amazon Web Services clusters"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-yaml-provider-spec-aws_cpmso-configuration" class="j-doc-nav__link ">
    12.3.2.1. Sample AWS provider specification
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-yaml-failure-domain-aws_cpmso-configuration" class="j-doc-nav__link ">
    12.3.2.2. Sample AWS failure domain configuration
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-sample-yaml-gcp_cpmso-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.3.3. Sample YAML for configuring Google Cloud Platform clusters
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.3.3. Sample YAML for configuring Google Cloud Platform clusters"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.3.3. Sample YAML for configuring Google Cloud Platform clusters"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-yaml-provider-spec-gcp_cpmso-configuration" class="j-doc-nav__link ">
    12.3.3.1. Sample GCP provider specification
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-yaml-failure-domain-gcp_cpmso-configuration" class="j-doc-nav__link ">
    12.3.3.2. Sample GCP failure domain configuration
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-sample-yaml-azure_cpmso-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.3.4. Sample YAML for configuring Microsoft Azure clusters
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.3.4. Sample YAML for configuring Microsoft Azure clusters"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.3.4. Sample YAML for configuring Microsoft Azure clusters"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-yaml-provider-spec-azure_cpmso-configuration" class="j-doc-nav__link ">
    12.3.4.1. Sample Azure provider specification
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-yaml-failure-domain-azure_cpmso-configuration" class="j-doc-nav__link ">
    12.3.4.2. Sample Azure failure domain configuration
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-sample-yaml-vsphere_cpmso-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.3.5. Sample YAML for configuring VMware vSphere clusters
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.3.5. Sample YAML for configuring VMware vSphere clusters"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.3.5. Sample YAML for configuring VMware vSphere clusters"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-yaml-provider-spec-vsphere_cpmso-configuration" class="j-doc-nav__link ">
    12.3.5.1. Sample vSphere provider specification
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-using" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.4. Managing control plane machines with control plane machine sets
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.4. Managing control plane machines with control plane machine sets"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.4. Managing control plane machines with control plane machine sets"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-feat-replace_cpmso-using" class="j-doc-nav__link ">
    12.4.1. Replacing a control plane machine
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-feat-config-update_cpmso-using" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.4.2. Updating the control plane configuration
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.4.2. Updating the control plane configuration"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.4.2. Updating the control plane configuration"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-feat-auto-update_cpmso-using" class="j-doc-nav__link ">
    12.4.2.1. Automatic updates to the control plane configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-feat-ondelete-update_cpmso-using" class="j-doc-nav__link ">
    12.4.2.2. Manual updates to the control plane configuration
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-supported-features-aws_cpmso-using" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.4.3. Enabling Amazon Web Services features for control plane machines
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.4.3. Enabling Amazon Web Services features for control plane machines"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.4.3. Enabling Amazon Web Services features for control plane machines"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#private-clusters-setting-api-private_cpmso-using-aws" class="j-doc-nav__link ">
    12.4.3.1. Restricting the API server to private
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpms-changing-aws-instance-type_cpmso-using" class="j-doc-nav__link ">
    12.4.3.2. Changing the Amazon Web Services instance type by using a control plane machine set
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-imds-options_cpmso-using" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.4.3.3. Machine set options for the Amazon EC2 Instance Metadata Service
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.4.3.3. Machine set options for the Amazon EC2 Instance Metadata Service"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.4.3.3. Machine set options for the Amazon EC2 Instance Metadata Service"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating-imds-options_cpmso-using" class="j-doc-nav__link ">
    12.4.3.3.1. Configuring IMDS by using machine sets
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-dedicated-instance_cpmso-using" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.4.3.4. Machine sets that deploy machines as Dedicated Instances
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.4.3.4. Machine sets that deploy machines as Dedicated Instances"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.4.3.4. Machine sets that deploy machines as Dedicated Instances"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating-dedicated-instance_cpmso-using" class="j-doc-nav__link ">
    12.4.3.4.1. Creating Dedicated Instances by using machine sets
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-supported-features-azure_cpmso-using" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.4.4. Enabling Microsoft Azure features for control plane machines
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.4.4. Enabling Microsoft Azure features for control plane machines"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.4.4. Enabling Microsoft Azure features for control plane machines"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#private-clusters-setting-api-private_cpmso-using-azure" class="j-doc-nav__link ">
    12.4.4.1. Restricting the API server to private
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#installation-azure-marketplace-subscribe_cpmso-using" class="j-doc-nav__link ">
    12.4.4.2. Selecting an Azure Marketplace image
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-azure-boot-diagnostics_cpmso-using" class="j-doc-nav__link ">
    12.4.4.3. Enabling Azure boot diagnostics
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-azure-ultra-disk_cpmso-using" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.4.4.4. Machine sets that deploy machines with ultra disks as data disks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.4.4.4. Machine sets that deploy machines with ultra disks as data disks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.4.4.4. Machine sets that deploy machines with ultra disks as data disks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-creating-azure-ultra-disk_cpmso-using" class="j-doc-nav__link ">
    12.4.4.4.1. Creating machines with ultra disks by using machine sets
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-troubleshooting-azure-ultra-disk_cpmso-using" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.4.4.4.2. Troubleshooting resources for machine sets that enable ultra disks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.4.4.4.2. Troubleshooting resources for machine sets that enable ultra disks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.4.4.4.2. Troubleshooting resources for machine sets that enable ultra disks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#ts-mapi-attach-misconfigure_cpmso-using" class="j-doc-nav__link ">
    12.4.4.4.2.1. Incorrect ultra disk configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#ts-mapi-attach-unsupported_cpmso-using" class="j-doc-nav__link ">
    12.4.4.4.2.2. Unsupported disk parameters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#ts-mapi-delete_cpmso-using" class="j-doc-nav__link ">
    12.4.4.4.2.3. Unable to delete disks
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-enabling-customer-managed-encryption-azure_cpmso-using" class="j-doc-nav__link ">
    12.4.4.5. Enabling customer-managed encryption keys for a machine set
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-azure-accelerated-networking_cpmso-using" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.4.4.6. Accelerated Networking for Microsoft Azure VMs
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.4.4.6. Accelerated Networking for Microsoft Azure VMs"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.4.4.6. Accelerated Networking for Microsoft Azure VMs"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-azure-accelerated-networking-limits_cpmso-using" class="j-doc-nav__link ">
    12.4.4.6.1. Limitations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-azure-enabling-accelerated-networking-existing_cpmso-using" class="j-doc-nav__link ">
    12.4.4.6.2. Enabling Accelerated Networking on an existing Microsoft Azure cluster
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-supported-features-gcp_cpmso-using" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.4.5. Enabling Google Cloud Platform features for control plane machines
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.4.5. Enabling Google Cloud Platform features for control plane machines"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.4.5. Enabling Google Cloud Platform features for control plane machines"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-gcp-pd-disk-types_cpmso-using" class="j-doc-nav__link ">
    12.4.5.1. Configuring persistent disk types by using machine sets
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-gcp-confidential-vm_cpmso-using" class="j-doc-nav__link ">
    12.4.5.2. Configuring Confidential VM by using machine sets
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-gcp-shielded-vms_cpmso-using" class="j-doc-nav__link ">
    12.4.5.3. Configuring Shielded VM options by using machine sets
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machineset-gcp-enabling-customer-managed-encryption_cpmso-using" class="j-doc-nav__link ">
    12.4.5.4. Enabling customer-managed encryption keys for a machine set
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-resiliency" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.5. Control plane resiliency and recovery
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.5. Control plane resiliency and recovery"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.5. Control plane resiliency and recovery"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-failure-domains_cpmso-resiliency" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.5.1. High availability and fault tolerance with failure domains
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.5.1. High availability and fault tolerance with failure domains"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.5.1. High availability and fault tolerance with failure domains"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-failure-domains-provider_cpmso-resiliency" class="j-doc-nav__link ">
    12.5.1.1. Failure domain platform support and configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-failure-domains-balancing_cpmso-resiliency" class="j-doc-nav__link ">
    12.5.1.2. Balancing control plane machines
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-control-plane-recovery_cpmso-resiliency" class="j-doc-nav__link ">
    12.5.2. Recovery of failed control plane machines
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-lifecycle-hook-deletion-etcd_cpmso-resiliency" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.5.3. Quorum protection with machine lifecycle hooks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.5.3. Quorum protection with machine lifecycle hooks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.5.3. Quorum protection with machine lifecycle hooks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-lifecycle-hook-deletion-etcd-order_cpmso-resiliency" class="j-doc-nav__link ">
    12.5.3.1. Control plane deletion with quorum protection processing order
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-troubleshooting" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.6. Troubleshooting the control plane machine set
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.6. Troubleshooting the control plane machine set"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.6. Troubleshooting the control plane machine set"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-checking-status_cpmso-troubleshooting" class="j-doc-nav__link ">
    12.6.1. Checking the control plane machine set custom resource state
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-ts-ilb-missing_cpmso-troubleshooting" class="j-doc-nav__link ">
    12.6.2. Adding a missing Azure internal load balancer
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-ts-etcd-degraded_cpmso-troubleshooting" class="j-doc-nav__link ">
    12.6.3. Recovering a degraded etcd Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-disabling" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.7. Disabling the control plane machine set
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.7. Disabling the control plane machine set"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.7. Disabling the control plane machine set"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-deleting_cpmso-disabling" class="j-doc-nav__link ">
    12.7.1. Deleting the control plane machine set
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-checking-status_cpmso-disabling" class="j-doc-nav__link ">
    12.7.2. Checking the control plane machine set custom resource state
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#cpmso-reenabling_cpmso-disabling" class="j-doc-nav__link ">
    12.7.3. Re-enabling the control plane machine set
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#deploying-machine-health-checks" class="j-doc-nav__link j-doc-nav__link--has-children">
    13. Deploying machine health checks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13. Deploying machine health checks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13. Deploying machine health checks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-health-checks-about_deploying-machine-health-checks" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.1. About machine health checks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.1. About machine health checks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.1. About machine health checks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-health-checks-limitations_deploying-machine-health-checks" class="j-doc-nav__link ">
    13.1.1. Limitations when deploying machine health checks
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-health-checks-resource_deploying-machine-health-checks" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.2. Sample MachineHealthCheck resource
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.2. Sample MachineHealthCheck resource"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.2. Sample MachineHealthCheck resource"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-health-checks-short-circuiting_deploying-machine-health-checks" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.2.1. Short-circuiting machine health check remediation
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.2.1. Short-circuiting machine health check remediation"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.2.1. Short-circuiting machine health check remediation"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#setting-maxunhealthy-by-using-an-absolute-value" class="j-doc-nav__link ">
    13.2.1.1. Setting maxUnhealthy by using an absolute value
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#setting-maxunhealthy-by-using-percentages" class="j-doc-nav__link ">
    13.2.1.2. Setting maxUnhealthy by using percentages
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-health-checks-creating_deploying-machine-health-checks" class="j-doc-nav__link ">
    13.3. Creating a machine health check resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#mgmt-power-remediation-baremetal-about_deploying-machine-health-checks" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.4. About power-based remediation of bare metal
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.4. About power-based remediation of bare metal"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.4. About power-based remediation of bare metal"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#machine-health-checks-bare-metal_deploying-machine-health-checks" class="j-doc-nav__link ">
    13.4.1. MachineHealthChecks on bare metal
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#mgmt-understanding-remediation-process_deploying-machine-health-checks" class="j-doc-nav__link ">
    13.4.2. Understanding the annotation-based remediation process
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#mgmt-understanding-metal3-remediation-process_deploying-machine-health-checks" class="j-doc-nav__link ">
    13.4.3. Understanding the metal3-based remediation process
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#mgmt-creating-mhc-baremetal_deploying-machine-health-checks" class="j-doc-nav__link ">
    13.4.4. Creating a MachineHealthCheck resource for bare metal
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                    

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management#idm140311151301584" class="j-doc-nav__link ">
    Legal Notice
  </a>
  
          </li>
              </ol>
    
  </div>
</nav>


              </div>
            </div>
            <div class="j-options-container j-options-container--mobile">
              <button class="j-sidebar__menu-trigger j-sidebar__menu-trigger--options content-expander__trigger">
                <span class="j-sidebar__menu-trigger__open-headline">
                  Settings
                </span>
                <span class="j-sidebar__menu-trigger__close-text">Close</span>
              </button>
              

  <ul class="j-doc-options__list content-expander">
    <li class="j-doc-options__item">
          <label class="j-doc-option__label j-doc-option__label--language" for="j-doc-language">
        Language:
      </label>
      <select id="j-doc-language" class="j-doc-option__select">
                  <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management" selected=''>
            English
          </option>
                  <option value="/documentation/ja-jp/openshift_container_platform/4.13/html-single/machine_management" >
            日本語
          </option>
                  <option value="/documentation/zh-cn/openshift_container_platform/4.13/html-single/machine_management" >
            简体中文
          </option>
                  <option value="/documentation/ko-kr/openshift_container_platform/4.13/html-single/machine_management" >
            한국어
          </option>
              </select>

            <noscript>
        <div class="j-doc-option__label j-doc-option__label--language" id="j-doc-option__label--language--nojs">
          Language:
        </div>
        <ul aria-labelledby="j-doc-option__label--language--nojs" class="j-doc-option__languages-list">
                      <li>
              <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management">English</a>
            </li>
                      <li>
              <a href="/documentation/ja-jp/openshift_container_platform/4.13/html-single/machine_management">日本語</a>
            </li>
                      <li>
              <a href="/documentation/zh-cn/openshift_container_platform/4.13/html-single/machine_management">简体中文</a>
            </li>
                      <li>
              <a href="/documentation/ko-kr/openshift_container_platform/4.13/html-single/machine_management">한국어</a>
            </li>
                  </ul>
      </noscript>

      </li>

    <li class="j-doc-options__item">
    <label for="j-doc-mode" class="j-doc-option__label j-doc-option__label--format">
      Format:
    </label>
    <select id="j-doc-mode" class="j-doc-option__select">
              <option value="/documentation/en-us/openshift_container_platform/4.13/html/machine_management"  class="j-doc-options__option j-doc-options__option--multi-page">
          Multi-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management" selected='' class="j-doc-options__option j-doc-options__option--single-page">
          Single-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/pdf/machine_management/OpenShift_Container_Platform-4.13-Machine_management-en-US.pdf"  class="j-doc-options__option j-doc-options__option--pdf">
          PDF
        </option>
          </select>

        <noscript>
      <div class="j-doc-option__label j-doc-option__label--format" id="j-doc-option__label--format--nojs">
        Format:
      </div>
      <ul class="j-doc-option__format-list" aria-labelledby="j-doc-option__label--format--nojs">
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--multi-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html/machine_management">Multi-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--single-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management">Single-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--pdf"><a href="/documentation/en-us/openshift_container_platform/4.13/pdf/machine_management/OpenShift_Container_Platform-4.13-Machine_management-en-US.pdf">PDF</a></li>
              </ul>
    </noscript>
  </li>
</ul>


              </div>
          </div>
                <div class="pvof-doc__tertiary-sidebar j-doc__tertiary-sidebar">
          <div class="pvof-doc__tertiary-sidebar__inner j-doc__tertiary-sidebar__inner">
            <div class="j-doc__doc-options">
              <div class="j-options-container j-options-container--desktop">
                <button class="j-sidebar__menu-trigger j-sidebar__menu-trigger--tablet content-expander__trigger">
                  <span class="j-sidebar__menu-trigger-icon"></span>
                  <h2 class="visually-hidden">Language and Page Formatting Options</h2>
                </button>
                  

  <ul class="j-doc-options__list content-expander">
    <li class="j-doc-options__item">
          <label class="j-doc-option__label j-doc-option__label--language" for="j-doc-language--2">
        Language:
      </label>
      <select id="j-doc-language--2" class="j-doc-option__select">
                  <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management" selected=''>
            English
          </option>
                  <option value="/documentation/ja-jp/openshift_container_platform/4.13/html-single/machine_management" >
            日本語
          </option>
                  <option value="/documentation/zh-cn/openshift_container_platform/4.13/html-single/machine_management" >
            简体中文
          </option>
                  <option value="/documentation/ko-kr/openshift_container_platform/4.13/html-single/machine_management" >
            한국어
          </option>
              </select>

            <noscript>
        <div class="j-doc-option__label j-doc-option__label--language" id="j-doc-option__label--language--nojs">
          Language:
        </div>
        <ul aria-labelledby="j-doc-option__label--language--nojs" class="j-doc-option__languages-list">
                      <li>
              <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management">English</a>
            </li>
                      <li>
              <a href="/documentation/ja-jp/openshift_container_platform/4.13/html-single/machine_management">日本語</a>
            </li>
                      <li>
              <a href="/documentation/zh-cn/openshift_container_platform/4.13/html-single/machine_management">简体中文</a>
            </li>
                      <li>
              <a href="/documentation/ko-kr/openshift_container_platform/4.13/html-single/machine_management">한국어</a>
            </li>
                  </ul>
      </noscript>

      </li>

    <li class="j-doc-options__item">
    <label for="j-doc-mode--2" class="j-doc-option__label j-doc-option__label--format">
      Format:
    </label>
    <select id="j-doc-mode--2" class="j-doc-option__select">
              <option value="/documentation/en-us/openshift_container_platform/4.13/html/machine_management"  class="j-doc-options__option j-doc-options__option--multi-page">
          Multi-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management" selected='' class="j-doc-options__option j-doc-options__option--single-page">
          Single-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/pdf/machine_management/OpenShift_Container_Platform-4.13-Machine_management-en-US.pdf"  class="j-doc-options__option j-doc-options__option--pdf">
          PDF
        </option>
          </select>

        <noscript>
      <div class="j-doc-option__label j-doc-option__label--format" id="j-doc-option__label--format--nojs">
        Format:
      </div>
      <ul class="j-doc-option__format-list" aria-labelledby="j-doc-option__label--format--nojs">
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--multi-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html/machine_management">Multi-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--single-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management">Single-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--pdf"><a href="/documentation/en-us/openshift_container_platform/4.13/pdf/machine_management/OpenShift_Container_Platform-4.13-Machine_management-en-US.pdf">PDF</a></li>
              </ul>
    </noscript>
  </li>
</ul>


                </div>
              </div>
          </div>
        </div>

                  <div class="doc-wrapper pvof-doc__wrapper j-superdoc__content-wrapper" id="doc-wrapper">
            

  <div class="pane-page-title">
    <h1 class="title" itemprop="name">Machine management</h1>
  </div>


  <div xml:lang="en-US" class="book" id="idm140311147525280"><div class="titlepage"><div><div class="producttitle"><span class="productname">OpenShift Container Platform</span> <span class="productnumber">4.13</span></div><div><h2 class="subtitle">Adding and maintaining cluster machines</h2></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat OpenShift Documentation Team</span></div></div><div><a href="#idm140311151301584">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				This document provides instructions for managing the machines that make up an OpenShift Container Platform cluster. Some tasks make use of the enhanced automatic machine management functions of an OpenShift Container Platform cluster and some tasks are manual. Not all tasks that are described in this document are available in all installation types.
			</div></div></div></div><hr/></div><section class="chapter" id="overview-of-machine-management"><div class="titlepage"><div><div><h1 class="title">Chapter 1. Overview of machine management</h1></div></div></div><p>
			You can use machine management to flexibly work with underlying infrastructure such as Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), Red Hat OpenStack Platform (RHOSP), Red Hat Virtualization (RHV), and VMware vSphere to manage the OpenShift Container Platform cluster. You can control the cluster and perform auto-scaling, such as scaling up and down the cluster based on specific workload policies.
		</p><p>
			It is important to have a cluster that adapts to changing workloads. The OpenShift Container Platform cluster can horizontally scale up and down when the load increases or decreases.
		</p><p>
			Machine management is implemented as a <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#crd-extending-api-with-crds">custom resource definition</a> (CRD). A CRD object defines a new unique object <code class="literal">Kind</code> in the cluster and enables the Kubernetes API server to handle the object’s entire lifecycle.
		</p><p>
			The Machine API Operator provisions the following resources:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<code class="literal">MachineSet</code>
				</li><li class="listitem">
					<code class="literal">Machine</code>
				</li><li class="listitem">
					<code class="literal">ClusterAutoscaler</code>
				</li><li class="listitem">
					<code class="literal">MachineAutoscaler</code>
				</li><li class="listitem">
					<code class="literal">MachineHealthCheck</code>
				</li></ul></div><section class="section" id="machine-api-overview_overview-of-machine-management"><div class="titlepage"><div><div><h2 class="title">1.1. Machine API overview</h2></div></div></div><p>
				The Machine API is a combination of primary resources that are based on the upstream Cluster API project and custom OpenShift Container Platform resources.
			</p><p>
				For OpenShift Container Platform 4.13 clusters, the Machine API performs all node host provisioning management actions after the cluster installation finishes. Because of this system, OpenShift Container Platform 4.13 offers an elastic, dynamic provisioning method on top of public or private cloud infrastructure.
			</p><p>
				The two primary resources are:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Machines</span></dt><dd>
							A fundamental unit that describes the host for a node. A machine has a <code class="literal">providerSpec</code> specification, which describes the types of compute nodes that are offered for different cloud platforms. For example, a machine type for a worker node on Amazon Web Services (AWS) might define a specific machine type and required metadata.
						</dd><dt><span class="term">Machine sets</span></dt><dd><p class="simpara">
							<code class="literal">MachineSet</code> resources are groups of compute machines. Compute machine sets are to compute machines as replica sets are to pods. If you need more compute machines or must scale them down, you change the <code class="literal">replicas</code> field on the <code class="literal">MachineSet</code> resource to meet your compute need.
						</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
								Control plane machines cannot be managed by compute machine sets.
							</p><p>
								Control plane machine sets provide management capabilities for supported control plane machines that are similar to what compute machine sets provide for compute machines.
							</p><p>
								For more information, see “Managing control plane machines".
							</p></div></div></dd></dl></div><p>
				The following custom resources add more capabilities to your cluster:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Machine autoscaler</span></dt><dd><p class="simpara">
							The <code class="literal">MachineAutoscaler</code> resource automatically scales compute machines in a cloud. You can set the minimum and maximum scaling boundaries for nodes in a specified compute machine set, and the machine autoscaler maintains that range of nodes.
						</p><p class="simpara">
							The <code class="literal">MachineAutoscaler</code> object takes effect after a <code class="literal">ClusterAutoscaler</code> object exists. Both <code class="literal">ClusterAutoscaler</code> and <code class="literal">MachineAutoscaler</code> resources are made available by the <code class="literal">ClusterAutoscalerOperator</code> object.
						</p></dd><dt><span class="term">Cluster autoscaler</span></dt><dd><p class="simpara">
							This resource is based on the upstream cluster autoscaler project. In the OpenShift Container Platform implementation, it is integrated with the Machine API by extending the compute machine set API. You can use the cluster autoscaler to manage your cluster in the following ways:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Set cluster-wide scaling limits for resources such as cores, nodes, memory, and GPU
								</li><li class="listitem">
									Set the priority so that the cluster prioritizes pods and new nodes are not brought online for less important pods
								</li><li class="listitem">
									Set the scaling policy so that you can scale up nodes but not scale them down
								</li></ul></div></dd><dt><span class="term">Machine health check</span></dt><dd>
							The <code class="literal">MachineHealthCheck</code> resource detects when a machine is unhealthy, deletes it, and, on supported platforms, makes a new machine.
						</dd></dl></div><p>
				In OpenShift Container Platform version 3.11, you could not roll out a multi-zone architecture easily because the cluster did not manage machine provisioning. Beginning with OpenShift Container Platform version 4.1, this process is easier. Each compute machine set is scoped to a single zone, so the installation program sends out compute machine sets across availability zones on your behalf. And then because your compute is dynamic, and in the face of a zone failure, you always have a zone for when you must rebalance your machines. In global Azure regions that do not have multiple availability zones, you can use availability sets to ensure high availability. The autoscaler provides best-effort balancing over the life of a cluster.
			</p></section><section class="section" id="machine-mgmt-intro-managing-compute_overview-of-machine-management"><div class="titlepage"><div><div><h2 class="title">1.2. Managing compute machines</h2></div></div></div><p>
				As a cluster administrator, you can perform the following actions:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Create a compute machine set for the following cloud providers:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-machineset-aws">AWS</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-machineset-azure">Azure</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-machineset-gcp">GCP</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-machineset-osp">RHOSP</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-machineset-rhv">RHV</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-machineset-vsphere">vSphere</a>
							</li></ul></div></li><li class="listitem">
						Create a machine set for a bare metal deployment: <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-machineset-bare-metal">Creating a compute machine set on bare metal</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#manually-scaling-machineset">Manually scale a compute machine set</a> by adding or removing a machine from the compute machine set.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#modifying-machineset">Modify a compute machine set</a> through the <code class="literal">MachineSet</code> YAML configuration file.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#deleting-machine">Delete</a> a machine.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-infrastructure-machinesets">Create infrastructure compute machine sets</a>.
					</li><li class="listitem">
						Configure and deploy a <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#deploying-machine-health-checks">machine health check</a> to automatically fix damaged machines in a machine pool.
					</li></ul></div></section><section class="section" id="machine-mgmt-intro-managing-control-plane_overview-of-machine-management"><div class="titlepage"><div><div><h2 class="title">1.3. Managing control plane machines</h2></div></div></div><p>
				As a cluster administrator, you can perform the following actions:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-feat-config-update_cpmso-using">Update your control plane configuration</a> with a control plane machine set for the following cloud providers:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-sample-yaml-aws_cpmso-configuration">AWS</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-sample-yaml-azure_cpmso-configuration">Azure</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-sample-yaml-vsphere_cpmso-configuration">vSphere</a>
							</li></ul></div></li><li class="listitem">
						Configure and deploy a <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#deploying-machine-health-checks">machine health check</a> to automatically recover unhealthy control plane machines.
					</li></ul></div></section><section class="section" id="machine-mgmt-intro-autoscaling_overview-of-machine-management"><div class="titlepage"><div><div><h2 class="title">1.4. Applying autoscaling to an OpenShift Container Platform cluster</h2></div></div></div><p>
				You can automatically scale your OpenShift Container Platform cluster to ensure flexibility for changing workloads. To <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#applying-autoscaling">autoscale</a> your cluster, you must first deploy a cluster autoscaler, and then deploy a machine autoscaler for each compute machine set.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cluster-autoscaler-about_applying-autoscaling"><span class="emphasis"><em>cluster autoscaler</em></span></a> increases and decreases the size of the cluster based on deployment needs.
					</li><li class="listitem">
						The <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machine-autoscaler-about_applying-autoscaling"><span class="emphasis"><em>machine autoscaler</em></span></a> adjusts the number of machines in the compute machine sets that you deploy in your OpenShift Container Platform cluster.
					</li></ul></div></section><section class="section" id="machine-mgmt-intro-add-for-upi_overview-of-machine-management"><div class="titlepage"><div><div><h2 class="title">1.5. Adding compute machines on user-provisioned infrastructure</h2></div></div></div><p>
				User-provisioned infrastructure is an environment where you can deploy infrastructure such as compute, network, and storage resources that host the OpenShift Container Platform. You can <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#adding-compute-user-infra-general">add compute machines</a> to a cluster on user-provisioned infrastructure during or after the installation process.
			</p></section><section class="section" id="machine-mgmt-intro-add-rhel_overview-of-machine-management"><div class="titlepage"><div><div><h2 class="title">1.6. Adding RHEL compute machines to your cluster</h2></div></div></div><p>
				As a cluster administrator, you can perform the following actions:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#adding-rhel-compute">Add Red Hat Enterprise Linux (RHEL) compute machines</a>, also known as worker machines, to a user-provisioned infrastructure cluster or an installation-provisioned infrastructure cluster.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#more-rhel-compute">Add more Red Hat Enterprise Linux (RHEL) compute machines</a> to an existing cluster.
					</li></ul></div></section></section><section class="chapter" id="managing-compute-machines-with-the-machine-api"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Managing compute machines with the Machine API</h1></div></div></div><section class="section" id="creating-machineset-alibaba"><div class="titlepage"><div><div><h2 class="title">2.1. Creating a compute machine set on Alibaba Cloud</h2></div></div></div><p>
				You can create a different compute machine set to serve a specific purpose in your OpenShift Container Platform cluster on Alibaba Cloud. For example, you might create infrastructure machine sets and related machines so that you can move supporting workloads to the new machines.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.
				</p><p>
					Clusters with the infrastructure platform type <code class="literal">none</code> cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.
				</p><p>
					To view the platform type for your cluster, run the following command:
				</p><pre class="programlisting language-terminal">$ oc get infrastructure cluster -o jsonpath='{.status.platform}'</pre></div></div><section class="section" id="machineset-yaml-alibaba_creating-machineset-alibaba"><div class="titlepage"><div><div><h3 class="title">2.1.1. Sample YAML for a compute machine set custom resource on Alibaba Cloud</h3></div></div></div><p>
					This sample YAML defines a compute machine set that runs in a specified Alibaba Cloud zone in a region and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/&lt;role&gt;: ""</code>.
				</p><p>
					In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;role&gt;</code> is the node label to add.
				</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO1-1"><!--Empty--></span><span class="callout">1</span>
    machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO1-2"><!--Empty--></span><span class="callout">2</span>
    machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO1-3"><!--Empty--></span><span class="callout">3</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;zone&gt; <span id="CO1-4"><!--Empty--></span><span class="callout">4</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO1-5"><!--Empty--></span><span class="callout">5</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;zone&gt; <span id="CO1-6"><!--Empty--></span><span class="callout">6</span>
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO1-7"><!--Empty--></span><span class="callout">7</span>
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO1-8"><!--Empty--></span><span class="callout">8</span>
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO1-9"><!--Empty--></span><span class="callout">9</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;zone&gt; <span id="CO1-10"><!--Empty--></span><span class="callout">10</span>
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/&lt;role&gt;: ""
      providerSpec:
        value:
          apiVersion: machine.openshift.io/v1
          credentialsSecret:
            name: alibabacloud-credentials
          imageId: &lt;image_id&gt; <span id="CO1-11"><!--Empty--></span><span class="callout">11</span>
          instanceType: &lt;instance_type&gt; <span id="CO1-12"><!--Empty--></span><span class="callout">12</span>
          kind: AlibabaCloudMachineProviderConfig
          ramRoleName: &lt;infrastructure_id&gt;-role-worker <span id="CO1-13"><!--Empty--></span><span class="callout">13</span>
          regionId: &lt;region&gt; <span id="CO1-14"><!--Empty--></span><span class="callout">14</span>
          resourceGroup: <span id="CO1-15"><!--Empty--></span><span class="callout">15</span>
            id: &lt;resource_group_id&gt;
            type: ID
          securityGroups:
          - tags: <span id="CO1-16"><!--Empty--></span><span class="callout">16</span>
            - Key: Name
              Value: &lt;infrastructure_id&gt;-sg-&lt;role&gt;
            type: Tags
          systemDisk: <span id="CO1-17"><!--Empty--></span><span class="callout">17</span>
            category: cloud_essd
            size: &lt;disk_size&gt;
          tag: <span id="CO1-18"><!--Empty--></span><span class="callout">18</span>
          - Key: kubernetes.io/cluster/&lt;infrastructure_id&gt;
            Value: owned
          userDataSecret:
            name: &lt;user_data_secret&gt; <span id="CO1-19"><!--Empty--></span><span class="callout">19</span>
          vSwitch:
            tags: <span id="CO1-20"><!--Empty--></span><span class="callout">20</span>
            - Key: Name
              Value: &lt;infrastructure_id&gt;-vswitch-&lt;zone&gt;
            type: Tags
          vpcId: ""
          zoneId: &lt;zone&gt; <span id="CO1-21"><!--Empty--></span><span class="callout">21</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO1-1"><span class="callout">1</span></a> <a href="#CO1-5"><span class="callout">5</span></a> <a href="#CO1-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI (<code class="literal">oc</code>) installed, you can obtain the infrastructure ID by running the following command:
						</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><a href="#CO1-2"><span class="callout">2</span></a> <a href="#CO1-3"><span class="callout">3</span></a> <a href="#CO1-8"><span class="callout">8</span></a> <a href="#CO1-9"><span class="callout">9</span></a> </dt><dd><div class="para">
							Specify the node label to add.
						</div></dd><dt><a href="#CO1-4"><span class="callout">4</span></a> <a href="#CO1-6"><span class="callout">6</span></a> <a href="#CO1-10"><span class="callout">10</span></a> </dt><dd><div class="para">
							Specify the infrastructure ID, node label, and zone.
						</div></dd><dt><a href="#CO1-11"><span class="callout">11</span></a> </dt><dd><div class="para">
							Specify the image to use. Use an image from an existing default compute machine set for the cluster.
						</div></dd><dt><a href="#CO1-12"><span class="callout">12</span></a> </dt><dd><div class="para">
							Specify the instance type you want to use for the compute machine set.
						</div></dd><dt><a href="#CO1-13"><span class="callout">13</span></a> </dt><dd><div class="para">
							Specify the name of the RAM role to use for the compute machine set. Use the value that the installer populates in the default compute machine set.
						</div></dd><dt><a href="#CO1-14"><span class="callout">14</span></a> </dt><dd><div class="para">
							Specify the region to place machines on.
						</div></dd><dt><a href="#CO1-15"><span class="callout">15</span></a> </dt><dd><div class="para">
							Specify the resource group and type for the cluster. You can use the value that the installer populates in the default compute machine set, or specify a different one.
						</div></dd><dt><a href="#CO1-16"><span class="callout">16</span></a> <a href="#CO1-18"><span class="callout">18</span></a> <a href="#CO1-20"><span class="callout">20</span></a> </dt><dd><div class="para">
							Specify the tags to use for the compute machine set. Minimally, you must include the tags shown in this example, with appropriate values for your cluster. You can include additional tags, including the tags that the installer populates in the default compute machine set it creates, as needed.
						</div></dd><dt><a href="#CO1-17"><span class="callout">17</span></a> </dt><dd><div class="para">
							Specify the type and size of the root disk. Use the <code class="literal">category</code> value that the installer populates in the default compute machine set it creates. If required, specify a different value in gigabytes for <code class="literal">size</code>.
						</div></dd><dt><a href="#CO1-19"><span class="callout">19</span></a> </dt><dd><div class="para">
							Specify the name of the secret in the user data YAML file that is in the <code class="literal">openshift-machine-api</code> namespace. Use the value that the installer populates in the default compute machine set.
						</div></dd><dt><a href="#CO1-21"><span class="callout">21</span></a> </dt><dd><div class="para">
							Specify the zone within your region to place machines on. Be sure that your region supports the zone that you specify.
						</div></dd></dl></div><section class="section" id="machineset-yaml-alibaba-usage-stats_creating-machineset-alibaba"><div class="titlepage"><div><div><h4 class="title">2.1.1.1. Machine set parameters for Alibaba Cloud usage statistics</h4></div></div></div><p>
						The default compute machine sets that the installer creates for Alibaba Cloud clusters include nonessential tag values that Alibaba Cloud uses internally to track usage statistics. These tags are populated in the <code class="literal">securityGroups</code>, <code class="literal">tag</code>, and <code class="literal">vSwitch</code> parameters of the <code class="literal">spec.template.spec.providerSpec.value</code> list.
					</p><p>
						When creating compute machine sets to deploy additional machines, you must include the required Kubernetes tags. The usage statistics tags are applied by default, even if they are not specified in the compute machine sets you create. You can also include additional tags as needed.
					</p><p>
						The following YAML snippets indicate which tags in the default compute machine sets are optional and which are required.
					</p><div class="formalpara"><p class="title"><strong>Tags in <code class="literal">spec.template.spec.providerSpec.value.securityGroups</code></strong></p><p>
							
<pre class="programlisting language-yaml">spec:
  template:
    spec:
      providerSpec:
        value:
          securityGroups:
          - tags:
            - Key: kubernetes.io/cluster/&lt;infrastructure_id&gt; <span id="CO2-1"><!--Empty--></span><span class="callout">1</span>
              Value: owned
            - Key: GISV
              Value: ocp
            - Key: sigs.k8s.io/cloud-provider-alibaba/origin <span id="CO2-2"><!--Empty--></span><span class="callout">2</span>
              Value: ocp
            - Key: Name
              Value: &lt;infrastructure_id&gt;-sg-&lt;role&gt; <span id="CO2-3"><!--Empty--></span><span class="callout">3</span>
            type: Tags</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO2-1"><span class="callout">1</span></a> <a href="#CO2-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Optional: This tag is applied even when not specified in the compute machine set.
							</div></dd><dt><a href="#CO2-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Required.
							</div><p>
								where:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster.
									</li><li class="listitem">
										<code class="literal">&lt;role&gt;</code> is the node label to add.
									</li></ul></div></dd></dl></div><div class="formalpara"><p class="title"><strong>Tags in <code class="literal">spec.template.spec.providerSpec.value.tag</code></strong></p><p>
							
<pre class="programlisting language-yaml">spec:
  template:
    spec:
      providerSpec:
        value:
          tag:
          - Key: kubernetes.io/cluster/&lt;infrastructure_id&gt; <span id="CO3-1"><!--Empty--></span><span class="callout">1</span>
            Value: owned
          - Key: GISV <span id="CO3-2"><!--Empty--></span><span class="callout">2</span>
            Value: ocp
          - Key: sigs.k8s.io/cloud-provider-alibaba/origin <span id="CO3-3"><!--Empty--></span><span class="callout">3</span>
            Value: ocp</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO3-2"><span class="callout">2</span></a> <a href="#CO3-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Optional: This tag is applied even when not specified in the compute machine set.
							</div></dd><dt><a href="#CO3-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Required.
							</div><p>
								where <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster.
							</p></dd></dl></div><div class="formalpara"><p class="title"><strong>Tags in <code class="literal">spec.template.spec.providerSpec.value.vSwitch</code></strong></p><p>
							
<pre class="programlisting language-yaml">spec:
  template:
    spec:
      providerSpec:
        value:
          vSwitch:
            tags:
            - Key: kubernetes.io/cluster/&lt;infrastructure_id&gt; <span id="CO4-1"><!--Empty--></span><span class="callout">1</span>
              Value: owned
            - Key: GISV <span id="CO4-2"><!--Empty--></span><span class="callout">2</span>
              Value: ocp
            - Key: sigs.k8s.io/cloud-provider-alibaba/origin <span id="CO4-3"><!--Empty--></span><span class="callout">3</span>
              Value: ocp
            - Key: Name
              Value: &lt;infrastructure_id&gt;-vswitch-&lt;zone&gt; <span id="CO4-4"><!--Empty--></span><span class="callout">4</span>
            type: Tags</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO4-1"><span class="callout">1</span></a> <a href="#CO4-2"><span class="callout">2</span></a> <a href="#CO4-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Optional: This tag is applied even when not specified in the compute machine set.
							</div></dd><dt><a href="#CO4-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Required.
							</div><p>
								where:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster.
									</li><li class="listitem">
										<code class="literal">&lt;zone&gt;</code> is the zone within your region to place machines on.
									</li></ul></div></dd></dl></div></section></section><section class="section" id="machineset-creating_creating-machineset-alibaba"><div class="titlepage"><div><div><h3 class="title">2.1.2. Creating a compute machine set</h3></div></div></div><p>
					In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Deploy an OpenShift Container Platform cluster.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <code class="literal">&lt;file_name&gt;.yaml</code>.
						</p><p class="simpara">
							Ensure that you set the <code class="literal">&lt;clusterID&gt;</code> and <code class="literal">&lt;role&gt;</code> parameter values.
						</p></li><li class="listitem"><p class="simpara">
							Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									To list the compute machine sets in your cluster, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									To view values of a specific compute machine set custom resource (CR), run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machineset &lt;machineset_name&gt; \
  -n openshift-machine-api -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO5-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO5-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
    spec:
      providerSpec: <span id="CO5-3"><!--Empty--></span><span class="callout">3</span>
        ...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO5-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The cluster infrastructure ID.
										</div></dd><dt><a href="#CO5-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											A default node label.
										</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												For clusters that have user-provisioned infrastructure, a compute machine set can only create <code class="literal">worker</code> and <code class="literal">infra</code> type machines.
											</p></div></div></dd><dt><a href="#CO5-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											The values in the <code class="literal">&lt;providerSpec&gt;</code> section of the compute machine set CR are platform-specific. For more information about <code class="literal">&lt;providerSpec&gt;</code> parameters in the CR, see the sample compute machine set CR configuration for your provider.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">MachineSet</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							View the list of compute machine sets by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-infra-us-east-1a    1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

							</p></div><p class="simpara">
							When the new compute machine set is available, the <code class="literal">DESIRED</code> and <code class="literal">CURRENT</code> values match. If the compute machine set is not available, wait a few minutes and run the command again.
						</p></li></ul></div></section></section><section class="section" id="creating-machineset-aws"><div class="titlepage"><div><div><h2 class="title">2.2. Creating a compute machine set on AWS</h2></div></div></div><p>
				You can create a different compute machine set to serve a specific purpose in your OpenShift Container Platform cluster on Amazon Web Services (AWS). For example, you might create infrastructure machine sets and related machines so that you can move supporting workloads to the new machines.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.
				</p><p>
					Clusters with the infrastructure platform type <code class="literal">none</code> cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.
				</p><p>
					To view the platform type for your cluster, run the following command:
				</p><pre class="programlisting language-terminal">$ oc get infrastructure cluster -o jsonpath='{.status.platform}'</pre></div></div><section class="section" id="machineset-yaml-aws_creating-machineset-aws"><div class="titlepage"><div><div><h3 class="title">2.2.1. Sample YAML for a compute machine set custom resource on AWS</h3></div></div></div><p>
					This sample YAML defines a compute machine set that runs in the <code class="literal">us-east-1a</code> Amazon Web Services (AWS) zone and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/&lt;role&gt;: ""</code>.
				</p><p>
					In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;role&gt;</code> is the node label to add.
				</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO6-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;zone&gt; <span id="CO6-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO6-3"><!--Empty--></span><span class="callout">3</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;zone&gt; <span id="CO6-4"><!--Empty--></span><span class="callout">4</span>
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO6-5"><!--Empty--></span><span class="callout">5</span>
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO6-6"><!--Empty--></span><span class="callout">6</span>
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO6-7"><!--Empty--></span><span class="callout">7</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;zone&gt; <span id="CO6-8"><!--Empty--></span><span class="callout">8</span>
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/&lt;role&gt;: "" <span id="CO6-9"><!--Empty--></span><span class="callout">9</span>
      providerSpec:
        value:
          ami:
            id: ami-046fe691f52a953f9 <span id="CO6-10"><!--Empty--></span><span class="callout">10</span>
          apiVersion: awsproviderconfig.openshift.io/v1beta1
          blockDevices:
            - ebs:
                iops: 0
                volumeSize: 120
                volumeType: gp2
          credentialsSecret:
            name: aws-cloud-credentials
          deviceIndex: 0
          iamInstanceProfile:
            id: &lt;infrastructure_id&gt;-worker-profile <span id="CO6-11"><!--Empty--></span><span class="callout">11</span>
          instanceType: m6i.large
          kind: AWSMachineProviderConfig
          placement:
            availabilityZone: &lt;zone&gt; <span id="CO6-12"><!--Empty--></span><span class="callout">12</span>
            region: &lt;region&gt; <span id="CO6-13"><!--Empty--></span><span class="callout">13</span>
          securityGroups:
            - filters:
                - name: tag:Name
                  values:
                    - &lt;infrastructure_id&gt;-worker-sg <span id="CO6-14"><!--Empty--></span><span class="callout">14</span>
          subnet:
            filters:
              - name: tag:Name
                values:
                  - &lt;infrastructure_id&gt;-private-&lt;zone&gt; <span id="CO6-15"><!--Empty--></span><span class="callout">15</span>
          tags:
            - name: kubernetes.io/cluster/&lt;infrastructure_id&gt; <span id="CO6-16"><!--Empty--></span><span class="callout">16</span>
              value: owned
            - name: &lt;custom_tag_name&gt; <span id="CO6-17"><!--Empty--></span><span class="callout">17</span>
              value: &lt;custom_tag_value&gt; <span id="CO6-18"><!--Empty--></span><span class="callout">18</span>
          userDataSecret:
            name: worker-user-data</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO6-1"><span class="callout">1</span></a> <a href="#CO6-3"><span class="callout">3</span></a> <a href="#CO6-5"><span class="callout">5</span></a> <a href="#CO6-11"><span class="callout">11</span></a> <a href="#CO6-14"><span class="callout">14</span></a> <a href="#CO6-16"><span class="callout">16</span></a> </dt><dd><div class="para">
							Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
						</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><a href="#CO6-2"><span class="callout">2</span></a> <a href="#CO6-4"><span class="callout">4</span></a> <a href="#CO6-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							Specify the infrastructure ID, role node label, and zone.
						</div></dd><dt><a href="#CO6-6"><span class="callout">6</span></a> <a href="#CO6-7"><span class="callout">7</span></a> <a href="#CO6-9"><span class="callout">9</span></a> </dt><dd><div class="para">
							Specify the role node label to add.
						</div></dd><dt><a href="#CO6-10"><span class="callout">10</span></a> </dt><dd><div class="para">
							Specify a valid Red Hat Enterprise Linux CoreOS (RHCOS) Amazon Machine Image (AMI) for your AWS zone for your OpenShift Container Platform nodes. If you want to use an AWS Marketplace image, you must complete the OpenShift Container Platform subscription from the <a class="link" href="https://aws.amazon.com/marketplace/fulfillment?productId=59ead7de-2540-4653-a8b0-fa7926d5c845">AWS Marketplace</a> to obtain an AMI ID for your region.
						</div><pre class="programlisting language-terminal">$ oc -n openshift-machine-api \
    -o jsonpath='{.spec.template.spec.providerSpec.value.ami.id}{"\n"}' \
    get machineset/&lt;infrastructure_id&gt;-&lt;role&gt;-&lt;zone&gt;</pre></dd><dt><a href="#CO6-17"><span class="callout">17</span></a> <a href="#CO6-18"><span class="callout">18</span></a> </dt><dd><div class="para">
							Optional: Specify custom tag data for your cluster. For example, you might add an admin contact email address by specifying a <code class="literal">name:value</code> pair of <code class="literal">Email:admin-email@example.com</code>.
						</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Custom tags can also be specified during installation in the <code class="literal">install-config.yml</code> file. If the <code class="literal">install-config.yml</code> file and the machine set include a tag with the same <code class="literal">name</code> data, the value for the tag from the machine set takes priority over the value for the tag in the <code class="literal">install-config.yml</code> file.
							</p></div></div></dd><dt><a href="#CO6-12"><span class="callout">12</span></a> </dt><dd><div class="para">
							Specify the zone, for example, <code class="literal">us-east-1a</code>.
						</div></dd><dt><a href="#CO6-13"><span class="callout">13</span></a> </dt><dd><div class="para">
							Specify the region, for example, <code class="literal">us-east-1</code>.
						</div></dd><dt><a href="#CO6-15"><span class="callout">15</span></a> </dt><dd><div class="para">
							Specify the infrastructure ID and zone.
						</div></dd></dl></div></section><section class="section" id="machineset-creating_creating-machineset-aws"><div class="titlepage"><div><div><h3 class="title">2.2.2. Creating a compute machine set</h3></div></div></div><p>
					In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Deploy an OpenShift Container Platform cluster.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <code class="literal">&lt;file_name&gt;.yaml</code>.
						</p><p class="simpara">
							Ensure that you set the <code class="literal">&lt;clusterID&gt;</code> and <code class="literal">&lt;role&gt;</code> parameter values.
						</p></li><li class="listitem"><p class="simpara">
							Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									To list the compute machine sets in your cluster, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									To view values of a specific compute machine set custom resource (CR), run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machineset &lt;machineset_name&gt; \
  -n openshift-machine-api -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO7-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO7-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
    spec:
      providerSpec: <span id="CO7-3"><!--Empty--></span><span class="callout">3</span>
        ...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO7-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The cluster infrastructure ID.
										</div></dd><dt><a href="#CO7-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											A default node label.
										</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												For clusters that have user-provisioned infrastructure, a compute machine set can only create <code class="literal">worker</code> and <code class="literal">infra</code> type machines.
											</p></div></div></dd><dt><a href="#CO7-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											The values in the <code class="literal">&lt;providerSpec&gt;</code> section of the compute machine set CR are platform-specific. For more information about <code class="literal">&lt;providerSpec&gt;</code> parameters in the CR, see the sample compute machine set CR configuration for your provider.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">MachineSet</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li><li class="listitem">
							If you need compute machine sets in other availability zones, repeat this process to create more compute machine sets.
						</li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							View the list of compute machine sets by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-infra-us-east-1a    1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

							</p></div><p class="simpara">
							When the new compute machine set is available, the <code class="literal">DESIRED</code> and <code class="literal">CURRENT</code> values match. If the compute machine set is not available, wait a few minutes and run the command again.
						</p></li></ul></div></section><section class="section" id="machineset-imds-options_creating-machineset-aws"><div class="titlepage"><div><div><h3 class="title">2.2.3. Machine set options for the Amazon EC2 Instance Metadata Service</h3></div></div></div><p>
					You can use machine sets to create machines that use a specific version of the Amazon EC2 Instance Metadata Service (IMDS). Machine sets can create machines that allow the use of both IMDSv1 and <a class="link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html">IMDSv2</a> or machines that require the use of IMDSv2.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Using IMDSv2 is only supported on AWS clusters that were created with OpenShift Container Platform version 4.7 or later.
					</p></div></div><p>
					To change the IMDS configuration for existing machines, edit the machine set YAML file that manages those machines. To deploy new compute machines with your preferred IMDS configuration, create a compute machine set YAML file with the appropriate values.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Before configuring a machine set to create machines that require IMDSv2, ensure that any workloads that interact with the AWS metadata service support IMDSv2.
					</p></div></div><section class="section" id="machineset-creating-imds-options_creating-machineset-aws"><div class="titlepage"><div><div><h4 class="title">2.2.3.1. Configuring IMDS by using machine sets</h4></div></div></div><p>
						You can specify whether to require the use of IMDSv2 by adding or editing the value of <code class="literal">metadataServiceOptions.authentication</code> in the machine set YAML file for your machines.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								To use IMDSv2, your AWS cluster must have been created with OpenShift Container Platform version 4.7 or later.
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Add or edit the following lines under the <code class="literal">providerSpec</code> field:
							</p><pre class="programlisting language-yaml">providerSpec:
  value:
    metadataServiceOptions:
      authentication: Required <span id="CO8-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO8-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										To require IMDSv2, set the parameter value to <code class="literal">Required</code>. To allow the use of both IMDSv1 and IMDSv2, set the parameter value to <code class="literal">Optional</code>. If no value is specified, both IMDSv1 and IMDSv2 are allowed.
									</div></dd></dl></div></li></ul></div></section></section><section class="section" id="machineset-dedicated-instance_creating-machineset-aws"><div class="titlepage"><div><div><h3 class="title">2.2.4. Machine sets that deploy machines as Dedicated Instances</h3></div></div></div><p>
					You can create a machine set running on AWS that deploys machines as Dedicated Instances. Dedicated Instances run in a virtual private cloud (VPC) on hardware that is dedicated to a single customer. These Amazon EC2 instances are physically isolated at the host hardware level. The isolation of Dedicated Instances occurs even if the instances belong to different AWS accounts that are linked to a single payer account. However, other instances that are not dedicated can share hardware with Dedicated Instances if they belong to the same AWS account.
				</p><p>
					Instances with either public or dedicated tenancy are supported by the Machine API. Instances with public tenancy run on shared hardware. Public tenancy is the default tenancy. Instances with dedicated tenancy run on single-tenant hardware.
				</p><section class="section" id="machineset-creating-dedicated-instance_creating-machineset-aws"><div class="titlepage"><div><div><h4 class="title">2.2.4.1. Creating Dedicated Instances by using machine sets</h4></div></div></div><p>
						You can run a machine that is backed by a Dedicated Instance by using Machine API integration. Set the <code class="literal">tenancy</code> field in your machine set YAML file to launch a Dedicated Instance on AWS.
					</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Specify a dedicated tenancy under the <code class="literal">providerSpec</code> field:
							</p><pre class="programlisting language-yaml">providerSpec:
  placement:
    tenancy: dedicated</pre></li></ul></div></section></section><section class="section" id="machineset-non-guaranteed-instance_creating-machineset-aws"><div class="titlepage"><div><div><h3 class="title">2.2.5. Machine sets that deploy machines as Spot Instances</h3></div></div></div><p>
					You can save on costs by creating a compute machine set running on AWS that deploys machines as non-guaranteed Spot Instances. Spot Instances utilize unused AWS EC2 capacity and are less expensive than On-Demand Instances. You can use Spot Instances for workloads that can tolerate interruptions, such as batch or stateless, horizontally scalable workloads.
				</p><p>
					AWS EC2 can terminate a Spot Instance at any time. AWS gives a two-minute warning to the user when an interruption occurs. OpenShift Container Platform begins to remove the workloads from the affected instances when AWS issues the termination warning.
				</p><p>
					Interruptions can occur when using Spot Instances for the following reasons:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The instance price exceeds your maximum price
						</li><li class="listitem">
							The demand for Spot Instances increases
						</li><li class="listitem">
							The supply of Spot Instances decreases
						</li></ul></div><p>
					When AWS terminates an instance, a termination handler running on the Spot Instance node deletes the machine resource. To satisfy the compute machine set <code class="literal">replicas</code> quantity, the compute machine set creates a machine that requests a Spot Instance.
				</p><section class="section" id="machineset-creating-non-guaranteed-instance_creating-machineset-aws"><div class="titlepage"><div><div><h4 class="title">2.2.5.1. Creating Spot Instances by using compute machine sets</h4></div></div></div><p>
						You can launch a Spot Instance on AWS by adding <code class="literal">spotMarketOptions</code> to your compute machine set YAML file.
					</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Add the following line under the <code class="literal">providerSpec</code> field:
							</p><pre class="programlisting language-yaml">providerSpec:
  value:
    spotMarketOptions: {}</pre><p class="simpara">
								You can optionally set the <code class="literal">spotMarketOptions.maxPrice</code> field to limit the cost of the Spot Instance. For example you can set <code class="literal">maxPrice: '2.50'</code>.
							</p><p class="simpara">
								If the <code class="literal">maxPrice</code> is set, this value is used as the hourly maximum spot price. If it is not set, the maximum price defaults to charge up to the On-Demand Instance price.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									It is strongly recommended to use the default On-Demand price as the <code class="literal">maxPrice</code> value and to not set the maximum price for Spot Instances.
								</p></div></div></li></ul></div></section></section><section class="section" id="nvidia-gpu-aws-adding-a-gpu-node_creating-machineset-aws"><div class="titlepage"><div><div><h3 class="title">2.2.6. Adding a GPU node to an existing OpenShift Container Platform cluster</h3></div></div></div><p>
					You can copy and modify a default compute machine set configuration to create a GPU-enabled machine set and machines for the AWS EC2 cloud provider.
				</p><p>
					The following table lists the validated instance types:
				</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140311159589280" scope="col">Instance type</th><th align="left" valign="top" id="idm140311159588192" scope="col">NVIDIA GPU accelerator</th><th align="left" valign="top" id="idm140311159587104" scope="col">Maximum number of GPUs</th><th align="left" valign="top" id="idm140311147022608" scope="col">Architecture</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140311159589280"> <p>
									<code class="literal">p4d.24xlarge</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140311159588192"> <p>
									A100
								</p>
								 </td><td align="left" valign="top" headers="idm140311159587104"> <p>
									8
								</p>
								 </td><td align="left" valign="top" headers="idm140311147022608"> <p>
									x86
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140311159589280"> <p>
									<code class="literal">g4dn.xlarge</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140311159588192"> <p>
									T4
								</p>
								 </td><td align="left" valign="top" headers="idm140311159587104"> <p>
									1
								</p>
								 </td><td align="left" valign="top" headers="idm140311147022608"> <p>
									x86
								</p>
								 </td></tr></tbody></table></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the existing nodes, machines, and machine sets by running the following command. Note that each node is an instance of a machine definition with a specific AWS region and OpenShift Container Platform role.
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                        STATUS   ROLES                  AGE     VERSION
ip-10-0-52-50.us-east-2.compute.internal    Ready    worker                 3d17h   v1.26.0
ip-10-0-58-24.us-east-2.compute.internal    Ready    control-plane,master   3d17h   v1.26.0
ip-10-0-68-148.us-east-2.compute.internal   Ready    worker                 3d17h   v1.26.0
ip-10-0-68-68.us-east-2.compute.internal    Ready    control-plane,master   3d17h   v1.26.0
ip-10-0-72-170.us-east-2.compute.internal   Ready    control-plane,master   3d17h   v1.26.0
ip-10-0-74-50.us-east-2.compute.internal    Ready    worker                 3d17h   v1.26.0</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							View the machines and machine sets that exist in the <code class="literal">openshift-machine-api</code> namespace by running the following command. Each compute machine set is associated with a different availability zone within the AWS region. The installer automatically load balances compute machines across availability zones.
						</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                        DESIRED   CURRENT   READY   AVAILABLE   AGE
preserve-dsoc12r4-ktjfc-worker-us-east-2a   1         1         1       1           3d11h
preserve-dsoc12r4-ktjfc-worker-us-east-2b   2         2         2       2           3d11h</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							View the machines that exist in the <code class="literal">openshift-machine-api</code> namespace by running the following command. At this time, there is only one compute machine per machine set, though a compute machine set could be scaled to add a node in a particular region and zone.
						</p><pre class="programlisting language-terminal">$ oc get machines -n openshift-machine-api | grep worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">preserve-dsoc12r4-ktjfc-worker-us-east-2a-dts8r      Running   m5.xlarge   us-east-2   us-east-2a   3d11h
preserve-dsoc12r4-ktjfc-worker-us-east-2b-dkv7w      Running   m5.xlarge   us-east-2   us-east-2b   3d11h
preserve-dsoc12r4-ktjfc-worker-us-east-2b-k58cw      Running   m5.xlarge   us-east-2   us-east-2b   3d11h</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Make a copy of one of the existing compute <code class="literal">MachineSet</code> definitions and output the result to a JSON file by running the following command. This will be the basis for the GPU-enabled compute machine set definition.
						</p><pre class="programlisting language-terminal">$ oc get machineset preserve-dsoc12r4-ktjfc-worker-us-east-2a -n openshift-machine-api -o json &gt; &lt;output_file.json&gt;</pre></li><li class="listitem"><p class="simpara">
							Edit the JSON file and make the following changes to the new <code class="literal">MachineSet</code> definition:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Replace <code class="literal">worker</code> with <code class="literal">gpu</code>. This will be the name of the new machine set.
								</li><li class="listitem"><p class="simpara">
									Change the instance type of the new <code class="literal">MachineSet</code> definition to <code class="literal">g4dn</code>, which includes an NVIDIA Tesla T4 GPU. To learn more about AWS <code class="literal">g4dn</code> instance types, see <a class="link" href="https://aws.amazon.com/ec2/instance-types/#Accelerated_Computing">Accelerated Computing</a>.
								</p><pre class="programlisting language-terminal">$ jq .spec.template.spec.providerSpec.value.instanceType preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a.json

"g4dn.xlarge"</pre><p class="simpara">
									The <code class="literal">&lt;output_file.json&gt;</code> file is saved as <code class="literal">preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a.json</code>.
								</p></li></ul></div></li><li class="listitem"><p class="simpara">
							Update the following fields in <code class="literal">preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a.json</code>:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">.metadata.name</code> to a name containing <code class="literal">gpu</code>.
								</li><li class="listitem">
									<code class="literal">.spec.selector.matchLabels["machine.openshift.io/cluster-api-machineset"]</code> to match the new <code class="literal">.metadata.name</code>.
								</li><li class="listitem">
									<code class="literal">.spec.template.metadata.labels["machine.openshift.io/cluster-api-machineset"]</code> to match the new <code class="literal">.metadata.name</code>.
								</li><li class="listitem">
									<code class="literal">.spec.template.spec.providerSpec.value.instanceType</code> to <code class="literal">g4dn.xlarge</code>.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							To verify your changes, perform a <code class="literal">diff</code> of the original compute definition and the new GPU-enabled node definition by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api get preserve-dsoc12r4-ktjfc-worker-us-east-2a -o json | diff preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a.json -</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">10c10

&lt; "name": "preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a",
---
&gt; "name": "preserve-dsoc12r4-ktjfc-worker-us-east-2a",

21c21

&lt; "machine.openshift.io/cluster-api-machineset": "preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a"
---
&gt; "machine.openshift.io/cluster-api-machineset": "preserve-dsoc12r4-ktjfc-worker-us-east-2a"

31c31

&lt; "machine.openshift.io/cluster-api-machineset": "preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a"
---
&gt; "machine.openshift.io/cluster-api-machineset": "preserve-dsoc12r4-ktjfc-worker-us-east-2a"

60c60

&lt; "instanceType": "g4dn.xlarge",
---
&gt; "instanceType": "m5.xlarge",</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create the GPU-enabled compute machine set from the definition by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a.json</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">machineset.machine.openshift.io/preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a created</pre>

							</p></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the machine set you created by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api get machinesets | grep gpu</pre><p class="simpara">
							The MachineSet replica count is set to <code class="literal">1</code> so a new <code class="literal">Machine</code> object is created automatically.
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a   1         1         1       1           4m21s</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							View the <code class="literal">Machine</code> object that the machine set created by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api get machines | grep gpu</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">preserve-dsoc12r4-ktjfc-worker-gpu-us-east-2a    running    g4dn.xlarge   us-east-2   us-east-2a  4m36s</pre>

							</p></div></li></ol></div><p>
					Note that there is no need to specify a namespace for the node. The node definition is cluster scoped.
				</p></section><section class="section" id="nvidia-gpu-aws-deploying-the-node-feature-discovery-operator_creating-machineset-aws"><div class="titlepage"><div><div><h3 class="title">2.2.7. Deploying the Node Feature Discovery Operator</h3></div></div></div><p>
					After the GPU-enabled node is created, you need to discover the GPU-enabled node so it can be scheduled. To do this, install the Node Feature Discovery (NFD) Operator. The NFD Operator identifies hardware device features in nodes. It solves the general problem of identifying and cataloging hardware resources in the infrastructure nodes so they can be made available to OpenShift Container Platform.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Install the Node Feature Discovery Operator from <span class="strong strong"><strong>OperatorHub</strong></span> in the OpenShift Container Platform console.
						</li><li class="listitem">
							After installing the NFD Operator into <span class="strong strong"><strong>OperatorHub</strong></span>, select <span class="strong strong"><strong>Node Feature Discovery</strong></span> from the installed Operators list and select <span class="strong strong"><strong>Create instance</strong></span>. This installs the <code class="literal">nfd-master</code> and <code class="literal">nfd-worker</code> pods, one <code class="literal">nfd-worker</code> pod for each compute node, in the <code class="literal">openshift-nfd</code> namespace.
						</li><li class="listitem"><p class="simpara">
							Verify that the Operator is installed and running by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-nfd</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                       READY    STATUS     RESTARTS   AGE

nfd-controller-manager-8646fcbb65-x5qgk    2/2      Running 7  (8h ago)   1d</pre>

							</p></div></li><li class="listitem">
							Browse to the installed Oerator in the console and select <span class="strong strong"><strong>Create Node Feature Discovery</strong></span>.
						</li><li class="listitem">
							Select <span class="strong strong"><strong>Create</strong></span> to build a NFD custom resource. This creates NFD pods in the <code class="literal">openshift-nfd</code> namespace that poll the OpenShift Container Platform nodes for hardware resources and catalogue them.
						</li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							After a successful build, verify that a NFD pod is running on each nodes by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-nfd</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                       READY   STATUS      RESTARTS        AGE
nfd-controller-manager-8646fcbb65-x5qgk    2/2     Running     7 (8h ago)      12d
nfd-master-769656c4cb-w9vrv                1/1     Running     0               12d
nfd-worker-qjxb2                           1/1     Running     3 (3d14h ago)   12d
nfd-worker-xtz9b                           1/1     Running     5 (3d14h ago)   12d</pre>

							</p></div><p class="simpara">
							The NFD Operator uses vendor PCI IDs to identify hardware in a node. NVIDIA uses the PCI ID <code class="literal">10de</code>.
						</p></li><li class="listitem"><p class="simpara">
							View the NVIDIA GPU discovered by the NFD Operator by running the following command:
						</p><pre class="programlisting language-terminal">$ oc describe node ip-10-0-132-138.us-east-2.compute.internal | egrep 'Roles|pci'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Roles: worker

feature.node.kubernetes.io/pci-1013.present=true

feature.node.kubernetes.io/pci-10de.present=true

feature.node.kubernetes.io/pci-1d0f.present=true</pre>

							</p></div><p class="simpara">
							<code class="literal">10de</code> appears in the node feature list for the GPU-enabled node. This mean the NFD Operator correctly identified the node from the GPU-enabled MachineSet.
						</p></li></ol></div></section></section><section class="section" id="creating-machineset-azure"><div class="titlepage"><div><div><h2 class="title">2.3. Creating a compute machine set on Azure</h2></div></div></div><p>
				You can create a different compute machine set to serve a specific purpose in your OpenShift Container Platform cluster on Microsoft Azure. For example, you might create infrastructure machine sets and related machines so that you can move supporting workloads to the new machines.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.
				</p><p>
					Clusters with the infrastructure platform type <code class="literal">none</code> cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.
				</p><p>
					To view the platform type for your cluster, run the following command:
				</p><pre class="programlisting language-terminal">$ oc get infrastructure cluster -o jsonpath='{.status.platform}'</pre></div></div><section class="section" id="machineset-yaml-azure_creating-machineset-azure"><div class="titlepage"><div><div><h3 class="title">2.3.1. Sample YAML for a compute machine set custom resource on Azure</h3></div></div></div><p>
					This sample YAML defines a compute machine set that runs in the <code class="literal">1</code> Microsoft Azure zone in a region and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/&lt;role&gt;: ""</code>.
				</p><p>
					In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;role&gt;</code> is the node label to add.
				</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO9-1"><!--Empty--></span><span class="callout">1</span>
    machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO9-2"><!--Empty--></span><span class="callout">2</span>
    machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO9-3"><!--Empty--></span><span class="callout">3</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;region&gt; <span id="CO9-4"><!--Empty--></span><span class="callout">4</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO9-5"><!--Empty--></span><span class="callout">5</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;region&gt; <span id="CO9-6"><!--Empty--></span><span class="callout">6</span>
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO9-7"><!--Empty--></span><span class="callout">7</span>
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO9-8"><!--Empty--></span><span class="callout">8</span>
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO9-9"><!--Empty--></span><span class="callout">9</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;region&gt; <span id="CO9-10"><!--Empty--></span><span class="callout">10</span>
    spec:
      metadata:
        creationTimestamp: null
        labels:
          machine.openshift.io/cluster-api-machineset: &lt;machineset_name&gt; <span id="CO9-11"><!--Empty--></span><span class="callout">11</span>
          node-role.kubernetes.io/&lt;role&gt;: "" <span id="CO9-12"><!--Empty--></span><span class="callout">12</span>
      providerSpec:
        value:
          apiVersion: azureproviderconfig.openshift.io/v1beta1
          credentialsSecret:
            name: azure-cloud-credentials
            namespace: openshift-machine-api
          image: <span id="CO9-13"><!--Empty--></span><span class="callout">13</span>
            offer: ""
            publisher: ""
            resourceID: /resourceGroups/&lt;infrastructure_id&gt;-rg/providers/Microsoft.Compute/images/&lt;infrastructure_id&gt; <span id="CO9-14"><!--Empty--></span><span class="callout">14</span>
            sku: ""
            version: ""
          internalLoadBalancer: ""
          kind: AzureMachineProviderSpec
          location: &lt;region&gt; <span id="CO9-15"><!--Empty--></span><span class="callout">15</span>
          managedIdentity: &lt;infrastructure_id&gt;-identity <span id="CO9-16"><!--Empty--></span><span class="callout">16</span>
          metadata:
            creationTimestamp: null
          natRule: null
          networkResourceGroup: ""
          osDisk:
            diskSizeGB: 128
            managedDisk:
              storageAccountType: Premium_LRS
            osType: Linux
          publicIP: false
          publicLoadBalancer: ""
          resourceGroup: &lt;infrastructure_id&gt;-rg <span id="CO9-17"><!--Empty--></span><span class="callout">17</span>
          sshPrivateKey: ""
          sshPublicKey: ""
          tags:
            - name: &lt;custom_tag_name&gt; <span id="CO9-18"><!--Empty--></span><span class="callout">18</span>
              value: &lt;custom_tag_value&gt; <span id="CO9-19"><!--Empty--></span><span class="callout">19</span>
          subnet: &lt;infrastructure_id&gt;-&lt;role&gt;-subnet <span id="CO9-20"><!--Empty--></span><span class="callout">20</span> <span id="CO9-21"><!--Empty--></span><span class="callout">21</span>
          userDataSecret:
            name: worker-user-data <span id="CO9-22"><!--Empty--></span><span class="callout">22</span>
          vmSize: Standard_D4s_v3
          vnet: &lt;infrastructure_id&gt;-vnet <span id="CO9-23"><!--Empty--></span><span class="callout">23</span>
          zone: "1" <span id="CO9-24"><!--Empty--></span><span class="callout">24</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO9-1"><span class="callout">1</span></a> <a href="#CO9-5"><span class="callout">5</span></a> <a href="#CO9-7"><span class="callout">7</span></a> <a href="#CO9-16"><span class="callout">16</span></a> <a href="#CO9-17"><span class="callout">17</span></a> <a href="#CO9-20"><span class="callout">20</span></a> <a href="#CO9-23"><span class="callout">23</span></a> </dt><dd><div class="para">
							Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
						</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre><p>
							You can obtain the subnet by running the following command:
						</p><pre class="programlisting language-terminal">$  oc -n openshift-machine-api \
    -o jsonpath='{.spec.template.spec.providerSpec.value.subnet}{"\n"}' \
    get machineset/&lt;infrastructure_id&gt;-worker-centralus1</pre><p>
							You can obtain the vnet by running the following command:
						</p><pre class="programlisting language-terminal">$  oc -n openshift-machine-api \
    -o jsonpath='{.spec.template.spec.providerSpec.value.vnet}{"\n"}' \
    get machineset/&lt;infrastructure_id&gt;-worker-centralus1</pre></dd><dt><a href="#CO9-2"><span class="callout">2</span></a> <a href="#CO9-3"><span class="callout">3</span></a> <a href="#CO9-8"><span class="callout">8</span></a> <a href="#CO9-9"><span class="callout">9</span></a> <a href="#CO9-12"><span class="callout">12</span></a> <a href="#CO9-21"><span class="callout">21</span></a> <a href="#CO9-22"><span class="callout">22</span></a> </dt><dd><div class="para">
							Specify the node label to add.
						</div></dd><dt><a href="#CO9-4"><span class="callout">4</span></a> <a href="#CO9-6"><span class="callout">6</span></a> <a href="#CO9-10"><span class="callout">10</span></a> </dt><dd><div class="para">
							Specify the infrastructure ID, node label, and region.
						</div></dd><dt><a href="#CO9-11"><span class="callout">11</span></a> </dt><dd><div class="para">
							Optional: Specify the compute machine set name to enable the use of availability sets. This setting only applies to new compute machines.
						</div></dd><dt><a href="#CO9-13"><span class="callout">13</span></a> </dt><dd><div class="para">
							Specify the image details for your compute machine set. If you want to use an Azure Marketplace image, see "Selecting an Azure Marketplace image".
						</div></dd><dt><a href="#CO9-14"><span class="callout">14</span></a> </dt><dd><div class="para">
							Specify an image that is compatible with your instance type. The Hyper-V generation V2 images created by the installation program have a <code class="literal">-gen2</code> suffix, while V1 images have the same name without the suffix.
						</div></dd><dt><a href="#CO9-15"><span class="callout">15</span></a> </dt><dd><div class="para">
							Specify the region to place machines on.
						</div></dd><dt><a href="#CO9-24"><span class="callout">24</span></a> </dt><dd><div class="para">
							Specify the zone within your region to place machines on. Be sure that your region supports the zone that you specify.
						</div></dd><dt><a href="#CO9-18"><span class="callout">18</span></a> <a href="#CO9-19"><span class="callout">19</span></a> </dt><dd><div class="para">
							Optional: Specify custom tags in your machine set. Provide the tag name in <code class="literal">&lt;custom_tag_name&gt;</code> field and the corresponding tag value in <code class="literal">&lt;custom_tag_value&gt;</code> field.
						</div></dd></dl></div></section><section class="section" id="machineset-creating_creating-machineset-azure"><div class="titlepage"><div><div><h3 class="title">2.3.2. Creating a compute machine set</h3></div></div></div><p>
					In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Deploy an OpenShift Container Platform cluster.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <code class="literal">&lt;file_name&gt;.yaml</code>.
						</p><p class="simpara">
							Ensure that you set the <code class="literal">&lt;clusterID&gt;</code> and <code class="literal">&lt;role&gt;</code> parameter values.
						</p></li><li class="listitem"><p class="simpara">
							Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									To list the compute machine sets in your cluster, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									To view values of a specific compute machine set custom resource (CR), run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machineset &lt;machineset_name&gt; \
  -n openshift-machine-api -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO10-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO10-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
    spec:
      providerSpec: <span id="CO10-3"><!--Empty--></span><span class="callout">3</span>
        ...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO10-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The cluster infrastructure ID.
										</div></dd><dt><a href="#CO10-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											A default node label.
										</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												For clusters that have user-provisioned infrastructure, a compute machine set can only create <code class="literal">worker</code> and <code class="literal">infra</code> type machines.
											</p></div></div></dd><dt><a href="#CO10-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											The values in the <code class="literal">&lt;providerSpec&gt;</code> section of the compute machine set CR are platform-specific. For more information about <code class="literal">&lt;providerSpec&gt;</code> parameters in the CR, see the sample compute machine set CR configuration for your provider.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">MachineSet</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							View the list of compute machine sets by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-infra-us-east-1a    1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

							</p></div><p class="simpara">
							When the new compute machine set is available, the <code class="literal">DESIRED</code> and <code class="literal">CURRENT</code> values match. If the compute machine set is not available, wait a few minutes and run the command again.
						</p></li></ul></div></section><section class="section" id="installation-azure-marketplace-subscribe_creating-machineset-azure"><div class="titlepage"><div><div><h3 class="title">2.3.3. Selecting an Azure Marketplace image</h3></div></div></div><p>
					You can create a machine set running on Azure that deploys machines that use the Azure Marketplace offering. To use this offering, you must first obtain the Azure Marketplace image. When obtaining your image, consider the following:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							While the images are the same, the Azure Marketplace publisher is different depending on your region. If you are located in North America, specify <code class="literal">redhat</code> as the publisher. If you are located in EMEA, specify <code class="literal">redhat-limited</code> as the publisher.
						</li><li class="listitem">
							The offer includes a <code class="literal">rh-ocp-worker</code> SKU and a <code class="literal">rh-ocp-worker-gen1</code> SKU. The <code class="literal">rh-ocp-worker</code> SKU represents a Hyper-V generation version 2 VM image. The default instance types used in OpenShift Container Platform are version 2 compatible. If you plan to use an instance type that is only version 1 compatible, use the image associated with the <code class="literal">rh-ocp-worker-gen1</code> SKU. The <code class="literal">rh-ocp-worker-gen1</code> SKU represents a Hyper-V version 1 VM image.
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Installing images with the Azure marketplace is not supported on clusters with 64-bit ARM instances.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the Azure CLI client <code class="literal">(az)</code>.
						</li><li class="listitem">
							Your Azure account is entitled for the offer and you have logged into this account with the Azure CLI client.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Display all of the available OpenShift Container Platform images by running one of the following commands:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									North America:
								</p><pre class="programlisting language-terminal">$  az vm image list --all --offer rh-ocp-worker --publisher redhat -o table</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">Offer          Publisher       Sku                 Urn                                                             Version
-------------  --------------  ------------------  --------------------------------------------------------------  --------------
rh-ocp-worker  RedHat          rh-ocp-worker       RedHat:rh-ocp-worker:rh-ocpworker:4.8.2021122100               4.8.2021122100
rh-ocp-worker  RedHat          rh-ocp-worker-gen1  RedHat:rh-ocp-worker:rh-ocp-worker-gen1:4.8.2021122100         4.8.2021122100</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									EMEA:
								</p><pre class="programlisting language-terminal">$  az vm image list --all --offer rh-ocp-worker --publisher redhat-limited -o table</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">Offer          Publisher       Sku                 Urn                                                             Version
-------------  --------------  ------------------  --------------------------------------------------------------  --------------
rh-ocp-worker  redhat-limited  rh-ocp-worker       redhat-limited:rh-ocp-worker:rh-ocp-worker:4.8.2021122100       4.8.2021122100
rh-ocp-worker  redhat-limited  rh-ocp-worker-gen1  redhat-limited:rh-ocp-worker:rh-ocp-worker-gen1:4.8.2021122100  4.8.2021122100</pre>

									</p></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Regardless of the version of OpenShift Container Platform that you install, the correct version of the Azure Marketplace image to use is 4.8. If required, your VMs are automatically upgraded as part of the installation process.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Inspect the image for your offer by running one of the following commands:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									North America:
								</p><pre class="programlisting language-terminal">$ az vm image show --urn redhat:rh-ocp-worker:rh-ocp-worker:&lt;version&gt;</pre></li><li class="listitem"><p class="simpara">
									EMEA:
								</p><pre class="programlisting language-terminal">$ az vm image show --urn redhat-limited:rh-ocp-worker:rh-ocp-worker:&lt;version&gt;</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							Review the terms of the offer by running one of the following commands:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									North America:
								</p><pre class="programlisting language-terminal">$ az vm image terms show --urn redhat:rh-ocp-worker:rh-ocp-worker:&lt;version&gt;</pre></li><li class="listitem"><p class="simpara">
									EMEA:
								</p><pre class="programlisting language-terminal">$ az vm image terms show --urn redhat-limited:rh-ocp-worker:rh-ocp-worker:&lt;version&gt;</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							Accept the terms of the offering by running one of the following commands:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									North America:
								</p><pre class="programlisting language-terminal">$ az vm image terms accept --urn redhat:rh-ocp-worker:rh-ocp-worker:&lt;version&gt;</pre></li><li class="listitem"><p class="simpara">
									EMEA:
								</p><pre class="programlisting language-terminal">$ az vm image terms accept --urn redhat-limited:rh-ocp-worker:rh-ocp-worker:&lt;version&gt;</pre></li></ul></div></li><li class="listitem">
							Record the image details of your offer, specifically the values for <code class="literal">publisher</code>, <code class="literal">offer</code>, <code class="literal">sku</code>, and <code class="literal">version</code>.
						</li><li class="listitem"><p class="simpara">
							Add the following parameters to the <code class="literal">providerSpec</code> section of your machine set YAML file using the image details for your offer:
						</p><div class="formalpara"><p class="title"><strong>Sample <code class="literal">providerSpec</code> image values for Azure Marketplace machines</strong></p><p>
								
<pre class="programlisting language-yaml">providerSpec:
  value:
    image:
      offer: rh-ocp-worker
      publisher: redhat
      resourceID: ""
      sku: rh-ocp-worker
      type: MarketplaceWithPlan
      version: 4.8.2021122100</pre>

							</p></div></li></ol></div></section><section class="section" id="machineset-azure-boot-diagnostics_creating-machineset-azure"><div class="titlepage"><div><div><h3 class="title">2.3.4. Enabling Azure boot diagnostics</h3></div></div></div><p>
					You can enable boot diagnostics on Azure machines that your machine set creates.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Have an existing Microsoft Azure cluster.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Add the <code class="literal">diagnostics</code> configuration that is applicable to your storage type to the <code class="literal">providerSpec</code> field in your machine set YAML file:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
									For an Azure Managed storage account:
								</p><pre class="programlisting language-yaml">providerSpec:
  diagnostics:
    boot:
      storageAccountType: AzureManaged <span id="CO11-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO11-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specifies an Azure Managed storage account.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									For an Azure Unmanaged storage account:
								</p><pre class="programlisting language-yaml">providerSpec:
  diagnostics:
    boot:
      storageAccountType: CustomerManaged <span id="CO12-1"><!--Empty--></span><span class="callout">1</span>
      customerManaged:
        storageAccountURI: https://&lt;storage-account&gt;.blob.core.windows.net <span id="CO12-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO12-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specifies an Azure Unmanaged storage account.
										</div></dd><dt><a href="#CO12-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Replace <code class="literal">&lt;storage-account&gt;</code> with the name of your storage account.
										</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Only the Azure Blob Storage data service is supported.
									</p></div></div></li></ul></div></li></ul></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							On the Microsoft Azure portal, review the <span class="strong strong"><strong>Boot diagnostics</strong></span> page for a machine deployed by the machine set, and verify that you can see the serial logs for the machine.
						</li></ul></div></section><section class="section" id="machineset-non-guaranteed-instance_creating-machineset-azure"><div class="titlepage"><div><div><h3 class="title">2.3.5. Machine sets that deploy machines as Spot VMs</h3></div></div></div><p>
					You can save on costs by creating a compute machine set running on Azure that deploys machines as non-guaranteed Spot VMs. Spot VMs utilize unused Azure capacity and are less expensive than standard VMs. You can use Spot VMs for workloads that can tolerate interruptions, such as batch or stateless, horizontally scalable workloads.
				</p><p>
					Azure can terminate a Spot VM at any time. Azure gives a 30-second warning to the user when an interruption occurs. OpenShift Container Platform begins to remove the workloads from the affected instances when Azure issues the termination warning.
				</p><p>
					Interruptions can occur when using Spot VMs for the following reasons:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The instance price exceeds your maximum price
						</li><li class="listitem">
							The supply of Spot VMs decreases
						</li><li class="listitem">
							Azure needs capacity back
						</li></ul></div><p>
					When Azure terminates an instance, a termination handler running on the Spot VM node deletes the machine resource. To satisfy the compute machine set <code class="literal">replicas</code> quantity, the compute machine set creates a machine that requests a Spot VM.
				</p><section class="section" id="machineset-creating-non-guaranteed-instance_creating-machineset-azure"><div class="titlepage"><div><div><h4 class="title">2.3.5.1. Creating Spot VMs by using compute machine sets</h4></div></div></div><p>
						You can launch a Spot VM on Azure by adding <code class="literal">spotVMOptions</code> to your compute machine set YAML file.
					</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Add the following line under the <code class="literal">providerSpec</code> field:
							</p><pre class="programlisting language-yaml">providerSpec:
  value:
    spotVMOptions: {}</pre><p class="simpara">
								You can optionally set the <code class="literal">spotVMOptions.maxPrice</code> field to limit the cost of the Spot VM. For example you can set <code class="literal">maxPrice: '0.98765'</code>. If the <code class="literal">maxPrice</code> is set, this value is used as the hourly maximum spot price. If it is not set, the maximum price defaults to <code class="literal">-1</code> and charges up to the standard VM price.
							</p><p class="simpara">
								Azure caps Spot VM prices at the standard price. Azure will not evict an instance due to pricing if the instance is set with the default <code class="literal">maxPrice</code>. However, an instance can still be evicted due to capacity restrictions.
							</p></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							It is strongly recommended to use the default standard VM price as the <code class="literal">maxPrice</code> value and to not set the maximum price for Spot VMs.
						</p></div></div></section></section><section class="section" id="machineset-azure-ephemeral-os_creating-machineset-azure"><div class="titlepage"><div><div><h3 class="title">2.3.6. Machine sets that deploy machines on Ephemeral OS disks</h3></div></div></div><p>
					You can create a compute machine set running on Azure that deploys machines on Ephemeral OS disks. Ephemeral OS disks use local VM capacity rather than remote Azure Storage. This configuration therefore incurs no additional cost and provides lower latency for reading, writing, and reimaging.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For more information, see the Microsoft Azure documentation about <a class="link" href="https://docs.microsoft.com/en-us/azure/virtual-machines/ephemeral-os-disks">Ephemeral OS disks for Azure VMs</a>.
						</li></ul></div><section class="section" id="machineset-creating-azure-ephemeral-os_creating-machineset-azure"><div class="titlepage"><div><div><h4 class="title">2.3.6.1. Creating machines on Ephemeral OS disks by using compute machine sets</h4></div></div></div><p>
						You can launch machines on Ephemeral OS disks on Azure by editing your compute machine set YAML file.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Have an existing Microsoft Azure cluster.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the custom resource (CR) by running the following command:
							</p><pre class="programlisting language-terminal">$ oc edit machineset &lt;machine-set-name&gt;</pre><p class="simpara">
								where <code class="literal">&lt;machine-set-name&gt;</code> is the compute machine set that you want to provision machines on Ephemeral OS disks.
							</p></li><li class="listitem"><p class="simpara">
								Add the following to the <code class="literal">providerSpec</code> field:
							</p><pre class="programlisting language-yaml">providerSpec:
  value:
    ...
    osDisk:
       ...
       diskSettings: <span id="CO13-1"><!--Empty--></span><span class="callout">1</span>
         ephemeralStorageLocation: Local <span id="CO13-2"><!--Empty--></span><span class="callout">2</span>
       cachingType: ReadOnly <span id="CO13-3"><!--Empty--></span><span class="callout">3</span>
       managedDisk:
         storageAccountType: Standard_LRS <span id="CO13-4"><!--Empty--></span><span class="callout">4</span>
       ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO13-1"><span class="callout">1</span></a> <a href="#CO13-2"><span class="callout">2</span></a> <a href="#CO13-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										These lines enable the use of Ephemeral OS disks.
									</div></dd><dt><a href="#CO13-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										Ephemeral OS disks are only supported for VMs or scale set instances that use the Standard LRS storage account type.
									</div></dd></dl></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									The implementation of Ephemeral OS disk support in OpenShift Container Platform only supports the <code class="literal">CacheDisk</code> placement type. Do not change the <code class="literal">placement</code> configuration setting.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Create a compute machine set using the updated configuration:
							</p><pre class="programlisting language-terminal">$ oc create -f &lt;machine-set-config&gt;.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								On the Microsoft Azure portal, review the <span class="strong strong"><strong>Overview</strong></span> page for a machine deployed by the compute machine set, and verify that the <code class="literal">Ephemeral OS disk</code> field is set to <code class="literal">OS cache placement</code>.
							</li></ul></div></section></section><section class="section" id="machineset-azure-ultra-disk_creating-machineset-azure"><div class="titlepage"><div><div><h3 class="title">2.3.7. Machine sets that deploy machines with ultra disks as data disks</h3></div></div></div><p>
					You can create a machine set running on Azure that deploys machines with ultra disks. Ultra disks are high-performance storage that are intended for use with the most demanding data workloads.
				</p><p>
					You can also create a persistent volume claim (PVC) that dynamically binds to a storage class backed by Azure ultra disks and mounts them to pods.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Data disks do not support the ability to specify disk throughput or disk IOPS. You can configure these properties by using PVCs.
					</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://docs.microsoft.com/en-us/azure/virtual-machines/disks-types#ultra-disks">Microsoft Azure ultra disks documentation</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/storage/#machineset-azure-ultra-disk_persistent-storage-csi-azure">Machine sets that deploy machines on ultra disks using CSI PVCs</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/storage/#machineset-azure-ultra-disk_persistent-storage-azure">Machine sets that deploy machines on ultra disks using in-tree PVCs</a>
						</li></ul></div><section class="section" id="machineset-creating-azure-ultra-disk_creating-machineset-azure"><div class="titlepage"><div><div><h4 class="title">2.3.7.1. Creating machines with ultra disks by using machine sets</h4></div></div></div><p>
						You can deploy machines with ultra disks on Azure by editing your machine set YAML file.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Have an existing Microsoft Azure cluster.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a custom secret in the <code class="literal">openshift-machine-api</code> namespace using the <code class="literal">worker</code> data secret by running the following command:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api \
get secret &lt;role&gt;-user-data \ <span id="CO14-1"><!--Empty--></span><span class="callout">1</span>
--template='{{index .data.userData | base64decode}}' | jq &gt; userData.txt <span id="CO14-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO14-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;role&gt;</code> with <code class="literal">worker</code>.
									</div></dd><dt><a href="#CO14-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specify <code class="literal">userData.txt</code> as the name of the new custom secret.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								In a text editor, open the <code class="literal">userData.txt</code> file and locate the final <code class="literal">}</code> character in the file.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										On the immediately preceding line, add a <code class="literal">,</code>.
									</li><li class="listitem"><p class="simpara">
										Create a new line after the <code class="literal">,</code> and add the following configuration details:
									</p><pre class="programlisting language-json">"storage": {
  "disks": [ <span id="CO15-1"><!--Empty--></span><span class="callout">1</span>
    {
      "device": "/dev/disk/azure/scsi1/lun0", <span id="CO15-2"><!--Empty--></span><span class="callout">2</span>
      "partitions": [ <span id="CO15-3"><!--Empty--></span><span class="callout">3</span>
        {
          "label": "lun0p1", <span id="CO15-4"><!--Empty--></span><span class="callout">4</span>
          "sizeMiB": 1024, <span id="CO15-5"><!--Empty--></span><span class="callout">5</span>
          "startMiB": 0
        }
      ]
    }
  ],
  "filesystems": [ <span id="CO15-6"><!--Empty--></span><span class="callout">6</span>
    {
      "device": "/dev/disk/by-partlabel/lun0p1",
      "format": "xfs",
      "path": "/var/lib/lun0p1"
    }
  ]
},
"systemd": {
  "units": [ <span id="CO15-7"><!--Empty--></span><span class="callout">7</span>
    {
      "contents": "[Unit]\nBefore=local-fs.target\n[Mount]\nWhere=/var/lib/lun0p1\nWhat=/dev/disk/by-partlabel/lun0p1\nOptions=defaults,pquota\n[Install]\nWantedBy=local-fs.target\n", <span id="CO15-8"><!--Empty--></span><span class="callout">8</span>
      "enabled": true,
      "name": "var-lib-lun0p1.mount"
    }
  ]
}</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO15-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												The configuration details for the disk that you want to attach to a node as an ultra disk.
											</div></dd><dt><a href="#CO15-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Specify the <code class="literal">lun</code> value that is defined in the <code class="literal">dataDisks</code> stanza of the machine set you are using. For example, if the machine set contains <code class="literal">lun: 0</code>, specify <code class="literal">lun0</code>. You can initialize multiple data disks by specifying multiple <code class="literal">"disks"</code> entries in this configuration file. If you specify multiple <code class="literal">"disks"</code> entries, ensure that the <code class="literal">lun</code> value for each matches the value in the machine set.
											</div></dd><dt><a href="#CO15-3"><span class="callout">3</span></a> </dt><dd><div class="para">
												The configuration details for a new partition on the disk.
											</div></dd><dt><a href="#CO15-4"><span class="callout">4</span></a> </dt><dd><div class="para">
												Specify a label for the partition. You might find it helpful to use hierarchical names, such as <code class="literal">lun0p1</code> for the first partition of <code class="literal">lun0</code>.
											</div></dd><dt><a href="#CO15-5"><span class="callout">5</span></a> </dt><dd><div class="para">
												Specify the total size in MiB of the partition.
											</div></dd><dt><a href="#CO15-6"><span class="callout">6</span></a> </dt><dd><div class="para">
												Specify the filesystem to use when formatting a partition. Use the partition label to specify the partition.
											</div></dd><dt><a href="#CO15-7"><span class="callout">7</span></a> </dt><dd><div class="para">
												Specify a <code class="literal">systemd</code> unit to mount the partition at boot. Use the partition label to specify the partition. You can create multiple partitions by specifying multiple <code class="literal">"partitions"</code> entries in this configuration file. If you specify multiple <code class="literal">"partitions"</code> entries, you must specify a <code class="literal">systemd</code> unit for each.
											</div></dd><dt><a href="#CO15-8"><span class="callout">8</span></a> </dt><dd><div class="para">
												For <code class="literal">Where</code>, specify the value of <code class="literal">storage.filesystems.path</code>. For <code class="literal">What</code>, specify the value of <code class="literal">storage.filesystems.device</code>.
											</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
								Extract the disabling template value to a file called <code class="literal">disableTemplating.txt</code> by running the following command:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api get secret &lt;role&gt;-user-data \ <span id="CO16-1"><!--Empty--></span><span class="callout">1</span>
--template='{{index .data.disableTemplating | base64decode}}' | jq &gt; disableTemplating.txt</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO16-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;role&gt;</code> with <code class="literal">worker</code>.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Combine the <code class="literal">userData.txt</code> file and <code class="literal">disableTemplating.txt</code> file to create a data secret file by running the following command:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api create secret generic &lt;role&gt;-user-data-x5 \ <span id="CO17-1"><!--Empty--></span><span class="callout">1</span>
--from-file=userData=userData.txt \
--from-file=disableTemplating=disableTemplating.txt</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO17-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										For <code class="literal">&lt;role&gt;-user-data-x5</code>, specify the name of the secret. Replace <code class="literal">&lt;role&gt;</code> with <code class="literal">worker</code>.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Copy an existing Azure <code class="literal">MachineSet</code> custom resource (CR) and edit it by running the following command:
							</p><pre class="programlisting language-terminal">$ oc edit machineset &lt;machine-set-name&gt;</pre><p class="simpara">
								where <code class="literal">&lt;machine-set-name&gt;</code> is the machine set that you want to provision machines with ultra disks.
							</p></li><li class="listitem"><p class="simpara">
								Add the following lines in the positions indicated:
							</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
spec:
  template:
    spec:
      metadata:
        labels:
          disk: ultrassd <span id="CO18-1"><!--Empty--></span><span class="callout">1</span>
      providerSpec:
        value:
          ultraSSDCapability: Enabled <span id="CO18-2"><!--Empty--></span><span class="callout">2</span>
          dataDisks: <span id="CO18-3"><!--Empty--></span><span class="callout">3</span>
          - nameSuffix: ultrassd
            lun: 0
            diskSizeGB: 4
            deletionPolicy: Delete
            cachingType: None
            managedDisk:
              storageAccountType: UltraSSD_LRS
          userDataSecret:
            name: &lt;role&gt;-user-data-x5 <span id="CO18-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO18-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify a label to use to select a node that is created by this machine set. This procedure uses <code class="literal">disk.ultrassd</code> for this value.
									</div></dd><dt><a href="#CO18-2"><span class="callout">2</span></a> <a href="#CO18-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										These lines enable the use of ultra disks. For <code class="literal">dataDisks</code>, include the entire stanza.
									</div></dd><dt><a href="#CO18-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										Specify the user data secret created earlier. Replace <code class="literal">&lt;role&gt;</code> with <code class="literal">worker</code>.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create a machine set using the updated configuration by running the following command:
							</p><pre class="programlisting language-terminal">$ oc create -f &lt;machine-set-name&gt;.yaml</pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Validate that the machines are created by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get machines</pre><p class="simpara">
								The machines should be in the <code class="literal">Running</code> state.
							</p></li><li class="listitem"><p class="simpara">
								For a machine that is running and has a node attached, validate the partition by running the following command:
							</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node-name&gt; -- chroot /host lsblk</pre><p class="simpara">
								In this command, <code class="literal">oc debug node/&lt;node-name&gt;</code> starts a debugging shell on the node <code class="literal">&lt;node-name&gt;</code> and passes a command with <code class="literal">--</code>. The passed command <code class="literal">chroot /host</code> provides access to the underlying host OS binaries, and <code class="literal">lsblk</code> shows the block devices that are attached to the host OS machine.
							</p></li></ol></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								To use an ultra disk from within a pod, create a workload that uses the mount point. Create a YAML file similar to the following example:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: ssd-benchmark1
spec:
  containers:
  - name: ssd-benchmark1
    image: nginx
    ports:
      - containerPort: 80
        name: "http-server"
    volumeMounts:
    - name: lun0p1
      mountPath: "/tmp"
  volumes:
    - name: lun0p1
      hostPath:
        path: /var/lib/lun0p1
        type: DirectoryOrCreate
  nodeSelector:
    disktype: ultrassd</pre></li></ul></div></section><section class="section" id="machineset-troubleshooting-azure-ultra-disk_creating-machineset-azure"><div class="titlepage"><div><div><h4 class="title">2.3.7.2. Troubleshooting resources for machine sets that enable ultra disks</h4></div></div></div><p>
						Use the information in this section to understand and recover from issues you might encounter.
					</p><section class="section" id="ts-mapi-attach-misconfigure_creating-machineset-azure"><div class="titlepage"><div><div><h5 class="title">2.3.7.2.1. Incorrect ultra disk configuration</h5></div></div></div><p>
							If an incorrect configuration of the <code class="literal">ultraSSDCapability</code> parameter is specified in the machine set, the machine provisioning fails.
						</p><p>
							For example, if the <code class="literal">ultraSSDCapability</code> parameter is set to <code class="literal">Disabled</code>, but an ultra disk is specified in the <code class="literal">dataDisks</code> parameter, the following error message appears:
						</p><pre class="programlisting language-terminal">StorageAccountType UltraSSD_LRS can be used only when additionalCapabilities.ultraSSDEnabled is set.</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									To resolve this issue, verify that your machine set configuration is correct.
								</li></ul></div></section><section class="section" id="ts-mapi-attach-unsupported_creating-machineset-azure"><div class="titlepage"><div><div><h5 class="title">2.3.7.2.2. Unsupported disk parameters</h5></div></div></div><p>
							If a region, availability zone, or instance size that is not compatible with ultra disks is specified in the machine set, the machine provisioning fails. Check the logs for the following error message:
						</p><pre class="programlisting language-terminal">failed to create vm &lt;machine_name&gt;: failure sending request for machine &lt;machine_name&gt;: cannot create vm: compute.VirtualMachinesClient#CreateOrUpdate: Failure sending request: StatusCode=400 -- Original Error: Code="BadRequest" Message="Storage Account type 'UltraSSD_LRS' is not supported &lt;more_information_about_why&gt;."</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									To resolve this issue, verify that you are using this feature in a supported environment and that your machine set configuration is correct.
								</li></ul></div></section><section class="section" id="ts-mapi-delete_creating-machineset-azure"><div class="titlepage"><div><div><h5 class="title">2.3.7.2.3. Unable to delete disks</h5></div></div></div><p>
							If the deletion of ultra disks as data disks is not working as expected, the machines are deleted and the data disks are orphaned. You must delete the orphaned disks manually if desired.
						</p></section></section></section><section class="section" id="machineset-enabling-customer-managed-encryption-azure_creating-machineset-azure"><div class="titlepage"><div><div><h3 class="title">2.3.8. Enabling customer-managed encryption keys for a machine set</h3></div></div></div><p>
					You can supply an encryption key to Azure to encrypt data on managed disks at rest. You can enable server-side encryption with customer-managed keys by using the Machine API.
				</p><p>
					An Azure Key Vault, a disk encryption set, and an encryption key are required to use a customer-managed key. The disk encryption set must be in a resource group where the Cloud Credential Operator (CCO) has granted permissions. If not, an additional reader role is required to be granted on the disk encryption set.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://docs.microsoft.com/en-us/azure/aks/azure-disk-customer-managed-keys#create-an-azure-key-vault-instance">Create an Azure Key Vault instance</a>.
						</li><li class="listitem">
							<a class="link" href="https://docs.microsoft.com/en-us/azure/aks/azure-disk-customer-managed-keys#create-an-instance-of-a-diskencryptionset">Create an instance of a disk encryption set</a>.
						</li><li class="listitem">
							<a class="link" href="https://docs.microsoft.com/en-us/azure/aks/azure-disk-customer-managed-keys#grant-the-diskencryptionset-access-to-key-vault">Grant the disk encryption set access to key vault</a>.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Configure the disk encryption set under the <code class="literal">providerSpec</code> field in your machine set YAML file. For example:
						</p><pre class="programlisting language-yaml">providerSpec:
  value:
    osDisk:
      diskSizeGB: 128
      managedDisk:
        diskEncryptionSet:
          id: /subscriptions/&lt;subscription_id&gt;/resourceGroups/&lt;resource_group_name&gt;/providers/Microsoft.Compute/diskEncryptionSets/&lt;disk_encryption_set_name&gt;
        storageAccountType: Premium_LRS</pre></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://docs.microsoft.com/en-us/azure/virtual-machines/disk-encryption#customer-managed-keys">Azure documentation about customer-managed keys</a>
						</li></ul></div></section><section class="section" id="machineset-azure-accelerated-networking_creating-machineset-azure"><div class="titlepage"><div><div><h3 class="title">2.3.9. Accelerated Networking for Microsoft Azure VMs</h3></div></div></div><p>
					Accelerated Networking uses single root I/O virtualization (SR-IOV) to provide Microsoft Azure VMs with a more direct path to the switch. This enhances network performance. This feature can be enabled during or after installation.
				</p><section class="section" id="machineset-azure-accelerated-networking-limits_creating-machineset-azure"><div class="titlepage"><div><div><h4 class="title">2.3.9.1. Limitations</h4></div></div></div><p>
						Consider the following limitations when deciding whether to use Accelerated Networking:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Accelerated Networking is only supported on clusters where the Machine API is operational.
							</li><li class="listitem"><p class="simpara">

							</p><p class="simpara">
								Although the minimum requirement for an Azure worker node is two vCPUs, Accelerated Networking requires an Azure VM size that includes at least four vCPUs. To satisfy this requirement, you can change the value of <code class="literal">vmSize</code> in your machine set. For information about Azure VM sizes, see <a class="link" href="https://docs.microsoft.com/en-us/azure/virtual-machines/sizes">Microsoft Azure documentation</a>.
							</p></li></ul></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								When this feature is enabled on an existing Azure cluster, only newly provisioned nodes are affected. Currently running nodes are not reconciled. To enable the feature on all nodes, you must replace each existing machine. This can be done for each machine individually, or by scaling the replicas down to zero, and then scaling back up to your desired number of replicas.
							</li></ul></div></section></section><section class="section" id="nvidia-gpu-aws-adding-a-gpu-node_creating-machineset-azure"><div class="titlepage"><div><div><h3 class="title">2.3.10. Adding a GPU node to an existing OpenShift Container Platform cluster</h3></div></div></div><p>
					You can copy and modify a default compute machine set configuration to create a GPU-enabled machine set and machines for the Azure cloud provider.
				</p><p>
					The following table lists the validated instance types:
				</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140311148130752" scope="col">vmSize</th><th align="left" valign="top" id="idm140311148129664" scope="col">NVIDIA GPU accelerator</th><th align="left" valign="top" id="idm140311148128576" scope="col">Maximum number of GPUs</th><th align="left" valign="top" id="idm140311148127488" scope="col">Architecture</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140311148130752"> <p>
									<code class="literal">Standard_NC24s_v3</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140311148129664"> <p>
									V100
								</p>
								 </td><td align="left" valign="top" headers="idm140311148128576"> <p>
									4
								</p>
								 </td><td align="left" valign="top" headers="idm140311148127488"> <p>
									x86
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140311148130752"> <p>
									<code class="literal">Standard_NC4as_T4_v3</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140311148129664"> <p>
									T4
								</p>
								 </td><td align="left" valign="top" headers="idm140311148128576"> <p>
									1
								</p>
								 </td><td align="left" valign="top" headers="idm140311148127488"> <p>
									x86
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140311148130752"> <p>
									<code class="literal">ND A100 v4</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140311148129664"> <p>
									A100
								</p>
								 </td><td align="left" valign="top" headers="idm140311148128576"> <p>
									8
								</p>
								 </td><td align="left" valign="top" headers="idm140311148127488"> <p>
									x86
								</p>
								 </td></tr></tbody></table></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						By default, Azure subscriptions do not have a quota for the Azure instance types with GPU. Customers have to request a quota increase for the Azure instance families listed above.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the machines and machine sets that exist in the <code class="literal">openshift-machine-api</code> namespace by running the following command. Each compute machine set is associated with a different availability zone within the Azure region. The installer automatically load balances compute machines across availability zones.
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                              DESIRED   CURRENT   READY   AVAILABLE   AGE
myclustername-worker-centralus1   1         1         1       1           6h9m
myclustername-worker-centralus2   1         1         1       1           6h9m
myclustername-worker-centralus3   1         1         1       1           6h9m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Make a copy of one of the existing compute <code class="literal">MachineSet</code> definitions and output the result to a YAML file by running the following command. This will be the basis for the GPU-enabled compute machine set definition.
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api myclustername-worker-centralus1 -o yaml &gt; machineset-azure.yaml</pre></li><li class="listitem"><p class="simpara">
							View the content of the machineset:
						</p><pre class="programlisting language-terminal">$ cat machineset-azure.yaml</pre><div class="formalpara"><p class="title"><strong>Example <code class="literal">machineset-azure.yaml</code> file</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  annotations:
    machine.openshift.io/GPU: "0"
    machine.openshift.io/memoryMb: "16384"
    machine.openshift.io/vCPU: "4"
  creationTimestamp: "2023-02-06T14:08:19Z"
  generation: 1
  labels:
    machine.openshift.io/cluster-api-cluster: myclustername
    machine.openshift.io/cluster-api-machine-role: worker
    machine.openshift.io/cluster-api-machine-type: worker
  name: myclustername-worker-centralus1
  namespace: openshift-machine-api
  resourceVersion: "23601"
  uid: acd56e0c-7612-473a-ae37-8704f34b80de
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: myclustername
      machine.openshift.io/cluster-api-machineset: myclustername-worker-centralus1
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: myclustername
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: myclustername-worker-centralus1
    spec:
      lifecycleHooks: {}
      metadata: {}
      providerSpec:
        value:
          acceleratedNetworking: true
          apiVersion: machine.openshift.io/v1beta1
          credentialsSecret:
            name: azure-cloud-credentials
            namespace: openshift-machine-api
          diagnostics: {}
          image:
            offer: ""
            publisher: ""
            resourceID: /resourceGroups/myclustername-rg/providers/Microsoft.Compute/galleries/gallery_myclustername_n6n4r/images/myclustername-gen2/versions/latest
            sku: ""
            version: ""
          kind: AzureMachineProviderSpec
          location: centralus
          managedIdentity: myclustername-identity
          metadata:
            creationTimestamp: null
          networkResourceGroup: myclustername-rg
          osDisk:
            diskSettings: {}
            diskSizeGB: 128
            managedDisk:
              storageAccountType: Premium_LRS
            osType: Linux
          publicIP: false
          publicLoadBalancer: myclustername
          resourceGroup: myclustername-rg
          spotVMOptions: {}
          subnet: myclustername-worker-subnet
          userDataSecret:
            name: worker-user-data
          vmSize: Standard_D4s_v3
          vnet: myclustername-vnet
          zone: "1"
status:
  availableReplicas: 1
  fullyLabeledReplicas: 1
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Make a copy of the <code class="literal">machineset-azure.yaml</code> file by running the following command:
						</p><pre class="programlisting language-terminal">$ cp machineset-azure.yaml machineset-azure-gpu.yaml</pre></li><li class="listitem"><p class="simpara">
							Update the following fields in <code class="literal">machineset-azure-gpu.yaml</code>:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Change <code class="literal">.metadata.name</code> to a name containing <code class="literal">gpu</code>.
								</li><li class="listitem">
									Change <code class="literal">.spec.selector.matchLabels["machine.openshift.io/cluster-api-machineset"]</code> to match the new .metadata.name.
								</li><li class="listitem">
									Change <code class="literal">.spec.template.metadata.labels["machine.openshift.io/cluster-api-machineset"]</code> to match the new <code class="literal">.metadata.name</code>.
								</li><li class="listitem"><p class="simpara">
									Change <code class="literal">.spec.template.spec.providerSpec.value.vmSize</code> to <code class="literal">Standard_NC4as_T4_v3</code>.
								</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">machineset-azure-gpu.yaml</code> file</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  annotations:
    machine.openshift.io/GPU: "1"
    machine.openshift.io/memoryMb: "28672"
    machine.openshift.io/vCPU: "4"
  creationTimestamp: "2023-02-06T20:27:12Z"
  generation: 1
  labels:
    machine.openshift.io/cluster-api-cluster: myclustername
    machine.openshift.io/cluster-api-machine-role: worker
    machine.openshift.io/cluster-api-machine-type: worker
  name: myclustername-nc4ast4-gpu-worker-centralus1
  namespace: openshift-machine-api
  resourceVersion: "166285"
  uid: 4eedce7f-6a57-4abe-b529-031140f02ffa
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: myclustername
      machine.openshift.io/cluster-api-machineset: myclustername-nc4ast4-gpu-worker-centralus1
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: myclustername
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: myclustername-nc4ast4-gpu-worker-centralus1
    spec:
      lifecycleHooks: {}
      metadata: {}
      providerSpec:
        value:
          acceleratedNetworking: true
          apiVersion: machine.openshift.io/v1beta1
          credentialsSecret:
            name: azure-cloud-credentials
            namespace: openshift-machine-api
          diagnostics: {}
          image:
            offer: ""
            publisher: ""
            resourceID: /resourceGroups/myclustername-rg/providers/Microsoft.Compute/galleries/gallery_myclustername_n6n4r/images/myclustername-gen2/versions/latest
            sku: ""
            version: ""
          kind: AzureMachineProviderSpec
          location: centralus
          managedIdentity: myclustername-identity
          metadata:
            creationTimestamp: null
          networkResourceGroup: myclustername-rg
          osDisk:
            diskSettings: {}
            diskSizeGB: 128
            managedDisk:
              storageAccountType: Premium_LRS
            osType: Linux
          publicIP: false
          publicLoadBalancer: myclustername
          resourceGroup: myclustername-rg
          spotVMOptions: {}
          subnet: myclustername-worker-subnet
          userDataSecret:
            name: worker-user-data
          vmSize: Standard_NC4as_T4_v3
          vnet: myclustername-vnet
          zone: "1"
status:
  availableReplicas: 1
  fullyLabeledReplicas: 1
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1</pre>

									</p></div></li></ul></div></li><li class="listitem"><p class="simpara">
							To verify your changes, perform a <code class="literal">diff</code> of the original compute definition and the new GPU-enabled node definition by running the following command:
						</p><pre class="programlisting language-terminal">$ diff machineset-azure.yaml machineset-azure-gpu.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">14c14
&lt;   name: myclustername-worker-centralus1
---
&gt;   name: myclustername-nc4ast4-gpu-worker-centralus1
23c23
&lt;       machine.openshift.io/cluster-api-machineset: myclustername-worker-centralus1
---
&gt;       machine.openshift.io/cluster-api-machineset: myclustername-nc4ast4-gpu-worker-centralus1
30c30
&lt;         machine.openshift.io/cluster-api-machineset: myclustername-worker-centralus1
---
&gt;         machine.openshift.io/cluster-api-machineset: myclustername-nc4ast4-gpu-worker-centralus1
67c67
&lt;           vmSize: Standard_D4s_v3
---
&gt;           vmSize: Standard_NC4as_T4_v3</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create the GPU-enabled compute machine set from the definition file by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f machineset-azure-gpu.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">machineset.machine.openshift.io/myclustername-nc4ast4-gpu-worker-centralus1 created</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							View the machines and machine sets that exist in the <code class="literal">openshift-machine-api</code> namespace by running the following command. Each compute machine set is associated with a different availability zone within the Azure region. The installer automatically load balances compute machines across availability zones.
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                               DESIRED   CURRENT   READY   AVAILABLE   AGE
clustername-n6n4r-nc4ast4-gpu-worker-centralus1    1         1         1       1           122m
clustername-n6n4r-worker-centralus1                1         1         1       1           8h
clustername-n6n4r-worker-centralus2                1         1         1       1           8h
clustername-n6n4r-worker-centralus3                1         1         1       1           8h</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							View the machines that exist in the <code class="literal">openshift-machine-api</code> namespace by running the following command. You can only configure one compute machine per set, although you can scale a compute machine set to add a node in a particular region and zone.
						</p><pre class="programlisting language-terminal">$ oc get machines -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                PHASE     TYPE                   REGION      ZONE   AGE
myclustername-master-0                              Running   Standard_D8s_v3        centralus   2      6h40m
myclustername-master-1                              Running   Standard_D8s_v3        centralus   1      6h40m
myclustername-master-2                              Running   Standard_D8s_v3        centralus   3      6h40m
myclustername-nc4ast4-gpu-worker-centralus1-w9bqn   Running      centralus   1      21m
myclustername-worker-centralus1-rbh6b               Running   Standard_D4s_v3        centralus   1      6h38m
myclustername-worker-centralus2-dbz7w               Running   Standard_D4s_v3        centralus   2      6h38m
myclustername-worker-centralus3-p9b8c               Running   Standard_D4s_v3        centralus   3      6h38m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							View the existing nodes, machines, and machine sets by running the following command. Note that each node is an instance of a machine definition with a specific Azure region and OpenShift Container Platform role.
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                STATUS   ROLES                  AGE     VERSION
myclustername-master-0                              Ready    control-plane,master   6h39m   v1.26.0
myclustername-master-1                              Ready    control-plane,master   6h41m   v1.26.0
myclustername-master-2                              Ready    control-plane,master   6h39m   v1.26.0
myclustername-nc4ast4-gpu-worker-centralus1-w9bqn   Ready    worker                 14m     v1.26.0
myclustername-worker-centralus1-rbh6b               Ready    worker                 6h29m   v1.26.0
myclustername-worker-centralus2-dbz7w               Ready    worker                 6h29m   v1.26.0
myclustername-worker-centralus3-p9b8c               Ready    worker                 6h31m   v1.26.0</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							View the list of compute machine sets:
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                   DESIRED   CURRENT   READY   AVAILABLE   AGE
myclustername-worker-centralus1        1         1         1       1           8h
myclustername-worker-centralus2        1         1         1       1           8h
myclustername-worker-centralus3        1         1         1       1           8h</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create the GPU-enabled compute machine set from the definition file by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f machineset-azure-gpu.yaml</pre></li><li class="listitem"><p class="simpara">
							View the list of compute machine sets:
						</p><pre class="programlisting language-terminal">oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                          DESIRED   CURRENT   READY   AVAILABLE   AGE
myclustername-nc4ast4-gpu-worker-centralus1   1         1         1       1           121m
myclustername-worker-centralus1               1         1         1       1           8h
myclustername-worker-centralus2               1         1         1       1           8h
myclustername-worker-centralus3               1         1         1       1           8h</pre>

							</p></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the machine set you created by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api | grep gpu</pre><p class="simpara">
							The MachineSet replica count is set to <code class="literal">1</code> so a new <code class="literal">Machine</code> object is created automatically.
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">myclustername-nc4ast4-gpu-worker-centralus1   1         1         1       1           121m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							View the <code class="literal">Machine</code> object that the machine set created by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api get machines | grep gpu</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">myclustername-nc4ast4-gpu-worker-centralus1-w9bqn   Running   Standard_NC4as_T4_v3   centralus   1      21m</pre>

							</p></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						There is no need to specify a namespace for the node. The node definition is cluster scoped.
					</p></div></div></section><section class="section" id="nvidia-gpu-aws-deploying-the-node-feature-discovery-operator_creating-machineset-azure"><div class="titlepage"><div><div><h3 class="title">2.3.11. Deploying the Node Feature Discovery Operator</h3></div></div></div><p>
					After the GPU-enabled node is created, you need to discover the GPU-enabled node so it can be scheduled. To do this, install the Node Feature Discovery (NFD) Operator. The NFD Operator identifies hardware device features in nodes. It solves the general problem of identifying and cataloging hardware resources in the infrastructure nodes so they can be made available to OpenShift Container Platform.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Install the Node Feature Discovery Operator from <span class="strong strong"><strong>OperatorHub</strong></span> in the OpenShift Container Platform console.
						</li><li class="listitem">
							After installing the NFD Operator into <span class="strong strong"><strong>OperatorHub</strong></span>, select <span class="strong strong"><strong>Node Feature Discovery</strong></span> from the installed Operators list and select <span class="strong strong"><strong>Create instance</strong></span>. This installs the <code class="literal">nfd-master</code> and <code class="literal">nfd-worker</code> pods, one <code class="literal">nfd-worker</code> pod for each compute node, in the <code class="literal">openshift-nfd</code> namespace.
						</li><li class="listitem"><p class="simpara">
							Verify that the Operator is installed and running by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-nfd</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                       READY    STATUS     RESTARTS   AGE

nfd-controller-manager-8646fcbb65-x5qgk    2/2      Running 7  (8h ago)   1d</pre>

							</p></div></li><li class="listitem">
							Browse to the installed Oerator in the console and select <span class="strong strong"><strong>Create Node Feature Discovery</strong></span>.
						</li><li class="listitem">
							Select <span class="strong strong"><strong>Create</strong></span> to build a NFD custom resource. This creates NFD pods in the <code class="literal">openshift-nfd</code> namespace that poll the OpenShift Container Platform nodes for hardware resources and catalogue them.
						</li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							After a successful build, verify that a NFD pod is running on each nodes by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-nfd</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                       READY   STATUS      RESTARTS        AGE
nfd-controller-manager-8646fcbb65-x5qgk    2/2     Running     7 (8h ago)      12d
nfd-master-769656c4cb-w9vrv                1/1     Running     0               12d
nfd-worker-qjxb2                           1/1     Running     3 (3d14h ago)   12d
nfd-worker-xtz9b                           1/1     Running     5 (3d14h ago)   12d</pre>

							</p></div><p class="simpara">
							The NFD Operator uses vendor PCI IDs to identify hardware in a node. NVIDIA uses the PCI ID <code class="literal">10de</code>.
						</p></li><li class="listitem"><p class="simpara">
							View the NVIDIA GPU discovered by the NFD Operator by running the following command:
						</p><pre class="programlisting language-terminal">$ oc describe node ip-10-0-132-138.us-east-2.compute.internal | egrep 'Roles|pci'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Roles: worker

feature.node.kubernetes.io/pci-1013.present=true

feature.node.kubernetes.io/pci-10de.present=true

feature.node.kubernetes.io/pci-1d0f.present=true</pre>

							</p></div><p class="simpara">
							<code class="literal">10de</code> appears in the node feature list for the GPU-enabled node. This mean the NFD Operator correctly identified the node from the GPU-enabled MachineSet.
						</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#machineset-azure-enabling-accelerated-networking-new-install_installing-azure-customizations">Enabling Accelerated Networking during installation</a>
						</li></ul></div><section class="section" id="machineset-azure-enabling-accelerated-networking-existing_creating-machineset-azure"><div class="titlepage"><div><div><h4 class="title">2.3.11.1. Enabling Accelerated Networking on an existing Microsoft Azure cluster</h4></div></div></div><p>
						You can enable Accelerated Networking on Azure by adding <code class="literal">acceleratedNetworking</code> to your machine set YAML file.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Have an existing Microsoft Azure cluster where the Machine API is operational.
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Add the following to the <code class="literal">providerSpec</code> field:
							</p><pre class="programlisting language-yaml">providerSpec:
  value:
    acceleratedNetworking: true <span id="CO19-1"><!--Empty--></span><span class="callout">1</span>
    vmSize: &lt;azure-vm-size&gt; <span id="CO19-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO19-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										This line enables Accelerated Networking.
									</div></dd><dt><a href="#CO19-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specify an Azure VM size that includes at least four vCPUs. For information about VM sizes, see <a class="link" href="https://docs.microsoft.com/en-us/azure/virtual-machines/sizes">Microsoft Azure documentation</a>.
									</div></dd></dl></div></li></ul></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								To enable the feature on currently running nodes, you must replace each existing machine. This can be done for each machine individually, or by scaling the replicas down to zero, and then scaling back up to your desired number of replicas.
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								On the Microsoft Azure portal, review the <span class="strong strong"><strong>Networking</strong></span> settings page for a machine provisioned by the machine set, and verify that the <code class="literal">Accelerated networking</code> field is set to <code class="literal">Enabled</code>.
							</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#manually-scaling-machineset">Manually scaling a compute machine set</a>
							</li></ul></div></section></section></section><section class="section" id="creating-machineset-azure-stack-hub"><div class="titlepage"><div><div><h2 class="title">2.4. Creating a compute machine set on Azure Stack Hub</h2></div></div></div><p>
				You can create a different compute machine set to serve a specific purpose in your OpenShift Container Platform cluster on Microsoft Azure Stack Hub. For example, you might create infrastructure machine sets and related machines so that you can move supporting workloads to the new machines.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.
				</p><p>
					Clusters with the infrastructure platform type <code class="literal">none</code> cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.
				</p><p>
					To view the platform type for your cluster, run the following command:
				</p><pre class="programlisting language-terminal">$ oc get infrastructure cluster -o jsonpath='{.status.platform}'</pre></div></div><section class="section" id="machineset-yaml-azure-stack-hub_creating-machineset-azure-stack-hub"><div class="titlepage"><div><div><h3 class="title">2.4.1. Sample YAML for a compute machine set custom resource on Azure Stack Hub</h3></div></div></div><p>
					This sample YAML defines a compute machine set that runs in the <code class="literal">1</code> Microsoft Azure zone in a region and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/&lt;role&gt;: ""</code>.
				</p><p>
					In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;role&gt;</code> is the node label to add.
				</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO20-1"><!--Empty--></span><span class="callout">1</span>
    machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO20-2"><!--Empty--></span><span class="callout">2</span>
    machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO20-3"><!--Empty--></span><span class="callout">3</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;region&gt; <span id="CO20-4"><!--Empty--></span><span class="callout">4</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO20-5"><!--Empty--></span><span class="callout">5</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;region&gt; <span id="CO20-6"><!--Empty--></span><span class="callout">6</span>
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO20-7"><!--Empty--></span><span class="callout">7</span>
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO20-8"><!--Empty--></span><span class="callout">8</span>
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO20-9"><!--Empty--></span><span class="callout">9</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;region&gt; <span id="CO20-10"><!--Empty--></span><span class="callout">10</span>
    spec:
      metadata:
        creationTimestamp: null
        labels:
          node-role.kubernetes.io/&lt;role&gt;: "" <span id="CO20-11"><!--Empty--></span><span class="callout">11</span>
      providerSpec:
        value:
          apiVersion: machine.openshift.io/v1beta1
          availabilitySet: &lt;availability_set&gt; <span id="CO20-12"><!--Empty--></span><span class="callout">12</span>
          credentialsSecret:
            name: azure-cloud-credentials
            namespace: openshift-machine-api
          image:
            offer: ""
            publisher: ""
            resourceID: /resourceGroups/&lt;infrastructure_id&gt;-rg/providers/Microsoft.Compute/images/&lt;infrastructure_id&gt; <span id="CO20-13"><!--Empty--></span><span class="callout">13</span>
            sku: ""
            version: ""
          internalLoadBalancer: ""
          kind: AzureMachineProviderSpec
          location: &lt;region&gt; <span id="CO20-14"><!--Empty--></span><span class="callout">14</span>
          managedIdentity: &lt;infrastructure_id&gt;-identity <span id="CO20-15"><!--Empty--></span><span class="callout">15</span>
          metadata:
            creationTimestamp: null
          natRule: null
          networkResourceGroup: ""
          osDisk:
            diskSizeGB: 128
            managedDisk:
              storageAccountType: Premium_LRS
            osType: Linux
          publicIP: false
          publicLoadBalancer: ""
          resourceGroup: &lt;infrastructure_id&gt;-rg <span id="CO20-16"><!--Empty--></span><span class="callout">16</span>
          sshPrivateKey: ""
          sshPublicKey: ""
          subnet: &lt;infrastructure_id&gt;-&lt;role&gt;-subnet <span id="CO20-17"><!--Empty--></span><span class="callout">17</span> <span id="CO20-18"><!--Empty--></span><span class="callout">18</span>
          userDataSecret:
            name: worker-user-data <span id="CO20-19"><!--Empty--></span><span class="callout">19</span>
          vmSize: Standard_DS4_v2
          vnet: &lt;infrastructure_id&gt;-vnet <span id="CO20-20"><!--Empty--></span><span class="callout">20</span>
          zone: "1" <span id="CO20-21"><!--Empty--></span><span class="callout">21</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO20-1"><span class="callout">1</span></a> <a href="#CO20-5"><span class="callout">5</span></a> <a href="#CO20-7"><span class="callout">7</span></a> <a href="#CO20-13"><span class="callout">13</span></a> <a href="#CO20-15"><span class="callout">15</span></a> <a href="#CO20-16"><span class="callout">16</span></a> <a href="#CO20-17"><span class="callout">17</span></a> <a href="#CO20-20"><span class="callout">20</span></a> </dt><dd><div class="para">
							Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
						</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre><p>
							You can obtain the subnet by running the following command:
						</p><pre class="programlisting language-terminal">$  oc -n openshift-machine-api \
    -o jsonpath='{.spec.template.spec.providerSpec.value.subnet}{"\n"}' \
    get machineset/&lt;infrastructure_id&gt;-worker-centralus1</pre><p>
							You can obtain the vnet by running the following command:
						</p><pre class="programlisting language-terminal">$  oc -n openshift-machine-api \
    -o jsonpath='{.spec.template.spec.providerSpec.value.vnet}{"\n"}' \
    get machineset/&lt;infrastructure_id&gt;-worker-centralus1</pre></dd><dt><a href="#CO20-2"><span class="callout">2</span></a> <a href="#CO20-3"><span class="callout">3</span></a> <a href="#CO20-8"><span class="callout">8</span></a> <a href="#CO20-9"><span class="callout">9</span></a> <a href="#CO20-11"><span class="callout">11</span></a> <a href="#CO20-18"><span class="callout">18</span></a> <a href="#CO20-19"><span class="callout">19</span></a> </dt><dd><div class="para">
							Specify the node label to add.
						</div></dd><dt><a href="#CO20-4"><span class="callout">4</span></a> <a href="#CO20-6"><span class="callout">6</span></a> <a href="#CO20-10"><span class="callout">10</span></a> </dt><dd><div class="para">
							Specify the infrastructure ID, node label, and region.
						</div></dd><dt><a href="#CO20-14"><span class="callout">14</span></a> </dt><dd><div class="para">
							Specify the region to place machines on.
						</div></dd><dt><a href="#CO20-21"><span class="callout">21</span></a> </dt><dd><div class="para">
							Specify the zone within your region to place machines on. Be sure that your region supports the zone that you specify.
						</div></dd><dt><a href="#CO20-12"><span class="callout">12</span></a> </dt><dd><div class="para">
							Specify the availability set for the cluster.
						</div></dd></dl></div></section><section class="section" id="machineset-creating_creating-machineset-azure-stack-hub"><div class="titlepage"><div><div><h3 class="title">2.4.2. Creating a compute machine set</h3></div></div></div><p>
					In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Deploy an OpenShift Container Platform cluster.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
						</li><li class="listitem">
							Create an availability set in which to deploy Azure Stack Hub compute machines.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <code class="literal">&lt;file_name&gt;.yaml</code>.
						</p><p class="simpara">
							Ensure that you set the <code class="literal">&lt;availabilitySet&gt;</code>, <code class="literal">&lt;clusterID&gt;</code>, and <code class="literal">&lt;role&gt;</code> parameter values.
						</p></li><li class="listitem"><p class="simpara">
							Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									To list the compute machine sets in your cluster, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									To view values of a specific compute machine set custom resource (CR), run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machineset &lt;machineset_name&gt; \
  -n openshift-machine-api -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO21-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO21-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
    spec:
      providerSpec: <span id="CO21-3"><!--Empty--></span><span class="callout">3</span>
        ...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO21-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The cluster infrastructure ID.
										</div></dd><dt><a href="#CO21-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											A default node label.
										</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												For clusters that have user-provisioned infrastructure, a compute machine set can only create <code class="literal">worker</code> and <code class="literal">infra</code> type machines.
											</p></div></div></dd><dt><a href="#CO21-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											The values in the <code class="literal">&lt;providerSpec&gt;</code> section of the compute machine set CR are platform-specific. For more information about <code class="literal">&lt;providerSpec&gt;</code> parameters in the CR, see the sample compute machine set CR configuration for your provider.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">MachineSet</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							View the list of compute machine sets by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-infra-us-east-1a    1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

							</p></div><p class="simpara">
							When the new compute machine set is available, the <code class="literal">DESIRED</code> and <code class="literal">CURRENT</code> values match. If the compute machine set is not available, wait a few minutes and run the command again.
						</p></li></ul></div></section><section class="section" id="machineset-azure-boot-diagnostics_creating-machineset-azure-stack-hub"><div class="titlepage"><div><div><h3 class="title">2.4.3. Enabling Azure boot diagnostics</h3></div></div></div><p>
					You can enable boot diagnostics on Azure machines that your machine set creates.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Have an existing Microsoft Azure Stack Hub cluster.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Add the <code class="literal">diagnostics</code> configuration that is applicable to your storage type to the <code class="literal">providerSpec</code> field in your machine set YAML file:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
									For an Azure Managed storage account:
								</p><pre class="programlisting language-yaml">providerSpec:
  diagnostics:
    boot:
      storageAccountType: AzureManaged <span id="CO22-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO22-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specifies an Azure Managed storage account.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									For an Azure Unmanaged storage account:
								</p><pre class="programlisting language-yaml">providerSpec:
  diagnostics:
    boot:
      storageAccountType: CustomerManaged <span id="CO23-1"><!--Empty--></span><span class="callout">1</span>
      customerManaged:
        storageAccountURI: https://&lt;storage-account&gt;.blob.core.windows.net <span id="CO23-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO23-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specifies an Azure Unmanaged storage account.
										</div></dd><dt><a href="#CO23-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Replace <code class="literal">&lt;storage-account&gt;</code> with the name of your storage account.
										</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Only the Azure Blob Storage data service is supported.
									</p></div></div></li></ul></div></li></ul></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							On the Microsoft Azure portal, review the <span class="strong strong"><strong>Boot diagnostics</strong></span> page for a machine deployed by the machine set, and verify that you can see the serial logs for the machine.
						</li></ul></div></section><section class="section" id="machineset-enabling-customer-managed-encryption-azure_creating-machineset-azure-stack-hub"><div class="titlepage"><div><div><h3 class="title">2.4.4. Enabling customer-managed encryption keys for a machine set</h3></div></div></div><p>
					You can supply an encryption key to Azure to encrypt data on managed disks at rest. You can enable server-side encryption with customer-managed keys by using the Machine API.
				</p><p>
					An Azure Key Vault, a disk encryption set, and an encryption key are required to use a customer-managed key. The disk encryption set must be in a resource group where the Cloud Credential Operator (CCO) has granted permissions. If not, an additional reader role is required to be granted on the disk encryption set.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://docs.microsoft.com/en-us/azure/aks/azure-disk-customer-managed-keys#create-an-azure-key-vault-instance">Create an Azure Key Vault instance</a>.
						</li><li class="listitem">
							<a class="link" href="https://docs.microsoft.com/en-us/azure/aks/azure-disk-customer-managed-keys#create-an-instance-of-a-diskencryptionset">Create an instance of a disk encryption set</a>.
						</li><li class="listitem">
							<a class="link" href="https://docs.microsoft.com/en-us/azure/aks/azure-disk-customer-managed-keys#grant-the-diskencryptionset-access-to-key-vault">Grant the disk encryption set access to key vault</a>.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Configure the disk encryption set under the <code class="literal">providerSpec</code> field in your machine set YAML file. For example:
						</p><pre class="programlisting language-yaml">providerSpec:
  value:
    osDisk:
      diskSizeGB: 128
      managedDisk:
        diskEncryptionSet:
          id: /subscriptions/&lt;subscription_id&gt;/resourceGroups/&lt;resource_group_name&gt;/providers/Microsoft.Compute/diskEncryptionSets/&lt;disk_encryption_set_name&gt;
        storageAccountType: Premium_LRS</pre></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://docs.microsoft.com/en-us/azure/virtual-machines/disk-encryption#customer-managed-keys">Azure documentation about customer-managed keys</a>
						</li></ul></div></section></section><section class="section" id="creating-machineset-gcp"><div class="titlepage"><div><div><h2 class="title">2.5. Creating a compute machine set on GCP</h2></div></div></div><p>
				You can create a different compute machine set to serve a specific purpose in your OpenShift Container Platform cluster on Google Cloud Platform (GCP). For example, you might create infrastructure machine sets and related machines so that you can move supporting workloads to the new machines.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.
				</p><p>
					Clusters with the infrastructure platform type <code class="literal">none</code> cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.
				</p><p>
					To view the platform type for your cluster, run the following command:
				</p><pre class="programlisting language-terminal">$ oc get infrastructure cluster -o jsonpath='{.status.platform}'</pre></div></div><section class="section" id="machineset-yaml-gcp_creating-machineset-gcp"><div class="titlepage"><div><div><h3 class="title">2.5.1. Sample YAML for a compute machine set custom resource on GCP</h3></div></div></div><p>
					This sample YAML defines a compute machine set that runs in Google Cloud Platform (GCP) and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/&lt;role&gt;: ""</code>, where <code class="literal">&lt;role&gt;</code> is the node label to add.
				</p><h5 id="cpmso-yaml-provider-spec-gcp-oc_creating-machineset-gcp">Values obtained by using the OpenShift CLI</h5><p>
					In the following example, you can obtain some of the values for your cluster by using the OpenShift CLI.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Infrastructure ID</span></dt><dd><p class="simpara">
								The <code class="literal">&lt;infrastructure_id&gt;</code> string is the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><span class="term">Image path</span></dt><dd><p class="simpara">
								The <code class="literal">&lt;path_to_image&gt;</code> string is the path to the image that was used to create the disk. If you have the OpenShift CLI installed, you can obtain the path to the image by running the following command:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api \
  -o jsonpath='{.spec.template.spec.providerSpec.value.disks[0].image}{"\n"}' \
  get machineset/&lt;infrastructure_id&gt;-worker-a</pre></dd></dl></div><div class="formalpara"><p class="title"><strong>Sample GCP <code class="literal">MachineSet</code> values</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO24-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-w-a
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-w-a
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO24-2"><!--Empty--></span><span class="callout">2</span>
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-w-a
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/&lt;role&gt;: ""
      providerSpec:
        value:
          apiVersion: gcpprovider.openshift.io/v1beta1
          canIPForward: false
          credentialsSecret:
            name: gcp-cloud-credentials
          deletionProtection: false
          disks:
          - autoDelete: true
            boot: true
            image: &lt;path_to_image&gt; <span id="CO24-3"><!--Empty--></span><span class="callout">3</span>
            labels: null
            sizeGb: 128
            type: pd-ssd
          gcpMetadata: <span id="CO24-4"><!--Empty--></span><span class="callout">4</span>
          - key: &lt;custom_metadata_key&gt;
            value: &lt;custom_metadata_value&gt;
          kind: GCPMachineProviderSpec
          machineType: n1-standard-4
          metadata:
            creationTimestamp: null
          networkInterfaces:
          - network: &lt;infrastructure_id&gt;-network
            subnetwork: &lt;infrastructure_id&gt;-worker-subnet
          projectID: &lt;project_name&gt; <span id="CO24-5"><!--Empty--></span><span class="callout">5</span>
          region: us-central1
          serviceAccounts:
          - email: &lt;infrastructure_id&gt;-w@&lt;project_name&gt;.iam.gserviceaccount.com
            scopes:
            - https://www.googleapis.com/auth/cloud-platform
          tags:
            - &lt;infrastructure_id&gt;-worker
          userDataSecret:
            name: worker-user-data
          zone: us-central1-a</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO24-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							For <code class="literal">&lt;infrastructure_id&gt;</code>, specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster.
						</div></dd><dt><a href="#CO24-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							For <code class="literal">&lt;node&gt;</code>, specify the node label to add.
						</div></dd><dt><a href="#CO24-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Specify the path to the image that is used in current compute machine sets.
						</div><p>
							To use a GCP Marketplace image, specify the offer to use:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									OpenShift Container Platform: <code class="literal">https://www.googleapis.com/compute/v1/projects/redhat-marketplace-public/global/images/redhat-coreos-ocp-48-x86-64-202210040145</code>
								</li><li class="listitem">
									OpenShift Platform Plus: <code class="literal">https://www.googleapis.com/compute/v1/projects/redhat-marketplace-public/global/images/redhat-coreos-opp-48-x86-64-202206140145</code>
								</li><li class="listitem">
									OpenShift Kubernetes Engine: <code class="literal">https://www.googleapis.com/compute/v1/projects/redhat-marketplace-public/global/images/redhat-coreos-oke-48-x86-64-202206140145</code>
								</li></ul></div></dd><dt><a href="#CO24-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Optional: Specify custom metadata in the form of a <code class="literal">key:value</code> pair. For example use cases, see the GCP documentation for <a class="link" href="https://cloud.google.com/compute/docs/metadata/setting-custom-metadata">setting custom metadata</a>.
						</div></dd><dt><a href="#CO24-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							For <code class="literal">&lt;project_name&gt;</code>, specify the name of the GCP project that you use for your cluster.
						</div></dd></dl></div></section><section class="section" id="machineset-creating_creating-machineset-gcp"><div class="titlepage"><div><div><h3 class="title">2.5.2. Creating a compute machine set</h3></div></div></div><p>
					In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Deploy an OpenShift Container Platform cluster.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <code class="literal">&lt;file_name&gt;.yaml</code>.
						</p><p class="simpara">
							Ensure that you set the <code class="literal">&lt;clusterID&gt;</code> and <code class="literal">&lt;role&gt;</code> parameter values.
						</p></li><li class="listitem"><p class="simpara">
							Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									To list the compute machine sets in your cluster, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									To view values of a specific compute machine set custom resource (CR), run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machineset &lt;machineset_name&gt; \
  -n openshift-machine-api -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO25-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO25-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
    spec:
      providerSpec: <span id="CO25-3"><!--Empty--></span><span class="callout">3</span>
        ...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO25-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The cluster infrastructure ID.
										</div></dd><dt><a href="#CO25-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											A default node label.
										</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												For clusters that have user-provisioned infrastructure, a compute machine set can only create <code class="literal">worker</code> and <code class="literal">infra</code> type machines.
											</p></div></div></dd><dt><a href="#CO25-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											The values in the <code class="literal">&lt;providerSpec&gt;</code> section of the compute machine set CR are platform-specific. For more information about <code class="literal">&lt;providerSpec&gt;</code> parameters in the CR, see the sample compute machine set CR configuration for your provider.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">MachineSet</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							View the list of compute machine sets by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-infra-us-east-1a    1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

							</p></div><p class="simpara">
							When the new compute machine set is available, the <code class="literal">DESIRED</code> and <code class="literal">CURRENT</code> values match. If the compute machine set is not available, wait a few minutes and run the command again.
						</p></li></ul></div></section><section class="section" id="machineset-gcp-pd-disk-types_creating-machineset-gcp"><div class="titlepage"><div><div><h3 class="title">2.5.3. Configuring persistent disk types by using machine sets</h3></div></div></div><p>
					You can configure the type of persistent disk that a machine set deploys machines on by editing the machine set YAML file.
				</p><p>
					For more information about persistent disk types, compatibility, regional availability, and limitations, see the GCP Compute Engine documentation about <a class="link" href="https://cloud.google.com/compute/docs/disks#pdspecs">persistent disks</a>.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In a text editor, open the YAML file for an existing machine set or create a new one.
						</li><li class="listitem"><p class="simpara">
							Edit the following line under the <code class="literal">providerSpec</code> field:
						</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
...
spec:
  template:
    spec:
      providerSpec:
        value:
          disks:
            type: &lt;pd-disk-type&gt; <span id="CO26-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO26-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the disk persistent type. Valid values are <code class="literal">pd-ssd</code>, <code class="literal">pd-standard</code>, and <code class="literal">pd-balanced</code>. The default value is <code class="literal">pd-standard</code>.
								</div></dd></dl></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Using the Google Cloud console, review the details for a machine deployed by the machine set and verify that the <code class="literal">Type</code> field matches the configured disk type.
						</li></ul></div></section><section class="section" id="machineset-gcp-confidential-vm_creating-machineset-gcp"><div class="titlepage"><div><div><h3 class="title">2.5.4. Configuring Confidential VM by using machine sets</h3></div></div></div><p>
					By editing the machine set YAML file, you can configure the Confidential VM options that a machine set uses for machines that it deploys.
				</p><p>
					For more information about Confidential Compute features, functionality, and compatibility, see the GCP Compute Engine documentation about <a class="link" href="https://cloud.google.com/compute/confidential-vm/docs/about-cvm">Confidential VM</a>.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Confidential Computing is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
					</p><p>
						For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In a text editor, open the YAML file for an existing machine set or create a new one.
						</li><li class="listitem"><p class="simpara">
							Edit the following section under the <code class="literal">providerSpec</code> field:
						</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
...
spec:
  template:
    spec:
      providerSpec:
        value:
          confidentialCompute: Enabled <span id="CO27-1"><!--Empty--></span><span class="callout">1</span>
          onHostMaintenance: Terminate <span id="CO27-2"><!--Empty--></span><span class="callout">2</span>
          machineType: n2d-standard-8 <span id="CO27-3"><!--Empty--></span><span class="callout">3</span>
...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO27-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify whether Confidential VM is enabled. Valid values are <code class="literal">Disabled</code> or <code class="literal">Enabled</code>.
								</div></dd><dt><a href="#CO27-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the behavior of the VM during a host maintenance event, such as a hardware or software update. For a machine that uses Confidential VM, this value must be set to <code class="literal">Terminate</code>, which stops the VM. Confidential VM does not support live VM migration.
								</div></dd><dt><a href="#CO27-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify a machine type that supports Confidential VM. Confidential VM supports the N2D and C2D series of machine types.
								</div></dd></dl></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							On the Google Cloud console, review the details for a machine deployed by the machine set and verify that the Confidential VM options match the values that you configured.
						</li></ul></div></section><section class="section" id="machineset-non-guaranteed-instance_creating-machineset-gcp"><div class="titlepage"><div><div><h3 class="title">2.5.5. Machine sets that deploy machines as preemptible VM instances</h3></div></div></div><p>
					You can save on costs by creating a compute machine set running on GCP that deploys machines as non-guaranteed preemptible VM instances. Preemptible VM instances utilize excess Compute Engine capacity and are less expensive than normal instances. You can use preemptible VM instances for workloads that can tolerate interruptions, such as batch or stateless, horizontally scalable workloads.
				</p><p>
					GCP Compute Engine can terminate a preemptible VM instance at any time. Compute Engine sends a preemption notice to the user indicating that an interruption will occur in 30 seconds. OpenShift Container Platform begins to remove the workloads from the affected instances when Compute Engine issues the preemption notice. An ACPI G3 Mechanical Off signal is sent to the operating system after 30 seconds if the instance is not stopped. The preemptible VM instance is then transitioned to a <code class="literal">TERMINATED</code> state by Compute Engine.
				</p><p>
					Interruptions can occur when using preemptible VM instances for the following reasons:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							There is a system or maintenance event
						</li><li class="listitem">
							The supply of preemptible VM instances decreases
						</li><li class="listitem">
							The instance reaches the end of the allotted 24-hour period for preemptible VM instances
						</li></ul></div><p>
					When GCP terminates an instance, a termination handler running on the preemptible VM instance node deletes the machine resource. To satisfy the compute machine set <code class="literal">replicas</code> quantity, the compute machine set creates a machine that requests a preemptible VM instance.
				</p><section class="section" id="machineset-creating-non-guaranteed-instance_creating-machineset-gcp"><div class="titlepage"><div><div><h4 class="title">2.5.5.1. Creating preemptible VM instances by using compute machine sets</h4></div></div></div><p>
						You can launch a preemptible VM instance on GCP by adding <code class="literal">preemptible</code> to your compute machine set YAML file.
					</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Add the following line under the <code class="literal">providerSpec</code> field:
							</p><pre class="programlisting language-yaml">providerSpec:
  value:
    preemptible: true</pre><p class="simpara">
								If <code class="literal">preemptible</code> is set to <code class="literal">true</code>, the machine is labelled as an <code class="literal">interruptable-instance</code> after the instance is launched.
							</p></li></ul></div></section></section><section class="section" id="machineset-gcp-shielded-vms_creating-machineset-gcp"><div class="titlepage"><div><div><h3 class="title">2.5.6. Configuring Shielded VM options by using machine sets</h3></div></div></div><p>
					By editing the machine set YAML file, you can configure the Shielded VM options that a machine set uses for machines that it deploys.
				</p><p>
					For more information about Shielded VM features and functionality, see the GCP Compute Engine documentation about <a class="link" href="https://cloud.google.com/compute/shielded-vm/docs/shielded-vm">Shielded VM</a>.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In a text editor, open the YAML file for an existing machine set or create a new one.
						</li><li class="listitem"><p class="simpara">
							Edit the following section under the <code class="literal">providerSpec</code> field:
						</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
...
spec:
  template:
    spec:
      providerSpec:
        value:
          shieldedInstanceConfig: <span id="CO28-1"><!--Empty--></span><span class="callout">1</span>
            integrityMonitoring: Enabled <span id="CO28-2"><!--Empty--></span><span class="callout">2</span>
            secureBoot: Disabled <span id="CO28-3"><!--Empty--></span><span class="callout">3</span>
            virtualizedTrustedPlatformModule: Enabled <span id="CO28-4"><!--Empty--></span><span class="callout">4</span>
...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO28-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									In this section, specify any Shielded VM options that you want.
								</div></dd><dt><a href="#CO28-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify whether UEFI Secure Boot is enabled. Valid values are <code class="literal">Disabled</code> or <code class="literal">Enabled</code>.
								</div></dd><dt><a href="#CO28-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify whether integrity monitoring is enabled. Valid values are <code class="literal">Disabled</code> or <code class="literal">Enabled</code>.
								</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										When integrity monitoring is enabled, you must not disable virtual trusted platform module (vTPM).
									</p></div></div></dd><dt><a href="#CO28-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specify whether vTPM is enabled. Valid values are <code class="literal">Disabled</code> or <code class="literal">Enabled</code>.
								</div></dd></dl></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Using the Google Cloud console, review the details for a machine deployed by the machine set and verify that the Shielded VM options match the values that you configured.
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem"><p class="simpara">
							<a class="link" href="https://cloud.google.com/compute/shielded-vm/docs/shielded-vm">What is Shielded VM?</a>
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<a class="link" href="https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#secure-boot">Secure Boot</a>
								</li><li class="listitem">
									<a class="link" href="https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#vtpm">Virtual Trusted Platform Module (vTPM)</a>
								</li><li class="listitem">
									<a class="link" href="https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#integrity-monitoring">Integrity monitoring</a>
								</li></ul></div></li></ul></div></section><section class="section" id="machineset-gcp-enabling-customer-managed-encryption_creating-machineset-gcp"><div class="titlepage"><div><div><h3 class="title">2.5.7. Enabling customer-managed encryption keys for a machine set</h3></div></div></div><p>
					Google Cloud Platform (GCP) Compute Engine allows users to supply an encryption key to encrypt data on disks at rest. The key is used to encrypt the data encryption key, not to encrypt the customer’s data. By default, Compute Engine encrypts this data by using Compute Engine keys.
				</p><p>
					You can enable encryption with a customer-managed key in clusters that use the Machine API. You must first <a class="link" href="https://cloud.google.com/compute/docs/disks/customer-managed-encryption#before_you_begin">create a KMS key</a> and assign the correct permissions to a service account. The KMS key name, key ring name, and location are required to allow a service account to use your key.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you do not want to use a dedicated service account for the KMS encryption, the Compute Engine default service account is used instead. You must grant the default service account permission to access the keys if you do not use a dedicated service account. The Compute Engine default service account name follows the <code class="literal">service-&lt;project_number&gt;@compute-system.iam.gserviceaccount.com</code> pattern.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To allow a specific service account to use your KMS key and to grant the service account the correct IAM role, run the following command with your KMS key name, key ring name, and location:
						</p><pre class="programlisting language-terminal">$ gcloud kms keys add-iam-policy-binding &lt;key_name&gt; \
  --keyring &lt;key_ring_name&gt; \
  --location &lt;key_ring_location&gt; \
  --member "serviceAccount:service-&lt;project_number&gt;@compute-system.iam.gserviceaccount.com” \
  --role roles/cloudkms.cryptoKeyEncrypterDecrypter</pre></li><li class="listitem"><p class="simpara">
							Configure the encryption key under the <code class="literal">providerSpec</code> field in your machine set YAML file. For example:
						</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
...
spec:
  template:
    spec:
      providerSpec:
        value:
          disks:
          - type:
            encryptionKey:
              kmsKey:
                name: machine-encryption-key <span id="CO29-1"><!--Empty--></span><span class="callout">1</span>
                keyRing: openshift-encrpytion-ring <span id="CO29-2"><!--Empty--></span><span class="callout">2</span>
                location: global <span id="CO29-3"><!--Empty--></span><span class="callout">3</span>
                projectID: openshift-gcp-project <span id="CO29-4"><!--Empty--></span><span class="callout">4</span>
              kmsKeyServiceAccount: openshift-service-account@openshift-gcp-project.iam.gserviceaccount.com <span id="CO29-5"><!--Empty--></span><span class="callout">5</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO29-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the customer-managed encryption key that is used for the disk encryption.
								</div></dd><dt><a href="#CO29-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The name of the KMS key ring that the KMS key belongs to.
								</div></dd><dt><a href="#CO29-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The GCP location in which the KMS key ring exists.
								</div></dd><dt><a href="#CO29-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Optional: The ID of the project in which the KMS key ring exists. If a project ID is not set, the machine set <code class="literal">projectID</code> in which the machine set was created is used.
								</div></dd><dt><a href="#CO29-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Optional: The service account that is used for the encryption request for the given KMS key. If a service account is not set, the Compute Engine default service account is used.
								</div></dd></dl></div><p class="simpara">
							When a new machine is created by using the updated <code class="literal">providerSpec</code> object configuration, the disk encryption key is encrypted with the KMS key.
						</p></li></ol></div></section><section class="section" id="machineset-gcp-enabling-gpu-support_creating-machineset-gcp"><div class="titlepage"><div><div><h3 class="title">2.5.8. Enabling GPU support for a compute machine set</h3></div></div></div><p>
					Google Cloud Platform (GCP) Compute Engine enables users to add GPUs to VM instances. Workloads that benefit from access to GPU resources can perform better on compute machines with this feature enabled. OpenShift Container Platform on GCP supports NVIDIA GPU models in the A2 and N1 machine series.
				</p><div class="table" id="idm140311148527952"><p class="title"><strong>Table 2.1. Supported GPU configurations</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140311151942800" scope="col">Model name</th><th align="left" valign="top" id="idm140311151941712" scope="col">GPU type</th><th align="left" valign="top" id="idm140311151940624" scope="col">Machine types <sup>[1]</sup></th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140311151942800"> <p>
									NVIDIA A100
								</p>
								 </td><td align="left" valign="top" headers="idm140311151941712"> <p>
									<code class="literal">nvidia-tesla-a100</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140311151940624"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">a2-highgpu-1g</code>
										</li><li class="listitem">
											<code class="literal">a2-highgpu-2g</code>
										</li><li class="listitem">
											<code class="literal">a2-highgpu-4g</code>
										</li><li class="listitem">
											<code class="literal">a2-highgpu-8g</code>
										</li><li class="listitem">
											<code class="literal">a2-megagpu-16g</code>
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140311151942800"> <p>
									NVIDIA K80
								</p>
								 </td><td align="left" valign="top" headers="idm140311151941712"> <p>
									<code class="literal">nvidia-tesla-k80</code>
								</p>
								 </td><td rowspan="5" align="left" valign="top" headers="idm140311151940624"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">n1-standard-1</code>
										</li><li class="listitem">
											<code class="literal">n1-standard-2</code>
										</li><li class="listitem">
											<code class="literal">n1-standard-4</code>
										</li><li class="listitem">
											<code class="literal">n1-standard-8</code>
										</li><li class="listitem">
											<code class="literal">n1-standard-16</code>
										</li><li class="listitem">
											<code class="literal">n1-standard-32</code>
										</li><li class="listitem">
											<code class="literal">n1-standard-64</code>
										</li><li class="listitem">
											<code class="literal">n1-standard-96</code>
										</li><li class="listitem">
											<code class="literal">n1-highmem-2</code>
										</li><li class="listitem">
											<code class="literal">n1-highmem-4</code>
										</li><li class="listitem">
											<code class="literal">n1-highmem-8</code>
										</li><li class="listitem">
											<code class="literal">n1-highmem-16</code>
										</li><li class="listitem">
											<code class="literal">n1-highmem-32</code>
										</li><li class="listitem">
											<code class="literal">n1-highmem-64</code>
										</li><li class="listitem">
											<code class="literal">n1-highmem-96</code>
										</li><li class="listitem">
											<code class="literal">n1-highcpu-2</code>
										</li><li class="listitem">
											<code class="literal">n1-highcpu-4</code>
										</li><li class="listitem">
											<code class="literal">n1-highcpu-8</code>
										</li><li class="listitem">
											<code class="literal">n1-highcpu-16</code>
										</li><li class="listitem">
											<code class="literal">n1-highcpu-32</code>
										</li><li class="listitem">
											<code class="literal">n1-highcpu-64</code>
										</li><li class="listitem">
											<code class="literal">n1-highcpu-96</code>
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140311151942800"> <p>
									NVIDIA P100
								</p>
								 </td><td align="left" valign="top" headers="idm140311151941712"> <p>
									<code class="literal">nvidia-tesla-p100</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140311151942800"> <p>
									NVIDIA P4
								</p>
								 </td><td align="left" valign="top" headers="idm140311151941712"> <p>
									<code class="literal">nvidia-tesla-p4</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140311151942800"> <p>
									NVIDIA T4
								</p>
								 </td><td align="left" valign="top" headers="idm140311151941712"> <p>
									<code class="literal">nvidia-tesla-t4</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140311151942800"> <p>
									NVIDIA V100
								</p>
								 </td><td align="left" valign="top" headers="idm140311151941712"> <p>
									<code class="literal">nvidia-tesla-v100</code>
								</p>
								 </td></tr></tbody></table></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							For more information about machine types, including specifications, compatibility, regional availability, and limitations, see the GCP Compute Engine documentation about <a class="link" href="https://cloud.google.com/compute/docs/general-purpose-machines#n1_machines">N1 machine series</a>, <a class="link" href="https://cloud.google.com/compute/docs/accelerator-optimized-machines#a2_vms">A2 machine series</a>, and <a class="link" href="https://cloud.google.com/compute/docs/gpus/gpu-regions-zones#gpu_regions_and_zones">GPU regions and zones availability</a>.
						</li></ol></div><p>
					You can define which supported GPU to use for an instance by using the Machine API.
				</p><p>
					You can configure machines in the N1 machine series to deploy with one of the supported GPU types. Machines in the A2 machine series come with associated GPUs, and cannot use guest accelerators.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						GPUs for graphics workloads are not supported.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In a text editor, open the YAML file for an existing compute machine set or create a new one.
						</li><li class="listitem"><p class="simpara">
							Specify a GPU configuration under the <code class="literal">providerSpec</code> field in your compute machine set YAML file. See the following examples of valid configurations:
						</p><div class="formalpara"><p class="title"><strong>Example configuration for the A2 machine series:</strong></p><p>
								
<pre class="programlisting language-yaml">  providerSpec:
    value:
      machineType: a2-highgpu-1g <span id="CO30-1"><!--Empty--></span><span class="callout">1</span>
      onHostMaintenance: Terminate <span id="CO30-2"><!--Empty--></span><span class="callout">2</span>
      restartPolicy: Always <span id="CO30-3"><!--Empty--></span><span class="callout">3</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO30-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the machine type. Ensure that the machine type is included in the A2 machine series.
								</div></dd><dt><a href="#CO30-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									When using GPU support, you must set <code class="literal">onHostMaintenance</code> to <code class="literal">Terminate</code>.
								</div></dd><dt><a href="#CO30-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify the restart policy for machines deployed by the compute machine set. Allowed values are <code class="literal">Always</code> or <code class="literal">Never</code>.
								</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example configuration for the N1 machine series:</strong></p><p>
								
<pre class="programlisting language-yaml">providerSpec:
  value:
    gpus:
    - count: 1 <span id="CO31-1"><!--Empty--></span><span class="callout">1</span>
      type: nvidia-tesla-p100 <span id="CO31-2"><!--Empty--></span><span class="callout">2</span>
    machineType: n1-standard-1 <span id="CO31-3"><!--Empty--></span><span class="callout">3</span>
    onHostMaintenance: Terminate <span id="CO31-4"><!--Empty--></span><span class="callout">4</span>
    restartPolicy: Always <span id="CO31-5"><!--Empty--></span><span class="callout">5</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO31-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the number of GPUs to attach to the machine.
								</div></dd><dt><a href="#CO31-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the type of GPUs to attach to the machine. Ensure that the machine type and GPU type are compatible.
								</div></dd><dt><a href="#CO31-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify the machine type. Ensure that the machine type and GPU type are compatible.
								</div></dd><dt><a href="#CO31-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									When using GPU support, you must set <code class="literal">onHostMaintenance</code> to <code class="literal">Terminate</code>.
								</div></dd><dt><a href="#CO31-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Specify the restart policy for machines deployed by the compute machine set. Allowed values are <code class="literal">Always</code> or <code class="literal">Never</code>.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="nvidia-gpu-gcp-adding-a-gpu-node_creating-machineset-gcp"><div class="titlepage"><div><div><h3 class="title">2.5.9. Adding a GPU node to an existing OpenShift Container Platform cluster</h3></div></div></div><p>
					You can copy and modify a default compute machine set configuration to create a GPU-enabled machine set and machines for the GCP cloud provider.
				</p><p>
					The following table lists the validated instance types:
				</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140311159055936" scope="col">Instance type</th><th align="left" valign="top" id="idm140311159054848" scope="col">NVIDIA GPU accelerator</th><th align="left" valign="top" id="idm140311159053760" scope="col">Maximum number of GPUs</th><th align="left" valign="top" id="idm140311147774064" scope="col">Architecture</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140311159055936"> <p>
									<code class="literal">a2-highgpu-1g</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140311159054848"> <p>
									A100
								</p>
								 </td><td align="left" valign="top" headers="idm140311159053760"> <p>
									1
								</p>
								 </td><td align="left" valign="top" headers="idm140311147774064"> <p>
									x86
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140311159055936"> <p>
									<code class="literal">n1-standard-4</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140311159054848"> <p>
									T4
								</p>
								 </td><td align="left" valign="top" headers="idm140311159053760"> <p>
									1
								</p>
								 </td><td align="left" valign="top" headers="idm140311147774064"> <p>
									x86
								</p>
								 </td></tr></tbody></table></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Make a copy of an existing <code class="literal">MachineSet</code>.
						</li><li class="listitem">
							In the new copy, change the machine set <code class="literal">name</code> in <code class="literal">metadata.name</code> and in both instances of <code class="literal">machine.openshift.io/cluster-api-machineset</code>.
						</li><li class="listitem"><p class="simpara">
							Change the instance type to add the following two lines to the newly copied <code class="literal">MachineSet</code>:
						</p><pre class="screen">machineType: a2-highgpu-1g
onHostMaintenance: Terminate</pre><div class="formalpara"><p class="title"><strong>Example <code class="literal">a2-highgpu-1g.json</code> file</strong></p><p>
								
<pre class="programlisting language-json">{
    "apiVersion": "machine.openshift.io/v1beta1",
    "kind": "MachineSet",
    "metadata": {
        "annotations": {
            "machine.openshift.io/GPU": "0",
            "machine.openshift.io/memoryMb": "16384",
            "machine.openshift.io/vCPU": "4"
        },
        "creationTimestamp": "2023-01-13T17:11:02Z",
        "generation": 1,
        "labels": {
            "machine.openshift.io/cluster-api-cluster": "myclustername-2pt9p"
        },
        "name": "myclustername-2pt9p-worker-gpu-a",
        "namespace": "openshift-machine-api",
        "resourceVersion": "20185",
        "uid": "2daf4712-733e-4399-b4b4-d43cb1ed32bd"
    },
    "spec": {
        "replicas": 1,
        "selector": {
            "matchLabels": {
                "machine.openshift.io/cluster-api-cluster": "myclustername-2pt9p",
                "machine.openshift.io/cluster-api-machineset": "myclustername-2pt9p-worker-gpu-a"
            }
        },
        "template": {
            "metadata": {
                "labels": {
                    "machine.openshift.io/cluster-api-cluster": "myclustername-2pt9p",
                    "machine.openshift.io/cluster-api-machine-role": "worker",
                    "machine.openshift.io/cluster-api-machine-type": "worker",
                    "machine.openshift.io/cluster-api-machineset": "myclustername-2pt9p-worker-gpu-a"
                }
            },
            "spec": {
                "lifecycleHooks": {},
                "metadata": {},
                "providerSpec": {
                    "value": {
                        "apiVersion": "machine.openshift.io/v1beta1",
                        "canIPForward": false,
                        "credentialsSecret": {
                            "name": "gcp-cloud-credentials"
                        },
                        "deletionProtection": false,
                        "disks": [
                            {
                                "autoDelete": true,
                                "boot": true,
                                "image": "projects/rhcos-cloud/global/images/rhcos-412-86-202212081411-0-gcp-x86-64",
                                "labels": null,
                                "sizeGb": 128,
                                "type": "pd-ssd"
                            }
                        ],
                        "kind": "GCPMachineProviderSpec",
                        "machineType": "a2-highgpu-1g",
                        "onHostMaintenance": "Terminate",
                        "metadata": {
                            "creationTimestamp": null
                        },
                        "networkInterfaces": [
                            {
                                "network": "myclustername-2pt9p-network",
                                "subnetwork": "myclustername-2pt9p-worker-subnet"
                            }
                        ],
                        "preemptible": true,
                        "projectID": "myteam",
                        "region": "us-central1",
                        "serviceAccounts": [
                            {
                                "email": "myclustername-2pt9p-w@myteam.iam.gserviceaccount.com",
                                "scopes": [
                                    "https://www.googleapis.com/auth/cloud-platform"
                                ]
                            }
                        ],
                        "tags": [
                            "myclustername-2pt9p-worker"
                        ],
                        "userDataSecret": {
                            "name": "worker-user-data"
                        },
                        "zone": "us-central1-a"
                    }
                }
            }
        }
    },
    "status": {
        "availableReplicas": 1,
        "fullyLabeledReplicas": 1,
        "observedGeneration": 1,
        "readyReplicas": 1,
        "replicas": 1
    }
}</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							View the existing nodes, machines, and machine sets by running the following command. Note that each node is an instance of a machine definition with a specific GCP region and OpenShift Container Platform role.
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                             STATUS     ROLES                  AGE     VERSION
myclustername-2pt9p-master-0.c.openshift-qe.internal             Ready      control-plane,master   8h      v1.26.0
myclustername-2pt9p-master-1.c.openshift-qe.internal             Ready      control-plane,master   8h      v1.26.0
myclustername-2pt9p-master-2.c.openshift-qe.internal             Ready      control-plane,master   8h      v1.26.0
myclustername-2pt9p-worker-a-mxtnz.c.openshift-qe.internal       Ready      worker                 8h      v1.26.0
myclustername-2pt9p-worker-b-9pzzn.c.openshift-qe.internal       Ready      worker                 8h      v1.26.0
myclustername-2pt9p-worker-c-6pbg6.c.openshift-qe.internal       Ready      worker                 8h      v1.26.0
myclustername-2pt9p-worker-gpu-a-wxcr6.c.openshift-qe.internal   Ready      worker                 4h35m   v1.26.0</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							View the machines and machine sets that exist in the <code class="literal">openshift-machine-api</code> namespace by running the following command. Each compute machine set is associated with a different availability zone within the GCP region. The installer automatically load balances compute machines across availability zones.
						</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                               DESIRED   CURRENT   READY   AVAILABLE   AGE
myclustername-2pt9p-worker-a       1         1         1       1           8h
myclustername-2pt9p-worker-b       1         1         1       1           8h
myclustername-2pt9p-worker-c       1         1                             8h
myclustername-2pt9p-worker-f       0         0                             8h</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							View the machines that exist in the <code class="literal">openshift-machine-api</code> namespace by running the following command. You can only configure one compute machine per set, although you can scale a compute machine set to add a node in a particular region and zone.
						</p><pre class="programlisting language-terminal">$ oc get machines -n openshift-machine-api | grep worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">myclustername-2pt9p-worker-a-mxtnz       Running   n2-standard-4   us-central1   us-central1-a   8h
myclustername-2pt9p-worker-b-9pzzn       Running   n2-standard-4   us-central1   us-central1-b   8h
myclustername-2pt9p-worker-c-6pbg6       Running   n2-standard-4   us-central1   us-central1-c   8h</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Make a copy of one of the existing compute <code class="literal">MachineSet</code> definitions and output the result to a JSON file by running the following command. This will be the basis for the GPU-enabled compute machine set definition.
						</p><pre class="programlisting language-terminal">$ oc get machineset myclustername-2pt9p-worker-a -n openshift-machine-api -o json  &gt; &lt;output_file.json&gt;</pre></li><li class="listitem"><p class="simpara">
							Edit the JSON file to make the following changes to the new <code class="literal">MachineSet</code> definition:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Rename the machine set <code class="literal">name</code> by inserting the substring <code class="literal">gpu</code> in <code class="literal">metadata.name</code> and in both instances of <code class="literal">machine.openshift.io/cluster-api-machineset</code>.
								</li><li class="listitem"><p class="simpara">
									Change the <code class="literal">machineType</code> of the new <code class="literal">MachineSet</code> definition to <code class="literal">a2-highgpu-1g</code>, which includes an NVIDIA A100 GPU.
								</p><pre class="programlisting language-terminal">jq .spec.template.spec.providerSpec.value.machineType ocp_4.13_machineset-a2-highgpu-1g.json

"a2-highgpu-1g"</pre><p class="simpara">
									The <code class="literal">&lt;output_file.json&gt;</code> file is saved as <code class="literal">ocp_4.13_machineset-a2-highgpu-1g.json</code>.
								</p></li></ul></div></li><li class="listitem"><p class="simpara">
							Update the following fields in <code class="literal">ocp_4.13_machineset-a2-highgpu-1g.json</code>:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Change <code class="literal">.metadata.name</code> to a name containing <code class="literal">gpu</code>.
								</li><li class="listitem">
									Change <code class="literal">.spec.selector.matchLabels["machine.openshift.io/cluster-api-machineset"]</code> to match the new <code class="literal">.metadata.name</code>.
								</li><li class="listitem">
									Change <code class="literal">.spec.template.metadata.labels["machine.openshift.io/cluster-api-machineset"]</code> to match the new <code class="literal">.metadata.name</code>.
								</li><li class="listitem">
									Change <code class="literal">.spec.template.spec.providerSpec.value.MachineType</code> to <code class="literal">a2-highgpu-1g</code>.
								</li><li class="listitem"><p class="simpara">
									Add the following line under <code class="literal">machineType</code>: `"onHostMaintenance": "Terminate". For example:
								</p><pre class="programlisting language-json">"machineType": "a2-highgpu-1g",
"onHostMaintenance": "Terminate",</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							To verify your changes, perform a <code class="literal">diff</code> of the original compute definition and the new GPU-enabled node definition by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineset/myclustername-2pt9p-worker-a -n openshift-machine-api -o json | diff ocp_4.13_machineset-a2-highgpu-1g.json -</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">15c15
&lt;         "name": "myclustername-2pt9p-worker-gpu-a",
---
&gt;         "name": "myclustername-2pt9p-worker-a",
25c25
&lt;                 "machine.openshift.io/cluster-api-machineset": "myclustername-2pt9p-worker-gpu-a"
---
&gt;                 "machine.openshift.io/cluster-api-machineset": "myclustername-2pt9p-worker-a"
34c34
&lt;                     "machine.openshift.io/cluster-api-machineset": "myclustername-2pt9p-worker-gpu-a"
---
&gt;                     "machine.openshift.io/cluster-api-machineset": "myclustername-2pt9p-worker-a"
59,60c59
&lt;                         "machineType": "a2-highgpu-1g",
&lt;                         "onHostMaintenance": "Terminate",
---
&gt;                         "machineType": "n2-standard-4",</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create the GPU-enabled compute machine set from the definition file by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f ocp_4.13_machineset-a2-highgpu-1g.json</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">machineset.machine.openshift.io/myclustername-2pt9p-worker-gpu-a created</pre>

							</p></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the machine set you created by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api get machinesets | grep gpu</pre><p class="simpara">
							The MachineSet replica count is set to <code class="literal">1</code> so a new <code class="literal">Machine</code> object is created automatically.
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">myclustername-2pt9p-worker-gpu-a   1         1         1       1           5h24m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							View the <code class="literal">Machine</code> object that the machine set created by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api get machines | grep gpu</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">myclustername-2pt9p-worker-gpu-a-wxcr6   Running   a2-highgpu-1g   us-central1   us-central1-a   5h25m</pre>

							</p></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Note that there is no need to specify a namespace for the node. The node definition is cluster scoped.
					</p></div></div></section><section class="section" id="nvidia-gpu-aws-deploying-the-node-feature-discovery-operator_creating-machineset-gcp"><div class="titlepage"><div><div><h3 class="title">2.5.10. Deploying the Node Feature Discovery Operator</h3></div></div></div><p>
					After the GPU-enabled node is created, you need to discover the GPU-enabled node so it can be scheduled. To do this, install the Node Feature Discovery (NFD) Operator. The NFD Operator identifies hardware device features in nodes. It solves the general problem of identifying and cataloging hardware resources in the infrastructure nodes so they can be made available to OpenShift Container Platform.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Install the Node Feature Discovery Operator from <span class="strong strong"><strong>OperatorHub</strong></span> in the OpenShift Container Platform console.
						</li><li class="listitem">
							After installing the NFD Operator into <span class="strong strong"><strong>OperatorHub</strong></span>, select <span class="strong strong"><strong>Node Feature Discovery</strong></span> from the installed Operators list and select <span class="strong strong"><strong>Create instance</strong></span>. This installs the <code class="literal">nfd-master</code> and <code class="literal">nfd-worker</code> pods, one <code class="literal">nfd-worker</code> pod for each compute node, in the <code class="literal">openshift-nfd</code> namespace.
						</li><li class="listitem"><p class="simpara">
							Verify that the Operator is installed and running by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-nfd</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                       READY    STATUS     RESTARTS   AGE

nfd-controller-manager-8646fcbb65-x5qgk    2/2      Running 7  (8h ago)   1d</pre>

							</p></div></li><li class="listitem">
							Browse to the installed Oerator in the console and select <span class="strong strong"><strong>Create Node Feature Discovery</strong></span>.
						</li><li class="listitem">
							Select <span class="strong strong"><strong>Create</strong></span> to build a NFD custom resource. This creates NFD pods in the <code class="literal">openshift-nfd</code> namespace that poll the OpenShift Container Platform nodes for hardware resources and catalogue them.
						</li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							After a successful build, verify that a NFD pod is running on each nodes by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-nfd</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                       READY   STATUS      RESTARTS        AGE
nfd-controller-manager-8646fcbb65-x5qgk    2/2     Running     7 (8h ago)      12d
nfd-master-769656c4cb-w9vrv                1/1     Running     0               12d
nfd-worker-qjxb2                           1/1     Running     3 (3d14h ago)   12d
nfd-worker-xtz9b                           1/1     Running     5 (3d14h ago)   12d</pre>

							</p></div><p class="simpara">
							The NFD Operator uses vendor PCI IDs to identify hardware in a node. NVIDIA uses the PCI ID <code class="literal">10de</code>.
						</p></li><li class="listitem"><p class="simpara">
							View the NVIDIA GPU discovered by the NFD Operator by running the following command:
						</p><pre class="programlisting language-terminal">$ oc describe node ip-10-0-132-138.us-east-2.compute.internal | egrep 'Roles|pci'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Roles: worker

feature.node.kubernetes.io/pci-1013.present=true

feature.node.kubernetes.io/pci-10de.present=true

feature.node.kubernetes.io/pci-1d0f.present=true</pre>

							</p></div><p class="simpara">
							<code class="literal">10de</code> appears in the node feature list for the GPU-enabled node. This mean the NFD Operator correctly identified the node from the GPU-enabled MachineSet.
						</p></li></ol></div></section></section><section class="section" id="creating-machineset-ibm-cloud"><div class="titlepage"><div><div><h2 class="title">2.6. Creating a compute machine set on IBM Cloud</h2></div></div></div><p>
				You can create a different compute machine set to serve a specific purpose in your OpenShift Container Platform cluster on IBM Cloud. For example, you might create infrastructure machine sets and related machines so that you can move supporting workloads to the new machines.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.
				</p><p>
					Clusters with the infrastructure platform type <code class="literal">none</code> cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.
				</p><p>
					To view the platform type for your cluster, run the following command:
				</p><pre class="programlisting language-terminal">$ oc get infrastructure cluster -o jsonpath='{.status.platform}'</pre></div></div><section class="section" id="machineset-yaml-ibm-cloud_creating-machineset-ibm-cloud"><div class="titlepage"><div><div><h3 class="title">2.6.1. Sample YAML for a compute machine set custom resource on IBM Cloud</h3></div></div></div><p>
					This sample YAML defines a compute machine set that runs in a specified IBM Cloud zone in a region and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/&lt;role&gt;: ""</code>.
				</p><p>
					In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;role&gt;</code> is the node label to add.
				</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO32-1"><!--Empty--></span><span class="callout">1</span>
    machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO32-2"><!--Empty--></span><span class="callout">2</span>
    machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO32-3"><!--Empty--></span><span class="callout">3</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;region&gt; <span id="CO32-4"><!--Empty--></span><span class="callout">4</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO32-5"><!--Empty--></span><span class="callout">5</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;region&gt; <span id="CO32-6"><!--Empty--></span><span class="callout">6</span>
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO32-7"><!--Empty--></span><span class="callout">7</span>
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO32-8"><!--Empty--></span><span class="callout">8</span>
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO32-9"><!--Empty--></span><span class="callout">9</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;region&gt; <span id="CO32-10"><!--Empty--></span><span class="callout">10</span>
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/&lt;role&gt;: ""
      providerSpec:
        value:
          apiVersion: ibmcloudproviderconfig.openshift.io/v1beta1
          credentialsSecret:
            name: ibmcloud-credentials
          image: &lt;infrastructure_id&gt;-rhcos <span id="CO32-11"><!--Empty--></span><span class="callout">11</span>
          kind: IBMCloudMachineProviderSpec
          primaryNetworkInterface:
              securityGroups:
              - &lt;infrastructure_id&gt;-sg-cluster-wide
              - &lt;infrastructure_id&gt;-sg-openshift-net
              subnet: &lt;infrastructure_id&gt;-subnet-compute-&lt;zone&gt; <span id="CO32-12"><!--Empty--></span><span class="callout">12</span>
          profile: &lt;instance_profile&gt; <span id="CO32-13"><!--Empty--></span><span class="callout">13</span>
          region: &lt;region&gt; <span id="CO32-14"><!--Empty--></span><span class="callout">14</span>
          resourceGroup: &lt;resource_group&gt; <span id="CO32-15"><!--Empty--></span><span class="callout">15</span>
          userDataSecret:
              name: &lt;role&gt;-user-data <span id="CO32-16"><!--Empty--></span><span class="callout">16</span>
          vpc: &lt;vpc_name&gt; <span id="CO32-17"><!--Empty--></span><span class="callout">17</span>
          zone: &lt;zone&gt; <span id="CO32-18"><!--Empty--></span><span class="callout">18</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO32-1"><span class="callout">1</span></a> <a href="#CO32-5"><span class="callout">5</span></a> <a href="#CO32-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							The infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
						</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><a href="#CO32-2"><span class="callout">2</span></a> <a href="#CO32-3"><span class="callout">3</span></a> <a href="#CO32-8"><span class="callout">8</span></a> <a href="#CO32-9"><span class="callout">9</span></a> <a href="#CO32-16"><span class="callout">16</span></a> </dt><dd><div class="para">
							The node label to add.
						</div></dd><dt><a href="#CO32-4"><span class="callout">4</span></a> <a href="#CO32-6"><span class="callout">6</span></a> <a href="#CO32-10"><span class="callout">10</span></a> </dt><dd><div class="para">
							The infrastructure ID, node label, and region.
						</div></dd><dt><a href="#CO32-11"><span class="callout">11</span></a> </dt><dd><div class="para">
							The custom Red Hat Enterprise Linux CoreOS (RHCOS) image that was used for cluster installation.
						</div></dd><dt><a href="#CO32-12"><span class="callout">12</span></a> </dt><dd><div class="para">
							The infrastructure ID and zone within your region to place machines on. Be sure that your region supports the zone that you specify.
						</div></dd><dt><a href="#CO32-13"><span class="callout">13</span></a> </dt><dd><div class="para">
							Specify the <a class="link" href="https://cloud.ibm.com/docs/vpc?topic=vpc-profiles&amp;interface=ui">IBM Cloud instance profile</a>.
						</div></dd><dt><a href="#CO32-14"><span class="callout">14</span></a> </dt><dd><div class="para">
							Specify the region to place machines on.
						</div></dd><dt><a href="#CO32-15"><span class="callout">15</span></a> </dt><dd><div class="para">
							The resource group that machine resources are placed in. This is either an existing resource group specified at installation time, or an installer-created resource group named based on the infrastructure ID.
						</div></dd><dt><a href="#CO32-17"><span class="callout">17</span></a> </dt><dd><div class="para">
							The VPC name.
						</div></dd><dt><a href="#CO32-18"><span class="callout">18</span></a> </dt><dd><div class="para">
							Specify the zone within your region to place machines on. Be sure that your region supports the zone that you specify.
						</div></dd></dl></div></section><section class="section" id="machineset-creating_creating-machineset-ibm-cloud"><div class="titlepage"><div><div><h3 class="title">2.6.2. Creating a compute machine set</h3></div></div></div><p>
					In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Deploy an OpenShift Container Platform cluster.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <code class="literal">&lt;file_name&gt;.yaml</code>.
						</p><p class="simpara">
							Ensure that you set the <code class="literal">&lt;clusterID&gt;</code> and <code class="literal">&lt;role&gt;</code> parameter values.
						</p></li><li class="listitem"><p class="simpara">
							Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									To list the compute machine sets in your cluster, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									To view values of a specific compute machine set custom resource (CR), run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machineset &lt;machineset_name&gt; \
  -n openshift-machine-api -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO33-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO33-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
    spec:
      providerSpec: <span id="CO33-3"><!--Empty--></span><span class="callout">3</span>
        ...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO33-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The cluster infrastructure ID.
										</div></dd><dt><a href="#CO33-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											A default node label.
										</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												For clusters that have user-provisioned infrastructure, a compute machine set can only create <code class="literal">worker</code> and <code class="literal">infra</code> type machines.
											</p></div></div></dd><dt><a href="#CO33-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											The values in the <code class="literal">&lt;providerSpec&gt;</code> section of the compute machine set CR are platform-specific. For more information about <code class="literal">&lt;providerSpec&gt;</code> parameters in the CR, see the sample compute machine set CR configuration for your provider.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">MachineSet</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							View the list of compute machine sets by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-infra-us-east-1a    1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

							</p></div><p class="simpara">
							When the new compute machine set is available, the <code class="literal">DESIRED</code> and <code class="literal">CURRENT</code> values match. If the compute machine set is not available, wait a few minutes and run the command again.
						</p></li></ul></div></section></section><section class="section" id="creating-machineset-ibm-power-vs"><div class="titlepage"><div><div><h2 class="title">2.7. Creating a compute machine set on IBM Power Virtual Server</h2></div></div></div><p>
				You can create a different compute machine set to serve a specific purpose in your OpenShift Container Platform cluster on IBM Power Virtual Server. For example, you might create infrastructure machine sets and related machines so that you can move supporting workloads to the new machines.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.
				</p><p>
					Clusters with the infrastructure platform type <code class="literal">none</code> cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.
				</p><p>
					To view the platform type for your cluster, run the following command:
				</p><pre class="programlisting language-terminal">$ oc get infrastructure cluster -o jsonpath='{.status.platform}'</pre></div></div><section class="section" id="machineset-yaml-ibm-power-vs_creating-machineset-ibm-power-vs"><div class="titlepage"><div><div><h3 class="title">2.7.1. Sample YAML for a compute machine set custom resource on IBM Power Virtual Server</h3></div></div></div><p>
					This sample YAML file defines a compute machine set that runs in a specified IBM Power Virtual Server zone in a region and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/&lt;role&gt;: ""</code>.
				</p><p>
					In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;role&gt;</code> is the node label to add.
				</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO34-1"><!--Empty--></span><span class="callout">1</span>
    machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO34-2"><!--Empty--></span><span class="callout">2</span>
    machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO34-3"><!--Empty--></span><span class="callout">3</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;region&gt; <span id="CO34-4"><!--Empty--></span><span class="callout">4</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO34-5"><!--Empty--></span><span class="callout">5</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;region&gt; <span id="CO34-6"><!--Empty--></span><span class="callout">6</span>
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO34-7"><!--Empty--></span><span class="callout">7</span>
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO34-8"><!--Empty--></span><span class="callout">8</span>
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO34-9"><!--Empty--></span><span class="callout">9</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;region&gt; <span id="CO34-10"><!--Empty--></span><span class="callout">10</span>
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/&lt;role&gt;: ""
      providerSpec:
        value:
          apiVersion: machine.openshift.io/v1
          credentialsSecret:
            name: powervs-credentials
          image:
            name: rhcos-&lt;infrastructure_id&gt; <span id="CO34-11"><!--Empty--></span><span class="callout">11</span>
            type: Name
          keyPairName: &lt;infrastructure_id&gt;-key
          kind: PowerVSMachineProviderConfig
          memoryGiB: 32
          network:
            regex: ^DHCPSERVER[0-9a-z]{32}_Private$
            type: RegEx
          processorType: Shared
          processors: "0.5"
          serviceInstance:
            id: &lt;ibm_power_vs_service_instance_id&gt;
            type: ID <span id="CO34-12"><!--Empty--></span><span class="callout">12</span>
          systemType: s922
          userDataSecret:
            name: &lt;role&gt;-user-data</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO34-1"><span class="callout">1</span></a> <a href="#CO34-5"><span class="callout">5</span></a> <a href="#CO34-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							The infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
						</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><a href="#CO34-2"><span class="callout">2</span></a> <a href="#CO34-3"><span class="callout">3</span></a> <a href="#CO34-8"><span class="callout">8</span></a> <a href="#CO34-9"><span class="callout">9</span></a> </dt><dd><div class="para">
							The node label to add.
						</div></dd><dt><a href="#CO34-4"><span class="callout">4</span></a> <a href="#CO34-6"><span class="callout">6</span></a> <a href="#CO34-10"><span class="callout">10</span></a> </dt><dd><div class="para">
							The infrastructure ID, node label, and region.
						</div></dd><dt><a href="#CO34-11"><span class="callout">11</span></a> </dt><dd><div class="para">
							The custom Red Hat Enterprise Linux CoreOS (RHCOS) image that was used for cluster installation.
						</div></dd><dt><a href="#CO34-12"><span class="callout">12</span></a> </dt><dd><div class="para">
							The infrastructure ID within your region to place machines on.
						</div></dd></dl></div></section><section class="section" id="machineset-creating_creating-machineset-ibm-power-vs"><div class="titlepage"><div><div><h3 class="title">2.7.2. Creating a compute machine set</h3></div></div></div><p>
					In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Deploy an OpenShift Container Platform cluster.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <code class="literal">&lt;file_name&gt;.yaml</code>.
						</p><p class="simpara">
							Ensure that you set the <code class="literal">&lt;clusterID&gt;</code> and <code class="literal">&lt;role&gt;</code> parameter values.
						</p></li><li class="listitem"><p class="simpara">
							Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									To list the compute machine sets in your cluster, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									To view values of a specific compute machine set custom resource (CR), run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machineset &lt;machineset_name&gt; \
  -n openshift-machine-api -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO35-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO35-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
    spec:
      providerSpec: <span id="CO35-3"><!--Empty--></span><span class="callout">3</span>
        ...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO35-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The cluster infrastructure ID.
										</div></dd><dt><a href="#CO35-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											A default node label.
										</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												For clusters that have user-provisioned infrastructure, a compute machine set can only create <code class="literal">worker</code> and <code class="literal">infra</code> type machines.
											</p></div></div></dd><dt><a href="#CO35-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											The values in the <code class="literal">&lt;providerSpec&gt;</code> section of the compute machine set CR are platform-specific. For more information about <code class="literal">&lt;providerSpec&gt;</code> parameters in the CR, see the sample compute machine set CR configuration for your provider.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">MachineSet</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							View the list of compute machine sets by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-infra-us-east-1a    1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

							</p></div><p class="simpara">
							When the new compute machine set is available, the <code class="literal">DESIRED</code> and <code class="literal">CURRENT</code> values match. If the compute machine set is not available, wait a few minutes and run the command again.
						</p></li></ul></div></section></section><section class="section" id="creating-machineset-nutanix"><div class="titlepage"><div><div><h2 class="title">2.8. Creating a compute machine set on Nutanix</h2></div></div></div><p>
				You can create a different compute machine set to serve a specific purpose in your OpenShift Container Platform cluster on Nutanix. For example, you might create infrastructure machine sets and related machines so that you can move supporting workloads to the new machines.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.
				</p><p>
					Clusters with the infrastructure platform type <code class="literal">none</code> cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.
				</p><p>
					To view the platform type for your cluster, run the following command:
				</p><pre class="programlisting language-terminal">$ oc get infrastructure cluster -o jsonpath='{.status.platform}'</pre></div></div><section class="section" id="machineset-yaml-nutanix_creating-machineset-nutanix"><div class="titlepage"><div><div><h3 class="title">2.8.1. Sample YAML for a compute machine set custom resource on Nutanix</h3></div></div></div><p>
					This sample YAML defines a Nutanix compute machine set that creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/&lt;role&gt;: ""</code>.
				</p><p>
					In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;role&gt;</code> is the node label to add.
				</p><h5 id="machineset-yaml-nutanix-oc_creating-machineset-nutanix">Values obtained by using the OpenShift CLI</h5><p>
					In the following example, you can obtain some of the values for your cluster by using the OpenShift CLI (<code class="literal">oc</code>).
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Infrastructure ID</span></dt><dd><p class="simpara">
								The <code class="literal">&lt;infrastructure_id&gt;</code> string is the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd></dl></div><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO36-1"><!--Empty--></span><span class="callout">1</span>
    machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO36-2"><!--Empty--></span><span class="callout">2</span>
    machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
  name: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;zone&gt; <span id="CO36-3"><!--Empty--></span><span class="callout">3</span>
  namespace: openshift-machine-api
  annotations: <span id="CO36-4"><!--Empty--></span><span class="callout">4</span>
    machine.openshift.io/memoryMb: "16384"
    machine.openshift.io/vCPU: "4"
spec:
  replicas: 3
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;zone&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;zone&gt;
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/&lt;role&gt;: ""
      providerSpec:
        value:
          apiVersion: machine.openshift.io/v1
          bootType: "" <span id="CO36-5"><!--Empty--></span><span class="callout">5</span>
          categories: <span id="CO36-6"><!--Empty--></span><span class="callout">6</span>
          - key: &lt;category_name&gt;
            value: &lt;category_value&gt;
          cluster: <span id="CO36-7"><!--Empty--></span><span class="callout">7</span>
            type: uuid
            uuid: &lt;cluster_uuid&gt;
          credentialsSecret:
            name: nutanix-creds-secret
          image:
            name: &lt;infrastructure_id&gt;-rhcos <span id="CO36-8"><!--Empty--></span><span class="callout">8</span>
            type: name
          kind: NutanixMachineProviderConfig
          memorySize: 16Gi <span id="CO36-9"><!--Empty--></span><span class="callout">9</span>
          project: <span id="CO36-10"><!--Empty--></span><span class="callout">10</span>
            type: name
            name: &lt;project_name&gt;
          subnets:
          - type: uuid
            uuid: &lt;subnet_uuid&gt;
          systemDiskSize: 120Gi <span id="CO36-11"><!--Empty--></span><span class="callout">11</span>
          userDataSecret:
            name: &lt;user_data_secret&gt; <span id="CO36-12"><!--Empty--></span><span class="callout">12</span>
          vcpuSockets: 4 <span id="CO36-13"><!--Empty--></span><span class="callout">13</span>
          vcpusPerSocket: 1 <span id="CO36-14"><!--Empty--></span><span class="callout">14</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO36-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							For <code class="literal">&lt;infrastructure_id&gt;</code>, specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster.
						</div></dd><dt><a href="#CO36-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Specify the node label to add.
						</div></dd><dt><a href="#CO36-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Specify the infrastructure ID, node label, and zone.
						</div></dd><dt><a href="#CO36-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Annotations for the cluster autoscaler.
						</div></dd><dt><a href="#CO36-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Specifies the boot type that the compute machines use. For more information about boot types, see <a class="link" href="https://portal.nutanix.com/page/documents/kbs/details?targetId=kA07V000000H3K9SAK">Understanding UEFI, Secure Boot, and TPM in the Virtualized Environment</a>. Valid values are <code class="literal">Legacy</code>, <code class="literal">SecureBoot</code>, or <code class="literal">UEFI</code>. The default is <code class="literal">Legacy</code>.
						</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								You must use the <code class="literal">Legacy</code> boot type in OpenShift Container Platform 4.13.
							</p></div></div></dd><dt><a href="#CO36-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							Specify one or more Nutanix Prism categories to apply to compute machines. This stanza requires <code class="literal">key</code> and <code class="literal">value</code> parameters for a category key-value pair that exists in Prism Central. For more information about categories, see <a class="link" href="https://portal.nutanix.com/page/documents/details?targetId=Prism-Central-Guide-vpc_2022_6:ssp-ssp-categories-manage-pc-c.html">Category management</a>.
						</div></dd><dt><a href="#CO36-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							Specify a Nutanix Prism Element cluster configuration. In this example, the cluster type is <code class="literal">uuid</code>, so there is a <code class="literal">uuid</code> stanza.
						</div></dd><dt><a href="#CO36-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							Specify the image to use. Use an image from an existing default compute machine set for the cluster.
						</div></dd><dt><a href="#CO36-9"><span class="callout">9</span></a> </dt><dd><div class="para">
							Specify the amount of memory for the cluster in Gi.
						</div></dd><dt><a href="#CO36-10"><span class="callout">10</span></a> </dt><dd><div class="para">
							Specify the Nutanix project that you use for your cluster. In this example, the project type is <code class="literal">name</code>, so there is a <code class="literal">name</code> stanza.
						</div></dd><dt><a href="#CO36-11"><span class="callout">11</span></a> </dt><dd><div class="para">
							Specify the size of the system disk in Gi.
						</div></dd><dt><a href="#CO36-12"><span class="callout">12</span></a> </dt><dd><div class="para">
							Specify the name of the secret in the user data YAML file that is in the <code class="literal">openshift-machine-api</code> namespace. Use the value that installation program populates in the default compute machine set.
						</div></dd><dt><a href="#CO36-13"><span class="callout">13</span></a> </dt><dd><div class="para">
							Specify the number of vCPU sockets.
						</div></dd><dt><a href="#CO36-14"><span class="callout">14</span></a> </dt><dd><div class="para">
							Specify the number of vCPUs per socket.
						</div></dd></dl></div></section><section class="section" id="machineset-creating_creating-machineset-nutanix"><div class="titlepage"><div><div><h3 class="title">2.8.2. Creating a compute machine set</h3></div></div></div><p>
					In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Deploy an OpenShift Container Platform cluster.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <code class="literal">&lt;file_name&gt;.yaml</code>.
						</p><p class="simpara">
							Ensure that you set the <code class="literal">&lt;clusterID&gt;</code> and <code class="literal">&lt;role&gt;</code> parameter values.
						</p></li><li class="listitem"><p class="simpara">
							Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									To list the compute machine sets in your cluster, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									To view values of a specific compute machine set custom resource (CR), run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machineset &lt;machineset_name&gt; \
  -n openshift-machine-api -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO37-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO37-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
    spec:
      providerSpec: <span id="CO37-3"><!--Empty--></span><span class="callout">3</span>
        ...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO37-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The cluster infrastructure ID.
										</div></dd><dt><a href="#CO37-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											A default node label.
										</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												For clusters that have user-provisioned infrastructure, a compute machine set can only create <code class="literal">worker</code> and <code class="literal">infra</code> type machines.
											</p></div></div></dd><dt><a href="#CO37-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											The values in the <code class="literal">&lt;providerSpec&gt;</code> section of the compute machine set CR are platform-specific. For more information about <code class="literal">&lt;providerSpec&gt;</code> parameters in the CR, see the sample compute machine set CR configuration for your provider.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">MachineSet</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							View the list of compute machine sets by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-infra-us-east-1a    1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

							</p></div><p class="simpara">
							When the new compute machine set is available, the <code class="literal">DESIRED</code> and <code class="literal">CURRENT</code> values match. If the compute machine set is not available, wait a few minutes and run the command again.
						</p></li></ul></div></section></section><section class="section" id="creating-machineset-osp"><div class="titlepage"><div><div><h2 class="title">2.9. Creating a compute machine set on OpenStack</h2></div></div></div><p>
				You can create a different compute machine set to serve a specific purpose in your OpenShift Container Platform cluster on Red Hat OpenStack Platform (RHOSP). For example, you might create infrastructure machine sets and related machines so that you can move supporting workloads to the new machines.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.
				</p><p>
					Clusters with the infrastructure platform type <code class="literal">none</code> cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.
				</p><p>
					To view the platform type for your cluster, run the following command:
				</p><pre class="programlisting language-terminal">$ oc get infrastructure cluster -o jsonpath='{.status.platform}'</pre></div></div><section class="section" id="machineset-yaml-osp_creating-machineset-osp"><div class="titlepage"><div><div><h3 class="title">2.9.1. Sample YAML for a compute machine set custom resource on RHOSP</h3></div></div></div><p>
					This sample YAML defines a compute machine set that runs on Red Hat OpenStack Platform (RHOSP) and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/&lt;role&gt;: ""</code>.
				</p><p>
					In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;role&gt;</code> is the node label to add.
				</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO38-1"><!--Empty--></span><span class="callout">1</span>
    machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO38-2"><!--Empty--></span><span class="callout">2</span>
    machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO38-3"><!--Empty--></span><span class="callout">3</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO38-4"><!--Empty--></span><span class="callout">4</span>
  namespace: openshift-machine-api
spec:
  replicas: &lt;number_of_replicas&gt;
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO38-5"><!--Empty--></span><span class="callout">5</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO38-6"><!--Empty--></span><span class="callout">6</span>
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO38-7"><!--Empty--></span><span class="callout">7</span>
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO38-8"><!--Empty--></span><span class="callout">8</span>
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO38-9"><!--Empty--></span><span class="callout">9</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO38-10"><!--Empty--></span><span class="callout">10</span>
    spec:
      providerSpec:
        value:
          apiVersion: openstackproviderconfig.openshift.io/v1alpha1
          cloudName: openstack
          cloudsSecret:
            name: openstack-cloud-credentials
            namespace: openshift-machine-api
          flavor: &lt;nova_flavor&gt;
          image: &lt;glance_image_name_or_location&gt;
          serverGroupID: &lt;optional_UUID_of_server_group&gt; <span id="CO38-11"><!--Empty--></span><span class="callout">11</span>
          kind: OpenstackProviderSpec
          networks: <span id="CO38-12"><!--Empty--></span><span class="callout">12</span>
          - filter: {}
            subnets:
            - filter:
                name: &lt;subnet_name&gt;
                tags: openshiftClusterID=&lt;infrastructure_id&gt; <span id="CO38-13"><!--Empty--></span><span class="callout">13</span>
          primarySubnet: &lt;rhosp_subnet_UUID&gt; <span id="CO38-14"><!--Empty--></span><span class="callout">14</span>
          securityGroups:
          - filter: {}
            name: &lt;infrastructure_id&gt;-worker <span id="CO38-15"><!--Empty--></span><span class="callout">15</span>
          serverMetadata:
            Name: &lt;infrastructure_id&gt;-worker <span id="CO38-16"><!--Empty--></span><span class="callout">16</span>
            openshiftClusterID: &lt;infrastructure_id&gt; <span id="CO38-17"><!--Empty--></span><span class="callout">17</span>
          tags:
          - openshiftClusterID=&lt;infrastructure_id&gt; <span id="CO38-18"><!--Empty--></span><span class="callout">18</span>
          trunk: true
          userDataSecret:
            name: worker-user-data <span id="CO38-19"><!--Empty--></span><span class="callout">19</span>
          availabilityZone: &lt;optional_openstack_availability_zone&gt;</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO38-1"><span class="callout">1</span></a> <a href="#CO38-5"><span class="callout">5</span></a> <a href="#CO38-7"><span class="callout">7</span></a> <a href="#CO38-13"><span class="callout">13</span></a> <a href="#CO38-15"><span class="callout">15</span></a> <a href="#CO38-16"><span class="callout">16</span></a> <a href="#CO38-17"><span class="callout">17</span></a> <a href="#CO38-18"><span class="callout">18</span></a> </dt><dd><div class="para">
							Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
						</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><a href="#CO38-2"><span class="callout">2</span></a> <a href="#CO38-3"><span class="callout">3</span></a> <a href="#CO38-8"><span class="callout">8</span></a> <a href="#CO38-9"><span class="callout">9</span></a> <a href="#CO38-19"><span class="callout">19</span></a> </dt><dd><div class="para">
							Specify the node label to add.
						</div></dd><dt><a href="#CO38-4"><span class="callout">4</span></a> <a href="#CO38-6"><span class="callout">6</span></a> <a href="#CO38-10"><span class="callout">10</span></a> </dt><dd><div class="para">
							Specify the infrastructure ID and node label.
						</div></dd><dt><a href="#CO38-11"><span class="callout">11</span></a> </dt><dd><div class="para">
							To set a server group policy for the MachineSet, enter the value that is returned from <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.0/html/command_line_interface_reference/server#server_group_create">creating a server group</a>. For most deployments, <code class="literal">anti-affinity</code> or <code class="literal">soft-anti-affinity</code> policies are recommended.
						</div></dd><dt><a href="#CO38-12"><span class="callout">12</span></a> </dt><dd><div class="para">
							Required for deployments to multiple networks. To specify multiple networks, add another entry in the networks array. Also, you must include the network that is used as the <code class="literal">primarySubnet</code> value.
						</div></dd><dt><a href="#CO38-14"><span class="callout">14</span></a> </dt><dd><div class="para">
							Specify the RHOSP subnet that you want the endpoints of nodes to be published on. Usually, this is the same subnet that is used as the value of <code class="literal">machinesSubnet</code> in the <code class="literal">install-config.yaml</code> file.
						</div></dd></dl></div></section><section class="section" id="machineset-yaml-osp-sr-iov_creating-machineset-osp"><div class="titlepage"><div><div><h3 class="title">2.9.2. Sample YAML for a compute machine set custom resource that uses SR-IOV on RHOSP</h3></div></div></div><p>
					If you configured your cluster for single-root I/O virtualization (SR-IOV), you can create compute machine sets that use that technology.
				</p><p>
					This sample YAML defines a compute machine set that uses SR-IOV networks. The nodes that it creates are labeled with <code class="literal">node-role.openshift.io/&lt;node_role&gt;: ""</code>
				</p><p>
					In this sample, <code class="literal">infrastructure_id</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">node_role</code> is the node label to add.
				</p><p>
					The sample assumes two SR-IOV networks that are named "radio" and "uplink". The networks are used in port definitions in the <code class="literal">spec.template.spec.providerSpec.value.ports</code> list.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Only parameters that are specific to SR-IOV deployments are described in this sample. To review a more general sample, see "Sample YAML for a compute machine set custom resource on RHOSP".
					</p></div></div><div class="formalpara"><p class="title"><strong>An example compute machine set that uses SR-IOV networks</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
    machine.openshift.io/cluster-api-machine-role: &lt;node_role&gt;
    machine.openshift.io/cluster-api-machine-type: &lt;node_role&gt;
  name: &lt;infrastructure_id&gt;-&lt;node_role&gt;
  namespace: openshift-machine-api
spec:
  replicas: &lt;number_of_replicas&gt;
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;node_role&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;node_role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;node_role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;node_role&gt;
    spec:
      metadata:
      providerSpec:
        value:
          apiVersion: openstackproviderconfig.openshift.io/v1alpha1
          cloudName: openstack
          cloudsSecret:
            name: openstack-cloud-credentials
            namespace: openshift-machine-api
          flavor: &lt;nova_flavor&gt;
          image: &lt;glance_image_name_or_location&gt;
          serverGroupID: &lt;optional_UUID_of_server_group&gt;
          kind: OpenstackProviderSpec
          networks:
            - subnets:
              - UUID: &lt;machines_subnet_UUID&gt;
          ports:
            - networkID: &lt;radio_network_UUID&gt; <span id="CO39-1"><!--Empty--></span><span class="callout">1</span>
              nameSuffix: radio
              fixedIPs:
                - subnetID: &lt;radio_subnet_UUID&gt; <span id="CO39-2"><!--Empty--></span><span class="callout">2</span>
              tags:
                - sriov
                - radio
              vnicType: direct <span id="CO39-3"><!--Empty--></span><span class="callout">3</span>
              portSecurity: false <span id="CO39-4"><!--Empty--></span><span class="callout">4</span>
            - networkID: &lt;uplink_network_UUID&gt; <span id="CO39-5"><!--Empty--></span><span class="callout">5</span>
              nameSuffix: uplink
              fixedIPs:
                - subnetID: &lt;uplink_subnet_UUID&gt; <span id="CO39-6"><!--Empty--></span><span class="callout">6</span>
              tags:
                - sriov
                - uplink
              vnicType: direct <span id="CO39-7"><!--Empty--></span><span class="callout">7</span>
              portSecurity: false <span id="CO39-8"><!--Empty--></span><span class="callout">8</span>
          primarySubnet: &lt;machines_subnet_UUID&gt;
          securityGroups:
          - filter: {}
            name: &lt;infrastructure_id&gt;-&lt;node_role&gt;
          serverMetadata:
            Name: &lt;infrastructure_id&gt;-&lt;node_role&gt;
            openshiftClusterID: &lt;infrastructure_id&gt;
          tags:
          - openshiftClusterID=&lt;infrastructure_id&gt;
          trunk: true
          userDataSecret:
            name: &lt;node_role&gt;-user-data
          availabilityZone: &lt;optional_openstack_availability_zone&gt;</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO39-1"><span class="callout">1</span></a> <a href="#CO39-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Enter a network UUID for each port.
						</div></dd><dt><a href="#CO39-2"><span class="callout">2</span></a> <a href="#CO39-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							Enter a subnet UUID for each port.
						</div></dd><dt><a href="#CO39-3"><span class="callout">3</span></a> <a href="#CO39-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							The value of the <code class="literal">vnicType</code> parameter must be <code class="literal">direct</code> for each port.
						</div></dd><dt><a href="#CO39-4"><span class="callout">4</span></a> <a href="#CO39-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							The value of the <code class="literal">portSecurity</code> parameter must be <code class="literal">false</code> for each port.
						</div><p>
							You cannot set security groups and allowed address pairs for ports when port security is disabled. Setting security groups on the instance applies the groups to all ports that are attached to it.
						</p></dd></dl></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						After you deploy compute machines that are SR-IOV-capable, you must label them as such. For example, from a command line, enter:
					</p><pre class="programlisting language-terminal">$ oc label node &lt;NODE_NAME&gt; feature.node.kubernetes.io/network-sriov.capable="true"</pre></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Trunking is enabled for ports that are created by entries in the networks and subnets lists. The names of ports that are created from these lists follow the pattern <code class="literal">&lt;machine_name&gt;-&lt;nameSuffix&gt;</code>. The <code class="literal">nameSuffix</code> field is required in port definitions.
					</p><p>
						You can enable trunking for each port.
					</p><p>
						Optionally, you can add tags to ports as part of their <code class="literal">tags</code> lists.
					</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-openstack-nfv-preparing">Preparing to install a cluster that uses SR-IOV or OVS-DPDK on OpenStack</a>
						</li></ul></div></section><section class="section" id="machineset-yaml-osp-sr-iov-port-security_creating-machineset-osp"><div class="titlepage"><div><div><h3 class="title">2.9.3. Sample YAML for SR-IOV deployments where port security is disabled</h3></div></div></div><p>
					To create single-root I/O virtualization (SR-IOV) ports on a network that has port security disabled, define a compute machine set that includes the ports as items in the <code class="literal">spec.template.spec.providerSpec.value.ports</code> list. This difference from the standard SR-IOV compute machine set is due to the automatic security group and allowed address pair configuration that occurs for ports that are created by using the network and subnet interfaces.
				</p><p>
					Ports that you define for machines subnets require:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Allowed address pairs for the API and ingress virtual IP ports
						</li><li class="listitem">
							The compute security group
						</li><li class="listitem">
							Attachment to the machines network and subnet
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Only parameters that are specific to SR-IOV deployments where port security is disabled are described in this sample. To review a more general sample, see Sample YAML for a compute machine set custom resource that uses SR-IOV on RHOSP".
					</p></div></div><div class="formalpara"><p class="title"><strong>An example compute machine set that uses SR-IOV networks and has port security disabled</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
    machine.openshift.io/cluster-api-machine-role: &lt;node_role&gt;
    machine.openshift.io/cluster-api-machine-type: &lt;node_role&gt;
  name: &lt;infrastructure_id&gt;-&lt;node_role&gt;
  namespace: openshift-machine-api
spec:
  replicas: &lt;number_of_replicas&gt;
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;node_role&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;node_role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;node_role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;node_role&gt;
    spec:
      metadata: {}
      providerSpec:
        value:
          apiVersion: openstackproviderconfig.openshift.io/v1alpha1
          cloudName: openstack
          cloudsSecret:
            name: openstack-cloud-credentials
            namespace: openshift-machine-api
          flavor: &lt;nova_flavor&gt;
          image: &lt;glance_image_name_or_location&gt;
          kind: OpenstackProviderSpec
          ports:
            - allowedAddressPairs: <span id="CO40-1"><!--Empty--></span><span class="callout">1</span>
              - ipAddress: &lt;API_VIP_port_IP&gt;
              - ipAddress: &lt;ingress_VIP_port_IP&gt;
              fixedIPs:
                - subnetID: &lt;machines_subnet_UUID&gt; <span id="CO40-2"><!--Empty--></span><span class="callout">2</span>
              nameSuffix: nodes
              networkID: &lt;machines_network_UUID&gt; <span id="CO40-3"><!--Empty--></span><span class="callout">3</span>
              securityGroups:
                  - &lt;compute_security_group_UUID&gt; <span id="CO40-4"><!--Empty--></span><span class="callout">4</span>
            - networkID: &lt;SRIOV_network_UUID&gt;
              nameSuffix: sriov
              fixedIPs:
                - subnetID: &lt;SRIOV_subnet_UUID&gt;
              tags:
                - sriov
              vnicType: direct
              portSecurity: False
          primarySubnet: &lt;machines_subnet_UUID&gt;
          serverMetadata:
            Name: &lt;infrastructure_ID&gt;-&lt;node_role&gt;
            openshiftClusterID: &lt;infrastructure_id&gt;
          tags:
          - openshiftClusterID=&lt;infrastructure_id&gt;
          trunk: false
          userDataSecret:
            name: worker-user-data</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO40-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Specify allowed address pairs for the API and ingress ports.
						</div></dd><dt><a href="#CO40-2"><span class="callout">2</span></a> <a href="#CO40-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Specify the machines network and subnet.
						</div></dd><dt><a href="#CO40-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Specify the compute machines security group.
						</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Trunking is enabled for ports that are created by entries in the networks and subnets lists. The names of ports that are created from these lists follow the pattern <code class="literal">&lt;machine_name&gt;-&lt;nameSuffix&gt;</code>. The <code class="literal">nameSuffix</code> field is required in port definitions.
					</p><p>
						You can enable trunking for each port.
					</p><p>
						Optionally, you can add tags to ports as part of their <code class="literal">tags</code> lists.
					</p></div></div><p>
					If your cluster uses Kuryr and the RHOSP SR-IOV network has port security disabled, the primary port for compute machines must have:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The value of the <code class="literal">spec.template.spec.providerSpec.value.networks.portSecurityEnabled</code> parameter set to <code class="literal">false</code>.
						</li><li class="listitem">
							For each subnet, the value of the <code class="literal">spec.template.spec.providerSpec.value.networks.subnets.portSecurityEnabled</code> parameter set to <code class="literal">false</code>.
						</li><li class="listitem">
							The value of <code class="literal">spec.template.spec.providerSpec.value.securityGroups</code> set to empty: <code class="literal">[]</code>.
						</li></ul></div><div class="formalpara"><p class="title"><strong>An example section of a compute machine set for a cluster on Kuryr that uses SR-IOV and has port security disabled</strong></p><p>
						
<pre class="programlisting language-yaml">...
          networks:
            - subnets:
              - uuid: &lt;machines_subnet_UUID&gt;
                portSecurityEnabled: false
              portSecurityEnabled: false
          securityGroups: []
...</pre>

					</p></div><p>
					In that case, you can apply the compute security group to the primary VM interface after the VM is created. For example, from a command line:
				</p><pre class="programlisting language-terminal">$ openstack port set --enable-port-security --security-group &lt;infrastructure_id&gt;-&lt;node_role&gt; &lt;main_port_ID&gt;</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						After you deploy compute machines that are SR-IOV-capable, you must label them as such. For example, from a command line, enter:
					</p><pre class="programlisting language-terminal">$ oc label node &lt;NODE_NAME&gt; feature.node.kubernetes.io/network-sriov.capable="true"</pre></div></div></section><section class="section" id="machineset-creating_creating-machineset-osp"><div class="titlepage"><div><div><h3 class="title">2.9.4. Creating a compute machine set</h3></div></div></div><p>
					In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Deploy an OpenShift Container Platform cluster.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <code class="literal">&lt;file_name&gt;.yaml</code>.
						</p><p class="simpara">
							Ensure that you set the <code class="literal">&lt;clusterID&gt;</code> and <code class="literal">&lt;role&gt;</code> parameter values.
						</p></li><li class="listitem"><p class="simpara">
							Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									To list the compute machine sets in your cluster, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									To view values of a specific compute machine set custom resource (CR), run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machineset &lt;machineset_name&gt; \
  -n openshift-machine-api -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO41-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO41-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
    spec:
      providerSpec: <span id="CO41-3"><!--Empty--></span><span class="callout">3</span>
        ...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO41-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The cluster infrastructure ID.
										</div></dd><dt><a href="#CO41-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											A default node label.
										</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												For clusters that have user-provisioned infrastructure, a compute machine set can only create <code class="literal">worker</code> and <code class="literal">infra</code> type machines.
											</p></div></div></dd><dt><a href="#CO41-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											The values in the <code class="literal">&lt;providerSpec&gt;</code> section of the compute machine set CR are platform-specific. For more information about <code class="literal">&lt;providerSpec&gt;</code> parameters in the CR, see the sample compute machine set CR configuration for your provider.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">MachineSet</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							View the list of compute machine sets by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-infra-us-east-1a    1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

							</p></div><p class="simpara">
							When the new compute machine set is available, the <code class="literal">DESIRED</code> and <code class="literal">CURRENT</code> values match. If the compute machine set is not available, wait a few minutes and run the command again.
						</p></li></ul></div></section></section><section class="section" id="creating-machineset-rhv"><div class="titlepage"><div><div><h2 class="title">2.10. Creating a compute machine set on RHV</h2></div></div></div><p>
				You can create a different compute machine set to serve a specific purpose in your OpenShift Container Platform cluster on Red Hat Virtualization (RHV). For example, you might create infrastructure machine sets and related machines so that you can move supporting workloads to the new machines.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.
				</p><p>
					Clusters with the infrastructure platform type <code class="literal">none</code> cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.
				</p><p>
					To view the platform type for your cluster, run the following command:
				</p><pre class="programlisting language-terminal">$ oc get infrastructure cluster -o jsonpath='{.status.platform}'</pre></div></div><section class="section" id="machineset-yaml-rhv_creating-machineset-rhv"><div class="titlepage"><div><div><h3 class="title">2.10.1. Sample YAML for a compute machine set custom resource on RHV</h3></div></div></div><p>
					This sample YAML defines a compute machine set that runs on RHV and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/&lt;node_role&gt;: ""</code>.
				</p><p>
					In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;role&gt;</code> is the node label to add.
				</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO42-1"><!--Empty--></span><span class="callout">1</span>
    machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO42-2"><!--Empty--></span><span class="callout">2</span>
    machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO42-3"><!--Empty--></span><span class="callout">3</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO42-4"><!--Empty--></span><span class="callout">4</span>
  namespace: openshift-machine-api
spec:
  replicas: &lt;number_of_replicas&gt; <span id="CO42-5"><!--Empty--></span><span class="callout">5</span>
  Selector: <span id="CO42-6"><!--Empty--></span><span class="callout">6</span>
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO42-7"><!--Empty--></span><span class="callout">7</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO42-8"><!--Empty--></span><span class="callout">8</span>
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO42-9"><!--Empty--></span><span class="callout">9</span>
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO42-10"><!--Empty--></span><span class="callout">10</span>
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO42-11"><!--Empty--></span><span class="callout">11</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO42-12"><!--Empty--></span><span class="callout">12</span>
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/&lt;role&gt;: "" <span id="CO42-13"><!--Empty--></span><span class="callout">13</span>
      providerSpec:
        value:
          apiVersion: ovirtproviderconfig.machine.openshift.io/v1beta1
          cluster_id: &lt;ovirt_cluster_id&gt; <span id="CO42-14"><!--Empty--></span><span class="callout">14</span>
          template_name: &lt;ovirt_template_name&gt; <span id="CO42-15"><!--Empty--></span><span class="callout">15</span>
          sparse: &lt;boolean_value&gt; <span id="CO42-16"><!--Empty--></span><span class="callout">16</span>
          format: &lt;raw_or_cow&gt; <span id="CO42-17"><!--Empty--></span><span class="callout">17</span>
          cpu: <span id="CO42-18"><!--Empty--></span><span class="callout">18</span>
            sockets: &lt;number_of_sockets&gt; <span id="CO42-19"><!--Empty--></span><span class="callout">19</span>
            cores: &lt;number_of_cores&gt; <span id="CO42-20"><!--Empty--></span><span class="callout">20</span>
            threads: &lt;number_of_threads&gt; <span id="CO42-21"><!--Empty--></span><span class="callout">21</span>
          memory_mb: &lt;memory_size&gt; <span id="CO42-22"><!--Empty--></span><span class="callout">22</span>
          guaranteed_memory_mb:  &lt;memory_size&gt; <span id="CO42-23"><!--Empty--></span><span class="callout">23</span>
          os_disk: <span id="CO42-24"><!--Empty--></span><span class="callout">24</span>
            size_gb: &lt;disk_size&gt; <span id="CO42-25"><!--Empty--></span><span class="callout">25</span>
            storage_domain_id: &lt;storage_domain_UUID&gt; <span id="CO42-26"><!--Empty--></span><span class="callout">26</span>
          network_interfaces: <span id="CO42-27"><!--Empty--></span><span class="callout">27</span>
            vnic_profile_id:  &lt;vnic_profile_id&gt; <span id="CO42-28"><!--Empty--></span><span class="callout">28</span>
          credentialsSecret:
            name: ovirt-credentials <span id="CO42-29"><!--Empty--></span><span class="callout">29</span>
          kind: OvirtMachineProviderSpec
          type: &lt;workload_type&gt; <span id="CO42-30"><!--Empty--></span><span class="callout">30</span>
          auto_pinning_policy: &lt;auto_pinning_policy&gt; <span id="CO42-31"><!--Empty--></span><span class="callout">31</span>
          hugepages: &lt;hugepages&gt; <span id="CO42-32"><!--Empty--></span><span class="callout">32</span>
          affinityGroupsNames:
            - compute <span id="CO42-33"><!--Empty--></span><span class="callout">33</span>
          userDataSecret:
            name: worker-user-data</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO42-1"><span class="callout">1</span></a> <a href="#CO42-7"><span class="callout">7</span></a> <a href="#CO42-9"><span class="callout">9</span></a> </dt><dd><div class="para">
							Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI (<code class="literal">oc</code>) installed, you can obtain the infrastructure ID by running the following command:
						</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><a href="#CO42-2"><span class="callout">2</span></a> <a href="#CO42-3"><span class="callout">3</span></a> <a href="#CO42-10"><span class="callout">10</span></a> <a href="#CO42-11"><span class="callout">11</span></a> <a href="#CO42-13"><span class="callout">13</span></a> </dt><dd><div class="para">
							Specify the node label to add.
						</div></dd><dt><a href="#CO42-4"><span class="callout">4</span></a> <a href="#CO42-8"><span class="callout">8</span></a> <a href="#CO42-12"><span class="callout">12</span></a> </dt><dd><div class="para">
							Specify the infrastructure ID and node label. These two strings together cannot be longer than 35 characters.
						</div></dd><dt><a href="#CO42-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Specify the number of machines to create.
						</div></dd><dt><a href="#CO42-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							Selector for the machines.
						</div></dd><dt><a href="#CO42-14"><span class="callout">14</span></a> </dt><dd><div class="para">
							Specify the UUID for the RHV cluster to which this VM instance belongs.
						</div></dd><dt><a href="#CO42-15"><span class="callout">15</span></a> </dt><dd><div class="para">
							Specify the RHV VM template to use to create the machine.
						</div></dd><dt><a href="#CO42-16"><span class="callout">16</span></a> </dt><dd><div class="para">
							Setting this option to <code class="literal">false</code> enables preallocation of disks. The default is <code class="literal">true</code>. Setting <code class="literal">sparse</code> to <code class="literal">true</code> with <code class="literal">format</code> set to <code class="literal">raw</code> is not available for block storage domains. The <code class="literal">raw</code> format writes the entire virtual disk to the underlying physical disk.
						</div></dd><dt><a href="#CO42-17"><span class="callout">17</span></a> </dt><dd><div class="para">
							Can be set to <code class="literal">cow</code> or <code class="literal">raw</code>. The default is <code class="literal">cow</code>. The <code class="literal">cow</code> format is optimized for virtual machines.
						</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Preallocating disks on file storage domains writes zeroes to the file. This might not actually preallocate disks depending on the underlying storage.
							</p></div></div></dd><dt><a href="#CO42-18"><span class="callout">18</span></a> </dt><dd><div class="para">
							Optional: The CPU field contains the CPU configuration, including sockets, cores, and threads.
						</div></dd><dt><a href="#CO42-19"><span class="callout">19</span></a> </dt><dd><div class="para">
							Optional: Specify the number of sockets for a VM.
						</div></dd><dt><a href="#CO42-20"><span class="callout">20</span></a> </dt><dd><div class="para">
							Optional: Specify the number of cores per socket.
						</div></dd><dt><a href="#CO42-21"><span class="callout">21</span></a> </dt><dd><div class="para">
							Optional: Specify the number of threads per core.
						</div></dd><dt><a href="#CO42-22"><span class="callout">22</span></a> </dt><dd><div class="para">
							Optional: Specify the size of a VM’s memory in MiB.
						</div></dd><dt><a href="#CO42-23"><span class="callout">23</span></a> </dt><dd><div class="para">
							Optional: Specify the size of a virtual machine’s guaranteed memory in MiB. This is the amount of memory that is guaranteed not to be drained by the ballooning mechanism. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/administration_guide#memory_ballooning">Memory Ballooning</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/administration_guide#Cluster_Optimization_Settings_Explained">Optimization Settings Explained</a>.
						</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If you are using a version earlier than RHV 4.4.8, see <a class="link" href="https://access.redhat.com/articles/6454811">Guaranteed memory requirements for OpenShift on Red Hat Virtualization clusters</a>.
							</p></div></div></dd><dt><a href="#CO42-24"><span class="callout">24</span></a> </dt><dd><div class="para">
							Optional: Root disk of the node.
						</div></dd><dt><a href="#CO42-25"><span class="callout">25</span></a> </dt><dd><div class="para">
							Optional: Specify the size of the bootable disk in GiB.
						</div></dd><dt><a href="#CO42-26"><span class="callout">26</span></a> </dt><dd><div class="para">
							Optional: Specify the UUID of the storage domain for the compute node’s disks. If none is provided, the compute node is created on the same storage domain as the control nodes. (default)
						</div></dd><dt><a href="#CO42-27"><span class="callout">27</span></a> </dt><dd><div class="para">
							Optional: List of the network interfaces of the VM. If you include this parameter, OpenShift Container Platform discards all network interfaces from the template and creates new ones.
						</div></dd><dt><a href="#CO42-28"><span class="callout">28</span></a> </dt><dd><div class="para">
							Optional: Specify the vNIC profile ID.
						</div></dd><dt><a href="#CO42-29"><span class="callout">29</span></a> </dt><dd><div class="para">
							Specify the name of the secret object that holds the RHV credentials.
						</div></dd><dt><a href="#CO42-30"><span class="callout">30</span></a> </dt><dd><div class="para">
							Optional: Specify the workload type for which the instance is optimized. This value affects the <code class="literal">RHV VM</code> parameter. Supported values: <code class="literal">desktop</code>, <code class="literal">server</code> (default), <code class="literal">high_performance</code>. <code class="literal">high_performance</code> improves performance on the VM. Limitations exist, for example, you cannot access the VM with a graphical console. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/virtual_machine_management_guide/index#Configuring_High_Performance_Virtual_Machines_Templates_and_Pools">Configuring High Performance Virtual Machines, Templates, and Pools</a> in the <span class="emphasis"><em>Virtual Machine Management Guide</em></span>.
						</div></dd><dt><a href="#CO42-31"><span class="callout">31</span></a> </dt><dd><div class="para">
							Optional: AutoPinningPolicy defines the policy that automatically sets CPU and NUMA settings, including pinning to the host for this instance. Supported values: <code class="literal">none</code>, <code class="literal">resize_and_pin</code>. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/virtual_machine_management_guide/index#Setting_NUMA_Nodes">Setting NUMA Nodes</a> in the <span class="emphasis"><em>Virtual Machine Management Guide</em></span>.
						</div></dd><dt><a href="#CO42-32"><span class="callout">32</span></a> </dt><dd><div class="para">
							Optional: Hugepages is the size in KiB for defining hugepages in a VM. Supported values: <code class="literal">2048</code> or <code class="literal">1048576</code>. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/virtual_machine_management_guide/index#Configuring_Huge_Pages">Configuring Huge Pages</a> in the <span class="emphasis"><em>Virtual Machine Management Guide</em></span>.
						</div></dd><dt><a href="#CO42-33"><span class="callout">33</span></a> </dt><dd><div class="para">
							Optional: A list of affinity group names to be applied to the VMs. The affinity groups must exist in oVirt.
						</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Because RHV uses a template when creating a VM, if you do not specify a value for an optional parameter, RHV uses the value for that parameter that is specified in the template.
					</p></div></div></section><section class="section" id="machineset-creating_creating-machineset-rhv"><div class="titlepage"><div><div><h3 class="title">2.10.2. Creating a compute machine set</h3></div></div></div><p>
					In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Deploy an OpenShift Container Platform cluster.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <code class="literal">&lt;file_name&gt;.yaml</code>.
						</p><p class="simpara">
							Ensure that you set the <code class="literal">&lt;clusterID&gt;</code> and <code class="literal">&lt;role&gt;</code> parameter values.
						</p></li><li class="listitem"><p class="simpara">
							Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									To list the compute machine sets in your cluster, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									To view values of a specific compute machine set custom resource (CR), run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machineset &lt;machineset_name&gt; \
  -n openshift-machine-api -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO43-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO43-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
    spec:
      providerSpec: <span id="CO43-3"><!--Empty--></span><span class="callout">3</span>
        ...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO43-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The cluster infrastructure ID.
										</div></dd><dt><a href="#CO43-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											A default node label.
										</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												For clusters that have user-provisioned infrastructure, a compute machine set can only create <code class="literal">worker</code> and <code class="literal">infra</code> type machines.
											</p></div></div></dd><dt><a href="#CO43-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											The values in the <code class="literal">&lt;providerSpec&gt;</code> section of the compute machine set CR are platform-specific. For more information about <code class="literal">&lt;providerSpec&gt;</code> parameters in the CR, see the sample compute machine set CR configuration for your provider.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">MachineSet</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							View the list of compute machine sets by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-infra-us-east-1a    1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

							</p></div><p class="simpara">
							When the new compute machine set is available, the <code class="literal">DESIRED</code> and <code class="literal">CURRENT</code> values match. If the compute machine set is not available, wait a few minutes and run the command again.
						</p></li></ul></div></section></section><section class="section" id="creating-machineset-vsphere"><div class="titlepage"><div><div><h2 class="title">2.11. Creating a compute machine set on vSphere</h2></div></div></div><p>
				You can create a different compute machine set to serve a specific purpose in your OpenShift Container Platform cluster on VMware vSphere. For example, you might create infrastructure machine sets and related machines so that you can move supporting workloads to the new machines.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.
				</p><p>
					Clusters with the infrastructure platform type <code class="literal">none</code> cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.
				</p><p>
					To view the platform type for your cluster, run the following command:
				</p><pre class="programlisting language-terminal">$ oc get infrastructure cluster -o jsonpath='{.status.platform}'</pre></div></div><section class="section" id="machineset-yaml-vsphere_creating-machineset-vsphere"><div class="titlepage"><div><div><h3 class="title">2.11.1. Sample YAML for a compute machine set custom resource on vSphere</h3></div></div></div><p>
					This sample YAML defines a compute machine set that runs on VMware vSphere and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/&lt;role&gt;: ""</code>.
				</p><p>
					In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;role&gt;</code> is the node label to add.
				</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  creationTimestamp: null
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO44-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO44-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO44-3"><!--Empty--></span><span class="callout">3</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO44-4"><!--Empty--></span><span class="callout">4</span>
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO44-5"><!--Empty--></span><span class="callout">5</span>
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO44-6"><!--Empty--></span><span class="callout">6</span>
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO44-7"><!--Empty--></span><span class="callout">7</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO44-8"><!--Empty--></span><span class="callout">8</span>
    spec:
      metadata:
        creationTimestamp: null
        labels:
          node-role.kubernetes.io/&lt;role&gt;: "" <span id="CO44-9"><!--Empty--></span><span class="callout">9</span>
      providerSpec:
        value:
          apiVersion: vsphereprovider.openshift.io/v1beta1
          credentialsSecret:
            name: vsphere-cloud-credentials
          diskGiB: 120
          kind: VSphereMachineProviderSpec
          memoryMiB: 8192
          metadata:
            creationTimestamp: null
          network:
            devices:
            - networkName: "&lt;vm_network_name&gt;" <span id="CO44-10"><!--Empty--></span><span class="callout">10</span>
          numCPUs: 4
          numCoresPerSocket: 1
          snapshot: ""
          template: &lt;vm_template_name&gt; <span id="CO44-11"><!--Empty--></span><span class="callout">11</span>
          userDataSecret:
            name: worker-user-data
          workspace:
            datacenter: &lt;vcenter_datacenter_name&gt; <span id="CO44-12"><!--Empty--></span><span class="callout">12</span>
            datastore: &lt;vcenter_datastore_name&gt; <span id="CO44-13"><!--Empty--></span><span class="callout">13</span>
            folder: &lt;vcenter_vm_folder_path&gt; <span id="CO44-14"><!--Empty--></span><span class="callout">14</span>
            resourcepool: &lt;vsphere_resource_pool&gt; <span id="CO44-15"><!--Empty--></span><span class="callout">15</span>
            server: &lt;vcenter_server_ip&gt; <span id="CO44-16"><!--Empty--></span><span class="callout">16</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO44-1"><span class="callout">1</span></a> <a href="#CO44-3"><span class="callout">3</span></a> <a href="#CO44-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI (<code class="literal">oc</code>) installed, you can obtain the infrastructure ID by running the following command:
						</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><a href="#CO44-2"><span class="callout">2</span></a> <a href="#CO44-4"><span class="callout">4</span></a> <a href="#CO44-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							Specify the infrastructure ID and node label.
						</div></dd><dt><a href="#CO44-6"><span class="callout">6</span></a> <a href="#CO44-7"><span class="callout">7</span></a> <a href="#CO44-9"><span class="callout">9</span></a> </dt><dd><div class="para">
							Specify the node label to add.
						</div></dd><dt><a href="#CO44-10"><span class="callout">10</span></a> </dt><dd><div class="para">
							Specify the vSphere VM network to deploy the compute machine set to. This VM network must be where other compute machines reside in the cluster.
						</div></dd><dt><a href="#CO44-11"><span class="callout">11</span></a> </dt><dd><div class="para">
							Specify the vSphere VM template to use, such as <code class="literal">user-5ddjd-rhcos</code>.
						</div></dd><dt><a href="#CO44-12"><span class="callout">12</span></a> </dt><dd><div class="para">
							Specify the vCenter Datacenter to deploy the compute machine set on.
						</div></dd><dt><a href="#CO44-13"><span class="callout">13</span></a> </dt><dd><div class="para">
							Specify the vCenter Datastore to deploy the compute machine set on.
						</div></dd><dt><a href="#CO44-14"><span class="callout">14</span></a> </dt><dd><div class="para">
							Specify the path to the vSphere VM folder in vCenter, such as <code class="literal">/dc1/vm/user-inst-5ddjd</code>.
						</div></dd><dt><a href="#CO44-15"><span class="callout">15</span></a> </dt><dd><div class="para">
							Specify the vSphere resource pool for your VMs.
						</div></dd><dt><a href="#CO44-16"><span class="callout">16</span></a> </dt><dd><div class="para">
							Specify the vCenter server IP or fully qualified domain name.
						</div></dd></dl></div></section><section class="section" id="machineset-vsphere-requirements-user-provisioned-machine-sets_creating-machineset-vsphere"><div class="titlepage"><div><div><h3 class="title">2.11.2. Minimum required vCenter privileges for compute machine set management</h3></div></div></div><p>
					To manage compute machine sets in an OpenShift Container Platform cluster on vCenter, you must use an account with privileges to read, create, and delete the required resources. Using an account that has global administrative privileges is the simplest way to access all of the necessary permissions.
				</p><p>
					If you cannot use an account with global administrative privileges, you must create roles to grant the minimum required privileges. The following table lists the minimum vCenter roles and privileges that are required to create, scale, and delete compute machine sets and to delete machines in your OpenShift Container Platform cluster.
				</p><div class="example" id="idm140311147557520"><p class="title"><strong>Example 2.1. Minimum vCenter roles and privileges required for compute machine set management</strong></p><div class="example-contents"><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140311147551488" scope="col">vSphere object for role</th><th align="left" valign="top" id="idm140311151071600" scope="col">When required</th><th align="left" valign="top" id="idm140311151070512" scope="col">Required privileges</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140311147551488"> <p>
										vSphere vCenter
									</p>
									 </td><td align="left" valign="top" headers="idm140311151071600"> <p>
										Always
									</p>
									 </td><td align="left" valign="top" headers="idm140311151070512"> <p>
										<code class="literal">InventoryService.Tagging.AttachTag</code><br/><code class="literal">InventoryService.Tagging.CreateCategory</code><br/><code class="literal">InventoryService.Tagging.CreateTag</code><br/><code class="literal">InventoryService.Tagging.DeleteCategory</code><br/><code class="literal">InventoryService.Tagging.DeleteTag</code><br/><code class="literal">InventoryService.Tagging.EditCategory</code><br/><code class="literal">InventoryService.Tagging.EditTag</code><br/><code class="literal">Sessions.ValidateSession</code><br/><code class="literal">StorageProfile.Update</code><sup>1</sup><br/><code class="literal">StorageProfile.View</code><sup>1</sup>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140311147551488"> <p>
										vSphere vCenter Cluster
									</p>
									 </td><td align="left" valign="top" headers="idm140311151071600"> <p>
										Always
									</p>
									 </td><td align="left" valign="top" headers="idm140311151070512"> <p>
										<code class="literal">Resource.AssignVMToPool</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140311147551488"> <p>
										vSphere Datastore
									</p>
									 </td><td align="left" valign="top" headers="idm140311151071600"> <p>
										Always
									</p>
									 </td><td align="left" valign="top" headers="idm140311151070512"> <p>
										<code class="literal">Datastore.AllocateSpace</code><br/><code class="literal">Datastore.Browse</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140311147551488"> <p>
										vSphere Port Group
									</p>
									 </td><td align="left" valign="top" headers="idm140311151071600"> <p>
										Always
									</p>
									 </td><td align="left" valign="top" headers="idm140311151070512"> <p>
										<code class="literal">Network.Assign</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140311147551488"> <p>
										Virtual Machine Folder
									</p>
									 </td><td align="left" valign="top" headers="idm140311151071600"> <p>
										Always
									</p>
									 </td><td align="left" valign="top" headers="idm140311151070512"> <p>
										<code class="literal">VirtualMachine.Config.AddRemoveDevice</code><br/><code class="literal">VirtualMachine.Config.AdvancedConfig</code><br/><code class="literal">VirtualMachine.Config.Annotation</code><br/><code class="literal">VirtualMachine.Config.CPUCount</code><br/><code class="literal">VirtualMachine.Config.DiskExtend</code><br/><code class="literal">VirtualMachine.Config.Memory</code><br/><code class="literal">VirtualMachine.Config.Settings</code><br/><code class="literal">VirtualMachine.Interact.PowerOff</code><br/><code class="literal">VirtualMachine.Interact.PowerOn</code><br/><code class="literal">VirtualMachine.Inventory.CreateFromExisting</code><br/><code class="literal">VirtualMachine.Inventory.Delete</code><br/><code class="literal">VirtualMachine.Provisioning.Clone</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140311147551488"> <p>
										vSphere vCenter Datacenter
									</p>
									 </td><td align="left" valign="top" headers="idm140311151071600"> <p>
										If the installation program creates the virtual machine folder
									</p>
									 </td><td align="left" valign="top" headers="idm140311151070512"> <p>
										<code class="literal">Resource.AssignVMToPool</code><br/><code class="literal">VirtualMachine.Provisioning.DeployTemplate</code>
									</p>
									 </td></tr><tr><td colspan="3" align="left" valign="top" headers="idm140311147551488 idm140311151070512"> <p>
										<sup>1</sup> The <code class="literal">StorageProfile.Update</code> and <code class="literal">StorageProfile.View</code> permissions are required only for storage backends that use the Container Storage Interface (CSI).
									</p>
									 </td></tr></tbody></table></div></div></div><p>
					The following table details the permissions and propagation settings that are required for compute machine set management.
				</p><div class="example" id="idm140311151448208"><p class="title"><strong>Example 2.2. Required permissions and propagation settings</strong></p><div class="example-contents"><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140311146920400" scope="col">vSphere object</th><th align="left" valign="top" id="idm140311146919312" scope="col">Folder type</th><th align="left" valign="top" id="idm140311146918224" scope="col">Propagate to children</th><th align="left" valign="top" id="idm140311146917136" scope="col">Permissions required</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140311146920400"> <p>
										vSphere vCenter
									</p>
									 </td><td align="left" valign="top" headers="idm140311146919312"> <p>
										Always
									</p>
									 </td><td align="left" valign="top" headers="idm140311146918224"> <p>
										Not required
									</p>
									 </td><td align="left" valign="top" headers="idm140311146917136"> <p>
										Listed required privileges
									</p>
									 </td></tr><tr><td rowspan="2" align="left" valign="top" headers="idm140311146920400"> <p>
										vSphere vCenter Datacenter
									</p>
									 </td><td align="left" valign="top" headers="idm140311146919312"> <p>
										Existing folder
									</p>
									 </td><td align="left" valign="top" headers="idm140311146918224"> <p>
										Not required
									</p>
									 </td><td align="left" valign="top" headers="idm140311146917136"> <p>
										<code class="literal">ReadOnly</code> permission
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140311146919312"> <p>
										Installation program creates the folder
									</p>
									 </td><td align="left" valign="top" headers="idm140311146918224"> <p>
										Required
									</p>
									 </td><td align="left" valign="top" headers="idm140311146917136"> <p>
										Listed required privileges
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140311146920400"> <p>
										vSphere vCenter Cluster
									</p>
									 </td><td align="left" valign="top" headers="idm140311146919312"> <p>
										Always
									</p>
									 </td><td align="left" valign="top" headers="idm140311146918224"> <p>
										Required
									</p>
									 </td><td align="left" valign="top" headers="idm140311146917136"> <p>
										Listed required privileges
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140311146920400"> <p>
										vSphere vCenter Datastore
									</p>
									 </td><td align="left" valign="top" headers="idm140311146919312"> <p>
										Always
									</p>
									 </td><td align="left" valign="top" headers="idm140311146918224"> <p>
										Not required
									</p>
									 </td><td align="left" valign="top" headers="idm140311146917136"> <p>
										Listed required privileges
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140311146920400"> <p>
										vSphere Switch
									</p>
									 </td><td align="left" valign="top" headers="idm140311146919312"> <p>
										Always
									</p>
									 </td><td align="left" valign="top" headers="idm140311146918224"> <p>
										Not required
									</p>
									 </td><td align="left" valign="top" headers="idm140311146917136"> <p>
										<code class="literal">ReadOnly</code> permission
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140311146920400"> <p>
										vSphere Port Group
									</p>
									 </td><td align="left" valign="top" headers="idm140311146919312"> <p>
										Always
									</p>
									 </td><td align="left" valign="top" headers="idm140311146918224"> <p>
										Not required
									</p>
									 </td><td align="left" valign="top" headers="idm140311146917136"> <p>
										Listed required privileges
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140311146920400"> <p>
										vSphere vCenter Virtual Machine Folder
									</p>
									 </td><td align="left" valign="top" headers="idm140311146919312"> <p>
										Existing folder
									</p>
									 </td><td align="left" valign="top" headers="idm140311146918224"> <p>
										Required
									</p>
									 </td><td align="left" valign="top" headers="idm140311146917136"> <p>
										Listed required privileges
									</p>
									 </td></tr></tbody></table></div></div></div><p>
					For more information about creating an account with only the required privileges, see <a class="link" href="https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.security.doc/GUID-5372F580-5C23-4E9C-8A4E-EF1B4DD9033E.html">vSphere Permissions and User Management Tasks</a> in the vSphere documentation.
				</p></section><section class="section" id="compute-machineset-upi-reqs_creating-machineset-vsphere"><div class="titlepage"><div><div><h3 class="title">2.11.3. Requirements for clusters with user-provisioned infrastructure to use compute machine sets</h3></div></div></div><p>
					To use compute machine sets on clusters that have user-provisioned infrastructure, you must ensure that you cluster configuration supports using the Machine API.
				</p><h5 id="machineset-upi-reqs-infra-id_creating-machineset-vsphere">Obtaining the infrastructure ID</h5><p>
					To create compute machine sets, you must be able to supply the infrastructure ID for your cluster.
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To obtain the infrastructure ID for your cluster, run the following command:
						</p><pre class="programlisting language-terminal">$ oc get infrastructure cluster -o jsonpath='{.status.infrastructureName}'</pre></li></ul></div><h5 id="machineset-upi-reqs-vsphere-creds_creating-machineset-vsphere">Satisfying vSphere credentials requirements</h5><p>
					To use compute machine sets, the Machine API must be able to interact with vCenter. Credentials that authorize the Machine API components to interact with vCenter must exist in a secret in the <code class="literal">openshift-machine-api</code> namespace.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To determine whether the required credentials exist, run the following command:
						</p><pre class="programlisting language-terminal">$ oc get secret \
  -n openshift-machine-api vsphere-cloud-credentials \
  -o go-template='{{range $k,$v := .data}}{{printf "%s: " $k}}{{if not $v}}{{$v}}{{else}}{{$v | base64decode}}{{end}}{{"\n"}}{{end}}'</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
								
<pre class="programlisting language-terminal">&lt;vcenter-server&gt;.password=&lt;openshift-user-password&gt;
&lt;vcenter-server&gt;.username=&lt;openshift-user&gt;</pre>

							</p></div><p class="simpara">
							where <code class="literal">&lt;vcenter-server&gt;</code> is the IP address or fully qualified domain name (FQDN) of the vCenter server and <code class="literal">&lt;openshift-user&gt;</code> and <code class="literal">&lt;openshift-user-password&gt;</code> are the OpenShift Container Platform administrator credentials to use.
						</p></li><li class="listitem"><p class="simpara">
							If the secret does not exist, create it by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create secret generic vsphere-cloud-credentials \
  -n openshift-machine-api \
  --from-literal=&lt;vcenter-server&gt;.username=&lt;openshift-user&gt; --from-literal=&lt;vcenter-server&gt;.password=&lt;openshift-user-password&gt;</pre></li></ol></div><h5 id="machineset-upi-reqs-ignition-config_creating-machineset-vsphere">Satisfying Ignition configuration requirements</h5><p>
					Provisioning virtual machines (VMs) requires a valid Ignition configuration. The Ignition configuration contains the <code class="literal">machine-config-server</code> address and a system trust bundle for obtaining further Ignition configurations from the Machine Config Operator.
				</p><p>
					By default, this configuration is stored in the <code class="literal">worker-user-data</code> secret in the <code class="literal">machine-api-operator</code> namespace. Compute machine sets reference the secret during the machine creation process.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To determine whether the required secret exists, run the following command:
						</p><pre class="programlisting language-terminal">$ oc get secret \
  -n openshift-machine-api worker-user-data \
  -o go-template='{{range $k,$v := .data}}{{printf "%s: " $k}}{{if not $v}}{{$v}}{{else}}{{$v | base64decode}}{{end}}{{"\n"}}{{end}}'</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
								
<pre class="programlisting language-terminal">disableTemplating: false
userData: <span id="CO45-1"><!--Empty--></span><span class="callout">1</span>
  {
    "ignition": {
      ...
      },
    ...
  }</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO45-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The full output is omitted here, but should have this format.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							If the secret does not exist, create it by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create secret generic worker-user-data \
  -n openshift-machine-api \
  --from-file=&lt;installation_directory&gt;/worker.ign</pre><p class="simpara">
							where <code class="literal">&lt;installation_directory&gt;</code> is the directory that was used to store your installation assets during cluster installation.
						</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#understanding-the-machine-config-operator">Understanding the Machine Config Operator</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-vsphere-machines_installing-vsphere">Installing RHCOS and starting the OpenShift Container Platform bootstrap process</a>
						</li></ul></div></section><section class="section" id="machineset-creating_creating-machineset-vsphere"><div class="titlepage"><div><div><h3 class="title">2.11.4. Creating a compute machine set</h3></div></div></div><p>
					In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Clusters that are installed with user-provisioned infrastructure have a different networking stack than clusters with infrastructure that is provisioned by the installation program. As a result of this difference, automatic load balancer management is unsupported on clusters that have user-provisioned infrastructure. For these clusters, a compute machine set can only create <code class="literal">worker</code> and <code class="literal">infra</code> type machines.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Deploy an OpenShift Container Platform cluster.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
						</li><li class="listitem">
							Have the necessary permissions to deploy VMs in your vCenter instance and have the required access to the datastore specified.
						</li><li class="listitem">
							If your cluster uses user-provisioned infrastructure, you have satisfied the specific Machine API requirements for that configuration.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <code class="literal">&lt;file_name&gt;.yaml</code>.
						</p><p class="simpara">
							Ensure that you set the <code class="literal">&lt;clusterID&gt;</code> and <code class="literal">&lt;role&gt;</code> parameter values.
						</p></li><li class="listitem"><p class="simpara">
							Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									To list the compute machine sets in your cluster, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									To view values of a specific compute machine set custom resource (CR), run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machineset &lt;machineset_name&gt; \
  -n openshift-machine-api -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO46-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO46-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
    spec:
      providerSpec: <span id="CO46-3"><!--Empty--></span><span class="callout">3</span>
        ...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO46-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The cluster infrastructure ID.
										</div></dd><dt><a href="#CO46-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											A default node label.
										</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												For clusters that have user-provisioned infrastructure, a compute machine set can only create <code class="literal">worker</code> and <code class="literal">infra</code> type machines.
											</p></div></div></dd><dt><a href="#CO46-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											The values in the <code class="literal">&lt;providerSpec&gt;</code> section of the compute machine set CR are platform-specific. For more information about <code class="literal">&lt;providerSpec&gt;</code> parameters in the CR, see the sample compute machine set CR configuration for your provider.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									If you are creating a compute machine set for a cluster that has user-provisioned infrastructure, note the following important values:
								</p><div class="formalpara"><p class="title"><strong>Example vSphere <code class="literal">providerSpec</code> values</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
...
template:
  ...
  spec:
    providerSpec:
      value:
        apiVersion: machine.openshift.io/v1beta1
        credentialsSecret:
          name: vsphere-cloud-credentials <span id="CO47-1"><!--Empty--></span><span class="callout">1</span>
        diskGiB: 120
        kind: VSphereMachineProviderSpec
        memoryMiB: 16384
        network:
          devices:
            - networkName: "&lt;vm_network_name&gt;"
        numCPUs: 4
        numCoresPerSocket: 4
        snapshot: ""
        template: &lt;vm_template_name&gt; <span id="CO47-2"><!--Empty--></span><span class="callout">2</span>
        userDataSecret:
          name: worker-user-data <span id="CO47-3"><!--Empty--></span><span class="callout">3</span>
        workspace:
          datacenter: &lt;vcenter_datacenter_name&gt;
          datastore: &lt;vcenter_datastore_name&gt;
          folder: &lt;vcenter_vm_folder_path&gt;
          resourcepool: &lt;vsphere_resource_pool&gt;
          server: &lt;vcenter_server_address&gt; <span id="CO47-4"><!--Empty--></span><span class="callout">4</span></pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO47-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The name of the secret in the <code class="literal">openshift-machine-api</code> namespace that contains the required vCenter credentials.
										</div></dd><dt><a href="#CO47-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											The name of the RHCOS VM template for your cluster that was created during installation.
										</div></dd><dt><a href="#CO47-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											The name of the secret in the <code class="literal">openshift-machine-api</code> namespace that contains the required Ignition configuration credentials.
										</div></dd><dt><a href="#CO47-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											The IP address or fully qualified domain name (FQDN) of the vCenter server.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">MachineSet</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							View the list of compute machine sets by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-infra-us-east-1a    1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

							</p></div><p class="simpara">
							When the new compute machine set is available, the <code class="literal">DESIRED</code> and <code class="literal">CURRENT</code> values match. If the compute machine set is not available, wait a few minutes and run the command again.
						</p></li></ul></div></section></section><section class="section" id="creating-machineset-bare-metal"><div class="titlepage"><div><div><h2 class="title">2.12. Creating a compute machine set on bare metal</h2></div></div></div><p>
				You can create a different compute machine set to serve a specific purpose in your OpenShift Container Platform cluster on bare metal. For example, you might create infrastructure machine sets and related machines so that you can move supporting workloads to the new machines.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.
				</p><p>
					Clusters with the infrastructure platform type <code class="literal">none</code> cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.
				</p><p>
					To view the platform type for your cluster, run the following command:
				</p><pre class="programlisting language-terminal">$ oc get infrastructure cluster -o jsonpath='{.status.platform}'</pre></div></div><section class="section" id="machineset-yaml-vsphere_creating-machineset-bare-metal"><div class="titlepage"><div><div><h3 class="title">2.12.1. Sample YAML for a compute machine set custom resource on bare metal</h3></div></div></div><p>
					This sample YAML defines a compute machine set that runs on bare metal and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/&lt;role&gt;: ""</code>.
				</p><p>
					In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;role&gt;</code> is the node label to add.
				</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  creationTimestamp: null
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO48-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO48-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO48-3"><!--Empty--></span><span class="callout">3</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO48-4"><!--Empty--></span><span class="callout">4</span>
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO48-5"><!--Empty--></span><span class="callout">5</span>
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO48-6"><!--Empty--></span><span class="callout">6</span>
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO48-7"><!--Empty--></span><span class="callout">7</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO48-8"><!--Empty--></span><span class="callout">8</span>
    spec:
      metadata:
        creationTimestamp: null
        labels:
          node-role.kubernetes.io/&lt;role&gt;: "" <span id="CO48-9"><!--Empty--></span><span class="callout">9</span>
      providerSpec:
        value:
          apiVersion: baremetal.cluster.k8s.io/v1alpha1
          hostSelector: {}
          image:
            checksum: http:/172.22.0.3:6181/images/rhcos-&lt;version&gt;.&lt;architecture&gt;.qcow2.&lt;md5sum&gt; <span id="CO48-10"><!--Empty--></span><span class="callout">10</span>
            url: http://172.22.0.3:6181/images/rhcos-&lt;version&gt;.&lt;architecture&gt;.qcow2 <span id="CO48-11"><!--Empty--></span><span class="callout">11</span>
          kind: BareMetalMachineProviderSpec
          metadata:
            creationTimestamp: null
          userData:
            name: worker-user-data</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO48-1"><span class="callout">1</span></a> <a href="#CO48-3"><span class="callout">3</span></a> <a href="#CO48-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI (<code class="literal">oc</code>) installed, you can obtain the infrastructure ID by running the following command:
						</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><a href="#CO48-2"><span class="callout">2</span></a> <a href="#CO48-4"><span class="callout">4</span></a> <a href="#CO48-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							Specify the infrastructure ID and node label.
						</div></dd><dt><a href="#CO48-6"><span class="callout">6</span></a> <a href="#CO48-7"><span class="callout">7</span></a> <a href="#CO48-9"><span class="callout">9</span></a> </dt><dd><div class="para">
							Specify the node label to add.
						</div></dd><dt><a href="#CO48-10"><span class="callout">10</span></a> </dt><dd><div class="para">
							Edit the <code class="literal">checksum</code> URL to use the API VIP address.
						</div></dd><dt><a href="#CO48-11"><span class="callout">11</span></a> </dt><dd><div class="para">
							Edit the <code class="literal">url</code> URL to use the API VIP address.
						</div></dd></dl></div></section><section class="section" id="machineset-creating_creating-machineset-bare-metal"><div class="titlepage"><div><div><h3 class="title">2.12.2. Creating a compute machine set</h3></div></div></div><p>
					In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Deploy an OpenShift Container Platform cluster.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <code class="literal">&lt;file_name&gt;.yaml</code>.
						</p><p class="simpara">
							Ensure that you set the <code class="literal">&lt;clusterID&gt;</code> and <code class="literal">&lt;role&gt;</code> parameter values.
						</p></li><li class="listitem"><p class="simpara">
							Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									To list the compute machine sets in your cluster, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									To view values of a specific compute machine set custom resource (CR), run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machineset &lt;machineset_name&gt; \
  -n openshift-machine-api -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO49-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO49-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
    spec:
      providerSpec: <span id="CO49-3"><!--Empty--></span><span class="callout">3</span>
        ...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO49-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The cluster infrastructure ID.
										</div></dd><dt><a href="#CO49-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											A default node label.
										</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												For clusters that have user-provisioned infrastructure, a compute machine set can only create <code class="literal">worker</code> and <code class="literal">infra</code> type machines.
											</p></div></div></dd><dt><a href="#CO49-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											The values in the <code class="literal">&lt;providerSpec&gt;</code> section of the compute machine set CR are platform-specific. For more information about <code class="literal">&lt;providerSpec&gt;</code> parameters in the CR, see the sample compute machine set CR configuration for your provider.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">MachineSet</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							View the list of compute machine sets by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-infra-us-east-1a    1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

							</p></div><p class="simpara">
							When the new compute machine set is available, the <code class="literal">DESIRED</code> and <code class="literal">CURRENT</code> values match. If the compute machine set is not available, wait a few minutes and run the command again.
						</p></li></ul></div></section></section></section><section class="chapter" id="manually-scaling-machineset"><div class="titlepage"><div><div><h1 class="title">Chapter 3. Manually scaling a compute machine set</h1></div></div></div><p>
			You can add or remove an instance of a machine in a compute machine set.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				If you need to modify aspects of a compute machine set outside of scaling, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#modifying-machineset">Modifying a compute machine set</a>.
			</p></div></div><section class="section" id="prerequisites"><div class="titlepage"><div><div><h2 class="title">3.1. Prerequisites</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						If you enabled the cluster-wide proxy and scale up compute machines not included in <code class="literal">networking.machineNetwork[].cidr</code> from the installation configuration, you must <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-proxy-configure-object_config-cluster-wide-proxy">add the compute machines to the Proxy object’s <code class="literal">noProxy</code> field</a> to prevent connection issues.
					</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.
				</p><p>
					Clusters with the infrastructure platform type <code class="literal">none</code> cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.
				</p><p>
					To view the platform type for your cluster, run the following command:
				</p><pre class="programlisting language-terminal">$ oc get infrastructure cluster -o jsonpath='{.status.platform}'</pre></div></div></section><section class="section" id="machineset-manually-scaling_manually-scaling-machineset"><div class="titlepage"><div><div><h2 class="title">3.2. Scaling a compute machine set manually</h2></div></div></div><p>
				To add or remove an instance of a machine in a compute machine set, you can manually scale the compute machine set.
			</p><p>
				This guidance is relevant to fully automated, installer-provisioned infrastructure installations. Customized, user-provisioned infrastructure installations do not have compute machine sets.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install an OpenShift Container Platform cluster and the <code class="literal">oc</code> command line.
					</li><li class="listitem">
						Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						View the compute machine sets that are in the cluster by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><p class="simpara">
						The compute machine sets are listed in the form of <code class="literal">&lt;clusterid&gt;-worker-&lt;aws-region-az&gt;</code>.
					</p></li><li class="listitem"><p class="simpara">
						View the compute machines that are in the cluster by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get machine -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
						Set the annotation on the compute machine that you want to delete by running the following command:
					</p><pre class="programlisting language-terminal">$ oc annotate machine/&lt;machine_name&gt; -n openshift-machine-api machine.openshift.io/delete-machine="true"</pre></li><li class="listitem"><p class="simpara">
						Scale the compute machine set by running one of the following commands:
					</p><pre class="programlisting language-terminal">$ oc scale --replicas=2 machineset &lt;machineset&gt; -n openshift-machine-api</pre><p class="simpara">
						Or:
					</p><pre class="programlisting language-terminal">$ oc edit machineset &lt;machineset&gt; -n openshift-machine-api</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
						You can alternatively apply the following YAML to scale the compute machine set:
					</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: &lt;machineset&gt;
  namespace: openshift-machine-api
spec:
  replicas: 2</pre></div></div><p class="simpara">
						You can scale the compute machine set up or down. It takes several minutes for the new machines to be available.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							By default, the machine controller tries to drain the node that is backed by the machine until it succeeds. In some situations, such as with a misconfigured pod disruption budget, the drain operation might not be able to succeed. If the drain operation fails, the machine controller cannot proceed removing the machine.
						</p><p>
							You can skip draining the node by annotating <code class="literal">machine.openshift.io/exclude-node-draining</code> in a specific machine.
						</p></div></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Verify the deletion of the intended machine by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get machines</pre></li></ul></div></section><section class="section" id="machineset-delete-policy_manually-scaling-machineset"><div class="titlepage"><div><div><h2 class="title">3.3. The compute machine set deletion policy</h2></div></div></div><p>
				<code class="literal">Random</code>, <code class="literal">Newest</code>, and <code class="literal">Oldest</code> are the three supported deletion options. The default is <code class="literal">Random</code>, meaning that random machines are chosen and deleted when scaling compute machine sets down. The deletion policy can be set according to the use case by modifying the particular compute machine set:
			</p><pre class="programlisting language-yaml">spec:
  deletePolicy: &lt;delete_policy&gt;
  replicas: &lt;desired_replica_count&gt;</pre><p>
				Specific machines can also be prioritized for deletion by adding the annotation <code class="literal">machine.openshift.io/delete-machine=true</code> to the machine of interest, regardless of the deletion policy.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					By default, the OpenShift Container Platform router pods are deployed on workers. Because the router is required to access some cluster resources, including the web console, do not scale the worker compute machine set to <code class="literal">0</code> unless you first relocate the router pods.
				</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Custom compute machine sets can be used for use cases requiring that services run on specific nodes and that those services are ignored by the controller when the worker compute machine sets are scaling down. This prevents service disruption.
				</p></div></div></section><section class="section _additional-resources" id="additional-resources_manually-scaling-machineset"><div class="titlepage"><div><div><h2 class="title">3.4. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machine-lifecycle-hook-deletion_deleting-machine">Lifecycle hooks for the machine deletion phase</a>
					</li></ul></div></section></section><section class="chapter" id="modifying-machineset"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Modifying a compute machine set</h1></div></div></div><p>
			You can modify a compute machine set, such as adding labels, changing the instance type, or changing block storage.
		</p><p>
			On Red Hat Virtualization (RHV), you can also change a compute machine set to provision new nodes on a different storage domain.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				If you need to scale a compute machine set without making other changes, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#manually-scaling-machineset">Manually scaling a compute machine set</a>.
			</p></div></div><section class="section" id="machineset-modifying_modifying-machineset"><div class="titlepage"><div><div><h2 class="title">4.1. Modifying a compute machine set</h2></div></div></div><p>
				To make changes to a compute machine set, edit the <code class="literal">MachineSet</code> YAML. Then, remove all machines associated with the compute machine set by deleting each machine or scaling down the compute machine set to <code class="literal">0</code> replicas. Then, scale the replicas back to the desired number. Changes you make to a compute machine set do not affect existing machines.
			</p><p>
				If you need to scale a compute machine set without making other changes, you do not need to delete the machines.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					By default, the OpenShift Container Platform router pods are deployed on workers. Because the router is required to access some cluster resources, including the web console, do not scale the compute machine set to <code class="literal">0</code> unless you first relocate the router pods.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install an OpenShift Container Platform cluster and the <code class="literal">oc</code> command line.
					</li><li class="listitem">
						Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Edit the compute machine set by running the following command:
					</p><pre class="programlisting language-terminal">$ oc edit machineset &lt;machineset&gt; -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
						Scale down the compute machine set to <code class="literal">0</code> by running one of the following commands:
					</p><pre class="programlisting language-terminal">$ oc scale --replicas=0 machineset &lt;machineset&gt; -n openshift-machine-api</pre><p class="simpara">
						Or:
					</p><pre class="programlisting language-terminal">$ oc edit machineset &lt;machineset&gt; -n openshift-machine-api</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
						You can alternatively apply the following YAML to scale the compute machine set:
					</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: &lt;machineset&gt;
  namespace: openshift-machine-api
spec:
  replicas: 0</pre></div></div><p class="simpara">
						Wait for the machines to be removed.
					</p></li><li class="listitem"><p class="simpara">
						Scale up the compute machine set as needed by running one of the following commands:
					</p><pre class="programlisting language-terminal">$ oc scale --replicas=2 machineset &lt;machineset&gt; -n openshift-machine-api</pre><p class="simpara">
						Or:
					</p><pre class="programlisting language-terminal">$ oc edit machineset &lt;machineset&gt; -n openshift-machine-api</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
						You can alternatively apply the following YAML to scale the compute machine set:
					</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: &lt;machineset&gt;
  namespace: openshift-machine-api
spec:
  replicas: 2</pre></div></div><p class="simpara">
						Wait for the machines to start. The new machines contain changes you made to the compute machine set.
					</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machine-lifecycle-hook-deletion_deleting-machine">Lifecycle hooks for the machine deletion phase</a>
					</li></ul></div></section><section class="section" id="migrating-nodes-to-a-different-storage-domain-rhv_modifying-machineset"><div class="titlepage"><div><div><h2 class="title">4.2. Migrating nodes to a different storage domain on RHV</h2></div></div></div><p>
				You can migrate the OpenShift Container Platform control plane and compute nodes to a different storage domain in a Red Hat Virtualization (RHV) cluster.
			</p><section class="section" id="machineset-migrating-compute-nodes-to-diff-sd-rhv_modifying-machineset"><div class="titlepage"><div><div><h3 class="title">4.2.1. Migrating compute nodes to a different storage domain in RHV</h3></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You are logged in to the Manager.
						</li><li class="listitem">
							You have the name of the target storage domain.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Identify the virtual machine template by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.items[0].spec.template.spec.providerSpec.value.template_name}{"\n"}' machineset -A</pre></li><li class="listitem"><p class="simpara">
							Create a new virtual machine in the Manager, based on the template you identified. Leave all other settings unchanged. For details, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/virtual_machine_management_guide/index#Creating_a_Virtual_Machine_Based_on_a_Template">Creating a Virtual Machine Based on a Template</a> in the Red Hat Virtualization <span class="emphasis"><em>Virtual Machine Management Guide</em></span>.
						</p><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You do not need to start the new virtual machine.
						</p></div></div></li><li class="listitem">
							Create a new template from the new virtual machine. Specify the target storage domain under <span class="strong strong"><strong>Target</strong></span>. For details, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/virtual_machine_management_guide/index#Creating_a_template_from_an_existing_virtual_machine">Creating a Template</a> in the Red Hat Virtualization <span class="emphasis"><em>Virtual Machine Management Guide</em></span>.
						</li><li class="listitem"><p class="simpara">
							Add a new compute machine set to the OpenShift Container Platform cluster with the new template.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Get the details of the current compute machine set by running the following command:
								</p><pre class="programlisting language-terminal">$ oc get machineset -o yaml</pre></li><li class="listitem"><p class="simpara">
									Use these details to create a compute machine set. For more information see <span class="emphasis"><em>Creating a compute machine set</em></span>.
								</p><p class="simpara">
									Enter the new virtual machine template name in the <span class="strong strong"><strong>template_name</strong></span> field. Use the same template name you used in the <span class="strong strong"><strong>New template</strong></span> dialog in the Manager.
								</p></li><li class="listitem">
									Note the names of both the old and new compute machine sets. You need to refer to them in subsequent steps.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Migrate the workloads.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Scale up the new compute machine set. For details on manually scaling compute machine sets, see <span class="emphasis"><em>Scaling a compute machine set manually</em></span>.
								</p><p class="simpara">
									OpenShift Container Platform moves the pods to an available worker when the old machine is removed.
								</p></li><li class="listitem">
									Scale down the old compute machine set.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Remove the old compute machine set by running the following command:
						</p><pre class="programlisting language-terminal">$ oc delete machineset &lt;machineset-name&gt;</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machineset-creating_creating-machineset-rhv">Creating a compute machine set</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machineset-manually-scaling_manually-scaling-machineset">Scaling a compute machine set manually</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-about">Controlling pod placement using the scheduler</a>
						</li></ul></div></section><section class="section" id="machineset-migrating-control-plane-nodes-to-diff-sd-rhv_modifying-machineset"><div class="titlepage"><div><div><h3 class="title">4.2.2. Migrating control plane nodes to a different storage domain on RHV</h3></div></div></div><p>
					OpenShift Container Platform does not manage control plane nodes, so they are easier to migrate than compute nodes. You can migrate them like any other virtual machine on Red Hat Virtualization (RHV).
				</p><p>
					Perform this procedure for each node separately.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You are logged in to the Manager.
						</li><li class="listitem">
							You have identified the control plane nodes. They are labeled <span class="strong strong"><strong>master</strong></span> in the Manager.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Select the virtual machine labeled <span class="strong strong"><strong>master</strong></span>.
						</li><li class="listitem">
							Shut down the virtual machine.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>Disks</strong></span> tab.
						</li><li class="listitem">
							Click the virtual machine’s disk.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>More Actions</strong></span> <span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Machine_management-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
							 and select <span class="strong strong"><strong>Move</strong></span>.
						</li><li class="listitem">
							Select the target storage domain and wait for the migration process to complete.
						</li><li class="listitem">
							Start the virtual machine.
						</li><li class="listitem"><p class="simpara">
							Verify that the OpenShift Container Platform cluster is stable:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><p class="simpara">
							The output should display the node with the status <code class="literal">Ready</code>.
						</p></li><li class="listitem">
							Repeat this procedure for each control plane node.
						</li></ol></div></section></section></section><section class="chapter" id="deleting-machine"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Deleting a machine</h1></div></div></div><p>
			You can delete a specific machine.
		</p><section class="section" id="machine-delete_deleting-machine"><div class="titlepage"><div><div><h2 class="title">5.1. Deleting a specific machine</h2></div></div></div><p>
				You can delete a specific machine.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Do not delete a control plane machine unless your cluster uses a control plane machine set.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install an OpenShift Container Platform cluster.
					</li><li class="listitem">
						Install the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						View the machines that are in the cluster by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get machine -n openshift-machine-api</pre><p class="simpara">
						The command output contains a list of machines in the <code class="literal">&lt;clusterid&gt;-&lt;role&gt;-&lt;cloud_region&gt;</code> format.
					</p></li><li class="listitem">
						Identify the machine that you want to delete.
					</li><li class="listitem"><p class="simpara">
						Delete the machine by running the following command:
					</p><pre class="programlisting language-terminal">$ oc delete machine &lt;machine&gt; -n openshift-machine-api</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							By default, the machine controller tries to drain the node that is backed by the machine until it succeeds. In some situations, such as with a misconfigured pod disruption budget, the drain operation might not be able to succeed. If the drain operation fails, the machine controller cannot proceed removing the machine.
						</p><p>
							You can skip draining the node by annotating <code class="literal">machine.openshift.io/exclude-node-draining</code> in a specific machine.
						</p></div></div><p class="simpara">
						If the machine that you delete belongs to a machine set, a new machine is immediately created to satisfy the specified number of replicas.
					</p></li></ol></div></section><section class="section" id="machine-lifecycle-hook-deletion_deleting-machine"><div class="titlepage"><div><div><h2 class="title">5.2. Lifecycle hooks for the machine deletion phase</h2></div></div></div><p>
				Machine lifecycle hooks are points in the reconciliation lifecycle of a machine where the normal lifecycle process can be interrupted. In the machine <code class="literal">Deleting</code> phase, these interruptions provide the opportunity for components to modify the machine deletion process.
			</p><section class="section" id="machine-lifecycle-hook-deletion-terms_deleting-machine"><div class="titlepage"><div><div><h3 class="title">5.2.1. Terminology and definitions</h3></div></div></div><p>
					To understand the behavior of lifecycle hooks for the machine deletion phase, you must understand the following concepts:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Reconciliation</span></dt><dd>
								Reconciliation is the process by which a controller attempts to make the real state of the cluster and the objects that it comprises match the requirements in an object specification.
							</dd><dt><span class="term">Machine controller</span></dt><dd><p class="simpara">
								The machine controller manages the reconciliation lifecycle for a machine. For machines on cloud platforms, the machine controller is the combination of an OpenShift Container Platform controller and a platform-specific actuator from the cloud provider.
							</p><p class="simpara">
								In the context of machine deletion, the machine controller performs the following actions:
							</p></dd></dl></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Drain the node that is backed by the machine.
						</li><li class="listitem">
							Delete the machine instance from the cloud provider.
						</li><li class="listitem">
							Delete the <code class="literal">Node</code> object.
						</li></ul></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Lifecycle hook</span></dt><dd><p class="simpara">
								A lifecycle hook is a defined point in the reconciliation lifecycle of an object where the normal lifecycle process can be interrupted. Components can use a lifecycle hook to inject changes into the process to accomplish a desired outcome.
							</p><p class="simpara">
								There are two lifecycle hooks in the machine <code class="literal">Deleting</code> phase:
							</p></dd></dl></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">preDrain</code> lifecycle hooks must be resolved before the node that is backed by the machine can be drained.
						</li><li class="listitem">
							<code class="literal">preTerminate</code> lifecycle hooks must be resolved before the instance can be removed from the infrastructure provider.
						</li></ul></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Hook-implementing controller</span></dt><dd><p class="simpara">
								A hook-implementing controller is a controller, other than the machine controller, that can interact with a lifecycle hook. A hook-implementing controller can do one or more of the following actions:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Add a lifecycle hook.
									</li><li class="listitem">
										Respond to a lifecycle hook.
									</li><li class="listitem">
										Remove a lifecycle hook.
									</li></ul></div><p class="simpara">
								Each lifecycle hook has a single hook-implementing controller, but a hook-implementing controller can manage one or more hooks.
							</p></dd></dl></div></section><section class="section" id="machine-lifecycle-hook-deletion-order_deleting-machine"><div class="titlepage"><div><div><h3 class="title">5.2.2. Machine deletion processing order</h3></div></div></div><p>
					In OpenShift Container Platform 4.13, there are two lifecycle hooks for the machine deletion phase: <code class="literal">preDrain</code> and <code class="literal">preTerminate</code>. When all hooks for a given lifecycle point are removed, reconciliation continues as normal.
				</p><div class="figure" id="idm140311138926688"><p class="title"><strong>Figure 5.1. Machine deletion flow</strong></p><div class="figure-contents"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Machine_management-en-US/images/7072018272ecd1a19846843f926ad414/310_OpenShift_machine_deletion_hooks_0223.png" alt="The sequence of events in the machine `Deleting` phase."/></div></div></div><p>
					The machine <code class="literal">Deleting</code> phase proceeds in the following order:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							An existing machine is slated for deletion for one of the following reasons:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									A user with <code class="literal">cluster-admin</code> permissions uses the <code class="literal">oc delete machine</code> command.
								</li><li class="listitem">
									The machine gets a <code class="literal">machine.openshift.io/delete-machine</code> annotation.
								</li><li class="listitem">
									The machine set that manages the machine marks it for deletion to reduce the replica count as part of reconciliation.
								</li><li class="listitem">
									The cluster autoscaler identifies a node that is unnecessary to meet the deployment needs of the cluster.
								</li><li class="listitem">
									A machine health check is configured to replace an unhealthy machine.
								</li></ul></div></li><li class="listitem">
							The machine enters the <code class="literal">Deleting</code> phase, in which it is marked for deletion but is still present in the API.
						</li><li class="listitem"><p class="simpara">
							If a <code class="literal">preDrain</code> lifecycle hook exists, the hook-implementing controller that manages it does a specified action.
						</p><p class="simpara">
							Until all <code class="literal">preDrain</code> lifecycle hooks are satisfied, the machine status condition <code class="literal">Drainable</code> is set to <code class="literal">False</code>.
						</p></li><li class="listitem">
							There are no unresolved <code class="literal">preDrain</code> lifecycle hooks and the machine status condition <code class="literal">Drainable</code> is set to <code class="literal">True</code>.
						</li><li class="listitem"><p class="simpara">
							The machine controller attempts to drain the node that is backed by the machine.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									If draining fails, <code class="literal">Drained</code> is set to <code class="literal">False</code> and the machine controller attempts to drain the node again.
								</li><li class="listitem">
									If draining succeeds, <code class="literal">Drained</code> is set to <code class="literal">True</code>.
								</li></ul></div></li><li class="listitem">
							The machine status condition <code class="literal">Drained</code> is set to <code class="literal">True</code>.
						</li><li class="listitem"><p class="simpara">
							If a <code class="literal">preTerminate</code> lifecycle hook exists, the hook-implementing controller that manages it does a specified action.
						</p><p class="simpara">
							Until all <code class="literal">preTerminate</code> lifecycle hooks are satisfied, the machine status condition <code class="literal">Terminable</code> is set to <code class="literal">False</code>.
						</p></li><li class="listitem">
							There are no unresolved <code class="literal">preTerminate</code> lifecycle hooks and the machine status condition <code class="literal">Terminable</code> is set to <code class="literal">True</code>.
						</li><li class="listitem">
							The machine controller removes the instance from the infrastructure provider.
						</li><li class="listitem">
							The machine controller deletes the <code class="literal">Node</code> object.
						</li></ol></div></section><section class="section" id="machine-lifecycle-hook-deletion-format_deleting-machine"><div class="titlepage"><div><div><h3 class="title">5.2.3. Deletion lifecycle hook configuration</h3></div></div></div><p>
					The following YAML snippets demonstrate the format and placement of deletion lifecycle hook configurations within a machine set:
				</p><div class="formalpara"><p class="title"><strong>YAML snippet demonstrating a <code class="literal">preDrain</code> lifecycle hook</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: Machine
metadata:
  ...
spec:
  lifecycleHooks:
    preDrain:
    - name: &lt;hook_name&gt; <span id="CO50-1"><!--Empty--></span><span class="callout">1</span>
      owner: &lt;hook_owner&gt; <span id="CO50-2"><!--Empty--></span><span class="callout">2</span>
  ...</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO50-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The name of the <code class="literal">preDrain</code> lifecycle hook.
						</div></dd><dt><a href="#CO50-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							The hook-implementing controller that manages the <code class="literal">preDrain</code> lifecycle hook.
						</div></dd></dl></div><div class="formalpara"><p class="title"><strong>YAML snippet demonstrating a <code class="literal">preTerminate</code> lifecycle hook</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: Machine
metadata:
  ...
spec:
  lifecycleHooks:
    preTerminate:
    - name: &lt;hook_name&gt; <span id="CO51-1"><!--Empty--></span><span class="callout">1</span>
      owner: &lt;hook_owner&gt; <span id="CO51-2"><!--Empty--></span><span class="callout">2</span>
  ...</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO51-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The name of the <code class="literal">preTerminate</code> lifecycle hook.
						</div></dd><dt><a href="#CO51-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							The hook-implementing controller that manages the <code class="literal">preTerminate</code> lifecycle hook.
						</div></dd></dl></div><h5 id="machine-lifecycle-hook-deletion-example_deleting-machine">Example lifecycle hook configuration</h5><p>
					The following example demonstrates the implementation of multiple fictional lifecycle hooks that interrupt the machine deletion process:
				</p><div class="formalpara"><p class="title"><strong>Example configuration for lifecycle hooks</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: Machine
metadata:
  ...
spec:
  lifecycleHooks:
    preDrain: <span id="CO52-1"><!--Empty--></span><span class="callout">1</span>
    - name: MigrateImportantApp
      owner: my-app-migration-controller
    preTerminate: <span id="CO52-2"><!--Empty--></span><span class="callout">2</span>
    - name: BackupFileSystem
      owner: my-backup-controller
    - name: CloudProviderSpecialCase
      owner: my-custom-storage-detach-controller <span id="CO52-3"><!--Empty--></span><span class="callout">3</span>
    - name: WaitForStorageDetach
      owner: my-custom-storage-detach-controller
  ...</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO52-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							A <code class="literal">preDrain</code> lifecycle hook stanza that contains a single lifecycle hook.
						</div></dd><dt><a href="#CO52-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							A <code class="literal">preTerminate</code> lifecycle hook stanza that contains three lifecycle hooks.
						</div></dd><dt><a href="#CO52-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							A hook-implementing controller that manages two <code class="literal">preTerminate</code> lifecycle hooks: <code class="literal">CloudProviderSpecialCase</code> and <code class="literal">WaitForStorageDetach</code>.
						</div></dd></dl></div></section><section class="section" id="machine-lifecycle-hook-deletion-uses_deleting-machine"><div class="titlepage"><div><div><h3 class="title">5.2.4. Machine deletion lifecycle hook examples for Operator developers</h3></div></div></div><p>
					Operators can use lifecycle hooks for the machine deletion phase to modify the machine deletion process. The following examples demonstrate possible ways that an Operator can use this functionality.
				</p><h5 id="machine-lifecycle-hook-deletion-uses-predrain_deleting-machine">Example use cases for <code class="literal">preDrain</code> lifecycle hooks</h5><div class="variablelist"><dl class="variablelist"><dt><span class="term">Proactively replacing machines</span></dt><dd>
								An Operator can use a <code class="literal">preDrain</code> lifecycle hook to ensure that a replacement machine is successfully created and joined to the cluster before removing the instance of a deleted machine. This can mitigate the impact of disruptions during machine replacement or of replacement instances that do not initialize promptly.
							</dd><dt><span class="term">Implementing custom draining logic</span></dt><dd><p class="simpara">
								An Operator can use a <code class="literal">preDrain</code> lifecycle hook to replace the machine controller draining logic with a different draining controller. By replacing the draining logic, the Operator would have more flexibility and control over the lifecycle of the workloads on each node.
							</p><p class="simpara">
								For example, the machine controller drain libraries do not support ordering, but a custom drain provider could provide this functionality. By using a custom drain provider, an Operator could prioritize moving mission-critical applications before draining the node to ensure that service interruptions are minimized in cases where cluster capacity is limited.
							</p></dd></dl></div><h5 id="machine-lifecycle-hook-deletion-uses-preterminate_deleting-machine">Example use cases for <code class="literal">preTerminate</code> lifecycle hooks</h5><div class="variablelist"><dl class="variablelist"><dt><span class="term">Verifying storage detachment</span></dt><dd>
								An Operator can use a <code class="literal">preTerminate</code> lifecycle hook to ensure that storage that is attached to a machine is detached before the machine is removed from the infrastructure provider.
							</dd><dt><span class="term">Improving log reliability</span></dt><dd><p class="simpara">
								After a node is drained, the log exporter daemon requires some time to synchronize logs to the centralized logging system.
							</p><p class="simpara">
								A logging Operator can use a <code class="literal">preTerminate</code> lifecycle hook to add a delay between when the node drains and when the machine is removed from the infrastructure provider. This delay would provide time for the Operator to ensure that the main workloads are removed and no longer adding to the log backlog. When no new data is being added to the log backlog, the log exporter can catch up on the synchronization process, thus ensuring that all application logs are captured.
							</p></dd></dl></div></section><section class="section" id="machine-lifecycle-hook-deletion-etcd_deleting-machine"><div class="titlepage"><div><div><h3 class="title">5.2.5. Quorum protection with machine lifecycle hooks</h3></div></div></div><p>
					For OpenShift Container Platform clusters that use the Machine API Operator, the etcd Operator uses lifecycle hooks for the machine deletion phase to implement a quorum protection mechanism.
				</p><p>
					By using a <code class="literal">preDrain</code> lifecycle hook, the etcd Operator can control when the pods on a control plane machine are drained and removed. To protect etcd quorum, the etcd Operator prevents the removal of an etcd member until it migrates that member onto a new node within the cluster.
				</p><p>
					This mechanism allows the etcd Operator precise control over the members of the etcd quorum and allows the Machine API Operator to safely create and remove control plane machines without specific operational knowledge of the etcd cluster.
				</p><section class="section" id="machine-lifecycle-hook-deletion-etcd-order_deleting-machine"><div class="titlepage"><div><div><h4 class="title">5.2.5.1. Control plane deletion with quorum protection processing order</h4></div></div></div><p>
						When a control plane machine is replaced on a cluster that uses a control plane machine set, the cluster temporarily has four control plane machines. When the fourth control plane node joins the cluster, the etcd Operator starts a new etcd member on the replacement node. When the etcd Operator observes that the old control plane machine is marked for deletion, it stops the etcd member on the old node and promotes the replacement etcd member to join the quorum of the cluster.
					</p><p>
						The control plane machine <code class="literal">Deleting</code> phase proceeds in the following order:
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								A control plane machine is slated for deletion.
							</li><li class="listitem">
								The control plane machine enters the <code class="literal">Deleting</code> phase.
							</li><li class="listitem"><p class="simpara">
								To satisfy the <code class="literal">preDrain</code> lifecycle hook, the etcd Operator takes the following actions:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										The etcd Operator waits until a fourth control plane machine is added to the cluster as an etcd member. This new etcd member has a state of <code class="literal">Running</code> but not <code class="literal">ready</code> until it receives the full database update from the etcd leader.
									</li><li class="listitem">
										When the new etcd member receives the full database update, the etcd Operator promotes the new etcd member to a voting member and removes the old etcd member from the cluster.
									</li></ol></div><p class="simpara">
								After this transition is complete, it is safe for the old etcd pod and its data to be removed, so the <code class="literal">preDrain</code> lifecycle hook is removed.
							</p></li><li class="listitem">
								The control plane machine status condition <code class="literal">Drainable</code> is set to <code class="literal">True</code>.
							</li><li class="listitem"><p class="simpara">
								The machine controller attempts to drain the node that is backed by the control plane machine.
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										If draining fails, <code class="literal">Drained</code> is set to <code class="literal">False</code> and the machine controller attempts to drain the node again.
									</li><li class="listitem">
										If draining succeeds, <code class="literal">Drained</code> is set to <code class="literal">True</code>.
									</li></ul></div></li><li class="listitem">
								The control plane machine status condition <code class="literal">Drained</code> is set to <code class="literal">True</code>.
							</li><li class="listitem">
								If no other Operators have added a <code class="literal">preTerminate</code> lifecycle hook, the control plane machine status condition <code class="literal">Terminable</code> is set to <code class="literal">True</code>.
							</li><li class="listitem">
								The machine controller removes the instance from the infrastructure provider.
							</li><li class="listitem">
								The machine controller deletes the <code class="literal">Node</code> object.
							</li></ol></div><div class="formalpara"><p class="title"><strong>YAML snippet demonstrating the etcd quorum protection <code class="literal">preDrain</code> lifecycle hook</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: Machine
metadata:
  ...
spec:
  lifecycleHooks:
    preDrain:
    - name: EtcdQuorumOperator <span id="CO53-1"><!--Empty--></span><span class="callout">1</span>
      owner: clusteroperator/etcd <span id="CO53-2"><!--Empty--></span><span class="callout">2</span>
  ...</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO53-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The name of the <code class="literal">preDrain</code> lifecycle hook.
							</div></dd><dt><a href="#CO53-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The hook-implementing controller that manages the <code class="literal">preDrain</code> lifecycle hook.
							</div></dd></dl></div></section></section></section><section class="section _additional-resources" id="additional-resources_unhealthy-etcd-member"><div class="titlepage"><div><div><h2 class="title">5.3. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/backup_and_restore/#replacing-unhealthy-etcd-member">Replacing an unhealthy etcd member</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-using">Managing control plane machines with control plane machine sets</a>
					</li></ul></div></section></section><section class="chapter" id="applying-autoscaling"><div class="titlepage"><div><div><h1 class="title">Chapter 6. Applying autoscaling to an OpenShift Container Platform cluster</h1></div></div></div><p>
			Applying autoscaling to an OpenShift Container Platform cluster involves deploying a cluster autoscaler and then deploying machine autoscalers for each machine type in your cluster.
		</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				You can configure the cluster autoscaler only in clusters where the Machine API Operator is operational.
			</p></div></div><section class="section" id="cluster-autoscaler-about_applying-autoscaling"><div class="titlepage"><div><div><h2 class="title">6.1. About the cluster autoscaler</h2></div></div></div><p>
				The cluster autoscaler adjusts the size of an OpenShift Container Platform cluster to meet its current deployment needs. It uses declarative, Kubernetes-style arguments to provide infrastructure management that does not rely on objects of a specific cloud provider. The cluster autoscaler has a cluster scope, and is not associated with a particular namespace.
			</p><p>
				The cluster autoscaler increases the size of the cluster when there are pods that fail to schedule on any of the current worker nodes due to insufficient resources or when another node is necessary to meet deployment needs. The cluster autoscaler does not increase the cluster resources beyond the limits that you specify.
			</p><p>
				The cluster autoscaler computes the total memory, CPU, and GPU on all nodes the cluster, even though it does not manage the control plane nodes. These values are not single-machine oriented. They are an aggregation of all the resources in the entire cluster. For example, if you set the maximum memory resource limit, the cluster autoscaler includes all the nodes in the cluster when calculating the current memory usage. That calculation is then used to determine if the cluster autoscaler has the capacity to add more worker resources.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Ensure that the <code class="literal">maxNodesTotal</code> value in the <code class="literal">ClusterAutoscaler</code> resource definition that you create is large enough to account for the total possible number of machines in your cluster. This value must encompass the number of control plane machines and the possible number of compute machines that you might scale to.
				</p></div></div><p>
				Every 10 seconds, the cluster autoscaler checks which nodes are unnecessary in the cluster and removes them. The cluster autoscaler considers a node for removal if the following conditions apply:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The node utilization is less than the <span class="emphasis"><em>node utilization level</em></span> threshold for the cluster. The node utilization level is the sum of the requested resources divided by the allocated resources for the node. If you do not specify a value in the <code class="literal">ClusterAutoscaler</code> custom resource, the cluster autoscaler uses a default value of <code class="literal">0.5</code>, which corresponds to 50% utilization.
					</li><li class="listitem">
						The cluster autoscaler can move all pods running on the node to the other nodes. The Kubernetes scheduler is responsible for scheduling pods on the nodes.
					</li><li class="listitem">
						The cluster autoscaler does not have scale down disabled annotation.
					</li></ul></div><p>
				If the following types of pods are present on a node, the cluster autoscaler will not remove the node:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Pods with restrictive pod disruption budgets (PDBs).
					</li><li class="listitem">
						Kube-system pods that do not run on the node by default.
					</li><li class="listitem">
						Kube-system pods that do not have a PDB or have a PDB that is too restrictive.
					</li><li class="listitem">
						Pods that are not backed by a controller object such as a deployment, replica set, or stateful set.
					</li><li class="listitem">
						Pods with local storage.
					</li><li class="listitem">
						Pods that cannot be moved elsewhere because of a lack of resources, incompatible node selectors or affinity, matching anti-affinity, and so on.
					</li><li class="listitem">
						Unless they also have a <code class="literal">"cluster-autoscaler.kubernetes.io/safe-to-evict": "true"</code> annotation, pods that have a <code class="literal">"cluster-autoscaler.kubernetes.io/safe-to-evict": "false"</code> annotation.
					</li></ul></div><p>
				For example, you set the maximum CPU limit to 64 cores and configure the cluster autoscaler to only create machines that have 8 cores each. If your cluster starts with 30 cores, the cluster autoscaler can add up to 4 more nodes with 32 cores, for a total of 62.
			</p><p>
				If you configure the cluster autoscaler, additional usage restrictions apply:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Do not modify the nodes that are in autoscaled node groups directly. All nodes within the same node group have the same capacity and labels and run the same system pods.
					</li><li class="listitem">
						Specify requests for your pods.
					</li><li class="listitem">
						If you have to prevent pods from being deleted too quickly, configure appropriate PDBs.
					</li><li class="listitem">
						Confirm that your cloud provider quota is large enough to support the maximum node pools that you configure.
					</li><li class="listitem">
						Do not run additional node group autoscalers, especially the ones offered by your cloud provider.
					</li></ul></div><p>
				The horizontal pod autoscaler (HPA) and the cluster autoscaler modify cluster resources in different ways. The HPA changes the deployment’s or replica set’s number of replicas based on the current CPU load. If the load increases, the HPA creates new replicas, regardless of the amount of resources available to the cluster. If there are not enough resources, the cluster autoscaler adds resources so that the HPA-created pods can run. If the load decreases, the HPA stops some replicas. If this action causes some nodes to be underutilized or completely empty, the cluster autoscaler deletes the unnecessary nodes.
			</p><p>
				The cluster autoscaler takes pod priorities into account. The Pod Priority and Preemption feature enables scheduling pods based on priorities if the cluster does not have enough resources, but the cluster autoscaler ensures that the cluster has resources to run all pods. To honor the intention of both features, the cluster autoscaler includes a priority cutoff function. You can use this cutoff to schedule "best-effort" pods, which do not cause the cluster autoscaler to increase resources but instead run only when spare resources are available.
			</p><p>
				Pods with priority lower than the cutoff value do not cause the cluster to scale up or prevent the cluster from scaling down. No new nodes are added to run the pods, and nodes running these pods might be deleted to free resources.
			</p><p>
				Cluster autoscaling is supported for the platforms that have machine API available on it.
			</p></section><section class="section" id="configuring-clusterautoscaler_applying-autoscaling"><div class="titlepage"><div><div><h2 class="title">6.2. Configuring the cluster autoscaler</h2></div></div></div><p>
				First, deploy the cluster autoscaler to manage automatic resource scaling in your OpenShift Container Platform cluster.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Because the cluster autoscaler is scoped to the entire cluster, you can make only one cluster autoscaler for the cluster.
				</p></div></div><section class="section" id="cluster-autoscaler-cr_applying-autoscaling"><div class="titlepage"><div><div><h3 class="title">6.2.1. Cluster autoscaler resource definition</h3></div></div></div><p>
					This <code class="literal">ClusterAutoscaler</code> resource definition shows the parameters and sample values for the cluster autoscaler.
				</p><pre class="programlisting language-yaml">apiVersion: "autoscaling.openshift.io/v1"
kind: "ClusterAutoscaler"
metadata:
  name: "default"
spec:
  podPriorityThreshold: -10 <span id="CO54-1"><!--Empty--></span><span class="callout">1</span>
  resourceLimits:
    maxNodesTotal: 24 <span id="CO54-2"><!--Empty--></span><span class="callout">2</span>
    cores:
      min: 8 <span id="CO54-3"><!--Empty--></span><span class="callout">3</span>
      max: 128 <span id="CO54-4"><!--Empty--></span><span class="callout">4</span>
    memory:
      min: 4 <span id="CO54-5"><!--Empty--></span><span class="callout">5</span>
      max: 256 <span id="CO54-6"><!--Empty--></span><span class="callout">6</span>
    gpus:
      - type: nvidia.com/gpu <span id="CO54-7"><!--Empty--></span><span class="callout">7</span>
        min: 0 <span id="CO54-8"><!--Empty--></span><span class="callout">8</span>
        max: 16 <span id="CO54-9"><!--Empty--></span><span class="callout">9</span>
      - type: amd.com/gpu
        min: 0
        max: 4
  logVerbosity: 4 <span id="CO54-10"><!--Empty--></span><span class="callout">10</span>
  scaleDown: <span id="CO54-11"><!--Empty--></span><span class="callout">11</span>
    enabled: true <span id="CO54-12"><!--Empty--></span><span class="callout">12</span>
    delayAfterAdd: 10m <span id="CO54-13"><!--Empty--></span><span class="callout">13</span>
    delayAfterDelete: 5m <span id="CO54-14"><!--Empty--></span><span class="callout">14</span>
    delayAfterFailure: 30s <span id="CO54-15"><!--Empty--></span><span class="callout">15</span>
    unneededTime: 5m <span id="CO54-16"><!--Empty--></span><span class="callout">16</span>
    utilizationThreshold: "0.4" <span id="CO54-17"><!--Empty--></span><span class="callout">17</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO54-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Specify the priority that a pod must exceed to cause the cluster autoscaler to deploy additional nodes. Enter a 32-bit integer value. The <code class="literal">podPriorityThreshold</code> value is compared to the value of the <code class="literal">PriorityClass</code> that you assign to each pod.
						</div></dd><dt><a href="#CO54-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Specify the maximum number of nodes to deploy. This value is the total number of machines that are deployed in your cluster, not just the ones that the autoscaler controls. Ensure that this value is large enough to account for all of your control plane and compute machines and the total number of replicas that you specify in your <code class="literal">MachineAutoscaler</code> resources.
						</div></dd><dt><a href="#CO54-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Specify the minimum number of cores to deploy in the cluster.
						</div></dd><dt><a href="#CO54-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Specify the maximum number of cores to deploy in the cluster.
						</div></dd><dt><a href="#CO54-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Specify the minimum amount of memory, in GiB, in the cluster.
						</div></dd><dt><a href="#CO54-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							Specify the maximum amount of memory, in GiB, in the cluster.
						</div></dd><dt><a href="#CO54-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							Optional: Specify the type of GPU node to deploy. Only <code class="literal">nvidia.com/gpu</code> and <code class="literal">amd.com/gpu</code> are valid types.
						</div></dd><dt><a href="#CO54-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							Specify the minimum number of GPUs to deploy in the cluster.
						</div></dd><dt><a href="#CO54-9"><span class="callout">9</span></a> </dt><dd><div class="para">
							Specify the maximum number of GPUs to deploy in the cluster.
						</div></dd><dt><a href="#CO54-10"><span class="callout">10</span></a> </dt><dd><div class="para">
							Specify the logging verbosity level between <code class="literal">0</code> and <code class="literal">10</code>. The following log level thresholds are provided for guidance:
						</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">1</code>: (Default) Basic information about changes.
								</li><li class="listitem">
									<code class="literal">4</code>: Debug-level verbosity for troubleshooting typical issues.
								</li><li class="listitem">
									<code class="literal">9</code>: Extensive, protocol-level debugging information.
								</li></ul></div><p>
							If you do not specify a value, the default value of <code class="literal">1</code> is used.
						</p></dd><dt><a href="#CO54-11"><span class="callout">11</span></a> </dt><dd><div class="para">
							In this section, you can specify the period to wait for each action by using any valid <a class="link" href="https://golang.org/pkg/time/#ParseDuration">ParseDuration</a> interval, including <code class="literal">ns</code>, <code class="literal">us</code>, <code class="literal">ms</code>, <code class="literal">s</code>, <code class="literal">m</code>, and <code class="literal">h</code>.
						</div></dd><dt><a href="#CO54-12"><span class="callout">12</span></a> </dt><dd><div class="para">
							Specify whether the cluster autoscaler can remove unnecessary nodes.
						</div></dd><dt><a href="#CO54-13"><span class="callout">13</span></a> </dt><dd><div class="para">
							Optional: Specify the period to wait before deleting a node after a node has recently been <span class="emphasis"><em>added</em></span>. If you do not specify a value, the default value of <code class="literal">10m</code> is used.
						</div></dd><dt><a href="#CO54-14"><span class="callout">14</span></a> </dt><dd><div class="para">
							Optional: Specify the period to wait before deleting a node after a node has recently been <span class="emphasis"><em>deleted</em></span>. If you do not specify a value, the default value of <code class="literal">0s</code> is used.
						</div></dd><dt><a href="#CO54-15"><span class="callout">15</span></a> </dt><dd><div class="para">
							Optional: Specify the period to wait before deleting a node after a scale down failure occurred. If you do not specify a value, the default value of <code class="literal">3m</code> is used.
						</div></dd><dt><a href="#CO54-16"><span class="callout">16</span></a> </dt><dd><div class="para">
							Optional: Specify a period of time before an unnecessary node is eligible for deletion. If you do not specify a value, the default value of <code class="literal">10m</code> is used.
						</div></dd><dt><a href="#CO54-17"><span class="callout">17</span></a> </dt><dd><div class="para">
							Optional: Specify the <span class="emphasis"><em>node utilization level</em></span>. Nodes below this utilization level are eligible for deletion. If you do not specify a value, the default value of <code class="literal">10m</code> is used.. The node utilization level is the sum of the requested resources divided by the allocated resources for the node, and must be a value greater than <code class="literal">"0"</code> but less than <code class="literal">"1"</code>. If you do not specify a value, the cluster autoscaler uses a default value of <code class="literal">"0.5"</code>, which corresponds to 50% utilization. This value must be expressed as a string.
						</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When performing a scaling operation, the cluster autoscaler remains within the ranges set in the <code class="literal">ClusterAutoscaler</code> resource definition, such as the minimum and maximum number of cores to deploy or the amount of memory in the cluster. However, the cluster autoscaler does not correct the current values in your cluster to be within those ranges.
					</p><p>
						The minimum and maximum CPUs, memory, and GPU values are determined by calculating those resources on all nodes in the cluster, even if the cluster autoscaler does not manage the nodes. For example, the control plane nodes are considered in the total memory in the cluster, even though the cluster autoscaler does not manage the control plane nodes.
					</p></div></div></section><section class="section" id="ClusterAutoscaler-deploying_applying-autoscaling"><div class="titlepage"><div><div><h3 class="title">6.2.2. Deploying a cluster autoscaler</h3></div></div></div><p>
					To deploy a cluster autoscaler, you create an instance of the <code class="literal">ClusterAutoscaler</code> resource.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Create a YAML file for a <code class="literal">ClusterAutoscaler</code> resource that contains the custom resource definition.
						</li><li class="listitem"><p class="simpara">
							Create the custom resource in the cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;filename&gt;.yaml <span id="CO55-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO55-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal">&lt;filename&gt;</code> is the name of the custom resource file.
								</div></dd></dl></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							After you configure the cluster autoscaler, you must <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#configuring-machineautoscaler_applying-autoscaling">configure at least one machine autoscaler</a>.
						</li></ul></div></section></section><section class="section" id="machine-autoscaler-about_applying-autoscaling"><div class="titlepage"><div><div><h2 class="title">6.3. About the machine autoscaler</h2></div></div></div><p>
				The machine autoscaler adjusts the number of Machines in the compute machine sets that you deploy in an OpenShift Container Platform cluster. You can scale both the default <code class="literal">worker</code> compute machine set and any other compute machine sets that you create. The machine autoscaler makes more Machines when the cluster runs out of resources to support more deployments. Any changes to the values in <code class="literal">MachineAutoscaler</code> resources, such as the minimum or maximum number of instances, are immediately applied to the compute machine set they target.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You must deploy a machine autoscaler for the cluster autoscaler to scale your machines. The cluster autoscaler uses the annotations on compute machine sets that the machine autoscaler sets to determine the resources that it can scale. If you define a cluster autoscaler without also defining machine autoscalers, the cluster autoscaler will never scale your cluster.
				</p></div></div></section><section class="section" id="configuring-machineautoscaler_applying-autoscaling"><div class="titlepage"><div><div><h2 class="title">6.4. Configuring machine autoscalers</h2></div></div></div><p>
				After you deploy the cluster autoscaler, deploy <code class="literal">MachineAutoscaler</code> resources that reference the compute machine sets that are used to scale the cluster.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You must deploy at least one <code class="literal">MachineAutoscaler</code> resource after you deploy the <code class="literal">ClusterAutoscaler</code> resource.
				</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					You must configure separate resources for each compute machine set. Remember that compute machine sets are different in each region, so consider whether you want to enable machine scaling in multiple regions. The compute machine set that you scale must have at least one machine in it.
				</p></div></div><section class="section" id="machine-autoscaler-cr_applying-autoscaling"><div class="titlepage"><div><div><h3 class="title">6.4.1. Machine autoscaler resource definition</h3></div></div></div><p>
					This <code class="literal">MachineAutoscaler</code> resource definition shows the parameters and sample values for the machine autoscaler.
				</p><pre class="programlisting language-yaml">apiVersion: "autoscaling.openshift.io/v1beta1"
kind: "MachineAutoscaler"
metadata:
  name: "worker-us-east-1a" <span id="CO56-1"><!--Empty--></span><span class="callout">1</span>
  namespace: "openshift-machine-api"
spec:
  minReplicas: 1 <span id="CO56-2"><!--Empty--></span><span class="callout">2</span>
  maxReplicas: 12 <span id="CO56-3"><!--Empty--></span><span class="callout">3</span>
  scaleTargetRef: <span id="CO56-4"><!--Empty--></span><span class="callout">4</span>
    apiVersion: machine.openshift.io/v1beta1
    kind: MachineSet <span id="CO56-5"><!--Empty--></span><span class="callout">5</span>
    name: worker-us-east-1a <span id="CO56-6"><!--Empty--></span><span class="callout">6</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO56-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Specify the machine autoscaler name. To make it easier to identify which compute machine set this machine autoscaler scales, specify or include the name of the compute machine set to scale. The compute machine set name takes the following form: <code class="literal">&lt;clusterid&gt;-&lt;machineset&gt;-&lt;region&gt;</code>.
						</div></dd><dt><a href="#CO56-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Specify the minimum number machines of the specified type that must remain in the specified zone after the cluster autoscaler initiates cluster scaling. If running in AWS, GCP, Azure, RHOSP, or vSphere, this value can be set to <code class="literal">0</code>. For other providers, do not set this value to <code class="literal">0</code>.
						</div><p>
							You can save on costs by setting this value to <code class="literal">0</code> for use cases such as running expensive or limited-usage hardware that is used for specialized workloads, or by scaling a compute machine set with extra large machines. The cluster autoscaler scales the compute machine set down to zero if the machines are not in use.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Do not set the <code class="literal">spec.minReplicas</code> value to <code class="literal">0</code> for the three compute machine sets that are created during the OpenShift Container Platform installation process for an installer provisioned infrastructure.
							</p></div></div></dd><dt><a href="#CO56-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Specify the maximum number machines of the specified type that the cluster autoscaler can deploy in the specified zone after it initiates cluster scaling. Ensure that the <code class="literal">maxNodesTotal</code> value in the <code class="literal">ClusterAutoscaler</code> resource definition is large enough to allow the machine autoscaler to deploy this number of machines.
						</div></dd><dt><a href="#CO56-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							In this section, provide values that describe the existing compute machine set to scale.
						</div></dd><dt><a href="#CO56-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							The <code class="literal">kind</code> parameter value is always <code class="literal">MachineSet</code>.
						</div></dd><dt><a href="#CO56-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							The <code class="literal">name</code> value must match the name of an existing compute machine set, as shown in the <code class="literal">metadata.name</code> parameter value.
						</div></dd></dl></div></section><section class="section" id="MachineAutoscaler-deploying_applying-autoscaling"><div class="titlepage"><div><div><h3 class="title">6.4.2. Deploying a machine autoscaler</h3></div></div></div><p>
					To deploy a machine autoscaler, you create an instance of the <code class="literal">MachineAutoscaler</code> resource.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Create a YAML file for a <code class="literal">MachineAutoscaler</code> resource that contains the custom resource definition.
						</li><li class="listitem"><p class="simpara">
							Create the custom resource in the cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;filename&gt;.yaml <span id="CO57-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO57-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal">&lt;filename&gt;</code> is the name of the custom resource file.
								</div></dd></dl></div></li></ol></div></section></section><section class="section" id="disabling-autoscaling_applying-autoscaling"><div class="titlepage"><div><div><h2 class="title">6.5. Disabling autoscaling</h2></div></div></div><p>
				You can disable an individual machine autoscaler in your cluster or disable autoscaling on the cluster entirely.
			</p><section class="section" id="deleting-machine-autoscaler_applying-autoscaling"><div class="titlepage"><div><div><h3 class="title">6.5.1. Disabling a machine autoscaler</h3></div></div></div><p>
					To disable a machine autoscaler, you delete the corresponding <code class="literal">MachineAutoscaler</code> custom resource (CR).
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Disabling a machine autoscaler does not disable the cluster autoscaler. To disable the cluster autoscaler, follow the instructions in "Disabling the cluster autoscaler".
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							List the <code class="literal">MachineAutoscaler</code> CRs for the cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get MachineAutoscaler -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                 REF KIND     REF NAME             MIN   MAX   AGE
compute-us-east-1a   MachineSet   compute-us-east-1a   1     12    39m
compute-us-west-1a   MachineSet   compute-us-west-1a   2     4     37m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Optional: Create a YAML file backup of the <code class="literal">MachineAutoscaler</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get MachineAutoscaler/&lt;machine_autoscaler_name&gt; \<span id="CO58-1"><!--Empty--></span><span class="callout">1</span>
  -n openshift-machine-api \
  -o yaml&gt; &lt;machine_autoscaler_name_backup&gt;.yaml <span id="CO58-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO58-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal">&lt;machine_autoscaler_name&gt;</code> is the name of the CR that you want to delete.
								</div></dd><dt><a href="#CO58-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									<code class="literal">&lt;machine_autoscaler_name_backup&gt;</code> is the name for the backup of the CR.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Delete the <code class="literal">MachineAutoscaler</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc delete MachineAutoscaler/&lt;machine_autoscaler_name&gt; -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">machineautoscaler.autoscaling.openshift.io "compute-us-east-1a" deleted</pre>

							</p></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To verify that the machine autoscaler is disabled, run the following command:
						</p><pre class="programlisting language-terminal">$ oc get MachineAutoscaler -n openshift-machine-api</pre><p class="simpara">
							The disabled machine autoscaler does not appear in the list of machine autoscalers.
						</p></li></ul></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							If you need to re-enable the machine autoscaler, use the <code class="literal">&lt;machine_autoscaler_name_backup&gt;.yaml</code> backup file and follow the instructions in "Deploying a machine autoscaler".
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#deleting-cluster-autoscaler_applying-autoscaling">Disabling the cluster autoscaler</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#MachineAutoscaler-deploying_applying-autoscaling">Deploying a machine autoscaler</a>
						</li></ul></div></section><section class="section" id="deleting-cluster-autoscaler_applying-autoscaling"><div class="titlepage"><div><div><h3 class="title">6.5.2. Disabling the cluster autoscaler</h3></div></div></div><p>
					To disable the cluster autoscaler, you delete the corresponding <code class="literal">ClusterAutoscaler</code> resource.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Disabling the cluster autoscaler disables autoscaling on the cluster, even if the cluster has existing machine autoscalers.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							List the <code class="literal">ClusterAutoscaler</code> resource for the cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get ClusterAutoscaler</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      AGE
default   42m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Optional: Create a YAML file backup of the <code class="literal">ClusterAutoscaler</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get ClusterAutoscaler/default \<span id="CO59-1"><!--Empty--></span><span class="callout">1</span>
  -o yaml&gt; &lt;cluster_autoscaler_backup_name&gt;.yaml <span id="CO59-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO59-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal">default</code> is the name of the <code class="literal">ClusterAutoscaler</code> CR.
								</div></dd><dt><a href="#CO59-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									<code class="literal">&lt;cluster_autoscaler_backup_name&gt;</code> is the name for the backup of the CR.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Delete the <code class="literal">ClusterAutoscaler</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc delete ClusterAutoscaler/default</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">clusterautoscaler.autoscaling.openshift.io "default" deleted</pre>

							</p></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To verify that the cluster autoscaler is disabled, run the following command:
						</p><pre class="programlisting language-terminal">$ oc get ClusterAutoscaler</pre><div class="formalpara"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">No resources found</pre>

							</p></div></li></ul></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Disabling the cluster autoscaler by deleting the <code class="literal">ClusterAutoscaler</code> CR prevents the cluster from autoscaling but does not delete any existing machine autoscalers on the cluster. To clean up unneeded machine autoscalers, see "Disabling a machine autoscaler".
						</li><li class="listitem">
							If you need to re-enable the cluster autoscaler, use the <code class="literal">&lt;cluster_autoscaler_name_backup&gt;.yaml</code> backup file and follow the instructions in "Deploying a cluster autoscaler".
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#deleting-machine-autoscaler_applying-autoscaling">Disabling the machine autoscaler</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#ClusterAutoscaler-deploying_applying-autoscaling">Deploying a cluster autoscaler</a>
						</li></ul></div></section></section><section class="section _additional-resources" id="additional-resources"><div class="titlepage"><div><div><h2 class="title">6.6. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-priority">Including pod priority in pod scheduling decisions in OpenShift Container Platform</a>
					</li></ul></div></section></section><section class="chapter" id="creating-infrastructure-machinesets"><div class="titlepage"><div><div><h1 class="title">Chapter 7. Creating infrastructure machine sets</h1></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.
			</p><p>
				Clusters with the infrastructure platform type <code class="literal">none</code> cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.
			</p><p>
				To view the platform type for your cluster, run the following command:
			</p><pre class="programlisting language-terminal">$ oc get infrastructure cluster -o jsonpath='{.status.platform}'</pre></div></div><p>
			You can use infrastructure machine sets to create machines that host only infrastructure components, such as the default router, the integrated container image registry, and the components for cluster metrics and monitoring. These infrastructure machines are not counted toward the total number of subscriptions that are required to run the environment.
		</p><p>
			In a production deployment, it is recommended that you deploy at least three machine sets to hold infrastructure components. Both OpenShift Logging and Red Hat OpenShift Service Mesh deploy Elasticsearch, which requires three instances to be installed on different nodes. Each of these nodes can be deployed to different availability zones for high availability. This configuration requires three different machine sets, one for each availability zone. In global Azure regions that do not have multiple availability zones, you can use availability sets to ensure high availability.
		</p><section class="section" id="infrastructure-components_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h2 class="title">7.1. OpenShift Container Platform infrastructure components</h2></div></div></div><p>
				The following infrastructure workloads do not incur OpenShift Container Platform worker subscriptions:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Kubernetes and OpenShift Container Platform control plane services that run on masters
					</li><li class="listitem">
						The default router
					</li><li class="listitem">
						The integrated container image registry
					</li><li class="listitem">
						The HAProxy-based Ingress Controller
					</li><li class="listitem">
						The cluster metrics collection, or monitoring service, including components for monitoring user-defined projects
					</li><li class="listitem">
						Cluster aggregated logging
					</li><li class="listitem">
						Service brokers
					</li><li class="listitem">
						Red Hat Quay
					</li><li class="listitem">
						Red Hat OpenShift Data Foundation
					</li><li class="listitem">
						Red Hat Advanced Cluster Manager
					</li><li class="listitem">
						Red Hat Advanced Cluster Security for Kubernetes
					</li><li class="listitem">
						Red Hat OpenShift GitOps
					</li><li class="listitem">
						Red Hat OpenShift Pipelines
					</li></ul></div><p>
				Any node that runs any other container, pod, or component is a worker node that your subscription must cover.
			</p><p>
				For information about infrastructure nodes and which components can run on infrastructure nodes, see the "Red Hat OpenShift control plane and infrastructure nodes" section in the <a class="link" href="https://www.redhat.com/en/resources/openshift-subscription-sizing-guide">OpenShift sizing and subscription guide for enterprise Kubernetes</a> document.
			</p><p>
				To create an infrastructure node, you can <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machineset-creating_creating-infrastructure-machinesets">use a machine set</a>, <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-an-infra-node_creating-infrastructure-machinesets">label the node</a>, or <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-infra-machines_creating-infrastructure-machinesets">use a machine config pool</a>.
			</p></section><section class="section" id="creating-infrastructure-machinesets-production"><div class="titlepage"><div><div><h2 class="title">7.2. Creating infrastructure machine sets for production environments</h2></div></div></div><p>
				In a production deployment, it is recommended that you deploy at least three compute machine sets to hold infrastructure components. Both OpenShift Logging and Red Hat OpenShift Service Mesh deploy Elasticsearch, which requires three instances to be installed on different nodes. Each of these nodes can be deployed to different availability zones for high availability. A configuration like this requires three different compute machine sets, one for each availability zone. In global Azure regions that do not have multiple availability zones, you can use availability sets to ensure high availability.
			</p><section class="section" id="creating-infrastructure-machinesets-clouds"><div class="titlepage"><div><div><h3 class="title">7.2.1. Creating infrastructure machine sets for different clouds</h3></div></div></div><p>
					Use the sample compute machine set for your cloud.
				</p><section class="section" id="machineset-yaml-alibaba_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h4 class="title">7.2.1.1. Sample YAML for a compute machine set custom resource on Alibaba Cloud</h4></div></div></div><p>
						This sample YAML defines a compute machine set that runs in a specified Alibaba Cloud zone in a region and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/infra: ""</code>.
					</p><p>
						In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;infra&gt;</code> is the node label to add.
					</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO60-1"><!--Empty--></span><span class="callout">1</span>
    machine.openshift.io/cluster-api-machine-role: &lt;infra&gt; <span id="CO60-2"><!--Empty--></span><span class="callout">2</span>
    machine.openshift.io/cluster-api-machine-type: &lt;infra&gt; <span id="CO60-3"><!--Empty--></span><span class="callout">3</span>
  name: &lt;infrastructure_id&gt;-&lt;infra&gt;-&lt;zone&gt; <span id="CO60-4"><!--Empty--></span><span class="callout">4</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO60-5"><!--Empty--></span><span class="callout">5</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;infra&gt;-&lt;zone&gt; <span id="CO60-6"><!--Empty--></span><span class="callout">6</span>
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO60-7"><!--Empty--></span><span class="callout">7</span>
        machine.openshift.io/cluster-api-machine-role: &lt;infra&gt; <span id="CO60-8"><!--Empty--></span><span class="callout">8</span>
        machine.openshift.io/cluster-api-machine-type: &lt;infra&gt; <span id="CO60-9"><!--Empty--></span><span class="callout">9</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;infra&gt;-&lt;zone&gt; <span id="CO60-10"><!--Empty--></span><span class="callout">10</span>
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/infra: ""
      providerSpec:
        value:
          apiVersion: machine.openshift.io/v1
          credentialsSecret:
            name: alibabacloud-credentials
          imageId: &lt;image_id&gt; <span id="CO60-11"><!--Empty--></span><span class="callout">11</span>
          instanceType: &lt;instance_type&gt; <span id="CO60-12"><!--Empty--></span><span class="callout">12</span>
          kind: AlibabaCloudMachineProviderConfig
          ramRoleName: &lt;infrastructure_id&gt;-role-worker <span id="CO60-13"><!--Empty--></span><span class="callout">13</span>
          regionId: &lt;region&gt; <span id="CO60-14"><!--Empty--></span><span class="callout">14</span>
          resourceGroup: <span id="CO60-15"><!--Empty--></span><span class="callout">15</span>
            id: &lt;resource_group_id&gt;
            type: ID
          securityGroups:
          - tags: <span id="CO60-16"><!--Empty--></span><span class="callout">16</span>
            - Key: Name
              Value: &lt;infrastructure_id&gt;-sg-&lt;role&gt;
            type: Tags
          systemDisk: <span id="CO60-17"><!--Empty--></span><span class="callout">17</span>
            category: cloud_essd
            size: &lt;disk_size&gt;
          tag: <span id="CO60-18"><!--Empty--></span><span class="callout">18</span>
          - Key: kubernetes.io/cluster/&lt;infrastructure_id&gt;
            Value: owned
          userDataSecret:
            name: &lt;user_data_secret&gt; <span id="CO60-19"><!--Empty--></span><span class="callout">19</span>
          vSwitch:
            tags: <span id="CO60-20"><!--Empty--></span><span class="callout">20</span>
            - Key: Name
              Value: &lt;infrastructure_id&gt;-vswitch-&lt;zone&gt;
            type: Tags
          vpcId: ""
          zoneId: &lt;zone&gt; <span id="CO60-21"><!--Empty--></span><span class="callout">21</span>
      taints: <span id="CO60-22"><!--Empty--></span><span class="callout">22</span>
      - key: node-role.kubernetes.io/infra
        effect: NoSchedule</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO60-1"><span class="callout">1</span></a> <a href="#CO60-5"><span class="callout">5</span></a> <a href="#CO60-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI (<code class="literal">oc</code>) installed, you can obtain the infrastructure ID by running the following command:
							</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><a href="#CO60-2"><span class="callout">2</span></a> <a href="#CO60-3"><span class="callout">3</span></a> <a href="#CO60-8"><span class="callout">8</span></a> <a href="#CO60-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								Specify the <code class="literal">&lt;infra&gt;</code> node label.
							</div></dd><dt><a href="#CO60-4"><span class="callout">4</span></a> <a href="#CO60-6"><span class="callout">6</span></a> <a href="#CO60-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								Specify the infrastructure ID, <code class="literal">&lt;infra&gt;</code> node label, and zone.
							</div></dd><dt><a href="#CO60-11"><span class="callout">11</span></a> </dt><dd><div class="para">
								Specify the image to use. Use an image from an existing default compute machine set for the cluster.
							</div></dd><dt><a href="#CO60-12"><span class="callout">12</span></a> </dt><dd><div class="para">
								Specify the instance type you want to use for the compute machine set.
							</div></dd><dt><a href="#CO60-13"><span class="callout">13</span></a> </dt><dd><div class="para">
								Specify the name of the RAM role to use for the compute machine set. Use the value that the installer populates in the default compute machine set.
							</div></dd><dt><a href="#CO60-14"><span class="callout">14</span></a> </dt><dd><div class="para">
								Specify the region to place machines on.
							</div></dd><dt><a href="#CO60-15"><span class="callout">15</span></a> </dt><dd><div class="para">
								Specify the resource group and type for the cluster. You can use the value that the installer populates in the default compute machine set, or specify a different one.
							</div></dd><dt><a href="#CO60-16"><span class="callout">16</span></a> <a href="#CO60-18"><span class="callout">18</span></a> <a href="#CO60-20"><span class="callout">20</span></a> </dt><dd><div class="para">
								Specify the tags to use for the compute machine set. Minimally, you must include the tags shown in this example, with appropriate values for your cluster. You can include additional tags, including the tags that the installer populates in the default compute machine set it creates, as needed.
							</div></dd><dt><a href="#CO60-17"><span class="callout">17</span></a> </dt><dd><div class="para">
								Specify the type and size of the root disk. Use the <code class="literal">category</code> value that the installer populates in the default compute machine set it creates. If required, specify a different value in gigabytes for <code class="literal">size</code>.
							</div></dd><dt><a href="#CO60-19"><span class="callout">19</span></a> </dt><dd><div class="para">
								Specify the name of the secret in the user data YAML file that is in the <code class="literal">openshift-machine-api</code> namespace. Use the value that the installer populates in the default compute machine set.
							</div></dd><dt><a href="#CO60-21"><span class="callout">21</span></a> </dt><dd><div class="para">
								Specify the zone within your region to place machines on. Be sure that your region supports the zone that you specify.
							</div></dd><dt><a href="#CO60-22"><span class="callout">22</span></a> </dt><dd><div class="para">
								Specify a taint to prevent user workloads from being scheduled on infra nodes.
							</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									After adding the <code class="literal">NoSchedule</code> taint on the infrastructure node, existing DNS pods running on that node are marked as <code class="literal">misscheduled</code>. You must either delete or <a class="link" href="https://access.redhat.com/solutions/6592171">add toleration on <code class="literal">misscheduled</code> DNS pods</a>.
								</p></div></div></dd></dl></div><h6 id="machineset-yaml-alibaba-usage-stats_creating-infrastructure-machinesets">Machine set parameters for Alibaba Cloud usage statistics</h6><p>
						The default compute machine sets that the installer creates for Alibaba Cloud clusters include nonessential tag values that Alibaba Cloud uses internally to track usage statistics. These tags are populated in the <code class="literal">securityGroups</code>, <code class="literal">tag</code>, and <code class="literal">vSwitch</code> parameters of the <code class="literal">spec.template.spec.providerSpec.value</code> list.
					</p><p>
						When creating compute machine sets to deploy additional machines, you must include the required Kubernetes tags. The usage statistics tags are applied by default, even if they are not specified in the compute machine sets you create. You can also include additional tags as needed.
					</p><p>
						The following YAML snippets indicate which tags in the default compute machine sets are optional and which are required.
					</p><div class="formalpara"><p class="title"><strong>Tags in <code class="literal">spec.template.spec.providerSpec.value.securityGroups</code></strong></p><p>
							
<pre class="programlisting language-yaml">spec:
  template:
    spec:
      providerSpec:
        value:
          securityGroups:
          - tags:
            - Key: kubernetes.io/cluster/&lt;infrastructure_id&gt; <span id="CO61-1"><!--Empty--></span><span class="callout">1</span>
              Value: owned
            - Key: GISV
              Value: ocp
            - Key: sigs.k8s.io/cloud-provider-alibaba/origin <span id="CO61-2"><!--Empty--></span><span class="callout">2</span>
              Value: ocp
            - Key: Name
              Value: &lt;infrastructure_id&gt;-sg-&lt;role&gt; <span id="CO61-3"><!--Empty--></span><span class="callout">3</span>
            type: Tags</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO61-1"><span class="callout">1</span></a> <a href="#CO61-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Optional: This tag is applied even when not specified in the compute machine set.
							</div></dd><dt><a href="#CO61-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Required.
							</div><p>
								where:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster.
									</li><li class="listitem">
										<code class="literal">&lt;role&gt;</code> is the node label to add.
									</li></ul></div></dd></dl></div><div class="formalpara"><p class="title"><strong>Tags in <code class="literal">spec.template.spec.providerSpec.value.tag</code></strong></p><p>
							
<pre class="programlisting language-yaml">spec:
  template:
    spec:
      providerSpec:
        value:
          tag:
          - Key: kubernetes.io/cluster/&lt;infrastructure_id&gt; <span id="CO62-1"><!--Empty--></span><span class="callout">1</span>
            Value: owned
          - Key: GISV <span id="CO62-2"><!--Empty--></span><span class="callout">2</span>
            Value: ocp
          - Key: sigs.k8s.io/cloud-provider-alibaba/origin <span id="CO62-3"><!--Empty--></span><span class="callout">3</span>
            Value: ocp</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO62-2"><span class="callout">2</span></a> <a href="#CO62-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Optional: This tag is applied even when not specified in the compute machine set.
							</div></dd><dt><a href="#CO62-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Required.
							</div><p>
								where <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster.
							</p></dd></dl></div><div class="formalpara"><p class="title"><strong>Tags in <code class="literal">spec.template.spec.providerSpec.value.vSwitch</code></strong></p><p>
							
<pre class="programlisting language-yaml">spec:
  template:
    spec:
      providerSpec:
        value:
          vSwitch:
            tags:
            - Key: kubernetes.io/cluster/&lt;infrastructure_id&gt; <span id="CO63-1"><!--Empty--></span><span class="callout">1</span>
              Value: owned
            - Key: GISV <span id="CO63-2"><!--Empty--></span><span class="callout">2</span>
              Value: ocp
            - Key: sigs.k8s.io/cloud-provider-alibaba/origin <span id="CO63-3"><!--Empty--></span><span class="callout">3</span>
              Value: ocp
            - Key: Name
              Value: &lt;infrastructure_id&gt;-vswitch-&lt;zone&gt; <span id="CO63-4"><!--Empty--></span><span class="callout">4</span>
            type: Tags</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO63-1"><span class="callout">1</span></a> <a href="#CO63-2"><span class="callout">2</span></a> <a href="#CO63-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Optional: This tag is applied even when not specified in the compute machine set.
							</div></dd><dt><a href="#CO63-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Required.
							</div><p>
								where:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster.
									</li><li class="listitem">
										<code class="literal">&lt;zone&gt;</code> is the zone within your region to place machines on.
									</li></ul></div></dd></dl></div></section><section class="section" id="machineset-yaml-aws_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h4 class="title">7.2.1.2. Sample YAML for a compute machine set custom resource on AWS</h4></div></div></div><p>
						This sample YAML defines a compute machine set that runs in the <code class="literal">us-east-1a</code> Amazon Web Services (AWS) zone and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/infra: ""</code>.
					</p><p>
						In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;infra&gt;</code> is the node label to add.
					</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO64-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-infra-&lt;zone&gt; <span id="CO64-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO64-3"><!--Empty--></span><span class="callout">3</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-infra-&lt;zone&gt; <span id="CO64-4"><!--Empty--></span><span class="callout">4</span>
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO64-5"><!--Empty--></span><span class="callout">5</span>
        machine.openshift.io/cluster-api-machine-role: infra <span id="CO64-6"><!--Empty--></span><span class="callout">6</span>
        machine.openshift.io/cluster-api-machine-type: infra <span id="CO64-7"><!--Empty--></span><span class="callout">7</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-infra-&lt;zone&gt; <span id="CO64-8"><!--Empty--></span><span class="callout">8</span>
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/infra: "" <span id="CO64-9"><!--Empty--></span><span class="callout">9</span>
      providerSpec:
        value:
          ami:
            id: ami-046fe691f52a953f9 <span id="CO64-10"><!--Empty--></span><span class="callout">10</span>
          apiVersion: awsproviderconfig.openshift.io/v1beta1
          blockDevices:
            - ebs:
                iops: 0
                volumeSize: 120
                volumeType: gp2
          credentialsSecret:
            name: aws-cloud-credentials
          deviceIndex: 0
          iamInstanceProfile:
            id: &lt;infrastructure_id&gt;-worker-profile <span id="CO64-11"><!--Empty--></span><span class="callout">11</span>
          instanceType: m6i.large
          kind: AWSMachineProviderConfig
          placement:
            availabilityZone: &lt;zone&gt; <span id="CO64-12"><!--Empty--></span><span class="callout">12</span>
            region: &lt;region&gt; <span id="CO64-13"><!--Empty--></span><span class="callout">13</span>
          securityGroups:
            - filters:
                - name: tag:Name
                  values:
                    - &lt;infrastructure_id&gt;-worker-sg <span id="CO64-14"><!--Empty--></span><span class="callout">14</span>
          subnet:
            filters:
              - name: tag:Name
                values:
                  - &lt;infrastructure_id&gt;-private-&lt;zone&gt; <span id="CO64-15"><!--Empty--></span><span class="callout">15</span>
          tags:
            - name: kubernetes.io/cluster/&lt;infrastructure_id&gt; <span id="CO64-16"><!--Empty--></span><span class="callout">16</span>
              value: owned
            - name: &lt;custom_tag_name&gt; <span id="CO64-17"><!--Empty--></span><span class="callout">17</span>
              value: &lt;custom_tag_value&gt; <span id="CO64-18"><!--Empty--></span><span class="callout">18</span>
          userDataSecret:
            name: worker-user-data
      taints: <span id="CO64-19"><!--Empty--></span><span class="callout">19</span>
        - key: node-role.kubernetes.io/infra
          effect: NoSchedule</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO64-1"><span class="callout">1</span></a> <a href="#CO64-3"><span class="callout">3</span></a> <a href="#CO64-5"><span class="callout">5</span></a> <a href="#CO64-11"><span class="callout">11</span></a> <a href="#CO64-14"><span class="callout">14</span></a> <a href="#CO64-16"><span class="callout">16</span></a> </dt><dd><div class="para">
								Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
							</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><a href="#CO64-2"><span class="callout">2</span></a> <a href="#CO64-4"><span class="callout">4</span></a> <a href="#CO64-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								Specify the infrastructure ID, <code class="literal">infra</code> role node label, and zone.
							</div></dd><dt><a href="#CO64-6"><span class="callout">6</span></a> <a href="#CO64-7"><span class="callout">7</span></a> <a href="#CO64-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								Specify the <code class="literal">infra</code> role node label.
							</div></dd><dt><a href="#CO64-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								Specify a valid Red Hat Enterprise Linux CoreOS (RHCOS) Amazon Machine Image (AMI) for your AWS zone for your OpenShift Container Platform nodes. If you want to use an AWS Marketplace image, you must complete the OpenShift Container Platform subscription from the <a class="link" href="https://aws.amazon.com/marketplace/fulfillment?productId=59ead7de-2540-4653-a8b0-fa7926d5c845">AWS Marketplace</a> to obtain an AMI ID for your region.
							</div><pre class="programlisting language-terminal">$ oc -n openshift-machine-api \
    -o jsonpath='{.spec.template.spec.providerSpec.value.ami.id}{"\n"}' \
    get machineset/&lt;infrastructure_id&gt;-&lt;role&gt;-&lt;zone&gt;</pre></dd><dt><a href="#CO64-17"><span class="callout">17</span></a> <a href="#CO64-18"><span class="callout">18</span></a> </dt><dd><div class="para">
								Optional: Specify custom tag data for your cluster. For example, you might add an admin contact email address by specifying a <code class="literal">name:value</code> pair of <code class="literal">Email:admin-email@example.com</code>.
							</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Custom tags can also be specified during installation in the <code class="literal">install-config.yml</code> file. If the <code class="literal">install-config.yml</code> file and the machine set include a tag with the same <code class="literal">name</code> data, the value for the tag from the machine set takes priority over the value for the tag in the <code class="literal">install-config.yml</code> file.
								</p></div></div></dd><dt><a href="#CO64-12"><span class="callout">12</span></a> </dt><dd><div class="para">
								Specify the zone, for example, <code class="literal">us-east-1a</code>.
							</div></dd><dt><a href="#CO64-13"><span class="callout">13</span></a> </dt><dd><div class="para">
								Specify the region, for example, <code class="literal">us-east-1</code>.
							</div></dd><dt><a href="#CO64-15"><span class="callout">15</span></a> </dt><dd><div class="para">
								Specify the infrastructure ID and zone.
							</div></dd><dt><a href="#CO64-19"><span class="callout">19</span></a> </dt><dd><div class="para">
								Specify a taint to prevent user workloads from being scheduled on infra nodes.
							</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									After adding the <code class="literal">NoSchedule</code> taint on the infrastructure node, existing DNS pods running on that node are marked as <code class="literal">misscheduled</code>. You must either delete or <a class="link" href="https://access.redhat.com/solutions/6592171">add toleration on <code class="literal">misscheduled</code> DNS pods</a>.
								</p></div></div></dd></dl></div><p>
						Machine sets running on AWS support non-guaranteed <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machineset-non-guaranteed-instance_creating-machineset-aws">Spot Instances</a>. You can save on costs by using Spot Instances at a lower price compared to On-Demand Instances on AWS. <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machineset-creating-non-guaranteed-instance_creating-machineset-aws">Configure Spot Instances</a> by adding <code class="literal">spotMarketOptions</code> to the <code class="literal">MachineSet</code> YAML file.
					</p></section><section class="section" id="machineset-yaml-azure_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h4 class="title">7.2.1.3. Sample YAML for a compute machine set custom resource on Azure</h4></div></div></div><p>
						This sample YAML defines a compute machine set that runs in the <code class="literal">1</code> Microsoft Azure zone in a region and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/infra: ""</code>.
					</p><p>
						In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;infra&gt;</code> is the node label to add.
					</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO65-1"><!--Empty--></span><span class="callout">1</span>
    machine.openshift.io/cluster-api-machine-role: &lt;infra&gt; <span id="CO65-2"><!--Empty--></span><span class="callout">2</span>
    machine.openshift.io/cluster-api-machine-type: &lt;infra&gt; <span id="CO65-3"><!--Empty--></span><span class="callout">3</span>
  name: &lt;infrastructure_id&gt;-infra-&lt;region&gt; <span id="CO65-4"><!--Empty--></span><span class="callout">4</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO65-5"><!--Empty--></span><span class="callout">5</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-infra-&lt;region&gt; <span id="CO65-6"><!--Empty--></span><span class="callout">6</span>
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO65-7"><!--Empty--></span><span class="callout">7</span>
        machine.openshift.io/cluster-api-machine-role: &lt;infra&gt; <span id="CO65-8"><!--Empty--></span><span class="callout">8</span>
        machine.openshift.io/cluster-api-machine-type: &lt;infra&gt; <span id="CO65-9"><!--Empty--></span><span class="callout">9</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-infra-&lt;region&gt; <span id="CO65-10"><!--Empty--></span><span class="callout">10</span>
    spec:
      metadata:
        creationTimestamp: null
        labels:
          machine.openshift.io/cluster-api-machineset: &lt;machineset_name&gt; <span id="CO65-11"><!--Empty--></span><span class="callout">11</span>
          node-role.kubernetes.io/infra: "" <span id="CO65-12"><!--Empty--></span><span class="callout">12</span>
      providerSpec:
        value:
          apiVersion: azureproviderconfig.openshift.io/v1beta1
          credentialsSecret:
            name: azure-cloud-credentials
            namespace: openshift-machine-api
          image: <span id="CO65-13"><!--Empty--></span><span class="callout">13</span>
            offer: ""
            publisher: ""
            resourceID: /resourceGroups/&lt;infrastructure_id&gt;-rg/providers/Microsoft.Compute/images/&lt;infrastructure_id&gt; <span id="CO65-14"><!--Empty--></span><span class="callout">14</span>
            sku: ""
            version: ""
          internalLoadBalancer: ""
          kind: AzureMachineProviderSpec
          location: &lt;region&gt; <span id="CO65-15"><!--Empty--></span><span class="callout">15</span>
          managedIdentity: &lt;infrastructure_id&gt;-identity <span id="CO65-16"><!--Empty--></span><span class="callout">16</span>
          metadata:
            creationTimestamp: null
          natRule: null
          networkResourceGroup: ""
          osDisk:
            diskSizeGB: 128
            managedDisk:
              storageAccountType: Premium_LRS
            osType: Linux
          publicIP: false
          publicLoadBalancer: ""
          resourceGroup: &lt;infrastructure_id&gt;-rg <span id="CO65-17"><!--Empty--></span><span class="callout">17</span>
          sshPrivateKey: ""
          sshPublicKey: ""
          tags:
            - name: &lt;custom_tag_name&gt; <span id="CO65-18"><!--Empty--></span><span class="callout">18</span>
              value: &lt;custom_tag_value&gt; <span id="CO65-19"><!--Empty--></span><span class="callout">19</span>
          subnet: &lt;infrastructure_id&gt;-&lt;role&gt;-subnet <span id="CO65-20"><!--Empty--></span><span class="callout">20</span> <span id="CO65-21"><!--Empty--></span><span class="callout">21</span>
          userDataSecret:
            name: worker-user-data <span id="CO65-22"><!--Empty--></span><span class="callout">22</span>
          vmSize: Standard_D4s_v3
          vnet: &lt;infrastructure_id&gt;-vnet <span id="CO65-23"><!--Empty--></span><span class="callout">23</span>
          zone: "1" <span id="CO65-24"><!--Empty--></span><span class="callout">24</span>
      taints: <span id="CO65-25"><!--Empty--></span><span class="callout">25</span>
      - key: node-role.kubernetes.io/infra
        effect: NoSchedule</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO65-1"><span class="callout">1</span></a> <a href="#CO65-5"><span class="callout">5</span></a> <a href="#CO65-7"><span class="callout">7</span></a> <a href="#CO65-16"><span class="callout">16</span></a> <a href="#CO65-17"><span class="callout">17</span></a> <a href="#CO65-20"><span class="callout">20</span></a> <a href="#CO65-23"><span class="callout">23</span></a> </dt><dd><div class="para">
								Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
							</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre><p>
								You can obtain the subnet by running the following command:
							</p><pre class="programlisting language-terminal">$  oc -n openshift-machine-api \
    -o jsonpath='{.spec.template.spec.providerSpec.value.subnet}{"\n"}' \
    get machineset/&lt;infrastructure_id&gt;-worker-centralus1</pre><p>
								You can obtain the vnet by running the following command:
							</p><pre class="programlisting language-terminal">$  oc -n openshift-machine-api \
    -o jsonpath='{.spec.template.spec.providerSpec.value.vnet}{"\n"}' \
    get machineset/&lt;infrastructure_id&gt;-worker-centralus1</pre></dd><dt><a href="#CO65-2"><span class="callout">2</span></a> <a href="#CO65-3"><span class="callout">3</span></a> <a href="#CO65-8"><span class="callout">8</span></a> <a href="#CO65-9"><span class="callout">9</span></a> <a href="#CO65-12"><span class="callout">12</span></a> <a href="#CO65-21"><span class="callout">21</span></a> <a href="#CO65-22"><span class="callout">22</span></a> </dt><dd><div class="para">
								Specify the <code class="literal">&lt;infra&gt;</code> node label.
							</div></dd><dt><a href="#CO65-4"><span class="callout">4</span></a> <a href="#CO65-6"><span class="callout">6</span></a> <a href="#CO65-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								Specify the infrastructure ID, <code class="literal">&lt;infra&gt;</code> node label, and region.
							</div></dd><dt><a href="#CO65-11"><span class="callout">11</span></a> </dt><dd><div class="para">
								Optional: Specify the compute machine set name to enable the use of availability sets. This setting only applies to new compute machines.
							</div></dd><dt><a href="#CO65-13"><span class="callout">13</span></a> </dt><dd><div class="para">
								Specify the image details for your compute machine set. If you want to use an Azure Marketplace image, see "Selecting an Azure Marketplace image".
							</div></dd><dt><a href="#CO65-14"><span class="callout">14</span></a> </dt><dd><div class="para">
								Specify an image that is compatible with your instance type. The Hyper-V generation V2 images created by the installation program have a <code class="literal">-gen2</code> suffix, while V1 images have the same name without the suffix.
							</div></dd><dt><a href="#CO65-15"><span class="callout">15</span></a> </dt><dd><div class="para">
								Specify the region to place machines on.
							</div></dd><dt><a href="#CO65-24"><span class="callout">24</span></a> </dt><dd><div class="para">
								Specify the zone within your region to place machines on. Be sure that your region supports the zone that you specify.
							</div></dd><dt><a href="#CO65-18"><span class="callout">18</span></a> <a href="#CO65-19"><span class="callout">19</span></a> </dt><dd><div class="para">
								Optional: Specify custom tags in your machine set. Provide the tag name in <code class="literal">&lt;custom_tag_name&gt;</code> field and the corresponding tag value in <code class="literal">&lt;custom_tag_value&gt;</code> field.
							</div></dd><dt><a href="#CO65-25"><span class="callout">25</span></a> </dt><dd><div class="para">
								Specify a taint to prevent user workloads from being scheduled on infra nodes.
							</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									After adding the <code class="literal">NoSchedule</code> taint on the infrastructure node, existing DNS pods running on that node are marked as <code class="literal">misscheduled</code>. You must either delete or <a class="link" href="https://access.redhat.com/solutions/6592171">add toleration on <code class="literal">misscheduled</code> DNS pods</a>.
								</p></div></div></dd></dl></div><p>
						Machine sets running on Azure support non-guaranteed <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machineset-non-guaranteed-instance_creating-machineset-azure">Spot VMs</a>. You can save on costs by using Spot VMs at a lower price compared to standard VMs on Azure. You can <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machineset-creating-non-guaranteed-instance_creating-machineset-azure">configure Spot VMs</a> by adding <code class="literal">spotVMOptions</code> to the <code class="literal">MachineSet</code> YAML file.
					</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#installation-azure-marketplace-subscribe_creating-machineset-azure">Selecting an Azure Marketplace image</a>
							</li></ul></div></section><section class="section" id="machineset-yaml-azure-stack-hub_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h4 class="title">7.2.1.4. Sample YAML for a compute machine set custom resource on Azure Stack Hub</h4></div></div></div><p>
						This sample YAML defines a compute machine set that runs in the <code class="literal">1</code> Microsoft Azure zone in a region and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/infra: ""</code>.
					</p><p>
						In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;infra&gt;</code> is the node label to add.
					</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO66-1"><!--Empty--></span><span class="callout">1</span>
    machine.openshift.io/cluster-api-machine-role: &lt;infra&gt; <span id="CO66-2"><!--Empty--></span><span class="callout">2</span>
    machine.openshift.io/cluster-api-machine-type: &lt;infra&gt; <span id="CO66-3"><!--Empty--></span><span class="callout">3</span>
  name: &lt;infrastructure_id&gt;-infra-&lt;region&gt; <span id="CO66-4"><!--Empty--></span><span class="callout">4</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO66-5"><!--Empty--></span><span class="callout">5</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-infra-&lt;region&gt; <span id="CO66-6"><!--Empty--></span><span class="callout">6</span>
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO66-7"><!--Empty--></span><span class="callout">7</span>
        machine.openshift.io/cluster-api-machine-role: &lt;infra&gt; <span id="CO66-8"><!--Empty--></span><span class="callout">8</span>
        machine.openshift.io/cluster-api-machine-type: &lt;infra&gt; <span id="CO66-9"><!--Empty--></span><span class="callout">9</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-infra-&lt;region&gt; <span id="CO66-10"><!--Empty--></span><span class="callout">10</span>
    spec:
      metadata:
        creationTimestamp: null
        labels:
          node-role.kubernetes.io/infra: "" <span id="CO66-11"><!--Empty--></span><span class="callout">11</span>
      taints: <span id="CO66-12"><!--Empty--></span><span class="callout">12</span>
      - key: node-role.kubernetes.io/infra
        effect: NoSchedule
      providerSpec:
        value:
          apiVersion: machine.openshift.io/v1beta1
          availabilitySet: &lt;availability_set&gt; <span id="CO66-13"><!--Empty--></span><span class="callout">13</span>
          credentialsSecret:
            name: azure-cloud-credentials
            namespace: openshift-machine-api
          image:
            offer: ""
            publisher: ""
            resourceID: /resourceGroups/&lt;infrastructure_id&gt;-rg/providers/Microsoft.Compute/images/&lt;infrastructure_id&gt; <span id="CO66-14"><!--Empty--></span><span class="callout">14</span>
            sku: ""
            version: ""
          internalLoadBalancer: ""
          kind: AzureMachineProviderSpec
          location: &lt;region&gt; <span id="CO66-15"><!--Empty--></span><span class="callout">15</span>
          managedIdentity: &lt;infrastructure_id&gt;-identity <span id="CO66-16"><!--Empty--></span><span class="callout">16</span>
          metadata:
            creationTimestamp: null
          natRule: null
          networkResourceGroup: ""
          osDisk:
            diskSizeGB: 128
            managedDisk:
              storageAccountType: Premium_LRS
            osType: Linux
          publicIP: false
          publicLoadBalancer: ""
          resourceGroup: &lt;infrastructure_id&gt;-rg <span id="CO66-17"><!--Empty--></span><span class="callout">17</span>
          sshPrivateKey: ""
          sshPublicKey: ""
          subnet: &lt;infrastructure_id&gt;-&lt;role&gt;-subnet <span id="CO66-18"><!--Empty--></span><span class="callout">18</span> <span id="CO66-19"><!--Empty--></span><span class="callout">19</span>
          userDataSecret:
            name: worker-user-data <span id="CO66-20"><!--Empty--></span><span class="callout">20</span>
          vmSize: Standard_DS4_v2
          vnet: &lt;infrastructure_id&gt;-vnet <span id="CO66-21"><!--Empty--></span><span class="callout">21</span>
          zone: "1" <span id="CO66-22"><!--Empty--></span><span class="callout">22</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO66-1"><span class="callout">1</span></a> <a href="#CO66-5"><span class="callout">5</span></a> <a href="#CO66-7"><span class="callout">7</span></a> <a href="#CO66-14"><span class="callout">14</span></a> <a href="#CO66-16"><span class="callout">16</span></a> <a href="#CO66-17"><span class="callout">17</span></a> <a href="#CO66-18"><span class="callout">18</span></a> <a href="#CO66-21"><span class="callout">21</span></a> </dt><dd><div class="para">
								Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
							</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre><p>
								You can obtain the subnet by running the following command:
							</p><pre class="programlisting language-terminal">$  oc -n openshift-machine-api \
    -o jsonpath='{.spec.template.spec.providerSpec.value.subnet}{"\n"}' \
    get machineset/&lt;infrastructure_id&gt;-worker-centralus1</pre><p>
								You can obtain the vnet by running the following command:
							</p><pre class="programlisting language-terminal">$  oc -n openshift-machine-api \
    -o jsonpath='{.spec.template.spec.providerSpec.value.vnet}{"\n"}' \
    get machineset/&lt;infrastructure_id&gt;-worker-centralus1</pre></dd><dt><a href="#CO66-2"><span class="callout">2</span></a> <a href="#CO66-3"><span class="callout">3</span></a> <a href="#CO66-8"><span class="callout">8</span></a> <a href="#CO66-9"><span class="callout">9</span></a> <a href="#CO66-11"><span class="callout">11</span></a> <a href="#CO66-19"><span class="callout">19</span></a> <a href="#CO66-20"><span class="callout">20</span></a> </dt><dd><div class="para">
								Specify the <code class="literal">&lt;infra&gt;</code> node label.
							</div></dd><dt><a href="#CO66-4"><span class="callout">4</span></a> <a href="#CO66-6"><span class="callout">6</span></a> <a href="#CO66-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								Specify the infrastructure ID, <code class="literal">&lt;infra&gt;</code> node label, and region.
							</div></dd><dt><a href="#CO66-12"><span class="callout">12</span></a> </dt><dd><div class="para">
								Specify a taint to prevent user workloads from being scheduled on infra nodes.
							</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									After adding the <code class="literal">NoSchedule</code> taint on the infrastructure node, existing DNS pods running on that node are marked as <code class="literal">misscheduled</code>. You must either delete or <a class="link" href="https://access.redhat.com/solutions/6592171">add toleration on <code class="literal">misscheduled</code> DNS pods</a>.
								</p></div></div></dd><dt><a href="#CO66-15"><span class="callout">15</span></a> </dt><dd><div class="para">
								Specify the region to place machines on.
							</div></dd><dt><a href="#CO66-13"><span class="callout">13</span></a> </dt><dd><div class="para">
								Specify the availability set for the cluster.
							</div></dd><dt><a href="#CO66-22"><span class="callout">22</span></a> </dt><dd><div class="para">
								Specify the zone within your region to place machines on. Be sure that your region supports the zone that you specify.
							</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Machine sets running on Azure Stack Hub do not support non-guaranteed Spot VMs.
						</p></div></div></section><section class="section" id="machineset-yaml-ibm-cloud_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h4 class="title">7.2.1.5. Sample YAML for a compute machine set custom resource on IBM Cloud</h4></div></div></div><p>
						This sample YAML defines a compute machine set that runs in a specified IBM Cloud zone in a region and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/infra: ""</code>.
					</p><p>
						In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;infra&gt;</code> is the node label to add.
					</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO67-1"><!--Empty--></span><span class="callout">1</span>
    machine.openshift.io/cluster-api-machine-role: &lt;infra&gt; <span id="CO67-2"><!--Empty--></span><span class="callout">2</span>
    machine.openshift.io/cluster-api-machine-type: &lt;infra&gt; <span id="CO67-3"><!--Empty--></span><span class="callout">3</span>
  name: &lt;infrastructure_id&gt;-&lt;infra&gt;-&lt;region&gt; <span id="CO67-4"><!--Empty--></span><span class="callout">4</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO67-5"><!--Empty--></span><span class="callout">5</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;infra&gt;-&lt;region&gt; <span id="CO67-6"><!--Empty--></span><span class="callout">6</span>
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO67-7"><!--Empty--></span><span class="callout">7</span>
        machine.openshift.io/cluster-api-machine-role: &lt;infra&gt; <span id="CO67-8"><!--Empty--></span><span class="callout">8</span>
        machine.openshift.io/cluster-api-machine-type: &lt;infra&gt; <span id="CO67-9"><!--Empty--></span><span class="callout">9</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;infra&gt;-&lt;region&gt; <span id="CO67-10"><!--Empty--></span><span class="callout">10</span>
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/infra: ""
      providerSpec:
        value:
          apiVersion: ibmcloudproviderconfig.openshift.io/v1beta1
          credentialsSecret:
            name: ibmcloud-credentials
          image: &lt;infrastructure_id&gt;-rhcos <span id="CO67-11"><!--Empty--></span><span class="callout">11</span>
          kind: IBMCloudMachineProviderSpec
          primaryNetworkInterface:
              securityGroups:
              - &lt;infrastructure_id&gt;-sg-cluster-wide
              - &lt;infrastructure_id&gt;-sg-openshift-net
              subnet: &lt;infrastructure_id&gt;-subnet-compute-&lt;zone&gt; <span id="CO67-12"><!--Empty--></span><span class="callout">12</span>
          profile: &lt;instance_profile&gt; <span id="CO67-13"><!--Empty--></span><span class="callout">13</span>
          region: &lt;region&gt; <span id="CO67-14"><!--Empty--></span><span class="callout">14</span>
          resourceGroup: &lt;resource_group&gt; <span id="CO67-15"><!--Empty--></span><span class="callout">15</span>
          userDataSecret:
              name: &lt;role&gt;-user-data <span id="CO67-16"><!--Empty--></span><span class="callout">16</span>
          vpc: &lt;vpc_name&gt; <span id="CO67-17"><!--Empty--></span><span class="callout">17</span>
          zone: &lt;zone&gt; <span id="CO67-18"><!--Empty--></span><span class="callout">18</span>
        taints: <span id="CO67-19"><!--Empty--></span><span class="callout">19</span>
        - key: node-role.kubernetes.io/infra
          effect: NoSchedule</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO67-1"><span class="callout">1</span></a> <a href="#CO67-5"><span class="callout">5</span></a> <a href="#CO67-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								The infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
							</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><a href="#CO67-2"><span class="callout">2</span></a> <a href="#CO67-3"><span class="callout">3</span></a> <a href="#CO67-8"><span class="callout">8</span></a> <a href="#CO67-9"><span class="callout">9</span></a> <a href="#CO67-16"><span class="callout">16</span></a> </dt><dd><div class="para">
								The <code class="literal">&lt;infra&gt;</code> node label.
							</div></dd><dt><a href="#CO67-4"><span class="callout">4</span></a> <a href="#CO67-6"><span class="callout">6</span></a> <a href="#CO67-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								The infrastructure ID, <code class="literal">&lt;infra&gt;</code> node label, and region.
							</div></dd><dt><a href="#CO67-11"><span class="callout">11</span></a> </dt><dd><div class="para">
								The custom Red Hat Enterprise Linux CoreOS (RHCOS) image that was used for cluster installation.
							</div></dd><dt><a href="#CO67-12"><span class="callout">12</span></a> </dt><dd><div class="para">
								The infrastructure ID and zone within your region to place machines on. Be sure that your region supports the zone that you specify.
							</div></dd><dt><a href="#CO67-13"><span class="callout">13</span></a> </dt><dd><div class="para">
								Specify the <a class="link" href="https://cloud.ibm.com/docs/vpc?topic=vpc-profiles&amp;interface=ui">IBM Cloud instance profile</a>.
							</div></dd><dt><a href="#CO67-14"><span class="callout">14</span></a> </dt><dd><div class="para">
								Specify the region to place machines on.
							</div></dd><dt><a href="#CO67-15"><span class="callout">15</span></a> </dt><dd><div class="para">
								The resource group that machine resources are placed in. This is either an existing resource group specified at installation time, or an installer-created resource group named based on the infrastructure ID.
							</div></dd><dt><a href="#CO67-17"><span class="callout">17</span></a> </dt><dd><div class="para">
								The VPC name.
							</div></dd><dt><a href="#CO67-18"><span class="callout">18</span></a> </dt><dd><div class="para">
								Specify the zone within your region to place machines on. Be sure that your region supports the zone that you specify.
							</div></dd><dt><a href="#CO67-19"><span class="callout">19</span></a> </dt><dd><div class="para">
								The taint to prevent user workloads from being scheduled on infra nodes.
							</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									After adding the <code class="literal">NoSchedule</code> taint on the infrastructure node, existing DNS pods running on that node are marked as <code class="literal">misscheduled</code>. You must either delete or <a class="link" href="https://access.redhat.com/solutions/6592171">add toleration on <code class="literal">misscheduled</code> DNS pods</a>.
								</p></div></div></dd></dl></div></section><section class="section" id="machineset-yaml-gcp_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h4 class="title">7.2.1.6. Sample YAML for a compute machine set custom resource on GCP</h4></div></div></div><p>
						This sample YAML defines a compute machine set that runs in Google Cloud Platform (GCP) and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/infra: ""</code>, where <code class="literal">infra</code> is the node label to add.
					</p><h6 id="cpmso-yaml-provider-spec-gcp-oc_creating-infrastructure-machinesets">Values obtained by using the OpenShift CLI</h6><p>
						In the following example, you can obtain some of the values for your cluster by using the OpenShift CLI.
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Infrastructure ID</span></dt><dd><p class="simpara">
									The <code class="literal">&lt;infrastructure_id&gt;</code> string is the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
								</p><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><span class="term">Image path</span></dt><dd><p class="simpara">
									The <code class="literal">&lt;path_to_image&gt;</code> string is the path to the image that was used to create the disk. If you have the OpenShift CLI installed, you can obtain the path to the image by running the following command:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api \
  -o jsonpath='{.spec.template.spec.providerSpec.value.disks[0].image}{"\n"}' \
  get machineset/&lt;infrastructure_id&gt;-worker-a</pre></dd></dl></div><div class="formalpara"><p class="title"><strong>Sample GCP <code class="literal">MachineSet</code> values</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO68-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-w-a
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-w-a
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;infra&gt; <span id="CO68-2"><!--Empty--></span><span class="callout">2</span>
        machine.openshift.io/cluster-api-machine-type: &lt;infra&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-w-a
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/infra: ""
      providerSpec:
        value:
          apiVersion: gcpprovider.openshift.io/v1beta1
          canIPForward: false
          credentialsSecret:
            name: gcp-cloud-credentials
          deletionProtection: false
          disks:
          - autoDelete: true
            boot: true
            image: &lt;path_to_image&gt; <span id="CO68-3"><!--Empty--></span><span class="callout">3</span>
            labels: null
            sizeGb: 128
            type: pd-ssd
          gcpMetadata: <span id="CO68-4"><!--Empty--></span><span class="callout">4</span>
          - key: &lt;custom_metadata_key&gt;
            value: &lt;custom_metadata_value&gt;
          kind: GCPMachineProviderSpec
          machineType: n1-standard-4
          metadata:
            creationTimestamp: null
          networkInterfaces:
          - network: &lt;infrastructure_id&gt;-network
            subnetwork: &lt;infrastructure_id&gt;-worker-subnet
          projectID: &lt;project_name&gt; <span id="CO68-5"><!--Empty--></span><span class="callout">5</span>
          region: us-central1
          serviceAccounts:
          - email: &lt;infrastructure_id&gt;-w@&lt;project_name&gt;.iam.gserviceaccount.com
            scopes:
            - https://www.googleapis.com/auth/cloud-platform
          tags:
            - &lt;infrastructure_id&gt;-worker
          userDataSecret:
            name: worker-user-data
          zone: us-central1-a
      taints: <span id="CO68-6"><!--Empty--></span><span class="callout">6</span>
      - key: node-role.kubernetes.io/infra
        effect: NoSchedule</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO68-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								For <code class="literal">&lt;infrastructure_id&gt;</code>, specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster.
							</div></dd><dt><a href="#CO68-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								For <code class="literal">&lt;infra&gt;</code>, specify the <code class="literal">&lt;infra&gt;</code> node label.
							</div></dd><dt><a href="#CO68-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specify the path to the image that is used in current compute machine sets.
							</div><p>
								To use a GCP Marketplace image, specify the offer to use:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										OpenShift Container Platform: <code class="literal">https://www.googleapis.com/compute/v1/projects/redhat-marketplace-public/global/images/redhat-coreos-ocp-48-x86-64-202210040145</code>
									</li><li class="listitem">
										OpenShift Platform Plus: <code class="literal">https://www.googleapis.com/compute/v1/projects/redhat-marketplace-public/global/images/redhat-coreos-opp-48-x86-64-202206140145</code>
									</li><li class="listitem">
										OpenShift Kubernetes Engine: <code class="literal">https://www.googleapis.com/compute/v1/projects/redhat-marketplace-public/global/images/redhat-coreos-oke-48-x86-64-202206140145</code>
									</li></ul></div></dd><dt><a href="#CO68-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Optional: Specify custom metadata in the form of a <code class="literal">key:value</code> pair. For example use cases, see the GCP documentation for <a class="link" href="https://cloud.google.com/compute/docs/metadata/setting-custom-metadata">setting custom metadata</a>.
							</div></dd><dt><a href="#CO68-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								For <code class="literal">&lt;project_name&gt;</code>, specify the name of the GCP project that you use for your cluster.
							</div></dd><dt><a href="#CO68-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								Specify a taint to prevent user workloads from being scheduled on infra nodes.
							</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									After adding the <code class="literal">NoSchedule</code> taint on the infrastructure node, existing DNS pods running on that node are marked as <code class="literal">misscheduled</code>. You must either delete or <a class="link" href="https://access.redhat.com/solutions/6592171">add toleration on <code class="literal">misscheduled</code> DNS pods</a>.
								</p></div></div></dd></dl></div><p>
						Machine sets running on GCP support non-guaranteed <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machineset-non-guaranteed-instance_creating-machineset-gcp">preemptible VM instances</a>. You can save on costs by using preemptible VM instances at a lower price compared to normal instances on GCP. You can <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machineset-creating-non-guaranteed-instance_creating-machineset-gcp">configure preemptible VM instances</a> by adding <code class="literal">preemptible</code> to the <code class="literal">MachineSet</code> YAML file.
					</p></section><section class="section" id="machineset-yaml-nutanix_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h4 class="title">7.2.1.7. Sample YAML for a compute machine set custom resource on Nutanix</h4></div></div></div><p>
						This sample YAML defines a Nutanix compute machine set that creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/infra: ""</code>.
					</p><p>
						In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;infra&gt;</code> is the node label to add.
					</p><h6 id="machineset-yaml-nutanix-oc_creating-infrastructure-machinesets">Values obtained by using the OpenShift CLI</h6><p>
						In the following example, you can obtain some of the values for your cluster by using the OpenShift CLI (<code class="literal">oc</code>).
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Infrastructure ID</span></dt><dd><p class="simpara">
									The <code class="literal">&lt;infrastructure_id&gt;</code> string is the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
								</p><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd></dl></div><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO69-1"><!--Empty--></span><span class="callout">1</span>
    machine.openshift.io/cluster-api-machine-role: &lt;infra&gt; <span id="CO69-2"><!--Empty--></span><span class="callout">2</span>
    machine.openshift.io/cluster-api-machine-type: &lt;infra&gt;
  name: &lt;infrastructure_id&gt;-&lt;infra&gt;-&lt;zone&gt; <span id="CO69-3"><!--Empty--></span><span class="callout">3</span>
  namespace: openshift-machine-api
  annotations: <span id="CO69-4"><!--Empty--></span><span class="callout">4</span>
    machine.openshift.io/memoryMb: "16384"
    machine.openshift.io/vCPU: "4"
spec:
  replicas: 3
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;infra&gt;-&lt;zone&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;infra&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;infra&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;infra&gt;-&lt;zone&gt;
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/infra: ""
      providerSpec:
        value:
          apiVersion: machine.openshift.io/v1
          bootType: "" <span id="CO69-5"><!--Empty--></span><span class="callout">5</span>
          categories: <span id="CO69-6"><!--Empty--></span><span class="callout">6</span>
          - key: &lt;category_name&gt;
            value: &lt;category_value&gt;
          cluster: <span id="CO69-7"><!--Empty--></span><span class="callout">7</span>
            type: uuid
            uuid: &lt;cluster_uuid&gt;
          credentialsSecret:
            name: nutanix-creds-secret
          image:
            name: &lt;infrastructure_id&gt;-rhcos <span id="CO69-8"><!--Empty--></span><span class="callout">8</span>
            type: name
          kind: NutanixMachineProviderConfig
          memorySize: 16Gi <span id="CO69-9"><!--Empty--></span><span class="callout">9</span>
          project: <span id="CO69-10"><!--Empty--></span><span class="callout">10</span>
            type: name
            name: &lt;project_name&gt;
          subnets:
          - type: uuid
            uuid: &lt;subnet_uuid&gt;
          systemDiskSize: 120Gi <span id="CO69-11"><!--Empty--></span><span class="callout">11</span>
          userDataSecret:
            name: &lt;user_data_secret&gt; <span id="CO69-12"><!--Empty--></span><span class="callout">12</span>
          vcpuSockets: 4 <span id="CO69-13"><!--Empty--></span><span class="callout">13</span>
          vcpusPerSocket: 1 <span id="CO69-14"><!--Empty--></span><span class="callout">14</span>
      taints: <span id="CO69-15"><!--Empty--></span><span class="callout">15</span>
      - key: node-role.kubernetes.io/infra
        effect: NoSchedule</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO69-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								For <code class="literal">&lt;infrastructure_id&gt;</code>, specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster.
							</div></dd><dt><a href="#CO69-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specify the <code class="literal">&lt;infra&gt;</code> node label.
							</div></dd><dt><a href="#CO69-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specify the infrastructure ID, <code class="literal">&lt;infra&gt;</code> node label, and zone.
							</div></dd><dt><a href="#CO69-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Annotations for the cluster autoscaler.
							</div></dd><dt><a href="#CO69-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Specifies the boot type that the compute machines use. For more information about boot types, see <a class="link" href="https://portal.nutanix.com/page/documents/kbs/details?targetId=kA07V000000H3K9SAK">Understanding UEFI, Secure Boot, and TPM in the Virtualized Environment</a>. Valid values are <code class="literal">Legacy</code>, <code class="literal">SecureBoot</code>, or <code class="literal">UEFI</code>. The default is <code class="literal">Legacy</code>.
							</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									You must use the <code class="literal">Legacy</code> boot type in OpenShift Container Platform 4.13.
								</p></div></div></dd><dt><a href="#CO69-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								Specify one or more Nutanix Prism categories to apply to compute machines. This stanza requires <code class="literal">key</code> and <code class="literal">value</code> parameters for a category key-value pair that exists in Prism Central. For more information about categories, see <a class="link" href="https://portal.nutanix.com/page/documents/details?targetId=Prism-Central-Guide-vpc_2022_6:ssp-ssp-categories-manage-pc-c.html">Category management</a>.
							</div></dd><dt><a href="#CO69-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								Specify a Nutanix Prism Element cluster configuration. In this example, the cluster type is <code class="literal">uuid</code>, so there is a <code class="literal">uuid</code> stanza.
							</div></dd><dt><a href="#CO69-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								Specify the image to use. Use an image from an existing default compute machine set for the cluster.
							</div></dd><dt><a href="#CO69-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								Specify the amount of memory for the cluster in Gi.
							</div></dd><dt><a href="#CO69-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								Specify the Nutanix project that you use for your cluster. In this example, the project type is <code class="literal">name</code>, so there is a <code class="literal">name</code> stanza.
							</div></dd><dt><a href="#CO69-11"><span class="callout">11</span></a> </dt><dd><div class="para">
								Specify the size of the system disk in Gi.
							</div></dd><dt><a href="#CO69-12"><span class="callout">12</span></a> </dt><dd><div class="para">
								Specify the name of the secret in the user data YAML file that is in the <code class="literal">openshift-machine-api</code> namespace. Use the value that installation program populates in the default compute machine set.
							</div></dd><dt><a href="#CO69-13"><span class="callout">13</span></a> </dt><dd><div class="para">
								Specify the number of vCPU sockets.
							</div></dd><dt><a href="#CO69-14"><span class="callout">14</span></a> </dt><dd><div class="para">
								Specify the number of vCPUs per socket.
							</div></dd><dt><a href="#CO69-15"><span class="callout">15</span></a> </dt><dd><div class="para">
								Specify a taint to prevent user workloads from being scheduled on infra nodes.
							</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									After adding the <code class="literal">NoSchedule</code> taint on the infrastructure node, existing DNS pods running on that node are marked as <code class="literal">misscheduled</code>. You must either delete or <a class="link" href="https://access.redhat.com/solutions/6592171">add toleration on <code class="literal">misscheduled</code> DNS pods</a>.
								</p></div></div></dd></dl></div></section><section class="section" id="machineset-yaml-osp_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h4 class="title">7.2.1.8. Sample YAML for a compute machine set custom resource on RHOSP</h4></div></div></div><p>
						This sample YAML defines a compute machine set that runs on Red Hat OpenStack Platform (RHOSP) and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/infra: ""</code>.
					</p><p>
						In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;infra&gt;</code> is the node label to add.
					</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO70-1"><!--Empty--></span><span class="callout">1</span>
    machine.openshift.io/cluster-api-machine-role: &lt;infra&gt; <span id="CO70-2"><!--Empty--></span><span class="callout">2</span>
    machine.openshift.io/cluster-api-machine-type: &lt;infra&gt; <span id="CO70-3"><!--Empty--></span><span class="callout">3</span>
  name: &lt;infrastructure_id&gt;-infra <span id="CO70-4"><!--Empty--></span><span class="callout">4</span>
  namespace: openshift-machine-api
spec:
  replicas: &lt;number_of_replicas&gt;
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO70-5"><!--Empty--></span><span class="callout">5</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-infra <span id="CO70-6"><!--Empty--></span><span class="callout">6</span>
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO70-7"><!--Empty--></span><span class="callout">7</span>
        machine.openshift.io/cluster-api-machine-role: &lt;infra&gt; <span id="CO70-8"><!--Empty--></span><span class="callout">8</span>
        machine.openshift.io/cluster-api-machine-type: &lt;infra&gt; <span id="CO70-9"><!--Empty--></span><span class="callout">9</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-infra <span id="CO70-10"><!--Empty--></span><span class="callout">10</span>
    spec:
      metadata:
        creationTimestamp: null
        labels:
          node-role.kubernetes.io/infra: ""
      taints: <span id="CO70-11"><!--Empty--></span><span class="callout">11</span>
      - key: node-role.kubernetes.io/infra
        effect: NoSchedule
      providerSpec:
        value:
          apiVersion: openstackproviderconfig.openshift.io/v1alpha1
          cloudName: openstack
          cloudsSecret:
            name: openstack-cloud-credentials
            namespace: openshift-machine-api
          flavor: &lt;nova_flavor&gt;
          image: &lt;glance_image_name_or_location&gt;
          serverGroupID: &lt;optional_UUID_of_server_group&gt; <span id="CO70-12"><!--Empty--></span><span class="callout">12</span>
          kind: OpenstackProviderSpec
          networks: <span id="CO70-13"><!--Empty--></span><span class="callout">13</span>
          - filter: {}
            subnets:
            - filter:
                name: &lt;subnet_name&gt;
                tags: openshiftClusterID=&lt;infrastructure_id&gt; <span id="CO70-14"><!--Empty--></span><span class="callout">14</span>
          primarySubnet: &lt;rhosp_subnet_UUID&gt; <span id="CO70-15"><!--Empty--></span><span class="callout">15</span>
          securityGroups:
          - filter: {}
            name: &lt;infrastructure_id&gt;-worker <span id="CO70-16"><!--Empty--></span><span class="callout">16</span>
          serverMetadata:
            Name: &lt;infrastructure_id&gt;-worker <span id="CO70-17"><!--Empty--></span><span class="callout">17</span>
            openshiftClusterID: &lt;infrastructure_id&gt; <span id="CO70-18"><!--Empty--></span><span class="callout">18</span>
          tags:
          - openshiftClusterID=&lt;infrastructure_id&gt; <span id="CO70-19"><!--Empty--></span><span class="callout">19</span>
          trunk: true
          userDataSecret:
            name: worker-user-data <span id="CO70-20"><!--Empty--></span><span class="callout">20</span>
          availabilityZone: &lt;optional_openstack_availability_zone&gt;</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO70-1"><span class="callout">1</span></a> <a href="#CO70-5"><span class="callout">5</span></a> <a href="#CO70-7"><span class="callout">7</span></a> <a href="#CO70-14"><span class="callout">14</span></a> <a href="#CO70-16"><span class="callout">16</span></a> <a href="#CO70-17"><span class="callout">17</span></a> <a href="#CO70-18"><span class="callout">18</span></a> <a href="#CO70-19"><span class="callout">19</span></a> </dt><dd><div class="para">
								Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
							</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><a href="#CO70-2"><span class="callout">2</span></a> <a href="#CO70-3"><span class="callout">3</span></a> <a href="#CO70-8"><span class="callout">8</span></a> <a href="#CO70-9"><span class="callout">9</span></a> <a href="#CO70-20"><span class="callout">20</span></a> </dt><dd><div class="para">
								Specify the <code class="literal">&lt;infra&gt;</code> node label.
							</div></dd><dt><a href="#CO70-4"><span class="callout">4</span></a> <a href="#CO70-6"><span class="callout">6</span></a> <a href="#CO70-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								Specify the infrastructure ID and <code class="literal">&lt;infra&gt;</code> node label.
							</div></dd><dt><a href="#CO70-11"><span class="callout">11</span></a> </dt><dd><div class="para">
								Specify a taint to prevent user workloads from being scheduled on infra nodes.
							</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									After adding the <code class="literal">NoSchedule</code> taint on the infrastructure node, existing DNS pods running on that node are marked as <code class="literal">misscheduled</code>. You must either delete or <a class="link" href="https://access.redhat.com/solutions/6592171">add toleration on <code class="literal">misscheduled</code> DNS pods</a>.
								</p></div></div></dd><dt><a href="#CO70-12"><span class="callout">12</span></a> </dt><dd><div class="para">
								To set a server group policy for the MachineSet, enter the value that is returned from <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.0/html/command_line_interface_reference/server#server_group_create">creating a server group</a>. For most deployments, <code class="literal">anti-affinity</code> or <code class="literal">soft-anti-affinity</code> policies are recommended.
							</div></dd><dt><a href="#CO70-13"><span class="callout">13</span></a> </dt><dd><div class="para">
								Required for deployments to multiple networks. If deploying to multiple networks, this list must include the network that is used as the <code class="literal">primarySubnet</code> value.
							</div></dd><dt><a href="#CO70-15"><span class="callout">15</span></a> </dt><dd><div class="para">
								Specify the RHOSP subnet that you want the endpoints of nodes to be published on. Usually, this is the same subnet that is used as the value of <code class="literal">machinesSubnet</code> in the <code class="literal">install-config.yaml</code> file.
							</div></dd></dl></div></section><section class="section" id="machineset-yaml-rhv_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h4 class="title">7.2.1.9. Sample YAML for a compute machine set custom resource on RHV</h4></div></div></div><p>
						This sample YAML defines a compute machine set that runs on RHV and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/&lt;node_role&gt;: ""</code>.
					</p><p>
						In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;role&gt;</code> is the node label to add.
					</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO71-1"><!--Empty--></span><span class="callout">1</span>
    machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO71-2"><!--Empty--></span><span class="callout">2</span>
    machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO71-3"><!--Empty--></span><span class="callout">3</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO71-4"><!--Empty--></span><span class="callout">4</span>
  namespace: openshift-machine-api
spec:
  replicas: &lt;number_of_replicas&gt; <span id="CO71-5"><!--Empty--></span><span class="callout">5</span>
  Selector: <span id="CO71-6"><!--Empty--></span><span class="callout">6</span>
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO71-7"><!--Empty--></span><span class="callout">7</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO71-8"><!--Empty--></span><span class="callout">8</span>
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO71-9"><!--Empty--></span><span class="callout">9</span>
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO71-10"><!--Empty--></span><span class="callout">10</span>
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO71-11"><!--Empty--></span><span class="callout">11</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO71-12"><!--Empty--></span><span class="callout">12</span>
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/&lt;role&gt;: "" <span id="CO71-13"><!--Empty--></span><span class="callout">13</span>
      providerSpec:
        value:
          apiVersion: ovirtproviderconfig.machine.openshift.io/v1beta1
          cluster_id: &lt;ovirt_cluster_id&gt; <span id="CO71-14"><!--Empty--></span><span class="callout">14</span>
          template_name: &lt;ovirt_template_name&gt; <span id="CO71-15"><!--Empty--></span><span class="callout">15</span>
          sparse: &lt;boolean_value&gt; <span id="CO71-16"><!--Empty--></span><span class="callout">16</span>
          format: &lt;raw_or_cow&gt; <span id="CO71-17"><!--Empty--></span><span class="callout">17</span>
          cpu: <span id="CO71-18"><!--Empty--></span><span class="callout">18</span>
            sockets: &lt;number_of_sockets&gt; <span id="CO71-19"><!--Empty--></span><span class="callout">19</span>
            cores: &lt;number_of_cores&gt; <span id="CO71-20"><!--Empty--></span><span class="callout">20</span>
            threads: &lt;number_of_threads&gt; <span id="CO71-21"><!--Empty--></span><span class="callout">21</span>
          memory_mb: &lt;memory_size&gt; <span id="CO71-22"><!--Empty--></span><span class="callout">22</span>
          guaranteed_memory_mb:  &lt;memory_size&gt; <span id="CO71-23"><!--Empty--></span><span class="callout">23</span>
          os_disk: <span id="CO71-24"><!--Empty--></span><span class="callout">24</span>
            size_gb: &lt;disk_size&gt; <span id="CO71-25"><!--Empty--></span><span class="callout">25</span>
            storage_domain_id: &lt;storage_domain_UUID&gt; <span id="CO71-26"><!--Empty--></span><span class="callout">26</span>
          network_interfaces: <span id="CO71-27"><!--Empty--></span><span class="callout">27</span>
            vnic_profile_id:  &lt;vnic_profile_id&gt; <span id="CO71-28"><!--Empty--></span><span class="callout">28</span>
          credentialsSecret:
            name: ovirt-credentials <span id="CO71-29"><!--Empty--></span><span class="callout">29</span>
          kind: OvirtMachineProviderSpec
          type: &lt;workload_type&gt; <span id="CO71-30"><!--Empty--></span><span class="callout">30</span>
          auto_pinning_policy: &lt;auto_pinning_policy&gt; <span id="CO71-31"><!--Empty--></span><span class="callout">31</span>
          hugepages: &lt;hugepages&gt; <span id="CO71-32"><!--Empty--></span><span class="callout">32</span>
          affinityGroupsNames:
            - compute <span id="CO71-33"><!--Empty--></span><span class="callout">33</span>
          userDataSecret:
            name: worker-user-data</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO71-1"><span class="callout">1</span></a> <a href="#CO71-7"><span class="callout">7</span></a> <a href="#CO71-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI (<code class="literal">oc</code>) installed, you can obtain the infrastructure ID by running the following command:
							</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><a href="#CO71-2"><span class="callout">2</span></a> <a href="#CO71-3"><span class="callout">3</span></a> <a href="#CO71-10"><span class="callout">10</span></a> <a href="#CO71-11"><span class="callout">11</span></a> <a href="#CO71-13"><span class="callout">13</span></a> </dt><dd><div class="para">
								Specify the node label to add.
							</div></dd><dt><a href="#CO71-4"><span class="callout">4</span></a> <a href="#CO71-8"><span class="callout">8</span></a> <a href="#CO71-12"><span class="callout">12</span></a> </dt><dd><div class="para">
								Specify the infrastructure ID and node label. These two strings together cannot be longer than 35 characters.
							</div></dd><dt><a href="#CO71-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Specify the number of machines to create.
							</div></dd><dt><a href="#CO71-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								Selector for the machines.
							</div></dd><dt><a href="#CO71-14"><span class="callout">14</span></a> </dt><dd><div class="para">
								Specify the UUID for the RHV cluster to which this VM instance belongs.
							</div></dd><dt><a href="#CO71-15"><span class="callout">15</span></a> </dt><dd><div class="para">
								Specify the RHV VM template to use to create the machine.
							</div></dd><dt><a href="#CO71-16"><span class="callout">16</span></a> </dt><dd><div class="para">
								Setting this option to <code class="literal">false</code> enables preallocation of disks. The default is <code class="literal">true</code>. Setting <code class="literal">sparse</code> to <code class="literal">true</code> with <code class="literal">format</code> set to <code class="literal">raw</code> is not available for block storage domains. The <code class="literal">raw</code> format writes the entire virtual disk to the underlying physical disk.
							</div></dd><dt><a href="#CO71-17"><span class="callout">17</span></a> </dt><dd><div class="para">
								Can be set to <code class="literal">cow</code> or <code class="literal">raw</code>. The default is <code class="literal">cow</code>. The <code class="literal">cow</code> format is optimized for virtual machines.
							</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Preallocating disks on file storage domains writes zeroes to the file. This might not actually preallocate disks depending on the underlying storage.
								</p></div></div></dd><dt><a href="#CO71-18"><span class="callout">18</span></a> </dt><dd><div class="para">
								Optional: The CPU field contains the CPU configuration, including sockets, cores, and threads.
							</div></dd><dt><a href="#CO71-19"><span class="callout">19</span></a> </dt><dd><div class="para">
								Optional: Specify the number of sockets for a VM.
							</div></dd><dt><a href="#CO71-20"><span class="callout">20</span></a> </dt><dd><div class="para">
								Optional: Specify the number of cores per socket.
							</div></dd><dt><a href="#CO71-21"><span class="callout">21</span></a> </dt><dd><div class="para">
								Optional: Specify the number of threads per core.
							</div></dd><dt><a href="#CO71-22"><span class="callout">22</span></a> </dt><dd><div class="para">
								Optional: Specify the size of a VM’s memory in MiB.
							</div></dd><dt><a href="#CO71-23"><span class="callout">23</span></a> </dt><dd><div class="para">
								Optional: Specify the size of a virtual machine’s guaranteed memory in MiB. This is the amount of memory that is guaranteed not to be drained by the ballooning mechanism. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/administration_guide#memory_ballooning">Memory Ballooning</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/administration_guide#Cluster_Optimization_Settings_Explained">Optimization Settings Explained</a>.
							</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									If you are using a version earlier than RHV 4.4.8, see <a class="link" href="https://access.redhat.com/articles/6454811">Guaranteed memory requirements for OpenShift on Red Hat Virtualization clusters</a>.
								</p></div></div></dd><dt><a href="#CO71-24"><span class="callout">24</span></a> </dt><dd><div class="para">
								Optional: Root disk of the node.
							</div></dd><dt><a href="#CO71-25"><span class="callout">25</span></a> </dt><dd><div class="para">
								Optional: Specify the size of the bootable disk in GiB.
							</div></dd><dt><a href="#CO71-26"><span class="callout">26</span></a> </dt><dd><div class="para">
								Optional: Specify the UUID of the storage domain for the compute node’s disks. If none is provided, the compute node is created on the same storage domain as the control nodes. (default)
							</div></dd><dt><a href="#CO71-27"><span class="callout">27</span></a> </dt><dd><div class="para">
								Optional: List of the network interfaces of the VM. If you include this parameter, OpenShift Container Platform discards all network interfaces from the template and creates new ones.
							</div></dd><dt><a href="#CO71-28"><span class="callout">28</span></a> </dt><dd><div class="para">
								Optional: Specify the vNIC profile ID.
							</div></dd><dt><a href="#CO71-29"><span class="callout">29</span></a> </dt><dd><div class="para">
								Specify the name of the secret object that holds the RHV credentials.
							</div></dd><dt><a href="#CO71-30"><span class="callout">30</span></a> </dt><dd><div class="para">
								Optional: Specify the workload type for which the instance is optimized. This value affects the <code class="literal">RHV VM</code> parameter. Supported values: <code class="literal">desktop</code>, <code class="literal">server</code> (default), <code class="literal">high_performance</code>. <code class="literal">high_performance</code> improves performance on the VM. Limitations exist, for example, you cannot access the VM with a graphical console. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/virtual_machine_management_guide/index#Configuring_High_Performance_Virtual_Machines_Templates_and_Pools">Configuring High Performance Virtual Machines, Templates, and Pools</a> in the <span class="emphasis"><em>Virtual Machine Management Guide</em></span>.
							</div></dd><dt><a href="#CO71-31"><span class="callout">31</span></a> </dt><dd><div class="para">
								Optional: AutoPinningPolicy defines the policy that automatically sets CPU and NUMA settings, including pinning to the host for this instance. Supported values: <code class="literal">none</code>, <code class="literal">resize_and_pin</code>. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/virtual_machine_management_guide/index#Setting_NUMA_Nodes">Setting NUMA Nodes</a> in the <span class="emphasis"><em>Virtual Machine Management Guide</em></span>.
							</div></dd><dt><a href="#CO71-32"><span class="callout">32</span></a> </dt><dd><div class="para">
								Optional: Hugepages is the size in KiB for defining hugepages in a VM. Supported values: <code class="literal">2048</code> or <code class="literal">1048576</code>. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/virtual_machine_management_guide/index#Configuring_Huge_Pages">Configuring Huge Pages</a> in the <span class="emphasis"><em>Virtual Machine Management Guide</em></span>.
							</div></dd><dt><a href="#CO71-33"><span class="callout">33</span></a> </dt><dd><div class="para">
								Optional: A list of affinity group names to be applied to the VMs. The affinity groups must exist in oVirt.
							</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Because RHV uses a template when creating a VM, if you do not specify a value for an optional parameter, RHV uses the value for that parameter that is specified in the template.
						</p></div></div></section><section class="section" id="machineset-yaml-vsphere_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h4 class="title">7.2.1.10. Sample YAML for a compute machine set custom resource on vSphere</h4></div></div></div><p>
						This sample YAML defines a compute machine set that runs on VMware vSphere and creates nodes that are labeled with <code class="literal">node-role.kubernetes.io/infra: ""</code>.
					</p><p>
						In this sample, <code class="literal">&lt;infrastructure_id&gt;</code> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and <code class="literal">&lt;infra&gt;</code> is the node label to add.
					</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  creationTimestamp: null
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO72-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-infra <span id="CO72-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO72-3"><!--Empty--></span><span class="callout">3</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-infra <span id="CO72-4"><!--Empty--></span><span class="callout">4</span>
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO72-5"><!--Empty--></span><span class="callout">5</span>
        machine.openshift.io/cluster-api-machine-role: &lt;infra&gt; <span id="CO72-6"><!--Empty--></span><span class="callout">6</span>
        machine.openshift.io/cluster-api-machine-type: &lt;infra&gt; <span id="CO72-7"><!--Empty--></span><span class="callout">7</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-infra <span id="CO72-8"><!--Empty--></span><span class="callout">8</span>
    spec:
      metadata:
        creationTimestamp: null
        labels:
          node-role.kubernetes.io/infra: "" <span id="CO72-9"><!--Empty--></span><span class="callout">9</span>
      taints: <span id="CO72-10"><!--Empty--></span><span class="callout">10</span>
      - key: node-role.kubernetes.io/infra
        effect: NoSchedule
      providerSpec:
        value:
          apiVersion: vsphereprovider.openshift.io/v1beta1
          credentialsSecret:
            name: vsphere-cloud-credentials
          diskGiB: 120
          kind: VSphereMachineProviderSpec
          memoryMiB: 8192
          metadata:
            creationTimestamp: null
          network:
            devices:
            - networkName: "&lt;vm_network_name&gt;" <span id="CO72-11"><!--Empty--></span><span class="callout">11</span>
          numCPUs: 4
          numCoresPerSocket: 1
          snapshot: ""
          template: &lt;vm_template_name&gt; <span id="CO72-12"><!--Empty--></span><span class="callout">12</span>
          userDataSecret:
            name: worker-user-data
          workspace:
            datacenter: &lt;vcenter_datacenter_name&gt; <span id="CO72-13"><!--Empty--></span><span class="callout">13</span>
            datastore: &lt;vcenter_datastore_name&gt; <span id="CO72-14"><!--Empty--></span><span class="callout">14</span>
            folder: &lt;vcenter_vm_folder_path&gt; <span id="CO72-15"><!--Empty--></span><span class="callout">15</span>
            resourcepool: &lt;vsphere_resource_pool&gt; <span id="CO72-16"><!--Empty--></span><span class="callout">16</span>
            server: &lt;vcenter_server_ip&gt; <span id="CO72-17"><!--Empty--></span><span class="callout">17</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO72-1"><span class="callout">1</span></a> <a href="#CO72-3"><span class="callout">3</span></a> <a href="#CO72-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI (<code class="literal">oc</code>) installed, you can obtain the infrastructure ID by running the following command:
							</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><a href="#CO72-2"><span class="callout">2</span></a> <a href="#CO72-4"><span class="callout">4</span></a> <a href="#CO72-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								Specify the infrastructure ID and <code class="literal">&lt;infra&gt;</code> node label.
							</div></dd><dt><a href="#CO72-6"><span class="callout">6</span></a> <a href="#CO72-7"><span class="callout">7</span></a> <a href="#CO72-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								Specify the <code class="literal">&lt;infra&gt;</code> node label.
							</div></dd><dt><a href="#CO72-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								Specify a taint to prevent user workloads from being scheduled on infra nodes.
							</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									After adding the <code class="literal">NoSchedule</code> taint on the infrastructure node, existing DNS pods running on that node are marked as <code class="literal">misscheduled</code>. You must either delete or <a class="link" href="https://access.redhat.com/solutions/6592171">add toleration on <code class="literal">misscheduled</code> DNS pods</a>.
								</p></div></div></dd><dt><a href="#CO72-11"><span class="callout">11</span></a> </dt><dd><div class="para">
								Specify the vSphere VM network to deploy the compute machine set to. This VM network must be where other compute machines reside in the cluster.
							</div></dd><dt><a href="#CO72-12"><span class="callout">12</span></a> </dt><dd><div class="para">
								Specify the vSphere VM template to use, such as <code class="literal">user-5ddjd-rhcos</code>.
							</div></dd><dt><a href="#CO72-13"><span class="callout">13</span></a> </dt><dd><div class="para">
								Specify the vCenter Datacenter to deploy the compute machine set on.
							</div></dd><dt><a href="#CO72-14"><span class="callout">14</span></a> </dt><dd><div class="para">
								Specify the vCenter Datastore to deploy the compute machine set on.
							</div></dd><dt><a href="#CO72-15"><span class="callout">15</span></a> </dt><dd><div class="para">
								Specify the path to the vSphere VM folder in vCenter, such as <code class="literal">/dc1/vm/user-inst-5ddjd</code>.
							</div></dd><dt><a href="#CO72-16"><span class="callout">16</span></a> </dt><dd><div class="para">
								Specify the vSphere resource pool for your VMs.
							</div></dd><dt><a href="#CO72-17"><span class="callout">17</span></a> </dt><dd><div class="para">
								Specify the vCenter server IP or fully qualified domain name.
							</div></dd></dl></div></section></section><section class="section" id="machineset-creating_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h3 class="title">7.2.2. Creating a compute machine set</h3></div></div></div><p>
					In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Deploy an OpenShift Container Platform cluster.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <code class="literal">&lt;file_name&gt;.yaml</code>.
						</p><p class="simpara">
							Ensure that you set the <code class="literal">&lt;clusterID&gt;</code> and <code class="literal">&lt;role&gt;</code> parameter values.
						</p></li><li class="listitem"><p class="simpara">
							Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									To list the compute machine sets in your cluster, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									To view values of a specific compute machine set custom resource (CR), run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machineset &lt;machineset_name&gt; \
  -n openshift-machine-api -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO73-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO73-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
    spec:
      providerSpec: <span id="CO73-3"><!--Empty--></span><span class="callout">3</span>
        ...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO73-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The cluster infrastructure ID.
										</div></dd><dt><a href="#CO73-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											A default node label.
										</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												For clusters that have user-provisioned infrastructure, a compute machine set can only create <code class="literal">worker</code> and <code class="literal">infra</code> type machines.
											</p></div></div></dd><dt><a href="#CO73-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											The values in the <code class="literal">&lt;providerSpec&gt;</code> section of the compute machine set CR are platform-specific. For more information about <code class="literal">&lt;providerSpec&gt;</code> parameters in the CR, see the sample compute machine set CR configuration for your provider.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">MachineSet</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							View the list of compute machine sets by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-infra-us-east-1a    1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

							</p></div><p class="simpara">
							When the new compute machine set is available, the <code class="literal">DESIRED</code> and <code class="literal">CURRENT</code> values match. If the compute machine set is not available, wait a few minutes and run the command again.
						</p></li></ul></div></section><section class="section" id="creating-an-infra-node_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h3 class="title">7.2.3. Creating an infrastructure node</h3></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						See Creating infrastructure machine sets for installer-provisioned infrastructure environments or for any cluster where the control plane nodes are managed by the machine API.
					</p></div></div><p>
					Requirements of the cluster dictate that infrastructure, also called <code class="literal">infra</code> nodes, be provisioned. The installer only provides provisions for control plane and worker nodes. Worker nodes can be designated as infrastructure nodes or application, also called <code class="literal">app</code>, nodes through labeling.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add a label to the worker node that you want to act as application node:
						</p><pre class="programlisting language-terminal">$ oc label node &lt;node-name&gt; node-role.kubernetes.io/app=""</pre></li><li class="listitem"><p class="simpara">
							Add a label to the worker nodes that you want to act as infrastructure nodes:
						</p><pre class="programlisting language-terminal">$ oc label node &lt;node-name&gt; node-role.kubernetes.io/infra=""</pre></li><li class="listitem"><p class="simpara">
							Check to see if applicable nodes now have the <code class="literal">infra</code> role and <code class="literal">app</code> roles:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre></li><li class="listitem"><p class="simpara">
							Create a default cluster-wide node selector. The default node selector is applied to pods created in all namespaces. This creates an intersection with any existing node selectors on a pod, which additionally constrains the pod’s selector.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								If the default node selector key conflicts with the key of a pod’s label, then the default node selector is not applied.
							</p><p>
								However, do not set a default node selector that might cause a pod to become unschedulable. For example, setting the default node selector to a specific node role, such as <code class="literal">node-role.kubernetes.io/infra=""</code>, when a pod’s label is set to a different node role, such as <code class="literal">node-role.kubernetes.io/master=""</code>, can cause the pod to become unschedulable. For this reason, use caution when setting the default node selector to specific node roles.
							</p><p>
								You can alternatively use a project node selector to avoid cluster-wide node selector key conflicts.
							</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Edit the <code class="literal">Scheduler</code> object:
								</p><pre class="programlisting language-terminal">$ oc edit scheduler cluster</pre></li><li class="listitem"><p class="simpara">
									Add the <code class="literal">defaultNodeSelector</code> field with the appropriate node selector:
								</p><pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  name: cluster
spec:
  defaultNodeSelector: topology.kubernetes.io/region=us-east-1 <span id="CO74-1"><!--Empty--></span><span class="callout">1</span>
# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO74-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											This example node selector deploys pods on nodes in the <code class="literal">us-east-1</code> region by default.
										</div></dd></dl></div></li><li class="listitem">
									Save the file to apply the changes.
								</li></ol></div></li></ol></div><p>
					You can now move infrastructure resources to the newly labeled <code class="literal">infra</code> nodes.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="#moving-resources-to-infrastructure-machinesets" title="7.4. Moving resources to infrastructure machine sets">Moving resources to infrastructure machine sets</a>
						</li></ul></div></section><section class="section" id="creating-infra-machines_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h3 class="title">7.2.4. Creating a machine config pool for infrastructure machines</h3></div></div></div><p>
					If you need infrastructure machines to have dedicated configurations, you must create an infra pool.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add a label to the node you want to assign as the infra node with a specific label:
						</p><pre class="programlisting language-terminal">$ oc label node &lt;node_name&gt; &lt;label&gt;</pre><pre class="programlisting language-terminal">$ oc label node ci-ln-n8mqwr2-f76d1-xscn2-worker-c-6fmtx node-role.kubernetes.io/infra=</pre></li><li class="listitem"><p class="simpara">
							Create a machine config pool that contains both the worker role and your custom role as machine config selector:
						</p><pre class="programlisting language-terminal">$ cat infra.mcp.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: infra
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,infra]} <span id="CO75-1"><!--Empty--></span><span class="callout">1</span>
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/infra: "" <span id="CO75-2"><!--Empty--></span><span class="callout">2</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO75-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add the worker role and your custom role.
								</div></dd><dt><a href="#CO75-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Add the label you added to the node as a <code class="literal">nodeSelector</code>.
								</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Custom machine config pools inherit machine configs from the worker pool. Custom pools use any machine config targeted for the worker pool, but add the ability to also deploy changes that are targeted at only the custom pool. Because a custom pool inherits resources from the worker pool, any change to the worker pool also affects the custom pool.
							</p></div></div></li><li class="listitem"><p class="simpara">
							After you have the YAML file, you can create the machine config pool:
						</p><pre class="programlisting language-terminal">$ oc create -f infra.mcp.yaml</pre></li><li class="listitem"><p class="simpara">
							Check the machine configs to ensure that the infrastructure configuration rendered successfully:
						</p><pre class="programlisting language-terminal">$ oc get machineconfig</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                        GENERATEDBYCONTROLLER                      IGNITIONVERSION   CREATED
00-master                                                   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
00-worker                                                   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
01-master-container-runtime                                 365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
01-master-kubelet                                           365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
01-worker-container-runtime                                 365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
01-worker-kubelet                                           365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
99-master-1ae2a1e0-a115-11e9-8f14-005056899d54-registries   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
99-master-ssh                                                                                          3.2.0             31d
99-worker-1ae64748-a115-11e9-8f14-005056899d54-registries   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
99-worker-ssh                                                                                          3.2.0             31d
rendered-infra-4e48906dca84ee702959c71a53ee80e7             365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             23m
rendered-master-072d4b2da7f88162636902b074e9e28e            5b6fb8349a29735e48446d435962dec4547d3090   3.2.0             31d
rendered-master-3e88ec72aed3886dec061df60d16d1af            02c07496ba0417b3e12b78fb32baf6293d314f79   3.2.0             31d
rendered-master-419bee7de96134963a15fdf9dd473b25            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             17d
rendered-master-53f5c91c7661708adce18739cc0f40fb            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             13d
rendered-master-a6a357ec18e5bce7f5ac426fc7c5ffcd            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             7d3h
rendered-master-dc7f874ec77fc4b969674204332da037            5b6fb8349a29735e48446d435962dec4547d3090   3.2.0             31d
rendered-worker-1a75960c52ad18ff5dfa6674eb7e533d            5b6fb8349a29735e48446d435962dec4547d3090   3.2.0             31d
rendered-worker-2640531be11ba43c61d72e82dc634ce6            5b6fb8349a29735e48446d435962dec4547d3090   3.2.0             31d
rendered-worker-4e48906dca84ee702959c71a53ee80e7            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             7d3h
rendered-worker-4f110718fe88e5f349987854a1147755            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             17d
rendered-worker-afc758e194d6188677eb837842d3b379            02c07496ba0417b3e12b78fb32baf6293d314f79   3.2.0             31d
rendered-worker-daa08cc1e8f5fcdeba24de60cd955cc3            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             13d</pre>

							</p></div><p class="simpara">
							You should see a new machine config, with the <code class="literal">rendered-infra-*</code> prefix.
						</p></li><li class="listitem"><p class="simpara">
							Optional: To deploy changes to a custom pool, create a machine config that uses the custom pool name as the label, such as <code class="literal">infra</code>. Note that this is not required and only shown for instructional purposes. In this manner, you can apply any custom configurations specific to only your infra nodes.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								After you create the new machine config pool, the MCO generates a new rendered config for that pool, and associated nodes of that pool reboot to apply the new configuration.
							</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a machine config:
								</p><pre class="programlisting language-terminal">$ cat infra.mc.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  name: 51-infra
  labels:
    machineconfiguration.openshift.io/role: infra <span id="CO76-1"><!--Empty--></span><span class="callout">1</span>
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
      - path: /etc/infratest
        mode: 0644
        contents:
          source: data:,infra</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO76-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Add the label you added to the node as a <code class="literal">nodeSelector</code>.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Apply the machine config to the infra-labeled nodes:
								</p><pre class="programlisting language-terminal">$ oc create -f infra.mc.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Confirm that your new machine config pool is available:
						</p><pre class="programlisting language-terminal">$ oc get mcp</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
infra    rendered-infra-60e35c2e99f42d976e084fa94da4d0fc    True      False      False      1              1                   1                     0                      4m20s
master   rendered-master-9360fdb895d4c131c7c4bebbae099c90   True      False      False      3              3                   3                     0                      91m
worker   rendered-worker-60e35c2e99f42d976e084fa94da4d0fc   True      False      False      2              2                   2                     0                      91m</pre>

							</p></div><p class="simpara">
							In this example, a worker node was changed to an infra node.
						</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/architecture/#architecture-machine-config-pools_control-plane">Node configuration management with machine config pools</a> for more information on grouping infra machines in a custom pool.
						</li></ul></div></section></section><section class="section" id="assigning-machineset-resources-to-infra-nodes"><div class="titlepage"><div><div><h2 class="title">7.3. Assigning machine set resources to infrastructure nodes</h2></div></div></div><p>
				After creating an infrastructure machine set, the <code class="literal">worker</code> and <code class="literal">infra</code> roles are applied to new infra nodes. Nodes with the <code class="literal">infra</code> role applied are not counted toward the total number of subscriptions that are required to run the environment, even when the <code class="literal">worker</code> role is also applied.
			</p><p>
				However, with an infra node being assigned as a worker, there is a chance user workloads could get inadvertently assigned to an infra node. To avoid this, you can apply a taint to the infra node and tolerations for the pods you want to control.
			</p><section class="section" id="binding-infra-node-workloads-using-taints-tolerations_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h3 class="title">7.3.1. Binding infrastructure node workloads using taints and tolerations</h3></div></div></div><p>
					If you have an infra node that has the <code class="literal">infra</code> and <code class="literal">worker</code> roles assigned, you must configure the node so that user workloads are not assigned to it.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						It is recommended that you preserve the dual <code class="literal">infra,worker</code> label that is created for infra nodes and use taints and tolerations to manage nodes that user workloads are scheduled on. If you remove the <code class="literal">worker</code> label from the node, you must create a custom pool to manage it. A node with a label other than <code class="literal">master</code> or <code class="literal">worker</code> is not recognized by the MCO without a custom pool. Maintaining the <code class="literal">worker</code> label allows the node to be managed by the default worker machine config pool, if no custom pools that select the custom label exists. The <code class="literal">infra</code> label communicates to the cluster that it does not count toward the total number of subscriptions.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Configure additional <code class="literal">MachineSet</code> objects in your OpenShift Container Platform cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add a taint to the infra node to prevent scheduling user workloads on it:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Determine if the node has the taint:
								</p><pre class="programlisting language-terminal">$ oc describe nodes &lt;node_name&gt;</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
										
<pre class="programlisting language-text">oc describe node ci-ln-iyhx092-f76d1-nvdfm-worker-b-wln2l
Name:               ci-ln-iyhx092-f76d1-nvdfm-worker-b-wln2l
Roles:              worker
 ...
Taints:             node-role.kubernetes.io/infra:NoSchedule
 ...</pre>

									</p></div><p class="simpara">
									This example shows that the node has a taint. You can proceed with adding a toleration to your pod in the next step.
								</p></li><li class="listitem"><p class="simpara">
									If you have not configured a taint to prevent scheduling user workloads on it:
								</p><pre class="programlisting language-terminal">$ oc adm taint nodes &lt;node_name&gt; &lt;key&gt;=&lt;value&gt;:&lt;effect&gt;</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc adm taint nodes node1 node-role.kubernetes.io/infra=reserved:NoExecute</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
									You can alternatively apply the following YAML to add the taint:
								</p><pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  name: &lt;node_name&gt;
  labels:
    ...
spec:
  taints:
    - key: node-role.kubernetes.io/infra
      effect: NoExecute
      value: reserved
  ...</pre></div></div><p class="simpara">
									This example places a taint on <code class="literal">node1</code> that has key <code class="literal">node-role.kubernetes.io/infra</code> and taint effect <code class="literal">NoSchedule</code>. Nodes with the <code class="literal">NoSchedule</code> effect schedule only pods that tolerate the taint, but allow existing pods to remain scheduled on the node.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										If a descheduler is used, pods violating node taints could be evicted from the cluster.
									</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Add tolerations for the pod configurations you want to schedule on the infra node, like router, registry, and monitoring workloads. Add the following code to the <code class="literal">Pod</code> object specification:
						</p><pre class="programlisting language-yaml">tolerations:
  - effect: NoExecute <span id="CO77-1"><!--Empty--></span><span class="callout">1</span>
    key: node-role.kubernetes.io/infra <span id="CO77-2"><!--Empty--></span><span class="callout">2</span>
    operator: Exists <span id="CO77-3"><!--Empty--></span><span class="callout">3</span>
    value: reserved <span id="CO77-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO77-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the effect that you added to the node.
								</div></dd><dt><a href="#CO77-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the key that you added to the node.
								</div></dd><dt><a href="#CO77-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">Exists</code> Operator to require a taint with the key <code class="literal">node-role.kubernetes.io/infra</code> to be present on the node.
								</div></dd><dt><a href="#CO77-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specify the value of the key-value pair taint that you added to the node.
								</div></dd></dl></div><p class="simpara">
							This toleration matches the taint created by the <code class="literal">oc adm taint</code> command. A pod with this toleration can be scheduled onto the infra node.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Moving pods for an Operator installed via OLM to an infra node is not always possible. The capability to move Operator pods depends on the configuration of each Operator.
							</p></div></div></li><li class="listitem">
							Schedule the pod to the infra node using a scheduler. See the documentation for <span class="emphasis"><em>Controlling pod placement onto nodes</em></span> for details.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-about">Controlling pod placement using the scheduler</a> for general information on scheduling a pod to a node.
						</li><li class="listitem">
							See <a class="link" href="#moving-resources-to-infrastructure-machinesets" title="7.4. Moving resources to infrastructure machine sets">Moving resources to infrastructure machine sets</a> for instructions on scheduling pods to infra nodes.
						</li></ul></div></section></section><section class="section" id="moving-resources-to-infrastructure-machinesets"><div class="titlepage"><div><div><h2 class="title">7.4. Moving resources to infrastructure machine sets</h2></div></div></div><p>
				Some of the infrastructure resources are deployed in your cluster by default. You can move them to the infrastructure machine sets that you created by adding the infrastructure node selector, as shown:
			</p><pre class="programlisting language-yaml">spec:
  nodePlacement: <span id="CO78-1"><!--Empty--></span><span class="callout">1</span>
    nodeSelector:
      matchLabels:
        node-role.kubernetes.io/infra: ""
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/infra
      value: reserved
    - effect: NoExecute
      key: node-role.kubernetes.io/infra
      value: reserved</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO78-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						Add a <code class="literal">nodeSelector</code> parameter with the appropriate value to the component you want to move. You can use a <code class="literal">nodeSelector</code> in the format shown or use <code class="literal">&lt;key&gt;: &lt;value&gt;</code> pairs, based on the value specified for the node. If you added a taint to the infrasructure node, also add a matching toleration.
					</div></dd></dl></div><p>
				Applying a specific node selector to all infrastructure components causes OpenShift Container Platform to <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#moving-resources-to-infrastructure-machinesets">schedule those workloads on nodes with that label</a>.
			</p><section class="section" id="infrastructure-moving-router_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h3 class="title">7.4.1. Moving the router</h3></div></div></div><p>
					You can deploy the router pod to a different compute machine set. By default, the pod is deployed to a worker node.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Configure additional compute machine sets in your OpenShift Container Platform cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the <code class="literal">IngressController</code> custom resource for the router Operator:
						</p><pre class="programlisting language-terminal">$ oc get ingresscontroller default -n openshift-ingress-operator -o yaml</pre><p class="simpara">
							The command output resembles the following text:
						</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: 2019-04-18T12:35:39Z
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 1
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "11341"
  selfLink: /apis/operator.openshift.io/v1/namespaces/openshift-ingress-operator/ingresscontrollers/default
  uid: 79509e05-61d6-11e9-bc55-02ce4781844a
spec: {}
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2019-04-18T12:36:15Z
    status: "True"
    type: Available
  domain: apps.&lt;cluster&gt;.example.com
  endpointPublishingStrategy:
    type: LoadBalancerService
  selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default</pre></li><li class="listitem"><p class="simpara">
							Edit the <code class="literal">ingresscontroller</code> resource and change the <code class="literal">nodeSelector</code> to use the <code class="literal">infra</code> label:
						</p><pre class="programlisting language-terminal">$ oc edit ingresscontroller default -n openshift-ingress-operator</pre><pre class="programlisting language-yaml">  spec:
    nodePlacement:
      nodeSelector: <span id="CO79-1"><!--Empty--></span><span class="callout">1</span>
        matchLabels:
          node-role.kubernetes.io/infra: ""
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/infra
        value: reserved
      - effect: NoExecute
        key: node-role.kubernetes.io/infra
        value: reserved</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO79-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add a <code class="literal">nodeSelector</code> parameter with the appropriate value to the component you want to move. You can use a <code class="literal">nodeSelector</code> in the format shown or use <code class="literal">&lt;key&gt;: &lt;value&gt;</code> pairs, based on the value specified for the node. If you added a taint to the infrastructure node, also add a matching toleration.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Confirm that the router pod is running on the <code class="literal">infra</code> node.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									View the list of router pods and note the node name of the running pod:
								</p><pre class="programlisting language-terminal">$ oc get pod -n openshift-ingress -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                              READY     STATUS        RESTARTS   AGE       IP           NODE                           NOMINATED NODE   READINESS GATES
router-default-86798b4b5d-bdlvd   1/1      Running       0          28s       10.130.2.4   ip-10-0-217-226.ec2.internal   &lt;none&gt;           &lt;none&gt;
router-default-955d875f4-255g8    0/1      Terminating   0          19h       10.129.2.4   ip-10-0-148-172.ec2.internal   &lt;none&gt;           &lt;none&gt;</pre>

									</p></div><p class="simpara">
									In this example, the running pod is on the <code class="literal">ip-10-0-217-226.ec2.internal</code> node.
								</p></li><li class="listitem"><p class="simpara">
									View the node status of the running pod:
								</p><pre class="programlisting language-terminal">$ oc get node &lt;node_name&gt; <span id="CO80-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO80-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the <code class="literal">&lt;node_name&gt;</code> that you obtained from the pod list.
										</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                          STATUS  ROLES         AGE   VERSION
ip-10-0-217-226.ec2.internal  Ready   infra,worker  17h   v1.26.0</pre>

									</p></div><p class="simpara">
									Because the role list includes <code class="literal">infra</code>, the pod is running on the correct node.
								</p></li></ol></div></li></ol></div></section><section class="section" id="infrastructure-moving-registry_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h3 class="title">7.4.2. Moving the default registry</h3></div></div></div><p>
					You configure the registry Operator to deploy its pods to different nodes.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Configure additional compute machine sets in your OpenShift Container Platform cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the <code class="literal">config/instance</code> object:
						</p><pre class="programlisting language-terminal">$ oc get configs.imageregistry.operator.openshift.io/cluster -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: imageregistry.operator.openshift.io/v1
kind: Config
metadata:
  creationTimestamp: 2019-02-05T13:52:05Z
  finalizers:
  - imageregistry.operator.openshift.io/finalizer
  generation: 1
  name: cluster
  resourceVersion: "56174"
  selfLink: /apis/imageregistry.operator.openshift.io/v1/configs/cluster
  uid: 36fd3724-294d-11e9-a524-12ffeee2931b
spec:
  httpSecret: d9a012ccd117b1e6616ceccb2c3bb66a5fed1b5e481623
  logging: 2
  managementState: Managed
  proxy: {}
  replicas: 1
  requests:
    read: {}
    write: {}
  storage:
    s3:
      bucket: image-registry-us-east-1-c92e88cad85b48ec8b312344dff03c82-392c
      region: us-east-1
status:
...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Edit the <code class="literal">config/instance</code> object:
						</p><pre class="programlisting language-terminal">$ oc edit configs.imageregistry.operator.openshift.io/cluster</pre><pre class="programlisting language-yaml">spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          namespaces:
          - openshift-image-registry
          topologyKey: kubernetes.io/hostname
        weight: 100
  logLevel: Normal
  managementState: Managed
  nodeSelector: <span id="CO81-1"><!--Empty--></span><span class="callout">1</span>
    node-role.kubernetes.io/infra: ""
  tolerations:
  - effect: NoSchedule
    key: node-role.kubernetes.io/infra
    value: reserved
  - effect: NoExecute
    key: node-role.kubernetes.io/infra
    value: reserved</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO81-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add a <code class="literal">nodeSelector</code> parameter with the appropriate value to the component you want to move. You can use a <code class="literal">nodeSelector</code> in the format shown or use <code class="literal">&lt;key&gt;: &lt;value&gt;</code> pairs, based on the value specified for the node. If you added a taint to the infrasructure node, also add a matching toleration.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Verify the registry pod has been moved to the infrastructure node.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Run the following command to identify the node where the registry pod is located:
								</p><pre class="programlisting language-terminal">$ oc get pods -o wide -n openshift-image-registry</pre></li><li class="listitem"><p class="simpara">
									Confirm the node has the label you specified:
								</p><pre class="programlisting language-terminal">$ oc describe node &lt;node_name&gt;</pre><p class="simpara">
									Review the command output and confirm that <code class="literal">node-role.kubernetes.io/infra</code> is in the <code class="literal">LABELS</code> list.
								</p></li></ol></div></li></ol></div></section><section class="section" id="infrastructure-moving-monitoring_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h3 class="title">7.4.3. Moving the monitoring solution</h3></div></div></div><p>
					The monitoring stack includes multiple components, including Prometheus, Thanos Querier, and Alertmanager. The Cluster Monitoring Operator manages this stack. To redeploy the monitoring stack to infrastructure nodes, you can create and apply a custom config map.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">cluster-monitoring-config</code> config map and change the <code class="literal">nodeSelector</code> to use the <code class="literal">infra</code> label:
						</p><pre class="programlisting language-terminal">$ oc edit configmap cluster-monitoring-config -n openshift-monitoring</pre><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |+
    alertmanagerMain:
      nodeSelector: <span id="CO82-1"><!--Empty--></span><span class="callout">1</span>
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    prometheusK8s:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    prometheusOperator:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    kubeStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    telemeterClient:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    openshiftStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    thanosQuerier:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO82-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add a <code class="literal">nodeSelector</code> parameter with the appropriate value to the component you want to move. You can use a <code class="literal">nodeSelector</code> in the format shown or use <code class="literal">&lt;key&gt;: &lt;value&gt;</code> pairs, based on the value specified for the node. If you added a taint to the infrasructure node, also add a matching toleration.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Watch the monitoring pods move to the new machines:
						</p><pre class="programlisting language-terminal">$ watch 'oc get pod -n openshift-monitoring -o wide'</pre></li><li class="listitem"><p class="simpara">
							If a component has not moved to the <code class="literal">infra</code> node, delete the pod with this component:
						</p><pre class="programlisting language-terminal">$ oc delete pod -n openshift-monitoring &lt;pod&gt;</pre><p class="simpara">
							The component from the deleted pod is re-created on the <code class="literal">infra</code> node.
						</p></li></ol></div></section><section class="section" id="infrastructure-moving-logging_creating-infrastructure-machinesets"><div class="titlepage"><div><div><h3 class="title">7.4.4. Moving OpenShift Logging resources</h3></div></div></div><p>
					You can configure the Cluster Logging Operator to deploy the pods for logging subsystem components, such as Elasticsearch and Kibana, to different nodes. You cannot move the Cluster Logging Operator pod from its installed location.
				</p><p>
					For example, you can move the Elasticsearch pods to a separate node because of high CPU, memory, and disk requirements.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The Red Hat OpenShift Logging and Elasticsearch Operators must be installed. These features are not installed by default.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">ClusterLogging</code> custom resource (CR) in the <code class="literal">openshift-logging</code> project:
						</p><pre class="programlisting language-terminal">$ oc edit ClusterLogging instance</pre><pre class="programlisting language-yaml">apiVersion: logging.openshift.io/v1
kind: ClusterLogging

...

spec:
  collection:
    logs:
      fluentd:
        resources: null
      type: fluentd
  logStore:
    elasticsearch:
      nodeCount: 3
      nodeSelector: <span id="CO83-1"><!--Empty--></span><span class="callout">1</span>
        node-role.kubernetes.io/infra: ''
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/infra
        value: reserved
      - effect: NoExecute
        key: node-role.kubernetes.io/infra
        value: reserved
      redundancyPolicy: SingleRedundancy
      resources:
        limits:
          cpu: 500m
          memory: 16Gi
        requests:
          cpu: 500m
          memory: 16Gi
      storage: {}
    type: elasticsearch
  managementState: Managed
  visualization:
    kibana:
      nodeSelector: <span id="CO83-2"><!--Empty--></span><span class="callout">2</span>
        node-role.kubernetes.io/infra: ''
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/infra
        value: reserved
      - effect: NoExecute
        key: node-role.kubernetes.io/infra
        value: reserved
      proxy:
        resources: null
      replicas: 1
      resources: null
    type: kibana

...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO83-1"><span class="callout">1</span></a> <a href="#CO83-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Add a <code class="literal">nodeSelector</code> parameter with the appropriate value to the component you want to move. You can use a <code class="literal">nodeSelector</code> in the format shown or use <code class="literal">&lt;key&gt;: &lt;value&gt;</code> pairs, based on the value specified for the node. If you added a taint to the infrasructure node, also add a matching toleration.
								</div></dd></dl></div></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						To verify that a component has moved, you can use the <code class="literal">oc get pod -o wide</code> command.
					</p></div><p>
					For example:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							You want to move the Kibana pod from the <code class="literal">ip-10-0-147-79.us-east-2.compute.internal</code> node:
						</p><pre class="programlisting language-terminal">$ oc get pod kibana-5b8bdf44f9-ccpq9 -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                      READY   STATUS    RESTARTS   AGE   IP            NODE                                        NOMINATED NODE   READINESS GATES
kibana-5b8bdf44f9-ccpq9   2/2     Running   0          27s   10.129.2.18   ip-10-0-147-79.us-east-2.compute.internal   &lt;none&gt;           &lt;none&gt;</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							You want to move the Kibana pod to the <code class="literal">ip-10-0-139-48.us-east-2.compute.internal</code> node, a dedicated infrastructure node:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                         STATUS   ROLES          AGE   VERSION
ip-10-0-133-216.us-east-2.compute.internal   Ready    master         60m   v1.26.0
ip-10-0-139-146.us-east-2.compute.internal   Ready    master         60m   v1.26.0
ip-10-0-139-192.us-east-2.compute.internal   Ready    worker         51m   v1.26.0
ip-10-0-139-241.us-east-2.compute.internal   Ready    worker         51m   v1.26.0
ip-10-0-147-79.us-east-2.compute.internal    Ready    worker         51m   v1.26.0
ip-10-0-152-241.us-east-2.compute.internal   Ready    master         60m   v1.26.0
ip-10-0-139-48.us-east-2.compute.internal    Ready    infra          51m   v1.26.0</pre>

							</p></div><p class="simpara">
							Note that the node has a <code class="literal">node-role.kubernetes.io/infra: ''</code> label:
						</p><pre class="programlisting language-terminal">$ oc get node ip-10-0-139-48.us-east-2.compute.internal -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  name: ip-10-0-139-48.us-east-2.compute.internal
  selfLink: /api/v1/nodes/ip-10-0-139-48.us-east-2.compute.internal
  uid: 62038aa9-661f-41d7-ba93-b5f1b6ef8751
  resourceVersion: '39083'
  creationTimestamp: '2020-04-13T19:07:55Z'
  labels:
    node-role.kubernetes.io/infra: ''
...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To move the Kibana pod, edit the <code class="literal">ClusterLogging</code> CR to add a node selector:
						</p><pre class="programlisting language-yaml">apiVersion: logging.openshift.io/v1
kind: ClusterLogging

...

spec:

...

  visualization:
    kibana:
      nodeSelector: <span id="CO84-1"><!--Empty--></span><span class="callout">1</span>
        node-role.kubernetes.io/infra: ''
      proxy:
        resources: null
      replicas: 1
      resources: null
    type: kibana</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO84-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add a node selector to match the label in the node specification.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							After you save the CR, the current Kibana pod is terminated and new pod is deployed:
						</p><pre class="programlisting language-terminal">$ oc get pods</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                            READY   STATUS        RESTARTS   AGE
cluster-logging-operator-84d98649c4-zb9g7       1/1     Running       0          29m
elasticsearch-cdm-hwv01pf7-1-56588f554f-kpmlg   2/2     Running       0          28m
elasticsearch-cdm-hwv01pf7-2-84c877d75d-75wqj   2/2     Running       0          28m
elasticsearch-cdm-hwv01pf7-3-f5d95b87b-4nx78    2/2     Running       0          28m
fluentd-42dzz                                   1/1     Running       0          28m
fluentd-d74rq                                   1/1     Running       0          28m
fluentd-m5vr9                                   1/1     Running       0          28m
fluentd-nkxl7                                   1/1     Running       0          28m
fluentd-pdvqb                                   1/1     Running       0          28m
fluentd-tflh6                                   1/1     Running       0          28m
kibana-5b8bdf44f9-ccpq9                         2/2     Terminating   0          4m11s
kibana-7d85dcffc8-bfpfp                         2/2     Running       0          33s</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							The new pod is on the <code class="literal">ip-10-0-139-48.us-east-2.compute.internal</code> node:
						</p><pre class="programlisting language-terminal">$ oc get pod kibana-7d85dcffc8-bfpfp -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                      READY   STATUS        RESTARTS   AGE   IP            NODE                                        NOMINATED NODE   READINESS GATES
kibana-7d85dcffc8-bfpfp   2/2     Running       0          43s   10.131.0.22   ip-10-0-139-48.us-east-2.compute.internal   &lt;none&gt;           &lt;none&gt;</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							After a few moments, the original Kibana pod is removed.
						</p><pre class="programlisting language-terminal">$ oc get pods</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                            READY   STATUS    RESTARTS   AGE
cluster-logging-operator-84d98649c4-zb9g7       1/1     Running   0          30m
elasticsearch-cdm-hwv01pf7-1-56588f554f-kpmlg   2/2     Running   0          29m
elasticsearch-cdm-hwv01pf7-2-84c877d75d-75wqj   2/2     Running   0          29m
elasticsearch-cdm-hwv01pf7-3-f5d95b87b-4nx78    2/2     Running   0          29m
fluentd-42dzz                                   1/1     Running   0          29m
fluentd-d74rq                                   1/1     Running   0          29m
fluentd-m5vr9                                   1/1     Running   0          29m
fluentd-nkxl7                                   1/1     Running   0          29m
fluentd-pdvqb                                   1/1     Running   0          29m
fluentd-tflh6                                   1/1     Running   0          29m
kibana-7d85dcffc8-bfpfp                         2/2     Running   0          62s</pre>

							</p></div></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/monitoring/#moving-monitoring-components-to-different-nodes_configuring-the-monitoring-stack">the monitoring documentation</a> for the general instructions on moving OpenShift Container Platform components.
						</li></ul></div></section></section></section><section class="chapter" id="adding-rhel-compute"><div class="titlepage"><div><div><h1 class="title">Chapter 8. Adding RHEL compute machines to an OpenShift Container Platform cluster</h1></div></div></div><p>
			In OpenShift Container Platform, you can add Red Hat Enterprise Linux (RHEL) compute machines to a user-provisioned infrastructure cluster or an installation-provisioned infrastructure cluster on the <code class="literal">x86_64</code> architecture. You can use RHEL as the operating system only on compute machines.
		</p><section class="section" id="rhel-compute-overview_adding-rhel-compute"><div class="titlepage"><div><div><h2 class="title">8.1. About adding RHEL compute nodes to a cluster</h2></div></div></div><p>
				In OpenShift Container Platform 4.13, you have the option of using Red Hat Enterprise Linux (RHEL) machines as compute machines in your cluster if you use a user-provisioned or installer-provisioned infrastructure installation on the <code class="literal">x86_64</code> architecture. You must use Red Hat Enterprise Linux CoreOS (RHCOS) machines for the control plane machines in your cluster.
			</p><p>
				If you choose to use RHEL compute machines in your cluster, you are responsible for all operating system life cycle management and maintenance. You must perform system updates, apply patches, and complete all other required tasks.
			</p><p>
				For installer-provisioned infrastructure clusters, you must manually add RHEL compute machines because automatic scaling in installer-provisioned infrastructure clusters adds Red Hat Enterprise Linux CoreOS (RHCOS) compute machines by default.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Because removing OpenShift Container Platform from a machine in the cluster requires destroying the operating system, you must use dedicated hardware for any RHEL machines that you add to the cluster.
						</li><li class="listitem">
							Swap memory is disabled on all RHEL machines that you add to your OpenShift Container Platform cluster. You cannot enable swap memory on these machines.
						</li></ul></div></div></div><p>
				You must add any RHEL compute machines to the cluster after you initialize the control plane.
			</p></section><section class="section" id="rhel-compute-requirements_adding-rhel-compute"><div class="titlepage"><div><div><h2 class="title">8.2. System requirements for RHEL compute nodes</h2></div></div></div><p>
				The Red Hat Enterprise Linux (RHEL) compute machine hosts in your OpenShift Container Platform environment must meet the following minimum hardware specifications and system-level requirements:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						You must have an active OpenShift Container Platform subscription on your Red Hat account. If you do not, contact your sales representative for more information.
					</li><li class="listitem">
						Production environments must provide compute machines to support your expected workloads. As a cluster administrator, you must calculate the expected workload and add about 10% for overhead. For production environments, allocate enough resources so that a node host failure does not affect your maximum capacity.
					</li><li class="listitem"><p class="simpara">
						Each system must meet the following hardware requirements:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Physical or virtual system, or an instance running on a public or private IaaS.
							</li><li class="listitem"><p class="simpara">
								Base OS: <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/index">RHEL 8.6, 8.7, or 8.8</a> with "Minimal" installation option.
							</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									Adding RHEL 7 compute machines to an OpenShift Container Platform cluster is not supported.
								</p><p>
									If you have RHEL 7 compute machines that were previously supported in a past OpenShift Container Platform version, you cannot upgrade them to RHEL 8. You must deploy new RHEL 8 hosts, and the old RHEL 7 hosts should be removed. See the "Deleting nodes" section for more information.
								</p><p>
									For the most recent list of major functionality that has been deprecated or removed within OpenShift Container Platform, refer to the <span class="emphasis"><em>Deprecated and removed features</em></span> section of the OpenShift Container Platform release notes.
								</p></div></div></li></ul></div></li></ul></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						NetworkManager 1.0 or later.
					</li><li class="listitem">
						1 vCPU.
					</li><li class="listitem">
						Minimum 8 GB RAM.
					</li><li class="listitem">
						Minimum 15 GB hard disk space for the file system containing <code class="literal">/var/</code>.
					</li><li class="listitem">
						Minimum 1 GB hard disk space for the file system containing <code class="literal">/usr/local/bin/</code>.
					</li><li class="listitem"><p class="simpara">
						Minimum 1 GB hard disk space for the file system containing its temporary directory. The temporary system directory is determined according to the rules defined in the tempfile module in the Python standard library.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Each system must meet any additional requirements for your system provider. For example, if you installed your cluster on VMware vSphere, your disks must be configured according to its <a class="link" href="https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/index.html">storage guidelines</a> and the <code class="literal">disk.enableUUID=true</code> attribute must be set.
							</li><li class="listitem">
								Each system must be able to access the cluster’s API endpoints by using DNS-resolvable hostnames. Any network security access control that is in place must allow system access to the cluster’s API service endpoints.
							</li></ul></div></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-working-deleting_nodes-nodes-working">Deleting nodes</a>
					</li></ul></div><section class="section" id="csr-management_adding-rhel-compute"><div class="titlepage"><div><div><h3 class="title">8.2.1. Certificate signing requests management</h3></div></div></div><p>
					Because your cluster has limited access to automatic machine management when you use infrastructure that you provision, you must provide a mechanism for approving cluster certificate signing requests (CSRs) after installation. The <code class="literal">kube-controller-manager</code> only approves the kubelet client CSRs. The <code class="literal">machine-approver</code> cannot guarantee the validity of a serving certificate that is requested by using kubelet credentials because it cannot confirm that the correct machine issued the request. You must determine and implement a method of verifying the validity of the kubelet serving certificate requests and approving them.
				</p></section></section><section class="section" id="adding-rhel-compute-preparing-image-cloud"><div class="titlepage"><div><div><h2 class="title">8.3. Preparing an image for your cloud</h2></div></div></div><p>
				Amazon Machine Images (AMI) are required because various image formats cannot be used directly by AWS. You may use the AMIs that Red Hat has provided, or you can manually import your own images. The AMI must exist before the EC2 instance can be provisioned. You will need a valid AMI ID so that the correct RHEL version needed for the compute machines is selected.
			</p><section class="section" id="rhel-images-aws_adding-rhel-compute"><div class="titlepage"><div><div><h3 class="title">8.3.1. Listing latest available RHEL images on AWS</h3></div></div></div><p>
					AMI IDs correspond to native boot images for AWS. Because an AMI must exist before the EC2 instance is provisioned, you will need to know the AMI ID before configuration. The <a class="link" href="https://aws.amazon.com/cli/">AWS Command Line Interface (CLI)</a> is used to list the available Red Hat Enterprise Linux (RHEL) image IDs.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the AWS CLI.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Use this command to list RHEL 8.4 Amazon Machine Images (AMI):
						</p><pre class="programlisting language-terminal">$ aws ec2 describe-images --owners 309956199498 \ <span id="CO85-1"><!--Empty--></span><span class="callout">1</span>
--query 'sort_by(Images, &amp;CreationDate)[*].[CreationDate,Name,ImageId]' \ <span id="CO85-2"><!--Empty--></span><span class="callout">2</span>
--filters "Name=name,Values=RHEL-8.4*" \ <span id="CO85-3"><!--Empty--></span><span class="callout">3</span>
--region us-east-1 \ <span id="CO85-4"><!--Empty--></span><span class="callout">4</span>
--output table <span id="CO85-5"><!--Empty--></span><span class="callout">5</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO85-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal">--owners</code> command option shows Red Hat images based on the account ID <code class="literal">309956199498</code>.
								</div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
										This account ID is required to display AMI IDs for images that are provided by Red Hat.
									</p></div></div></dd><dt><a href="#CO85-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The <code class="literal">--query</code> command option sets how the images are sorted with the parameters <code class="literal">'sort_by(Images, &amp;CreationDate)[*].[CreationDate,Name,ImageId]'</code>. In this case, the images are sorted by the creation date, and the table is structured to show the creation date, the name of the image, and the AMI IDs.
								</div></dd><dt><a href="#CO85-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The <code class="literal">--filter</code> command option sets which version of RHEL is shown. In this example, since the filter is set by <code class="literal">"Name=name,Values=RHEL-8.4*"</code>, then RHEL 8.4 AMIs are shown.
								</div></dd><dt><a href="#CO85-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The <code class="literal">--region</code> command option sets where the region where an AMI is stored.
								</div></dd><dt><a href="#CO85-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									The <code class="literal">--output</code> command option sets how the results are displayed.
								</div></dd></dl></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When creating a RHEL compute machine for AWS, ensure that the AMI is RHEL 8.4 or 8.5.
					</p></div></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">------------------------------------------------------------------------------------------------------------
|                                              DescribeImages                                              |
+---------------------------+-----------------------------------------------------+------------------------+
|  2021-03-18T14:23:11.000Z |  RHEL-8.4.0_HVM_BETA-20210309-x86_64-1-Hourly2-GP2  |  ami-07eeb4db5f7e5a8fb |
|  2021-03-18T14:38:28.000Z |  RHEL-8.4.0_HVM_BETA-20210309-arm64-1-Hourly2-GP2   |  ami-069d22ec49577d4bf |
|  2021-05-18T19:06:34.000Z |  RHEL-8.4.0_HVM-20210504-arm64-2-Hourly2-GP2        |  ami-01fc429821bf1f4b4 |
|  2021-05-18T20:09:47.000Z |  RHEL-8.4.0_HVM-20210504-x86_64-2-Hourly2-GP2       |  ami-0b0af3577fe5e3532 |
+---------------------------+-----------------------------------------------------+------------------------+</pre>

					</p></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							You may also manually <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/image_builder_guide/sect-documentation-image_builder-chapter5-section_2">import RHEL images to AWS</a>.
						</li></ul></div></section></section><section class="section" id="rhel-preparing-playbook-machine_adding-rhel-compute"><div class="titlepage"><div><div><h2 class="title">8.4. Preparing the machine to run the playbook</h2></div></div></div><p>
				Before you can add compute machines that use Red Hat Enterprise Linux (RHEL) as the operating system to an OpenShift Container Platform 4.13 cluster, you must prepare a RHEL 8 machine to run an Ansible playbook that adds the new node to the cluster. This machine is not part of the cluster but must be able to access it.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install the OpenShift CLI (<code class="literal">oc</code>) on the machine that you run the playbook on.
					</li><li class="listitem">
						Log in as a user with <code class="literal">cluster-admin</code> permission.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Ensure that the <code class="literal">kubeconfig</code> file for the cluster and the installation program that you used to install the cluster are on the RHEL 8 machine. One way to accomplish this is to use the same machine that you used to install the cluster.
					</li><li class="listitem">
						Configure the machine to access all of the RHEL hosts that you plan to use as compute machines. You can use any method that your company allows, including a bastion with an SSH proxy or a VPN.
					</li><li class="listitem"><p class="simpara">
						Configure a user on the machine that you run the playbook on that has SSH access to all of the RHEL hosts.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							If you use SSH key-based authentication, you must manage the key with an SSH agent.
						</p></div></div></li><li class="listitem"><p class="simpara">
						If you have not already done so, register the machine with RHSM and attach a pool with an <code class="literal">OpenShift</code> subscription to it:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Register the machine with RHSM:
							</p><pre class="programlisting language-terminal"># subscription-manager register --username=&lt;user_name&gt; --password=&lt;password&gt;</pre></li><li class="listitem"><p class="simpara">
								Pull the latest subscription data from RHSM:
							</p><pre class="programlisting language-terminal"># subscription-manager refresh</pre></li><li class="listitem"><p class="simpara">
								List the available subscriptions:
							</p><pre class="programlisting language-terminal"># subscription-manager list --available --matches '*OpenShift*'</pre></li><li class="listitem"><p class="simpara">
								In the output for the previous command, find the pool ID for an OpenShift Container Platform subscription and attach it:
							</p><pre class="programlisting language-terminal"># subscription-manager attach --pool=&lt;pool_id&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Enable the repositories required by OpenShift Container Platform 4.13:
					</p><pre class="programlisting language-terminal"># subscription-manager repos \
    --enable="rhel-8-for-x86_64-baseos-rpms" \
    --enable="rhel-8-for-x86_64-appstream-rpms" \
    --enable="rhocp-4.13-for-rhel-8-x86_64-rpms"</pre></li><li class="listitem"><p class="simpara">
						Install the required packages, including <code class="literal">openshift-ansible</code>:
					</p><pre class="programlisting language-terminal"># yum install openshift-ansible openshift-clients jq</pre><p class="simpara">
						The <code class="literal">openshift-ansible</code> package provides installation program utilities and pulls in other packages that you require to add a RHEL compute node to your cluster, such as Ansible, playbooks, and related configuration files. The <code class="literal">openshift-clients</code> provides the <code class="literal">oc</code> CLI, and the <code class="literal">jq</code> package improves the display of JSON output on your command line.
					</p></li></ol></div></section><section class="section" id="rhel-preparing-node_adding-rhel-compute"><div class="titlepage"><div><div><h2 class="title">8.5. Preparing a RHEL compute node</h2></div></div></div><p>
				Before you add a Red Hat Enterprise Linux (RHEL) machine to your OpenShift Container Platform cluster, you must register each host with Red Hat Subscription Manager (RHSM), attach an active OpenShift Container Platform subscription, and enable the required repositories.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On each host, register with RHSM:
					</p><pre class="programlisting language-terminal"># subscription-manager register --username=&lt;user_name&gt; --password=&lt;password&gt;</pre></li><li class="listitem"><p class="simpara">
						Pull the latest subscription data from RHSM:
					</p><pre class="programlisting language-terminal"># subscription-manager refresh</pre></li><li class="listitem"><p class="simpara">
						List the available subscriptions:
					</p><pre class="programlisting language-terminal"># subscription-manager list --available --matches '*OpenShift*'</pre></li><li class="listitem"><p class="simpara">
						In the output for the previous command, find the pool ID for an OpenShift Container Platform subscription and attach it:
					</p><pre class="programlisting language-terminal"># subscription-manager attach --pool=&lt;pool_id&gt;</pre></li><li class="listitem"><p class="simpara">
						Disable all yum repositories:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Disable all the enabled RHSM repositories:
							</p><pre class="programlisting language-terminal"># subscription-manager repos --disable="*"</pre></li><li class="listitem"><p class="simpara">
								List the remaining yum repositories and note their names under <code class="literal">repo id</code>, if any:
							</p><pre class="programlisting language-terminal"># yum repolist</pre></li><li class="listitem"><p class="simpara">
								Use <code class="literal">yum-config-manager</code> to disable the remaining yum repositories:
							</p><pre class="programlisting language-terminal"># yum-config-manager --disable &lt;repo_id&gt;</pre><p class="simpara">
								Alternatively, disable all repositories:
							</p><pre class="programlisting language-terminal"># yum-config-manager --disable \*</pre><p class="simpara">
								Note that this might take a few minutes if you have a large number of available repositories
							</p></li></ol></div></li><li class="listitem"><p class="simpara">
						Enable only the repositories required by OpenShift Container Platform 4.13:
					</p><pre class="programlisting language-terminal"># subscription-manager repos \
    --enable="rhel-8-for-x86_64-baseos-rpms" \
    --enable="rhel-8-for-x86_64-appstream-rpms" \
    --enable="rhocp-4.13-for-rhel-8-x86_64-rpms" \
    --enable="fast-datapath-for-rhel-8-x86_64-rpms"</pre></li><li class="listitem"><p class="simpara">
						Stop and disable firewalld on the host:
					</p><pre class="programlisting language-terminal"># systemctl disable --now firewalld.service</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You must not enable firewalld later. If you do, you cannot access OpenShift Container Platform logs on the worker.
						</p></div></div></li></ol></div></section><section class="section" id="rhel-attaching-instance-aws_adding-rhel-compute"><div class="titlepage"><div><div><h2 class="title">8.6. Attaching the role permissions to RHEL instance in AWS</h2></div></div></div><p>
				Using the Amazon IAM console in your browser, you may select the needed roles and assign them to a worker node.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						From the AWS IAM console, create your <a class="link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html#create-iam-role">desired IAM role</a>.
					</li><li class="listitem">
						<a class="link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html#attach-iam-role">Attach the IAM role</a> to the desired worker node.
					</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-aws-permissions-iam-roles_installing-aws-account">Required AWS permissions for IAM roles</a>.
					</li></ul></div></section><section class="section" id="rhel-worker-tag_adding-rhel-compute"><div class="titlepage"><div><div><h2 class="title">8.7. Tagging a RHEL worker node as owned or shared</h2></div></div></div><p>
				A cluster uses the value of the <code class="literal">kubernetes.io/cluster/&lt;clusterid&gt;,Value=(owned|shared)</code> tag to determine the lifetime of the resources related to the AWS cluster.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The <code class="literal">owned</code> tag value should be added if the resource should be destroyed as part of destroying the cluster.
					</li><li class="listitem">
						The <code class="literal">shared</code> tag value should be added if the resource continues to exist after the cluster has been destroyed. This tagging denotes that the cluster uses this resource, but there is a separate owner for the resource.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						With RHEL compute machines, the RHEL worker instance must be tagged with <code class="literal">kubernetes.io/cluster/&lt;clusterid&gt;=owned</code> or <code class="literal">kubernetes.io/cluster/&lt;cluster-id&gt;=shared</code>.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Do not tag all existing security groups with the <code class="literal">kubernetes.io/cluster/&lt;name&gt;,Value=&lt;clusterid&gt;</code> tag, or the Elastic Load Balancing (ELB) will not be able to create a load balancer.
				</p></div></div></section><section class="section" id="rhel-adding-node_adding-rhel-compute"><div class="titlepage"><div><div><h2 class="title">8.8. Adding a RHEL compute machine to your cluster</h2></div></div></div><p>
				You can add compute machines that use Red Hat Enterprise Linux as the operating system to an OpenShift Container Platform 4.13 cluster.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You installed the required packages and performed the necessary configuration on the machine that you run the playbook on.
					</li><li class="listitem">
						You prepared the RHEL hosts for installation.
					</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					Perform the following steps on the machine that you prepared to run the playbook:
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create an Ansible inventory file that is named <code class="literal">/&lt;path&gt;/inventory/hosts</code> that defines your compute machine hosts and required variables:
					</p><pre class="screen">[all:vars]
ansible_user=root <span id="CO86-1"><!--Empty--></span><span class="callout">1</span>
#ansible_become=True <span id="CO86-2"><!--Empty--></span><span class="callout">2</span>

openshift_kubeconfig_path="~/.kube/config" <span id="CO86-3"><!--Empty--></span><span class="callout">3</span>

[new_workers] <span id="CO86-4"><!--Empty--></span><span class="callout">4</span>
mycluster-rhel8-0.example.com
mycluster-rhel8-1.example.com</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO86-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specify the user name that runs the Ansible tasks on the remote compute machines.
							</div></dd><dt><a href="#CO86-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								If you do not specify <code class="literal">root</code> for the <code class="literal">ansible_user</code>, you must set <code class="literal">ansible_become</code> to <code class="literal">True</code> and assign the user sudo permissions.
							</div></dd><dt><a href="#CO86-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specify the path and file name of the <code class="literal">kubeconfig</code> file for your cluster.
							</div></dd><dt><a href="#CO86-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								List each RHEL machine to add to your cluster. You must provide the fully-qualified domain name for each host. This name is the hostname that the cluster uses to access the machine, so set the correct public or private name to access the machine.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Navigate to the Ansible playbook directory:
					</p><pre class="programlisting language-terminal">$ cd /usr/share/ansible/openshift-ansible</pre></li><li class="listitem"><p class="simpara">
						Run the playbook:
					</p><pre class="programlisting language-terminal">$ ansible-playbook -i /&lt;path&gt;/inventory/hosts playbooks/scaleup.yml <span id="CO87-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO87-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								For <code class="literal">&lt;path&gt;</code>, specify the path to the Ansible inventory file that you created.
							</div></dd></dl></div></li></ol></div></section><section class="section" id="installation-approve-csrs_adding-rhel-compute"><div class="titlepage"><div><div><h2 class="title">8.9. Approving the certificate signing requests for your machines</h2></div></div></div><p>
				When you add machines to a cluster, two pending certificate signing requests (CSRs) are generated for each machine that you added. You must confirm that these CSRs are approved or, if necessary, approve them yourself. The client requests must be approved first, followed by the server requests.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You added machines to your cluster.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Confirm that the cluster recognizes the machines:
					</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  63m  v1.26.0
master-1  Ready     master  63m  v1.26.0
master-2  Ready     master  64m  v1.26.0</pre>

						</p></div><p class="simpara">
						The output lists all of the machines that you created.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The preceding output might not include the compute nodes, also known as worker nodes, until some CSRs are approved.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Review the pending CSRs and ensure that you see the client requests with the <code class="literal">Pending</code> or <code class="literal">Approved</code> status for each machine that you added to the cluster:
					</p><pre class="programlisting language-terminal">$ oc get csr</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-8b2br   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-8vnps   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
...</pre>

						</p></div><p class="simpara">
						In this example, two machines are joining the cluster. You might see more approved CSRs in the list.
					</p></li><li class="listitem"><p class="simpara">
						If the CSRs were not approved, after all of the pending CSRs for the machines you added are in <code class="literal">Pending</code> status, approve the CSRs for your cluster machines:
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Because the CSRs rotate automatically, approve your CSRs within an hour of adding the machines to the cluster. If you do not approve them within an hour, the certificates will rotate, and more than two certificates will be present for each node. You must approve all of these certificates. After the client CSR is approved, the Kubelet creates a secondary CSR for the serving certificate, which requires manual approval. Then, subsequent serving certificate renewal requests are automatically approved by the <code class="literal">machine-approver</code> if the Kubelet requests a new certificate with identical parameters.
						</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							For clusters running on platforms that are not machine API enabled, such as bare metal and other user-provisioned infrastructure, you must implement a method of automatically approving the kubelet serving certificate requests (CSRs). If a request is not approved, then the <code class="literal">oc exec</code>, <code class="literal">oc rsh</code>, and <code class="literal">oc logs</code> commands cannot succeed, because a serving certificate is required when the API server connects to the kubelet. Any operation that contacts the Kubelet endpoint requires this certificate approval to be in place. The method must watch for new CSRs, confirm that the CSR was submitted by the <code class="literal">node-bootstrapper</code> service account in the <code class="literal">system:node</code> or <code class="literal">system:admin</code> groups, and confirm the identity of the node.
						</p></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								To approve them individually, run the following command for each valid CSR:
							</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt; <span id="CO88-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO88-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										<code class="literal">&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								To approve all pending CSRs, run the following command:
							</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs --no-run-if-empty oc adm certificate approve</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Some Operators might not become available until some CSRs are approved.
								</p></div></div></li></ul></div></li><li class="listitem"><p class="simpara">
						Now that your client requests are approved, you must review the server requests for each machine that you added to the cluster:
					</p><pre class="programlisting language-terminal">$ oc get csr</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-bfd72   5m26s   system:node:ip-10-0-50-126.us-east-2.compute.internal                       Pending
csr-c57lv   5m26s   system:node:ip-10-0-95-157.us-east-2.compute.internal                       Pending
...</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						If the remaining CSRs are not approved, and are in the <code class="literal">Pending</code> status, approve the CSRs for your cluster machines:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								To approve them individually, run the following command for each valid CSR:
							</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt; <span id="CO89-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO89-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										<code class="literal">&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								To approve all pending CSRs, run the following command:
							</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs oc adm certificate approve</pre></li></ul></div></li><li class="listitem"><p class="simpara">
						After all client and server CSRs have been approved, the machines have the <code class="literal">Ready</code> status. Verify this by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  73m  v1.26.0
master-1  Ready     master  73m  v1.26.0
master-2  Ready     master  74m  v1.26.0
worker-0  Ready     worker  11m  v1.26.0
worker-1  Ready     worker  11m  v1.26.0</pre>

						</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							It can take a few minutes after approval of the server CSRs for the machines to transition to the <code class="literal">Ready</code> status.
						</p></div></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Additional information</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						For more information on CSRs, see <a class="link" href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/">Certificate Signing Requests</a>.
					</li></ul></div></section><section class="section" id="rhel-ansible-parameters_adding-rhel-compute"><div class="titlepage"><div><div><h2 class="title">8.10. Required parameters for the Ansible hosts file</h2></div></div></div><p>
				You must define the following parameters in the Ansible hosts file before you add Red Hat Enterprise Linux (RHEL) compute machines to your cluster.
			</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 40%; " class="col_2"><!--Empty--></col><col style="width: 40%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140311137839152" scope="col">Parameter</th><th align="left" valign="top" id="idm140311137838064" scope="col">Description</th><th align="left" valign="top" id="idm140311137836976" scope="col">Values</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140311137839152"> <p>
								<code class="literal">ansible_user</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140311137838064"> <p>
								The SSH user that allows SSH-based authentication without requiring a password. If you use SSH key-based authentication, then you must manage the key with an SSH agent.
							</p>
							 </td><td align="left" valign="top" headers="idm140311137836976"> <p>
								A user name on the system. The default value is <code class="literal">root</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140311137839152"> <p>
								<code class="literal">ansible_become</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140311137838064"> <p>
								If the values of <code class="literal">ansible_user</code> is not root, you must set <code class="literal">ansible_become</code> to <code class="literal">True</code>, and the user that you specify as the <code class="literal">ansible_user</code> must be configured for passwordless sudo access.
							</p>
							 </td><td align="left" valign="top" headers="idm140311137836976"> <p>
								<code class="literal">True</code>. If the value is not <code class="literal">True</code>, do not specify and define this parameter.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140311137839152"> <p>
								<code class="literal">openshift_kubeconfig_path</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140311137838064"> <p>
								Specifies a path and file name to a local directory that contains the <code class="literal">kubeconfig</code> file for your cluster.
							</p>
							 </td><td align="left" valign="top" headers="idm140311137836976"> <p>
								The path and name of the configuration file.
							</p>
							 </td></tr></tbody></table></div><section class="section" id="rhel-removing-rhcos_adding-rhel-compute"><div class="titlepage"><div><div><h3 class="title">8.10.1. Optional: Removing RHCOS compute machines from a cluster</h3></div></div></div><p>
					After you add the Red Hat Enterprise Linux (RHEL) compute machines to your cluster, you can optionally remove the Red Hat Enterprise Linux CoreOS (RHCOS) compute machines to free up resources.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have added RHEL compute machines to your cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the list of machines and record the node names of the RHCOS compute machines:
						</p><pre class="programlisting language-terminal">$ oc get nodes -o wide</pre></li><li class="listitem"><p class="simpara">
							For each RHCOS compute machine, delete the node:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Mark the node as unschedulable by running the <code class="literal">oc adm cordon</code> command:
								</p><pre class="programlisting language-terminal">$ oc adm cordon &lt;node_name&gt; <span id="CO90-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO90-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the node name of one of the RHCOS compute machines.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Drain all the pods from the node:
								</p><pre class="programlisting language-terminal">$ oc adm drain &lt;node_name&gt; --force --delete-emptydir-data --ignore-daemonsets <span id="CO91-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO91-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the node name of the RHCOS compute machine that you isolated.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Delete the node:
								</p><pre class="programlisting language-terminal">$ oc delete nodes &lt;node_name&gt; <span id="CO92-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO92-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the node name of the RHCOS compute machine that you drained.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Review the list of compute machines to ensure that only the RHEL nodes remain:
						</p><pre class="programlisting language-terminal">$ oc get nodes -o wide</pre></li><li class="listitem">
							Remove the RHCOS machines from the load balancer for your cluster’s compute machines. You can delete the virtual machines or reimage the physical hardware for the RHCOS compute machines.
						</li></ol></div></section></section></section><section class="chapter" id="more-rhel-compute"><div class="titlepage"><div><div><h1 class="title">Chapter 9. Adding more RHEL compute machines to an OpenShift Container Platform cluster</h1></div></div></div><p>
			If your OpenShift Container Platform cluster already includes Red Hat Enterprise Linux (RHEL) compute machines, which are also known as worker machines, you can add more RHEL compute machines to it.
		</p><section class="section" id="rhel-compute-overview_more-rhel-compute"><div class="titlepage"><div><div><h2 class="title">9.1. About adding RHEL compute nodes to a cluster</h2></div></div></div><p>
				In OpenShift Container Platform 4.13, you have the option of using Red Hat Enterprise Linux (RHEL) machines as compute machines in your cluster if you use a user-provisioned or installer-provisioned infrastructure installation on the <code class="literal">x86_64</code> architecture. You must use Red Hat Enterprise Linux CoreOS (RHCOS) machines for the control plane machines in your cluster.
			</p><p>
				If you choose to use RHEL compute machines in your cluster, you are responsible for all operating system life cycle management and maintenance. You must perform system updates, apply patches, and complete all other required tasks.
			</p><p>
				For installer-provisioned infrastructure clusters, you must manually add RHEL compute machines because automatic scaling in installer-provisioned infrastructure clusters adds Red Hat Enterprise Linux CoreOS (RHCOS) compute machines by default.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Because removing OpenShift Container Platform from a machine in the cluster requires destroying the operating system, you must use dedicated hardware for any RHEL machines that you add to the cluster.
						</li><li class="listitem">
							Swap memory is disabled on all RHEL machines that you add to your OpenShift Container Platform cluster. You cannot enable swap memory on these machines.
						</li></ul></div></div></div><p>
				You must add any RHEL compute machines to the cluster after you initialize the control plane.
			</p></section><section class="section" id="rhel-compute-requirements_more-rhel-compute"><div class="titlepage"><div><div><h2 class="title">9.2. System requirements for RHEL compute nodes</h2></div></div></div><p>
				The Red Hat Enterprise Linux (RHEL) compute machine hosts in your OpenShift Container Platform environment must meet the following minimum hardware specifications and system-level requirements:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						You must have an active OpenShift Container Platform subscription on your Red Hat account. If you do not, contact your sales representative for more information.
					</li><li class="listitem">
						Production environments must provide compute machines to support your expected workloads. As a cluster administrator, you must calculate the expected workload and add about 10% for overhead. For production environments, allocate enough resources so that a node host failure does not affect your maximum capacity.
					</li><li class="listitem"><p class="simpara">
						Each system must meet the following hardware requirements:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Physical or virtual system, or an instance running on a public or private IaaS.
							</li><li class="listitem"><p class="simpara">
								Base OS: <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/index">RHEL 8.6, 8.7, or 8.8</a> with "Minimal" installation option.
							</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									Adding RHEL 7 compute machines to an OpenShift Container Platform cluster is not supported.
								</p><p>
									If you have RHEL 7 compute machines that were previously supported in a past OpenShift Container Platform version, you cannot upgrade them to RHEL 8. You must deploy new RHEL 8 hosts, and the old RHEL 7 hosts should be removed. See the "Deleting nodes" section for more information.
								</p><p>
									For the most recent list of major functionality that has been deprecated or removed within OpenShift Container Platform, refer to the <span class="emphasis"><em>Deprecated and removed features</em></span> section of the OpenShift Container Platform release notes.
								</p></div></div></li></ul></div></li></ul></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						NetworkManager 1.0 or later.
					</li><li class="listitem">
						1 vCPU.
					</li><li class="listitem">
						Minimum 8 GB RAM.
					</li><li class="listitem">
						Minimum 15 GB hard disk space for the file system containing <code class="literal">/var/</code>.
					</li><li class="listitem">
						Minimum 1 GB hard disk space for the file system containing <code class="literal">/usr/local/bin/</code>.
					</li><li class="listitem"><p class="simpara">
						Minimum 1 GB hard disk space for the file system containing its temporary directory. The temporary system directory is determined according to the rules defined in the tempfile module in the Python standard library.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Each system must meet any additional requirements for your system provider. For example, if you installed your cluster on VMware vSphere, your disks must be configured according to its <a class="link" href="https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/index.html">storage guidelines</a> and the <code class="literal">disk.enableUUID=true</code> attribute must be set.
							</li><li class="listitem">
								Each system must be able to access the cluster’s API endpoints by using DNS-resolvable hostnames. Any network security access control that is in place must allow system access to the cluster’s API service endpoints.
							</li></ul></div></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-working-deleting_nodes-nodes-working">Deleting nodes</a>
					</li></ul></div><section class="section" id="csr-management_more-rhel-compute"><div class="titlepage"><div><div><h3 class="title">9.2.1. Certificate signing requests management</h3></div></div></div><p>
					Because your cluster has limited access to automatic machine management when you use infrastructure that you provision, you must provide a mechanism for approving cluster certificate signing requests (CSRs) after installation. The <code class="literal">kube-controller-manager</code> only approves the kubelet client CSRs. The <code class="literal">machine-approver</code> cannot guarantee the validity of a serving certificate that is requested by using kubelet credentials because it cannot confirm that the correct machine issued the request. You must determine and implement a method of verifying the validity of the kubelet serving certificate requests and approving them.
				</p></section></section><section class="section" id="more-rhel-compute-preparing-image-cloud"><div class="titlepage"><div><div><h2 class="title">9.3. Preparing an image for your cloud</h2></div></div></div><p>
				Amazon Machine Images (AMI) are required since various image formats cannot be used directly by AWS. You may use the AMIs that Red Hat has provided, or you can manually import your own images. The AMI must exist before the EC2 instance can be provisioned. You must list the AMI IDs so that the correct RHEL version needed for the compute machines is selected.
			</p><section class="section" id="rhel-images-aws_more-rhel-compute"><div class="titlepage"><div><div><h3 class="title">9.3.1. Listing latest available RHEL images on AWS</h3></div></div></div><p>
					AMI IDs correspond to native boot images for AWS. Because an AMI must exist before the EC2 instance is provisioned, you will need to know the AMI ID before configuration. The <a class="link" href="https://aws.amazon.com/cli/">AWS Command Line Interface (CLI)</a> is used to list the available Red Hat Enterprise Linux (RHEL) image IDs.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the AWS CLI.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Use this command to list RHEL 8.4 Amazon Machine Images (AMI):
						</p><pre class="programlisting language-terminal">$ aws ec2 describe-images --owners 309956199498 \ <span id="CO93-1"><!--Empty--></span><span class="callout">1</span>
--query 'sort_by(Images, &amp;CreationDate)[*].[CreationDate,Name,ImageId]' \ <span id="CO93-2"><!--Empty--></span><span class="callout">2</span>
--filters "Name=name,Values=RHEL-8.4*" \ <span id="CO93-3"><!--Empty--></span><span class="callout">3</span>
--region us-east-1 \ <span id="CO93-4"><!--Empty--></span><span class="callout">4</span>
--output table <span id="CO93-5"><!--Empty--></span><span class="callout">5</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO93-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal">--owners</code> command option shows Red Hat images based on the account ID <code class="literal">309956199498</code>.
								</div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
										This account ID is required to display AMI IDs for images that are provided by Red Hat.
									</p></div></div></dd><dt><a href="#CO93-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The <code class="literal">--query</code> command option sets how the images are sorted with the parameters <code class="literal">'sort_by(Images, &amp;CreationDate)[*].[CreationDate,Name,ImageId]'</code>. In this case, the images are sorted by the creation date, and the table is structured to show the creation date, the name of the image, and the AMI IDs.
								</div></dd><dt><a href="#CO93-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The <code class="literal">--filter</code> command option sets which version of RHEL is shown. In this example, since the filter is set by <code class="literal">"Name=name,Values=RHEL-8.4*"</code>, then RHEL 8.4 AMIs are shown.
								</div></dd><dt><a href="#CO93-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The <code class="literal">--region</code> command option sets where the region where an AMI is stored.
								</div></dd><dt><a href="#CO93-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									The <code class="literal">--output</code> command option sets how the results are displayed.
								</div></dd></dl></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When creating a RHEL compute machine for AWS, ensure that the AMI is RHEL 8.4 or 8.5.
					</p></div></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">------------------------------------------------------------------------------------------------------------
|                                              DescribeImages                                              |
+---------------------------+-----------------------------------------------------+------------------------+
|  2021-03-18T14:23:11.000Z |  RHEL-8.4.0_HVM_BETA-20210309-x86_64-1-Hourly2-GP2  |  ami-07eeb4db5f7e5a8fb |
|  2021-03-18T14:38:28.000Z |  RHEL-8.4.0_HVM_BETA-20210309-arm64-1-Hourly2-GP2   |  ami-069d22ec49577d4bf |
|  2021-05-18T19:06:34.000Z |  RHEL-8.4.0_HVM-20210504-arm64-2-Hourly2-GP2        |  ami-01fc429821bf1f4b4 |
|  2021-05-18T20:09:47.000Z |  RHEL-8.4.0_HVM-20210504-x86_64-2-Hourly2-GP2       |  ami-0b0af3577fe5e3532 |
+---------------------------+-----------------------------------------------------+------------------------+</pre>

					</p></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							You may also manually <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/image_builder_guide/sect-documentation-image_builder-chapter5-section_2">import RHEL images to AWS</a>.
						</li></ul></div></section></section><section class="section" id="rhel-preparing-node_more-rhel-compute"><div class="titlepage"><div><div><h2 class="title">9.4. Preparing a RHEL compute node</h2></div></div></div><p>
				Before you add a Red Hat Enterprise Linux (RHEL) machine to your OpenShift Container Platform cluster, you must register each host with Red Hat Subscription Manager (RHSM), attach an active OpenShift Container Platform subscription, and enable the required repositories.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On each host, register with RHSM:
					</p><pre class="programlisting language-terminal"># subscription-manager register --username=&lt;user_name&gt; --password=&lt;password&gt;</pre></li><li class="listitem"><p class="simpara">
						Pull the latest subscription data from RHSM:
					</p><pre class="programlisting language-terminal"># subscription-manager refresh</pre></li><li class="listitem"><p class="simpara">
						List the available subscriptions:
					</p><pre class="programlisting language-terminal"># subscription-manager list --available --matches '*OpenShift*'</pre></li><li class="listitem"><p class="simpara">
						In the output for the previous command, find the pool ID for an OpenShift Container Platform subscription and attach it:
					</p><pre class="programlisting language-terminal"># subscription-manager attach --pool=&lt;pool_id&gt;</pre></li><li class="listitem"><p class="simpara">
						Disable all yum repositories:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Disable all the enabled RHSM repositories:
							</p><pre class="programlisting language-terminal"># subscription-manager repos --disable="*"</pre></li><li class="listitem"><p class="simpara">
								List the remaining yum repositories and note their names under <code class="literal">repo id</code>, if any:
							</p><pre class="programlisting language-terminal"># yum repolist</pre></li><li class="listitem"><p class="simpara">
								Use <code class="literal">yum-config-manager</code> to disable the remaining yum repositories:
							</p><pre class="programlisting language-terminal"># yum-config-manager --disable &lt;repo_id&gt;</pre><p class="simpara">
								Alternatively, disable all repositories:
							</p><pre class="programlisting language-terminal"># yum-config-manager --disable \*</pre><p class="simpara">
								Note that this might take a few minutes if you have a large number of available repositories
							</p></li></ol></div></li><li class="listitem"><p class="simpara">
						Enable only the repositories required by OpenShift Container Platform 4.13:
					</p><pre class="programlisting language-terminal"># subscription-manager repos \
    --enable="rhel-8-for-x86_64-baseos-rpms" \
    --enable="rhel-8-for-x86_64-appstream-rpms" \
    --enable="rhocp-4.13-for-rhel-8-x86_64-rpms" \
    --enable="fast-datapath-for-rhel-8-x86_64-rpms"</pre></li><li class="listitem"><p class="simpara">
						Stop and disable firewalld on the host:
					</p><pre class="programlisting language-terminal"># systemctl disable --now firewalld.service</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You must not enable firewalld later. If you do, you cannot access OpenShift Container Platform logs on the worker.
						</p></div></div></li></ol></div></section><section class="section" id="rhel-attaching-instance-aws_more-rhel-compute"><div class="titlepage"><div><div><h2 class="title">9.5. Attaching the role permissions to RHEL instance in AWS</h2></div></div></div><p>
				Using the Amazon IAM console in your browser, you may select the needed roles and assign them to a worker node.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						From the AWS IAM console, create your <a class="link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html#create-iam-role">desired IAM role</a>.
					</li><li class="listitem">
						<a class="link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html#attach-iam-role">Attach the IAM role</a> to the desired worker node.
					</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-aws-permissions-iam-roles_installing-aws-account">Required AWS permissions for IAM roles</a>.
					</li></ul></div></section><section class="section" id="rhel-worker-tag_more-rhel-compute"><div class="titlepage"><div><div><h2 class="title">9.6. Tagging a RHEL worker node as owned or shared</h2></div></div></div><p>
				A cluster uses the value of the <code class="literal">kubernetes.io/cluster/&lt;clusterid&gt;,Value=(owned|shared)</code> tag to determine the lifetime of the resources related to the AWS cluster.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The <code class="literal">owned</code> tag value should be added if the resource should be destroyed as part of destroying the cluster.
					</li><li class="listitem">
						The <code class="literal">shared</code> tag value should be added if the resource continues to exist after the cluster has been destroyed. This tagging denotes that the cluster uses this resource, but there is a separate owner for the resource.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						With RHEL compute machines, the RHEL worker instance must be tagged with <code class="literal">kubernetes.io/cluster/&lt;clusterid&gt;=owned</code> or <code class="literal">kubernetes.io/cluster/&lt;cluster-id&gt;=shared</code>.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Do not tag all existing security groups with the <code class="literal">kubernetes.io/cluster/&lt;name&gt;,Value=&lt;clusterid&gt;</code> tag, or the Elastic Load Balancing (ELB) will not be able to create a load balancer.
				</p></div></div></section><section class="section" id="rhel-adding-more-nodes_more-rhel-compute"><div class="titlepage"><div><div><h2 class="title">9.7. Adding more RHEL compute machines to your cluster</h2></div></div></div><p>
				You can add more compute machines that use Red Hat Enterprise Linux (RHEL) as the operating system to an OpenShift Container Platform 4.13 cluster.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Your OpenShift Container Platform cluster already contains RHEL compute nodes.
					</li><li class="listitem">
						The <code class="literal">hosts</code> file that you used to add the first RHEL compute machines to your cluster is on the machine that you use the run the playbook.
					</li><li class="listitem">
						The machine that you run the playbook on must be able to access all of the RHEL hosts. You can use any method that your company allows, including a bastion with an SSH proxy or a VPN.
					</li><li class="listitem">
						The <code class="literal">kubeconfig</code> file for the cluster and the installation program that you used to install the cluster are on the machine that you use the run the playbook.
					</li><li class="listitem">
						You must prepare the RHEL hosts for installation.
					</li><li class="listitem">
						Configure a user on the machine that you run the playbook on that has SSH access to all of the RHEL hosts.
					</li><li class="listitem">
						If you use SSH key-based authentication, you must manage the key with an SSH agent.
					</li><li class="listitem">
						Install the OpenShift CLI (<code class="literal">oc</code>) on the machine that you run the playbook on.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Open the Ansible inventory file at <code class="literal">/&lt;path&gt;/inventory/hosts</code> that defines your compute machine hosts and required variables.
					</li><li class="listitem">
						Rename the <code class="literal">[new_workers]</code> section of the file to <code class="literal">[workers]</code>.
					</li><li class="listitem"><p class="simpara">
						Add a <code class="literal">[new_workers]</code> section to the file and define the fully-qualified domain names for each new host. The file resembles the following example:
					</p><pre class="screen">[all:vars]
ansible_user=root
#ansible_become=True

openshift_kubeconfig_path="~/.kube/config"

[workers]
mycluster-rhel8-0.example.com
mycluster-rhel8-1.example.com

[new_workers]
mycluster-rhel8-2.example.com
mycluster-rhel8-3.example.com</pre><p class="simpara">
						In this example, the <code class="literal">mycluster-rhel8-0.example.com</code> and <code class="literal">mycluster-rhel8-1.example.com</code> machines are in the cluster and you add the <code class="literal">mycluster-rhel8-2.example.com</code> and <code class="literal">mycluster-rhel8-3.example.com</code> machines.
					</p></li><li class="listitem"><p class="simpara">
						Navigate to the Ansible playbook directory:
					</p><pre class="programlisting language-terminal">$ cd /usr/share/ansible/openshift-ansible</pre></li><li class="listitem"><p class="simpara">
						Run the scaleup playbook:
					</p><pre class="programlisting language-terminal">$ ansible-playbook -i /&lt;path&gt;/inventory/hosts playbooks/scaleup.yml <span id="CO94-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO94-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								For <code class="literal">&lt;path&gt;</code>, specify the path to the Ansible inventory file that you created.
							</div></dd></dl></div></li></ol></div></section><section class="section" id="installation-approve-csrs_more-rhel-compute"><div class="titlepage"><div><div><h2 class="title">9.8. Approving the certificate signing requests for your machines</h2></div></div></div><p>
				When you add machines to a cluster, two pending certificate signing requests (CSRs) are generated for each machine that you added. You must confirm that these CSRs are approved or, if necessary, approve them yourself. The client requests must be approved first, followed by the server requests.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You added machines to your cluster.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Confirm that the cluster recognizes the machines:
					</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  63m  v1.26.0
master-1  Ready     master  63m  v1.26.0
master-2  Ready     master  64m  v1.26.0</pre>

						</p></div><p class="simpara">
						The output lists all of the machines that you created.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The preceding output might not include the compute nodes, also known as worker nodes, until some CSRs are approved.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Review the pending CSRs and ensure that you see the client requests with the <code class="literal">Pending</code> or <code class="literal">Approved</code> status for each machine that you added to the cluster:
					</p><pre class="programlisting language-terminal">$ oc get csr</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-8b2br   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-8vnps   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
...</pre>

						</p></div><p class="simpara">
						In this example, two machines are joining the cluster. You might see more approved CSRs in the list.
					</p></li><li class="listitem"><p class="simpara">
						If the CSRs were not approved, after all of the pending CSRs for the machines you added are in <code class="literal">Pending</code> status, approve the CSRs for your cluster machines:
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Because the CSRs rotate automatically, approve your CSRs within an hour of adding the machines to the cluster. If you do not approve them within an hour, the certificates will rotate, and more than two certificates will be present for each node. You must approve all of these certificates. After the client CSR is approved, the Kubelet creates a secondary CSR for the serving certificate, which requires manual approval. Then, subsequent serving certificate renewal requests are automatically approved by the <code class="literal">machine-approver</code> if the Kubelet requests a new certificate with identical parameters.
						</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							For clusters running on platforms that are not machine API enabled, such as bare metal and other user-provisioned infrastructure, you must implement a method of automatically approving the kubelet serving certificate requests (CSRs). If a request is not approved, then the <code class="literal">oc exec</code>, <code class="literal">oc rsh</code>, and <code class="literal">oc logs</code> commands cannot succeed, because a serving certificate is required when the API server connects to the kubelet. Any operation that contacts the Kubelet endpoint requires this certificate approval to be in place. The method must watch for new CSRs, confirm that the CSR was submitted by the <code class="literal">node-bootstrapper</code> service account in the <code class="literal">system:node</code> or <code class="literal">system:admin</code> groups, and confirm the identity of the node.
						</p></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								To approve them individually, run the following command for each valid CSR:
							</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt; <span id="CO95-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO95-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										<code class="literal">&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								To approve all pending CSRs, run the following command:
							</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs --no-run-if-empty oc adm certificate approve</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Some Operators might not become available until some CSRs are approved.
								</p></div></div></li></ul></div></li><li class="listitem"><p class="simpara">
						Now that your client requests are approved, you must review the server requests for each machine that you added to the cluster:
					</p><pre class="programlisting language-terminal">$ oc get csr</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-bfd72   5m26s   system:node:ip-10-0-50-126.us-east-2.compute.internal                       Pending
csr-c57lv   5m26s   system:node:ip-10-0-95-157.us-east-2.compute.internal                       Pending
...</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						If the remaining CSRs are not approved, and are in the <code class="literal">Pending</code> status, approve the CSRs for your cluster machines:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								To approve them individually, run the following command for each valid CSR:
							</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt; <span id="CO96-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO96-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										<code class="literal">&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								To approve all pending CSRs, run the following command:
							</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs oc adm certificate approve</pre></li></ul></div></li><li class="listitem"><p class="simpara">
						After all client and server CSRs have been approved, the machines have the <code class="literal">Ready</code> status. Verify this by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  73m  v1.26.0
master-1  Ready     master  73m  v1.26.0
master-2  Ready     master  74m  v1.26.0
worker-0  Ready     worker  11m  v1.26.0
worker-1  Ready     worker  11m  v1.26.0</pre>

						</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							It can take a few minutes after approval of the server CSRs for the machines to transition to the <code class="literal">Ready</code> status.
						</p></div></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Additional information</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						For more information on CSRs, see <a class="link" href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/">Certificate Signing Requests</a>.
					</li></ul></div></section><section class="section" id="rhel-ansible-parameters_more-rhel-compute"><div class="titlepage"><div><div><h2 class="title">9.9. Required parameters for the Ansible hosts file</h2></div></div></div><p>
				You must define the following parameters in the Ansible hosts file before you add Red Hat Enterprise Linux (RHEL) compute machines to your cluster.
			</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 40%; " class="col_2"><!--Empty--></col><col style="width: 40%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140311138481152" scope="col">Parameter</th><th align="left" valign="top" id="idm140311138480064" scope="col">Description</th><th align="left" valign="top" id="idm140311138478976" scope="col">Values</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140311138481152"> <p>
								<code class="literal">ansible_user</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140311138480064"> <p>
								The SSH user that allows SSH-based authentication without requiring a password. If you use SSH key-based authentication, then you must manage the key with an SSH agent.
							</p>
							 </td><td align="left" valign="top" headers="idm140311138478976"> <p>
								A user name on the system. The default value is <code class="literal">root</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140311138481152"> <p>
								<code class="literal">ansible_become</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140311138480064"> <p>
								If the values of <code class="literal">ansible_user</code> is not root, you must set <code class="literal">ansible_become</code> to <code class="literal">True</code>, and the user that you specify as the <code class="literal">ansible_user</code> must be configured for passwordless sudo access.
							</p>
							 </td><td align="left" valign="top" headers="idm140311138478976"> <p>
								<code class="literal">True</code>. If the value is not <code class="literal">True</code>, do not specify and define this parameter.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140311138481152"> <p>
								<code class="literal">openshift_kubeconfig_path</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140311138480064"> <p>
								Specifies a path and file name to a local directory that contains the <code class="literal">kubeconfig</code> file for your cluster.
							</p>
							 </td><td align="left" valign="top" headers="idm140311138478976"> <p>
								The path and name of the configuration file.
							</p>
							 </td></tr></tbody></table></div></section></section><section class="chapter" id="managing-user-provisioned-infrastructure-manually"><div class="titlepage"><div><div><h1 class="title">Chapter 10. Managing user-provisioned infrastructure manually</h1></div></div></div><section class="section" id="adding-compute-user-infra-general"><div class="titlepage"><div><div><h2 class="title">10.1. Adding compute machines to clusters with user-provisioned infrastructure manually</h2></div></div></div><p>
				You can add compute machines to a cluster on user-provisioned infrastructure either as part of the installation process or after installation. The post-installation process requires some of the same configuration files and parameters that were used during installation.
			</p><section class="section" id="upi-adding-compute-aws"><div class="titlepage"><div><div><h3 class="title">10.1.1. Adding compute machines to Amazon Web Services</h3></div></div></div><p>
					To add more compute machines to your OpenShift Container Platform cluster on Amazon Web Services (AWS), see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#adding-aws-compute-user-infra">Adding compute machines to AWS by using CloudFormation templates</a>.
				</p></section><section class="section" id="upi-adding-compute-azure"><div class="titlepage"><div><div><h3 class="title">10.1.2. Adding compute machines to Microsoft Azure</h3></div></div></div><p>
					To add more compute machines to your OpenShift Container Platform cluster on Microsoft Azure, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-creating-azure-worker_installing-azure-user-infra">Creating additional worker machines in Azure</a>.
				</p></section><section class="section" id="upi-adding-compute-ash"><div class="titlepage"><div><div><h3 class="title">10.1.3. Adding compute machines to Azure Stack Hub</h3></div></div></div><p>
					To add more compute machines to your OpenShift Container Platform cluster on Azure Stack Hub, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-creating-azure-worker_installing-azure-stack-hub-user-infra">Creating additional worker machines in Azure Stack Hub</a>.
				</p></section><section class="section" id="upi-adding-compute-gcp"><div class="titlepage"><div><div><h3 class="title">10.1.4. Adding compute machines to Google Cloud Platform</h3></div></div></div><p>
					To add more compute machines to your OpenShift Container Platform cluster on Google Cloud Platform (GCP), see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-creating-gcp-worker_installing-restricted-networks-gcp">Creating additional worker machines in GCP</a>.
				</p></section><section class="section" id="upi-adding-compute-vsphere"><div class="titlepage"><div><div><h3 class="title">10.1.5. Adding compute machines to vSphere</h3></div></div></div><p>
					You can <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-machineset-vsphere">use compute machine sets</a> to automate the creation of additional compute machines for your OpenShift Container Platform cluster on vSphere.
				</p><p>
					To manually add more compute machines to your cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#adding-vsphere-compute-user-infra">Adding compute machines to vSphere manually</a>.
				</p></section><section class="section" id="upi-adding-compute-rhv"><div class="titlepage"><div><div><h3 class="title">10.1.6. Adding compute machines to RHV</h3></div></div></div><p>
					To add more compute machines to your OpenShift Container Platform cluster on RHV, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#adding-rhv-compute-user-infra">Adding compute machines to RHV</a>.
				</p></section><section class="section" id="upi-adding-compute-bare-metal"><div class="titlepage"><div><div><h3 class="title">10.1.7. Adding compute machines to bare metal</h3></div></div></div><p>
					To add more compute machines to your OpenShift Container Platform cluster on bare metal, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#adding-bare-metal-compute-user-infra">Adding compute machines to bare metal</a>.
				</p></section></section><section class="section" id="adding-aws-compute-user-infra"><div class="titlepage"><div><div><h2 class="title">10.2. Adding compute machines to AWS by using CloudFormation templates</h2></div></div></div><p>
				You can add more compute machines to your OpenShift Container Platform cluster on Amazon Web Services (AWS) that you created by using the sample CloudFormation templates.
			</p><section class="section" id="prerequisites_adding-aws-compute-user-infra"><div class="titlepage"><div><div><h3 class="title">10.2.1. Prerequisites</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							You installed your cluster on AWS by using the provided <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-aws-user-infra">AWS CloudFormation templates</a>.
						</li><li class="listitem">
							You have the JSON file and CloudFormation template that you used to create the compute machines during cluster installation. If you do not have these files, you must recreate them by following the instructions in the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-aws-user-infra">installation procedure</a>.
						</li></ul></div></section><section class="section" id="machine-adding-aws-compute-cloudformation_adding-aws-compute-user-infra"><div class="titlepage"><div><div><h3 class="title">10.2.2. Adding more compute machines to your AWS cluster by using CloudFormation templates</h3></div></div></div><p>
					You can add more compute machines to your OpenShift Container Platform cluster on Amazon Web Services (AWS) that you created by using the sample CloudFormation templates.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The CloudFormation template creates a stack that represents one compute machine. You must create a stack for each compute machine.
					</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you do not use the provided CloudFormation template to create your compute nodes, you must review the provided information and manually create the infrastructure. If your cluster does not initialize correctly, you might have to contact Red Hat support with your installation logs.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You installed an OpenShift Container Platform cluster by using CloudFormation templates and have access to the JSON file and CloudFormation template that you used to create the compute machines during cluster installation.
						</li><li class="listitem">
							You installed the AWS CLI.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create another compute stack.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Launch the template:
								</p><pre class="programlisting language-terminal">$ aws cloudformation create-stack --stack-name &lt;name&gt; \ <span id="CO97-1"><!--Empty--></span><span class="callout">1</span>
     --template-body file://&lt;template&gt;.yaml \ <span id="CO97-2"><!--Empty--></span><span class="callout">2</span>
     --parameters file://&lt;parameters&gt;.json <span id="CO97-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO97-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											<code class="literal">&lt;name&gt;</code> is the name for the CloudFormation stack, such as <code class="literal">cluster-workers</code>. You must provide the name of this stack if you remove the cluster.
										</div></dd><dt><a href="#CO97-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											<code class="literal">&lt;template&gt;</code> is the relative path to and name of the CloudFormation template YAML file that you saved.
										</div></dd><dt><a href="#CO97-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											<code class="literal">&lt;parameters&gt;</code> is the relative path to and name of the CloudFormation parameters JSON file.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Confirm that the template components exist:
								</p><pre class="programlisting language-terminal">$ aws cloudformation describe-stacks --stack-name &lt;name&gt;</pre></li></ol></div></li><li class="listitem">
							Continue to create compute stacks until you have created enough compute machines for your cluster.
						</li></ol></div></section><section class="section" id="installation-approve-csrs_adding-aws-compute-user-infra"><div class="titlepage"><div><div><h3 class="title">10.2.3. Approving the certificate signing requests for your machines</h3></div></div></div><p>
					When you add machines to a cluster, two pending certificate signing requests (CSRs) are generated for each machine that you added. You must confirm that these CSRs are approved or, if necessary, approve them yourself. The client requests must be approved first, followed by the server requests.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You added machines to your cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Confirm that the cluster recognizes the machines:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  63m  v1.26.0
master-1  Ready     master  63m  v1.26.0
master-2  Ready     master  64m  v1.26.0</pre>

							</p></div><p class="simpara">
							The output lists all of the machines that you created.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The preceding output might not include the compute nodes, also known as worker nodes, until some CSRs are approved.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Review the pending CSRs and ensure that you see the client requests with the <code class="literal">Pending</code> or <code class="literal">Approved</code> status for each machine that you added to the cluster:
						</p><pre class="programlisting language-terminal">$ oc get csr</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-8b2br   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-8vnps   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
...</pre>

							</p></div><p class="simpara">
							In this example, two machines are joining the cluster. You might see more approved CSRs in the list.
						</p></li><li class="listitem"><p class="simpara">
							If the CSRs were not approved, after all of the pending CSRs for the machines you added are in <code class="literal">Pending</code> status, approve the CSRs for your cluster machines:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Because the CSRs rotate automatically, approve your CSRs within an hour of adding the machines to the cluster. If you do not approve them within an hour, the certificates will rotate, and more than two certificates will be present for each node. You must approve all of these certificates. After the client CSR is approved, the Kubelet creates a secondary CSR for the serving certificate, which requires manual approval. Then, subsequent serving certificate renewal requests are automatically approved by the <code class="literal">machine-approver</code> if the Kubelet requests a new certificate with identical parameters.
							</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								For clusters running on platforms that are not machine API enabled, such as bare metal and other user-provisioned infrastructure, you must implement a method of automatically approving the kubelet serving certificate requests (CSRs). If a request is not approved, then the <code class="literal">oc exec</code>, <code class="literal">oc rsh</code>, and <code class="literal">oc logs</code> commands cannot succeed, because a serving certificate is required when the API server connects to the kubelet. Any operation that contacts the Kubelet endpoint requires this certificate approval to be in place. The method must watch for new CSRs, confirm that the CSR was submitted by the <code class="literal">node-bootstrapper</code> service account in the <code class="literal">system:node</code> or <code class="literal">system:admin</code> groups, and confirm the identity of the node.
							</p></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To approve them individually, run the following command for each valid CSR:
								</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt; <span id="CO98-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO98-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											<code class="literal">&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To approve all pending CSRs, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs --no-run-if-empty oc adm certificate approve</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Some Operators might not become available until some CSRs are approved.
									</p></div></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Now that your client requests are approved, you must review the server requests for each machine that you added to the cluster:
						</p><pre class="programlisting language-terminal">$ oc get csr</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-bfd72   5m26s   system:node:ip-10-0-50-126.us-east-2.compute.internal                       Pending
csr-c57lv   5m26s   system:node:ip-10-0-95-157.us-east-2.compute.internal                       Pending
...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							If the remaining CSRs are not approved, and are in the <code class="literal">Pending</code> status, approve the CSRs for your cluster machines:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To approve them individually, run the following command for each valid CSR:
								</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt; <span id="CO99-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO99-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											<code class="literal">&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To approve all pending CSRs, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs oc adm certificate approve</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							After all client and server CSRs have been approved, the machines have the <code class="literal">Ready</code> status. Verify this by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  73m  v1.26.0
master-1  Ready     master  73m  v1.26.0
master-2  Ready     master  74m  v1.26.0
worker-0  Ready     worker  11m  v1.26.0
worker-1  Ready     worker  11m  v1.26.0</pre>

							</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								It can take a few minutes after approval of the server CSRs for the machines to transition to the <code class="literal">Ready</code> status.
							</p></div></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Additional information</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							For more information on CSRs, see <a class="link" href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/">Certificate Signing Requests</a>.
						</li></ul></div></section></section><section class="section" id="adding-vsphere-compute-user-infra"><div class="titlepage"><div><div><h2 class="title">10.3. Adding compute machines to vSphere manually</h2></div></div></div><p>
				You can add more compute machines to your OpenShift Container Platform cluster on VMware vSphere manually.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					You can also <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-machineset-vsphere">use compute machine sets</a> to automate the creation of additional VMware vSphere compute machines for your cluster.
				</p></div></div><section class="section" id="prerequisites-2"><div class="titlepage"><div><div><h3 class="title">10.3.1. Prerequisites</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							You <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-vsphere">installed a cluster on vSphere</a>.
						</li><li class="listitem">
							You have installation media and Red Hat Enterprise Linux CoreOS (RHCOS) images that you used to create your cluster. If you do not have these files, you must obtain them by following the instructions in the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-vsphere">installation procedure</a>.
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						If you do not have access to the Red Hat Enterprise Linux CoreOS (RHCOS) images that were used to create your cluster, you can add more compute machines to your OpenShift Container Platform cluster with newer versions of Red Hat Enterprise Linux CoreOS (RHCOS) images. For instructions, see <a class="link" href="https://access.redhat.com/solutions/5514051">Adding new nodes to UPI cluster fails after upgrading to OpenShift 4.6+</a>.
					</p></div></div></section><section class="section" id="machine-vsphere-machines_adding-vsphere-compute-user-infra"><div class="titlepage"><div><div><h3 class="title">10.3.2. Adding more compute machines to a cluster in vSphere</h3></div></div></div><p>
					You can add more compute machines to a user-provisioned OpenShift Container Platform cluster on VMware vSphere.
				</p><p>
					After your vSphere template deploys in your OpenShift Container Platform cluster, you can deploy a virtual machine (VM) for a machine in that cluster.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Obtain the base64-encoded Ignition file for your compute machines.
						</li><li class="listitem">
							You have access to the vSphere template that you created for your cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Right-click the template’s name and click <span class="strong strong"><strong>Clone</strong></span> → <span class="strong strong"><strong>Clone to Virtual Machine</strong></span>.
						</li><li class="listitem"><p class="simpara">
							On the <span class="strong strong"><strong>Select a name and folder</strong></span> tab, specify a name for the VM. You might include the machine type in the name, such as <code class="literal">compute-1</code>.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Ensure that all virtual machine names across a vSphere installation are unique.
							</p></div></div></li><li class="listitem">
							On the <span class="strong strong"><strong>Select a name and folder</strong></span> tab, select the name of the folder that you created for the cluster.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>Select a compute resource</strong></span> tab, select the name of a host in your datacenter.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>Select storage</strong></span> tab, select storage for your configuration and disk files.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>Select clone options</strong></span> tab, select <span class="strong strong"><strong>Customize this virtual machine’s hardware</strong></span>.
						</li><li class="listitem"><p class="simpara">
							On the <span class="strong strong"><strong>Customize hardware</strong></span> tab, click <span class="strong strong"><strong>Advanced Parameters</strong></span>.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Add the following configuration parameter names and values by specifying data in the <span class="strong strong"><strong>Attribute</strong></span> and <span class="strong strong"><strong>Values</strong></span> fields. Ensure that you select the <span class="strong strong"><strong>Add</strong></span> button for each parameter that you create.
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
											<code class="literal">guestinfo.ignition.config.data</code>: Paste the contents of the base64-encoded compute Ignition config file for this machine type.
										</li><li class="listitem">
											<code class="literal">guestinfo.ignition.config.data.encoding</code>: Specify <code class="literal">base64</code>.
										</li><li class="listitem">
											<code class="literal">disk.EnableUUID</code>: Specify <code class="literal">TRUE</code>.
										</li></ul></div></li></ul></div></li><li class="listitem">
							In the <span class="strong strong"><strong>Virtual Hardware</strong></span> panel of the <span class="strong strong"><strong>Customize hardware</strong></span> tab, modify the specified values as required. Ensure that the amount of RAM, CPU, and disk storage meets the minimum requirements for the machine type. If many networks exist, select <span class="strong strong"><strong>Add New Device</strong></span> &gt; <span class="strong strong"><strong>Network Adapter</strong></span>, and then enter your network information in the fields provided by the <span class="strong strong"><strong>New Network</strong></span> menu item.
						</li><li class="listitem">
							Complete the remaining configuration steps. On clicking the <span class="strong strong"><strong>Finish</strong></span> button, you have completed the cloning operation.
						</li><li class="listitem">
							From the <span class="strong strong"><strong>Virtual Machines</strong></span> tab, right-click on your VM and then select <span class="strong strong"><strong>Power</strong></span> → <span class="strong strong"><strong>Power On</strong></span>.
						</li></ol></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Continue to create more compute machines for your cluster.
						</li></ul></div></section><section class="section" id="installation-approve-csrs_adding-vsphere-compute-user-infra"><div class="titlepage"><div><div><h3 class="title">10.3.3. Approving the certificate signing requests for your machines</h3></div></div></div><p>
					When you add machines to a cluster, two pending certificate signing requests (CSRs) are generated for each machine that you added. You must confirm that these CSRs are approved or, if necessary, approve them yourself. The client requests must be approved first, followed by the server requests.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You added machines to your cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Confirm that the cluster recognizes the machines:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  63m  v1.26.0
master-1  Ready     master  63m  v1.26.0
master-2  Ready     master  64m  v1.26.0</pre>

							</p></div><p class="simpara">
							The output lists all of the machines that you created.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The preceding output might not include the compute nodes, also known as worker nodes, until some CSRs are approved.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Review the pending CSRs and ensure that you see the client requests with the <code class="literal">Pending</code> or <code class="literal">Approved</code> status for each machine that you added to the cluster:
						</p><pre class="programlisting language-terminal">$ oc get csr</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-8b2br   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-8vnps   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
...</pre>

							</p></div><p class="simpara">
							In this example, two machines are joining the cluster. You might see more approved CSRs in the list.
						</p></li><li class="listitem"><p class="simpara">
							If the CSRs were not approved, after all of the pending CSRs for the machines you added are in <code class="literal">Pending</code> status, approve the CSRs for your cluster machines:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Because the CSRs rotate automatically, approve your CSRs within an hour of adding the machines to the cluster. If you do not approve them within an hour, the certificates will rotate, and more than two certificates will be present for each node. You must approve all of these certificates. After the client CSR is approved, the Kubelet creates a secondary CSR for the serving certificate, which requires manual approval. Then, subsequent serving certificate renewal requests are automatically approved by the <code class="literal">machine-approver</code> if the Kubelet requests a new certificate with identical parameters.
							</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								For clusters running on platforms that are not machine API enabled, such as bare metal and other user-provisioned infrastructure, you must implement a method of automatically approving the kubelet serving certificate requests (CSRs). If a request is not approved, then the <code class="literal">oc exec</code>, <code class="literal">oc rsh</code>, and <code class="literal">oc logs</code> commands cannot succeed, because a serving certificate is required when the API server connects to the kubelet. Any operation that contacts the Kubelet endpoint requires this certificate approval to be in place. The method must watch for new CSRs, confirm that the CSR was submitted by the <code class="literal">node-bootstrapper</code> service account in the <code class="literal">system:node</code> or <code class="literal">system:admin</code> groups, and confirm the identity of the node.
							</p></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To approve them individually, run the following command for each valid CSR:
								</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt; <span id="CO100-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO100-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											<code class="literal">&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To approve all pending CSRs, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs --no-run-if-empty oc adm certificate approve</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Some Operators might not become available until some CSRs are approved.
									</p></div></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Now that your client requests are approved, you must review the server requests for each machine that you added to the cluster:
						</p><pre class="programlisting language-terminal">$ oc get csr</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-bfd72   5m26s   system:node:ip-10-0-50-126.us-east-2.compute.internal                       Pending
csr-c57lv   5m26s   system:node:ip-10-0-95-157.us-east-2.compute.internal                       Pending
...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							If the remaining CSRs are not approved, and are in the <code class="literal">Pending</code> status, approve the CSRs for your cluster machines:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To approve them individually, run the following command for each valid CSR:
								</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt; <span id="CO101-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO101-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											<code class="literal">&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To approve all pending CSRs, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs oc adm certificate approve</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							After all client and server CSRs have been approved, the machines have the <code class="literal">Ready</code> status. Verify this by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  73m  v1.26.0
master-1  Ready     master  73m  v1.26.0
master-2  Ready     master  74m  v1.26.0
worker-0  Ready     worker  11m  v1.26.0
worker-1  Ready     worker  11m  v1.26.0</pre>

							</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								It can take a few minutes after approval of the server CSRs for the machines to transition to the <code class="literal">Ready</code> status.
							</p></div></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Additional information</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							For more information on CSRs, see <a class="link" href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/">Certificate Signing Requests</a>.
						</li></ul></div></section></section><section class="section" id="adding-rhv-compute-user-infra"><div class="titlepage"><div><div><h2 class="title">10.4. Adding compute machines to a cluster on RHV</h2></div></div></div><p>
				In OpenShift Container Platform version 4.13, you can add more compute machines to a user-provisioned OpenShift Container Platform cluster on RHV.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You installed a cluster on RHV with user-provisioned infrastructure.
					</li></ul></div><section class="section" id="machine-user-provisioned-rhv_adding-rhv-compute-user-infra"><div class="titlepage"><div><div><h3 class="title">10.4.1. Adding more compute machines to a cluster on RHV</h3></div></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Modify the <code class="literal">inventory.yml</code> file to include the new workers.
						</li><li class="listitem"><p class="simpara">
							Run the <code class="literal">create-templates-and-vms</code> Ansible playbook to create the disks and virtual machines:
						</p><pre class="programlisting language-terminal">$ ansible-playbook -i inventory.yml create-templates-and-vms.yml</pre></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">workers.yml</code> Ansible playbook to start the virtual machines:
						</p><pre class="programlisting language-terminal">$ ansible-playbook -i inventory.yml workers.yml</pre></li><li class="listitem"><p class="simpara">
							CSRs for new workers joining the cluster must be approved by the administrator. The following command helps to approve all pending requests:
						</p><pre class="programlisting language-terminal">$ oc get csr -ojson | jq -r '.items[] | select(.status == {} ) | .metadata.name' | xargs oc adm certificate approve</pre></li></ol></div></section></section><section class="section" id="adding-bare-metal-compute-user-infra"><div class="titlepage"><div><div><h2 class="title">10.5. Adding compute machines to bare metal</h2></div></div></div><p>
				You can add more compute machines to your OpenShift Container Platform cluster on bare metal.
			</p><section class="section" id="prerequisites-3"><div class="titlepage"><div><div><h3 class="title">10.5.1. Prerequisites</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							You <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-bare-metal">installed a cluster on bare metal</a>.
						</li><li class="listitem">
							You have installation media and Red Hat Enterprise Linux CoreOS (RHCOS) images that you used to create your cluster. If you do not have these files, you must obtain them by following the instructions in the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-bare-metal">installation procedure</a>.
						</li><li class="listitem">
							If a DHCP server is available for your user-provisioned infrastructure, you have added the details for the additional compute machines to your DHCP server configuration. This includes a persistent IP address, DNS server information, and a hostname for each machine.
						</li><li class="listitem">
							You have updated your DNS configuration to include the record name and IP address of each compute machine that you are adding. You have validated that DNS lookup and reverse DNS lookup resolve correctly.
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						If you do not have access to the Red Hat Enterprise Linux CoreOS (RHCOS) images that were used to create your cluster, you can add more compute machines to your OpenShift Container Platform cluster with newer versions of Red Hat Enterprise Linux CoreOS (RHCOS) images. For instructions, see <a class="link" href="https://access.redhat.com/solutions/5514051">Adding new nodes to UPI cluster fails after upgrading to OpenShift 4.6+</a>.
					</p></div></div></section><section class="section" id="creating-rhcos-machines-bare-metal"><div class="titlepage"><div><div><h3 class="title">10.5.2. Creating Red Hat Enterprise Linux CoreOS (RHCOS) machines</h3></div></div></div><p>
					Before you add more compute machines to a cluster that you installed on bare metal infrastructure, you must create RHCOS machines for it to use. You can either use an ISO image or network PXE booting to create the machines.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You must use the same ISO image that you used to install a cluster to deploy all new nodes in a cluster. It is recommended to use the same Ignition config file. The nodes automatically upgrade themselves on the first boot before running the workloads. You can add the nodes before or after the upgrade.
					</p></div></div><section class="section" id="machine-user-infra-machines-iso_adding-bare-metal-compute-user-infra"><div class="titlepage"><div><div><h4 class="title">10.5.2.1. Creating RHCOS machines using an ISO image</h4></div></div></div><p>
						You can create more Red Hat Enterprise Linux CoreOS (RHCOS) compute machines for your bare metal cluster by using an ISO image to create the machines.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Obtain the URL of the Ignition config file for the compute machines for your cluster. You uploaded this file to your HTTP server during installation.
							</li><li class="listitem">
								You must have the OpenShift CLI (<code class="literal">oc</code>) installed.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Extract the Ignition config file from the cluster by running the following command:
							</p><pre class="programlisting language-terminal">$ oc extract -n openshift-machine-api secret/worker-user-data-managed --keys=userData --to=- &gt; worker.ign</pre></li><li class="listitem">
								Upload the <code class="literal">worker.ign</code> Ignition config file you exported from your cluster to your HTTP server. Note the URLs of these files.
							</li><li class="listitem"><p class="simpara">
								You can validate that the ignition files are available on the URLs. The following example gets the Ignition config files for the compute node:
							</p><pre class="programlisting language-terminal">$ curl -k http://&lt;HTTP_server&gt;/worker.ign</pre></li><li class="listitem"><p class="simpara">
								You can access the ISO image for booting your new machine by running to following command:
							</p><pre class="programlisting language-terminal">RHCOS_VHD_ORIGIN_URL=$(oc -n openshift-machine-config-operator get configmap/coreos-bootimages -o jsonpath='{.data.stream}' | jq -r '.architectures.&lt;architecture&gt;.artifacts.metal.formats.iso.disk.location')</pre></li><li class="listitem"><p class="simpara">
								Use the ISO file to install RHCOS on more compute machines. Use the same method that you used when you created machines before you installed the cluster:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Burn the ISO image to a disk and boot it directly.
									</li><li class="listitem">
										Use ISO redirection with a LOM interface.
									</li></ul></div></li><li class="listitem"><p class="simpara">
								Boot the RHCOS ISO image without specifying any options, or interrupting the live boot sequence. Wait for the installer to boot into a shell prompt in the RHCOS live environment.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									You can interrupt the RHCOS installation boot process to add kernel arguments. However, for this ISO procedure you must use the <code class="literal">coreos-installer</code> command as outlined in the following steps, instead of adding kernel arguments.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Run the <code class="literal">coreos-installer</code> command and specify the options that meet your installation requirements. At a minimum, you must specify the URL that points to the Ignition config file for the node type, and the device that you are installing to:
							</p><pre class="programlisting language-terminal">$ sudo coreos-installer install --ignition-url=http://&lt;HTTP_server&gt;/&lt;node_type&gt;.ign &lt;device&gt; --ignition-hash=sha512-&lt;digest&gt; <span id="CO102-1"><!--Empty--></span><span class="callout">1</span><span id="CO102-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO102-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										You must run the <code class="literal">coreos-installer</code> command by using <code class="literal">sudo</code>, because the <code class="literal">core</code> user does not have the required root privileges to perform the installation.
									</div></dd><dt><a href="#CO102-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The <code class="literal">--ignition-hash</code> option is required when the Ignition config file is obtained through an HTTP URL to validate the authenticity of the Ignition config file on the cluster node. <code class="literal">&lt;digest&gt;</code> is the Ignition config file SHA512 digest obtained in a preceding step.
									</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									If you want to provide your Ignition config files through an HTTPS server that uses TLS, you can add the internal certificate authority (CA) to the system trust store before running <code class="literal">coreos-installer</code>.
								</p></div></div><p class="simpara">
								The following example initializes a bootstrap node installation to the <code class="literal">/dev/sda</code> device. The Ignition config file for the bootstrap node is obtained from an HTTP web server with the IP address 192.168.1.2:
							</p><pre class="programlisting language-terminal">$ sudo coreos-installer install --ignition-url=http://192.168.1.2:80/installation_directory/bootstrap.ign /dev/sda --ignition-hash=sha512-a5a2d43879223273c9b60af66b44202a1d1248fc01cf156c46d4a79f552b6bad47bc8cc78ddf0116e80c59d2ea9e32ba53bc807afbca581aa059311def2c3e3b</pre></li><li class="listitem"><p class="simpara">
								Monitor the progress of the RHCOS installation on the console of the machine.
							</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									Ensure that the installation is successful on each node before commencing with the OpenShift Container Platform installation. Observing the installation process can also help to determine the cause of RHCOS installation issues that might arise.
								</p></div></div></li><li class="listitem">
								Continue to create more compute machines for your cluster.
							</li></ol></div></section><section class="section" id="machine-user-infra-machines-pxe_adding-bare-metal-compute-user-infra"><div class="titlepage"><div><div><h4 class="title">10.5.2.2. Creating RHCOS machines by PXE or iPXE booting</h4></div></div></div><p>
						You can create more Red Hat Enterprise Linux CoreOS (RHCOS) compute machines for your bare metal cluster by using PXE or iPXE booting.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Obtain the URL of the Ignition config file for the compute machines for your cluster. You uploaded this file to your HTTP server during installation.
							</li><li class="listitem">
								Obtain the URLs of the RHCOS ISO image, compressed metal BIOS, <code class="literal">kernel</code>, and <code class="literal">initramfs</code> files that you uploaded to your HTTP server during cluster installation.
							</li><li class="listitem">
								You have access to the PXE booting infrastructure that you used to create the machines for your OpenShift Container Platform cluster during installation. The machines must boot from their local disks after RHCOS is installed on them.
							</li><li class="listitem">
								If you use UEFI, you have access to the <code class="literal">grub.conf</code> file that you modified during OpenShift Container Platform installation.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Confirm that your PXE or iPXE installation for the RHCOS images is correct.
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
										For PXE:
									</p><pre class="screen">DEFAULT pxeboot
TIMEOUT 20
PROMPT 0
LABEL pxeboot
    KERNEL http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-kernel-&lt;architecture&gt; <span id="CO103-1"><!--Empty--></span><span class="callout">1</span>
    APPEND initrd=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-initramfs.&lt;architecture&gt;.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://&lt;HTTP_server&gt;/worker.ign coreos.live.rootfs_url=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-rootfs.&lt;architecture&gt;.img <span id="CO103-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO103-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Specify the location of the live <code class="literal">kernel</code> file that you uploaded to your HTTP server.
											</div></dd><dt><a href="#CO103-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Specify locations of the RHCOS files that you uploaded to your HTTP server. The <code class="literal">initrd</code> parameter value is the location of the live <code class="literal">initramfs</code> file, the <code class="literal">coreos.inst.ignition_url</code> parameter value is the location of the worker Ignition config file, and the <code class="literal">coreos.live.rootfs_url</code> parameter value is the location of the live <code class="literal">rootfs</code> file. The <code class="literal">coreos.inst.ignition_url</code> and <code class="literal">coreos.live.rootfs_url</code> parameters only support HTTP and HTTPS.
											</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											This configuration does not enable serial console access on machines with a graphical console. To configure a different console, add one or more <code class="literal">console=</code> arguments to the <code class="literal">APPEND</code> line. For example, add <code class="literal">console=tty0 console=ttyS0</code> to set the first PC serial port as the primary console and the graphical console as a secondary console. For more information, see <a class="link" href="https://access.redhat.com/articles/7212">How does one set up a serial terminal and/or console in Red Hat Enterprise Linux?</a>.
										</p></div></div></li><li class="listitem"><p class="simpara">
										For iPXE (<code class="literal">x86_64</code> + <code class="literal">aarch64</code>):
									</p><pre class="screen">kernel http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-kernel-&lt;architecture&gt; initrd=main coreos.live.rootfs_url=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-rootfs.&lt;architecture&gt;.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://&lt;HTTP_server&gt;/worker.ign <span id="CO104-1"><!--Empty--></span><span class="callout">1</span> <span id="CO104-2"><!--Empty--></span><span class="callout">2</span>
initrd --name main http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-initramfs.&lt;architecture&gt;.img <span id="CO104-3"><!--Empty--></span><span class="callout">3</span>
boot</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO104-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Specify the locations of the RHCOS files that you uploaded to your HTTP server. The <code class="literal">kernel</code> parameter value is the location of the <code class="literal">kernel</code> file, the <code class="literal">initrd=main</code> argument is needed for booting on UEFI systems, the <code class="literal">coreos.live.rootfs_url</code> parameter value is the location of the <code class="literal">rootfs</code> file, and the <code class="literal">coreos.inst.ignition_url</code> parameter value is the location of the worker Ignition config file.
											</div></dd><dt><a href="#CO104-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												If you use multiple NICs, specify a single interface in the <code class="literal">ip</code> option. For example, to use DHCP on a NIC that is named <code class="literal">eno1</code>, set <code class="literal">ip=eno1:dhcp</code>.
											</div></dd><dt><a href="#CO104-3"><span class="callout">3</span></a> </dt><dd><div class="para">
												Specify the location of the <code class="literal">initramfs</code> file that you uploaded to your HTTP server.
											</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											This configuration does not enable serial console access on machines with a graphical console To configure a different console, add one or more <code class="literal">console=</code> arguments to the <code class="literal">kernel</code> line. For example, add <code class="literal">console=tty0 console=ttyS0</code> to set the first PC serial port as the primary console and the graphical console as a secondary console. For more information, see <a class="link" href="https://access.redhat.com/articles/7212">How does one set up a serial terminal and/or console in Red Hat Enterprise Linux?</a> and "Enabling the serial console for PXE and ISO installation" in the "Advanced RHCOS installation configuration" section.
										</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											To network boot the CoreOS <code class="literal">kernel</code> on <code class="literal">aarch64</code> architecture, you need to use a version of iPXE build with the <code class="literal">IMAGE_GZIP</code> option enabled. See <a class="link" href="https://ipxe.org/buildcfg/image_gzip"><code class="literal">IMAGE_GZIP</code> option in iPXE</a>.
										</p></div></div></li><li class="listitem"><p class="simpara">
										For PXE (with UEFI and GRUB as second stage) on <code class="literal">aarch64</code>:
									</p><pre class="screen">menuentry 'Install CoreOS' {
    linux rhcos-&lt;version&gt;-live-kernel-&lt;architecture&gt;  coreos.live.rootfs_url=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-rootfs.&lt;architecture&gt;.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://&lt;HTTP_server&gt;/worker.ign <span id="CO105-1"><!--Empty--></span><span class="callout">1</span> <span id="CO105-2"><!--Empty--></span><span class="callout">2</span>
    initrd rhcos-&lt;version&gt;-live-initramfs.&lt;architecture&gt;.img <span id="CO105-3"><!--Empty--></span><span class="callout">3</span>
}</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO105-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Specify the locations of the RHCOS files that you uploaded to your HTTP/TFTP server. The <code class="literal">kernel</code> parameter value is the location of the <code class="literal">kernel</code> file on your TFTP server. The <code class="literal">coreos.live.rootfs_url</code> parameter value is the location of the <code class="literal">rootfs</code> file, and the <code class="literal">coreos.inst.ignition_url</code> parameter value is the location of the worker Ignition config file on your HTTP Server.
											</div></dd><dt><a href="#CO105-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												If you use multiple NICs, specify a single interface in the <code class="literal">ip</code> option. For example, to use DHCP on a NIC that is named <code class="literal">eno1</code>, set <code class="literal">ip=eno1:dhcp</code>.
											</div></dd><dt><a href="#CO105-3"><span class="callout">3</span></a> </dt><dd><div class="para">
												Specify the location of the <code class="literal">initramfs</code> file that you uploaded to your TFTP server.
											</div></dd></dl></div></li></ul></div></li><li class="listitem">
								Use the PXE or iPXE infrastructure to create the required compute machines for your cluster.
							</li></ol></div></section></section><section class="section" id="installation-approve-csrs_adding-bare-metal-compute-user-infra"><div class="titlepage"><div><div><h3 class="title">10.5.3. Approving the certificate signing requests for your machines</h3></div></div></div><p>
					When you add machines to a cluster, two pending certificate signing requests (CSRs) are generated for each machine that you added. You must confirm that these CSRs are approved or, if necessary, approve them yourself. The client requests must be approved first, followed by the server requests.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You added machines to your cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Confirm that the cluster recognizes the machines:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  63m  v1.26.0
master-1  Ready     master  63m  v1.26.0
master-2  Ready     master  64m  v1.26.0</pre>

							</p></div><p class="simpara">
							The output lists all of the machines that you created.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The preceding output might not include the compute nodes, also known as worker nodes, until some CSRs are approved.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Review the pending CSRs and ensure that you see the client requests with the <code class="literal">Pending</code> or <code class="literal">Approved</code> status for each machine that you added to the cluster:
						</p><pre class="programlisting language-terminal">$ oc get csr</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-8b2br   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-8vnps   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
...</pre>

							</p></div><p class="simpara">
							In this example, two machines are joining the cluster. You might see more approved CSRs in the list.
						</p></li><li class="listitem"><p class="simpara">
							If the CSRs were not approved, after all of the pending CSRs for the machines you added are in <code class="literal">Pending</code> status, approve the CSRs for your cluster machines:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Because the CSRs rotate automatically, approve your CSRs within an hour of adding the machines to the cluster. If you do not approve them within an hour, the certificates will rotate, and more than two certificates will be present for each node. You must approve all of these certificates. After the client CSR is approved, the Kubelet creates a secondary CSR for the serving certificate, which requires manual approval. Then, subsequent serving certificate renewal requests are automatically approved by the <code class="literal">machine-approver</code> if the Kubelet requests a new certificate with identical parameters.
							</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								For clusters running on platforms that are not machine API enabled, such as bare metal and other user-provisioned infrastructure, you must implement a method of automatically approving the kubelet serving certificate requests (CSRs). If a request is not approved, then the <code class="literal">oc exec</code>, <code class="literal">oc rsh</code>, and <code class="literal">oc logs</code> commands cannot succeed, because a serving certificate is required when the API server connects to the kubelet. Any operation that contacts the Kubelet endpoint requires this certificate approval to be in place. The method must watch for new CSRs, confirm that the CSR was submitted by the <code class="literal">node-bootstrapper</code> service account in the <code class="literal">system:node</code> or <code class="literal">system:admin</code> groups, and confirm the identity of the node.
							</p></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To approve them individually, run the following command for each valid CSR:
								</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt; <span id="CO106-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO106-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											<code class="literal">&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To approve all pending CSRs, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs --no-run-if-empty oc adm certificate approve</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Some Operators might not become available until some CSRs are approved.
									</p></div></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Now that your client requests are approved, you must review the server requests for each machine that you added to the cluster:
						</p><pre class="programlisting language-terminal">$ oc get csr</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-bfd72   5m26s   system:node:ip-10-0-50-126.us-east-2.compute.internal                       Pending
csr-c57lv   5m26s   system:node:ip-10-0-95-157.us-east-2.compute.internal                       Pending
...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							If the remaining CSRs are not approved, and are in the <code class="literal">Pending</code> status, approve the CSRs for your cluster machines:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To approve them individually, run the following command for each valid CSR:
								</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt; <span id="CO107-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO107-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											<code class="literal">&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To approve all pending CSRs, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs oc adm certificate approve</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							After all client and server CSRs have been approved, the machines have the <code class="literal">Ready</code> status. Verify this by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  73m  v1.26.0
master-1  Ready     master  73m  v1.26.0
master-2  Ready     master  74m  v1.26.0
worker-0  Ready     worker  11m  v1.26.0
worker-1  Ready     worker  11m  v1.26.0</pre>

							</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								It can take a few minutes after approval of the server CSRs for the machines to transition to the <code class="literal">Ready</code> status.
							</p></div></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Additional information</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							For more information on CSRs, see <a class="link" href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/">Certificate Signing Requests</a>.
						</li></ul></div></section></section></section><section class="chapter" id="capi-machine-management"><div class="titlepage"><div><div><h1 class="title">Chapter 11. Managing machines with the Cluster API</h1></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				Managing machines with the Cluster API is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
			</p><p>
				For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
			</p></div></div><p>
			The <a class="link" href="https://cluster-api.sigs.k8s.io/">Cluster API</a> is an upstream project that is integrated into OpenShift Container Platform as a Technology Preview for Amazon Web Services (AWS) and Google Cloud Platform (GCP) clusters. You can use the Cluster API to create and manage compute machine sets and machines in your OpenShift Container Platform cluster. This capability is in addition or an alternative to managing machines with the Machine API.
		</p><p>
			For OpenShift Container Platform 4.13 clusters, you can use the Cluster API to perform node host provisioning management actions after the cluster installation finishes. This system enables an elastic, dynamic provisioning method on top of public or private cloud infrastructure.
		</p><p>
			With the Cluster API Technology Preview, you can create compute machines and compute machine sets on OpenShift Container Platform clusters for supported providers. You can also explore the features that are enabled by this implementation that might not be available with the Machine API.
		</p><h3 id="cluster-api-benefits_capi-machine-management">Benefits</h3><p>
			By using the Cluster API, OpenShift Container Platform users and developers are able to realize the following advantages:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					The option to use upstream community Cluster API infrastructure providers which might not be supported by the Machine API.
				</li><li class="listitem">
					The opportunity to collaborate with third parties who maintain machine controllers for infrastructure providers.
				</li><li class="listitem">
					The ability to use the same set of Kubernetes tools for infrastructure management in OpenShift Container Platform.
				</li><li class="listitem">
					The ability to create compute machine sets using the Cluster API that support features that are not available with the Machine API.
				</li></ul></div><h3 id="capi-tech-preview-limitations">Limitations</h3><p>
			Using the Cluster API to manage machines is a Technology Preview feature and has the following limitations:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Only AWS and GCP clusters are supported.
				</li><li class="listitem">
					To use this feature, you must enable the <code class="literal">TechPreviewNoUpgrade</code> <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-cluster-enabling-features-about_nodes-cluster-enabling">feature set</a>. Enabling this feature set cannot be undone and prevents minor version updates.
				</li><li class="listitem">
					You must create the primary resources that the Cluster API requires manually.
				</li><li class="listitem">
					Control plane machines cannot be managed by the Cluster API.
				</li><li class="listitem">
					Migration of existing compute machine sets created by the Machine API to Cluster API compute machine sets is not supported.
				</li><li class="listitem">
					Full feature parity with the Machine API is not available.
				</li></ul></div><section class="section" id="cluster-api-architecture_capi-machine-management"><div class="titlepage"><div><div><h2 class="title">11.1. Cluster API architecture</h2></div></div></div><p>
				The OpenShift Container Platform integration of the upstream Cluster API is implemented and managed by the Cluster CAPI Operator. The Cluster CAPI Operator and its operands are provisioned in the <code class="literal">openshift-cluster-api</code> namespace, in contrast to the Machine API, which uses the <code class="literal">openshift-machine-api</code> namespace.
			</p><section class="section" id="capi-arch-operator"><div class="titlepage"><div><div><h3 class="title">11.1.1. The Cluster CAPI Operator</h3></div></div></div><p>
					The Cluster CAPI Operator is an OpenShift Container Platform Operator that maintains the lifecycle of Cluster API resources. This Operator is responsible for all administrative tasks related to deploying the Cluster API project within an OpenShift Container Platform cluster.
				</p><p>
					If a cluster is configured correctly to allow the use of the Cluster API, the Cluster CAPI Operator installs the Cluster API Operator on the cluster.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The Cluster CAPI Operator is distinct from the upstream Cluster API Operator.
					</p></div></div><p>
					For more information, see the entry for the Cluster CAPI Operator in the <span class="emphasis"><em>Cluster Operators reference</em></span> content.
				</p></section><section class="section" id="capi-arch-resources"><div class="titlepage"><div><div><h3 class="title">11.1.2. Primary resources</h3></div></div></div><p>
					The Cluster API is comprised of the following primary resources. For the Technology Preview of this feature, you must create these resources manually in the <code class="literal">openshift-cluster-api</code> namespace.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Cluster</span></dt><dd>
								A fundamental unit that represents a cluster that is managed by the Cluster API.
							</dd><dt><span class="term">Infrastructure</span></dt><dd>
								A provider-specific resource that defines properties that are shared by all the compute machine sets in the cluster, such as the region and subnets.
							</dd><dt><span class="term">Machine template</span></dt><dd>
								A provider-specific template that defines the properties of the machines that a compute machine set creates.
							</dd><dt><span class="term">Machine set</span></dt><dd><p class="simpara">
								A group of machines.
							</p><p class="simpara">
								Compute machine sets are to machines as replica sets are to pods. If you need more machines or must scale them down, you change the <code class="literal">replicas</code> field on the compute machine set to meet your compute needs.
							</p><p class="simpara">
								With the Cluster API, a compute machine set references a <code class="literal">Cluster</code> object and a provider-specific machine template.
							</p></dd><dt><span class="term">Machine</span></dt><dd><p class="simpara">
								A fundamental unit that describes the host for a node.
							</p><p class="simpara">
								The Cluster API creates machines based on the configuration in the machine template.
							</p></dd></dl></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#cluster-capi-operator_cluster-operators-ref">Cluster CAPI Operator</a>
						</li></ul></div></section></section><section class="section" id="capi-sample-yaml-files"><div class="titlepage"><div><div><h2 class="title">11.2. Sample YAML files</h2></div></div></div><p>
				For the Cluster API Technology Preview, you must create the primary resources that the Cluster API requires manually. The example YAML files in this section demonstrate how to make these resources work together and configure settings for the machines that they create that are appropriate for your environment.
			</p><section class="section" id="capi-yaml-cluster_capi-machine-management"><div class="titlepage"><div><div><h3 class="title">11.2.1. Sample YAML for a Cluster API cluster resource</h3></div></div></div><p>
					The cluster resource defines the name and infrastructure provider for the cluster and is managed by the Cluster API. This resource has the same structure for all providers.
				</p><pre class="programlisting language-yaml">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: &lt;cluster_name&gt; <span id="CO108-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-cluster-api
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: &lt;infrastructure_kind&gt; <span id="CO108-2"><!--Empty--></span><span class="callout">2</span>
    name: &lt;cluster_name&gt; <span id="CO108-3"><!--Empty--></span><span class="callout">3</span>
    namespace: openshift-cluster-api</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO108-1"><span class="callout">1</span></a> <a href="#CO108-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Specify the name of the cluster.
						</div></dd><dt><a href="#CO108-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Specify the infrastructure kind for the cluster. Valid values are:
						</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">AWSCluster</code>: The cluster is running on Amazon Web Services (AWS).
								</li><li class="listitem">
									<code class="literal">GCPCluster</code>: The cluster is running on Google Cloud Platform (GCP).
								</li></ul></div></dd></dl></div><p>
					The remaining Cluster API resources are provider-specific. Refer to the example YAML files for your cluster:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#capi-sample-yaml-files-aws">Sample YAML files for configuring Amazon Web Services clusters</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#capi-sample-yaml-files-gcp">Sample YAML files for configuring Google Cloud Platform clusters</a>
						</li></ul></div></section><section class="section" id="capi-sample-yaml-files-aws"><div class="titlepage"><div><div><h3 class="title">11.2.2. Sample YAML files for configuring Amazon Web Services clusters</h3></div></div></div><p>
					Some Cluster API resources are provider-specific. The example YAML files in this section show configurations for an Amazon Web Services (AWS) cluster.
				</p><section class="section" id="capi-yaml-infrastructure-aws_capi-machine-management"><div class="titlepage"><div><div><h4 class="title">11.2.2.1. Sample YAML for a Cluster API infrastructure resource on Amazon Web Services</h4></div></div></div><p>
						The infrastructure resource is provider-specific and defines properties that are shared by all the compute machine sets in the cluster, such as the region and subnets. The compute machine set references this resource when creating machines.
					</p><pre class="programlisting language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSCluster <span id="CO109-1"><!--Empty--></span><span class="callout">1</span>
metadata:
  name: &lt;cluster_name&gt; <span id="CO109-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-cluster-api
spec:
  region: &lt;region&gt; <span id="CO109-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO109-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specify the infrastructure kind for the cluster. This value must match the value for your platform.
							</div></dd><dt><a href="#CO109-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specify the name of the cluster.
							</div></dd><dt><a href="#CO109-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specify the AWS region.
							</div></dd></dl></div></section><section class="section" id="capi-yaml-machine-template-aws_capi-machine-management"><div class="titlepage"><div><div><h4 class="title">11.2.2.2. Sample YAML for a Cluster API machine template resource on Amazon Web Services</h4></div></div></div><p>
						The machine template resource is provider-specific and defines the basic properties of the machines that a compute machine set creates. The compute machine set references this template when creating machines.
					</p><pre class="programlisting language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1alpha4
kind: AWSMachineTemplate <span id="CO110-1"><!--Empty--></span><span class="callout">1</span>
metadata:
  name: &lt;template_name&gt; <span id="CO110-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-cluster-api
spec:
  template:
    spec: <span id="CO110-3"><!--Empty--></span><span class="callout">3</span>
      uncompressedUserData: true
      iamInstanceProfile: ....
      instanceType: m5.large
      cloudInit:
        insecureSkipSecretsManager: true
      ami:
        id: ....
      subnet:
        filters:
        - name: tag:Name
          values:
          - ...
      additionalSecurityGroups:
      - filters:
        - name: tag:Name
          values:
          - ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO110-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specify the machine template kind. This value must match the value for your platform.
							</div></dd><dt><a href="#CO110-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specify a name for the machine template.
							</div></dd><dt><a href="#CO110-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specify the details for your environment. The values here are examples.
							</div></dd></dl></div></section><section class="section" id="capi-yaml-machine-set-aws_capi-machine-management"><div class="titlepage"><div><div><h4 class="title">11.2.2.3. Sample YAML for a Cluster API compute machine set resource on Amazon Web Services</h4></div></div></div><p>
						The compute machine set resource defines additional properties of the machines that it creates. The compute machine set also references the infrastructure resource and machine template when creating machines.
					</p><pre class="programlisting language-yaml">apiVersion: cluster.x-k8s.io/v1alpha4
kind: MachineSet
metadata:
  name: &lt;machine_set_name&gt; <span id="CO111-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-cluster-api
spec:
  clusterName: &lt;cluster_name&gt; <span id="CO111-2"><!--Empty--></span><span class="callout">2</span>
  replicas: 1
  selector:
    matchLabels:
      test: example
  template:
    metadata:
      labels:
        test: example
    spec:
      bootstrap:
         dataSecretName: worker-user-data <span id="CO111-3"><!--Empty--></span><span class="callout">3</span>
      clusterName: &lt;cluster_name&gt; <span id="CO111-4"><!--Empty--></span><span class="callout">4</span>
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1alpha4
        kind: AWSMachineTemplate <span id="CO111-5"><!--Empty--></span><span class="callout">5</span>
        name: &lt;cluster_name&gt; <span id="CO111-6"><!--Empty--></span><span class="callout">6</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO111-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specify a name for the compute machine set.
							</div></dd><dt><a href="#CO111-2"><span class="callout">2</span></a> <a href="#CO111-4"><span class="callout">4</span></a> <a href="#CO111-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								Specify the name of the cluster.
							</div></dd><dt><a href="#CO111-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								For the Cluster API Technology Preview, the Operator can use the worker user data secret from <code class="literal">openshift-machine-api</code> namespace.
							</div></dd><dt><a href="#CO111-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Specify the machine template kind. This value must match the value for your platform.
							</div></dd></dl></div></section></section><section class="section" id="capi-sample-yaml-files-gcp"><div class="titlepage"><div><div><h3 class="title">11.2.3. Sample YAML files for configuring Google Cloud Platform clusters</h3></div></div></div><p>
					Some Cluster API resources are provider-specific. The example YAML files in this section show configurations for a Google Cloud Platform (GCP) cluster.
				</p><section class="section" id="capi-yaml-infrastructure-gcp_capi-machine-management"><div class="titlepage"><div><div><h4 class="title">11.2.3.1. Sample YAML for a Cluster API infrastructure resource on Google Cloud Platform</h4></div></div></div><p>
						The infrastructure resource is provider-specific and defines properties that are shared by all the compute machine sets in the cluster, such as the region and subnets. The compute machine set references this resource when creating machines.
					</p><pre class="programlisting language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: GCPCluster <span id="CO112-1"><!--Empty--></span><span class="callout">1</span>
metadata:
  name: &lt;cluster_name&gt; <span id="CO112-2"><!--Empty--></span><span class="callout">2</span>
spec:
  network:
    name: &lt;cluster_name&gt;-network <span id="CO112-3"><!--Empty--></span><span class="callout">3</span>
  project: &lt;project&gt; <span id="CO112-4"><!--Empty--></span><span class="callout">4</span>
  region: &lt;region&gt; <span id="CO112-5"><!--Empty--></span><span class="callout">5</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO112-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specify the infrastructure kind for the cluster. This value must match the value for your platform.
							</div></dd><dt><a href="#CO112-2"><span class="callout">2</span></a> <a href="#CO112-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specify the name of the cluster.
							</div></dd><dt><a href="#CO112-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Specify the GCP project name.
							</div></dd><dt><a href="#CO112-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Specify the GCP region.
							</div></dd></dl></div></section><section class="section" id="capi-yaml-machine-template-gcp_capi-machine-management"><div class="titlepage"><div><div><h4 class="title">11.2.3.2. Sample YAML for a Cluster API machine template resource on Google Cloud Platform</h4></div></div></div><p>
						The machine template resource is provider-specific and defines the basic properties of the machines that a compute machine set creates. The compute machine set references this template when creating machines.
					</p><pre class="programlisting language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: GCPMachineTemplate <span id="CO113-1"><!--Empty--></span><span class="callout">1</span>
metadata:
  name: &lt;template_name&gt; <span id="CO113-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-cluster-api
spec:
  template:
    spec: <span id="CO113-3"><!--Empty--></span><span class="callout">3</span>
      rootDeviceType: pd-ssd
      rootDeviceSize: 128
      instanceType: n1-standard-4
      image: projects/rhcos-cloud/global/images/rhcos-411-85-202203181601-0-gcp-x86-64
      subnet: &lt;cluster_name&gt;-worker-subnet
      serviceAccounts:
        email: &lt;service_account_email_address&gt;
        scopes:
          - https://www.googleapis.com/auth/cloud-platform
      additionalLabels:
        kubernetes-io-cluster-&lt;cluster_name&gt;: owned
      additionalNetworkTags:
        - &lt;cluster_name&gt;-worker
      ipForwarding: Disabled</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO113-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specify the machine template kind. This value must match the value for your platform.
							</div></dd><dt><a href="#CO113-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specify a name for the machine template.
							</div></dd><dt><a href="#CO113-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specify the details for your environment. The values here are examples.
							</div></dd></dl></div></section><section class="section" id="capi-yaml-machine-set-gcp_capi-machine-management"><div class="titlepage"><div><div><h4 class="title">11.2.3.3. Sample YAML for a Cluster API compute machine set resource on Google Cloud Platform</h4></div></div></div><p>
						The compute machine set resource defines additional properties of the machines that it creates. The compute machine set also references the infrastructure resource and machine template when creating machines.
					</p><pre class="programlisting language-yaml">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineSet
metadata:
  name: &lt;machine_set_name&gt; <span id="CO114-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-cluster-api
spec:
  clusterName: &lt;cluster_name&gt; <span id="CO114-2"><!--Empty--></span><span class="callout">2</span>
  replicas: 1
  selector:
    matchLabels:
      test: test
  template:
    metadata:
      labels:
        test: test
    spec:
      bootstrap:
         dataSecretName: worker-user-data <span id="CO114-3"><!--Empty--></span><span class="callout">3</span>
      clusterName: &lt;cluster_name&gt; <span id="CO114-4"><!--Empty--></span><span class="callout">4</span>
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: GCPMachineTemplate <span id="CO114-5"><!--Empty--></span><span class="callout">5</span>
        name: &lt;machine_set_name&gt; <span id="CO114-6"><!--Empty--></span><span class="callout">6</span>
      failureDomain: &lt;failure_domain&gt; <span id="CO114-7"><!--Empty--></span><span class="callout">7</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO114-1"><span class="callout">1</span></a> <a href="#CO114-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								Specify a name for the compute machine set.
							</div></dd><dt><a href="#CO114-2"><span class="callout">2</span></a> <a href="#CO114-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Specify the name of the cluster.
							</div></dd><dt><a href="#CO114-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								For the Cluster API Technology Preview, the Operator can use the worker user data secret from <code class="literal">openshift-machine-api</code> namespace.
							</div></dd><dt><a href="#CO114-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Specify the machine template kind. This value must match the value for your platform.
							</div></dd><dt><a href="#CO114-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								Specify the failure domain within the GCP region.
							</div></dd></dl></div></section></section></section><section class="section" id="capi-machine-set-creating_capi-machine-management"><div class="titlepage"><div><div><h2 class="title">11.3. Creating a Cluster API compute machine set</h2></div></div></div><p>
				You can create compute machine sets that use the Cluster API to dynamically manage the machine compute resources for specific workloads of your choice.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Deploy an OpenShift Container Platform cluster.
					</li><li class="listitem">
						Enable the use of the Cluster API.
					</li><li class="listitem">
						Install the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a YAML file that contains the cluster custom resource (CR) and is named <code class="literal">&lt;cluster_resource_file&gt;.yaml</code>.
					</p><p class="simpara">
						If you are not sure which value to set for the <code class="literal">&lt;cluster_name&gt;</code> parameter, you can check the value for an existing Machine API compute machine set in your cluster.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								To list the Machine API compute machine sets, run the following command:
							</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api <span id="CO115-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO115-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify the <code class="literal">openshift-machine-api</code> namespace.
									</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								To display the contents of a specific compute machine set CR, run the following command:
							</p><pre class="programlisting language-terminal">$ oc get machineset &lt;machineset_name&gt; \
-n openshift-machine-api \
-o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-yaml">...
template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: agl030519-vplxk <span id="CO116-1"><!--Empty--></span><span class="callout">1</span>
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: agl030519-vplxk-worker-us-east-1a
...</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO116-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The cluster ID, which you use for the <code class="literal">&lt;cluster_name&gt;</code> parameter.
									</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
						Create the cluster CR by running the following command:
					</p><pre class="programlisting language-terminal">$ oc create -f &lt;cluster_resource_file&gt;.yaml</pre><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
							To confirm that the cluster CR is created, run the following command:
						</p></div><pre class="programlisting language-terminal">$ oc get cluster</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME           PHASE        AGE  VERSION
&lt;cluster_name&gt; Provisioning 4h6m</pre>

						</p></div></li><li class="listitem">
						Create a YAML file that contains the infrastructure CR and is named <code class="literal">&lt;infrastructure_resource_file&gt;.yaml</code>.
					</li><li class="listitem"><p class="simpara">
						Create the infrastructure CR by running the following command:
					</p><pre class="programlisting language-terminal">$ oc create -f &lt;infrastructure_resource_file&gt;.yaml</pre><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
							To confirm that the infrastructure CR is created, run the following command:
						</p></div><pre class="programlisting language-terminal">$ oc get &lt;infrastructure_kind&gt;</pre><p class="simpara">
						where <code class="literal">&lt;infrastructure_kind&gt;</code> is the value that corresponds to your platform.
					</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME           CLUSTER        READY VPC BASTION IP
&lt;cluster_name&gt; &lt;cluster_name&gt; true</pre>

						</p></div></li><li class="listitem">
						Create a YAML file that contains the machine template CR and is named <code class="literal">&lt;machine_template_resource_file&gt;.yaml</code>.
					</li><li class="listitem"><p class="simpara">
						Create the machine template CR by running the following command:
					</p><pre class="programlisting language-terminal">$ oc create -f &lt;machine_template_resource_file&gt;.yaml</pre><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
							To confirm that the machine template CR is created, run the following command:
						</p></div><pre class="programlisting language-terminal">$ oc get &lt;machine_template_kind&gt;</pre><p class="simpara">
						where <code class="literal">&lt;machine_template_kind&gt;</code> is the value that corresponds to your platform.
					</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME            AGE
&lt;template_name&gt; 77m</pre>

						</p></div></li><li class="listitem">
						Create a YAML file that contains the compute machine set CR and is named <code class="literal">&lt;machine_set_resource_file&gt;.yaml</code>.
					</li><li class="listitem"><p class="simpara">
						Create the compute machine set CR by running the following command:
					</p><pre class="programlisting language-terminal">$ oc create -f &lt;machine_set_resource_file&gt;.yaml</pre><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
							To confirm that the compute machine set CR is created, run the following command:
						</p></div><pre class="programlisting language-terminal">$ oc get machineset -n openshift-cluster-api <span id="CO117-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO117-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specify the <code class="literal">openshift-cluster-api</code> namespace.
							</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME               CLUSTER        REPLICAS READY AVAILABLE AGE VERSION
&lt;machine_set_name&gt; &lt;cluster_name&gt; 1        1     1         17m</pre>

						</p></div><p class="simpara">
						When the new compute machine set is available, the <code class="literal">REPLICAS</code> and <code class="literal">AVAILABLE</code> values match. If the compute machine set is not available, wait a few minutes and run the command again.
					</p></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						To verify that the compute machine set is creating machines according to your desired configuration, you can review the lists of machines and nodes in the cluster.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
								To view the list of Cluster API machines, run the following command:
							</p><pre class="programlisting language-terminal">$ oc get machine -n openshift-cluster-api <span id="CO118-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO118-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify the <code class="literal">openshift-cluster-api</code> namespace.
									</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                           CLUSTER        NODENAME                               PROVIDERID    PHASE   AGE   VERSION
&lt;machine_set_name&gt;-&lt;string_id&gt; &lt;cluster_name&gt; &lt;ip_address&gt;.&lt;region&gt;.compute.internal &lt;provider_id&gt; Running 8m23s</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								To view the list of nodes, run the following command:
							</p><pre class="programlisting language-terminal">$ oc get node</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                                     STATUS ROLES  AGE   VERSION
&lt;ip_address_1&gt;.&lt;region&gt;.compute.internal Ready  worker 5h14m v1.26.0
&lt;ip_address_2&gt;.&lt;region&gt;.compute.internal Ready  master 5h19m v1.26.0
&lt;ip_address_3&gt;.&lt;region&gt;.compute.internal Ready  worker 7m    v1.26.0</pre>

								</p></div></li></ul></div></li></ul></div></section><section class="section" id="capi-troubleshooting_capi-machine-management"><div class="titlepage"><div><div><h2 class="title">11.4. Troubleshooting clusters that use the Cluster API</h2></div></div></div><p>
				Use the information in this section to understand and recover from issues you might encounter. Generally, troubleshooting steps for problems with the Cluster API are similar to those steps for problems with the Machine API.
			</p><p>
				The Cluster CAPI Operator and its operands are provisioned in the <code class="literal">openshift-cluster-api</code> namespace, whereas the Machine API uses the <code class="literal">openshift-machine-api</code> namespace. When using <code class="literal">oc</code> commands that reference a namespace, be sure to reference the correct one.
			</p><section class="section" id="ts-capi-cli_capi-machine-management"><div class="titlepage"><div><div><h3 class="title">11.4.1. CLI commands return Cluster API machines</h3></div></div></div><p>
					For clusters that use the Cluster API, <code class="literal">oc</code> commands such as <code class="literal">oc get machine</code> return results for Cluster API machines. Because the letter <code class="literal">c</code> precedes the letter <code class="literal">m</code> alphabetically, Cluster API machines appear in the return before Machine API machines do.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To list only Machine API machines, use the fully qualified name <code class="literal">machines.machine.openshift.io</code> when running the <code class="literal">oc get machine</code> command:
						</p><pre class="programlisting language-terminal">$ oc get machines.machine.openshift.io</pre></li><li class="listitem"><p class="simpara">
							To list only Cluster API machines, use the fully qualified name <code class="literal">machines.cluster.x-k8s.io</code> when running the <code class="literal">oc get machine</code> command:
						</p><pre class="programlisting language-terminal">$ oc get machines.cluster.x-k8s.io</pre></li></ul></div></section></section></section><section class="chapter" id="managing-control-plane-machines"><div class="titlepage"><div><div><h1 class="title">Chapter 12. Managing control plane machines</h1></div></div></div><section class="section" id="cpmso-about"><div class="titlepage"><div><div><h2 class="title">12.1. About control plane machine sets</h2></div></div></div><p>
				With control plane machine sets, you can automate management of the control plane machine resources within your OpenShift Container Platform cluster.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Control plane machine sets cannot manage compute machines, and compute machine sets cannot manage control plane machines.
				</p></div></div><p>
				Control plane machine sets provide for control plane machines similar management capabilities as compute machine sets provide for compute machines. However, these two types of machine sets are separate custom resources defined within the Machine API and have several fundamental differences in their architecture and functionality.
			</p><section class="section" id="cpmso-overview_cpmso-about"><div class="titlepage"><div><div><h3 class="title">12.1.1. Control Plane Machine Set Operator overview</h3></div></div></div><p>
					The Control Plane Machine Set Operator uses the <code class="literal">ControlPlaneMachineSet</code> custom resource (CR) to automate management of the control plane machine resources within your OpenShift Container Platform cluster.
				</p><p>
					When the state of the cluster control plane machine set is set to <code class="literal">Active</code>, the Operator ensures that the cluster has the correct number of control plane machines with the specified configuration. This allows the automated replacement of degraded control plane machines and rollout of changes to the control plane.
				</p><p>
					A cluster has only one control plane machine set, and the Operator only manages objects in the <code class="literal">openshift-machine-api</code> namespace.
				</p></section><section class="section" id="cpmso-limitations_cpmso-about"><div class="titlepage"><div><div><h3 class="title">12.1.2. Limitations</h3></div></div></div><p>
					The Control Plane Machine Set Operator has the following limitations:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The Operator requires the Machine API Operator to be operational and is therefore not supported on clusters with manually provisioned machines. When installing a OpenShift Container Platform cluster with manually provisioned machines for a platform that creates an active generated <code class="literal">ControlPlaneMachineSet</code> custom resource (CR), you must remove the Kubernetes manifest files that define the control plane machine set as instructed in the installation process.
						</li><li class="listitem">
							Only Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, and VMware vSphere clusters are supported.
						</li><li class="listitem">
							Only clusters with three control plane machines are supported.
						</li><li class="listitem">
							Horizontal scaling of the control plane is not supported.
						</li><li class="listitem">
							Deploying Azure control plane machines on Ephemeral OS disks increases risk for data loss and is not supported.
						</li><li class="listitem"><p class="simpara">
							Deploying control plane machines as AWS Spot Instances, GCP preemptible VMs, or Azure Spot VMs is not supported.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Attempting to deploy control plane machines as AWS Spot Instances, GCP preemptible VMs, or Azure Spot VMs might cause the cluster to lose etcd quorum. A cluster that loses all control plane machines simultaneously is unrecoverable.
							</p></div></div></li><li class="listitem">
							Making changes to the control plane machine set during or prior to installation is not supported. You must make any changes to the control plane machine set only after installation.
						</li></ul></div></section><section class="section _additional-resources" id="additional-resources_cpmso-about"><div class="titlepage"><div><div><h3 class="title">12.1.3. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#control-plane-machine-set-operator_cluster-operators-ref">Control Plane Machine Set Operator reference</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/api_reference/#controlplanemachineset-machine-openshift-io-v1"><code class="literal">ControlPlaneMachineSet</code> custom resource</a>
						</li></ul></div></section></section><section class="section" id="cpmso-getting-started"><div class="titlepage"><div><div><h2 class="title">12.2. Getting started with control plane machine sets</h2></div></div></div><p>
				The process for getting started with control plane machine sets depends on the state of the <code class="literal">ControlPlaneMachineSet</code> custom resource (CR) in your cluster.
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Clusters with an active generated CR</span></dt><dd>
							Clusters that have a generated CR with an active state use the control plane machine set by default. No administrator action is required.
						</dd><dt><span class="term">Clusters with an inactive generated CR</span></dt><dd>
							For clusters that include an inactive generated CR, you must review the CR configuration and <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-activating_cpmso-getting-started">activate the CR</a>.
						</dd><dt><span class="term">Clusters without a generated CR</span></dt><dd>
							For clusters that do not include a generated CR, you must <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-creating-cr_cpmso-getting-started">create and activate a CR</a> with the appropriate configuration for your cluster.
						</dd></dl></div><p>
				If you are uncertain about the state of the <code class="literal">ControlPlaneMachineSet</code> CR in your cluster, you can <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-checking-status_cpmso-getting-started">verify the CR status</a>.
			</p><section class="section" id="cpmso-platform-matrix_cpmso-getting-started"><div class="titlepage"><div><div><h3 class="title">12.2.1. Supported cloud providers</h3></div></div></div><p>
					In OpenShift Container Platform 4.13, the control plane machine sets are supported for Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, and VMware vSphere clusters.
				</p><p>
					The status of the control plane machine set after installation depends on your cloud provider and the version of OpenShift Container Platform that you installed on your cluster.
				</p><div class="table" id="idm140311140043712"><p class="title"><strong>Table 12.1. Control plane machine set implementation for OpenShift Container Platform 4.13</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 29%; " class="col_1"><!--Empty--></col><col style="width: 24%; " class="col_2"><!--Empty--></col><col style="width: 24%; " class="col_3"><!--Empty--></col><col style="width: 24%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140311140036960" scope="col">Cloud provider</th><th align="center" valign="middle" id="idm140311140035872" scope="col">Active by default</th><th align="center" valign="middle" id="idm140311140034784" scope="col">Generated CR</th><th align="center" valign="middle" id="idm140311140033696" scope="col">Manual CR required</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140311140036960"> <p>
									Amazon Web Services (AWS)
								</p>
								 </td><td align="center" valign="middle" headers="idm140311140035872"> <p>
									X <sup>[1]</sup>
								</p>
								 </td><td align="center" valign="middle" headers="idm140311140034784"> <p>
									X
								</p>
								 </td><td align="center" valign="middle" headers="idm140311140033696"> </td></tr><tr><td align="left" valign="middle" headers="idm140311140036960"> <p>
									Google Cloud Platform (GCP)
								</p>
								 </td><td align="center" valign="middle" headers="idm140311140035872"> <p>
									X <sup>[2]</sup>
								</p>
								 </td><td align="center" valign="middle" headers="idm140311140034784"> <p>
									X
								</p>
								 </td><td align="center" valign="middle" headers="idm140311140033696"> </td></tr><tr><td align="left" valign="middle" headers="idm140311140036960"> <p>
									Microsoft Azure
								</p>
								 </td><td align="center" valign="middle" headers="idm140311140035872"> <p>
									X <sup>[2]</sup>
								</p>
								 </td><td align="center" valign="middle" headers="idm140311140034784"> <p>
									X
								</p>
								 </td><td align="center" valign="middle" headers="idm140311140033696"> </td></tr><tr><td align="left" valign="middle" headers="idm140311140036960"> <p>
									VMware vSphere
								</p>
								 </td><td align="center" valign="middle" headers="idm140311140035872"> </td><td align="center" valign="middle" headers="idm140311140034784"> </td><td align="center" valign="middle" headers="idm140311140033696"> <p>
									X
								</p>
								 </td></tr></tbody></table></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							AWS clusters that are upgraded from version 4.11 or earlier require <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-activating_cpmso-getting-started">CR activation</a>.
						</li><li class="listitem">
							GCP and Azure clusters that are upgraded from version 4.12 or earlier require <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-activating_cpmso-getting-started">CR activation</a>.
						</li></ol></div></section><section class="section" id="cpmso-checking-status_cpmso-getting-started"><div class="titlepage"><div><div><h3 class="title">12.2.2. Checking the control plane machine set custom resource state</h3></div></div></div><p>
					You can verify the existence and state of the <code class="literal">ControlPlaneMachineSet</code> custom resource (CR).
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Determine the state of the CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get controlplanemachineset.machine.openshift.io cluster \
  --namespace openshift-machine-api</pre><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									A result of <code class="literal">Active</code> indicates that the <code class="literal">ControlPlaneMachineSet</code> CR exists and is activated. No administrator action is required.
								</li><li class="listitem">
									A result of <code class="literal">Inactive</code> indicates that a <code class="literal">ControlPlaneMachineSet</code> CR exists but is not activated.
								</li><li class="listitem">
									A result of <code class="literal">NotFound</code> indicates that there is no existing <code class="literal">ControlPlaneMachineSet</code> CR.
								</li></ul></div></li></ul></div><div class="formalpara"><p class="title"><strong>Next steps</strong></p><p>
						To use the control plane machine set, you must ensure that a <code class="literal">ControlPlaneMachineSet</code> CR with the correct settings for your cluster exists.
					</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							If your cluster has an existing CR, you must verify that the configuration in the CR is correct for your cluster.
						</li><li class="listitem">
							If your cluster does not have an existing CR, you must create one with the correct configuration for your cluster.
						</li></ul></div></section><section class="section" id="cpmso-activating_cpmso-getting-started"><div class="titlepage"><div><div><h3 class="title">12.2.3. Activating the control plane machine set custom resource</h3></div></div></div><p>
					To use the control plane machine set, you must ensure that a <code class="literal">ControlPlaneMachineSet</code> custom resource (CR) with the correct settings for your cluster exists. On a cluster with a generated CR, you must verify that the configuration in the CR is correct for your cluster and activate it.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						For more information about the parameters in the CR, see "Control plane machine set configuration".
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the configuration of the CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc --namespace openshift-machine-api edit controlplanemachineset.machine.openshift.io cluster</pre></li><li class="listitem">
							Change the values of any fields that are incorrect for your cluster configuration.
						</li><li class="listitem"><p class="simpara">
							When the configuration is correct, activate the CR by setting the <code class="literal">.spec.state</code> field to <code class="literal">Active</code> and saving your changes.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								To activate the CR, you must change the <code class="literal">.spec.state</code> field to <code class="literal">Active</code> in the same <code class="literal">oc edit</code> session that you use to update the CR configuration. If the CR is saved with the state left as <code class="literal">Inactive</code>, the control plane machine set generator resets the CR to its original settings.
							</p></div></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-configuration">Control Plane Machine Set Operator configuration</a>
						</li></ul></div></section><section class="section" id="cpmso-creating-cr_cpmso-getting-started"><div class="titlepage"><div><div><h3 class="title">12.2.4. Creating a control plane machine set custom resource</h3></div></div></div><p>
					To use the control plane machine set, you must ensure that a <code class="literal">ControlPlaneMachineSet</code> custom resource (CR) with the correct settings for your cluster exists. On a cluster without a generated CR, you must create the CR manually and activate it.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						For more information about the structure and parameters of the CR, see "Control plane machine set configuration".
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a YAML file using the following template:
						</p><div class="formalpara"><p class="title"><strong>Control plane machine set CR YAML file template</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1
kind: ControlPlaneMachineSet
metadata:
  name: cluster
  namespace: openshift-machine-api
spec:
  replicas: 3
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;cluster_id&gt; <span id="CO119-1"><!--Empty--></span><span class="callout">1</span>
      machine.openshift.io/cluster-api-machine-role: master
      machine.openshift.io/cluster-api-machine-type: master
  state: Active <span id="CO119-2"><!--Empty--></span><span class="callout">2</span>
  strategy:
    type: RollingUpdate <span id="CO119-3"><!--Empty--></span><span class="callout">3</span>
  template:
    machineType: machines_v1beta1_machine_openshift_io
    machines_v1beta1_machine_openshift_io:
      failureDomains:
        platform: &lt;platform&gt; <span id="CO119-4"><!--Empty--></span><span class="callout">4</span>
        &lt;platform_failure_domains&gt; <span id="CO119-5"><!--Empty--></span><span class="callout">5</span>
      metadata:
        labels:
          machine.openshift.io/cluster-api-cluster: &lt;cluster_id&gt; <span id="CO119-6"><!--Empty--></span><span class="callout">6</span>
          machine.openshift.io/cluster-api-machine-role: master
          machine.openshift.io/cluster-api-machine-type: master
      spec:
        providerSpec:
          value:
            &lt;platform_provider_spec&gt; <span id="CO119-7"><!--Empty--></span><span class="callout">7</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO119-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. You must specify this value when you create a <code class="literal">ControlPlaneMachineSet</code> CR. If you have the OpenShift CLI (<code class="literal">oc</code>) installed, you can obtain the infrastructure ID by running the following command:
								</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><a href="#CO119-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the state of the Operator. When the state is <code class="literal">Inactive</code>, the Operator is not operational. You can activate the Operator by setting the value to <code class="literal">Active</code>.
								</div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
										Before you activate the CR, you must ensure that its configuration is correct for your cluster requirements.
									</p></div></div></dd><dt><a href="#CO119-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify the update strategy for the cluster. The allowed values are <code class="literal">OnDelete</code> and <code class="literal">RollingUpdate</code>. The default value is <code class="literal">RollingUpdate</code>.
								</div></dd><dt><a href="#CO119-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specify your cloud provider platform name. The allowed values are <code class="literal">AWS</code>, <code class="literal">Azure</code>, <code class="literal">GCP</code>, and <code class="literal">VSphere</code>.
								</div></dd><dt><a href="#CO119-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Add the <code class="literal">&lt;platform_failure_domains&gt;</code> configuration for the cluster. The format and values of this section are provider-specific. For more information, see the sample failure domain configuration for your cloud provider.
								</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										VMware vSphere does not support failure domains. For vSphere clusters, replace <code class="literal">&lt;platform_failure_domains&gt;</code> with an empty <code class="literal">failureDomains:</code> parameter.
									</p></div></div></dd><dt><a href="#CO119-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Specify the infrastructure ID.
								</div></dd><dt><a href="#CO119-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									Add the <code class="literal">&lt;platform_provider_spec&gt;</code> configuration for the cluster. The format and values of this section are provider-specific. For more information, see the sample provider specification for your cloud provider.
								</div></dd></dl></div></li><li class="listitem">
							Refer to the sample YAML for a control plane machine set CR and populate your file with values that are appropriate for your cluster configuration.
						</li><li class="listitem">
							Refer to the sample failure domain configuration and sample provider specification for your cloud provider and update those sections of your file with the appropriate values.
						</li><li class="listitem">
							When the configuration is correct, activate the CR by setting the <code class="literal">.spec.state</code> field to <code class="literal">Active</code> and saving your changes.
						</li><li class="listitem"><p class="simpara">
							Create the CR from your YAML file by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;control_plane_machine_set&gt;.yaml</pre><p class="simpara">
							where <code class="literal">&lt;control_plane_machine_set&gt;</code> is the name of the YAML file that contains the CR configuration.
						</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-configuration">Control Plane Machine Set Operator configuration</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-sample-yaml-aws_cpmso-configuration">Sample YAML for configuring Amazon Web Services clusters</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-sample-yaml-gcp_cpmso-configuration">Sample YAML for configuring Google Cloud Platform clusters</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-sample-yaml-azure_cpmso-configuration">Sample YAML for configuring Microsoft Azure clusters</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-sample-yaml-vsphere_cpmso-configuration">Sample YAML for configuring VMware vSphere clusters</a>
						</li></ul></div></section></section><section class="section" id="cpmso-configuration"><div class="titlepage"><div><div><h2 class="title">12.3. Control plane machine set configuration</h2></div></div></div><p>
				These example YAML file and snippets demonstrate the base structure for a control plane machine set custom resource (CR) and platform-specific samples for provider specification and failure domain configurations.
			</p><section class="section" id="cpmso-yaml-sample-cr_cpmso-configuration"><div class="titlepage"><div><div><h3 class="title">12.3.1. Sample YAML for a control plane machine set custom resource</h3></div></div></div><p>
					The base of the <code class="literal">ControlPlaneMachineSet</code> CR is structured the same way for all platforms.
				</p><div class="formalpara"><p class="title"><strong>Sample <code class="literal">ControlPlaneMachineSet</code> CR YAML file</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1
kind: ControlPlaneMachineSet
metadata:
  name: cluster <span id="CO120-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-machine-api
spec:
  replicas: 3 <span id="CO120-2"><!--Empty--></span><span class="callout">2</span>
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;cluster_id&gt; <span id="CO120-3"><!--Empty--></span><span class="callout">3</span>
      machine.openshift.io/cluster-api-machine-role: master
      machine.openshift.io/cluster-api-machine-type: master
  state: Active <span id="CO120-4"><!--Empty--></span><span class="callout">4</span>
  strategy:
    type: RollingUpdate <span id="CO120-5"><!--Empty--></span><span class="callout">5</span>
  template:
    machineType: machines_v1beta1_machine_openshift_io
    machines_v1beta1_machine_openshift_io:
      failureDomains:
        platform: &lt;platform&gt; <span id="CO120-6"><!--Empty--></span><span class="callout">6</span>
        &lt;platform_failure_domains&gt; <span id="CO120-7"><!--Empty--></span><span class="callout">7</span>
      metadata:
        labels:
          machine.openshift.io/cluster-api-cluster: &lt;cluster_id&gt;
          machine.openshift.io/cluster-api-machine-role: master
          machine.openshift.io/cluster-api-machine-type: master
      spec:
        providerSpec:
          value:
            &lt;platform_provider_spec&gt; <span id="CO120-8"><!--Empty--></span><span class="callout">8</span></pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO120-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Specifies the name of the <code class="literal">ControlPlaneMachineSet</code> CR, which is <code class="literal">cluster</code>. Do not change this value.
						</div></dd><dt><a href="#CO120-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Specifies the number of control plane machines. Only clusters with three control plane machines are supported, so the <code class="literal">replicas</code> value is <code class="literal">3</code>. Horizontal scaling is not supported. Do not change this value.
						</div></dd><dt><a href="#CO120-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Specifies the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. You must specify this value when you create a <code class="literal">ControlPlaneMachineSet</code> CR. If you have the OpenShift CLI (<code class="literal">oc</code>) installed, you can obtain the infrastructure ID by running the following command:
						</div><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><a href="#CO120-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Specifies the state of the Operator. When the state is <code class="literal">Inactive</code>, the Operator is not operational. You can activate the Operator by setting the value to <code class="literal">Active</code>.
						</div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Before you activate the Operator, you must ensure that the <code class="literal">ControlPlaneMachineSet</code> CR configuration is correct for your cluster requirements. For more information about activating the Control Plane Machine Set Operator, see "Getting started with control plane machine sets".
							</p></div></div></dd><dt><a href="#CO120-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Specifies the update strategy for the cluster. The allowed values are <code class="literal">OnDelete</code> and <code class="literal">RollingUpdate</code>. The default value is <code class="literal">RollingUpdate</code>. For more information about update strategies, see "Updating the control plane configuration".
						</div></dd><dt><a href="#CO120-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							Specifies the cloud provider platform name. Do not change this value.
						</div></dd><dt><a href="#CO120-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							Specifies the <code class="literal">&lt;platform_failure_domains&gt;</code> configuration for the cluster. The format and values of this section are provider-specific. For more information, see the sample failure domain configuration for your cloud provider.
						</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								VMware vSphere does not support failure domains.
							</p></div></div></dd><dt><a href="#CO120-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							Specifies the <code class="literal">&lt;platform_provider_spec&gt;</code> configuration for the cluster. The format and values of this section are provider-specific. For more information, see the sample provider specification for your cloud provider.
						</div></dd></dl></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-getting-started">Getting started with control plane machine sets</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-feat-config-update_cpmso-using">Updating the control plane configuration</a>
						</li></ul></div><h5 id="cpmso-sample-yaml-provider-specific_cpmso-configuration">Provider-specific configuration</h5><p>
					The <code class="literal">&lt;platform_provider_spec&gt;</code> and <code class="literal">&lt;platform_failure_domains&gt;</code> sections of the control plane machine set resources are provider-specific. Refer to the example YAML for your cluster:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-sample-yaml-aws_cpmso-configuration">Sample YAML snippets for configuring Amazon Web Services clusters</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-sample-yaml-gcp_cpmso-configuration">Sample YAML snippets for configuring Google Cloud Platform clusters</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-sample-yaml-azure_cpmso-configuration">Sample YAML snippets for configuring Microsoft Azure clusters</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-sample-yaml-vsphere_cpmso-configuration">Sample YAML snippets for configuring VMware vSphere clusters</a>
						</li></ul></div></section><section class="section" id="cpmso-sample-yaml-aws_cpmso-configuration"><div class="titlepage"><div><div><h3 class="title">12.3.2. Sample YAML for configuring Amazon Web Services clusters</h3></div></div></div><p>
					Some sections of the control plane machine set CR are provider-specific. The example YAML in this section show provider specification and failure domain configurations for an Amazon Web Services (AWS) cluster.
				</p><section class="section" id="cpmso-yaml-provider-spec-aws_cpmso-configuration"><div class="titlepage"><div><div><h4 class="title">12.3.2.1. Sample AWS provider specification</h4></div></div></div><p>
						When you create a control plane machine set for an existing cluster, the provider specification must match the <code class="literal">providerSpec</code> configuration in the control plane <code class="literal">machine</code> CR that is created by the installation program. You can omit any field that is set in the failure domain section of the CR.
					</p><p>
						In the following example, <code class="literal">&lt;cluster_id&gt;</code> is the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre><div class="formalpara"><p class="title"><strong>Sample AWS <code class="literal">providerSpec</code> values</strong></p><p>
							
<pre class="programlisting language-yaml">providerSpec:
  value:
    ami:
      id: ami-&lt;ami_id_string&gt; <span id="CO121-1"><!--Empty--></span><span class="callout">1</span>
    apiVersion: machine.openshift.io/v1beta1
    blockDevices:
    - ebs: <span id="CO121-2"><!--Empty--></span><span class="callout">2</span>
        encrypted: true
        iops: 0
        kmsKey:
          arn: ""
        volumeSize: 120
        volumeType: gp3
    credentialsSecret:
      name: aws-cloud-credentials <span id="CO121-3"><!--Empty--></span><span class="callout">3</span>
    deviceIndex: 0
    iamInstanceProfile:
      id: &lt;cluster_id&gt;-master-profile <span id="CO121-4"><!--Empty--></span><span class="callout">4</span>
    instanceType: m6i.xlarge <span id="CO121-5"><!--Empty--></span><span class="callout">5</span>
    kind: AWSMachineProviderConfig <span id="CO121-6"><!--Empty--></span><span class="callout">6</span>
    loadBalancers: <span id="CO121-7"><!--Empty--></span><span class="callout">7</span>
    - name: &lt;cluster_id&gt;-int
      type: network
    - name: &lt;cluster_id&gt;-ext
      type: network
    metadata:
      creationTimestamp: null
    metadataServiceOptions: {}
    placement: <span id="CO121-8"><!--Empty--></span><span class="callout">8</span>
      region: &lt;region&gt; <span id="CO121-9"><!--Empty--></span><span class="callout">9</span>
    securityGroups:
    - filters:
      - name: tag:Name
        values:
        - &lt;cluster_id&gt;-master-sg <span id="CO121-10"><!--Empty--></span><span class="callout">10</span>
    subnet: {} <span id="CO121-11"><!--Empty--></span><span class="callout">11</span>
    userDataSecret:
      name: master-user-data <span id="CO121-12"><!--Empty--></span><span class="callout">12</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO121-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specifies the Red Hat Enterprise Linux CoreOS (RHCOS) Amazon Machine Images (AMI) ID for the cluster. The AMI must belong to the same region as the cluster. If you want to use an AWS Marketplace image, you must complete the OpenShift Container Platform subscription from the <a class="link" href="https://aws.amazon.com/marketplace/fulfillment?productId=59ead7de-2540-4653-a8b0-fa7926d5c845">AWS Marketplace</a> to obtain an AMI ID for your region.
							</div></dd><dt><a href="#CO121-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specifies the configuration of an encrypted EBS volume.
							</div></dd><dt><a href="#CO121-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specifies the secret name for the cluster. Do not change this value.
							</div></dd><dt><a href="#CO121-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Specifies the AWS Identity and Access Management (IAM) instance profile. Do not change this value.
							</div></dd><dt><a href="#CO121-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Specifies the AWS instance type for the control plane.
							</div></dd><dt><a href="#CO121-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								Specifies the cloud provider platform type. Do not change this value.
							</div></dd><dt><a href="#CO121-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								Specifies the internal (<code class="literal">int</code>) and external (<code class="literal">ext</code>) load balancers for the cluster.
							</div></dd><dt><a href="#CO121-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								This parameter is configured in the failure domain, and is shown with an empty value here. If a value specified for this parameter differs from the value in the failure domain, the Operator overwrites it with the value in the failure domain.
							</div></dd><dt><a href="#CO121-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								Specifies the AWS region for the cluster.
							</div></dd><dt><a href="#CO121-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								Specifies the control plane machines security group.
							</div></dd><dt><a href="#CO121-11"><span class="callout">11</span></a> </dt><dd><div class="para">
								This parameter is configured in the failure domain, and is shown with an empty value here. If a value specified for this parameter differs from the value in the failure domain, the Operator overwrites it with the value in the failure domain.
							</div></dd><dt><a href="#CO121-12"><span class="callout">12</span></a> </dt><dd><div class="para">
								Specifies the control plane user data secret. Do not change this value.
							</div></dd></dl></div></section><section class="section" id="cpmso-yaml-failure-domain-aws_cpmso-configuration"><div class="titlepage"><div><div><h4 class="title">12.3.2.2. Sample AWS failure domain configuration</h4></div></div></div><p>
						The control plane machine set concept of a failure domain is analogous to existing AWS concept of an <a class="link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-availability-zones"><span class="emphasis"><em>Availability Zone (AZ)</em></span></a>. The <code class="literal">ControlPlaneMachineSet</code> CR spreads control plane machines across multiple failure domains when possible.
					</p><p>
						When configuring AWS failure domains in the control plane machine set, you must specify the availability zone name and the subnet to use.
					</p><div class="formalpara"><p class="title"><strong>Sample AWS failure domain values</strong></p><p>
							
<pre class="programlisting language-yaml">failureDomains:
  aws:
  - placement:
      availabilityZone: &lt;aws_zone_a&gt; <span id="CO122-1"><!--Empty--></span><span class="callout">1</span>
    subnet: <span id="CO122-2"><!--Empty--></span><span class="callout">2</span>
      filters:
      - name: tag:Name
        values:
        - &lt;cluster_id&gt;-private-&lt;aws_zone_a&gt; <span id="CO122-3"><!--Empty--></span><span class="callout">3</span>
      type: Filters <span id="CO122-4"><!--Empty--></span><span class="callout">4</span>
  - placement:
      availabilityZone: &lt;aws_zone_b&gt; <span id="CO122-5"><!--Empty--></span><span class="callout">5</span>
    subnet:
      filters:
      - name: tag:Name
        values:
        - &lt;cluster_id&gt;-private-&lt;aws_zone_b&gt; <span id="CO122-6"><!--Empty--></span><span class="callout">6</span>
      type: Filters
  platform: AWS <span id="CO122-7"><!--Empty--></span><span class="callout">7</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO122-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specifies an AWS availability zone for the first failure domain.
							</div></dd><dt><a href="#CO122-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specifies a subnet configuration. In this example, the subnet type is <code class="literal">Filters</code>, so there is a <code class="literal">filters</code> stanza.
							</div></dd><dt><a href="#CO122-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specifies the subnet name for the first failure domain, using the infrastructure ID and the AWS availability zone.
							</div></dd><dt><a href="#CO122-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Specifies the subnet type. The allowed values are: <code class="literal">ARN</code>, <code class="literal">Filters</code> and <code class="literal">ID</code>. The default value is <code class="literal">Filters</code>.
							</div></dd><dt><a href="#CO122-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Specifies the subnet name for an additional failure domain, using the infrastructure ID and the AWS availability zone.
							</div></dd><dt><a href="#CO122-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								Specifies the cluster’s infrastructure ID and the AWS availability zone for the additional failure domain.
							</div></dd><dt><a href="#CO122-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								Specifies the cloud provider platform name. Do not change this value.
							</div></dd></dl></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-supported-features-aws_cpmso-using">Enabling Amazon Web Services features for control plane machines</a>
							</li></ul></div></section></section><section class="section" id="cpmso-sample-yaml-gcp_cpmso-configuration"><div class="titlepage"><div><div><h3 class="title">12.3.3. Sample YAML for configuring Google Cloud Platform clusters</h3></div></div></div><p>
					Some sections of the control plane machine set CR are provider-specific. The example YAML in this section show provider specification and failure domain configurations for a Google Cloud Platform (GCP) cluster.
				</p><section class="section" id="cpmso-yaml-provider-spec-gcp_cpmso-configuration"><div class="titlepage"><div><div><h4 class="title">12.3.3.1. Sample GCP provider specification</h4></div></div></div><p>
						When you create a control plane machine set for an existing cluster, the provider specification must match the <code class="literal">providerSpec</code> configuration in the control plane machine custom resource (CR) that is created by the installation program. You can omit any field that is set in the failure domain section of the CR.
					</p><h6 id="cpmso-yaml-provider-spec-gcp-oc_cpmso-configuration">Values obtained by using the OpenShift CLI</h6><p>
						In the following example, you can obtain some of the values for your cluster by using the OpenShift CLI.
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Infrastructure ID</span></dt><dd><p class="simpara">
									The <code class="literal">&lt;cluster_id&gt;</code> string is the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
								</p><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre></dd><dt><span class="term">Image path</span></dt><dd><p class="simpara">
									The <code class="literal">&lt;path_to_image&gt;</code> string is the path to the image that was used to create the disk. If you have the OpenShift CLI installed, you can obtain the path to the image by running the following command:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api \
  -o jsonpath='{.spec.template.machines_v1beta1_machine_openshift_io.spec.providerSpec.value.disks[0].image}{"\n"}' \
  get ControlPlaneMachineSet/cluster</pre></dd></dl></div><div class="formalpara"><p class="title"><strong>Sample GCP <code class="literal">providerSpec</code> values</strong></p><p>
							
<pre class="programlisting language-yaml">providerSpec:
  value:
    apiVersion: machine.openshift.io/v1beta1
    canIPForward: false
    credentialsSecret:
      name: gcp-cloud-credentials <span id="CO123-1"><!--Empty--></span><span class="callout">1</span>
    deletionProtection: false
    disks:
    - autoDelete: true
      boot: true
      image: &lt;path_to_image&gt; <span id="CO123-2"><!--Empty--></span><span class="callout">2</span>
      labels: null
      sizeGb: 200
      type: pd-ssd
    kind: GCPMachineProviderSpec <span id="CO123-3"><!--Empty--></span><span class="callout">3</span>
    machineType: e2-standard-4
    metadata:
      creationTimestamp: null
    metadataServiceOptions: {}
    networkInterfaces:
    - network: &lt;cluster_id&gt;-network
      subnetwork: &lt;cluster_id&gt;-master-subnet
    projectID: &lt;project_name&gt; <span id="CO123-4"><!--Empty--></span><span class="callout">4</span>
    region: &lt;region&gt; <span id="CO123-5"><!--Empty--></span><span class="callout">5</span>
    serviceAccounts:
    - email: &lt;cluster_id&gt;-m@&lt;project_name&gt;.iam.gserviceaccount.com
      scopes:
      - https://www.googleapis.com/auth/cloud-platform
    shieldedInstanceConfig: {}
    tags:
    - &lt;cluster_id&gt;-master
    targetPools:
    - &lt;cluster_id&gt;-api
    userDataSecret:
      name: master-user-data <span id="CO123-6"><!--Empty--></span><span class="callout">6</span>
    zone: "" <span id="CO123-7"><!--Empty--></span><span class="callout">7</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO123-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specifies the secret name for the cluster. Do not change this value.
							</div></dd><dt><a href="#CO123-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specifies the path to the image that was used to create the disk.
							</div><p>
								To use a GCP Marketplace image, specify the offer to use:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										OpenShift Container Platform: <code class="literal">https://www.googleapis.com/compute/v1/projects/redhat-marketplace-public/global/images/redhat-coreos-ocp-48-x86-64-202210040145</code>
									</li><li class="listitem">
										OpenShift Platform Plus: <code class="literal">https://www.googleapis.com/compute/v1/projects/redhat-marketplace-public/global/images/redhat-coreos-opp-48-x86-64-202206140145</code>
									</li><li class="listitem">
										OpenShift Kubernetes Engine: <code class="literal">https://www.googleapis.com/compute/v1/projects/redhat-marketplace-public/global/images/redhat-coreos-oke-48-x86-64-202206140145</code>
									</li></ul></div></dd><dt><a href="#CO123-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specifies the cloud provider platform type. Do not change this value.
							</div></dd><dt><a href="#CO123-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Specifies the name of the GCP project that you use for your cluster.
							</div></dd><dt><a href="#CO123-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Specifies the GCP region for the cluster.
							</div></dd><dt><a href="#CO123-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								Specifies the control plane user data secret. Do not change this value.
							</div></dd><dt><a href="#CO123-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								This parameter is configured in the failure domain, and is shown with an empty value here. If a value specified for this parameter differs from the value in the failure domain, the Operator overwrites it with the value in the failure domain.
							</div></dd></dl></div></section><section class="section" id="cpmso-yaml-failure-domain-gcp_cpmso-configuration"><div class="titlepage"><div><div><h4 class="title">12.3.3.2. Sample GCP failure domain configuration</h4></div></div></div><p>
						The control plane machine set concept of a failure domain is analogous to the existing GCP concept of a <a class="link" href="https://cloud.google.com/compute/docs/regions-zones"><span class="emphasis"><em>zone</em></span></a>. The <code class="literal">ControlPlaneMachineSet</code> CR spreads control plane machines across multiple failure domains when possible.
					</p><p>
						When configuring GCP failure domains in the control plane machine set, you must specify the zone name to use.
					</p><div class="formalpara"><p class="title"><strong>Sample GCP failure domain values</strong></p><p>
							
<pre class="programlisting language-yaml">failureDomains:
  gcp:
  - zone: &lt;gcp_zone_a&gt; <span id="CO124-1"><!--Empty--></span><span class="callout">1</span>
  - zone: &lt;gcp_zone_b&gt; <span id="CO124-2"><!--Empty--></span><span class="callout">2</span>
  - zone: &lt;gcp_zone_c&gt;
  - zone: &lt;gcp_zone_d&gt;
  platform: GCP <span id="CO124-3"><!--Empty--></span><span class="callout">3</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO124-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specifies a GCP zone for the first failure domain.
							</div></dd><dt><a href="#CO124-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specifies an additional failure domain. Further failure domains are added the same way.
							</div></dd><dt><a href="#CO124-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specifies the cloud provider platform name. Do not change this value.
							</div></dd></dl></div></section></section><section class="section" id="cpmso-sample-yaml-azure_cpmso-configuration"><div class="titlepage"><div><div><h3 class="title">12.3.4. Sample YAML for configuring Microsoft Azure clusters</h3></div></div></div><p>
					Some sections of the control plane machine set CR are provider-specific. The example YAML in this section show provider specification and failure domain configurations for an Azure cluster.
				</p><section class="section" id="cpmso-yaml-provider-spec-azure_cpmso-configuration"><div class="titlepage"><div><div><h4 class="title">12.3.4.1. Sample Azure provider specification</h4></div></div></div><p>
						When you create a control plane machine set for an existing cluster, the provider specification must match the <code class="literal">providerSpec</code> configuration in the control plane <code class="literal">Machine</code> CR that is created by the installation program. You can omit any field that is set in the failure domain section of the CR.
					</p><p>
						In the following example, <code class="literal">&lt;cluster_id&gt;</code> is the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</pre><div class="formalpara"><p class="title"><strong>Sample Azure <code class="literal">providerSpec</code> values</strong></p><p>
							
<pre class="programlisting language-yaml">providerSpec:
  value:
    acceleratedNetworking: true
    apiVersion: machine.openshift.io/v1beta1
    credentialsSecret:
      name: azure-cloud-credentials <span id="CO125-1"><!--Empty--></span><span class="callout">1</span>
      namespace: openshift-machine-api
    diagnostics: {}
    image: <span id="CO125-2"><!--Empty--></span><span class="callout">2</span>
      offer: ""
      publisher: ""
      resourceID: /resourceGroups/&lt;cluster_id&gt;-rg/providers/Microsoft.Compute/galleries/gallery_&lt;cluster_id&gt;/images/&lt;cluster_id&gt;-gen2/versions/412.86.20220930 <span id="CO125-3"><!--Empty--></span><span class="callout">3</span>
      sku: ""
      version: ""
    internalLoadBalancer: &lt;cluster_id&gt;-internal <span id="CO125-4"><!--Empty--></span><span class="callout">4</span>
    kind: AzureMachineProviderSpec <span id="CO125-5"><!--Empty--></span><span class="callout">5</span>
    location: &lt;region&gt; <span id="CO125-6"><!--Empty--></span><span class="callout">6</span>
    managedIdentity: &lt;cluster_id&gt;-identity
    metadata:
      creationTimestamp: null
      name: &lt;cluster_id&gt;
    networkResourceGroup: &lt;cluster_id&gt;-rg
    osDisk: <span id="CO125-7"><!--Empty--></span><span class="callout">7</span>
      diskSettings: {}
      diskSizeGB: 1024
      managedDisk:
        storageAccountType: Premium_LRS
      osType: Linux
    publicIP: false
    publicLoadBalancer: &lt;cluster_id&gt; <span id="CO125-8"><!--Empty--></span><span class="callout">8</span>
    resourceGroup: &lt;cluster_id&gt;-rg
    subnet: &lt;cluster_id&gt;-master-subnet <span id="CO125-9"><!--Empty--></span><span class="callout">9</span>
    userDataSecret:
      name: master-user-data <span id="CO125-10"><!--Empty--></span><span class="callout">10</span>
    vmSize: Standard_D8s_v3
    vnet: &lt;cluster_id&gt;-vnet
    zone: "" <span id="CO125-11"><!--Empty--></span><span class="callout">11</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO125-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specifies the secret name for the cluster. Do not change this value.
							</div></dd><dt><a href="#CO125-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specifies the image details for your control plane machine set.
							</div></dd><dt><a href="#CO125-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specifies an image that is compatible with your instance type. The Hyper-V generation V2 images created by the installation program have a <code class="literal">-gen2</code> suffix, while V1 images have the same name without the suffix.
							</div></dd><dt><a href="#CO125-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Specifies the internal load balancer for the control plane. This field might not be preconfigured but is required in both the <code class="literal">ControlPlaneMachineSet</code> and control plane <code class="literal">Machine</code> CRs.
							</div></dd><dt><a href="#CO125-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Specifies the cloud provider platform type. Do not change this value.
							</div></dd><dt><a href="#CO125-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								Specifies the region to place control plane machines on.
							</div></dd><dt><a href="#CO125-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								Specifies the disk configuration for the control plane.
							</div></dd><dt><a href="#CO125-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								Specifies the public load balancer for the control plane.
							</div></dd><dt><a href="#CO125-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								Specifies the subnet for the control plane.
							</div></dd><dt><a href="#CO125-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								Specifies the control plane user data secret. Do not change this value.
							</div></dd><dt><a href="#CO125-11"><span class="callout">11</span></a> </dt><dd><div class="para">
								This parameter is configured in the failure domain, and is shown with an empty value here. If a value specified for this parameter differs from the value in the failure domain, the Operator overwrites it with the value in the failure domain.
							</div></dd></dl></div></section><section class="section" id="cpmso-yaml-failure-domain-azure_cpmso-configuration"><div class="titlepage"><div><div><h4 class="title">12.3.4.2. Sample Azure failure domain configuration</h4></div></div></div><p>
						The control plane machine set concept of a failure domain is analogous to existing Azure concept of an <a class="link" href="https://learn.microsoft.com/en-us/azure/azure-web-pubsub/concept-availability-zones"><span class="emphasis"><em>Azure availability zone</em></span></a>. The <code class="literal">ControlPlaneMachineSet</code> CR spreads control plane machines across multiple failure domains when possible.
					</p><p>
						When configuring Azure failure domains in the control plane machine set, you must specify the availability zone name.
					</p><div class="formalpara"><p class="title"><strong>Sample Azure failure domain values</strong></p><p>
							
<pre class="programlisting language-yaml">failureDomains:
  azure: <span id="CO126-1"><!--Empty--></span><span class="callout">1</span>
  - zone: "1"
  - zone: "2"
  - zone: "3"
  platform: Azure <span id="CO126-2"><!--Empty--></span><span class="callout">2</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO126-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Each instance of <code class="literal">zone</code> specifies an Azure availability zone for a failure domain.
							</div></dd><dt><a href="#CO126-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specifies the cloud provider platform name. Do not change this value.
							</div></dd></dl></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-supported-features-azure_cpmso-using">Enabling Microsoft Azure features for control plane machines</a>
							</li></ul></div></section></section><section class="section" id="cpmso-sample-yaml-vsphere_cpmso-configuration"><div class="titlepage"><div><div><h3 class="title">12.3.5. Sample YAML for configuring VMware vSphere clusters</h3></div></div></div><p>
					Some sections of the control plane machine set CR are provider-specific. The example YAML in this section shows a provider specification configuration for a VMware vSphere cluster.
				</p><section class="section" id="cpmso-yaml-provider-spec-vsphere_cpmso-configuration"><div class="titlepage"><div><div><h4 class="title">12.3.5.1. Sample vSphere provider specification</h4></div></div></div><p>
						When you create a control plane machine set for an existing cluster, the provider specification must match the <code class="literal">providerSpec</code> configuration in the control plane <code class="literal">machine</code> CR that is created by the installation program.
					</p><div class="formalpara"><p class="title"><strong>Sample vSphere <code class="literal">providerSpec</code> values</strong></p><p>
							
<pre class="programlisting language-yaml">providerSpec:
  value:
    apiVersion: machine.openshift.io/v1beta1
    credentialsSecret:
      name: vsphere-cloud-credentials <span id="CO127-1"><!--Empty--></span><span class="callout">1</span>
    diskGiB: 120 <span id="CO127-2"><!--Empty--></span><span class="callout">2</span>
    kind: VSphereMachineProviderSpec <span id="CO127-3"><!--Empty--></span><span class="callout">3</span>
    memoryMiB: 16384 <span id="CO127-4"><!--Empty--></span><span class="callout">4</span>
    metadata:
      creationTimestamp: null
    network: <span id="CO127-5"><!--Empty--></span><span class="callout">5</span>
      devices:
      - networkName: &lt;vm_network_name&gt;
    numCPUs: 4 <span id="CO127-6"><!--Empty--></span><span class="callout">6</span>
    numCoresPerSocket: 4 <span id="CO127-7"><!--Empty--></span><span class="callout">7</span>
    snapshot: ""
    template: &lt;vm_template_name&gt; <span id="CO127-8"><!--Empty--></span><span class="callout">8</span>
    userDataSecret:
      name: master-user-data <span id="CO127-9"><!--Empty--></span><span class="callout">9</span>
    workspace:
      datacenter: &lt;vcenter_datacenter_name&gt; <span id="CO127-10"><!--Empty--></span><span class="callout">10</span>
      datastore: &lt;vcenter_datastore_name&gt; <span id="CO127-11"><!--Empty--></span><span class="callout">11</span>
      folder: &lt;path_to_vcenter_vm_folder&gt; <span id="CO127-12"><!--Empty--></span><span class="callout">12</span>
      resourcePool: &lt;vsphere_resource_pool&gt; <span id="CO127-13"><!--Empty--></span><span class="callout">13</span>
      server: &lt;vcenter_server_ip&gt; <span id="CO127-14"><!--Empty--></span><span class="callout">14</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO127-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specifies the secret name for the cluster. Do not change this value.
							</div></dd><dt><a href="#CO127-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specifies the VM disk size for the control plane machines.
							</div></dd><dt><a href="#CO127-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specifies the cloud provider platform type. Do not change this value.
							</div></dd><dt><a href="#CO127-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Specifies the memory allocated for the control plane machines.
							</div></dd><dt><a href="#CO127-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Specifies the network on which the control plane is deployed.
							</div></dd><dt><a href="#CO127-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								Specifies the number of CPUs allocated for the control plane machines.
							</div></dd><dt><a href="#CO127-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								Specifies the number of cores for each control plane CPU.
							</div></dd><dt><a href="#CO127-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								Specifies the vSphere VM template to use, such as <code class="literal">user-5ddjd-rhcos</code>.
							</div></dd><dt><a href="#CO127-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								Specifies the control plane user data secret. Do not change this value.
							</div></dd><dt><a href="#CO127-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								Specifies the vCenter Datacenter for the control plane.
							</div></dd><dt><a href="#CO127-11"><span class="callout">11</span></a> </dt><dd><div class="para">
								Specifies the vCenter Datastore for the control plane.
							</div></dd><dt><a href="#CO127-12"><span class="callout">12</span></a> </dt><dd><div class="para">
								Specifies the path to the vSphere VM folder in vCenter, such as <code class="literal">/dc1/vm/user-inst-5ddjd</code>.
							</div></dd><dt><a href="#CO127-13"><span class="callout">13</span></a> </dt><dd><div class="para">
								Specifies the vSphere resource pool for your VMs.
							</div></dd><dt><a href="#CO127-14"><span class="callout">14</span></a> </dt><dd><div class="para">
								Specifies the vCenter server IP or fully qualified domain name.
							</div></dd></dl></div></section></section></section><section class="section" id="cpmso-using"><div class="titlepage"><div><div><h2 class="title">12.4. Managing control plane machines with control plane machine sets</h2></div></div></div><p>
				Control plane machine sets automate several essential aspects of control plane management.
			</p><section class="section" id="cpmso-feat-replace_cpmso-using"><div class="titlepage"><div><div><h3 class="title">12.4.1. Replacing a control plane machine</h3></div></div></div><p>
					To replace a control plane machine in a cluster that has a control plane machine set, you delete the machine manually. The control plane machine set replaces the deleted machine with one using the specification in the control plane machine set custom resource (CR).
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							List the control plane machines in your cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machines \
  -l machine.openshift.io/cluster-api-machine-role==master \
  -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
							Delete a control plane machine by running the following command:
						</p><pre class="programlisting language-terminal">$ oc delete machine \
  -n openshift-machine-api \
  &lt;control_plane_machine_name&gt; <span id="CO128-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO128-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the name of the control plane machine to delete.
								</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If you delete multiple control plane machines, the control plane machine set replaces them according to the configured update strategy:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										For clusters that use the default <code class="literal">RollingUpdate</code> update strategy, the Operator replaces one machine at a time until each machine is replaced.
									</li><li class="listitem">
										For clusters that are configured to use the <code class="literal">OnDelete</code> update strategy, the Operator creates all of the required replacement machines simultaneously.
									</li></ul></div><p>
								Both strategies maintain etcd health during control plane machine replacement.
							</p></div></div></li></ol></div></section><section class="section" id="cpmso-feat-config-update_cpmso-using"><div class="titlepage"><div><div><h3 class="title">12.4.2. Updating the control plane configuration</h3></div></div></div><p>
					You can make changes to the configuration of the machines in the control plane by updating the specification in the control plane machine set custom resource (CR).
				</p><p>
					The Control Plane Machine Set Operator monitors the control plane machines and compares their configuration with the specification in the control plane machine set CR. When there is a discrepancy between the specification in the CR and the configuration of a control plane machine, the Operator marks that control plane machine for replacement.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						For more information about the parameters in the CR, see "Control plane machine set configuration".
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Your cluster has an activated and functioning Control Plane Machine Set Operator.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit your control plane machine set CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc edit controlplanemachineset.machine.openshift.io cluster \
  -n openshift-machine-api</pre></li><li class="listitem">
							Change the values of any fields that you want to update in your cluster configuration.
						</li><li class="listitem">
							Save your changes.
						</li></ol></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							For clusters that use the default <code class="literal">RollingUpdate</code> update strategy, the control plane machine set propagates changes to your control plane configuration automatically.
						</li><li class="listitem">
							For clusters that are configured to use the <code class="literal">OnDelete</code> update strategy, you must replace your control plane machines manually.
						</li></ul></div><section class="section" id="cpmso-feat-auto-update_cpmso-using"><div class="titlepage"><div><div><h4 class="title">12.4.2.1. Automatic updates to the control plane configuration</h4></div></div></div><p>
						The <code class="literal">RollingUpdate</code> update strategy automatically propagates changes to your control plane configuration. This update strategy is the default configuration for the control plane machine set.
					</p><p>
						For clusters that use the <code class="literal">RollingUpdate</code> update strategy, the Operator creates a replacement control plane machine with the configuration that is specified in the CR. When the replacement control plane machine is ready, the Operator deletes the control plane machine that is marked for replacement. The replacement machine then joins the control plane.
					</p><p>
						If multiple control plane machines are marked for replacement, the Operator protects etcd health during replacement by repeating this replacement process one machine at a time until it has replaced each machine.
					</p></section><section class="section" id="cpmso-feat-ondelete-update_cpmso-using"><div class="titlepage"><div><div><h4 class="title">12.4.2.2. Manual updates to the control plane configuration</h4></div></div></div><p>
						You can use the <code class="literal">OnDelete</code> update strategy to propagate changes to your control plane configuration by replacing machines manually. Manually replacing machines allows you to test changes to your configuration on a single machine before applying the changes more broadly.
					</p><p>
						For clusters that are configured to use the <code class="literal">OnDelete</code> update strategy, the Operator creates a replacement control plane machine when you delete an existing machine. When the replacement control plane machine is ready, the etcd Operator allows the existing machine to be deleted. The replacement machine then joins the control plane.
					</p><p>
						If multiple control plane machines are deleted, the Operator creates all of the required replacement machines simultaneously. The Operator maintains etcd health by preventing more than one machine being removed from the control plane at once.
					</p></section></section><section class="section" id="cpmso-supported-features-aws_cpmso-using"><div class="titlepage"><div><div><h3 class="title">12.4.3. Enabling Amazon Web Services features for control plane machines</h3></div></div></div><p>
					You can enable Amazon Web Services (AWS) features on control plane machines by changing the configuration of your control plane machine set. When you save an update to the control plane machine set, the Control Plane Machine Set Operator updates the control plane machines according to your configured update strategy.
				</p><section class="section" id="private-clusters-setting-api-private_cpmso-using-aws"><div class="titlepage"><div><div><h4 class="title">12.4.3.1. Restricting the API server to private</h4></div></div></div><p>
						After you deploy a cluster to Amazon Web Services (AWS), you can reconfigure the API server to use only the private zone.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Install the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								Have access to the web console as a user with <code class="literal">admin</code> privileges.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								In the web portal or console for your cloud provider, take the following actions:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Locate and delete the appropriate load balancer component:
									</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												For AWS, delete the external load balancer. The API DNS entry in the private zone already points to the internal load balancer, which uses an identical configuration, so you do not need to modify the internal load balancer.
											</li></ul></div></li><li class="listitem">
										Delete the <code class="literal">api.$clustername.$yourdomain</code> DNS entry in the public zone.
									</li></ol></div></li><li class="listitem"><p class="simpara">
								Remove the external load balancers by deleting the following lines in the control plane machine set custom resource:
							</p><pre class="programlisting language-yaml">providerSpec:
  value:
    loadBalancers:
    - name: lk4pj-ext <span id="CO129-1"><!--Empty--></span><span class="callout">1</span>
      type: network <span id="CO129-2"><!--Empty--></span><span class="callout">2</span>
    - name: lk4pj-int
      type: network</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO129-1"><span class="callout">1</span></a> <a href="#CO129-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Delete this line.
									</div></dd></dl></div></li></ol></div></section><section class="section" id="cpms-changing-aws-instance-type_cpmso-using"><div class="titlepage"><div><div><h4 class="title">12.4.3.2. Changing the Amazon Web Services instance type by using a control plane machine set</h4></div></div></div><p>
						You can change the Amazon Web Services (AWS) instance type that your control plane machines use by updating the specification in the control plane machine set custom resource (CR).
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Your AWS cluster uses a control plane machine set.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the following line under the <code class="literal">providerSpec</code> field:
							</p><pre class="programlisting language-yaml">providerSpec:
  value:
    ...
    instanceType: &lt;compatible_aws_instance_type&gt; <span id="CO130-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO130-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify a larger AWS instance type with the same base as the previous selection. For example, you can change <code class="literal">m6i.xlarge</code> to <code class="literal">m6i.2xlarge</code> or <code class="literal">m6i.4xlarge</code>.
									</div></dd></dl></div></li><li class="listitem">
								Save your changes.
							</li></ol></div></section><section class="section" id="machineset-imds-options_cpmso-using"><div class="titlepage"><div><div><h4 class="title">12.4.3.3. Machine set options for the Amazon EC2 Instance Metadata Service</h4></div></div></div><p>
						You can use machine sets to create machines that use a specific version of the Amazon EC2 Instance Metadata Service (IMDS). Machine sets can create machines that allow the use of both IMDSv1 and <a class="link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html">IMDSv2</a> or machines that require the use of IMDSv2.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Using IMDSv2 is only supported on AWS clusters that were created with OpenShift Container Platform version 4.7 or later.
						</p></div></div><p>
						To change the IMDS configuration for existing machines, edit the machine set YAML file that manages those machines.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Before configuring a machine set to create machines that require IMDSv2, ensure that any workloads that interact with the AWS metadata service support IMDSv2.
						</p></div></div><section class="section" id="machineset-creating-imds-options_cpmso-using"><div class="titlepage"><div><div><h5 class="title">12.4.3.3.1. Configuring IMDS by using machine sets</h5></div></div></div><p>
							You can specify whether to require the use of IMDSv2 by adding or editing the value of <code class="literal">metadataServiceOptions.authentication</code> in the machine set YAML file for your machines.
						</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
									To use IMDSv2, your AWS cluster must have been created with OpenShift Container Platform version 4.7 or later.
								</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Add or edit the following lines under the <code class="literal">providerSpec</code> field:
								</p><pre class="programlisting language-yaml">providerSpec:
  value:
    metadataServiceOptions:
      authentication: Required <span id="CO131-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO131-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											To require IMDSv2, set the parameter value to <code class="literal">Required</code>. To allow the use of both IMDSv1 and IMDSv2, set the parameter value to <code class="literal">Optional</code>. If no value is specified, both IMDSv1 and IMDSv2 are allowed.
										</div></dd></dl></div></li></ul></div></section></section><section class="section" id="machineset-dedicated-instance_cpmso-using"><div class="titlepage"><div><div><h4 class="title">12.4.3.4. Machine sets that deploy machines as Dedicated Instances</h4></div></div></div><p>
						You can create a machine set running on AWS that deploys machines as Dedicated Instances. Dedicated Instances run in a virtual private cloud (VPC) on hardware that is dedicated to a single customer. These Amazon EC2 instances are physically isolated at the host hardware level. The isolation of Dedicated Instances occurs even if the instances belong to different AWS accounts that are linked to a single payer account. However, other instances that are not dedicated can share hardware with Dedicated Instances if they belong to the same AWS account.
					</p><p>
						Instances with either public or dedicated tenancy are supported by the Machine API. Instances with public tenancy run on shared hardware. Public tenancy is the default tenancy. Instances with dedicated tenancy run on single-tenant hardware.
					</p><section class="section" id="machineset-creating-dedicated-instance_cpmso-using"><div class="titlepage"><div><div><h5 class="title">12.4.3.4.1. Creating Dedicated Instances by using machine sets</h5></div></div></div><p>
							You can run a machine that is backed by a Dedicated Instance by using Machine API integration. Set the <code class="literal">tenancy</code> field in your machine set YAML file to launch a Dedicated Instance on AWS.
						</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Specify a dedicated tenancy under the <code class="literal">providerSpec</code> field:
								</p><pre class="programlisting language-yaml">providerSpec:
  placement:
    tenancy: dedicated</pre></li></ul></div></section></section></section><section class="section" id="cpmso-supported-features-azure_cpmso-using"><div class="titlepage"><div><div><h3 class="title">12.4.4. Enabling Microsoft Azure features for control plane machines</h3></div></div></div><p>
					You can enable Microsoft Azure features on control plane machines by changing the configuration of your control plane machine set. When you save an update to the control plane machine set, the Control Plane Machine Set Operator updates the control plane machines according to your configured update strategy.
				</p><section class="section" id="private-clusters-setting-api-private_cpmso-using-azure"><div class="titlepage"><div><div><h4 class="title">12.4.4.1. Restricting the API server to private</h4></div></div></div><p>
						After you deploy a cluster to Microsoft Azure, you can reconfigure the API server to use only the private zone.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Install the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								Have access to the web console as a user with <code class="literal">admin</code> privileges.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								In the web portal or console for your cloud provider, take the following actions:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Locate and delete the appropriate load balancer component:
									</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												For Azure, delete the <code class="literal">api-internal</code> rule for the load balancer.
											</li></ul></div></li><li class="listitem">
										Delete the <code class="literal">api.$clustername.$yourdomain</code> DNS entry in the public zone.
									</li></ol></div></li><li class="listitem"><p class="simpara">
								Remove the external load balancers by deleting the following lines in the control plane machine set custom resource:
							</p><pre class="programlisting language-yaml">providerSpec:
  value:
    loadBalancers:
    - name: lk4pj-ext <span id="CO132-1"><!--Empty--></span><span class="callout">1</span>
      type: network <span id="CO132-2"><!--Empty--></span><span class="callout">2</span>
    - name: lk4pj-int
      type: network</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO132-1"><span class="callout">1</span></a> <a href="#CO132-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Delete this line.
									</div></dd></dl></div></li></ol></div></section><section class="section" id="installation-azure-marketplace-subscribe_cpmso-using"><div class="titlepage"><div><div><h4 class="title">12.4.4.2. Selecting an Azure Marketplace image</h4></div></div></div><p>
						You can create a machine set running on Azure that deploys machines that use the Azure Marketplace offering. To use this offering, you must first obtain the Azure Marketplace image. When obtaining your image, consider the following:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								While the images are the same, the Azure Marketplace publisher is different depending on your region. If you are located in North America, specify <code class="literal">redhat</code> as the publisher. If you are located in EMEA, specify <code class="literal">redhat-limited</code> as the publisher.
							</li><li class="listitem">
								The offer includes a <code class="literal">rh-ocp-worker</code> SKU and a <code class="literal">rh-ocp-worker-gen1</code> SKU. The <code class="literal">rh-ocp-worker</code> SKU represents a Hyper-V generation version 2 VM image. The default instance types used in OpenShift Container Platform are version 2 compatible. If you plan to use an instance type that is only version 1 compatible, use the image associated with the <code class="literal">rh-ocp-worker-gen1</code> SKU. The <code class="literal">rh-ocp-worker-gen1</code> SKU represents a Hyper-V version 1 VM image.
							</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Installing images with the Azure marketplace is not supported on clusters with 64-bit ARM instances.
						</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the Azure CLI client <code class="literal">(az)</code>.
							</li><li class="listitem">
								Your Azure account is entitled for the offer and you have logged into this account with the Azure CLI client.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Display all of the available OpenShift Container Platform images by running one of the following commands:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
										North America:
									</p><pre class="programlisting language-terminal">$  az vm image list --all --offer rh-ocp-worker --publisher redhat -o table</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">Offer          Publisher       Sku                 Urn                                                             Version
-------------  --------------  ------------------  --------------------------------------------------------------  --------------
rh-ocp-worker  RedHat          rh-ocp-worker       RedHat:rh-ocp-worker:rh-ocpworker:4.8.2021122100               4.8.2021122100
rh-ocp-worker  RedHat          rh-ocp-worker-gen1  RedHat:rh-ocp-worker:rh-ocp-worker-gen1:4.8.2021122100         4.8.2021122100</pre>

										</p></div></li><li class="listitem"><p class="simpara">
										EMEA:
									</p><pre class="programlisting language-terminal">$  az vm image list --all --offer rh-ocp-worker --publisher redhat-limited -o table</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">Offer          Publisher       Sku                 Urn                                                             Version
-------------  --------------  ------------------  --------------------------------------------------------------  --------------
rh-ocp-worker  redhat-limited  rh-ocp-worker       redhat-limited:rh-ocp-worker:rh-ocp-worker:4.8.2021122100       4.8.2021122100
rh-ocp-worker  redhat-limited  rh-ocp-worker-gen1  redhat-limited:rh-ocp-worker:rh-ocp-worker-gen1:4.8.2021122100  4.8.2021122100</pre>

										</p></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Regardless of the version of OpenShift Container Platform that you install, the correct version of the Azure Marketplace image to use is 4.8. If required, your VMs are automatically upgraded as part of the installation process.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Inspect the image for your offer by running one of the following commands:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
										North America:
									</p><pre class="programlisting language-terminal">$ az vm image show --urn redhat:rh-ocp-worker:rh-ocp-worker:&lt;version&gt;</pre></li><li class="listitem"><p class="simpara">
										EMEA:
									</p><pre class="programlisting language-terminal">$ az vm image show --urn redhat-limited:rh-ocp-worker:rh-ocp-worker:&lt;version&gt;</pre></li></ul></div></li><li class="listitem"><p class="simpara">
								Review the terms of the offer by running one of the following commands:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
										North America:
									</p><pre class="programlisting language-terminal">$ az vm image terms show --urn redhat:rh-ocp-worker:rh-ocp-worker:&lt;version&gt;</pre></li><li class="listitem"><p class="simpara">
										EMEA:
									</p><pre class="programlisting language-terminal">$ az vm image terms show --urn redhat-limited:rh-ocp-worker:rh-ocp-worker:&lt;version&gt;</pre></li></ul></div></li><li class="listitem"><p class="simpara">
								Accept the terms of the offering by running one of the following commands:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
										North America:
									</p><pre class="programlisting language-terminal">$ az vm image terms accept --urn redhat:rh-ocp-worker:rh-ocp-worker:&lt;version&gt;</pre></li><li class="listitem"><p class="simpara">
										EMEA:
									</p><pre class="programlisting language-terminal">$ az vm image terms accept --urn redhat-limited:rh-ocp-worker:rh-ocp-worker:&lt;version&gt;</pre></li></ul></div></li><li class="listitem">
								Record the image details of your offer, specifically the values for <code class="literal">publisher</code>, <code class="literal">offer</code>, <code class="literal">sku</code>, and <code class="literal">version</code>.
							</li><li class="listitem"><p class="simpara">
								Add the following parameters to the <code class="literal">providerSpec</code> section of your machine set YAML file using the image details for your offer:
							</p><div class="formalpara"><p class="title"><strong>Sample <code class="literal">providerSpec</code> image values for Azure Marketplace machines</strong></p><p>
									
<pre class="programlisting language-yaml">providerSpec:
  value:
    image:
      offer: rh-ocp-worker
      publisher: redhat
      resourceID: ""
      sku: rh-ocp-worker
      type: MarketplaceWithPlan
      version: 4.8.2021122100</pre>

								</p></div></li></ol></div></section><section class="section" id="machineset-azure-boot-diagnostics_cpmso-using"><div class="titlepage"><div><div><h4 class="title">12.4.4.3. Enabling Azure boot diagnostics</h4></div></div></div><p>
						You can enable boot diagnostics on Azure machines that your machine set creates.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Have an existing Microsoft Azure cluster.
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Add the <code class="literal">diagnostics</code> configuration that is applicable to your storage type to the <code class="literal">providerSpec</code> field in your machine set YAML file:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
										For an Azure Managed storage account:
									</p><pre class="programlisting language-yaml">providerSpec:
  diagnostics:
    boot:
      storageAccountType: AzureManaged <span id="CO133-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO133-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Specifies an Azure Managed storage account.
											</div></dd></dl></div></li><li class="listitem"><p class="simpara">
										For an Azure Unmanaged storage account:
									</p><pre class="programlisting language-yaml">providerSpec:
  diagnostics:
    boot:
      storageAccountType: CustomerManaged <span id="CO134-1"><!--Empty--></span><span class="callout">1</span>
      customerManaged:
        storageAccountURI: https://&lt;storage-account&gt;.blob.core.windows.net <span id="CO134-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO134-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Specifies an Azure Unmanaged storage account.
											</div></dd><dt><a href="#CO134-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Replace <code class="literal">&lt;storage-account&gt;</code> with the name of your storage account.
											</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											Only the Azure Blob Storage data service is supported.
										</p></div></div></li></ul></div></li></ul></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								On the Microsoft Azure portal, review the <span class="strong strong"><strong>Boot diagnostics</strong></span> page for a machine deployed by the machine set, and verify that you can see the serial logs for the machine.
							</li></ul></div></section><section class="section" id="machineset-azure-ultra-disk_cpmso-using"><div class="titlepage"><div><div><h4 class="title">12.4.4.4. Machine sets that deploy machines with ultra disks as data disks</h4></div></div></div><p>
						You can create a machine set running on Azure that deploys machines with ultra disks. Ultra disks are high-performance storage that are intended for use with the most demanding data workloads.
					</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://docs.microsoft.com/en-us/azure/virtual-machines/disks-types#ultra-disks">Microsoft Azure ultra disks documentation</a>
							</li></ul></div><section class="section" id="machineset-creating-azure-ultra-disk_cpmso-using"><div class="titlepage"><div><div><h5 class="title">12.4.4.4.1. Creating machines with ultra disks by using machine sets</h5></div></div></div><p>
							You can deploy machines with ultra disks on Azure by editing your machine set YAML file.
						</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
									Have an existing Microsoft Azure cluster.
								</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
									Create a custom secret in the <code class="literal">openshift-machine-api</code> namespace using the <code class="literal">master</code> data secret by running the following command:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api \
get secret &lt;role&gt;-user-data \ <span id="CO135-1"><!--Empty--></span><span class="callout">1</span>
--template='{{index .data.userData | base64decode}}' | jq &gt; userData.txt <span id="CO135-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO135-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Replace <code class="literal">&lt;role&gt;</code> with <code class="literal">master</code>.
										</div></dd><dt><a href="#CO135-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Specify <code class="literal">userData.txt</code> as the name of the new custom secret.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									In a text editor, open the <code class="literal">userData.txt</code> file and locate the final <code class="literal">}</code> character in the file.
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
											On the immediately preceding line, add a <code class="literal">,</code>.
										</li><li class="listitem"><p class="simpara">
											Create a new line after the <code class="literal">,</code> and add the following configuration details:
										</p><pre class="programlisting language-json">"storage": {
  "disks": [ <span id="CO136-1"><!--Empty--></span><span class="callout">1</span>
    {
      "device": "/dev/disk/azure/scsi1/lun0", <span id="CO136-2"><!--Empty--></span><span class="callout">2</span>
      "partitions": [ <span id="CO136-3"><!--Empty--></span><span class="callout">3</span>
        {
          "label": "lun0p1", <span id="CO136-4"><!--Empty--></span><span class="callout">4</span>
          "sizeMiB": 1024, <span id="CO136-5"><!--Empty--></span><span class="callout">5</span>
          "startMiB": 0
        }
      ]
    }
  ],
  "filesystems": [ <span id="CO136-6"><!--Empty--></span><span class="callout">6</span>
    {
      "device": "/dev/disk/by-partlabel/lun0p1",
      "format": "xfs",
      "path": "/var/lib/lun0p1"
    }
  ]
},
"systemd": {
  "units": [ <span id="CO136-7"><!--Empty--></span><span class="callout">7</span>
    {
      "contents": "[Unit]\nBefore=local-fs.target\n[Mount]\nWhere=/var/lib/lun0p1\nWhat=/dev/disk/by-partlabel/lun0p1\nOptions=defaults,pquota\n[Install]\nWantedBy=local-fs.target\n", <span id="CO136-8"><!--Empty--></span><span class="callout">8</span>
      "enabled": true,
      "name": "var-lib-lun0p1.mount"
    }
  ]
}</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO136-1"><span class="callout">1</span></a> </dt><dd><div class="para">
													The configuration details for the disk that you want to attach to a node as an ultra disk.
												</div></dd><dt><a href="#CO136-2"><span class="callout">2</span></a> </dt><dd><div class="para">
													Specify the <code class="literal">lun</code> value that is defined in the <code class="literal">dataDisks</code> stanza of the machine set you are using. For example, if the machine set contains <code class="literal">lun: 0</code>, specify <code class="literal">lun0</code>. You can initialize multiple data disks by specifying multiple <code class="literal">"disks"</code> entries in this configuration file. If you specify multiple <code class="literal">"disks"</code> entries, ensure that the <code class="literal">lun</code> value for each matches the value in the machine set.
												</div></dd><dt><a href="#CO136-3"><span class="callout">3</span></a> </dt><dd><div class="para">
													The configuration details for a new partition on the disk.
												</div></dd><dt><a href="#CO136-4"><span class="callout">4</span></a> </dt><dd><div class="para">
													Specify a label for the partition. You might find it helpful to use hierarchical names, such as <code class="literal">lun0p1</code> for the first partition of <code class="literal">lun0</code>.
												</div></dd><dt><a href="#CO136-5"><span class="callout">5</span></a> </dt><dd><div class="para">
													Specify the total size in MiB of the partition.
												</div></dd><dt><a href="#CO136-6"><span class="callout">6</span></a> </dt><dd><div class="para">
													Specify the filesystem to use when formatting a partition. Use the partition label to specify the partition.
												</div></dd><dt><a href="#CO136-7"><span class="callout">7</span></a> </dt><dd><div class="para">
													Specify a <code class="literal">systemd</code> unit to mount the partition at boot. Use the partition label to specify the partition. You can create multiple partitions by specifying multiple <code class="literal">"partitions"</code> entries in this configuration file. If you specify multiple <code class="literal">"partitions"</code> entries, you must specify a <code class="literal">systemd</code> unit for each.
												</div></dd><dt><a href="#CO136-8"><span class="callout">8</span></a> </dt><dd><div class="para">
													For <code class="literal">Where</code>, specify the value of <code class="literal">storage.filesystems.path</code>. For <code class="literal">What</code>, specify the value of <code class="literal">storage.filesystems.device</code>.
												</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
									Extract the disabling template value to a file called <code class="literal">disableTemplating.txt</code> by running the following command:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api get secret &lt;role&gt;-user-data \ <span id="CO137-1"><!--Empty--></span><span class="callout">1</span>
--template='{{index .data.disableTemplating | base64decode}}' | jq &gt; disableTemplating.txt</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO137-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Replace <code class="literal">&lt;role&gt;</code> with <code class="literal">master</code>.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Combine the <code class="literal">userData.txt</code> file and <code class="literal">disableTemplating.txt</code> file to create a data secret file by running the following command:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api create secret generic &lt;role&gt;-user-data-x5 \ <span id="CO138-1"><!--Empty--></span><span class="callout">1</span>
--from-file=userData=userData.txt \
--from-file=disableTemplating=disableTemplating.txt</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO138-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											For <code class="literal">&lt;role&gt;-user-data-x5</code>, specify the name of the secret. Replace <code class="literal">&lt;role&gt;</code> with <code class="literal">master</code>.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Edit your control plane machine set CR by running the following command:
								</p><pre class="programlisting language-terminal">$ oc --namespace openshift-machine-api edit controlplanemachineset.machine.openshift.io cluster</pre></li><li class="listitem"><p class="simpara">
									Add the following lines in the positions indicated:
								</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: ControlPlaneMachineSet
spec:
  template:
    spec:
      metadata:
        labels:
          disk: ultrassd <span id="CO139-1"><!--Empty--></span><span class="callout">1</span>
      providerSpec:
        value:
          ultraSSDCapability: Enabled <span id="CO139-2"><!--Empty--></span><span class="callout">2</span>
          dataDisks: <span id="CO139-3"><!--Empty--></span><span class="callout">3</span>
          - nameSuffix: ultrassd
            lun: 0
            diskSizeGB: 4
            deletionPolicy: Delete
            cachingType: None
            managedDisk:
              storageAccountType: UltraSSD_LRS
          userDataSecret:
            name: &lt;role&gt;-user-data-x5 <span id="CO139-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO139-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify a label to use to select a node that is created by this machine set. This procedure uses <code class="literal">disk.ultrassd</code> for this value.
										</div></dd><dt><a href="#CO139-2"><span class="callout">2</span></a> <a href="#CO139-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											These lines enable the use of ultra disks. For <code class="literal">dataDisks</code>, include the entire stanza.
										</div></dd><dt><a href="#CO139-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Specify the user data secret created earlier. Replace <code class="literal">&lt;role&gt;</code> with <code class="literal">master</code>.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Save your changes.
								</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											For clusters that use the default <code class="literal">RollingUpdate</code> update strategy, the Operator automatically propagates the changes to your control plane configuration.
										</li><li class="listitem">
											For clusters that are configured to use the <code class="literal">OnDelete</code> update strategy, you must replace your control plane machines manually.
										</li></ul></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
									Validate that the machines are created by running the following command:
								</p><pre class="programlisting language-terminal">$ oc get machines</pre><p class="simpara">
									The machines should be in the <code class="literal">Running</code> state.
								</p></li><li class="listitem"><p class="simpara">
									For a machine that is running and has a node attached, validate the partition by running the following command:
								</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node-name&gt; -- chroot /host lsblk</pre><p class="simpara">
									In this command, <code class="literal">oc debug node/&lt;node-name&gt;</code> starts a debugging shell on the node <code class="literal">&lt;node-name&gt;</code> and passes a command with <code class="literal">--</code>. The passed command <code class="literal">chroot /host</code> provides access to the underlying host OS binaries, and <code class="literal">lsblk</code> shows the block devices that are attached to the host OS machine.
								</p></li></ol></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
									To use an ultra disk on the control plane, reconfigure your workload to use the control plane’s ultra disk mount point.
								</li></ul></div></section><section class="section" id="machineset-troubleshooting-azure-ultra-disk_cpmso-using"><div class="titlepage"><div><div><h5 class="title">12.4.4.4.2. Troubleshooting resources for machine sets that enable ultra disks</h5></div></div></div><p>
							Use the information in this section to understand and recover from issues you might encounter.
						</p><section class="section" id="ts-mapi-attach-misconfigure_cpmso-using"><div class="titlepage"><div><div><h6 class="title">12.4.4.4.2.1. Incorrect ultra disk configuration</h6></div></div></div><p>
								If an incorrect configuration of the <code class="literal">ultraSSDCapability</code> parameter is specified in the machine set, the machine provisioning fails.
							</p><p>
								For example, if the <code class="literal">ultraSSDCapability</code> parameter is set to <code class="literal">Disabled</code>, but an ultra disk is specified in the <code class="literal">dataDisks</code> parameter, the following error message appears:
							</p><pre class="programlisting language-terminal">StorageAccountType UltraSSD_LRS can be used only when additionalCapabilities.ultraSSDEnabled is set.</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										To resolve this issue, verify that your machine set configuration is correct.
									</li></ul></div></section><section class="section" id="ts-mapi-attach-unsupported_cpmso-using"><div class="titlepage"><div><div><h6 class="title">12.4.4.4.2.2. Unsupported disk parameters</h6></div></div></div><p>
								If a region, availability zone, or instance size that is not compatible with ultra disks is specified in the machine set, the machine provisioning fails. Check the logs for the following error message:
							</p><pre class="programlisting language-terminal">failed to create vm &lt;machine_name&gt;: failure sending request for machine &lt;machine_name&gt;: cannot create vm: compute.VirtualMachinesClient#CreateOrUpdate: Failure sending request: StatusCode=400 -- Original Error: Code="BadRequest" Message="Storage Account type 'UltraSSD_LRS' is not supported &lt;more_information_about_why&gt;."</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										To resolve this issue, verify that you are using this feature in a supported environment and that your machine set configuration is correct.
									</li></ul></div></section><section class="section" id="ts-mapi-delete_cpmso-using"><div class="titlepage"><div><div><h6 class="title">12.4.4.4.2.3. Unable to delete disks</h6></div></div></div><p>
								If the deletion of ultra disks as data disks is not working as expected, the machines are deleted and the data disks are orphaned. You must delete the orphaned disks manually if desired.
							</p></section></section></section><section class="section" id="machineset-enabling-customer-managed-encryption-azure_cpmso-using"><div class="titlepage"><div><div><h4 class="title">12.4.4.5. Enabling customer-managed encryption keys for a machine set</h4></div></div></div><p>
						You can supply an encryption key to Azure to encrypt data on managed disks at rest. You can enable server-side encryption with customer-managed keys by using the Machine API.
					</p><p>
						An Azure Key Vault, a disk encryption set, and an encryption key are required to use a customer-managed key. The disk encryption set must be in a resource group where the Cloud Credential Operator (CCO) has granted permissions. If not, an additional reader role is required to be granted on the disk encryption set.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								<a class="link" href="https://docs.microsoft.com/en-us/azure/aks/azure-disk-customer-managed-keys#create-an-azure-key-vault-instance">Create an Azure Key Vault instance</a>.
							</li><li class="listitem">
								<a class="link" href="https://docs.microsoft.com/en-us/azure/aks/azure-disk-customer-managed-keys#create-an-instance-of-a-diskencryptionset">Create an instance of a disk encryption set</a>.
							</li><li class="listitem">
								<a class="link" href="https://docs.microsoft.com/en-us/azure/aks/azure-disk-customer-managed-keys#grant-the-diskencryptionset-access-to-key-vault">Grant the disk encryption set access to key vault</a>.
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Configure the disk encryption set under the <code class="literal">providerSpec</code> field in your machine set YAML file. For example:
							</p><pre class="programlisting language-yaml">providerSpec:
  value:
    osDisk:
      diskSizeGB: 128
      managedDisk:
        diskEncryptionSet:
          id: /subscriptions/&lt;subscription_id&gt;/resourceGroups/&lt;resource_group_name&gt;/providers/Microsoft.Compute/diskEncryptionSets/&lt;disk_encryption_set_name&gt;
        storageAccountType: Premium_LRS</pre></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://docs.microsoft.com/en-us/azure/virtual-machines/disk-encryption#customer-managed-keys">Azure documentation about customer-managed keys</a>
							</li></ul></div></section><section class="section" id="machineset-azure-accelerated-networking_cpmso-using"><div class="titlepage"><div><div><h4 class="title">12.4.4.6. Accelerated Networking for Microsoft Azure VMs</h4></div></div></div><p>
						Accelerated Networking uses single root I/O virtualization (SR-IOV) to provide Microsoft Azure VMs with a more direct path to the switch. This enhances network performance. This feature can be enabled after installation.
					</p><section class="section" id="machineset-azure-accelerated-networking-limits_cpmso-using"><div class="titlepage"><div><div><h5 class="title">12.4.4.6.1. Limitations</h5></div></div></div><p>
							Consider the following limitations when deciding whether to use Accelerated Networking:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Accelerated Networking is only supported on clusters where the Machine API is operational.
								</li><li class="listitem"><p class="simpara">

								</p><p class="simpara">
									Accelerated Networking requires an Azure VM size that includes at least four vCPUs. To satisfy this requirement, you can change the value of <code class="literal">vmSize</code> in your machine set. For information about Azure VM sizes, see <a class="link" href="https://docs.microsoft.com/en-us/azure/virtual-machines/sizes">Microsoft Azure documentation</a>.
								</p></li></ul></div></section><section class="section" id="machineset-azure-enabling-accelerated-networking-existing_cpmso-using"><div class="titlepage"><div><div><h5 class="title">12.4.4.6.2. Enabling Accelerated Networking on an existing Microsoft Azure cluster</h5></div></div></div><p>
							You can enable Accelerated Networking on Azure by adding <code class="literal">acceleratedNetworking</code> to your machine set YAML file.
						</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
									Have an existing Microsoft Azure cluster where the Machine API is operational.
								</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Add the following to the <code class="literal">providerSpec</code> field:
								</p><pre class="programlisting language-yaml">providerSpec:
  value:
    acceleratedNetworking: true <span id="CO140-1"><!--Empty--></span><span class="callout">1</span>
    vmSize: &lt;azure-vm-size&gt; <span id="CO140-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO140-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											This line enables Accelerated Networking.
										</div></dd><dt><a href="#CO140-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Specify an Azure VM size that includes at least four vCPUs. For information about VM sizes, see <a class="link" href="https://docs.microsoft.com/en-us/azure/virtual-machines/sizes">Microsoft Azure documentation</a>.
										</div></dd></dl></div></li></ul></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
									On the Microsoft Azure portal, review the <span class="strong strong"><strong>Networking</strong></span> settings page for a machine provisioned by the machine set, and verify that the <code class="literal">Accelerated networking</code> field is set to <code class="literal">Enabled</code>.
								</li></ul></div></section></section></section><section class="section" id="cpmso-supported-features-gcp_cpmso-using"><div class="titlepage"><div><div><h3 class="title">12.4.5. Enabling Google Cloud Platform features for control plane machines</h3></div></div></div><p>
					You can enable Google Cloud Platform (GCP) features on control plane machines by changing the configuration of your control plane machine set. When you save an update to the control plane machine set, the Control Plane Machine Set Operator updates the control plane machines according to your configured update strategy.
				</p><section class="section" id="machineset-gcp-pd-disk-types_cpmso-using"><div class="titlepage"><div><div><h4 class="title">12.4.5.1. Configuring persistent disk types by using machine sets</h4></div></div></div><p>
						You can configure the type of persistent disk that a machine set deploys machines on by editing the machine set YAML file.
					</p><p>
						For more information about persistent disk types, compatibility, regional availability, and limitations, see the GCP Compute Engine documentation about <a class="link" href="https://cloud.google.com/compute/docs/disks#pdspecs">persistent disks</a>.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								In a text editor, open the YAML file for an existing machine set or create a new one.
							</li><li class="listitem"><p class="simpara">
								Edit the following line under the <code class="literal">providerSpec</code> field:
							</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1
kind: ControlPlaneMachineSet
...
spec:
  template:
    spec:
      providerSpec:
        value:
          disks:
            type: &lt;pd-disk-type&gt; <span id="CO141-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO141-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify the disk persistent type. Valid values are <code class="literal">pd-ssd</code>, <code class="literal">pd-standard</code>, and <code class="literal">pd-balanced</code>. The default value is <code class="literal">pd-standard</code>.
									</div></dd></dl></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Using the Google Cloud console, review the details for a machine deployed by the machine set and verify that the <code class="literal">Type</code> field matches the configured disk type.
							</li></ul></div></section><section class="section" id="machineset-gcp-confidential-vm_cpmso-using"><div class="titlepage"><div><div><h4 class="title">12.4.5.2. Configuring Confidential VM by using machine sets</h4></div></div></div><p>
						By editing the machine set YAML file, you can configure the Confidential VM options that a machine set uses for machines that it deploys.
					</p><p>
						For more information about Confidential Compute features, functionality, and compatibility, see the GCP Compute Engine documentation about <a class="link" href="https://cloud.google.com/compute/confidential-vm/docs/about-cvm">Confidential VM</a>.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Confidential Computing is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
						</p><p>
							For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
						</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								In a text editor, open the YAML file for an existing machine set or create a new one.
							</li><li class="listitem"><p class="simpara">
								Edit the following section under the <code class="literal">providerSpec</code> field:
							</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1
kind: ControlPlaneMachineSet
...
spec:
  template:
    spec:
      providerSpec:
        value:
          confidentialCompute: Enabled <span id="CO142-1"><!--Empty--></span><span class="callout">1</span>
          onHostMaintenance: Terminate <span id="CO142-2"><!--Empty--></span><span class="callout">2</span>
          machineType: n2d-standard-8 <span id="CO142-3"><!--Empty--></span><span class="callout">3</span>
...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO142-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify whether Confidential VM is enabled. Valid values are <code class="literal">Disabled</code> or <code class="literal">Enabled</code>.
									</div></dd><dt><a href="#CO142-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specify the behavior of the VM during a host maintenance event, such as a hardware or software update. For a machine that uses Confidential VM, this value must be set to <code class="literal">Terminate</code>, which stops the VM. Confidential VM does not support live VM migration.
									</div></dd><dt><a href="#CO142-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Specify a machine type that supports Confidential VM. Confidential VM supports the N2D and C2D series of machine types.
									</div></dd></dl></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								On the Google Cloud console, review the details for a machine deployed by the machine set and verify that the Confidential VM options match the values that you configured.
							</li></ul></div></section><section class="section" id="machineset-gcp-shielded-vms_cpmso-using"><div class="titlepage"><div><div><h4 class="title">12.4.5.3. Configuring Shielded VM options by using machine sets</h4></div></div></div><p>
						By editing the machine set YAML file, you can configure the Shielded VM options that a machine set uses for machines that it deploys.
					</p><p>
						For more information about Shielded VM features and functionality, see the GCP Compute Engine documentation about <a class="link" href="https://cloud.google.com/compute/shielded-vm/docs/shielded-vm">Shielded VM</a>.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								In a text editor, open the YAML file for an existing machine set or create a new one.
							</li><li class="listitem"><p class="simpara">
								Edit the following section under the <code class="literal">providerSpec</code> field:
							</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1
kind: ControlPlaneMachineSet
...
spec:
  template:
    spec:
      providerSpec:
        value:
          shieldedInstanceConfig: <span id="CO143-1"><!--Empty--></span><span class="callout">1</span>
            integrityMonitoring: Enabled <span id="CO143-2"><!--Empty--></span><span class="callout">2</span>
            secureBoot: Disabled <span id="CO143-3"><!--Empty--></span><span class="callout">3</span>
            virtualizedTrustedPlatformModule: Enabled <span id="CO143-4"><!--Empty--></span><span class="callout">4</span>
...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO143-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										In this section, specify any Shielded VM options that you want.
									</div></dd><dt><a href="#CO143-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specify whether UEFI Secure Boot is enabled. Valid values are <code class="literal">Disabled</code> or <code class="literal">Enabled</code>.
									</div></dd><dt><a href="#CO143-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Specify whether integrity monitoring is enabled. Valid values are <code class="literal">Disabled</code> or <code class="literal">Enabled</code>.
									</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											When integrity monitoring is enabled, you must not disable virtual trusted platform module (vTPM).
										</p></div></div></dd><dt><a href="#CO143-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										Specify whether vTPM is enabled. Valid values are <code class="literal">Disabled</code> or <code class="literal">Enabled</code>.
									</div></dd></dl></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Using the Google Cloud console, review the details for a machine deployed by the machine set and verify that the Shielded VM options match the values that you configured.
							</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem"><p class="simpara">
								<a class="link" href="https://cloud.google.com/compute/shielded-vm/docs/shielded-vm">What is Shielded VM?</a>
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										<a class="link" href="https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#secure-boot">Secure Boot</a>
									</li><li class="listitem">
										<a class="link" href="https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#vtpm">Virtual Trusted Platform Module (vTPM)</a>
									</li><li class="listitem">
										<a class="link" href="https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#integrity-monitoring">Integrity monitoring</a>
									</li></ul></div></li></ul></div></section><section class="section" id="machineset-gcp-enabling-customer-managed-encryption_cpmso-using"><div class="titlepage"><div><div><h4 class="title">12.4.5.4. Enabling customer-managed encryption keys for a machine set</h4></div></div></div><p>
						Google Cloud Platform (GCP) Compute Engine allows users to supply an encryption key to encrypt data on disks at rest. The key is used to encrypt the data encryption key, not to encrypt the customer’s data. By default, Compute Engine encrypts this data by using Compute Engine keys.
					</p><p>
						You can enable encryption with a customer-managed key in clusters that use the Machine API. You must first <a class="link" href="https://cloud.google.com/compute/docs/disks/customer-managed-encryption#before_you_begin">create a KMS key</a> and assign the correct permissions to a service account. The KMS key name, key ring name, and location are required to allow a service account to use your key.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you do not want to use a dedicated service account for the KMS encryption, the Compute Engine default service account is used instead. You must grant the default service account permission to access the keys if you do not use a dedicated service account. The Compute Engine default service account name follows the <code class="literal">service-&lt;project_number&gt;@compute-system.iam.gserviceaccount.com</code> pattern.
						</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								To allow a specific service account to use your KMS key and to grant the service account the correct IAM role, run the following command with your KMS key name, key ring name, and location:
							</p><pre class="programlisting language-terminal">$ gcloud kms keys add-iam-policy-binding &lt;key_name&gt; \
  --keyring &lt;key_ring_name&gt; \
  --location &lt;key_ring_location&gt; \
  --member "serviceAccount:service-&lt;project_number&gt;@compute-system.iam.gserviceaccount.com” \
  --role roles/cloudkms.cryptoKeyEncrypterDecrypter</pre></li><li class="listitem"><p class="simpara">
								Configure the encryption key under the <code class="literal">providerSpec</code> field in your machine set YAML file. For example:
							</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1
kind: ControlPlaneMachineSet
...
spec:
  template:
    spec:
      providerSpec:
        value:
          disks:
          - type:
            encryptionKey:
              kmsKey:
                name: machine-encryption-key <span id="CO144-1"><!--Empty--></span><span class="callout">1</span>
                keyRing: openshift-encrpytion-ring <span id="CO144-2"><!--Empty--></span><span class="callout">2</span>
                location: global <span id="CO144-3"><!--Empty--></span><span class="callout">3</span>
                projectID: openshift-gcp-project <span id="CO144-4"><!--Empty--></span><span class="callout">4</span>
              kmsKeyServiceAccount: openshift-service-account@openshift-gcp-project.iam.gserviceaccount.com <span id="CO144-5"><!--Empty--></span><span class="callout">5</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO144-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The name of the customer-managed encryption key that is used for the disk encryption.
									</div></dd><dt><a href="#CO144-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The name of the KMS key ring that the KMS key belongs to.
									</div></dd><dt><a href="#CO144-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The GCP location in which the KMS key ring exists.
									</div></dd><dt><a href="#CO144-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										Optional: The ID of the project in which the KMS key ring exists. If a project ID is not set, the machine set <code class="literal">projectID</code> in which the machine set was created is used.
									</div></dd><dt><a href="#CO144-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										Optional: The service account that is used for the encryption request for the given KMS key. If a service account is not set, the Compute Engine default service account is used.
									</div></dd></dl></div><p class="simpara">
								When a new machine is created by using the updated <code class="literal">providerSpec</code> object configuration, the disk encryption key is encrypted with the KMS key.
							</p></li></ol></div></section></section></section><section class="section" id="cpmso-resiliency"><div class="titlepage"><div><div><h2 class="title">12.5. Control plane resiliency and recovery</h2></div></div></div><p>
				You can use the control plane machine set to improve the resiliency of the control plane for your OpenShift Container Platform cluster.
			</p><section class="section" id="cpmso-failure-domains_cpmso-resiliency"><div class="titlepage"><div><div><h3 class="title">12.5.1. High availability and fault tolerance with failure domains</h3></div></div></div><p>
					When possible, the control plane machine set spreads the control plane machines across multiple failure domains. This configuration provides high availability and fault tolerance within the control plane. This strategy can help protect the control plane when issues arise within the infrastructure provider.
				</p><section class="section" id="cpmso-failure-domains-provider_cpmso-resiliency"><div class="titlepage"><div><div><h4 class="title">12.5.1.1. Failure domain platform support and configuration</h4></div></div></div><p>
						The control plane machine set concept of a failure domain is analogous to existing concepts on cloud providers. Not all platforms support the use of failure domains.
					</p><div class="table" id="idm140311138829760"><p class="title"><strong>Table 12.2. Failure domain support matrix</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140311138824032" scope="col">Cloud provider</th><th align="center" valign="middle" id="idm140311138822944" scope="col">Support for failure domains</th><th align="center" valign="middle" id="idm140311138821840" scope="col">Provider nomenclature</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140311138824032"> <p>
										Amazon Web Services (AWS)
									</p>
									 </td><td align="center" valign="middle" headers="idm140311138822944"> <p>
										X
									</p>
									 </td><td align="center" valign="middle" headers="idm140311138821840"> <p>
										<a class="link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-availability-zones">Availability Zone (AZ)</a>
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140311138824032"> <p>
										Google Cloud Platform (GCP)
									</p>
									 </td><td align="center" valign="middle" headers="idm140311138822944"> <p>
										X
									</p>
									 </td><td align="center" valign="middle" headers="idm140311138821840"> <p>
										<a class="link" href="https://cloud.google.com/compute/docs/regions-zones">zone</a>
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140311138824032"> <p>
										Microsoft Azure
									</p>
									 </td><td align="center" valign="middle" headers="idm140311138822944"> <p>
										X
									</p>
									 </td><td align="center" valign="middle" headers="idm140311138821840"> <p>
										<a class="link" href="https://learn.microsoft.com/en-us/azure/azure-web-pubsub/concept-availability-zones">Azure availability zone</a>
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140311138824032"> <p>
										VMware vSphere
									</p>
									 </td><td align="center" valign="middle" headers="idm140311138822944"> </td><td align="center" valign="middle" headers="idm140311138821840"> <p>
										Not applicable
									</p>
									 </td></tr></tbody></table></div></div><p>
						The failure domain configuration in the control plane machine set custom resource (CR) is platform-specific. For more information about failure domain parameters in the CR, see the sample failure domain configuration for your provider.
					</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-yaml-failure-domain-aws_cpmso-configuration">Sample Amazon Web Services failure domain configuration</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-yaml-failure-domain-gcp_cpmso-configuration">Sample Google Cloud Platform failure domain configuration</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-yaml-failure-domain-azure_cpmso-configuration">Sample Microsoft Azure failure domain configuration</a>
							</li></ul></div></section><section class="section" id="cpmso-failure-domains-balancing_cpmso-resiliency"><div class="titlepage"><div><div><h4 class="title">12.5.1.2. Balancing control plane machines</h4></div></div></div><p>
						The control plane machine set balances control plane machines across the failure domains that are specified in the custom resource (CR).
					</p><p>
						When possible, the control plane machine set uses each failure domain equally to ensure appropriate fault tolerance. If there are fewer failure domains than control plane machines, failure domains are selected for reuse alphabetically by name. For clusters with no failure domains specified, all control plane machines are placed within a single failure domain.
					</p><p>
						Some changes to the failure domain configuration cause the control plane machine set to rebalance the control plane machines. For example, if you add failure domains to a cluster with fewer failure domains than control plane machines, the control plane machine set rebalances the machines across all available failure domains.
					</p></section></section><section class="section" id="cpmso-control-plane-recovery_cpmso-resiliency"><div class="titlepage"><div><div><h3 class="title">12.5.2. Recovery of failed control plane machines</h3></div></div></div><p>
					The Control Plane Machine Set Operator automates the recovery of control plane machines. When a control plane machine is deleted, the Operator creates a replacement with the configuration that is specified in the <code class="literal">ControlPlaneMachineSet</code> custom resource (CR).
				</p><p>
					For clusters that use control plane machine sets, you can configure a machine health check. The machine health check deletes unhealthy control plane machines so that they are replaced.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						If you configure a <code class="literal">MachineHealthCheck</code> resource for the control plane, set the value of <code class="literal">maxUnhealthy</code> to <code class="literal">1</code>.
					</p><p>
						This configuration ensures that the machine health check takes no action when multiple control plane machines appear to be unhealthy. Multiple unhealthy control plane machines can indicate that the etcd cluster is degraded or that a scaling operation to replace a failed machine is in progress.
					</p><p>
						If the etcd cluster is degraded, manual intervention might be required. If a scaling operation is in progress, the machine health check should allow it to finish.
					</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#deploying-machine-health-checks">Deploying machine health checks</a>
						</li></ul></div></section><section class="section" id="machine-lifecycle-hook-deletion-etcd_cpmso-resiliency"><div class="titlepage"><div><div><h3 class="title">12.5.3. Quorum protection with machine lifecycle hooks</h3></div></div></div><p>
					For OpenShift Container Platform clusters that use the Machine API Operator, the etcd Operator uses lifecycle hooks for the machine deletion phase to implement a quorum protection mechanism.
				</p><p>
					By using a <code class="literal">preDrain</code> lifecycle hook, the etcd Operator can control when the pods on a control plane machine are drained and removed. To protect etcd quorum, the etcd Operator prevents the removal of an etcd member until it migrates that member onto a new node within the cluster.
				</p><p>
					This mechanism allows the etcd Operator precise control over the members of the etcd quorum and allows the Machine API Operator to safely create and remove control plane machines without specific operational knowledge of the etcd cluster.
				</p><section class="section" id="machine-lifecycle-hook-deletion-etcd-order_cpmso-resiliency"><div class="titlepage"><div><div><h4 class="title">12.5.3.1. Control plane deletion with quorum protection processing order</h4></div></div></div><p>
						When a control plane machine is replaced on a cluster that uses a control plane machine set, the cluster temporarily has four control plane machines. When the fourth control plane node joins the cluster, the etcd Operator starts a new etcd member on the replacement node. When the etcd Operator observes that the old control plane machine is marked for deletion, it stops the etcd member on the old node and promotes the replacement etcd member to join the quorum of the cluster.
					</p><p>
						The control plane machine <code class="literal">Deleting</code> phase proceeds in the following order:
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								A control plane machine is slated for deletion.
							</li><li class="listitem">
								The control plane machine enters the <code class="literal">Deleting</code> phase.
							</li><li class="listitem"><p class="simpara">
								To satisfy the <code class="literal">preDrain</code> lifecycle hook, the etcd Operator takes the following actions:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										The etcd Operator waits until a fourth control plane machine is added to the cluster as an etcd member. This new etcd member has a state of <code class="literal">Running</code> but not <code class="literal">ready</code> until it receives the full database update from the etcd leader.
									</li><li class="listitem">
										When the new etcd member receives the full database update, the etcd Operator promotes the new etcd member to a voting member and removes the old etcd member from the cluster.
									</li></ol></div><p class="simpara">
								After this transition is complete, it is safe for the old etcd pod and its data to be removed, so the <code class="literal">preDrain</code> lifecycle hook is removed.
							</p></li><li class="listitem">
								The control plane machine status condition <code class="literal">Drainable</code> is set to <code class="literal">True</code>.
							</li><li class="listitem"><p class="simpara">
								The machine controller attempts to drain the node that is backed by the control plane machine.
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										If draining fails, <code class="literal">Drained</code> is set to <code class="literal">False</code> and the machine controller attempts to drain the node again.
									</li><li class="listitem">
										If draining succeeds, <code class="literal">Drained</code> is set to <code class="literal">True</code>.
									</li></ul></div></li><li class="listitem">
								The control plane machine status condition <code class="literal">Drained</code> is set to <code class="literal">True</code>.
							</li><li class="listitem">
								If no other Operators have added a <code class="literal">preTerminate</code> lifecycle hook, the control plane machine status condition <code class="literal">Terminable</code> is set to <code class="literal">True</code>.
							</li><li class="listitem">
								The machine controller removes the instance from the infrastructure provider.
							</li><li class="listitem">
								The machine controller deletes the <code class="literal">Node</code> object.
							</li></ol></div><div class="formalpara"><p class="title"><strong>YAML snippet demonstrating the etcd quorum protection <code class="literal">preDrain</code> lifecycle hook</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: Machine
metadata:
  ...
spec:
  lifecycleHooks:
    preDrain:
    - name: EtcdQuorumOperator <span id="CO145-1"><!--Empty--></span><span class="callout">1</span>
      owner: clusteroperator/etcd <span id="CO145-2"><!--Empty--></span><span class="callout">2</span>
  ...</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO145-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The name of the <code class="literal">preDrain</code> lifecycle hook.
							</div></dd><dt><a href="#CO145-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The hook-implementing controller that manages the <code class="literal">preDrain</code> lifecycle hook.
							</div></dd></dl></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machine-lifecycle-hook-deletion_deleting-machine">Lifecycle hooks for the machine deletion phase</a>
							</li></ul></div></section></section></section><section class="section" id="cpmso-troubleshooting"><div class="titlepage"><div><div><h2 class="title">12.6. Troubleshooting the control plane machine set</h2></div></div></div><p>
				Use the information in this section to understand and recover from issues you might encounter.
			</p><section class="section" id="cpmso-checking-status_cpmso-troubleshooting"><div class="titlepage"><div><div><h3 class="title">12.6.1. Checking the control plane machine set custom resource state</h3></div></div></div><p>
					You can verify the existence and state of the <code class="literal">ControlPlaneMachineSet</code> custom resource (CR).
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Determine the state of the CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get controlplanemachineset.machine.openshift.io cluster \
  --namespace openshift-machine-api</pre><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									A result of <code class="literal">Active</code> indicates that the <code class="literal">ControlPlaneMachineSet</code> CR exists and is activated. No administrator action is required.
								</li><li class="listitem">
									A result of <code class="literal">Inactive</code> indicates that a <code class="literal">ControlPlaneMachineSet</code> CR exists but is not activated.
								</li><li class="listitem">
									A result of <code class="literal">NotFound</code> indicates that there is no existing <code class="literal">ControlPlaneMachineSet</code> CR.
								</li></ul></div></li></ul></div><div class="formalpara"><p class="title"><strong>Next steps</strong></p><p>
						To use the control plane machine set, you must ensure that a <code class="literal">ControlPlaneMachineSet</code> CR with the correct settings for your cluster exists.
					</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							If your cluster has an existing CR, you must verify that the configuration in the CR is correct for your cluster.
						</li><li class="listitem">
							If your cluster does not have an existing CR, you must create one with the correct configuration for your cluster.
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-activating_cpmso-getting-started">Activating the control plane machine set custom resource</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-creating-cr_cpmso-getting-started">Creating a control plane machine set custom resource</a>
						</li></ul></div></section><section class="section" id="cpmso-ts-ilb-missing_cpmso-troubleshooting"><div class="titlepage"><div><div><h3 class="title">12.6.2. Adding a missing Azure internal load balancer</h3></div></div></div><p>
					The <code class="literal">internalLoadBalancer</code> parameter is required in both the <code class="literal">ControlPlaneMachineSet</code> and control plane <code class="literal">Machine</code> custom resources (CRs) for Azure. If this parameter is not preconfigured on your cluster, you must add it to both CRs.
				</p><p>
					For more information about where this parameter is located in the Azure provider specification, see the sample Azure provider specification. The placement in the control plane <code class="literal">Machine</code> CR is similar.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							List the control plane machines in your cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machines \
  -l machine.openshift.io/cluster-api-machine-role==master \
  -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
							For each control plane machine, edit the CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc edit machine &lt;control_plane_machine_name&gt;</pre></li><li class="listitem">
							Add the <code class="literal">internalLoadBalancer</code> parameter with the correct details for your cluster and save your changes.
						</li><li class="listitem"><p class="simpara">
							Edit your control plane machine set CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc edit controlplanemachineset.machine.openshift.io cluster \
  -n openshift-machine-api</pre></li><li class="listitem">
							Add the <code class="literal">internalLoadBalancer</code> parameter with the correct details for your cluster and save your changes.
						</li></ol></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							For clusters that use the default <code class="literal">RollingUpdate</code> update strategy, the Operator automatically propagates the changes to your control plane configuration.
						</li><li class="listitem">
							For clusters that are configured to use the <code class="literal">OnDelete</code> update strategy, you must replace your control plane machines manually.
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-yaml-provider-spec-azure_cpmso-configuration">Sample Azure provider specification</a>
						</li></ul></div></section><section class="section" id="cpmso-ts-etcd-degraded_cpmso-troubleshooting"><div class="titlepage"><div><div><h3 class="title">12.6.3. Recovering a degraded etcd Operator</h3></div></div></div><p>
					Certain situations can cause the etcd Operator to become degraded.
				</p><p>
					For example, while performing remediation, the machine health check might delete a control plane machine that is hosting etcd. If the etcd member is not reachable at that time, the etcd Operator becomes degraded.
				</p><p>
					When the etcd Operator is degraded, manual intervention is required to force the Operator to remove the failed member and restore the cluster state.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							List the control plane machines in your cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machines \
  -l machine.openshift.io/cluster-api-machine-role==master \
  -n openshift-machine-api \
  -o wide</pre><p class="simpara">
							Any of the following conditions might indicate a failed control plane machine:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The <code class="literal">STATE</code> value is <code class="literal">stopped</code>.
								</li><li class="listitem">
									The <code class="literal">PHASE</code> value is <code class="literal">Failed</code>.
								</li><li class="listitem">
									The <code class="literal">PHASE</code> value is <code class="literal">Deleting</code> for more than ten minutes.
								</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Before continuing, ensure that your cluster has two healthy control plane machines. Performing the actions in this procedure on more than one control plane machine risks losing etcd quorum and can cause data loss.
							</p><p>
								If you have lost the majority of your control plane hosts, leading to etcd quorum loss, then you must follow the disaster recovery procedure "Restoring to a previous cluster state" instead of this procedure.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Edit the machine CR for the failed control plane machine by running the following command:
						</p><pre class="programlisting language-terminal">$ oc edit machine &lt;control_plane_machine_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Remove the contents of the <code class="literal">lifecycleHooks</code> parameter from the failed control plane machine and save your changes.
						</p><p class="simpara">
							The etcd Operator removes the failed machine from the cluster and can then safely add new etcd members.
						</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/backup_and_restore/#dr-restoring-cluster-state">Restoring to a previous cluster state</a>
						</li></ul></div></section></section><section class="section" id="cpmso-disabling"><div class="titlepage"><div><div><h2 class="title">12.7. Disabling the control plane machine set</h2></div></div></div><p>
				The <code class="literal">.spec.state</code> field in an activated <code class="literal">ControlPlaneMachineSet</code> custom resource (CR) cannot be changed from <code class="literal">Active</code> to <code class="literal">Inactive</code>. To disable the control plane machine set, you must delete the CR so that it is removed from the cluster.
			</p><p>
				When you delete the CR, the Control Plane Machine Set Operator performs cleanup operations and disables the control plane machine set. The Operator then removes the CR from the cluster and creates an inactive control plane machine set with default settings.
			</p><section class="section" id="cpmso-deleting_cpmso-disabling"><div class="titlepage"><div><div><h3 class="title">12.7.1. Deleting the control plane machine set</h3></div></div></div><p>
					To stop managing control plane machines with the control plane machine set on your cluster, you must delete the <code class="literal">ControlPlaneMachineSet</code> custom resource (CR).
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Delete the control plane machine set CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc delete controlplanemachineset.machine.openshift.io cluster \
  -n openshift-machine-api</pre></li></ul></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Check the control plane machine set custom resource state. A result of <code class="literal">Inactive</code> indicates that the removal and replacement process is successful. A <code class="literal">ControlPlaneMachineSet</code> CR exists but is not activated.
						</li></ul></div></section><section class="section" id="cpmso-checking-status_cpmso-disabling"><div class="titlepage"><div><div><h3 class="title">12.7.2. Checking the control plane machine set custom resource state</h3></div></div></div><p>
					You can verify the existence and state of the <code class="literal">ControlPlaneMachineSet</code> custom resource (CR).
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Determine the state of the CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get controlplanemachineset.machine.openshift.io cluster \
  --namespace openshift-machine-api</pre><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									A result of <code class="literal">Active</code> indicates that the <code class="literal">ControlPlaneMachineSet</code> CR exists and is activated. No administrator action is required.
								</li><li class="listitem">
									A result of <code class="literal">Inactive</code> indicates that a <code class="literal">ControlPlaneMachineSet</code> CR exists but is not activated.
								</li><li class="listitem">
									A result of <code class="literal">NotFound</code> indicates that there is no existing <code class="literal">ControlPlaneMachineSet</code> CR.
								</li></ul></div></li></ul></div></section><section class="section" id="cpmso-reenabling_cpmso-disabling"><div class="titlepage"><div><div><h3 class="title">12.7.3. Re-enabling the control plane machine set</h3></div></div></div><p>
					To re-enable the control plane machine set, you must ensure that the configuration in the CR is correct for your cluster and activate it.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-activating_cpmso-getting-started">Activating the control plane machine set custom resource</a>
						</li></ul></div></section></section></section><section class="chapter" id="deploying-machine-health-checks"><div class="titlepage"><div><div><h1 class="title">Chapter 13. Deploying machine health checks</h1></div></div></div><p>
			You can configure and deploy a machine health check to automatically repair damaged machines in a machine pool.
		</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.
			</p><p>
				Clusters with the infrastructure platform type <code class="literal">none</code> cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.
			</p><p>
				To view the platform type for your cluster, run the following command:
			</p><pre class="programlisting language-terminal">$ oc get infrastructure cluster -o jsonpath='{.status.platform}'</pre></div></div><section class="section" id="machine-health-checks-about_deploying-machine-health-checks"><div class="titlepage"><div><div><h2 class="title">13.1. About machine health checks</h2></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					You can only apply a machine health check to control plane machines on clusters that use control plane machine sets.
				</p></div></div><p>
				To monitor machine health, create a resource to define the configuration for a controller. Set a condition to check, such as staying in the <code class="literal">NotReady</code> status for five minutes or displaying a permanent condition in the node-problem-detector, and a label for the set of machines to monitor.
			</p><p>
				The controller that observes a <code class="literal">MachineHealthCheck</code> resource checks for the defined condition. If a machine fails the health check, the machine is automatically deleted and one is created to take its place. When a machine is deleted, you see a <code class="literal">machine deleted</code> event.
			</p><p>
				To limit disruptive impact of the machine deletion, the controller drains and deletes only one node at a time. If there are more unhealthy machines than the <code class="literal">maxUnhealthy</code> threshold allows for in the targeted pool of machines, remediation stops and therefore enables manual intervention.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Consider the timeouts carefully, accounting for workloads and requirements.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Long timeouts can result in long periods of downtime for the workload on the unhealthy machine.
						</li><li class="listitem">
							Too short timeouts can result in a remediation loop. For example, the timeout for checking the <code class="literal">NotReady</code> status must be long enough to allow the machine to complete the startup process.
						</li></ul></div></div></div><p>
				To stop the check, remove the resource.
			</p><section class="section" id="machine-health-checks-limitations_deploying-machine-health-checks"><div class="titlepage"><div><div><h3 class="title">13.1.1. Limitations when deploying machine health checks</h3></div></div></div><p>
					There are limitations to consider before deploying a machine health check:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Only machines owned by a machine set are remediated by a machine health check.
						</li><li class="listitem">
							If the node for a machine is removed from the cluster, a machine health check considers the machine to be unhealthy and remediates it immediately.
						</li><li class="listitem">
							If the corresponding node for a machine does not join the cluster after the <code class="literal">nodeStartupTimeout</code>, the machine is remediated.
						</li><li class="listitem">
							A machine is remediated immediately if the <code class="literal">Machine</code> resource phase is <code class="literal">Failed</code>.
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-viewing-listing_nodes-nodes-viewing">About listing all the nodes in a cluster</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machine-health-checks-short-circuiting_deploying-machine-health-checks">Short-circuiting machine health check remediation</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-about">About the Control Plane Machine Set Operator</a>
						</li></ul></div></section></section><section class="section" id="machine-health-checks-resource_deploying-machine-health-checks"><div class="titlepage"><div><div><h2 class="title">13.2. Sample MachineHealthCheck resource</h2></div></div></div><p>
				The <code class="literal">MachineHealthCheck</code> resource for all cloud-based installation types, and other than bare metal, resembles the following YAML file:
			</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: example <span id="CO146-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-machine-api
spec:
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO146-2"><!--Empty--></span><span class="callout">2</span>
      machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO146-3"><!--Empty--></span><span class="callout">3</span>
      machine.openshift.io/cluster-api-machineset: &lt;cluster_name&gt;-&lt;label&gt;-&lt;zone&gt; <span id="CO146-4"><!--Empty--></span><span class="callout">4</span>
  unhealthyConditions:
  - type:    "Ready"
    timeout: "300s" <span id="CO146-5"><!--Empty--></span><span class="callout">5</span>
    status: "False"
  - type:    "Ready"
    timeout: "300s" <span id="CO146-6"><!--Empty--></span><span class="callout">6</span>
    status: "Unknown"
  maxUnhealthy: "40%" <span id="CO146-7"><!--Empty--></span><span class="callout">7</span>
  nodeStartupTimeout: "10m" <span id="CO146-8"><!--Empty--></span><span class="callout">8</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO146-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						Specify the name of the machine health check to deploy.
					</div></dd><dt><a href="#CO146-2"><span class="callout">2</span></a> <a href="#CO146-3"><span class="callout">3</span></a> </dt><dd><div class="para">
						Specify a label for the machine pool that you want to check.
					</div></dd><dt><a href="#CO146-4"><span class="callout">4</span></a> </dt><dd><div class="para">
						Specify the machine set to track in <code class="literal">&lt;cluster_name&gt;-&lt;label&gt;-&lt;zone&gt;</code> format. For example, <code class="literal">prod-node-us-east-1a</code>.
					</div></dd><dt><a href="#CO146-5"><span class="callout">5</span></a> <a href="#CO146-6"><span class="callout">6</span></a> </dt><dd><div class="para">
						Specify the timeout duration for a node condition. If a condition is met for the duration of the timeout, the machine will be remediated. Long timeouts can result in long periods of downtime for a workload on an unhealthy machine.
					</div></dd><dt><a href="#CO146-7"><span class="callout">7</span></a> </dt><dd><div class="para">
						Specify the amount of machines allowed to be concurrently remediated in the targeted pool. This can be set as a percentage or an integer. If the number of unhealthy machines exceeds the limit set by <code class="literal">maxUnhealthy</code>, remediation is not performed.
					</div></dd><dt><a href="#CO146-8"><span class="callout">8</span></a> </dt><dd><div class="para">
						Specify the timeout duration that a machine health check must wait for a node to join the cluster before a machine is determined to be unhealthy.
					</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The <code class="literal">matchLabels</code> are examples only; you must map your machine groups based on your specific needs.
				</p></div></div><section class="section" id="machine-health-checks-short-circuiting_deploying-machine-health-checks"><div class="titlepage"><div><div><h3 class="title">13.2.1. Short-circuiting machine health check remediation</h3></div></div></div><p>
					Short-circuiting ensures that machine health checks remediate machines only when the cluster is healthy. Short-circuiting is configured through the <code class="literal">maxUnhealthy</code> field in the <code class="literal">MachineHealthCheck</code> resource.
				</p><p>
					If the user defines a value for the <code class="literal">maxUnhealthy</code> field, before remediating any machines, the <code class="literal">MachineHealthCheck</code> compares the value of <code class="literal">maxUnhealthy</code> with the number of machines within its target pool that it has determined to be unhealthy. Remediation is not performed if the number of unhealthy machines exceeds the <code class="literal">maxUnhealthy</code> limit.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						If <code class="literal">maxUnhealthy</code> is not set, the value defaults to <code class="literal">100%</code> and the machines are remediated regardless of the state of the cluster.
					</p></div></div><p>
					The appropriate <code class="literal">maxUnhealthy</code> value depends on the scale of the cluster you deploy and how many machines the <code class="literal">MachineHealthCheck</code> covers. For example, you can use the <code class="literal">maxUnhealthy</code> value to cover multiple compute machine sets across multiple availability zones so that if you lose an entire zone, your <code class="literal">maxUnhealthy</code> setting prevents further remediation within the cluster. In global Azure regions that do not have multiple availability zones, you can use availability sets to ensure high availability.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						If you configure a <code class="literal">MachineHealthCheck</code> resource for the control plane, set the value of <code class="literal">maxUnhealthy</code> to <code class="literal">1</code>.
					</p><p>
						This configuration ensures that the machine health check takes no action when multiple control plane machines appear to be unhealthy. Multiple unhealthy control plane machines can indicate that the etcd cluster is degraded or that a scaling operation to replace a failed machine is in progress.
					</p><p>
						If the etcd cluster is degraded, manual intervention might be required. If a scaling operation is in progress, the machine health check should allow it to finish.
					</p></div></div><p>
					The <code class="literal">maxUnhealthy</code> field can be set as either an integer or percentage. There are different remediation implementations depending on the <code class="literal">maxUnhealthy</code> value.
				</p><section class="section" id="setting-maxunhealthy-by-using-an-absolute-value"><div class="titlepage"><div><div><h4 class="title">13.2.1.1. Setting maxUnhealthy by using an absolute value</h4></div></div></div><p>
						If <code class="literal">maxUnhealthy</code> is set to <code class="literal">2</code>:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Remediation will be performed if 2 or fewer nodes are unhealthy
							</li><li class="listitem">
								Remediation will not be performed if 3 or more nodes are unhealthy
							</li></ul></div><p>
						These values are independent of how many machines are being checked by the machine health check.
					</p></section><section class="section" id="setting-maxunhealthy-by-using-percentages"><div class="titlepage"><div><div><h4 class="title">13.2.1.2. Setting maxUnhealthy by using percentages</h4></div></div></div><p>
						If <code class="literal">maxUnhealthy</code> is set to <code class="literal">40%</code> and there are 25 machines being checked:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Remediation will be performed if 10 or fewer nodes are unhealthy
							</li><li class="listitem">
								Remediation will not be performed if 11 or more nodes are unhealthy
							</li></ul></div><p>
						If <code class="literal">maxUnhealthy</code> is set to <code class="literal">40%</code> and there are 6 machines being checked:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Remediation will be performed if 2 or fewer nodes are unhealthy
							</li><li class="listitem">
								Remediation will not be performed if 3 or more nodes are unhealthy
							</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The allowed number of machines is rounded down when the percentage of <code class="literal">maxUnhealthy</code> machines that are checked is not a whole number.
						</p></div></div></section></section></section><section class="section" id="machine-health-checks-creating_deploying-machine-health-checks"><div class="titlepage"><div><div><h2 class="title">13.3. Creating a machine health check resource</h2></div></div></div><p>
				You can create a <code class="literal">MachineHealthCheck</code> resource for machine sets in your cluster.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					You can only apply a machine health check to control plane machines on clusters that use control plane machine sets.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install the <code class="literal">oc</code> command line interface.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Create a <code class="literal">healthcheck.yml</code> file that contains the definition of your machine health check.
					</li><li class="listitem"><p class="simpara">
						Apply the <code class="literal">healthcheck.yml</code> file to your cluster:
					</p><pre class="programlisting language-terminal">$ oc apply -f healthcheck.yml</pre></li></ol></div><p>
				You can configure and deploy a machine health check to detect and repair unhealthy bare metal nodes.
			</p></section><section class="section" id="mgmt-power-remediation-baremetal-about_deploying-machine-health-checks"><div class="titlepage"><div><div><h2 class="title">13.4. About power-based remediation of bare metal</h2></div></div></div><p>
				In a bare metal cluster, remediation of nodes is critical to ensuring the overall health of the cluster. Physically remediating a cluster can be challenging and any delay in putting the machine into a safe or an operational state increases the time the cluster remains in a degraded state, and the risk that subsequent failures might bring the cluster offline. Power-based remediation helps counter such challenges.
			</p><p>
				Instead of reprovisioning the nodes, power-based remediation uses a power controller to power off an inoperable node. This type of remediation is also called power fencing.
			</p><p>
				OpenShift Container Platform uses the <code class="literal">MachineHealthCheck</code> controller to detect faulty bare metal nodes. Power-based remediation is fast and reboots faulty nodes instead of removing them from the cluster.
			</p><p>
				Power-based remediation provides the following capabilities:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Allows the recovery of control plane nodes
					</li><li class="listitem">
						Reduces the risk of data loss in hyperconverged environments
					</li><li class="listitem">
						Reduces the downtime associated with recovering physical machines
					</li></ul></div><section class="section" id="machine-health-checks-bare-metal_deploying-machine-health-checks"><div class="titlepage"><div><div><h3 class="title">13.4.1. MachineHealthChecks on bare metal</h3></div></div></div><p>
					Machine deletion on bare metal cluster triggers reprovisioning of a bare metal host. Usually bare metal reprovisioning is a lengthy process, during which the cluster is missing compute resources and applications might be interrupted.
				</p><p>
					There are two ways to change the default remediation process from machine deletion to host power-cycle:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Annotate the <code class="literal">MachineHealthCheck</code> resource with the <code class="literal">machine.openshift.io/remediation-strategy: external-baremetal</code> annotation.
						</li><li class="listitem">
							Create a <code class="literal">Metal3RemediationTemplate</code> resource, and refer to it in the <code class="literal">spec.remediationTemplate</code> of the <code class="literal">MachineHealthCheck</code>.
						</li></ol></div><p>
					After using one of these methods, unhealthy machines are power-cycled by using Baseboard Management Controller (BMC) credentials.
				</p></section><section class="section" id="mgmt-understanding-remediation-process_deploying-machine-health-checks"><div class="titlepage"><div><div><h3 class="title">13.4.2. Understanding the annotation-based remediation process</h3></div></div></div><p>
					The remediation process operates as follows:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							The MachineHealthCheck (MHC) controller detects that a node is unhealthy.
						</li><li class="listitem">
							The MHC notifies the bare metal machine controller which requests to power-off the unhealthy node.
						</li><li class="listitem">
							After the power is off, the node is deleted, which allows the cluster to reschedule the affected workload on other nodes.
						</li><li class="listitem">
							The bare metal machine controller requests to power on the node.
						</li><li class="listitem">
							After the node is up, the node re-registers itself with the cluster, resulting in the creation of a new node.
						</li><li class="listitem">
							After the node is recreated, the bare metal machine controller restores the annotations and labels that existed on the unhealthy node before its deletion.
						</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If the power operations did not complete, the bare metal machine controller triggers the reprovisioning of the unhealthy node unless this is a control plane node or a node that was provisioned externally.
					</p></div></div></section><section class="section" id="mgmt-understanding-metal3-remediation-process_deploying-machine-health-checks"><div class="titlepage"><div><div><h3 class="title">13.4.3. Understanding the metal3-based remediation process</h3></div></div></div><p>
					The remediation process operates as follows:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							The MachineHealthCheck (MHC) controller detects that a node is unhealthy.
						</li><li class="listitem">
							The MHC creates a metal3 remediation custom resource for the metal3 remediation controller, which requests to power-off the unhealthy node.
						</li><li class="listitem">
							After the power is off, the node is deleted, which allows the cluster to reschedule the affected workload on other nodes.
						</li><li class="listitem">
							The metal3 remediation controller requests to power on the node.
						</li><li class="listitem">
							After the node is up, the node re-registers itself with the cluster, resulting in the creation of a new node.
						</li><li class="listitem">
							After the node is recreated, the metal3 remediation controller restores the annotations and labels that existed on the unhealthy node before its deletion.
						</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If the power operations did not complete, the metal3 remediation controller triggers the reprovisioning of the unhealthy node unless this is a control plane node or a node that was provisioned externally.
					</p></div></div></section><section class="section" id="mgmt-creating-mhc-baremetal_deploying-machine-health-checks"><div class="titlepage"><div><div><h3 class="title">13.4.4. Creating a MachineHealthCheck resource for bare metal</h3></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The OpenShift Container Platform is installed using installer-provisioned infrastructure (IPI).
						</li><li class="listitem">
							Access to BMC credentials (or BMC access to each node).
						</li><li class="listitem">
							Network access to the BMC interface of the unhealthy node.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Create a <code class="literal">healthcheck.yaml</code> file that contains the definition of your machine health check.
						</li><li class="listitem">
							Apply the <code class="literal">healthcheck.yaml</code> file to your cluster using the following command:
						</li></ol></div><pre class="programlisting language-terminal">$ oc apply -f healthcheck.yaml</pre><div class="formalpara"><p class="title"><strong>Sample <code class="literal">MachineHealthCheck</code> resource for bare metal, annotation-based remediation</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: example <span id="CO147-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-machine-api
  annotations:
    machine.openshift.io/remediation-strategy: external-baremetal <span id="CO147-2"><!--Empty--></span><span class="callout">2</span>
spec:
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO147-3"><!--Empty--></span><span class="callout">3</span>
      machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO147-4"><!--Empty--></span><span class="callout">4</span>
      machine.openshift.io/cluster-api-machineset: &lt;cluster_name&gt;-&lt;label&gt;-&lt;zone&gt; <span id="CO147-5"><!--Empty--></span><span class="callout">5</span>
  unhealthyConditions:
  - type:    "Ready"
    timeout: "300s" <span id="CO147-6"><!--Empty--></span><span class="callout">6</span>
    status: "False"
  - type:    "Ready"
    timeout: "300s" <span id="CO147-7"><!--Empty--></span><span class="callout">7</span>
    status: "Unknown"
  maxUnhealthy: "40%" <span id="CO147-8"><!--Empty--></span><span class="callout">8</span>
  nodeStartupTimeout: "10m" <span id="CO147-9"><!--Empty--></span><span class="callout">9</span></pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO147-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Specify the name of the machine health check to deploy.
						</div></dd><dt><a href="#CO147-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							For bare metal clusters, you must include the <code class="literal">machine.openshift.io/remediation-strategy: external-baremetal</code> annotation in the <code class="literal">annotations</code> section to enable power-cycle remediation. With this remediation strategy, unhealthy hosts are rebooted instead of removed from the cluster.
						</div></dd><dt><a href="#CO147-3"><span class="callout">3</span></a> <a href="#CO147-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Specify a label for the machine pool that you want to check.
						</div></dd><dt><a href="#CO147-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Specify the compute machine set to track in <code class="literal">&lt;cluster_name&gt;-&lt;label&gt;-&lt;zone&gt;</code> format. For example, <code class="literal">prod-node-us-east-1a</code>.
						</div></dd><dt><a href="#CO147-6"><span class="callout">6</span></a> <a href="#CO147-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							Specify the timeout duration for the node condition. If the condition is met for the duration of the timeout, the machine will be remediated. Long timeouts can result in long periods of downtime for a workload on an unhealthy machine.
						</div></dd><dt><a href="#CO147-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							Specify the amount of machines allowed to be concurrently remediated in the targeted pool. This can be set as a percentage or an integer. If the number of unhealthy machines exceeds the limit set by <code class="literal">maxUnhealthy</code>, remediation is not performed.
						</div></dd><dt><a href="#CO147-9"><span class="callout">9</span></a> </dt><dd><div class="para">
							Specify the timeout duration that a machine health check must wait for a node to join the cluster before a machine is determined to be unhealthy.
						</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The <code class="literal">matchLabels</code> are examples only; you must map your machine groups based on your specific needs.
					</p></div></div><div class="formalpara"><p class="title"><strong>Sample <code class="literal">MachineHealthCheck</code> resource for bare metal, metal3-based remediation</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: example
  namespace: openshift-machine-api
spec:
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
      machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
      machine.openshift.io/cluster-api-machineset: &lt;cluster_name&gt;-&lt;label&gt;-&lt;zone&gt;
  selector:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3RemediationTemplate
    name: metal3-remediation-template
    namespace: openshift-machine-api
  unhealthyConditions:
  - type:    "Ready"
    timeout: "300s"</pre>

					</p></div><div class="formalpara"><p class="title"><strong>Sample <code class="literal">Metal3RemediationTemplate</code> resource for bare metal, metal3-based remediation</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3RemediationTemplate
metadata:
  name: metal3-remediation-template
  namespace: openshift-machine-api
spec:
  template:
    spec:
      strategy:
        type: Reboot
        retryLimit: 1
        timeout: 5m0s</pre>

					</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The <code class="literal">matchLabels</code> are examples only; you must map your machine groups based on your specific needs. The <code class="literal">annotations</code> section does not apply to metal3-based remediation. Annotation-based remediation and metal3-based remediation are mutually exclusive.
					</p></div></div></section><span style="color: red">&lt;mgmt-troubleshooting-issue-power-remediation_deploying-machine-health-checks&gt;<span style="color: red">&lt;title&gt;Troubleshooting issues with power-based remediation&lt;/title&gt;</span>
			 <p>
				To troubleshoot an issue with power-based remediation, verify the following:
			</p>
			 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to the BMC.
					</li><li class="listitem">
						BMC is connected to the control plane node that is responsible for running the remediation task.
					</li></ul></div>
			 &lt;/mgmt-troubleshooting-issue-power-remediation_deploying-machine-health-checks&gt;</span></section></section><div><div xml:lang="en-US" class="legalnotice" id="idm140311151301584"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"><!--Empty--></span>© 2023 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div>


  <nav class="pvof-doc__book-nav">
  <ol class="book-nav__list">
              </ol>
</nav>


          </div>
              </div>
              <div id="comments-footer" class="book-comments">
          

  

        </div>
          </div>
  </article>
<meta itemscope="" itemref="md1">



    </div>
      <!-- CP_PRIMER_FOOTER -->            </div>
        </main>
    </div>
    <!--googleoff: all-->
    <div id="to-top"><a class="btn_slideto" href="#masthead" aria-label="Back to Top"><span class="web-icon-upload"></span></a></div>
    <footer class="footer-main">
        <div class="footer-top">
            <div class="container">

              <div class="brand">
                <a href="https://redhat.com">
                  <svg class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 613 145">
                    <defs>
                      <style>
                        .rh-logo-hat {
                          fill: #e00;
                        }
                        .rh-logo-type {
                          fill: #fff;
                        }
                      </style>
                    </defs>
                    <title>Red Hat</title>
                    <path
                      class="rh-logo-hat"
                      d="M127.47,83.49c12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42l-7.45-32.36c-1.72-7.12-3.23-10.35-15.73-16.6C124.89,8.69,103.76.5,97.51.5,91.69.5,90,8,83.06,8c-6.68,0-11.64-5.6-17.89-5.6-6,0-9.91,4.09-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33C22.27,52,.5,55,.5,74.22c0,31.48,74.59,70.28,133.65,70.28,45.28,0,56.7-20.48,56.7-36.65,0-12.72-11-27.16-30.83-35.78"/>
                      <path class="rh-logo-band"
                      d="M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45,12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/>
                      <path
                      class="rh-logo-type"
                      d="M579.74,92.8c0,11.89,7.15,17.67,20.19,17.67a52.11,52.11,0,0,0,11.89-1.68V95a24.84,24.84,0,0,1-7.68,1.16c-5.37,0-7.36-1.68-7.36-6.73V68.3h15.56V54.1H596.78v-18l-17,3.68V54.1H568.49V68.3h11.25Zm-53,.32c0-3.68,3.69-5.47,9.26-5.47a43.12,43.12,0,0,1,10.1,1.26v7.15a21.51,21.51,0,0,1-10.63,2.63c-5.46,0-8.73-2.1-8.73-5.57m5.2,17.56c6,0,10.84-1.26,15.36-4.31v3.37h16.82V74.08c0-13.56-9.14-21-24.39-21-8.52,0-16.94,2-26,6.1l6.1,12.52c6.52-2.74,12-4.42,16.83-4.42,7,0,10.62,2.73,10.62,8.31v2.73a49.53,49.53,0,0,0-12.62-1.58c-14.31,0-22.93,6-22.93,16.73,0,9.78,7.78,17.24,20.19,17.24m-92.44-.94h18.09V80.92h30.29v28.82H506V36.12H487.93V64.41H457.64V36.12H439.55ZM370.62,81.87c0-8,6.31-14.1,14.62-14.1A17.22,17.22,0,0,1,397,72.09V91.54A16.36,16.36,0,0,1,385.24,96c-8.2,0-14.62-6.1-14.62-14.09m26.61,27.87h16.83V32.44l-17,3.68V57.05a28.3,28.3,0,0,0-14.2-3.68c-16.19,0-28.92,12.51-28.92,28.5a28.25,28.25,0,0,0,28.4,28.6,25.12,25.12,0,0,0,14.93-4.83ZM320,67c5.36,0,9.88,3.47,11.67,8.83H308.47C310.15,70.3,314.36,67,320,67M291.33,82c0,16.2,13.25,28.82,30.28,28.82,9.36,0,16.2-2.53,23.25-8.42l-11.26-10c-2.63,2.74-6.52,4.21-11.14,4.21a14.39,14.39,0,0,1-13.68-8.83h39.65V83.55c0-17.67-11.88-30.39-28.08-30.39a28.57,28.57,0,0,0-29,28.81M262,51.58c6,0,9.36,3.78,9.36,8.31S268,68.2,262,68.2H244.11V51.58Zm-36,58.16h18.09V82.92h13.77l13.89,26.82H292l-16.2-29.45a22.27,22.27,0,0,0,13.88-20.72c0-13.25-10.41-23.45-26-23.45H226Z"/>
                  </svg>
                </a>
              </div>

              <div role="navigation" aria-label="quick">
                  <h3>Quick Links</h3>
                  <ul>
                      <li><a class="download-software" href="https://access.redhat.com/downloads/">Downloads</a></li>
                      <li><a class="manage-subscriptions" href="https://access.redhat.com/management">Subscriptions</a></li>
                      <li><a class="support-cases" href="https://access.redhat.com/support">Support Cases</a></li>
                      <li><a class="customer-service" href="https://access.redhat.com/support/customer-service">Customer Service</a></li>
                      <li><a class="quick-docs" href="https://access.redhat.com/documentation">Product Documentation</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="help">
                  <h3>Help</h3>
                  <ul>
                      <li><a class="contact-us" href="https://access.redhat.com/support/contact/">Contact Us</a></li>
                      <li><a class="cp-faqs" href="https://access.redhat.com/articles/33844">Customer Portal FAQ</a></li>
                      <li><a class="login-problems" href="https://access.redhat.com/help/login_assistance">Log-in Assistance</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="site">
                  <h3>Site Info</h3>
                  <ul>
                      <li><a class="trust-red-hat" href="https://www.redhat.com/en/trust">Trust Red Hat</a></li>
                      <li><a class="browser-support-policy" href="https://www.redhat.com/en/about/browser-support">Browser Support Policy</a></li>
                      <li><a class="accessibility" href="https://www.redhat.com/en/about/digital-accessibility">Accessibility</a></li>
                      <li><a class="recognition" href="https://access.redhat.com/recognition/">Awards and Recognition</a></li>
                      <li><a class="colophon" href="https://access.redhat.com/help/colophon/">Colophon</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="other">
                  <h3>Related Sites</h3>
                  <ul>
                      <li><a href="https://www.redhat.com/" class="red-hat-com">redhat.com</a></li>
                      <li><a href="http://developers.redhat.com/" class="red-hat-developers">developers.redhat.com</a></li>
                      <li><a href="https://connect.redhat.com/" class="partner-connect">connect.redhat.com</a></li>
                      <li><a href="https://cloud.redhat.com/" class="cloud-com">cloud.redhat.com</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="about">
                  <h3>About</h3>
                  <ul>
                      <li><a href="https://access.redhat.com/subscription-value" class="subscription-value">Red Hat Subscription Value</a></li>
                      <li><a href="https://www.redhat.com/about/" class="about-red-hat">About Red Hat</a></li>
                      <li><a href="http://jobs.redhat.com" class="about-jobs">Red Hat Jobs</a></li>
                  </ul>
              </div>

            </div>
        </div>

        <div class="anchor">
            <div class="container">
                <div class="status-legal">
                    <a hidden href="https://status.redhat.com" class="status-page-widget">
                        <span class="status-description"></span>
                        <span class="status-dot shape-circle"></span>
                    </a>
                    <div class="legal-copyright">
                        <div class="copyright">Copyright © 2023 Red Hat, Inc.</div>
                        <div role="navigation" aria-label="legal" class="legal">
                            <ul>
                                <li><a href="http://www.redhat.com/en/about/privacy-policy" class="privacy-policy">Privacy Statement</a></li>
                                <li><a href="https://www.redhat.com/en/about/terms-use" class="terms-of-use">Terms of Use</a></li>
                                <li><a href="http://www.redhat.com/en/about/all-policies-guidelines" class="all-policies">All Policies and Guidelines</a></li>
                                <li><a id="teconsent"></a></li>
                            </ul>
                            <div id="privacy_policy">We've updated our <a href='http://www.redhat.com/en/about/privacy-policy' class='privacy-policy'>Privacy Statement</a> effective September 15, 2023.
                            </div>
                          </div>
                        </div>
                </div>
                <div class="social">
                    <a href="http://www.redhat.com/summit/" class="summit">
                        <img src="https://access.redhat.com/chrome_themes/nimbus/img/rh-summit-red-a.svg" alt="Red Hat Summit" /> <span class="offscreen">Red Hat Summit</span>
                    </a>

                    <div class="social-media">
                        <a href="https://twitter.com/RedHat" class="sm-icon twitter"><span class="nicon-twitter"></span><span class="offscreen">Twitter</span></a>                        
                    </div>
                </div>
            </div>
        </div>
    </footer>
    <!-- TrustArc -->
    <div id="consent_blackbar"></div> 
    <!--googleon: all-->
</div>
<!-- /CP_PRIMER_FOOTER -->


  </div>

    
  </body>
</html>
