<!DOCTYPE html>
<html lang="en" dir="ltr" prefix="og: https://ogp.me/ns#">
  <head>
    <meta charset="utf-8" />
<link rel="canonical" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/index" />
<link rel="shortlink" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/index" />
<meta property="og:site_name" content="Red Hat Customer Portal" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/index" />
<meta property="og:title" content="Updating clusters OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<meta property="og:description" content="This document provides instructions for updating, or upgrading, OpenShift Container Platform clusters. Updating your cluster is a simple process that does not require you to take your cluster offline." />
<meta property="og:image" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:description" content="This document provides instructions for updating, or upgrading, OpenShift Container Platform clusters. Updating your cluster is a simple process that does not require you to take your cluster offline." />
<meta name="twitter:title" content="Updating clusters OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<meta name="twitter:url" content="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/index" />
<meta name="twitter:image" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
<meta name="title" content="Updating clusters OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<link rel="alternate" hreflang="en" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/index" />
<link rel="alternate" hreflang="ko" href="https://access.redhat.com/documentation/ko-kr/openshift_container_platform/4.13/html-single/updating_clusters/index" />
<link rel="alternate" hreflang="zh-hans" href="https://access.redhat.com/documentation/zh-cn/openshift_container_platform/4.13/html-single/updating_clusters/index" />
<link rel="alternate" hreflang="ja" href="https://access.redhat.com/documentation/ja-jp/openshift_container_platform/4.13/html-single/updating_clusters/index" />
<link rel="alternate" hreflang="es" href="https://access.redhat.com/documentation/es-es/openshift_container_platform/4.13/html-single/updating_clusters/index" />
<link rel="alternate" hreflang="ru" href="https://access.redhat.com/documentation/ru-ru/openshift_container_platform/4.13/html-single/updating_clusters/index" />
<link rel="alternate" hreflang="pt-br" href="https://access.redhat.com/documentation/pt-br/openshift_container_platform/4.13/html-single/updating_clusters/index" />
<link rel="alternate" hreflang="it" href="https://access.redhat.com/documentation/it-it/openshift_container_platform/4.13/html-single/updating_clusters/index" />
<link rel="alternate" hreflang="de" href="https://access.redhat.com/documentation/de-de/openshift_container_platform/4.13/html-single/updating_clusters/index" />
<link rel="alternate" hreflang="fr" href="https://access.redhat.com/documentation/fr-fr/openshift_container_platform/4.13/html-single/updating_clusters/index" />
<link rel="alternate" hreflang="zh-hant" href="https://access.redhat.com/documentation/zh-tw/openshift_container_platform/4.13/html-single/updating_clusters/index" />
<link rel="alternate" hreflang="id" href="https://access.redhat.com/documentation/id-id/openshift_container_platform/4.13/html-single/updating_clusters/index" />
<link rel="alternate" hreflang="th" href="https://access.redhat.com/documentation/th-th/openshift_container_platform/4.13/html-single/updating_clusters/index" />
<link rel="alternate" hreflang="vi" href="https://access.redhat.com/documentation/vi-vn/openshift_container_platform/4.13/html-single/updating_clusters/index" />
<meta name="Generator" content="Drupal 9 (https://www.drupal.org)" />
<meta name="MobileOptimized" content="width" />
<meta name="HandheldFriendly" content="true" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="revision" product="b0738f19-59ac-47eb-9512-8a439cd6dfb0" title="abbfcf10-1f7c-46f0-b2f7-474e55ec3aaa" page="8aa011b3-7eb9-4370-8d26-300a8c7757c4" revision="dccd48b1f252cc22dad43e9ae6252b9287d98ca1:en-us" body="14b2e781dd1c3733b5e603984839f094.html" toc="691c3d8dedff8d9a5aa0877d9ca71b74.json" />

    <title>Updating clusters OpenShift Container Platform 4.13 | Red Hat Customer Portal</title>
    <link rel="stylesheet" media="all" href="/sites/dxp-docs/files/css/css_87GMcmxT1ib8ziQiU2KUAnTDFtZQV6iP-KGslA9LigM.css" />
<link rel="stylesheet" media="all" href="/sites/dxp-docs/files/css/css__Xq4GfgPDJw9K_yYJFmlRZGJeCENu3R3r4s0K7Tr_9g.css" />

    
    <script type="application/json" data-drupal-selector="drupal-settings-json">{"path":{"baseUrl":"\/","scriptPath":null,"pathPrefix":"","currentPath":"documentation\/en-us\/openshift_container_platform\/4.13\/html-single\/updating_clusters\/index","currentPathIsAdmin":false,"isFront":false,"currentLanguage":"en"},"pluralDelimiter":"\u0003","suppressDeprecationErrors":true,"red_hat_jwt":{"client_id":"customer-portal","cookie_name":"rh_jwt","leeway":"0","realm":"redhat-external","sso_host":"https:\/\/sso.redhat.com\/","user_integration":1,"user_plugin":"drupal_user_auth","use_external_js":0,"use_internal_js":0,"use_in_admin":0},"user":{"uid":0,"permissionsHash":"d8ea0bce2d740dacbdfe0257cf55baa0e33f7fb8468a26d055ce75daaaa2d315"}}</script>
<script src="/sites/dxp-docs/files/js/js_EQWKo9EokWkWS99x_e1oM-NEM0zlKyTkp_83mGdm5Ks.js"></script>

    <!-- CP_PRIMER_HEAD -->  <!-- TrustArc & DTM -->
  <script src="//static.redhat.com/libs/redhat/marketing/latest/trustarc/trustarc.js"></script>
  <script src="//www.redhat.com/dtm.js"></script><meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<!--[if IEMobile]><meta http-equiv="cleartype" content="on"><![endif]-->

<!-- metaInclude -->
<meta name="avalon-host-info" content="dxp-kbase-prod-139-77b4fb8768-25dr9" />
<meta name="avalon-version" content="27861f77" />
<meta name="cp-chrome-build-date" content="2023-10-06T19:17:59.039Z" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<!-- Chrome, Firefox OS and Opera -->
<meta name="theme-color" content="#000000">
<!-- Windows Phone -->
<meta name="msapplication-navbutton-color" content="#000000">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-status-bar-style" content="#000000">
<link rel="manifest" href="https://access.redhat.com/webassets/avalon/j/manifest.json">
<!-- Open Search - Tap to search -->
<link rel="search" type="application/opensearchdescription+xml" title="Red Hat Customer Portal" href="https://access.redhat.com/webassets/avalon/j/opensearch.xml" />
<!-- title -->
<title>Red Hat Customer Portal - Access to 24x7 support and knowledge</title>
<!-- /title -->
<script type="text/javascript">
    window.portal = {
        analytics : {},
        host      : "https://access.redhat.com",
        idp_url   : "https://sso.redhat.com",
        lang      : "en", 
        version   : "27861f77",
        builddate : "2023-10-06T19:17:59.039Z",        fetchdate : "2023-10-10T17:45:08-0400",        nrid      : "NOLONGERSUPPORTED",
        nrlk      : "NOLONGERSUPPORTED"
    };
</script>
<script type="text/javascript">
    if (!/\/logout.*/.test(location.pathname) && portal.host === location.origin && document.cookie.indexOf('rh_sso_session') >= 0 && !(document.cookie.indexOf('rh_jwt') >= 0)) window.location = '/login?redirectTo=' + encodeURIComponent(window.location.href);
</script>
<!-- cssInclude -->

<link rel="shortcut icon" href="https://access.redhat.com/webassets/avalon/g/favicon.ico" /><link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/bootstrap.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/bootstrap-grid.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/main.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/components.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/pages.css?v=27861f77" />

<link href="https://access.redhat.com/webassets/avalon/s/chosen.css?v=27861f77" rel="stylesheet" type="text/css" />

<!--[if lte IE 9]>
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/ie.css" />
<![endif]--><noscript>
    <style type="text/css" media="screen"> .primary-nav { display: block; } </style>
</noscript>
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/webassets/avalon/j/public_modules/node_modules/@cpelements/pfe-navigation/dist/pfe-navigation--lightdom.min.css" />
<!-- /cssInclude -->
<script src="https://access.redhat.com/webassets/avalon/j/public_modules/node_modules/@cpelements/pfe-navigation/dist/ie-polyfills.js?v=27861f77"></script>

<script async>
  if (!HTMLScriptElement.supports || !HTMLScriptElement.supports('importmap')) {
    import("https://www.redhatstatic.com/dx/v1-alpha/es-module-shims@1.7.3.js");
  }
</script>
<script type="importmap">
{
  "imports": {
    "@patternfly/elements/" : "https://www.redhatstatic.com/dx/v1-alpha/@patternfly/elements@2.2.2/",
    "@rhds/elements/":"https://www.redhatstatic.com/dx/v1-alpha/@rhds/elements@1.1.0/elements/",
    "@rhds/elements/lib/":"https://www.redhatstatic.com/dx/v1-alpha/@rhds/elements@1.1.0/lib/",
    "@cpelements/elements/":"https://www.redhatstatic.com/dx/v1-alpha/@cpelements/elements@2.0.0-alpha.7/elements/"
  }
}
</script><script type="text/javascript" src="https://access.redhat.com/webassets/avalon/j/lib/require.js?v=27861f77" data-main="/webassets/avalon/j/"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<script src="https://access.redhat.com/chrome_themes/nimbus/js/ie8.js"></script>
<![endif]-->
<script type="text/javascript" src="https://access.redhat.com/chrome_themes/nimbus/js/new-nav.js?v=27861f77" ></script>
<!-- /CP_PRIMER_HEAD -->

  </head>
  <body>
    
      <div class="dialog-off-canvas-main-canvas" data-off-canvas-main-canvas>
      <!-- CP_PRIMER_HEADER -->
<div id="page-wrap" class="page-wrap">
    <div id="pers-top-page-wrap" class="top-page-wrap pers-loader-bg">

      <div id="hero-bg-top-left" class="summit-bg-shapes"></div>
      <div id="hero-bg-top-right" class="summit-bg-shapes"></div>

        <!--googleoff: all-->
        <header class="masthead" id="masthead">

            <a href="#pfe-navigation" id="global-skip-to-nav" class="skip-link visually-hidden">Skip to navigation</a>
            <a href="#cp-main" class="skip-link visually-hidden">Skip to main content</a>            <nav id="portal-utility-nav" class="utility-navigation utility-navigation--bar hidden-at-mobile" data-analytics-region="utility" aria-labelledby="nav__utility-nav--desktop">
                <h3 id="nav__utility-nav--desktop" class="element-invisible">Utilities
                </h3>
                <ul aria-labelledby="nav__utility-nav--desktop">
                    <li id="nav-subscription" data-portal-tour-1="1">
                        <a class="top-nav-subscriptions" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Subscription" href="https://access.redhat.com/management/" >Subscriptions
                        </a>
                    </li>
                    <li id="nav-downloads" data-portal-tour-1="2">
                        <a class="top-nav-downloads" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Downloads" href="https://access.redhat.com/downloads/" >Downloads
                        </a>
                    </li>
                    <li id="nav-containers">
                        <a class="top-nav-containers" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Containers" href="https://catalog.redhat.com/software/containers/explore/" >Containers
                        </a>
                    </li>
                    <li id="nav-support" data-portal-tour-1="3">
                        <a class="top-nav-support-cases" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Support Cases" href="https://access.redhat.com/support/cases/" >Support Cases
                        </a>
                    </li>
                </ul>
            </nav>

            <pfe-navigation id="pfe-navigation" data-analytics-region="mega menu">
                <div class="pfe-navigation__logo-wrapper" id="pfe-navigation__logo-wrapper">
                    <a href="https://access.redhat.com/" class="pfe-navigation__logo-link" data-analytics-text="logo" data-analytics-category="MM|logo">
                        <img class="pfe-navigation__logo-image" alt="Red Hat Customer Portal" src="https://access.redhat.com/chrome_themes/nimbus/img/red-hat-customer-portal.svg" />
                    </a>
                </div>

                <nav class="pfe-navigation" aria-label="Main Navigation" data-analytics-region="main nav">
                    <ul class="pfe-navigation__menu" id="pfe-navigation__menu">                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-subscription--mobile" data-portal-tour-1="1">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Subscription" href="https://access.redhat.com/management/" >Subscriptions
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-downloads--mobile" data-portal-tour-1="2">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Downloads" href="https://access.redhat.com/downloads/" >Downloads
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-containers--mobile">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Containers" href="https://catalog.redhat.com/software/containers/explore/" >Containers
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-support--mobile" data-portal-tour-1="3">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Support Cases" href="https://access.redhat.com/support/cases/" >Support Cases
                            </a>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a href="https://access.redhat.com/products/" class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Products and Services">Products &amp; Services
                            </a>
                            <div class="pfe-navigation__dropdown has-primary-detail">                                <div class="desktop-col-span-2 tablet-col-span-all">
                                    <h3>
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Products" href="https://access.redhat.com/products/">Products
                                        </a>
                                    </h3>
                                    <slot name="main-menu__dropdown--product__product-listing"></slot>
                                </div>                                <div>
                                    <h3 id="nav__products__support">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Support" href="https://access.redhat.com/support">Support
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__support">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Production Support" href="https://access.redhat.com/support/offerings/production/">Production Support
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Development Support" href="https://access.redhat.com/support/offerings/developer/">Development Support
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Product Life Cycles" href="https://access.redhat.com/product-life-cycles/">Product Life Cycles
                                                    </a></li>
                                    </ul>

                                    <h3 id="nav__products__services">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Services" href="https://www.redhat.com/en/services">Services
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__services">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Consulting" href="https://www.redhat.com/en/services/consulting">Consulting
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Technical Account Management" href="https://access.redhat.com/support/offerings/tam/">Technical Account Management
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Training and Certifications" href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications
                                                    </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__products__documentation">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Documentation" href="https://access.redhat.com/documentation">Documentation
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__documentation">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat Enterprise Linux" href="https://access.redhat.com/documentation/en/red_hat_enterprise_linux">Red Hat Enterprise Linux
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat JBoss Enterprise Application Platform" href="https://access.redhat.com/documentation/en/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat OpenStack Platform" href="https://access.redhat.com/documentation/en/red_hat_openstack_platform">Red Hat OpenStack Platform
                                                    </a></li>
                                                    <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat OpenShift Container Platform" href="https://access.redhat.com/documentation/en/openshift_container_platform">Red Hat OpenShift Container Platform
                                                        </a></li>
                                    </ul>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="All Documentation" data-analytics-linkType="cta" href="https://access.redhat.com/documentation">All Documentation
                                        </a>
                                    </pfe-cta>

                                    <h3 id="nav__products__catalog"><a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Ecosystem Catalog" href="https://catalog.redhat.com/">Ecosystem Catalog
                                        </a></h3>
                                        <ul aria-labelledby="nav__products__catalog">
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Ecosystem Catalog" data-analytics-text="Red Hat Partner Ecosystem" href="https://access.redhat.com/ecosystem/">Red Hat Partner Ecosystem
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Ecosystem Catalog" data-analytics-text="Partner Resources" href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources
                                                    </a></li>
                                        </ul>
                                </div>
                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Tools" href="https://access.redhat.com/labs/">Tools
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="nav__tools__tools" data-analytics-level="2" data-analytics-text="Tools" data-analytics-category="Tools">Tools
                                    </h3>
                                    <ul aria-labelledby="nav__tools__tools">
                                        <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Solution Engine" href="https://access.redhat.com/support/cases/#/troubleshoot">Troubleshoot a product issue
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Packages" href="https://access.redhat.com/downloads/content/package-browser">Packages
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Errata" href="https://access.redhat.com/errata/">Errata
                                                    </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__tools__labs">
                                        <a data-analytics-level="2" data-analytics-category="Tools" data-analytics-text="Customer Portal Labs" href="https://access.redhat.com/labs/">Customer Portal Labs
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__tools__labs">
                                        <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Configuration" href="https://access.redhat.com/labs/#!?type=config">Configuration
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Deployment" href="https://access.redhat.com/labs/#!?type=deploy">Deployment
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Security" href="https://access.redhat.com/labs/#!?type=security">Security
                                                    </a></li>                                                    <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Troubleshooting" href="https://access.redhat.com/labs/#!?type=troubleshoot">Troubleshoot
                                                        </a></li>
                                    </ul>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="All Labs" data-analytics-linkType="cta" href="https://access.redhat.com/labs/">All labs
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h4 id="nav__tools__red-hat-insights">
                                        <a data-analytics-level="2" data-analytics-category="Tools" data-analytics-text="Red Hat Insights" href="//www.redhat.com/en/technologies/management/insights">Red Hat Insights
                                        </a>
                                    </h4>
                                    <p>Increase visibility into IT operations to detect and resolve technical issues before they impact your business.</p>
                                    <a data-analytics-level="3" data-analytics-category="Tools|Red Hat Insights" data-analytics-text="Learn more" href="https://www.redhat.com/en/technologies/management/insights">Learn More
                                    </a>
                                    <br>
                                    <a data-analytics-level="3" data-analytics-category="Tools|Red Hat Insights" data-analytics-text="Go to Insights" href="https://cloud.redhat.com/insights">Go to Insights
                                    </a>
                                </div>
                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Security" href="https://access.redhat.com/security/">Security
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="security__security-center">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Red Hat Product Security Center" href="https://access.redhat.com/security">Red Hat Product Security Center
                                        </a>
                                    </h3>
                                    <p>Engage with our Red Hat Product Security team, access security updates, and ensure your environments are not exposed to any known security vulnerabilities.
                                    </p>
                                    <pfe-cta pfe-priority="primary">
                                        <a data-analytics-level="3" data-analytics-category="Security|Red Hat Product Security Center" data-analytics-text="Product Security Center" data-analytics-linkType="cta" href="https://access.redhat.com/security/">Product Security Center
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__security__updates" data-analytics-level="2" data-analytics-text="Security Updates" data-analytics-category="Security">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Security Updates" href="/security">Security Updates
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__security__updates">
                                        <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Security Advisories" href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Red Hat CVE Database" href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Security Labs" href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs
                                                    </a></li>
                                    </ul>
                                    <p class="margin-top-xl">Keep your systems secure with Red Hat&#039;s specialized responses to security vulnerabilities.
                                    </p>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="View Responses" data-analytics-linkType="cta" href="https://access.redhat.com/security/vulnerability">View Responses
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__security__resources">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Resources" href="https://access.redhat.com/security/overview">Resources
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__security__resources">                                            <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Security Blog" href="//redhat.com/en/blog/channel/security">Security Blog
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Security Measurement" href="https://www.redhat.com/security/data/metrics/">Security Measurement
                                                    </a></li>
                                                    <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Severity Ratings" href="https://access.redhat.com/security/updates/classification/">Severity Ratings
                                                        </a></li>
                                                        <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Backporting Policies" href="https://access.redhat.com/security/updates/backporting/">Backporting Policies
                                                            </a></li>
                                                            <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Product Signing (GPG) Keys" href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys
                                                                </a></li>
                                    </ul>
                                </div>

                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a href="https://access.redhat.com/community/" class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Community">Community
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="nav__community__cp-community">
                                        <a href="https://access.redhat.com/community" data-analytics-level="2" data-analytics-text="Customer Portal Community" data-analytics-text="Customer Portal Community" data-analytics-category="Community">Customer Portal Community
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__community__cp-community">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Discussions" href="https://access.redhat.com/discussions">Discussions
                                            </a></li>                                                <li><a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Private Groups" href="https://access.redhat.com/groups/">Private Groups
                                                    </a></li>
                                    </ul>

                                    <pfe-cta pfe-priority="primary">
                                        <a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Community Activity" data-analytics-linkType="cta" href="https://access.redhat.com/community/">Community Activity
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__community__events" data-analytics-level="2" data-analytics-text="Customer Events" data-analytics-category="Community">Customer Events
                                    </h3>
                                    <ul aria-labelledby="nav__community__events">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Customer Events" data-analytics-text="Red Hat Convergence" href="https://access.redhat.com/convergence/">Red Hat Convergence
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Community|Customer Events" data-analytics-text="Red Hat Summit" href="http://www.redhat.com/summit/">Red Hat Summit
                                                </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__community__stories" data-analytics-level="2" data-analytics-text="Stories" data-analytics-category="Community">Stories
                                    </h3>
                                    <ul aria-labelledby="nav__community__stories">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Stories" data-analytics-text="Red Hat Subscription Value" href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-text="You Asked. We Acted." data-analytics-category="Community|Stories" href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Community|Stories" data-analytics-text="Open Source Communities" href="http://www.redhat.com/en/open-source">Open Source Communities
                                                    </a></li>
                                    </ul>
                                </div>
                            </div>
                        </li>
                    </ul>                </nav>                <div id="site-search" slot="search" class="utility-link site-search">
                    <div class="content">
                        <form class="ng-pristine ng-valid topSearchForm" id="topSearchForm" name="topSearchForm" action="/search/browse/search/" method="get" enctype="application/x-www-form-urlencoded">
                            <cp-search-autocomplete class="push-bottom" path="/webassets/avalon/j/data.json"></cp-search-autocomplete>                            <div>Or <a href="/support/cases/#/troubleshoot">troubleshoot an issue</a>.
                            </div>
                        </form>
                    </div>
                </div>


                <div slot="secondary-links" id="localesMenu">
                    <button class="pfe-navigation__secondary-link">
                        <pfe-icon icon="web-icon-globe" size="sm" aria-hidden="true"></pfe-icon>English
                    </button>

                    <pfe-navigation-dropdown dropdown-width="single">
                        <h2 class="utility-header">Select Your Language
                        </h2>
                        <ul class="reset">
                            <li><a href="https://access.redhat.com/changeLanguage?language=en" data-lang="en" id="en" data-analytics-text="English">English</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=ko" data-lang="ko" id="ko" data-analytics-text="Korean">한국어</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=ja"    data-lang="ja"    id="ja" data-analytics-text="Japanese">日本語</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" data-lang="zh_CN" id="zh_CN" data-analytics-text="Chinese">中文 (中国)</a></li>
                        </ul>

                    </pfe-navigation-dropdown>
                </div>                <rh-account-dropdown slot="account"></rh-account-dropdown>                <pfe-primary-detail breakpoint-width="600" class="main-menu__dropdown--product__product-listing" slot="main-menu__dropdown--product__product-listing" consistent-height>
                    <h3 slot="details-nav">Infrastructure and Management                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Enterprise Linux" href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Satellite" href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Subscription Management" href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Insights" href="https://access.redhat.com/products/red-hat-insights/">Red Hat Insights
                                </a>
                            </li>
                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Ansible Automation Platform" href="https://access.redhat.com/products/red-hat-ansible-automation-platform/">Red Hat Ansible Automation Platform
                                </a></li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Cloud Computing                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift" href="https://access.redhat.com/products/openshift">Red Hat OpenShift
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenStack Platform" href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat OpenShift Container Platform" href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat OpenShift Data Science" href="https://access.redhat.com/products/red-hat-openshift-data-science/">Red Hat OpenShift Data Science
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift Dedicated" href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat Advanced Cluster Security for Kubernetes" href="https://access.redhat.com/products/red-hat-advanced-cluster-security-for-kubernetes/">Red Hat Advanced Cluster Security for Kubernetes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat Advanced Cluster Management for Kubernetes" href="https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes/">Red Hat Advanced Cluster Management for Kubernetes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat Quay" href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat CodeReady Workspaces" href="https://access.redhat.com/products/red-hat-codeready-workspaces/">OpenShift Dev Spaces
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift Service on AWS" href="https://access.redhat.com/products/red-hat-openshift-service-aws">Red Hat OpenShift Service on AWS
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Storage                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Gluster Storage" href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Hyperconverged Infrastructure" href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Ceph Storage" href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Openshift Container Storage" href="https://access.redhat.com/products/red-hat-openshift-data-foundation">Red Hat OpenShift Data Foundation
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Runtimes                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Runtimes" href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat JBoss Enterprise Application Platform" href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Data Grid" href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat JBoss Web Server" href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Single Sign On" href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat support for Spring Boot" href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat build of Node.js" href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js
                                </a>
                            </li>                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat build of Quarkus" href="https://access.redhat.com/products/quarkus/">Red Hat build of Quarkus
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Integration and Automation                    </h3>
                    <div slot="details">
                        <ul class="border-bottom" id="portal-menu-border-bottom">
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat Application Foundations" href="https://access.redhat.com/products/red-hat-application-foundations/">Red Hat Application Foundations
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat Fuse" href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat AMQ" href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat 3scale API Management" href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management
                                </a>
                            </li>
                        </ul>
                    </div>
                    <div slot="details-nav--footer">
                        <pfe-cta pfe-priority="primary">
                            <a href="https://access.redhat.com/products/" class="pfe-navigation__menu-link" data-analytics-level="2" data-analytics-text="All Products" data-analytics-category="Products and Services|Products:" data-analytics-linkType="cta">All Products
                            </a>
                        </pfe-cta>
                    </div>
                </pfe-primary-detail>

            </pfe-navigation>

            <div id="scroll-anchor"></div>

            <!--[if IE 8]>
                <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                <span class="icon-warning alert-icon" aria-hidden="true"></span>
                You are using an unsupported web browser. Update to a supported browser for the best experience. <a href="/announcements/2120951">Read the announcement</a>.
                </div>
                </div>
            <![endif]-->
            <!--[if IE 9]>
                <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                <span class="icon-warning alert-icon" aria-hidden="true"></span>As of March 1, 2016, the Red Hat Customer Portal will no longer support Internet Explorer 9. See our new <a href="/help/browsers">browser support policy</a> for more information.
                </div>
                </div>
            <![endif]-->
            <div id="site-section"></div>
        </header>
        <!--googleon: all-->

        <main id="cp-main" class="portal-content-area">
            <div id="cp-content" class="main-content">                            <!-- /CP_PRIMER_HEADER -->

      <div class="container">
        

                                                                                                        <script>breadcrumbs = [["Products & Services","\/products\/"],["Product Documentation","\/documentation"],["OpenShift Container Platform","\/documentation\/en-us\/openshift_container_platform"],["4.13","\/documentation\/en-us\/openshift_container_platform\/4.13"],["Updating clusters","\/documentation\/en-us\/openshift_container_platform\/4.13\/html\/updating_clusters"],["Updating clusters","\/documentation\/en-us\/openshift_container_platform\/4.13\/html\/updating_clusters\/--single-page-document--"]]</script>

<div data-drupal-messages-fallback class="hidden"></div>


    </div>
        <div class="container">
        

  

  


  <article class="pvof-doc__content-wrapper__outer pvof-doc__content-wrapper__outer--css-not-removed">
    <script>
      'use strict';

            var $outerWrapper = document.querySelector('.pvof-doc__content-wrapper__outer');
      if ($outerWrapper && $outerWrapper.closest) {
        var $containerWrapper = $outerWrapper.closest('.container');
        if ($containerWrapper) {
          $containerWrapper.classList.remove('container');
          $containerWrapper.classList.add('j-chrome-content-container');
        }
      }

            var cssRemoved = false;
      try {
        var $crapCss = document.querySelectorAll(
          'link[href*="/chrome_themes/nimbus/css/pages.css"], link[href*="/chrome_themes/nimbus/css/components.css"]'
        );
        if ($crapCss.length) {
          for (let index = 0; index < $crapCss.length; index++) {
            const $stylesheet = $crapCss[index];
            $stylesheet.remove();
          }
        }
        cssRemoved = true;
      }
      catch (error) {
        console.error('Ran into an issue while trying to retheme page', error);
        cssRemoved = false;
      }

            if (cssRemoved) {
        var $pvofOuterWrapper = document.querySelector('.pvof-doc__content-wrapper__outer--css-not-removed');
        if ($pvofOuterWrapper) {
          $pvofOuterWrapper.classList.remove('pvof-doc__content-wrapper__outer--css-not-removed');
        }
      }
    </script>
    <div itemscope="" itemtype="https://schema.org/TechArticle" itemref="techArticle-md1 techArticle-md2 techArticle-md3"></div>
    <div itemscope="" itemtype="https://schema.org/SoftwareApplication" itemref="softwareApplication-md1 softwareApplication-md2 softwareApplication-md3 softwareApplication-md4"></div>
    <div class="pvof-doc__content-wrapper pvof-doc__content-wrapper--has-sidebar">
                                <div class="pvof-doc__content-wrapper__inner j-superdoc j-superdoc--has-nav">
                            <div class="pvof-sidebar__wrapper j-doc-nav j-superdoc__nav">
            <div class="j-sidebar__menu-container">
              <button class="j-sidebar__menu-trigger content-expander__trigger">
                <span class="j-sidebar__menu-trigger__open-text">Jump To</span>
                <span class="j-sidebar__menu-trigger__close-text">Close</span>
              </button>

              <div class="pvof-sidebar__inner-wrapper j-doc-nav__wrapper content-expander">
                <div class="j-sidebar__menu-details-container">
                  <button class="j-sidebar__menu-details-button j-sidebar__menu-details-button--expand">
                    Expand all
                  </button>
                  <button class="j-sidebar__menu-details-button j-sidebar__menu-details-button--collapse">
                    Collapse all
                  </button>
                </div>
                

  <nav id="pvof-doc__toc" class="pvof-doc__toc">
  <h2 class="j-doc-nav__title" id="j-doc-nav__title">
    Table of contents
  </h2>
  <div class="pvof-doc__toc-inner">
              <ol class="j-doc-nav__list" aria-labelledby="j-doc-nav__title">
                  <li class="j-doc-nav__list-item">
                                    

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters" class="j-doc-nav__link ">
    Updating clusters
  </a>
  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-clusters-overview" class="j-doc-nav__link j-doc-nav__link--has-children">
    1. Updating clusters overview
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "1. Updating clusters overview"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "1. Updating clusters overview"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-clusters-overview-understanding-container-platform-updates" class="j-doc-nav__link ">
    1.1. Understanding OpenShift Container Platform updates
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-clusters-overview-upgrade-channels-and-releases" class="j-doc-nav__link ">
    1.2. Understanding update channels and releases
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#understanding_clusteroperator_conditiontypes_updating-clusters-overview" class="j-doc-nav__link ">
    1.3. Understanding cluster Operator condition types
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#understanding-clusterversion-conditiontypes_updating-clusters-overview" class="j-doc-nav__link ">
    1.4. Understanding cluster version condition types
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-clusters-overview-prepare-eus-to-eus-update" class="j-doc-nav__link ">
    1.5. Preparing to perform an EUS-to-EUS update
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-clusters-overview-update-cluster-using-web-console" class="j-doc-nav__link ">
    1.6. Updating a cluster using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-clusters-overview-update-cluster-using-cli" class="j-doc-nav__link ">
    1.7. Updating a cluster using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-clusters-overview-perform-canary-rollout-update" class="j-doc-nav__link ">
    1.8. Performing a canary rollout update
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-clusters-overview-update-cluster-with-rhel-compute-machines" class="j-doc-nav__link ">
    1.9. Updating a cluster that includes RHEL compute machines
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-clusters-overview-update-restricted-network-cluster" class="j-doc-nav__link ">
    1.10. Updating a cluster in a disconnected environment
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-clusters-overview-vsphere-updating-hardware" class="j-doc-nav__link ">
    1.11. Updating hardware on nodes running in vSphere
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-clusters-overview-hosted-control-planes" class="j-doc-nav__link ">
    1.12. Updating hosted control planes
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#understanding-openshift-updates-1" class="j-doc-nav__link j-doc-nav__link--has-children">
    2. Understanding OpenShift updates
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2. Understanding OpenShift updates"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2. Understanding OpenShift updates"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#understanding-openshift-updates" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.1. Introduction to OpenShift updates
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.1. Introduction to OpenShift updates"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.1. Introduction to OpenShift updates"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-availability_understanding-openshift-updates" class="j-doc-nav__link ">
    2.1.1. Common questions about update availability
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-service-about_understanding-openshift-updates" class="j-doc-nav__link ">
    2.1.2. About the OpenShift Update Service
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-common-terms_understanding-openshift-updates" class="j-doc-nav__link ">
    2.1.3. Common terms
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#understanding-openshift-updates-additional-resources" class="j-doc-nav__link ">
    2.1.4. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#how-updates-work" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.2. How cluster updates work
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.2. How cluster updates work"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.2. How cluster updates work"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-evaluate-availability_how-updates-work" class="j-doc-nav__link ">
    2.2.1. Evaluation of update availability
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-release-images_how-updates-work" class="j-doc-nav__link ">
    2.2.2. Release images
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-process-workflow_how-updates-work" class="j-doc-nav__link ">
    2.2.3. Update process workflow
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-manifest-application_how-updates-work" class="j-doc-nav__link ">
    2.2.4. Understanding how manifests are applied during an update
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#mco-update-process_how-updates-work" class="j-doc-nav__link ">
    2.2.5. Understanding how the Machine Config Operator updates nodes
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#understanding-upgrade-channels-releases" class="j-doc-nav__link j-doc-nav__link--has-children">
    3. Understanding update channels and releases
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3. Understanding update channels and releases"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3. Understanding update channels and releases"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#understanding-upgrade-channels_understanding-upgrade-channels-releases" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.1. Update channels
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.1. Update channels"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.1. Update channels"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#fast-version-channel_understanding-upgrade-channels-releases" class="j-doc-nav__link ">
    3.1.1. fast-4.13 channel
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#stable-version-channel_understanding-upgrade-channels-releases" class="j-doc-nav__link ">
    3.1.2. stable-4.13 channel
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#eus-4y-channel_understanding-upgrade-channels-releases" class="j-doc-nav__link ">
    3.1.3. eus-4.y channel
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#candidate-version-channel_understanding-upgrade-channels-releases" class="j-doc-nav__link ">
    3.1.4. candidate-4.13 channel
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#upgrade-version-paths_understanding-upgrade-channels-releases" class="j-doc-nav__link ">
    3.1.5. Update recommendations in the channel
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#conditional-updates-overview_understanding-upgrade-channels-releases" class="j-doc-nav__link ">
    3.1.6. Update recommendations and Conditional Updates
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#fast-stable-channel-strategies_understanding-upgrade-channels-releases" class="j-doc-nav__link ">
    3.1.7. Choosing the correct channel for your cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#restricted-network-clusters_understanding-upgrade-channels-releases" class="j-doc-nav__link ">
    3.1.8. Restricted network clusters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#switching-between-channels_understanding-upgrade-channels-releases" class="j-doc-nav__link ">
    3.1.9. Switching between channels
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#understanding-openshift-update-duration" class="j-doc-nav__link j-doc-nav__link--has-children">
    4. Understanding OpenShift Container Platform update duration
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4. Understanding OpenShift Container Platform update duration"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4. Understanding OpenShift Container Platform update duration"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-duration-prerequisites" class="j-doc-nav__link ">
    4.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#factors-affecting-update-duration_openshift-update-duration" class="j-doc-nav__link ">
    4.2. Factors affecting update duration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#cluster-update-phases" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.3. Cluster update phases
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.3. Cluster update phases"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.3. Cluster update phases"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#cluster-version-operator_openshift-update-duration" class="j-doc-nav__link ">
    4.3.1. Cluster Version Operator target update payload deployment
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#machine-config-operator-node-updates_openshift-update-duration" class="j-doc-nav__link ">
    4.3.2. Machine Config Operator node updates
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#estimating-cluster-update-time_openshift-update-duration" class="j-doc-nav__link ">
    4.4. Estimating cluster update time
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#redhat-enterprise-linux-nodes_openshift-update-duration" class="j-doc-nav__link ">
    4.5. Red Hat Enterprise Linux (RHEL) compute nodes
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-cluster-prepare" class="j-doc-nav__link j-doc-nav__link--has-children">
    5. Preparing to update to OpenShift Container Platform 4.13
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "5. Preparing to update to OpenShift Container Platform 4.13"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "5. Preparing to update to OpenShift Container Platform 4.13"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#kube-api-removals_updating-cluster-prepare" class="j-doc-nav__link j-doc-nav__link--has-children">
    5.1. Kubernetes API deprecations and removals
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "5.1. Kubernetes API deprecations and removals"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "5.1. Kubernetes API deprecations and removals"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-preparing-list_updating-cluster-prepare" class="j-doc-nav__link ">
    5.1.1. Removed Kubernetes APIs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#evaluating-cluster-removed-apis" class="j-doc-nav__link j-doc-nav__link--has-children">
    5.1.2. Evaluating your cluster for removed APIs
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "5.1.2. Evaluating your cluster for removed APIs"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "5.1.2. Evaluating your cluster for removed APIs"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-preparing-evaluate-alerts_updating-cluster-prepare" class="j-doc-nav__link ">
    5.1.2.1. Reviewing alerts to identify uses of removed APIs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-preparing-evaluate-apirequestcount_updating-cluster-prepare" class="j-doc-nav__link ">
    5.1.2.2. Using APIRequestCount to identify uses of removed APIs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-preparing-evaluate-apirequestcount-workloads_updating-cluster-prepare" class="j-doc-nav__link ">
    5.1.2.3. Using APIRequestCount to identify which workloads are using the removed APIs
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-preparing-migrate_updating-cluster-prepare" class="j-doc-nav__link ">
    5.1.3. Migrating instances of removed APIs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-preparing-ack_updating-cluster-prepare" class="j-doc-nav__link ">
    5.1.4. Providing the administrator acknowledgment
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-preparing-conditional_updating-cluster-prepare" class="j-doc-nav__link ">
    5.2. Assessing the risk of conditional updates
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#preparing-eus-eus-upgrade" class="j-doc-nav__link j-doc-nav__link--has-children">
    6. Preparing to perform an EUS-to-EUS update
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6. Preparing to perform an EUS-to-EUS update"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6. Preparing to perform an EUS-to-EUS update"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-eus-to-eus-upgrade_eus-to-eus-upgrade" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.1. EUS-to-EUS update
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.1. EUS-to-EUS update"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.1. EUS-to-EUS update"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-eus-to-eus-upgrade-console_eus-to-eus-upgrade" class="j-doc-nav__link ">
    6.1.1. EUS-to-EUS update using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-eus-to-eus-upgrade-cli_eus-to-eus-upgrade" class="j-doc-nav__link ">
    6.1.2. EUS-to-EUS update using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-eus-to-eus-olm-operators_eus-to-eus-upgrade" class="j-doc-nav__link ">
    6.1.3. EUS-to-EUS update for layered products and Operators installed through Operator Lifecycle Manager
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#preparing-manual-creds-update" class="j-doc-nav__link j-doc-nav__link--has-children">
    7. Preparing to update a cluster with manually maintained credentials
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7. Preparing to update a cluster with manually maintained credentials"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7. Preparing to update a cluster with manually maintained credentials"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#about-manually-maintained-credentials-upgrade_preparing-manual-creds-update" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.1. Update requirements for clusters with manually maintained credentials
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.1. Update requirements for clusters with manually maintained credentials"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.1. Update requirements for clusters with manually maintained credentials"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#cco-platform-options_preparing-manual-creds-update" class="j-doc-nav__link ">
    7.1.1. Cloud credential configuration options and update requirements by platform type
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#cco-determine-mode-gui_preparing-manual-creds-update" class="j-doc-nav__link ">
    7.1.2. Determining the Cloud Credential Operator mode by using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#cco-determine-mode-cli_preparing-manual-creds-update" class="j-doc-nav__link ">
    7.1.3. Determining the Cloud Credential Operator mode by using the CLI
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#cco-ccoctl-configuring_preparing-manual-creds-update" class="j-doc-nav__link ">
    7.2. Configuring the Cloud Credential Operator utility for a cluster update
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#cco-ccoctl-upgrading_preparing-manual-creds-update" class="j-doc-nav__link ">
    7.3. Updating cloud provider resources with the Cloud Credential Operator utility
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#manually-maintained-credentials-upgrade_preparing-manual-creds-update" class="j-doc-nav__link ">
    7.4. Updating cloud provider resources with manually maintained credentials
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#cco-manual-upgrade-annotation_preparing-manual-creds-update" class="j-doc-nav__link ">
    7.5. Indicating that the cluster is ready to upgrade
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-cluster-within-minor" class="j-doc-nav__link j-doc-nav__link--has-children">
    8. Updating a cluster using the web console
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8. Updating a cluster using the web console"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8. Updating a cluster using the web console"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#prerequisites" class="j-doc-nav__link ">
    8.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-using-custom-machine-config-pools-canary_updating-cluster-within-minor" class="j-doc-nav__link ">
    8.2. Performing a canary rollout update
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#machine-health-checks-pausing-web-console_updating-cluster-within-minor" class="j-doc-nav__link ">
    8.3. Pausing a MachineHealthCheck resource by using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-single-node-openshift_updating-cluster-within-minor" class="j-doc-nav__link ">
    8.4. About updating single node OpenShift Container Platform
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-upgrading-web_updating-cluster-within-minor" class="j-doc-nav__link ">
    8.5. Updating a cluster by using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-changing-update-server-web_updating-cluster-within-minor" class="j-doc-nav__link ">
    8.6. Changing the update server by using the web console
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-cluster-cli" class="j-doc-nav__link j-doc-nav__link--has-children">
    9. Updating a cluster using the CLI
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9. Updating a cluster using the CLI"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9. Updating a cluster using the CLI"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#prerequisites-2" class="j-doc-nav__link ">
    9.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#machine-health-checks-pausing_updating-cluster-cli" class="j-doc-nav__link ">
    9.2. Pausing a MachineHealthCheck resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-single-node-openshift_updating-cluster-cli" class="j-doc-nav__link ">
    9.3. About updating single node OpenShift Container Platform
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-upgrading-cli_updating-cluster-cli" class="j-doc-nav__link ">
    9.4. Updating a cluster by using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-conditional-upgrade-pathupdating-cluster-cli" class="j-doc-nav__link ">
    9.5. Updating along a conditional upgrade path
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-changing-update-server-cli_updating-cluster-cli" class="j-doc-nav__link ">
    9.6. Changing the update server by using the CLI
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#migrating-clusters-to-multi-payload" class="j-doc-nav__link j-doc-nav__link--has-children">
    10. Migrating to a cluster with multi-architecture compute machines
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10. Migrating to a cluster with multi-architecture compute machines"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10. Migrating to a cluster with multi-architecture compute machines"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#migrating-to-multi-arch-cli_updating-clusters-overview" class="j-doc-nav__link ">
    10.1. Migrating to a cluster with multi-architecture compute machines using the CLI
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-using-custom-machine-config-pools" class="j-doc-nav__link j-doc-nav__link--has-children">
    11. Performing a canary rollout update
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11. Performing a canary rollout update"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11. Performing a canary rollout update"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-using-custom-machine-config-pools-about-mcp_update-using-custom-machine-config-pools" class="j-doc-nav__link ">
    11.1. About the canary rollout update process and MCPs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-using-custom-machine-config-pools-about_update-using-custom-machine-config-pools" class="j-doc-nav__link ">
    11.2. About performing a canary rollout update
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-using-custom-machine-config-pools-mcp_update-using-custom-machine-config-pools" class="j-doc-nav__link ">
    11.3. Creating machine config pools to perform a canary rollout update
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-using-custom-machine-config-pools-pause_update-using-custom-machine-config-pools" class="j-doc-nav__link ">
    11.4. Pausing the machine config pools
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-using-custom-machine-config-pools-update_update-using-custom-machine-config-pools" class="j-doc-nav__link ">
    11.5. Performing the cluster update
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-using-custom-machine-config-pools-unpause_update-using-custom-machine-config-pools" class="j-doc-nav__link j-doc-nav__link--has-children">
    11.6. Unpausing the machine config pools
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11.6. Unpausing the machine config pools"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11.6. Unpausing the machine config pools"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-using-custom-machine-config-pools-fail_update-using-custom-machine-config-pools" class="j-doc-nav__link ">
    11.6.1. In case of application failure
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-using-custom-machine-config-pools-mcp-remove_update-using-custom-machine-config-pools" class="j-doc-nav__link ">
    11.7. Moving a node to the original machine config pool
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-cluster-rhel-compute" class="j-doc-nav__link j-doc-nav__link--has-children">
    12. Updating a cluster that includes RHEL compute machines
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12. Updating a cluster that includes RHEL compute machines"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12. Updating a cluster that includes RHEL compute machines"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#prerequisites-3" class="j-doc-nav__link ">
    12.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-upgrading-web_updating-cluster-rhel-compute" class="j-doc-nav__link ">
    12.2. Updating a cluster by using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-cluster-rhel-compute-hooks" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.3. Optional: Adding hooks to perform Ansible tasks on RHEL machines
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.3. Optional: Adding hooks to perform Ansible tasks on RHEL machines"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.3. Optional: Adding hooks to perform Ansible tasks on RHEL machines"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#rhel-compute-about-hooks_updating-cluster-rhel-compute" class="j-doc-nav__link ">
    12.3.1. About Ansible hooks for upgrades
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#rhel-compute-using-hooks_updating-cluster-rhel-compute" class="j-doc-nav__link ">
    12.3.2. Configuring the Ansible inventory file to use hooks
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#rhel-compute-available-hooks_updating-cluster-rhel-compute" class="j-doc-nav__link ">
    12.3.3. Available hooks for RHEL compute machines
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#rhel-compute-updating-minor_updating-cluster-rhel-compute" class="j-doc-nav__link ">
    12.4. Updating RHEL compute machines in your cluster
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-a-cluster-in-a-disconnected-environment" class="j-doc-nav__link j-doc-nav__link--has-children">
    13. Updating a cluster in a disconnected environment
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13. Updating a cluster in a disconnected environment"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13. Updating a cluster in a disconnected environment"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#about-restricted-network-updates" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.1. About cluster updates in a disconnected environment
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.1. About cluster updates in a disconnected environment"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.1. About cluster updates in a disconnected environment"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#about-disconnected-updates-mirroring" class="j-doc-nav__link ">
    13.1.1. Mirroring the OpenShift Container Platform image repository
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#about-disconnected-updates-update" class="j-doc-nav__link ">
    13.1.2. Performing a cluster update in a disconnected environment
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#about-disconnected-updates-uninstalling-osus" class="j-doc-nav__link ">
    13.1.3. Uninstalling the OpenShift Update Service from a cluster
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#mirroring-ocp-image-repository" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.2. Mirroring the OpenShift Container Platform image repository
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.2. Mirroring the OpenShift Container Platform image repository"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.2. Mirroring the OpenShift Container Platform image repository"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#prerequisites_updating-mirroring-disconnected" class="j-doc-nav__link ">
    13.2.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-restricted-network-mirror-host" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.2.2. Preparing your mirror host
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.2.2. Preparing your mirror host"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.2.2. Preparing your mirror host"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#cli-installing-cli_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.2.1. Installing the OpenShift CLI by downloading the binary
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#installation-adding-registry-pull-secret_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.2.2. Configuring credentials that allow images to be mirrored
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#mirroring-ocp-resources-ocmirror" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.2.3. Mirroring resources using the oc-mirror plugin
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.2.3. Mirroring resources using the oc-mirror plugin"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.2.3. Mirroring resources using the oc-mirror plugin"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#installation-oc-mirror-about_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.3.1. About the oc-mirror plugin
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#oc-mirror-support_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.3.2. oc-mirror compatibility and support
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#installation-about-mirror-registry_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.3.3. About the mirror registry
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#installation-oc-mirror-installing-plugin_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.3.4. Installing the oc-mirror OpenShift CLI plugin
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#oc-mirror-creating-image-set-config_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.3.5. Creating the image set configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#mirroring-image-set" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.2.3.6. Mirroring an image set to a mirror registry
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.2.3.6. Mirroring an image set to a mirror registry"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.2.3.6. Mirroring an image set to a mirror registry"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#mirroring-image-set-partial" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.2.3.6.1. Mirroring an image set in a partially disconnected environment
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.2.3.6.1. Mirroring an image set in a partially disconnected environment"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.2.3.6.1. Mirroring an image set in a partially disconnected environment"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#oc-mirror-mirror-to-mirror_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.3.6.1.1. Mirroring from mirror to mirror
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#mirroring-image-set-full" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.2.3.6.2. Mirroring an image set in a fully disconnected environment
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.2.3.6.2. Mirroring an image set in a fully disconnected environment"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.2.3.6.2. Mirroring an image set in a fully disconnected environment"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#oc-mirror-mirror-to-disk_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.3.6.2.1. Mirroring from mirror to disk
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#oc-mirror-disk-to-mirror_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.3.6.2.2. Mirroring from disk to mirror
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#oc-mirror-updating-cluster-manifests_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.3.7. Configuring your cluster to use the resources generated by oc-mirror
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-mirror-registry-content" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.2.3.8. Keeping your mirror registry content updated
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.2.3.8. Keeping your mirror registry content updated"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.2.3.8. Keeping your mirror registry content updated"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#oc-mirror-updating-registry-about_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.3.8.1. About updating your mirror registry content
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#oc-mirror-differential-updates_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.3.8.2. Updating your mirror registry content
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#oc-mirror-dry-run_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.3.9. Performing a dry run
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#oc-mirror-oci-format_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.3.10. Including local OCI Operator catalogs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#oc-mirror-imageset-config-params_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.3.11. Image set configuration parameters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#oc-mirror-image-set-examples_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.3.12. Image set configuration examples
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#oc-mirror-command-reference_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.3.13. Command reference for oc-mirror
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-mirror-repository-adm-release-mirror_mirroring-ocp-image-repository" class="j-doc-nav__link ">
    13.2.4. Mirroring images using the oc adm release mirror command
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-restricted-network-cluster-OSUS" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.3. Updating a cluster in a disconnected environment using the OpenShift Update Service
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.3. Updating a cluster in a disconnected environment using the OpenShift Update Service"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.3. Updating a cluster in a disconnected environment using the OpenShift Update Service"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-service-overview_updating-restricted-network-cluster-osus" class="j-doc-nav__link ">
    13.3.1. Using the OpenShift Update Service in a disconnected environment
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-service-prereqs" class="j-doc-nav__link ">
    13.3.2. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#registry-configuration-for-update-service" class="j-doc-nav__link ">
    13.3.3. Configuring access to a secured registry for the OpenShift Update Service
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#images-update-global-pull-secret_updating-restricted-network-cluster-osus" class="j-doc-nav__link ">
    13.3.4. Updating the global cluster pull secret
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-service-install" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.3.5. Installing the OpenShift Update Service Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.3.5. Installing the OpenShift Update Service Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.3.5. Installing the OpenShift Update Service Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-service-install-web-console_updating-restricted-network-cluster-osus" class="j-doc-nav__link ">
    13.3.5.1. Installing the OpenShift Update Service Operator by using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-service-install-cli_updating-restricted-network-cluster-osus" class="j-doc-nav__link ">
    13.3.5.2. Installing the OpenShift Update Service Operator by using the CLI
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-service-graph-data_updating-restricted-network-cluster-osus" class="j-doc-nav__link ">
    13.3.6. Creating the OpenShift Update Service graph data container image
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-service-create-service" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.3.7. Creating an OpenShift Update Service application
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.3.7. Creating an OpenShift Update Service application"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.3.7. Creating an OpenShift Update Service application"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-service-create-service-web-console_updating-restricted-network-cluster-osus" class="j-doc-nav__link ">
    13.3.7.1. Creating an OpenShift Update Service application by using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-service-create-service-cli_updating-restricted-network-cluster-osus" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.3.7.2. Creating an OpenShift Update Service application by using the CLI
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.3.7.2. Creating an OpenShift Update Service application by using the CLI"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.3.7.2. Creating an OpenShift Update Service application by using the CLI"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-service-configure-cvo" class="j-doc-nav__link ">
    13.3.7.2.1. Configuring the Cluster Version Operator (CVO)
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#next-steps_updating-restricted-network-cluster-osus" class="j-doc-nav__link ">
    13.3.8. Next steps
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-restricted-network-cluster" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.4. Updating a cluster in a disconnected environment without the OpenShift Update Service
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.4. Updating a cluster in a disconnected environment without the OpenShift Update Service"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.4. Updating a cluster in a disconnected environment without the OpenShift Update Service"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#prerequisites-4" class="j-doc-nav__link ">
    13.4.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#machine-health-checks-pausing_updating-restricted-network-cluster" class="j-doc-nav__link ">
    13.4.2. Pausing a MachineHealthCheck resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-restricted-image-digests_updating-restricted-network-cluster" class="j-doc-nav__link ">
    13.4.3. Retrieving a release image digest
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-restricted_updating-restricted-network-cluster" class="j-doc-nav__link ">
    13.4.4. Updating the disconnected cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#images-configuration-registry-mirror_updating-restricted-network-cluster" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.4.5. Configuring image registry repository mirroring
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.4.5. Configuring image registry repository mirroring"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.4.5. Configuring image registry repository mirroring"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#images-configuration-registry-mirror-convert_updating-restricted-network-cluster" class="j-doc-nav__link ">
    13.4.5.1. Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#generating-icsp-object-scoped-to-a-registry_updating-restricted-network-cluster" class="j-doc-nav__link ">
    13.4.6. Widening the scope of the mirror image catalog to reduce the frequency of cluster node reboots
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#additional-resources_security-container-signature" class="j-doc-nav__link ">
    13.4.7. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#uninstalling-osus" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.5. Uninstalling the OpenShift Update Service from a cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.5. Uninstalling the OpenShift Update Service from a cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.5. Uninstalling the OpenShift Update Service from a cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-service-delete-service" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.5.1. Deleting an OpenShift Update Service application
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.5.1. Deleting an OpenShift Update Service application"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.5.1. Deleting an OpenShift Update Service application"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-service-delete-service-web-console_uninstalling-osus" class="j-doc-nav__link ">
    13.5.1.1. Deleting an OpenShift Update Service application by using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-service-delete-service-cli_uninstalling-osus" class="j-doc-nav__link ">
    13.5.1.2. Deleting an OpenShift Update Service application by using the CLI
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-service-uninstall" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.5.2. Uninstalling the OpenShift Update Service Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.5.2. Uninstalling the OpenShift Update Service Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.5.2. Uninstalling the OpenShift Update Service Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-service-uninstall-web-console_uninstalling-osus" class="j-doc-nav__link ">
    13.5.2.1. Uninstalling the OpenShift Update Service Operator by using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-service-uninstall-cli_uninstalling-osus" class="j-doc-nav__link ">
    13.5.2.2. Uninstalling the OpenShift Update Service Operator by using the CLI
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-hardware-on-nodes-running-on-vsphere" class="j-doc-nav__link j-doc-nav__link--has-children">
    14. Updating hardware on nodes running on vSphere
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "14. Updating hardware on nodes running on vSphere"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "14. Updating hardware on nodes running on vSphere"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-virtual-hardware-on-vsphere_updating-hardware-on-nodes-running-in-vsphere" class="j-doc-nav__link j-doc-nav__link--has-children">
    14.1. Updating virtual hardware on vSphere
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "14.1. Updating virtual hardware on vSphere"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "14.1. Updating virtual hardware on vSphere"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-vsphere-virtual-hardware-on-control-plane-nodes_updating-hardware-on-nodes-running-in-vsphere" class="j-doc-nav__link ">
    14.1.1. Updating the virtual hardware for control plane nodes on vSphere
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-vsphere-virtual-hardware-on-compute-nodes_updating-hardware-on-nodes-running-in-vsphere" class="j-doc-nav__link ">
    14.1.2. Updating the virtual hardware for compute nodes on vSphere
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#update-vsphere-virtual-hardware-on-template_updating-hardware-on-nodes-running-in-vsphere" class="j-doc-nav__link ">
    14.1.3. Updating the virtual hardware for template on vSphere
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#scheduling-virtual-hardware-update-on-vsphere_updating-hardware-on-nodes-running-in-vsphere" class="j-doc-nav__link ">
    14.2. Scheduling an update for virtual hardware on vSphere
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#kmm-preflight-validation" class="j-doc-nav__link j-doc-nav__link--has-children">
    15. Preflight validation for Kernel Module Management (KMM) Modules
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "15. Preflight validation for Kernel Module Management (KMM) Modules"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "15. Preflight validation for Kernel Module Management (KMM) Modules"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#kmm-validation-kickoff_kmm-preflight-validation" class="j-doc-nav__link ">
    15.1. Validation kickoff
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#kmm-validation-lifecycle_kmm-preflight-validation" class="j-doc-nav__link ">
    15.2. Validation lifecycle
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#kmm-validation-status_kmm-preflight-validation" class="j-doc-nav__link ">
    15.3. Validation status
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#kmm-preflight-validation-stages-per-module_kmm-preflight-validation" class="j-doc-nav__link j-doc-nav__link--has-children">
    15.4. Preflight validation stages per Module
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "15.4. Preflight validation stages per Module"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "15.4. Preflight validation stages per Module"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#kmm-image-validation-stage_kmm-preflight-validation" class="j-doc-nav__link ">
    15.4.1. Image validation stage
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#kmm-build-validation-stage_kmm-preflight-validation" class="j-doc-nav__link ">
    15.4.2. Build validation stage
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#kmm-sign-validation-stage_kmm-preflight-validation" class="j-doc-nav__link ">
    15.4.3. Sign validation stage
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#kmm-example-cr_kmm-preflight-validation" class="j-doc-nav__link ">
    15.5. Example PreflightValidationOCP resource
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-hosted-control-planes" class="j-doc-nav__link j-doc-nav__link--has-children">
    16. Updating hosted control planes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16. Updating hosted control planes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16. Updating hosted control planes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updates-for-hosted-control-planes_updating-hosted-control-planes" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.1. Updates for hosted control planes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.1. Updates for hosted control planes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.1. Updates for hosted control planes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updates-for-hosted-control-planes-hostedcluster_updating-hosted-control-planes" class="j-doc-nav__link ">
    16.1.1. Updates for the hosted cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updates-for-hosted-control-planes-nodepools_updating-hosted-control-planes" class="j-doc-nav__link j-doc-nav__link--has-children">
    16.1.2. Updates for node pools
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16.1.2. Updates for node pools"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16.1.2. Updates for node pools"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updates-for-nodepools-replace_updating-hosted-control-planes" class="j-doc-nav__link ">
    16.1.2.1. Replace updates for node pools
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updates-for-nodepools-inplace_updating-hosted-control-planes" class="j-doc-nav__link ">
    16.1.2.2. In place updates for node pools
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#updating-node-pools-for-hcp_updating-hosted-control-planes" class="j-doc-nav__link ">
    16.2. Updating node pools for hosted control planes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#configuring-node-pools-for-hcp_updating-hosted-control-planes" class="j-doc-nav__link ">
    16.3. Configuring node pools for hosted control planes
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                    

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters#idm140049057122752" class="j-doc-nav__link ">
    Legal Notice
  </a>
  
          </li>
              </ol>
    
  </div>
</nav>


              </div>
            </div>
            <div class="j-options-container j-options-container--mobile">
              <button class="j-sidebar__menu-trigger j-sidebar__menu-trigger--options content-expander__trigger">
                <span class="j-sidebar__menu-trigger__open-headline">
                  Settings
                </span>
                <span class="j-sidebar__menu-trigger__close-text">Close</span>
              </button>
              

  <ul class="j-doc-options__list content-expander">
    <li class="j-doc-options__item">
          <label class="j-doc-option__label j-doc-option__label--language" for="j-doc-language">
        Language:
      </label>
      <select id="j-doc-language" class="j-doc-option__select">
                  <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters" selected=''>
            English
          </option>
                  <option value="/documentation/ja-jp/openshift_container_platform/4.13/html-single/updating_clusters" >
            日本語
          </option>
                  <option value="/documentation/zh-cn/openshift_container_platform/4.13/html-single/updating_clusters" >
            简体中文
          </option>
                  <option value="/documentation/ko-kr/openshift_container_platform/4.13/html-single/updating_clusters" >
            한국어
          </option>
              </select>

            <noscript>
        <div class="j-doc-option__label j-doc-option__label--language" id="j-doc-option__label--language--nojs">
          Language:
        </div>
        <ul aria-labelledby="j-doc-option__label--language--nojs" class="j-doc-option__languages-list">
                      <li>
              <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters">English</a>
            </li>
                      <li>
              <a href="/documentation/ja-jp/openshift_container_platform/4.13/html-single/updating_clusters">日本語</a>
            </li>
                      <li>
              <a href="/documentation/zh-cn/openshift_container_platform/4.13/html-single/updating_clusters">简体中文</a>
            </li>
                      <li>
              <a href="/documentation/ko-kr/openshift_container_platform/4.13/html-single/updating_clusters">한국어</a>
            </li>
                  </ul>
      </noscript>

      </li>

    <li class="j-doc-options__item">
    <label for="j-doc-mode" class="j-doc-option__label j-doc-option__label--format">
      Format:
    </label>
    <select id="j-doc-mode" class="j-doc-option__select">
              <option value="/documentation/en-us/openshift_container_platform/4.13/html/updating_clusters"  class="j-doc-options__option j-doc-options__option--multi-page">
          Multi-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters" selected='' class="j-doc-options__option j-doc-options__option--single-page">
          Single-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/pdf/updating_clusters/OpenShift_Container_Platform-4.13-Updating_clusters-en-US.pdf"  class="j-doc-options__option j-doc-options__option--pdf">
          PDF
        </option>
          </select>

        <noscript>
      <div class="j-doc-option__label j-doc-option__label--format" id="j-doc-option__label--format--nojs">
        Format:
      </div>
      <ul class="j-doc-option__format-list" aria-labelledby="j-doc-option__label--format--nojs">
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--multi-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html/updating_clusters">Multi-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--single-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters">Single-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--pdf"><a href="/documentation/en-us/openshift_container_platform/4.13/pdf/updating_clusters/OpenShift_Container_Platform-4.13-Updating_clusters-en-US.pdf">PDF</a></li>
              </ul>
    </noscript>
  </li>
</ul>


              </div>
          </div>
                <div class="pvof-doc__tertiary-sidebar j-doc__tertiary-sidebar">
          <div class="pvof-doc__tertiary-sidebar__inner j-doc__tertiary-sidebar__inner">
            <div class="j-doc__doc-options">
              <div class="j-options-container j-options-container--desktop">
                <button class="j-sidebar__menu-trigger j-sidebar__menu-trigger--tablet content-expander__trigger">
                  <span class="j-sidebar__menu-trigger-icon"></span>
                  <h2 class="visually-hidden">Language and Page Formatting Options</h2>
                </button>
                  

  <ul class="j-doc-options__list content-expander">
    <li class="j-doc-options__item">
          <label class="j-doc-option__label j-doc-option__label--language" for="j-doc-language--2">
        Language:
      </label>
      <select id="j-doc-language--2" class="j-doc-option__select">
                  <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters" selected=''>
            English
          </option>
                  <option value="/documentation/ja-jp/openshift_container_platform/4.13/html-single/updating_clusters" >
            日本語
          </option>
                  <option value="/documentation/zh-cn/openshift_container_platform/4.13/html-single/updating_clusters" >
            简体中文
          </option>
                  <option value="/documentation/ko-kr/openshift_container_platform/4.13/html-single/updating_clusters" >
            한국어
          </option>
              </select>

            <noscript>
        <div class="j-doc-option__label j-doc-option__label--language" id="j-doc-option__label--language--nojs">
          Language:
        </div>
        <ul aria-labelledby="j-doc-option__label--language--nojs" class="j-doc-option__languages-list">
                      <li>
              <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters">English</a>
            </li>
                      <li>
              <a href="/documentation/ja-jp/openshift_container_platform/4.13/html-single/updating_clusters">日本語</a>
            </li>
                      <li>
              <a href="/documentation/zh-cn/openshift_container_platform/4.13/html-single/updating_clusters">简体中文</a>
            </li>
                      <li>
              <a href="/documentation/ko-kr/openshift_container_platform/4.13/html-single/updating_clusters">한국어</a>
            </li>
                  </ul>
      </noscript>

      </li>

    <li class="j-doc-options__item">
    <label for="j-doc-mode--2" class="j-doc-option__label j-doc-option__label--format">
      Format:
    </label>
    <select id="j-doc-mode--2" class="j-doc-option__select">
              <option value="/documentation/en-us/openshift_container_platform/4.13/html/updating_clusters"  class="j-doc-options__option j-doc-options__option--multi-page">
          Multi-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters" selected='' class="j-doc-options__option j-doc-options__option--single-page">
          Single-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/pdf/updating_clusters/OpenShift_Container_Platform-4.13-Updating_clusters-en-US.pdf"  class="j-doc-options__option j-doc-options__option--pdf">
          PDF
        </option>
          </select>

        <noscript>
      <div class="j-doc-option__label j-doc-option__label--format" id="j-doc-option__label--format--nojs">
        Format:
      </div>
      <ul class="j-doc-option__format-list" aria-labelledby="j-doc-option__label--format--nojs">
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--multi-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html/updating_clusters">Multi-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--single-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters">Single-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--pdf"><a href="/documentation/en-us/openshift_container_platform/4.13/pdf/updating_clusters/OpenShift_Container_Platform-4.13-Updating_clusters-en-US.pdf">PDF</a></li>
              </ul>
    </noscript>
  </li>
</ul>


                </div>
              </div>
          </div>
        </div>

                  <div class="doc-wrapper pvof-doc__wrapper j-superdoc__content-wrapper" id="doc-wrapper">
            

  <div class="pane-page-title">
    <h1 class="title" itemprop="name">Updating clusters</h1>
  </div>


  <div xml:lang="en-US" class="book" id="idm140049058986656"><div class="titlepage"><div><div class="producttitle"><span class="productname">OpenShift Container Platform</span> <span class="productnumber">4.13</span></div><div><h2 class="subtitle">Updating OpenShift Container Platform clusters</h2></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat OpenShift Documentation Team</span></div></div><div><a href="#idm140049057122752">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				This document provides instructions for updating, or upgrading, OpenShift Container Platform clusters. Updating your cluster is a simple process that does not require you to take your cluster offline.
			</div></div></div></div><hr/></div><section class="chapter" id="updating-clusters-overview"><div class="titlepage"><div><div><h1 class="title">Chapter 1. Updating clusters overview</h1></div></div></div><p>
			You can update an OpenShift Container Platform 4 cluster with a single operation by using the web console or the OpenShift CLI (<code class="literal">oc</code>).
		</p><section class="section" id="updating-clusters-overview-understanding-container-platform-updates"><div class="titlepage"><div><div><h2 class="title">1.1. Understanding OpenShift Container Platform updates</h2></div></div></div><p>
				<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-service-about_understanding-openshift-updates">About the OpenShift Update Service</a>: For clusters with internet access, Red Hat provides over-the-air updates by using an OpenShift Container Platform update service as a hosted service located behind public APIs.
			</p></section><section class="section" id="updating-clusters-overview-upgrade-channels-and-releases"><div class="titlepage"><div><div><h2 class="title">1.2. Understanding update channels and releases</h2></div></div></div><p>
				<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#understanding-upgrade-channels-releases">Update channels and releases</a>: With update channels, you can choose an update strategy. Update channels are specific to a minor version of OpenShift Container Platform. Update channels only control release selection and do not impact the version of the cluster that you install. The <code class="literal">openshift-install</code> binary file for a specific version of the OpenShift Container Platform always installs that minor version. For more information, see the following:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#upgrade-version-paths_understanding-upgrade-channels-releases">Upgrading version paths</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#fast-stable-channel-strategies_understanding-upgrade-channels-releases">Understanding fast and stable channel use and strategies</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#restricted-network-clusters_understanding-upgrade-channels-releases">Understanding restricted network clusters</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#switching-between-channels_understanding-upgrade-channels-releases">Switching between channels</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#conditional-updates-overview_understanding-upgrade-channels-releases">Understanding conditional updates</a>
					</li></ul></div></section><section class="section" id="understanding_clusteroperator_conditiontypes_updating-clusters-overview"><div class="titlepage"><div><div><h2 class="title">1.3. Understanding cluster Operator condition types</h2></div></div></div><p>
				The status of cluster Operators includes their condition type, which informs you of the current state of your Operator’s health. The following definitions cover a list of some common ClusterOperator condition types. Operators that have additional condition types and use Operator-specific language have been omitted.
			</p><p>
				The Cluster Version Operator (CVO) is responsible for collecting the status conditions from cluster Operators so that cluster administrators can better understand the state of the OpenShift Container Platform cluster.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Available: The condition type <code class="literal">Available</code> indicates that an Operator is functional and available in the cluster. If the status is <code class="literal">False</code>, at least one part of the operand is non-functional and the condition requires an administrator to intervene.
					</li><li class="listitem"><p class="simpara">
						Progressing: The condition type <code class="literal">Progressing</code> indicates that an Operator is actively rolling out new code, propagating configuration changes, or otherwise moving from one steady state to another.
					</p><p class="simpara">
						Operators do not report the condition type <code class="literal">Progressing</code> as <code class="literal">True</code> when they are reconciling a previous known state. If the observed cluster state has changed and the Operator is reacting to it, then the status reports back as <code class="literal">True</code>, since it is moving from one steady state to another.
					</p></li><li class="listitem"><p class="simpara">
						Degraded: The condition type <code class="literal">Degraded</code> indicates that an Operator has a current state that does not match its required state over a period of time. The period of time can vary by component, but a <code class="literal">Degraded</code> status represents persistent observation of an Operator’s condition. As a result, an Operator does not fluctuate in and out of the <code class="literal">Degraded</code> state.
					</p><p class="simpara">
						There might be a different condition type if the transition from one state to another does not persist over a long enough period to report <code class="literal">Degraded</code>. An Operator does not report <code class="literal">Degraded</code> during the course of a normal update. An Operator may report <code class="literal">Degraded</code> in response to a persistent infrastructure failure that requires eventual administrator intervention.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							This condition type is only an indication that something may need investigation and adjustment. As long as the Operator is available, the <code class="literal">Degraded</code> condition does not cause user workload failure or application downtime.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Upgradeable: The condition type <code class="literal">Upgradeable</code> indicates whether the Operator is safe to update based on the current cluster state. The message field contains a human-readable description of what the administrator needs to do for the cluster to successfully update. The CVO allows updates when this condition is <code class="literal">True</code>, <code class="literal">Unknown</code> or missing.
					</p><p class="simpara">
						When the <code class="literal">Upgradeable</code> status is <code class="literal">False</code>, only minor updates are impacted, and the CVO prevents the cluster from performing impacted updates unless forced.
					</p></li></ul></div></section><section class="section" id="understanding-clusterversion-conditiontypes_updating-clusters-overview"><div class="titlepage"><div><div><h2 class="title">1.4. Understanding cluster version condition types</h2></div></div></div><p>
				The Cluster Version Operator (CVO) monitors cluster Operators and other components, and is responsible for collecting the status of both the cluster version and its Operators. This status includes the condition type, which informs you of the health and current state of the OpenShift Container Platform cluster.
			</p><p>
				In addition to <code class="literal">Available</code>, <code class="literal">Progressing</code>, and <code class="literal">Upgradeable</code>, there are condition types that affect cluster versions and Operators.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Failing: The cluster version condition type <code class="literal">Failing</code> indicates that a cluster cannot reach its desired state, is unhealthy, and requires an administrator to intervene.
					</li><li class="listitem">
						Invalid: The cluster version condition type <code class="literal">Invalid</code> indicates that the cluster version has an error that prevents the server from taking action. The CVO only reconciles the current state as long as this condition is set.
					</li><li class="listitem">
						RetrievedUpdates: The cluster version condition type <code class="literal">RetrievedUpdates</code> indicates whether or not available updates have been retrieved from the upstream update server. The condition is <code class="literal">Unknown</code> before retrieval, <code class="literal">False</code> if the updates either recently failed or could not be retrieved, or <code class="literal">True</code> if the <code class="literal">availableUpdates</code> field is both recent and accurate.
					</li><li class="listitem">
						ReleaseAccepted: The cluster version condition type <code class="literal">ReleaseAccepted</code> with a <code class="literal">True</code> status indicates that the requested release payload was successfully loaded without failure during image verification and precondition checking.
					</li><li class="listitem">
						ImplicitlyEnabledCapabilities: The cluster version condition type <code class="literal">ImplicitlyEnabledCapabilities</code> with a <code class="literal">True</code> status indicates that there are enabled capabilities that the user is not currently requesting through <code class="literal">spec.capabilities</code>. The CVO does not support disabling capabilities if any associated resources were previously managed by the CVO.
					</li></ul></div></section><section class="section" id="updating-clusters-overview-prepare-eus-to-eus-update"><div class="titlepage"><div><div><h2 class="title">1.5. Preparing to perform an EUS-to-EUS update</h2></div></div></div><p>
				<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#preparing-eus-eus-upgrade">Preparing to perform an EUS-to-EUS update</a>: Due to fundamental Kubernetes design, all OpenShift Container Platform updates between minor versions must be serialized. You must update from OpenShift Container Platform 4.10 to 4.11, and then to 4.12. You cannot update from OpenShift Container Platform 4.10 to 4.12 directly. However, if you want to update between two Extended Update Support (EUS) versions, you can do so by incurring only a single reboot of non-control plane hosts. For more information, see the following:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-eus-to-eus-upgrade_eus-to-eus-upgrade">Updating EUS-to-EUS</a>
					</li></ul></div></section><section class="section" id="updating-clusters-overview-update-cluster-using-web-console"><div class="titlepage"><div><div><h2 class="title">1.6. Updating a cluster using the web console</h2></div></div></div><p>
				<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-cluster-within-minor">Updating a cluster using the web console</a>: You can update an OpenShift Container Platform cluster by using the web console. The following steps update a cluster within a minor version. You can use the same instructions for updating a cluster between minor versions.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-using-custom-machine-config-pools-canary_updating-cluster-within-minor">Performing a canary rollout update</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#machine-health-checks-pausing-web-console_updating-cluster-within-minor">Pausing a MachineHealthCheck resource</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-single-node-openshift_updating-cluster-within-minor">About updating OpenShift Container Platform on a single-node cluster</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-upgrading-web_updating-cluster-within-minor">Updating a cluster by using the web console</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-changing-update-server-web_updating-cluster-within-minor">Changing the update server by using the web console</a>
					</li></ul></div></section><section class="section" id="updating-clusters-overview-update-cluster-using-cli"><div class="titlepage"><div><div><h2 class="title">1.7. Updating a cluster using the CLI</h2></div></div></div><p>
				<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-cluster-cli">Updating a cluster using the CLI</a>: You can update an OpenShift Container Platform cluster within a minor version by using the OpenShift CLI (<code class="literal">oc</code>). The following steps update a cluster within a minor version. You can use the same instructions for updating a cluster between minor versions.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#machine-health-checks-pausing_updating-cluster-cli">Pausing a MachineHealthCheck resource</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-single-node-openshift_updating-cluster-cli">About updating OpenShift Container Platform on a single-node cluster</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-upgrading-cli_updating-cluster-cli">Updating a cluster by using the CLI</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-changing-update-server-cli_updating-cluster-cli">Changing the update server by using the CLI</a>
					</li></ul></div></section><section class="section" id="updating-clusters-overview-perform-canary-rollout-update"><div class="titlepage"><div><div><h2 class="title">1.8. Performing a canary rollout update</h2></div></div></div><p>
				<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-using-custom-machine-config-pools">Performing a canary rollout update</a>: By controlling the rollout of an update to the worker nodes, you can ensure that mission-critical applications stay available during the whole update, even if the update process causes your applications to fail. Depending on your organizational needs, you might want to update a small subset of worker nodes, evaluate cluster and workload health over a period of time, and then update the remaining nodes. This is referred to as a <span class="emphasis"><em>canary</em></span> update. Alternatively, you might also want to fit worker node updates, which often requires a host reboot, into smaller defined maintenance windows when it is not possible to take a large maintenance window to update the entire cluster at one time. You can perform the following procedures:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-using-custom-machine-config-pools-mcp_update-using-custom-machine-config-pools">Creating machine configuration pools to perform a canary rollout update</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-using-custom-machine-config-pools-pause_update-using-custom-machine-config-pools">Pausing the machine configuration pools</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-using-custom-machine-config-pools-update_update-using-custom-machine-config-pools">Performing the cluster update</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-using-custom-machine-config-pools-unpause_update-using-custom-machine-config-pools">Unpausing the machine configuration pools</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-using-custom-machine-config-pools-mcp-remove_update-using-custom-machine-config-pools">Moving a node to the original machine configuration pool</a>
					</li></ul></div></section><section class="section" id="updating-clusters-overview-update-cluster-with-rhel-compute-machines"><div class="titlepage"><div><div><h2 class="title">1.9. Updating a cluster that includes RHEL compute machines</h2></div></div></div><p>
				<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-cluster-rhel-compute">Updating a cluster that includes RHEL compute machines</a>: If your cluster contains Red Hat Enterprise Linux (RHEL) machines, you must perform additional steps to update those machines. You can perform the following procedures:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-upgrading-web_updating-cluster-rhel-compute">Updating a cluster by using the web console</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-cluster-rhel-compute-hooks">Optional: Adding hooks to perform Ansible tasks on RHEL machines</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#rhel-compute-updating-minor_updating-cluster-rhel-compute">Updating RHEL compute machines in your cluster</a>
					</li></ul></div></section><section class="section" id="updating-clusters-overview-update-restricted-network-cluster"><div class="titlepage"><div><div><h2 class="title">1.10. Updating a cluster in a disconnected environment</h2></div></div></div><p>
				<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#about-restricted-network-updates">About cluster updates in a disconnected environment</a>: If your mirror host cannot access both the internet and the cluster, you can mirror the images to a file system that is disconnected from that environment. You can then bring that host or removable media across that gap. If the local container registry and the cluster are connected to the mirror host of a registry, you can directly push the release images to the local registry.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-restricted-network-mirror-host">Preparing your mirror host</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#installation-adding-registry-pull-secret_mirroring-ocp-image-repository">Configuring credentials that allow images to be mirrored</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#mirroring-ocp-image-repository">Mirroring the OpenShift Container Platform image repository</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-restricted_updating-restricted-network-cluster">Updating the disconnected cluster</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#images-configuration-registry-mirror_updating-restricted-network-cluster">Configuring image registry repository mirroring</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#generating-icsp-object-scoped-to-a-registry_updating-restricted-network-cluster">Widening the scope of the mirror image catalog to reduce the frequency of cluster node reboots</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-service-install">Installing the OpenShift Update Service Operator</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-service-create-service">Creating an OpenShift Update Service application</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-service-delete-service">Deleting an OpenShift Update Service application</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-service-uninstall">Uninstalling the OpenShift Update Service Operator</a>
					</li></ul></div></section><section class="section" id="updating-clusters-overview-vsphere-updating-hardware"><div class="titlepage"><div><div><h2 class="title">1.11. Updating hardware on nodes running in vSphere</h2></div></div></div><p>
				<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-hardware-on-nodes-running-on-vsphere">Updating hardware on vSphere</a>: You must ensure that your nodes running in vSphere are running on the hardware version supported by OpenShift Container Platform. Currently, hardware version 15 or later is supported for vSphere virtual machines in a cluster. For more information, see the following:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-virtual-hardware-on-vsphere_updating-hardware-on-nodes-running-in-vsphere">Updating virtual hardware on vSphere</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#scheduling-virtual-hardware-update-on-vsphere_updating-hardware-on-nodes-running-in-vsphere">Scheduling an update for virtual hardware on vSphere</a>
					</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Version 4.13 of OpenShift Container Platform requires VMware virtual hardware version 15 or later.
				</p></div></div></section><section class="section" id="updating-clusters-overview-hosted-control-planes"><div class="titlepage"><div><div><h2 class="title">1.12. Updating hosted control planes</h2></div></div></div><p>
				<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-hosted-control-planes">Updating hosted control planes</a>: On hosted control planes for OpenShift Container Platform, updates are decoupled between the control plane and the nodes. Your service cluster provider, which is the user that hosts the cluster control planes, can manage the updates as needed. The hosted cluster handles control plane updates, and node pools handle node upgrades. For more information, see the following information:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updates-for-hosted-control-planes_updating-hosted-control-planes">Updates for hosted control planes</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-node-pools-for-hcp_updating-hosted-control-planes">Updating node pools for hosted control planes</a>
					</li></ul></div></section></section><section class="chapter" id="understanding-openshift-updates-1"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Understanding OpenShift updates</h1></div></div></div><section class="section" id="understanding-openshift-updates"><div class="titlepage"><div><div><h2 class="title">2.1. Introduction to OpenShift updates</h2></div></div></div><p>
				With OpenShift Container Platform 4, you can update an OpenShift Container Platform cluster with a single operation by using the web console or the OpenShift CLI (<code class="literal">oc</code>). Platform administrators can view new update options either by going to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Cluster Settings</strong></span> in the web console or by looking at the output of the <code class="literal">oc adm upgrade</code> command.
			</p><p>
				Red Hat hosts a public OpenShift Update Service (OSUS), which serves a graph of update possibilities based on the OpenShift Container Platform release images in the official registry. The graph contains update information for any public OCP release. OpenShift Container Platform clusters are configured to connect to the OSUS by default, and the OSUS responds to clusters with information about known update targets.
			</p><p>
				An update begins when either a cluster administrator or an automatic update controller edits the custom resource (CR) of the Cluster Version Operator (CVO) with a new version. To reconcile the cluster with the newly specified version, the CVO retrieves the target release image from an image registry and begins to apply changes to the cluster.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Operators previously installed through Operator Lifecycle Manager (OLM) follow a different process for updates. See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-upgrading-operators">Updating installed Operators</a> for more information.
				</p></div></div><p>
				The target release image contains manifest files for all cluster components that form a specific OCP version. When updating the cluster to a new version, the CVO applies manifests in separate stages called Runlevels. Most, but not all, manifests support one of the cluster Operators. As the CVO applies a manifest to a cluster Operator, the Operator might perform update tasks to reconcile itself with its new specified version.
			</p><p>
				The CVO monitors the state of each applied resource and the states reported by all cluster Operators. The CVO only proceeds with the update when all manifests and cluster Operators in the active Runlevel reach a stable condition. After the CVO updates the entire control plane through this process, the Machine Config Operator (MCO) updates the operating system and configuration of every node in the cluster.
			</p><section class="section" id="update-availability_understanding-openshift-updates"><div class="titlepage"><div><div><h3 class="title">2.1.1. Common questions about update availability</h3></div></div></div><p>
					There are several factors that affect if and when an update is made available to an OpenShift Container Platform cluster. The following list provides common questions regarding the availability of an update:
				</p><p id="channel-differences_understanding-openshift-updates">
					<span class="strong strong"><strong>What are the differences between each of the update channels?</strong></span>
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							A new release is initially added to the <code class="literal">candidate</code> channel.
						</li><li class="listitem">
							After successful final testing, a release on the <code class="literal">candidate</code> channel is promoted to the <code class="literal">fast</code> channel, an errata is published, and the release is now fully supported.
						</li><li class="listitem"><p class="simpara">
							After a delay, a release on the <code class="literal">fast</code> channel is finally promoted to the <code class="literal">stable</code> channel. This delay represents the only difference between the <code class="literal">fast</code> and <code class="literal">stable</code> channels.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								For the latest z-stream releases, this delay may generally be a week or two. However, the delay for initial updates to the latest minor version may take much longer, generally 45-90 days.
							</p></div></div></li><li class="listitem">
							Releases promoted to the <code class="literal">stable</code> channel are simultaneously promoted to the <code class="literal">eus</code> channel. The primary purpose of the <code class="literal">eus</code> channel is to serve as a convenience for clusters performing an EUS-to-EUS update.
						</li></ul></div><p id="channel-safety_understanding-openshift-updates">
					<span class="strong strong"><strong>Is a release on the <code class="literal">stable</code> channel safer or more supported than a release on the <code class="literal">fast</code> channel?</strong></span>
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							If a regression is identified for a release on a <code class="literal">fast</code> channel, it will be resolved and managed to the same extent as if that regression was identified for a release on the <code class="literal">stable</code> channel.
						</li><li class="listitem">
							The only difference between releases on the <code class="literal">fast</code> and <code class="literal">stable</code> channels is that a release only appears on the <code class="literal">stable</code> channel after it has been on the <code class="literal">fast</code> channel for some time, which provides more time for new update risks to be discovered.
						</li></ul></div><p id="supported-updates_understanding-openshift-updates">
					<span class="strong strong"><strong>What does it mean if an update is supported but not recommended?</strong></span>
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Red Hat continuously evaluates data from multiple sources to determine whether updates from one version to another lead to issues. If an issue is identified, an update path may no longer be recommended to users. However, even if the update path is not recommended, customers are still supported if they perform the update.
						</li><li class="listitem"><p class="simpara">
							Red Hat does not block users from updating to a certain version. Red Hat may declare conditional update risks, which may or may not apply to a particular cluster.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									Declared risks provide cluster administrators more context about a supported update. Cluster administrators can still accept the risk and update to that particular target version. This update is always supported despite not being recommended in the context of the conditional risk.
								</li></ul></div></li></ul></div><p id="removed-recommendation_understanding-openshift-updates">
					<span class="strong strong"><strong>What if I see that an update to a particular release is no longer recommended?</strong></span>
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							If Red Hat removes update recommendations from any supported release due to a regression, a superseding update recommendation will be provided to a future version that corrects the regression. There may be a delay while the defect is corrected, tested, and promoted to your selected channel.
						</li></ul></div><p id="z-stream-release-cadence_understanding-openshift-updates">
					<span class="strong strong"><strong>How long until the next z-stream release is made available on the fast and stable channels?</strong></span>
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							While the specific cadence can vary based on a number of factors, new z-stream releases for the latest minor version are typically made available about every week. Older minor versions, which have become more stable over time, may take much longer for new z-stream releases to be made available.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								These are only estimates based on past data about z-stream releases. Red Hat reserves the right to change the release frequency as needed. Any number of issues could cause irregularities and delays in this release cadence.
							</p></div></div></li><li class="listitem">
							Once a z-stream release is published, it also appears in the <code class="literal">fast</code> channel for that minor version. After a delay, the z-stream release may then appear in that minor version’s <code class="literal">stable</code> channel.
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#understanding-upgrade-channels-releases">Understanding update channels and releases</a>
						</li></ul></div></section><section class="section" id="update-service-about_understanding-openshift-updates"><div class="titlepage"><div><div><h3 class="title">2.1.2. About the OpenShift Update Service</h3></div></div></div><p>
					The OpenShift Update Service (OSUS) provides update recommendations to OpenShift Container Platform, including Red Hat Enterprise Linux CoreOS (RHCOS). It provides a graph, or diagram, that contains the <span class="emphasis"><em>vertices</em></span> of component Operators and the <span class="emphasis"><em>edges</em></span> that connect them. The edges in the graph show which versions you can safely update to. The vertices are update payloads that specify the intended state of the managed cluster components.
				</p><p>
					The Cluster Version Operator (CVO) in your cluster checks with the OpenShift Update Service to see the valid updates and update paths based on current component versions and information in the graph. When you request an update, the CVO uses the corresponding release image to update your cluster. The release artifacts are hosted in Quay as container images.
				</p><p>
					To allow the OpenShift Update Service to provide only compatible updates, a release verification pipeline drives automation. Each release artifact is verified for compatibility with supported cloud platforms and system architectures, as well as other component packages. After the pipeline confirms the suitability of a release, the OpenShift Update Service notifies you that it is available.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The OpenShift Update Service displays all recommended updates for your current cluster. If an update path is not recommended by the OpenShift Update Service, it might be because of a known issue with the update or the target release.
					</p></div></div><p>
					Two controllers run during continuous update mode. The first controller continuously updates the payload manifests, applies the manifests to the cluster, and outputs the controlled rollout status of the Operators to indicate whether they are available, upgrading, or failed. The second controller polls the OpenShift Update Service to determine if updates are available.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Only updating to a newer version is supported. Reverting or rolling back your cluster to a previous version is not supported. If your update fails, contact Red Hat support.
					</p></div></div><p>
					During the update process, the Machine Config Operator (MCO) applies the new configuration to your cluster machines. The MCO cordons the number of nodes specified by the <code class="literal">maxUnavailable</code> field on the machine configuration pool and marks them unavailable. By default, this value is set to <code class="literal">1</code>. The MCO updates the affected nodes alphabetically by zone, based on the <code class="literal">topology.kubernetes.io/zone</code> label. If a zone has more than one node, the oldest nodes are updated first. For nodes that do not use zones, such as in bare metal deployments, the nodes are updated by age, with the oldest nodes updated first. The MCO updates the number of nodes as specified by the <code class="literal">maxUnavailable</code> field on the machine configuration pool at a time. The MCO then applies the new configuration and reboots the machine.
				</p><p>
					If you use Red Hat Enterprise Linux (RHEL) machines as workers, the MCO does not update the kubelet because you must update the OpenShift API on the machines first.
				</p><p>
					With the specification for the new version applied to the old kubelet, the RHEL machine cannot return to the <code class="literal">Ready</code> state. You cannot complete the update until the machines are available. However, the maximum number of unavailable nodes is set to ensure that normal cluster operations can continue with that number of machines out of service.
				</p><p>
					The OpenShift Update Service is composed of an Operator and one or more application instances.
				</p></section><section class="section" id="update-common-terms_understanding-openshift-updates"><div class="titlepage"><div><div><h3 class="title">2.1.3. Common terms</h3></div></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Control plane</span></dt><dd>
								The <span class="emphasis"><em>control plane</em></span>, which is composed of control plane machines, manages the OpenShift Container Platform cluster. The control plane machines manage workloads on the compute machines, which are also known as worker machines.
							</dd><dt><span class="term">Cluster Version Operator</span></dt><dd>
								The <span class="emphasis"><em>Cluster Version Operator</em></span> (CVO) starts the update process for the cluster. It checks with OSUS based on the current cluster version and retrieves the graph which contains available or possible update paths.
							</dd><dt><span class="term">Machine Config Operator</span></dt><dd>
								The <span class="emphasis"><em>Machine Config Operator</em></span> (MCO) is a cluster-level Operator that manages the operating system and machine configurations. Through the MCO, platform administrators can configure and update systemd, CRI-O and Kubelet, the kernel, NetworkManager, and other system features on the worker nodes.
							</dd><dt><span class="term">OpenShift Update Service</span></dt><dd>
								The <span class="emphasis"><em>OpenShift Update Service</em></span> (OSUS) provides over-the-air updates to OpenShift Container Platform, including to Red Hat Enterprise Linux CoreOS (RHCOS). It provides a graph, or diagram, that contains the vertices of component Operators and the edges that connect them.
							</dd><dt><span class="term">Channels</span></dt><dd>
								<span class="emphasis"><em>Channels</em></span> declare an update strategy tied to minor versions of OpenShift Container Platform. The OSUS uses this configured strategy to recommend update edges consistent with that strategy.
							</dd><dt><span class="term">Recommended update edge</span></dt><dd>
								A <span class="emphasis"><em>recommended update edge</em></span> is a recommended update between OpenShift Container Platform releases. Whether a given update is recommended can depend on the cluster’s configured channel, current version, known bugs, and other information. OSUS communicates the recommended edges to the CVO, which runs in every cluster.
							</dd><dt><span class="term">Extended Update Support</span></dt><dd><p class="simpara">
								All post-4.7 even-numbered minor releases are labeled as <span class="emphasis"><em>Extended Update Support</em></span> (EUS) releases. These releases introduce a verified update path between EUS releases, permitting customers to streamline updates of worker nodes and formulate update strategies of EUS-to-EUS OpenShift Container Platform releases that result in fewer reboots of worker nodes.
							</p><p class="simpara">
								For more information, see <a class="link" href="https://access.redhat.com/support/policy/updates/openshift-eus">Red Hat OpenShift Extended Update Support (EUS) Overview</a>.
							</p></dd></dl></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#machine-config-overview-post-install-machine-configuration-tasks">Machine config overview</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-service-overview_updating-restricted-network-cluster-osus">Using the OpenShift Update Service in a disconnected environment</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#understanding-upgrade-channels_understanding-upgrade-channels-releases">Update channels</a>
						</li></ul></div></section><section class="section _additional-resources" id="understanding-openshift-updates-additional-resources"><div class="titlepage"><div><div><h3 class="title">2.1.4. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							For more detailed information about each major aspect of the update process, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#how-updates-work">How cluster updates work</a>.
						</li></ul></div></section></section><section class="section" id="how-updates-work"><div class="titlepage"><div><div><h2 class="title">2.2. How cluster updates work</h2></div></div></div><p>
				The following sections describe each major aspect of the OpenShift Container Platform (OCP) update process in detail. For a general overview of how updates work, see the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#understanding-openshift-updates">Introduction to OpenShift updates</a>.
			</p><section class="section" id="update-evaluate-availability_how-updates-work"><div class="titlepage"><div><div><h3 class="title">2.2.1. Evaluation of update availability</h3></div></div></div><p>
					The Cluster Version Operator (CVO) periodically queries the OpenShift Update Service (OSUS) for the most recent data about update possibilities. This data is based on the cluster’s subscribed channel. The CVO then saves information about update recommendations into either the <code class="literal">availableUpdates</code> or <code class="literal">conditionalUpdates</code> field of its <code class="literal">ClusterVersion</code> resource.
				</p><p>
					The CVO periodically checks the conditional updates for update risks. These risks are conveyed through the data served by the OSUS, which contains information for each version about known issues that might affect a cluster updated to that version. Most risks are limited to clusters with specific characteristics, such as clusters with a certain size or clusters that are deployed in a particular cloud platform.
				</p><p>
					The CVO continuously evaluates its cluster characteristics against the conditional risk information for each conditional update. If the CVO finds that the cluster matches the criteria, the CVO stores this information in the <code class="literal">conditionalUpdates</code> field of its <code class="literal">ClusterVersion</code> resource. If the CVO finds that the cluster does not match the risks of an update, or that there are no risks associated with the update, it stores the target version in the <code class="literal">availableUpdates</code> field of its <code class="literal">ClusterVersion</code> resource.
				</p><p>
					The user interface, either the web console or the OpenShift CLI (<code class="literal">oc</code>), presents this information in sectioned headings to the administrator. Each <span class="strong strong"><strong>supported but not recommended</strong></span> update recommendation contains a link to further resources about the risk so that the administrator can make an informed decision about the update.
				</p><p>
					You can inspect all available updates with the following command:
				</p><pre class="programlisting language-terminal">$ oc adm upgrade --include-not-recommended</pre><p>
					The additional <code class="literal">--include-not-recommended</code> parameter includes updates that are available but not recommended due to a known risk that applies to the cluster.
				</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">Cluster version is 4.10.22

Upstream is unset, so the cluster will use an appropriate default.
Channel: fast-4.11 (available channels: candidate-4.10, candidate-4.11, eus-4.10, fast-4.10, fast-4.11, stable-4.10)

Recommended updates:

  VERSION     IMAGE
  4.10.26     quay.io/openshift-release-dev/ocp-release@sha256:e1fa1f513068082d97d78be643c369398b0e6820afab708d26acda2262940954
  4.10.25     quay.io/openshift-release-dev/ocp-release@sha256:ed84fb3fbe026b3bbb4a2637ddd874452ac49c6ead1e15675f257e28664879cc
  4.10.24     quay.io/openshift-release-dev/ocp-release@sha256:aab51636460b5a9757b736a29bc92ada6e6e6282e46b06e6fd483063d590d62a
  4.10.23     quay.io/openshift-release-dev/ocp-release@sha256:e40e49d722cb36a95fa1c03002942b967ccbd7d68de10e003f0baa69abad457b

Supported but not recommended updates:

  Version: 4.11.0
  Image: quay.io/openshift-release-dev/ocp-release@sha256:300bce8246cf880e792e106607925de0a404484637627edf5f517375517d54a4
  Recommended: False
  Reason: RPMOSTreeTimeout
  Message: Nodes with substantial numbers of containers and CPU contention may not reconcile machine configuration https://bugzilla.redhat.com/show_bug.cgi?id=2111817#c22</pre>

					</p></div><p>
					One way to inspect the underlying availability data created by the CVO is by querying the <code class="literal">ClusterVersion</code> resource with the following command:
				</p><pre class="programlisting language-terminal">$ oc get clusterversion version -o json | jq '.status.availableUpdates'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">[
  {
    "channels": [
      "candidate-4.11",
      "candidate-4.12",
      "fast-4.11",
      "fast-4.12"
    ],
    "image": "quay.io/openshift-release-dev/ocp-release@sha256:400267c7f4e61c6bfa0a59571467e8bd85c9188e442cbd820cc8263809be3775",
    "url": "https://access.redhat.com/errata/RHBA-2023:3213",
    "version": "4.11.41"
  },
  ...
]</pre>

					</p></div><p>
					A similar command can be used to check conditional updates:
				</p><pre class="programlisting language-terminal">$ oc get clusterversion version -o json | jq '.status.conditionalUpdates'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">[
  {
    "conditions": [
      {
        "lastTransitionTime": "2023-05-30T16:28:59Z",
        "message": "The 4.11.36 release only resolves an installation issue https://issues.redhat.com//browse/OCPBUGS-11663 , which does not affect already running clusters. 4.11.36 does not include fixes delivered in recent 4.11.z releases and therefore upgrading from these versions would cause fixed bugs to reappear. Red Hat does not recommend upgrading clusters to 4.11.36 version for this reason. https://access.redhat.com/solutions/7007136",
        "reason": "PatchesOlderRelease",
        "status": "False",
        "type": "Recommended"
      }
    ],
    "release": {
      "channels": [...],
      "image": "quay.io/openshift-release-dev/ocp-release@sha256:8c04176b771a62abd801fcda3e952633566c8b5ff177b93592e8e8d2d1f8471d",
      "url": "https://access.redhat.com/errata/RHBA-2023:1733",
      "version": "4.11.36"
    },
    "risks": [...]
  },
  ...
]</pre>

					</p></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#conditional-updates-overview_understanding-upgrade-channels-releases">Update recommendation removals and Conditional Updates</a>
						</li></ul></div></section><section class="section" id="update-release-images_how-updates-work"><div class="titlepage"><div><div><h3 class="title">2.2.2. Release images</h3></div></div></div><p>
					A release image is the delivery mechanism for a specific OpenShift Container Platform (OCP) version. It contains the release metadata, a Cluster Version Operator (CVO) binary matching the release version, every manifest needed to deploy individual OpenShift cluster Operators, and a list of SHA digest-versioned references to all container images that make up this OpenShift version.
				</p><p>
					You can inspect the content of a specific release image by running the following command:
				</p><pre class="programlisting language-terminal">$ oc adm release extract &lt;release image&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">$ oc adm release extract quay.io/openshift-release-dev/ocp-release:4.12.6-x86_64
Extracted release payload from digest sha256:800d1e39d145664975a3bb7cbc6e674fbf78e3c45b5dde9ff2c5a11a8690c87b created at 2023-03-01T12:46:29Z

$ ls
0000_03_authorization-openshift_01_rolebindingrestriction.crd.yaml
0000_03_config-operator_01_proxy.crd.yaml
0000_03_marketplace-operator_01_operatorhub.crd.yaml
0000_03_marketplace-operator_02_operatorhub.cr.yaml
0000_03_quota-openshift_01_clusterresourcequota.crd.yaml <span id="CO1-1"><!--Empty--></span><span class="callout">1</span>
...
0000_90_service-ca-operator_02_prometheusrolebinding.yaml <span id="CO1-2"><!--Empty--></span><span class="callout">2</span>
0000_90_service-ca-operator_03_servicemonitor.yaml
0000_99_machine-api-operator_00_tombstones.yaml
image-references <span id="CO1-3"><!--Empty--></span><span class="callout">3</span>
release-metadata</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO1-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Manifest for <code class="literal">ClusterResourceQuota</code> CRD, to be applied on Runlevel 03
						</div></dd><dt><a href="#CO1-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Manifest for <code class="literal">PrometheusRoleBinding</code> resource for the <code class="literal">service-ca-operator</code>, to be applied on Runlevel 90
						</div></dd><dt><a href="#CO1-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							List of SHA digest-versioned references to all required images
						</div></dd></dl></div></section><section class="section" id="update-process-workflow_how-updates-work"><div class="titlepage"><div><div><h3 class="title">2.2.3. Update process workflow</h3></div></div></div><p>
					The following steps represent a detailed workflow of the OpenShift Container Platform (OCP) update process:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							The target version is stored in the <code class="literal">spec.desiredUpdate.version</code> field of the <code class="literal">ClusterVersion</code> resource, which may be managed through the web console or the CLI.
						</li><li class="listitem">
							The Cluster Version Operator (CVO) detects that the <code class="literal">desiredUpdate</code> in the <code class="literal">ClusterVersion</code> resource differs from the current cluster version. Using graph data from the OpenShift Update Service, the CVO resolves the desired cluster version to a pull spec for the release image.
						</li><li class="listitem">
							The CVO validates the integrity and authenticity of the release image. Red Hat publishes cryptographically-signed statements about published release images at predefined locations by using image SHA digests as unique and immutable release image identifiers. The CVO utilizes a list of built-in public keys to validate the presence and signatures of the statement matching the checked release image.
						</li><li class="listitem">
							The CVO creates a job named <code class="literal">version-$version-$hash</code> in the <code class="literal">openshift-cluster-version</code> namespace. This job uses containers that are executing the release image, so the cluster downloads the image through the container runtime. The job then extracts the manifests and metadata from the release image to a shared volume that is accessible to the CVO.
						</li><li class="listitem">
							The CVO validates the extracted manifests and metadata.
						</li><li class="listitem">
							The CVO checks some preconditions to ensure that no problematic condition is detected in the cluster. Certain conditions can prevent updates from proceeding. These conditions are either determined by the CVO itself, or reported by individual cluster Operators that detect some details about the cluster that the Operator considers problematic for the update.
						</li><li class="listitem">
							The CVO records the accepted release in <code class="literal">status.desired</code> and creates a <code class="literal">status.history</code> entry about the new update.
						</li><li class="listitem">
							The CVO begins applying the manifests from the release image. Cluster Operators are updated in separate stages called Runlevels, and the CVO ensures that all Operators in a Runlevel finish updating before it proceeds to the next level.
						</li><li class="listitem">
							Manifests for the CVO itself are applied early in the process. When the CVO deployment is applied, the current CVO pod terminates, and a CVO pod using the new version starts. The new CVO proceeds to apply the remaining manifests.
						</li><li class="listitem">
							The update proceeds until the entire control plane is updated to the new version. Individual cluster Operators might perform update tasks on their domain of the cluster, and while they do so, they report their state through the <code class="literal">Progressing=True</code> condition.
						</li><li class="listitem">
							The Machine Config Operator (MCO) manifests are applied towards the end of the process. The updated MCO then begins updating the system configuration and operating system of every node. Each node might be drained, updated, and rebooted before it starts to accept workloads again.
						</li></ol></div><p>
					The cluster reports as updated after the control plane update is finished, usually before all nodes are updated. After the update, the CVO maintains all cluster resources to match the state delivered in the release image.
				</p></section><section class="section" id="update-manifest-application_how-updates-work"><div class="titlepage"><div><div><h3 class="title">2.2.4. Understanding how manifests are applied during an update</h3></div></div></div><p>
					Some manifests supplied in a release image must be applied in a certain order because of the dependencies between them. For example, the <code class="literal">CustomResourceDefinition</code> resource must be created before the matching custom resources. Additionally, there is a logical order in which the individual cluster Operators must be updated to minimize disruption in the cluster. The Cluster Version Operator (CVO) implements this logical order through the concept of Runlevels.
				</p><p>
					These dependencies are encoded in the filenames of the manifests in the release image:
				</p><pre class="programlisting language-terminal">0000_&lt;runlevel&gt;_&lt;component&gt;_&lt;manifest-name&gt;.yaml</pre><p>
					For example:
				</p><pre class="programlisting language-terminal">0000_03_config-operator_01_proxy.crd.yaml</pre><p>
					The CVO internally builds a dependency graph for the manifests, where the CVO obeys the following rules:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							During an update, manifests at a lower Runlevel are applied before those at a higher Runlevel.
						</li><li class="listitem">
							Within one Runlevel, manifests for different components can be applied in parallel.
						</li><li class="listitem">
							Within one Runlevel, manifests for a single component are applied in lexicographic order.
						</li></ul></div><p>
					The CVO then applies manifests following the generated dependency graph.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						For some resource types, the CVO monitors the resource after its manifest is applied, and considers it to be successfully updated only after the resource reaches a stable state. Achieving this stable state can take some time. This is especially true for cluster Operators, which might perform their own update actions in the cluster after the CVO deploys their new versions. While the additional update actions take place, these cluster Operators temporarily set their <code class="literal">Progressing</code> condition to <code class="literal">True</code>.
					</p></div></div><p>
					The CVO waits until all cluster Operators in the Runlevel meet the following conditions before it proceeds to the next Runlevel:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The cluster Operators have an <code class="literal">Available=True</code> condition.
						</li><li class="listitem">
							The cluster Operators have a <code class="literal">Degraded=False</code> condition.
						</li><li class="listitem">
							The cluster Operators declare they have achieved the desired version in their ClusterOperator resource.
						</li></ul></div><p>
					Some actions can take significant time to finish. The CVO waits for the actions to complete in order to ensure the subsequent Runlevels can proceed safely. The process of applying all manifests is expected to take 60 to 120 minutes in total; see <span class="strong strong"><strong>Understanding OpenShift Container Platform update duration</strong></span> for more information about factors that influence update duration.
				</p><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Updating_clusters-en-US/images/a66f5c8865f4181d7104390841e3c9ba/update-runlevels.png" alt="A diagram displaying the sequence of Runlevels and the manifests of components within each level"/></div></div><p>
					In the previous example diagram, the CVO is waiting until all work is completed at Runlevel 20. The CVO has applied all manifests to the Operators in the Runlevel, but the <code class="literal">kube-apiserver-operator ClusterOperator</code> performs some actions after its new version was deployed. The <code class="literal">kube-apiserver-operator ClusterOperator</code> declares this progress through the <code class="literal">Progressing=True</code> condition and by not declaring the new version as reconciled in its <code class="literal">status.versions</code>. The CVO waits until the ClusterOperator reports an acceptable status, and then it will start applying manifests at Runlevel 25.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#understanding-openshift-update-duration">Understanding OpenShift Container Platform update duration</a>
						</li></ul></div></section><section class="section" id="mco-update-process_how-updates-work"><div class="titlepage"><div><div><h3 class="title">2.2.5. Understanding how the Machine Config Operator updates nodes</h3></div></div></div><p>
					The Machine Config Operator (MCO) applies a new machine configuration to each control plane node and compute node. During the machine configuration update, control plane nodes and compute nodes are organized into their own machine config pools, where the pools of machines are updated in parallel. The <code class="literal">.spec.maxUnavailable</code> parameter, which has a default value of <code class="literal">1</code>, determines how many nodes in a machine config pool can simultaneously undergo the update process.
				</p><p>
					When the machine configuration update process begins, the MCO checks the amount of currently unavailable nodes in a pool. If there are fewer unavailable nodes than the value of <code class="literal">.spec.maxUnavailable</code>, the MCO initiates the following sequence of actions on available nodes in the pool:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Cordon and drain the node
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								When a node is cordoned, workloads cannot be scheduled to it.
							</p></div></div></li><li class="listitem">
							Update the system configuration and operating system (OS) of the node
						</li><li class="listitem">
							Reboot the node
						</li><li class="listitem">
							Uncordon the node
						</li></ol></div><p>
					A node undergoing this process is unavailable until it is uncordoned and workloads can be scheduled to it again. The MCO begins updating nodes until the number of unavailable nodes is equal to the value of <code class="literal">.spec.maxUnavailable</code>.
				</p><p>
					As a node completes its update and becomes available, the number of unavailable nodes in the machine config pool is once again fewer than <code class="literal">.spec.maxUnavailable</code>. If there are remaining nodes that need to be updated, the MCO initiates the update process on a node until the <code class="literal">.spec.maxUnavailable</code> limit is once again reached. This process repeats until each control plane node and compute node has been updated.
				</p><p>
					The following example workflow describes how this process might occur in a machine config pool with 5 nodes, where <code class="literal">.spec.maxUnavailable</code> is 3 and all nodes are initially available:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							The MCO cordons nodes 1, 2, and 3, and begins to drain them.
						</li><li class="listitem">
							Node 2 finishes draining, reboots, and becomes available again. The MCO cordons node 4 and begins draining it.
						</li><li class="listitem">
							Node 1 finishes draining, reboots, and becomes available again. The MCO cordons node 5 and begins draining it.
						</li><li class="listitem">
							Node 3 finishes draining, reboots, and becomes available again.
						</li><li class="listitem">
							Node 5 finishes draining, reboots, and becomes available again.
						</li><li class="listitem">
							Node 4 finishes draining, reboots, and becomes available again.
						</li></ol></div><p>
					Because the update process for each node is independent of other nodes, some nodes in the example above finish their update out of the order in which they were cordoned by the MCO.
				</p><p>
					You can check the status of the machine configuration update by running the following command:
				</p><pre class="programlisting language-terminal">$ oc get mcp</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">NAME         CONFIG                                                 UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master       rendered-master-acd1358917e9f98cbdb599aea622d78b       True      False      False      3              3                   3                     0                      22h
worker       rendered-worker-1d871ac76e1951d32b2fe92369879826       False     True       False      2              1                   1                     0                      22h</pre>

					</p></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#machine-config-overview-post-install-machine-configuration-tasks">Machine config overview</a>
						</li></ul></div></section></section></section><section class="chapter" id="understanding-upgrade-channels-releases"><div class="titlepage"><div><div><h1 class="title">Chapter 3. Understanding update channels and releases</h1></div></div></div><p>
			Update channels are the mechanism by which users declare the OpenShift Container Platform minor version they intend to update their clusters to. They also allow users to choose the timing and level of support their updates will have through the <code class="literal">fast</code>, <code class="literal">stable</code>, <code class="literal">candidate</code>, and <code class="literal">eus</code> channel options. The Cluster Version Operator uses an update graph based on the channel declaration, along with other conditional information, to provide a list of recommended and conditional updates available to the cluster.
		</p><p>
			Update channels correspond to a minor version of OpenShift Container Platform. The version number in the channel represents the target minor version that the cluster will eventually be updated to, even if it is higher than the cluster’s current minor version.
		</p><p>
			For instance, OpenShift Container Platform 4.10 update channels provide the following recommendations:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Updates within 4.10.
				</li><li class="listitem">
					Updates within 4.9.
				</li><li class="listitem">
					Updates from 4.9 to 4.10, allowing all 4.9 clusters to eventually update to 4.10, even if they do not immediately meet the minimum z-stream version requirements.
				</li><li class="listitem">
					<code class="literal">eus-4.10</code> only: updates within 4.8.
				</li><li class="listitem">
					<code class="literal">eus-4.10</code> only: updates from 4.8 to 4.9 to 4.10, allowing all 4.8 clusters to eventually update to 4.10.
				</li></ul></div><p>
			4.10 update channels do not recommend updates to 4.11 or later releases. This strategy ensures that administrators must explicitly decide to update to the next minor version of OpenShift Container Platform.
		</p><p>
			Update channels control only release selection and do not impact the version of the cluster that you install. The <code class="literal">openshift-install</code> binary file for a specific version of OpenShift Container Platform always installs that version.
		</p><p>
			OpenShift Container Platform 4.13 offers the following update channels:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<code class="literal">stable-4.13</code>
				</li><li class="listitem">
					<code class="literal">eus-4.y</code> (only offered for EUS versions and meant to facilitate updates between EUS versions)
				</li><li class="listitem">
					<code class="literal">fast-4.13</code>
				</li><li class="listitem">
					<code class="literal">candidate-4.13</code>
				</li></ul></div><p>
			If you do not want the Cluster Version Operator to fetch available updates from the update recommendation service, you can use the <code class="literal">oc adm upgrade channel</code> command in the OpenShift CLI to configure an empty channel. This configuration can be helpful if, for example, a cluster has restricted network access and there is no local, reachable update recommendation service.
		</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
				Red Hat recommends updating to versions suggested by OpenShift Update Service only. For a minor version update, versions must be contiguous. Red Hat does not test updates to noncontiguous versions and cannot guarantee compatibility with earlier versions.
			</p></div></div><section class="section" id="understanding-upgrade-channels_understanding-upgrade-channels-releases"><div class="titlepage"><div><div><h2 class="title">3.1. Update channels</h2></div></div></div><section class="section" id="fast-version-channel_understanding-upgrade-channels-releases"><div class="titlepage"><div><div><h3 class="title">3.1.1. fast-4.13 channel</h3></div></div></div><p>
					The <code class="literal">fast-4.13</code> channel is updated with new versions of OpenShift Container Platform 4.13 as soon as Red Hat declares the version as a general availability (GA) release. As such, these releases are fully supported and purposed to be used in production environments.
				</p></section><section class="section" id="stable-version-channel_understanding-upgrade-channels-releases"><div class="titlepage"><div><div><h3 class="title">3.1.2. stable-4.13 channel</h3></div></div></div><p>
					While the <code class="literal">fast-4.13</code> channel contains releases as soon as their errata are published, releases are added to the <code class="literal">stable-4.13</code> channel after a delay. During this delay, data is collected from multiple sources and analyzed for indications of product regressions. Once a significant number of data points have been collected, and absent negative signals, these releases are added to the stable channel.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Since the time required to obtain a significant number of data points varies based on many factors, Service LeveL Objective (SLO) is not offered for the delay duration between fast and stable channels. For more information, please see "Choosing the correct channel for your cluster"
					</p></div></div><p>
					Newly installed clusters default to using stable channels.
				</p></section><section class="section" id="eus-4y-channel_understanding-upgrade-channels-releases"><div class="titlepage"><div><div><h3 class="title">3.1.3. eus-4.y channel</h3></div></div></div><p>
					In addition to the stable channel, all even-numbered minor versions of OpenShift Container Platform offer <a class="link" href="https://access.redhat.com/support/policy/updates/openshift#ocp4_phases">Extended Update Support</a> (EUS). Releases promoted to the stable channel are also simultaneously promoted to the EUS channels. The primary purpose of the EUS channels is to serve as a convenience for clusters performing an EUS-to-EUS update.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Both standard and non-EUS subscribers can access all EUS repositories and necessary RPMs (<code class="literal">rhel-*-eus-rpms</code>) to be able to support critical purposes such as debugging and building drivers.
					</p></div></div></section><section class="section" id="candidate-version-channel_understanding-upgrade-channels-releases"><div class="titlepage"><div><div><h3 class="title">3.1.4. candidate-4.13 channel</h3></div></div></div><p>
					The <code class="literal">candidate-4.13</code> channel offers unsupported early access to releases as soon as they are built. Releases present only in candidate channels may not contain the full feature set of eventual GA releases or features may be removed prior to GA. Additionally, these releases have not been subject to full Red Hat Quality Assurance and may not offer update paths to later GA releases. Given these caveats, the candidate channel is only suitable for testing purposes where destroying and recreating a cluster is acceptable.
				</p></section><section class="section" id="upgrade-version-paths_understanding-upgrade-channels-releases"><div class="titlepage"><div><div><h3 class="title">3.1.5. Update recommendations in the channel</h3></div></div></div><p>
					OpenShift Container Platform maintains an update recommendation service that knows your installed OpenShift Container Platform version and the path to take within the channel to get you to the next release. Update paths are also limited to versions relevant to your currently selected channel and its promotion characteristics.
				</p><p>
					You can imagine seeing the following releases in your channel:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							4.13.0
						</li><li class="listitem">
							4.13.1
						</li><li class="listitem">
							4.13.3
						</li><li class="listitem">
							4.13.4
						</li></ul></div><p>
					The service recommends only updates that have been tested and have no known serious regressions. For example, if your cluster is on 4.13.1 and OpenShift Container Platform suggests 4.13.4, then it is recommended to update from 4.13.1 to 4.13.4.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Do not rely on consecutive patch numbers. In this example, 4.13.2 is not and never was available in the channel, therefore updates to 4.13.2 are not recommended or supported.
					</p></div></div></section><section class="section" id="conditional-updates-overview_understanding-upgrade-channels-releases"><div class="titlepage"><div><div><h3 class="title">3.1.6. Update recommendations and Conditional Updates</h3></div></div></div><p>
					Red Hat monitors newly released versions and update paths associated with those versions before and after they are added to supported channels.
				</p><p>
					If Red Hat removes update recommendations from any supported release, a superseding update recommendation will be provided to a future version that corrects the regression. There may however be a delay while the defect is corrected, tested, and promoted to your selected channel.
				</p><p>
					Beginning in OpenShift Container Platform 4.10, when update risks are confirmed, they are declared as Conditional Update risks for the relevant updates. Each known risk may apply to all clusters or only clusters matching certain conditions. Some examples include having the <code class="literal">Platform</code> set to <code class="literal">None</code> or the CNI provider set to <code class="literal">OpenShiftSDN</code>. The Cluster Version Operator (CVO) continually evaluates known risks against the current cluster state. If no risks match, the update is recommended. If the risk matches, those updates are supported but not recommended, and a reference link is provided. The reference link helps the cluster admin decide if they would like to accept the risk and update anyway.
				</p><p>
					When Red Hat chooses to declare Conditional Update risks, that action is taken in all relevant channels simultaneously. Declaration of a Conditional Update risk may happen either before or after the update has been promoted to supported channels.
				</p></section><section class="section" id="fast-stable-channel-strategies_understanding-upgrade-channels-releases"><div class="titlepage"><div><div><h3 class="title">3.1.7. Choosing the correct channel for your cluster</h3></div></div></div><p>
					Choosing the appropriate channel involves two decisions.
				</p><p>
					First, select the minor version you want for your cluster update. Selecting a channel which matches your current version ensures that you only apply z-stream updates and do not receive feature updates. Selecting an available channel which has a version greater than your current version will ensure that after one or more updates your cluster will have updated to that version. Your cluster will only be offered channels which match its current version, the next version, or the next EUS version.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Due to the complexity involved in planning updates between versions many minors apart, channels that assist in planning updates beyond a single EUS-to-EUS update are not offered.
					</p></div></div><p>
					Second, you should choose your desired rollout strategy. You may choose to update as soon as Red Hat declares a release GA by selecting from fast channels or you may want to wait for Red Hat to promote releases to the stable channel. Update recommendations offered in the <code class="literal">fast-4.13</code> and <code class="literal">stable-4.13</code> are both fully supported and benefit equally from ongoing data analysis. The promotion delay before promoting a release to the stable channel represents the only difference between the two channels. Updates to the latest z-streams are generally promoted to the stable channel within a week or two, however the delay when initially rolling out updates to the latest minor is much longer, generally 45-90 days. Please consider the promotion delay when choosing your desired channel, as waiting for promotion to the stable channel may affect your scheduling plans.
				</p><p>
					Additionally, there are several factors which may lead an organization to move clusters to the fast channel either permanently or temporarily including:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The desire to apply a specific fix known to affect your environment without delay.
						</li><li class="listitem">
							Application of CVE fixes without delay. CVE fixes may introduce regressions, so promotion delays still apply to z-streams with CVE fixes.
						</li><li class="listitem">
							Internal testing processes. If it takes your organization several weeks to qualify releases it is best test concurrently with our promotion process rather than waiting. This also assures that any telemetry signal provided to Red Hat is a factored into our rollout, so issues relevant to you can be fixed faster.
						</li></ul></div></section><section class="section" id="restricted-network-clusters_understanding-upgrade-channels-releases"><div class="titlepage"><div><div><h3 class="title">3.1.8. Restricted network clusters</h3></div></div></div><p>
					If you manage the container images for your OpenShift Container Platform clusters yourself, you must consult the Red Hat errata that is associated with product releases and note any comments that impact updates. During an update, the user interface might warn you about switching between these versions, so you must ensure that you selected an appropriate version before you bypass those warnings.
				</p></section><section class="section" id="switching-between-channels_understanding-upgrade-channels-releases"><div class="titlepage"><div><div><h3 class="title">3.1.9. Switching between channels</h3></div></div></div><p>
					A channel can be switched from the web console or through the <code class="literal">adm upgrade channel</code> command:
				</p><pre class="programlisting language-terminal">$ oc adm upgrade channel &lt;channel&gt;</pre><p>
					The web console will display an alert if you switch to a channel that does not include the current release. The web console does not recommend any updates while on a channel without the current release. You can return to the original channel at any point, however.
				</p><p>
					Changing your channel might impact the supportability of your cluster. The following conditions might apply:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Your cluster is still supported if you change from the <code class="literal">stable-4.13</code> channel to the <code class="literal">fast-4.13</code> channel.
						</li><li class="listitem">
							You can switch to the <code class="literal">candidate-4.13</code> channel at any time, but some releases for this channel might be unsupported.
						</li><li class="listitem">
							You can switch from the <code class="literal">candidate-4.13</code> channel to the <code class="literal">fast-4.13</code> channel if your current release is a general availability release.
						</li><li class="listitem">
							You can always switch from the <code class="literal">fast-4.13</code> channel to the <code class="literal">stable-4.13</code> channel. There is a possible delay of up to a day for the release to be promoted to <code class="literal">stable-4.13</code> if the current release was recently promoted.
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-conditional-upgrade-pathupdating-cluster-cli">Updating along a conditional upgrade path</a>
						</li><li class="listitem">
							<a class="link" href="#fast-stable-channel-strategies_understanding-upgrade-channels-releases" title="3.1.7. Choosing the correct channel for your cluster">Choosing the correct channel for your cluster</a>
						</li></ul></div></section></section></section><section class="chapter" id="understanding-openshift-update-duration"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Understanding OpenShift Container Platform update duration</h1></div></div></div><p>
			OpenShift Container Platform update duration varies based on the deployment topology. This page helps you understand the factors that affect update duration and estimate how long the cluster update takes in your environment.
		</p><section class="section" id="update-duration-prerequisites"><div class="titlepage"><div><div><h2 class="title">4.1. Prerequisites</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						You are familiar with <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/architecture/#architecture">OpenShift Container Platform architecture</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#understanding-openshift-updates">OpenShift Container Platform updates</a>.
					</li></ul></div></section><section class="section" id="factors-affecting-update-duration_openshift-update-duration"><div class="titlepage"><div><div><h2 class="title">4.2. Factors affecting update duration</h2></div></div></div><p>
				The following factors can affect your cluster update duration:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						The reboot of compute nodes to the new machine configuration by Machine Config Operator (MCO)
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								The value of <code class="literal">MaxUnavailable</code> in the machine config pool
							</li><li class="listitem">
								The minimum number or percentages of replicas set in pod disruption budget (PDB)
							</li></ul></div></li><li class="listitem">
						The number of nodes in the cluster
					</li><li class="listitem">
						The health of the cluster nodes
					</li></ul></div></section><section class="section" id="cluster-update-phases"><div class="titlepage"><div><div><h2 class="title">4.3. Cluster update phases</h2></div></div></div><p>
				In OpenShift Container Platform, the cluster update happens in two phases:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Cluster Version Operator (CVO) target update payload deployment
					</li><li class="listitem">
						Machine Config Operator (MCO) node updates
					</li></ul></div><section class="section" id="cluster-version-operator_openshift-update-duration"><div class="titlepage"><div><div><h3 class="title">4.3.1. Cluster Version Operator target update payload deployment</h3></div></div></div><p>
					The Cluster Version Operator (CVO) retrieves the target update release image and applies to the cluster. All components which run as pods are updated during this phase, whereas the host components are updated by the Machine Config Operator (MCO). This process might take 60 to 120 minutes.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The CVO phase of the update does not restart the nodes.
					</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#understanding-openshift-updates">Introduction to OpenShift Updates</a>
						</li></ul></div></section><section class="section" id="machine-config-operator-node-updates_openshift-update-duration"><div class="titlepage"><div><div><h3 class="title">4.3.2. Machine Config Operator node updates</h3></div></div></div><p>
					The Machine Config Operator (MCO) applies a new machine configuration to each control plane and compute node. During this process, the MCO performs the following sequential actions on each node of the cluster:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Cordon and drain all the nodes
						</li><li class="listitem">
							Update the operating system (OS)
						</li><li class="listitem">
							Reboot the nodes
						</li><li class="listitem">
							Uncordon all nodes and schedule workloads on the node
						</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When a node is cordoned, workloads cannot be scheduled to it.
					</p></div></div><p>
					The time to complete this process depends on several factors including the node and infrastructure configuration. This process might take 5 or more minutes to complete per node.
				</p><p>
					In addition to MCO, you should consider the impact of the following parameters:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The control plane node update duration is predictable and oftentimes shorter than compute nodes, because the control plane workloads are tuned for graceful updates and quick drains.
						</li><li class="listitem">
							You can update the compute nodes in parallel by setting the <code class="literal">maxUnavailable</code> field to greater than <code class="literal">1</code> in the Machine Config Pool (MCP). The MCO cordons the number of nodes specified in <code class="literal">maxUnavailable</code> and marks them unavailable for update.
						</li><li class="listitem">
							When you increase <code class="literal">maxUnavailable</code> on the MCP, it can help the pool to update more quickly. However, if <code class="literal">maxUnavailable</code> is set too high, and several nodes are cordoned simultaneously, the pod disruption budget (PDB) guarded workloads could fail to drain because a schedulable node cannot be found to run the replicas. If you increase <code class="literal">maxUnavailable</code> for the MCP, ensure that you still have sufficient schedulable nodes to allow PDB guarded workloads to drain.
						</li><li class="listitem"><p class="simpara">
							Before you begin the update, you must ensure that all the nodes are available. Any unavailable nodes can significantly impact the update duration because the node unavailability affects the <code class="literal">maxUnavailable</code> and pod disruption budgets.
						</p><p class="simpara">
							To check the status of nodes from the terminal, run the following command:
						</p><pre class="programlisting language-terminal">$ oc get node</pre><div class="formalpara"><p class="title"><strong>Example Output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                        STATUS                      ROLES   AGE     VERSION
ip-10-0-137-31.us-east-2.compute.internal   Ready,SchedulingDisabled    worker  12d     v1.23.5+3afdacb
ip-10-0-151-208.us-east-2.compute.internal  Ready                       master  12d     v1.23.5+3afdacb
ip-10-0-176-138.us-east-2.compute.internal  Ready                       master  12d     v1.23.5+3afdacb
ip-10-0-183-194.us-east-2.compute.internal  Ready                       worker  12d     v1.23.5+3afdacb
ip-10-0-204-102.us-east-2.compute.internal  Ready                       master  12d     v1.23.5+3afdacb
ip-10-0-207-224.us-east-2.compute.internal  Ready                       worker  12d     v1.23.5+3afdacb</pre>

							</p></div><p class="simpara">
							If the status of the node is <code class="literal">NotReady</code> or <code class="literal">SchedulingDisabled</code>, then the node is not available and this impacts the update duration.
						</p><p class="simpara">
							You can check the status of nodes from the <span class="strong strong"><strong>Administrator</strong></span> perspective in the web console by expanding <span class="strong strong"><strong>Compute</strong></span> → <span class="strong strong"><strong>Node</strong></span>.
						</p></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#machine-config-overview-post-install-machine-configuration-tasks">Machine config overview</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-configuring-pod-distruption-about_nodes-pods-configuring">Pod disruption budget</a>
						</li></ul></div></section></section><section class="section" id="estimating-cluster-update-time_openshift-update-duration"><div class="titlepage"><div><div><h2 class="title">4.4. Estimating cluster update time</h2></div></div></div><p>
				Historical update duration of similar clusters provides you the best estimate for the future cluster updates. However, if the historical data is not available, you can use the following convention to estimate your cluster update time:
			</p><pre class="screen">Cluster update time = CVO target update payload deployment time + (# node update iterations x MCO node update time)</pre><p>
				A node update iteration consists of one or more nodes updated in parallel. The control plane nodes are always updated in parallel with the compute nodes. In addition, one or more compute nodes can be updated in parallel based on the <code class="literal">maxUnavailable</code> value.
			</p><p>
				For example, to estimate the update time, consider an OpenShift Container Platform cluster with three control plane nodes and six compute nodes and each host takes about 5 minutes to reboot.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The time it takes to reboot a particular node varies significantly. In cloud instances, the reboot might take about 1 to 2 minutes, whereas in physical bare metal hosts the reboot might take more than 15 minutes.
				</p></div></div><div class="formalpara"><p class="title"><strong>Scenario-1</strong></p><p>
					When you set <code class="literal">maxUnavailable</code> to <code class="literal">1</code> for both the control plane and compute nodes Machine Config Pool (MCP), then all the six compute nodes will update one after another in each iteration:
				</p></div><pre class="screen">Cluster update time = 60 + (6 x 5) = 90 minutes</pre><div class="formalpara"><p class="title"><strong>Scenario-2</strong></p><p>
					When you set <code class="literal">maxUnavailable</code> to <code class="literal">2</code> for the compute node MCP, then two compute nodes will update in parallel in each iteration. Therefore it takes total three iterations to update all the nodes.
				</p></div><pre class="screen">Cluster update time = 60 + (3 x 5) = 75 minutes</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					The default setting for <code class="literal">maxUnavailable</code> is <code class="literal">1</code> for all the MCPs in OpenShift Container Platform. It is recommended that you do not change the <code class="literal">maxUnavailable</code> in the control plane MCP.
				</p></div></div></section><section class="section" id="redhat-enterprise-linux-nodes_openshift-update-duration"><div class="titlepage"><div><div><h2 class="title">4.5. Red Hat Enterprise Linux (RHEL) compute nodes</h2></div></div></div><p>
				Red Hat Enterprise Linux (RHEL) compute nodes require an additional usage of <code class="literal">openshift-ansible</code> to update node binary components. The actual time spent updating RHEL compute nodes should not be significantly different from Red Hat Enterprise Linux CoreOS (RHCOS) compute nodes.
			</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-cluster-rhel-compute">Updating RHEL compute machines</a>
					</li></ul></div></section></section><section class="chapter" id="updating-cluster-prepare"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Preparing to update to OpenShift Container Platform 4.13</h1></div></div></div><p>
			Learn more about administrative tasks that cluster admins must perform to successfully initialize an update, as well as optional guidelines for ensuring a successful update.
		</p><section class="section" id="kube-api-removals_updating-cluster-prepare"><div class="titlepage"><div><div><h2 class="title">5.1. Kubernetes API deprecations and removals</h2></div></div></div><p>
				OpenShift Container Platform 4.13 uses Kubernetes 1.26, which removed several deprecated APIs.
			</p><p>
				A cluster administrator must provide a manual acknowledgment before the cluster can be updated from OpenShift Container Platform 4.12 to 4.13. This is to help prevent issues after upgrading to OpenShift Container Platform 4.13, where APIs that have been removed are still in use by workloads, tools, or other components running on or interacting with the cluster. Administrators must evaluate their cluster for any APIs in use that will be removed and migrate the affected components to use the appropriate new API version. After this evaluation and migration is complete, the administrator can provide the acknowledgment.
			</p><p>
				Before you can update your OpenShift Container Platform 4.12 cluster to 4.13, you must provide the administrator acknowledgment.
			</p><section class="section" id="update-preparing-list_updating-cluster-prepare"><div class="titlepage"><div><div><h3 class="title">5.1.1. Removed Kubernetes APIs</h3></div></div></div><p>
					OpenShift Container Platform 4.13 uses Kubernetes 1.26, which removed the following deprecated APIs. You must migrate manifests and API clients to use the appropriate API version. For more information about migrating removed APIs, see the <a class="link" href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-26">Kubernetes documentation</a>.
				</p><div class="table" id="idm140049053835808"><p class="title"><strong>Table 5.1. APIs removed from Kubernetes 1.26</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140049057873760" scope="col">Resource</th><th align="left" valign="top" id="idm140049057872672" scope="col">Removed API</th><th align="left" valign="top" id="idm140049057871584" scope="col">Migrate to</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140049057873760"> <p>
									<code class="literal">FlowSchema</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140049057872672"> <p>
									<code class="literal">flowcontrol.apiserver.k8s.io/v1beta1</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140049057871584"> <p>
									<code class="literal">flowcontrol.apiserver.k8s.io/v1beta3</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140049057873760"> <p>
									<code class="literal">HorizontalPodAutoscaler</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140049057872672"> <p>
									<code class="literal">autoscaling/v2beta2</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140049057871584"> <p>
									<code class="literal">autoscaling/v2</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140049057873760"> <p>
									<code class="literal">PriorityLevelConfiguration</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140049057872672"> <p>
									<code class="literal">flowcontrol.apiserver.k8s.io/v1beta1</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140049057871584"> <p>
									<code class="literal">flowcontrol.apiserver.k8s.io/v1beta3</code>
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section" id="evaluating-cluster-removed-apis"><div class="titlepage"><div><div><h3 class="title">5.1.2. Evaluating your cluster for removed APIs</h3></div></div></div><p>
					There are several methods to help administrators identify where APIs that will be removed are in use. However, OpenShift Container Platform cannot identify all instances, especially workloads that are idle or external tools that are used. It is the responsibility of the administrator to properly evaluate all workloads and other integrations for instances of removed APIs.
				</p><section class="section" id="update-preparing-evaluate-alerts_updating-cluster-prepare"><div class="titlepage"><div><div><h4 class="title">5.1.2.1. Reviewing alerts to identify uses of removed APIs</h4></div></div></div><p>
						Two alerts fire when an API is in use that will be removed in the next release:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<code class="literal">APIRemovedInNextReleaseInUse</code> - for APIs that will be removed in the next OpenShift Container Platform release.
							</li><li class="listitem">
								<code class="literal">APIRemovedInNextEUSReleaseInUse</code> - for APIs that will be removed in the next OpenShift Container Platform Extended Update Support (EUS) release.
							</li></ul></div><p>
						If either of these alerts are firing in your cluster, review the alerts and take action to clear the alerts by migrating manifests and API clients to use the new API version.
					</p><p>
						Use the <code class="literal">APIRequestCount</code> API to get more information about which APIs are in use and which workloads are using removed APIs, because the alerts do not provide this information. Additionally, some APIs might not trigger these alerts but are still captured by <code class="literal">APIRequestCount</code>. The alerts are tuned to be less sensitive to avoid alerting fatigue in production systems.
					</p></section><section class="section" id="update-preparing-evaluate-apirequestcount_updating-cluster-prepare"><div class="titlepage"><div><div><h4 class="title">5.1.2.2. Using APIRequestCount to identify uses of removed APIs</h4></div></div></div><p>
						You can use the <code class="literal">APIRequestCount</code> API to track API requests and review whether any of them are using one of the removed APIs.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You must have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Run the following command and examine the <code class="literal">REMOVEDINRELEASE</code> column of the output to identify the removed APIs that are currently in use:
							</p><pre class="programlisting language-terminal">$ oc get apirequestcounts</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                                                                      REMOVEDINRELEASE   REQUESTSINCURRENTHOUR   REQUESTSINLAST24H
...
flowschemas.v1beta1.flowcontrol.apiserver.k8s.io                          1.26               0                       16
flowschemas.v1beta2.flowcontrol.apiserver.k8s.io                                             101                     857
groups.v1.user.openshift.io                                                                  22                      201
hardwaredata.v1alpha1.metal3.io                                                              3                       33
helmchartrepositories.v1beta1.helm.openshift.io                                              142                     628
horizontalpodautoscalers.v2.autoscaling                                                      11                      103
horizontalpodautoscalers.v2beta2.autoscaling                              1.26               0                       15
...</pre>

								</p></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									You can safely ignore the following entries that appear in the results:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
											The <code class="literal">system:serviceaccount:kube-system:generic-garbage-collector</code> and the <code class="literal">system:serviceaccount:kube-system:namespace-controller</code> users might appear in the results because these services invoke all registered APIs when searching for resources to remove.
										</li><li class="listitem">
											The <code class="literal">system:kube-controller-manager</code> and <code class="literal">system:cluster-policy-controller</code> users might appear in the results because they walk through all resources while enforcing various policies.
										</li></ul></div></div></div><p class="simpara">
								You can also use <code class="literal">-o jsonpath</code> to filter the results:
							</p><pre class="programlisting language-terminal">$ oc get apirequestcounts -o jsonpath='{range .items[?(@.status.removedInRelease!="")]}{.status.removedInRelease}{"\t"}{.metadata.name}{"\n"}{end}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">1.26	flowschemas.v1beta1.flowcontrol.apiserver.k8s.io
1.26	horizontalpodautoscalers.v2beta2.autoscaling</pre>

								</p></div></li></ul></div></section><section class="section" id="update-preparing-evaluate-apirequestcount-workloads_updating-cluster-prepare"><div class="titlepage"><div><div><h4 class="title">5.1.2.3. Using APIRequestCount to identify which workloads are using the removed APIs</h4></div></div></div><p>
						You can examine the <code class="literal">APIRequestCount</code> resource for a given API version to help identify which workloads are using the API.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You must have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Run the following command and examine the <code class="literal">username</code> and <code class="literal">userAgent</code> fields to help identify the workloads that are using the API:
							</p><pre class="programlisting language-terminal">$ oc get apirequestcounts &lt;resource&gt;.&lt;version&gt;.&lt;group&gt; -o yaml</pre><p class="simpara">
								For example:
							</p><pre class="programlisting language-terminal">$ oc get apirequestcounts flowschemas.v1beta1.flowcontrol.apiserver.k8s.io -o yaml</pre><p class="simpara">
								You can also use <code class="literal">-o jsonpath</code> to extract the <code class="literal">username</code> and <code class="literal">userAgent</code> values from an <code class="literal">APIRequestCount</code> resource:
							</p><pre class="programlisting language-terminal">$ oc get apirequestcounts flowschemas.v1beta1.flowcontrol.apiserver.k8s.io \
  -o jsonpath='{range .status.currentHour..byUser[*]}{..byVerb[*].verb}{","}{.username}{","}{.userAgent}{"\n"}{end}' \
  | sort -k 2 -t, -u | column -t -s, -NVERBS,USERNAME,USERAGENT</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">VERBS  USERNAME                                                            USERAGENT
get    system:serviceaccount:openshift-cluster-version:default             cluster-version-operator/v0.0.0
watch  system:serviceaccount:openshift-oauth-apiserver:oauth-apiserver-sa  oauth-apiserver/v0.0.0</pre>

								</p></div></li></ul></div></section></section><section class="section" id="update-preparing-migrate_updating-cluster-prepare"><div class="titlepage"><div><div><h3 class="title">5.1.3. Migrating instances of removed APIs</h3></div></div></div><p>
					For information about how to migrate removed Kubernetes APIs, see the <a class="link" href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-26">Deprecated API Migration Guide</a> in the Kubernetes documentation.
				</p></section><section class="section" id="update-preparing-ack_updating-cluster-prepare"><div class="titlepage"><div><div><h3 class="title">5.1.4. Providing the administrator acknowledgment</h3></div></div></div><p>
					After you have evaluated your cluster for any removed APIs and have migrated any removed APIs, you can acknowledge that your cluster is ready to upgrade from OpenShift Container Platform 4.12 to 4.13.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Be aware that all responsibility falls on the administrator to ensure that all uses of removed APIs have been resolved and migrated as necessary before providing this administrator acknowledgment. OpenShift Container Platform can assist with the evaluation, but cannot identify all possible uses of removed APIs, especially idle workloads or external tools.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You must have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Run the following command to acknowledge that you have completed the evaluation and your cluster is ready for the Kubernetes API removals in OpenShift Container Platform 4.13:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-config patch cm admin-acks --patch '{"data":{"ack-4.12-kube-1.26-api-removals-in-4.13":"true"}}' --type=merge</pre></li></ul></div></section></section><section class="section" id="update-preparing-conditional_updating-cluster-prepare"><div class="titlepage"><div><div><h2 class="title">5.2. Assessing the risk of conditional updates</h2></div></div></div><p>
				A <span class="emphasis"><em>conditional update</em></span> is an update target that is available but not recommended due to a known risk that applies to your cluster. The Cluster Version Operator (CVO) periodically queries the OpenShift Update Service (OSUS) for the most recent data about update recommendations, and some potential update targets might have risks associated with them.
			</p><p>
				The CVO evaluates the conditional risks, and if the risks are not applicable to the cluster, then the target version is available as a recommended update path for the cluster. If the risk is determined to be applicable, or if for some reason CVO cannot evaluate the risk, then the update target is available to the cluster as a conditional update.
			</p><p>
				When you encounter a conditional update while you are trying to update to a target version, you must assess the risk of updating your cluster to that version. Generally, if you do not have a specific need to update to that target version, it is best to wait for a recommended update path from Red Hat.
			</p><p>
				However, if you have a strong reason to update to that version, for example, if you need to fix an important CVE, then the benefit of fixing the CVE might outweigh the risk of the update being problematic for your cluster. You can complete the following tasks to determine whether you agree with the Red Hat assessment of the update risk:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Complete extensive testing in a non-production environment to the extent that you are comfortable completing the update in your production environment.
					</li><li class="listitem">
						Follow the links provided in the conditional update description, investigate the bug, and determine if it is likely to cause issues for your cluster. If you need help understanding the risk, contact Red Hat Support.
					</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-evaluate-availability_how-updates-work">Evaluation of update availability</a>
					</li></ul></div></section></section><section class="chapter" id="preparing-eus-eus-upgrade"><div class="titlepage"><div><div><h1 class="title">Chapter 6. Preparing to perform an EUS-to-EUS update</h1></div></div></div><p>
			Due to fundamental Kubernetes design, all OpenShift Container Platform updates between minor versions must be serialized. You must update from OpenShift Container Platform &lt;4.y&gt; to &lt;4.y+1&gt;, and then to &lt;4.y+2&gt;. You cannot update from OpenShift Container Platform &lt;4.y&gt; to &lt;4.y+2&gt; directly. However, administrators who want to update between two Extended Update Support (EUS) versions can do so incurring only a single reboot of non-control plane hosts.
		</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				EUS-to-EUS updates are only viable between <span class="strong strong"><strong>even-numbered minor versions</strong></span> of OpenShift Container Platform.
			</p></div></div><p>
			There are a number of caveats to consider when attempting an EUS-to-EUS update.
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					EUS-to-EUS updates are only offered after updates between all versions involved have been made available in <code class="literal">stable</code> channels.
				</li><li class="listitem">
					If you encounter issues during or after upgrading to the odd-numbered minor version but before upgrading to the next even-numbered version, then remediation of those issues may require that non-control plane hosts complete the update to the odd-numbered version before moving forward.
				</li><li class="listitem">
					You can do a partial update by updating the worker or custom pool nodes to accommodate the time it takes for maintenance.
				</li><li class="listitem">
					You can complete the update process during multiple maintenance windows by pausing at intermediate steps. However, plan to complete the entire update within 60 days. This is critical to ensure that normal cluster automation processes are completed.
				</li><li class="listitem">
					Until the machine config pools are unpaused and the update is complete, some features and bugs fixes in &lt;4.y+1&gt; and &lt;4.y+2&gt; of OpenShift Container Platform are not available.
				</li><li class="listitem">
					All the clusters might update using EUS channels for a conventional update without pools paused, but only clusters with non control-plane <code class="literal">MachineConfigPools</code> objects can do EUS-to-EUS update with pools paused.
				</li></ul></div><section class="section" id="updating-eus-to-eus-upgrade_eus-to-eus-upgrade"><div class="titlepage"><div><div><h2 class="title">6.1. EUS-to-EUS update</h2></div></div></div><p>
				The following procedure pauses all non-master machine config pools and performs updates from OpenShift Container Platform &lt;4.y&gt; to &lt;4.y+1&gt; to &lt;4.y+2&gt;, then unpauses the previously paused machine config pools. Following this procedure reduces the total update duration and the number of times worker nodes are restarted.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Review the release notes for OpenShift Container Platform &lt;4.y+1&gt; and &lt;4.y+2&gt;
					</li><li class="listitem">
						Review the release notes and product lifecycles for any layered products and Operator Lifecycle Manager (OLM) Operators. Some may require updates either before or during an EUS-to-EUS update.
					</li><li class="listitem">
						Ensure that you are familiar with version-specific prerequisites, such as the removal of deprecated APIs, that are required prior to updating from OpenShift Container Platform &lt;4.y+1&gt; to &lt;4.y+2&gt;.
					</li></ul></div><section class="section" id="updating-eus-to-eus-upgrade-console_eus-to-eus-upgrade"><div class="titlepage"><div><div><h3 class="title">6.1.1. EUS-to-EUS update using the web console</h3></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Verify that machine config pools are unpaused.
						</li><li class="listitem">
							Have access to the web console as a user with <code class="literal">admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Using the Administrator perspective on the web console, update any Operator Lifecycle Manager (OLM) Operators to the versions that are compatible with your intended updated version. You can find more information on how to perform this action in "Updating installed Operators"; see "Additional resources".
						</li><li class="listitem"><p class="simpara">
							Verify that all machine config pools display a status of <code class="literal">Up to date</code> and that no machine config pool displays a status of <code class="literal">UPDATING</code>.
						</p><p class="simpara">
							To view the status of all machine config pools, click <span class="strong strong"><strong>Compute</strong></span> → <span class="strong strong"><strong>MachineConfigPools</strong></span> and review the contents of the <span class="strong strong"><strong>Update status</strong></span> column.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If your machine config pools have an <code class="literal">Updating</code> status, please wait for this status to change to <code class="literal">Up to date</code>. This process could take several minutes.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Set your channel to <code class="literal">eus-&lt;4.y+2&gt;</code>.
						</p><p class="simpara">
							To set your channel, click <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Cluster Settings</strong></span> → <span class="strong strong"><strong>Channel</strong></span>. You can edit your channel by clicking on the current hyperlinked channel.
						</p></li><li class="listitem">
							Pause all worker machine pools except for the master pool. You can perform this action on the <span class="strong strong"><strong>MachineConfigPools</strong></span> tab under the <span class="strong strong"><strong>Compute</strong></span> page. Select the vertical ellipses next to the machine config pool you’d like to pause and click <span class="strong strong"><strong>Pause updates</strong></span>.
						</li><li class="listitem">
							Update to version &lt;4.y+1&gt; and complete up to the <span class="strong strong"><strong>Save</strong></span> step. You can find more information on how to perform these actions in "Updating a cluster by using the web console"; see "Additional resources".
						</li><li class="listitem">
							Ensure that the &lt;4.y+1&gt; updates are complete by viewing the <span class="strong strong"><strong>Last completed version</strong></span> of your cluster. You can find this information on the <span class="strong strong"><strong>Cluster Settings</strong></span> page under the <span class="strong strong"><strong>Details</strong></span> tab.
						</li><li class="listitem">
							If necessary, update your OLM Operators by using the Administrator perspective on the web console. You can find more information on how to perform these actions in "Updating installed Operators"; see "Additional resources".
						</li><li class="listitem">
							Update to version &lt;4.y+2&gt; and complete up to the <span class="strong strong"><strong>Save</strong></span> step. You can find more information on how to perform these actions in "Updating a cluster by using the web console"; see "Additional resources".
						</li><li class="listitem">
							Ensure that the &lt;4.y+2&gt; update is complete by viewing the <span class="strong strong"><strong>Last completed version</strong></span> of your cluster. You can find this information on the <span class="strong strong"><strong>Cluster Settings</strong></span> page under the <span class="strong strong"><strong>Details</strong></span> tab.
						</li><li class="listitem"><p class="simpara">
							Unpause all previously paused machine config pools. You can perform this action on the <span class="strong strong"><strong>MachineConfigPools</strong></span> tab under the <span class="strong strong"><strong>Compute</strong></span> page. Select the vertical ellipses next to the machine config pool you’d like to unpause and click <span class="strong strong"><strong>Unpause updates</strong></span>.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								If pools are paused, the cluster is not permitted to upgrade to any future minor versions, and some maintenance tasks are inhibited. This puts the cluster at risk for future degradation.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Verify that your previously paused pools are updated and that your cluster has completed the update to version &lt;4.y+2&gt;.
						</p><p class="simpara">
							You can verify that your pools have updated on the <span class="strong strong"><strong>MachineConfigPools</strong></span> tab under the <span class="strong strong"><strong>Compute</strong></span> page by confirming that the <span class="strong strong"><strong>Update status</strong></span> has a value of <span class="strong strong"><strong>Up to date</strong></span>.
						</p><p class="simpara">
							You can verify that your cluster has completed the update by viewing the <span class="strong strong"><strong>Last completed version</strong></span> of your cluster. You can find this information on the <span class="strong strong"><strong>Cluster Settings</strong></span> page under the <span class="strong strong"><strong>Details</strong></span> tab.
						</p></li></ol></div><div class="itemizedlist _additional-resources" id="additional-resources_updating-eus-to-eus-upgrade-console"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-changing-update-channel_olm-upgrading-operators">Preparing for an Operator update</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-upgrading-web_updating-cluster-within-minor">Updating a cluster by using the web console</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-upgrading-operators">Updating installed Operators</a>
						</li></ul></div></section><section class="section" id="updating-eus-to-eus-upgrade-cli_eus-to-eus-upgrade"><div class="titlepage"><div><div><h3 class="title">6.1.2. EUS-to-EUS update using the CLI</h3></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Verify that machine config pools are unpaused.
						</li><li class="listitem">
							Update the OpenShift CLI (<code class="literal">oc</code>) to the target version before each update.
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						It is highly discouraged to skip this prerequisite. If the OpenShift CLI (<code class="literal">oc</code>) is not updated to the target version before your update, unexpected issues may occur.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Using the Administrator perspective on the web console, update any Operator Lifecycle Manager (OLM) Operators to the versions that are compatible with your intended updated version. You can find more information on how to perform this action in "Updating installed Operators"; see "Additional resources".
						</li><li class="listitem"><p class="simpara">
							Verify that all machine config pools display a status of <code class="literal">UPDATED</code> and that no machine config pool displays a status of <code class="literal">UPDATING</code>. To view the status of all machine config pools, run the following command:
						</p><pre class="programlisting language-terminal">$ oc get mcp</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME     CONFIG                                         	UPDATED   UPDATING
master   rendered-master-ecbb9582781c1091e1c9f19d50cf836c       True  	  False
worker   rendered-worker-00a3f0c68ae94e747193156b491553d5       True  	  False</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Your current version is &lt;4.y&gt;, and your intended version to update is &lt;4.y+2&gt;. Change to the <code class="literal">eus-&lt;4.y+2&gt;</code> channel by running the following command:
						</p><pre class="programlisting language-terminal">$ oc adm upgrade channel eus-&lt;4.y+2&gt;</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If you receive an error message indicating that <code class="literal">eus-&lt;4.y+2&gt;</code> is not one of the available channels, this indicates that Red Hat is still rolling out EUS version updates. This rollout process generally takes 45-90 days starting at the GA date.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Pause all worker machine pools except for the master pool by running the following command:
						</p><pre class="programlisting language-terminal">$ oc patch mcp/worker --type merge --patch '{"spec":{"paused":true}}'</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								You cannot pause the master pool.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Update to the latest version by running the following command:
						</p><pre class="programlisting language-terminal">$ oc adm upgrade --to-latest</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Updating to latest version &lt;4.y+1.z&gt;</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Review the cluster version to ensure that the updates are complete by running the following command:
						</p><pre class="programlisting language-terminal">$ oc adm upgrade</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Cluster version is &lt;4.y+1.z&gt;
...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Update to version &lt;4.y+2&gt; by running the following command:
						</p><pre class="programlisting language-terminal">$ oc adm upgrade --to-latest</pre></li><li class="listitem"><p class="simpara">
							Retrieve the cluster version to ensure that the &lt;4.y+2&gt; updates are complete by running the following command:
						</p><pre class="programlisting language-terminal">$ oc adm upgrade</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Cluster version is &lt;4.y+1.z&gt;
...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To update your worker nodes to &lt;4.y+2&gt;, unpause all previously paused machine config pools by running the following command:
						</p><pre class="programlisting language-terminal">$ oc patch mcp/worker --type merge --patch '{"spec":{"paused":false}}'</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								If pools are not unpaused, the cluster is not permitted to update to any future minor versions, and some maintenance tasks are inhibited. This puts the cluster at risk for future degradation.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Verify that your previously paused pools are updated and that the update to version &lt;4.y+2&gt; is complete by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get mcp</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME 	   CONFIG                                            UPDATED     UPDATING
master   rendered-master-52da4d2760807cb2b96a3402179a9a4c    True  	 False
worker   rendered-worker-4756f60eccae96fb9dcb4c392c69d497    True 	 False</pre>

							</p></div></li></ol></div><div class="itemizedlist _additional-resources" id="additional-resources_updating-eus-to-eus-upgrade-cli"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-upgrading-operators">Updating installed Operators</a>
						</li></ul></div></section><section class="section" id="updating-eus-to-eus-olm-operators_eus-to-eus-upgrade"><div class="titlepage"><div><div><h3 class="title">6.1.3. EUS-to-EUS update for layered products and Operators installed through Operator Lifecycle Manager</h3></div></div></div><p>
					In addition to the EUS-to-EUS update steps mentioned for the web console and CLI, there are additional steps to consider when performing EUS-to-EUS updates for clusters with the following:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Layered products
						</li><li class="listitem">
							Operators installed through Operator Lifecycle Manager (OLM)
						</li></ul></div><div class="formalpara"><p class="title"><strong>What is a layered product?</strong></p><p>
						Layered products refer to products that are made of multiple underlying products that are intended to be used together and cannot be broken into individual subscriptions. For examples of layered OpenShift Container Platform products, see <a class="link" href="https://access.redhat.com/support/policy/updates/openshift/#layered">Layered Offering On OpenShift</a>.
					</p></div><p>
					As you perform an EUS-to-EUS update for the clusters of layered products and those of Operators that have been installed through OLM, you must complete the following:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Ensure that all of your Operators previously installed through OLM are updated to their latest version in their latest channel. Updating the Operators ensures that they have a valid update path when the default OperatorHub catalogs switch from the current minor version to the next during a cluster update. For information on how to update your Operators, see "Preparing for an Operator update" in "Additional resources".
						</li><li class="listitem">
							Confirm the cluster version compatibility between the current and intended Operator versions. You can verify which versions your OLM Operators are compatible with by using the <a class="link" href="https://access.redhat.com/labs/ocpouic/?operator=logging&amp;&amp;ocp_versions=4.10,4.11,4.12">Red Hat OpenShift Container Platform Operator Update Information Checker</a>.
						</li></ol></div><p>
					As an example, here are the steps to perform an EUS-to-EUS update from &lt;4.y&gt; to &lt;4.y+2&gt; for OpenShift Data Foundation (ODF). This can be done through the CLI or web console. For information on how to update clusters through your desired interface, see <span class="emphasis"><em>EUS-to-EUS update using the web console</em></span> and "EUS-to-EUS update using the CLI" in "Additional resources".
				</p><div class="orderedlist"><p class="title"><strong>Example workflow</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Pause the worker machine pools.
						</li><li class="listitem">
							Upgrade OpenShift &lt;4.y&gt; → OpenShift &lt;4.y+1&gt;.
						</li><li class="listitem">
							Upgrade ODF &lt;4.y&gt; → ODF &lt;4.y+1&gt;.
						</li><li class="listitem">
							Upgrade OpenShift &lt;4.y+1&gt; → OpenShift &lt;4.y+2&gt;.
						</li><li class="listitem">
							Upgrade to ODF &lt;4.y+2&gt;.
						</li><li class="listitem">
							Unpause the worker machine pools.
						</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The upgrade to ODF &lt;4.y+2&gt; can happen before or after worker machine pools have been unpaused.
					</p></div></div><div class="itemizedlist _additional-resources" id="additional-resources_updating-eus-to-eus-layered-products"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-preparing-upgrade_olm-upgrading-operators">Preparing for an Operator update</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-eus-to-eus-upgrade-console_eus-to-eus-upgrade">EUS-to-EUS update using the web console</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-eus-to-eus-upgrade-cli_eus-to-eus-upgrade">EUS-to-EUS update using the CLI</a>
						</li></ul></div></section></section></section><section class="chapter" id="preparing-manual-creds-update"><div class="titlepage"><div><div><h1 class="title">Chapter 7. Preparing to update a cluster with manually maintained credentials</h1></div></div></div><p>
			The Cloud Credential Operator (CCO) <code class="literal">Upgradable</code> status for a cluster with manually maintained credentials is <code class="literal">False</code> by default.
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					For minor releases, for example, from 4.12 to 4.13, this status prevents you from updating until you have addressed any updated permissions and annotated the <code class="literal">CloudCredential</code> resource to indicate that the permissions are updated as needed for the next version. This annotation changes the <code class="literal">Upgradable</code> status to <code class="literal">True</code>.
				</li><li class="listitem">
					For z-stream releases, for example, from 4.13.0 to 4.13.1, no permissions are added or changed, so the update is not blocked.
				</li></ul></div><p>
			Before updating a cluster with manually maintained credentials, you must accommodate any new or changed credentials in the release image for the version of OpenShift Container Platform you are updating to.
		</p><section class="section" id="about-manually-maintained-credentials-upgrade_preparing-manual-creds-update"><div class="titlepage"><div><div><h2 class="title">7.1. Update requirements for clusters with manually maintained credentials</h2></div></div></div><p>
				Before you update a cluster that uses manually maintained credentials with the Cloud Credential Operator (CCO), you must update the cloud provider resources for the new release.
			</p><p>
				If the cloud credential management for your cluster was configured using the CCO utility (<code class="literal">ccoctl</code>), use the <code class="literal">ccoctl</code> utility to update the resources. Clusters that were configured to use manual mode without the <code class="literal">ccoctl</code> utility require manual updates for the resources.
			</p><p>
				After updating the cloud provider resources, you must update the <code class="literal">upgradeable-to</code> annotation for the cluster to indicate that it is ready to update.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The process to update the cloud provider resources and the <code class="literal">upgradeable-to</code> annotation can only be completed by using command line tools.
				</p></div></div><section class="section" id="cco-platform-options_preparing-manual-creds-update"><div class="titlepage"><div><div><h3 class="title">7.1.1. Cloud credential configuration options and update requirements by platform type</h3></div></div></div><p>
					Some platforms only support using the CCO in one mode. For clusters that are installed on those platforms, the platform type determines the credentials update requirements.
				</p><p>
					For platforms that support using the CCO in multiple modes, you must determine which mode the cluster is configured to use and take the required actions for that configuration.
				</p><div class="figure" id="idm140049058096864"><p class="title"><strong>Figure 7.1. Credentials update requirements by platform type</strong></p><div class="figure-contents"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Updating_clusters-en-US/images/92ec474a88fe7046700f9649c8a728ca/334_OpenShift_cluster_updating_and_CCO_workflows_0523_4.11_B.png" alt="Decision tree showing the possible update paths for your cluster depending on the configured CCO credentials mode."/></div></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Red Hat OpenStack Platform (RHOSP), Red Hat Virtualization (RHV), and VMware vSphere</span></dt><dd><p class="simpara">
								These platforms do not support using the CCO in manual mode. Clusters on these platforms handle changes in cloud provider resources automatically and do not require an update to the <code class="literal">upgradeable-to</code> annotation.
							</p><p class="simpara">
								Administrators of clusters on these platforms should skip the manually maintained credentials section of the update process.
							</p></dd><dt><span class="term">Alibaba Cloud, IBM Cloud, and Nutanix</span></dt><dd><p class="simpara">
								Clusters installed on these platforms are configured using the <code class="literal">ccoctl</code> utility.
							</p><p class="simpara">
								Administrators of clusters on these platforms must take the following actions:
							</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
										Configure the <code class="literal">ccoctl</code> utility for the new release.
									</li><li class="listitem">
										Use the <code class="literal">ccoctl</code> utility to update the cloud provider resources.
									</li><li class="listitem">
										Indicate that the cluster is ready to update with the <code class="literal">upgradeable-to</code> annotation.
									</li></ol></div></dd><dt><span class="term">Microsoft Azure Stack Hub</span></dt><dd><p class="simpara">
								These clusters use manual mode with long-lived credentials and do not use the <code class="literal">ccoctl</code> utility.
							</p><p class="simpara">
								Administrators of clusters on these platforms must take the following actions:
							</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
										Manually update the cloud provider resources for the new release.
									</li><li class="listitem">
										Indicate that the cluster is ready to update with the <code class="literal">upgradeable-to</code> annotation.
									</li></ol></div></dd><dt><span class="term">Amazon Web Services (AWS), global Microsoft Azure, and Google Cloud Platform (GCP)</span></dt><dd><p class="simpara">
								Clusters installed on these platforms support multiple CCO modes.
							</p><p class="simpara">
								The required update process depends on the mode that the cluster is configured to use. If you are not sure what mode the CCO is configured to use on your cluster, you can use the web console or the CLI to determine this information.
							</p></dd></dl></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#cco-determine-mode-gui_preparing-manual-creds-update">Determining the Cloud Credential Operator mode by using the web console</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#cco-determine-mode-cli_preparing-manual-creds-update">Determining the Cloud Credential Operator mode by using the CLI</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#cco-ccoctl-configuring_preparing-manual-creds-update">Configuring the Cloud Credential Operator utility for a cluster update</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#manually-maintained-credentials-upgrade_preparing-manual-creds-update">Updating cloud provider resources with manually maintained credentials</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#about-cloud-credential-operator">About the Cloud Credential Operator</a>
						</li></ul></div></section><section class="section" id="cco-determine-mode-gui_preparing-manual-creds-update"><div class="titlepage"><div><div><h3 class="title">7.1.2. Determining the Cloud Credential Operator mode by using the web console</h3></div></div></div><p>
					You can determine what mode the Cloud Credential Operator (CCO) is configured to use by using the web console.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Only Amazon Web Services (AWS), global Microsoft Azure, and Google Cloud Platform (GCP) clusters support multiple CCO modes.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to an OpenShift Container Platform account with cluster administrator permissions.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the OpenShift Container Platform web console as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Cluster Settings</strong></span>.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>Cluster Settings</strong></span> page, select the <span class="strong strong"><strong>Configuration</strong></span> tab.
						</li><li class="listitem">
							Under <span class="strong strong"><strong>Configuration resource</strong></span>, select <span class="strong strong"><strong>CloudCredential</strong></span>.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>CloudCredential details</strong></span> page, select the <span class="strong strong"><strong>YAML</strong></span> tab.
						</li><li class="listitem"><p class="simpara">
							In the YAML block, check the value of <code class="literal">spec.credentialsMode</code>. The following values are possible, though not all are supported on all platforms:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">''</code>: The CCO is operating in the default mode. In this configuration, the CCO operates in mint or passthrough mode, depending on the credentials provided during installation.
								</li><li class="listitem">
									<code class="literal">Mint</code>: The CCO is operating in mint mode.
								</li><li class="listitem">
									<code class="literal">Passthrough</code>: The CCO is operating in passthrough mode.
								</li><li class="listitem">
									<code class="literal">Manual</code>: The CCO is operating in manual mode.
								</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								To determine the specific configuration of an AWS or GCP cluster that has a <code class="literal">spec.credentialsMode</code> of <code class="literal">''</code>, <code class="literal">Mint</code>, or <code class="literal">Manual</code>, you must investigate further.
							</p><p>
								AWS and GCP clusters support using mint mode with the root secret deleted. If the cluster is specifically configured to use mint mode or uses mint mode by default, you must determine if the root secret is present on the cluster before updating.
							</p><p>
								An AWS or GCP cluster that uses manual mode might be configured to create and manage cloud credentials from outside of the cluster using the AWS Security Token Service (STS) or GCP Workload Identity. You can determine whether your cluster uses this strategy by examining the cluster <code class="literal">Authentication</code> object.
							</p></div></div></li><li class="listitem"><p class="simpara">
							AWS or GCP clusters that use mint mode only: To determine whether the cluster is operating without the root secret, navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Secrets</strong></span> and look for the root secret for your cloud provider.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Ensure that the <span class="strong strong"><strong>Project</strong></span> dropdown is set to <span class="strong strong"><strong>All Projects</strong></span>.
							</p></div></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140049058011312" scope="col">Platform</th><th align="left" valign="top" id="idm140049058010224" scope="col">Secret name</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140049058011312"> <p>
											AWS
										</p>
										 </td><td align="left" valign="top" headers="idm140049058010224"> <p>
											<code class="literal">aws-creds</code>
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140049058011312"> <p>
											GCP
										</p>
										 </td><td align="left" valign="top" headers="idm140049058010224"> <p>
											<code class="literal">gcp-credentials</code>
										</p>
										 </td></tr></tbody></table></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									If you see one of these values, your cluster is using mint or passthrough mode with the root secret present.
								</li><li class="listitem">
									If you do not see these values, your cluster is using the CCO in mint mode with the root secret removed.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							AWS or GCP clusters that use manual mode only: To determine whether the cluster is configured to create and manage cloud credentials from outside of the cluster, you must check the cluster <code class="literal">Authentication</code> object YAML values.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Cluster Settings</strong></span>.
								</li><li class="listitem">
									On the <span class="strong strong"><strong>Cluster Settings</strong></span> page, select the <span class="strong strong"><strong>Configuration</strong></span> tab.
								</li><li class="listitem">
									Under <span class="strong strong"><strong>Configuration resource</strong></span>, select <span class="strong strong"><strong>Authentication</strong></span>.
								</li><li class="listitem">
									On the <span class="strong strong"><strong>Authentication details</strong></span> page, select the <span class="strong strong"><strong>YAML</strong></span> tab.
								</li><li class="listitem"><p class="simpara">
									In the YAML block, check the value of the <code class="literal">.spec.serviceAccountIssuer</code> parameter.
								</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											A value that contains a URL that is associated with your cloud provider indicates that the CCO is using manual mode with AWS STS or GCP Workload Identity to create and manage cloud credentials from outside of the cluster. These clusters are configured using the <code class="literal">ccoctl</code> utility.
										</li><li class="listitem">
											An empty value (<code class="literal">''</code>) indicates that the cluster is using the CCO in manual mode but was not configured using the <code class="literal">ccoctl</code> utility.
										</li></ul></div></li></ol></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							If you are updating a cluster that has the CCO operating in mint or passthrough mode and the root secret is present, you do not need to update any cloud provider resources and can continue to the next part of the update process.
						</li><li class="listitem">
							If your cluster is using the CCO in mint mode with the root secret removed, you must reinstate the credential secret with the administrator-level credential before continuing to the next part of the update process.
						</li><li class="listitem"><p class="simpara">
							If your cluster was configured using the CCO utility (<code class="literal">ccoctl</code>), you must take the following actions:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Configure the <code class="literal">ccoctl</code> utility for the new release and use it to update the cloud provider resources.
								</li><li class="listitem">
									Update the <code class="literal">upgradeable-to</code> annotation to indicate that the cluster is ready to update.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							If your cluster is using the CCO in manual mode but was not configured using the <code class="literal">ccoctl</code> utility, you must take the following actions:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Manually update the cloud provider resources for the new release.
								</li><li class="listitem">
									Update the <code class="literal">upgradeable-to</code> annotation to indicate that the cluster is ready to update.
								</li></ol></div></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#cco-ccoctl-configuring_preparing-manual-creds-update">Configuring the Cloud Credential Operator utility for a cluster update</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#manually-maintained-credentials-upgrade_preparing-manual-creds-update">Updating cloud provider resources with manually maintained credentials</a>
						</li></ul></div></section><section class="section" id="cco-determine-mode-cli_preparing-manual-creds-update"><div class="titlepage"><div><div><h3 class="title">7.1.3. Determining the Cloud Credential Operator mode by using the CLI</h3></div></div></div><p>
					You can determine what mode the Cloud Credential Operator (CCO) is configured to use by using the CLI.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Only Amazon Web Services (AWS), global Microsoft Azure, and Google Cloud Platform (GCP) clusters support multiple CCO modes.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to an OpenShift Container Platform account with cluster administrator permissions.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to <code class="literal">oc</code> on the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem"><p class="simpara">
							To determine the mode that the CCO is configured to use, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc get cloudcredentials cluster \
  -o=jsonpath={.spec.credentialsMode}</pre><p class="simpara">
							The following output values are possible, though not all are supported on all platforms:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">''</code>: The CCO is operating in the default mode. In this configuration, the CCO operates in mint or passthrough mode, depending on the credentials provided during installation.
								</li><li class="listitem">
									<code class="literal">Mint</code>: The CCO is operating in mint mode.
								</li><li class="listitem">
									<code class="literal">Passthrough</code>: The CCO is operating in passthrough mode.
								</li><li class="listitem">
									<code class="literal">Manual</code>: The CCO is operating in manual mode.
								</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								To determine the specific configuration of an AWS or GCP cluster that has a <code class="literal">spec.credentialsMode</code> of <code class="literal">''</code>, <code class="literal">Mint</code>, or <code class="literal">Manual</code>, you must investigate further.
							</p><p>
								AWS and GCP clusters support using mint mode with the root secret deleted. If the cluster is specifically configured to use mint mode or uses mint mode by default, you must determine if the root secret is present on the cluster before updating.
							</p><p>
								An AWS or GCP cluster that uses manual mode might be configured to create and manage cloud credentials from outside of the cluster using the AWS Security Token Service (STS) or GCP Workload Identity. You can determine whether your cluster uses this strategy by examining the cluster <code class="literal">Authentication</code> object.
							</p></div></div></li><li class="listitem"><p class="simpara">
							AWS or GCP clusters that use mint mode only: To determine whether the cluster is operating without the root secret, run the following command:
						</p><pre class="programlisting language-terminal">$ oc get secret &lt;secret_name&gt; \
  -n=kube-system</pre><p class="simpara">
							where <code class="literal">&lt;secret_name&gt;</code> is <code class="literal">aws-creds</code> for AWS or <code class="literal">gcp-credentials</code> for GCP.
						</p><p class="simpara">
							If the root secret is present, the output of this command returns information about the secret. An error indicates that the root secret is not present on the cluster.
						</p></li><li class="listitem"><p class="simpara">
							AWS or GCP clusters that use manual mode only: To determine whether the cluster is configured to create and manage cloud credentials from outside of the cluster, run the following command:
						</p><pre class="programlisting language-terminal">$ oc get authentication cluster \
  -o jsonpath \
  --template='{ .spec.serviceAccountIssuer }'</pre><p class="simpara">
							This command displays the value of the <code class="literal">.spec.serviceAccountIssuer</code> parameter in the cluster <code class="literal">Authentication</code> object.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									An output of a URL that is associated with your cloud provider indicates that the CCO is using manual mode with AWS STS or GCP Workload Identity to create and manage cloud credentials from outside of the cluster. These clusters are configured using the <code class="literal">ccoctl</code> utility.
								</li><li class="listitem">
									An empty output indicates that the cluster is using the CCO in manual mode but was not configured using the <code class="literal">ccoctl</code> utility.
								</li></ul></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							If you are updating a cluster that has the CCO operating in mint or passthrough mode and the root secret is present, you do not need to update any cloud provider resources and can continue to the next part of the update process.
						</li><li class="listitem">
							If your cluster is using the CCO in mint mode with the root secret removed, you must reinstate the credential secret with the administrator-level credential before continuing to the next part of the update process.
						</li><li class="listitem"><p class="simpara">
							If your cluster was configured using the CCO utility (<code class="literal">ccoctl</code>), you must take the following actions:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Configure the <code class="literal">ccoctl</code> utility for the new release and use it to update the cloud provider resources.
								</li><li class="listitem">
									Update the <code class="literal">upgradeable-to</code> annotation to indicate that the cluster is ready to update.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							If your cluster is using the CCO in manual mode but was not configured using the <code class="literal">ccoctl</code> utility, you must take the following actions:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Manually update the cloud provider resources for the new release.
								</li><li class="listitem">
									Update the <code class="literal">upgradeable-to</code> annotation to indicate that the cluster is ready to update.
								</li></ol></div></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#cco-ccoctl-configuring_preparing-manual-creds-update">Configuring the Cloud Credential Operator utility for a cluster update</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#manually-maintained-credentials-upgrade_preparing-manual-creds-update">Updating cloud provider resources with manually maintained credentials</a>
						</li></ul></div></section></section><section class="section" id="cco-ccoctl-configuring_preparing-manual-creds-update"><div class="titlepage"><div><div><h2 class="title">7.2. Configuring the Cloud Credential Operator utility for a cluster update</h2></div></div></div><p>
				To upgrade a cluster that uses the Cloud Credential Operator (CCO) in manual mode to create and manage cloud credentials from outside of the cluster, extract and prepare the CCO utility (<code class="literal">ccoctl</code>) binary.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The <code class="literal">ccoctl</code> utility is a Linux binary that must run in a Linux environment.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to an OpenShift Container Platform account with cluster administrator access.
					</li><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li></ul></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Your cluster was configured using the <code class="literal">ccoctl</code> utility to create and manage cloud credentials from outside of the cluster.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Obtain the OpenShift Container Platform release image by running the following command:
					</p><pre class="programlisting language-terminal">$ RELEASE_IMAGE=$(./openshift-install version | awk '/release image/ {print $3}')</pre></li><li class="listitem"><p class="simpara">
						Obtain the CCO container image from the OpenShift Container Platform release image by running the following command:
					</p><pre class="programlisting language-terminal">$ CCO_IMAGE=$(oc adm release info --image-for='cloud-credential-operator' $RELEASE_IMAGE -a ~/.pull-secret)</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Ensure that the architecture of the <code class="literal">$RELEASE_IMAGE</code> matches the architecture of the environment in which you will use the <code class="literal">ccoctl</code> tool.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Extract the <code class="literal">ccoctl</code> binary from the CCO container image within the OpenShift Container Platform release image by running the following command:
					</p><pre class="programlisting language-terminal">$ oc image extract $CCO_IMAGE --file="/usr/bin/ccoctl" -a ~/.pull-secret</pre></li><li class="listitem"><p class="simpara">
						Change the permissions to make <code class="literal">ccoctl</code> executable by running the following command:
					</p><pre class="programlisting language-terminal">$ chmod 775 ccoctl</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						To verify that <code class="literal">ccoctl</code> is ready to use, display the help file by running the following command:
					</p><pre class="programlisting language-terminal">$ ccoctl --help</pre><div class="formalpara"><p class="title"><strong>Output of <code class="literal">ccoctl --help</code></strong></p><p>
							
<pre class="programlisting language-terminal">OpenShift credentials provisioning tool

Usage:
  ccoctl [command]

Available Commands:
  alibabacloud Manage credentials objects for alibaba cloud
  aws          Manage credentials objects for AWS cloud
  gcp          Manage credentials objects for Google cloud
  help         Help about any command
  ibmcloud     Manage credentials objects for IBM Cloud
  nutanix      Manage credentials objects for Nutanix

Flags:
  -h, --help   help for ccoctl

Use "ccoctl [command] --help" for more information about a command.</pre>

						</p></div></li></ul></div></section><section class="section" id="cco-ccoctl-upgrading_preparing-manual-creds-update"><div class="titlepage"><div><div><h2 class="title">7.3. Updating cloud provider resources with the Cloud Credential Operator utility</h2></div></div></div><p>
				The process for upgrading an OpenShift Container Platform cluster that was configured using the CCO utility (<code class="literal">ccoctl</code>) is similar to creating the cloud provider resources during installation.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					By default, <code class="literal">ccoctl</code> creates objects in the directory in which the commands are run. To create the objects in a different directory, use the <code class="literal">--output-dir</code> flag. This procedure uses <code class="literal">&lt;path_to_ccoctl_output_dir&gt;</code> to refer to this directory.
				</p><p>
					On AWS clusters, some <code class="literal">ccoctl</code> commands make AWS API calls to create or modify AWS resources. You can use the <code class="literal">--dry-run</code> flag to avoid making API calls. Using this flag creates JSON files on the local file system instead. You can review and modify the JSON files and then apply them with the AWS CLI tool using the <code class="literal">--cli-input-json</code> parameters.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Obtain the OpenShift Container Platform release image for the version that you are upgrading to.
					</li><li class="listitem">
						Extract and prepare the <code class="literal">ccoctl</code> binary from the release image.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Extract the list of <code class="literal">CredentialsRequest</code> custom resources (CRs) from the OpenShift Container Platform release image by running the following command:
					</p><pre class="programlisting language-terminal">$ oc adm release extract --credentials-requests \
  --cloud=&lt;provider_type&gt; \
  --to=&lt;path_to_directory_with_list_of_credentials_requests&gt;/credrequests \
  quay.io/&lt;path_to&gt;/ocp-release:&lt;version&gt;</pre><p class="simpara">
						where:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<code class="literal">&lt;provider_type&gt;</code> is the value for your cloud provider. Valid values are <code class="literal">alibabacloud</code>, <code class="literal">aws</code>, <code class="literal">gcp</code>, <code class="literal">ibmcloud</code>, and <code class="literal">nutanix</code>.
							</li><li class="listitem">
								<code class="literal">credrequests</code> is the directory where the list of <code class="literal">CredentialsRequest</code> objects is stored. This command creates the directory if it does not exist.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						For each <code class="literal">CredentialsRequest</code> CR in the release image, ensure that a namespace that matches the text in the <code class="literal">spec.secretRef.namespace</code> field exists in the cluster. This field is where the generated secrets that hold the credentials configuration are stored.
					</p><div class="formalpara"><p class="title"><strong>Sample AWS <code class="literal">CredentialsRequest</code> object</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: cloudcredential.openshift.io/v1
kind: CredentialsRequest
metadata:
  name: cloud-credential-operator-iam-ro
  namespace: openshift-cloud-credential-operator
spec:
  providerSpec:
    apiVersion: cloudcredential.openshift.io/v1
    kind: AWSProviderSpec
    statementEntries:
    - effect: Allow
      action:
      - iam:GetUser
      - iam:GetUserPolicy
      - iam:ListAccessKeys
      resource: "*"
  secretRef:
    name: cloud-credential-operator-iam-ro-creds
    namespace: openshift-cloud-credential-operator <span id="CO2-1"><!--Empty--></span><span class="callout">1</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO2-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								This field indicates the namespace which needs to exist to hold the generated secret.
							</div></dd></dl></div><p class="simpara">
						The <code class="literal">CredentialsRequest</code> CRs for other platforms have a similar format with different platform-specific values.
					</p></li><li class="listitem"><p class="simpara">
						For any <code class="literal">CredentialsRequest</code> CR for which the cluster does not already have a namespace with the name specified in <code class="literal">spec.secretRef.namespace</code>, create the namespace by running the following command:
					</p><pre class="programlisting language-terminal">$ oc create namespace &lt;component_namespace&gt;</pre></li><li class="listitem"><p class="simpara">
						Use the <code class="literal">ccoctl</code> tool to process all <code class="literal">CredentialsRequest</code> objects in the <code class="literal">credrequests</code> directory by running the command for your cloud provider. The following commands process <code class="literal">CredentialsRequest</code> objects:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Alibaba Cloud: <code class="literal">ccoctl alibabacloud create-ram-users</code>
							</li><li class="listitem">
								Amazon Web Services (AWS): <code class="literal">ccoctl aws create-iam-roles</code>
							</li><li class="listitem">
								Google Cloud Platform (GCP): <code class="literal">ccoctl gcp create-all</code>
							</li><li class="listitem">
								IBM Cloud: <code class="literal">ccoctl ibmcloud create-service-id</code>
							</li><li class="listitem">
								Nutanix: <code class="literal">ccoctl nutanix create-shared-secrets</code>
							</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Refer to the <code class="literal">ccoctl</code> utility instructions in the installation content for your cloud provider for important platform-specific details about the required arguments and special considerations.
						</p></div></div><p class="simpara">
						For each <code class="literal">CredentialsRequest</code> object, <code class="literal">ccoctl</code> creates the required provider resources and a permissions policy as defined in each <code class="literal">CredentialsRequest</code> object from the OpenShift Container Platform release image.
					</p></li><li class="listitem"><p class="simpara">
						Apply the secrets to your cluster by running the following command:
					</p><pre class="programlisting language-terminal">$ ls &lt;path_to_ccoctl_output_dir&gt;/manifests/*-credentials.yaml | xargs -I{} oc apply -f {}</pre></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
					You can verify that the required provider resources and permissions policies are created by querying the cloud provider. For more information, refer to your cloud provider documentation on listing roles or service accounts.
				</p></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Update the <code class="literal">upgradeable-to</code> annotation to indicate that the cluster is ready to upgrade.
					</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#cco-ccoctl-creating-at-once_installing-alibaba-default">Creating Alibaba Cloud credentials for OpenShift Container Platform components with the <code class="literal">ccoctl</code> tool</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#sts-mode-create-aws-resources-ccoctl_cco-mode-sts">Creating AWS resources with the Cloud Credential Operator utility</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#cco-ccoctl-creating-at-once_cco-mode-gcp-workload-identity">Creating GCP resources with the Cloud Credential Operator utility</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#manually-create-iam-ibm-cloud_installing-ibm-cloud-customizations">Manually creating IAM for IBM Cloud VPC</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#manually-create-iam-nutanix_installing-nutanix-installer-provisioned">Configuring IAM for Nutanix</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#cco-manual-upgrade-annotation_preparing-manual-creds-update">Indicating that the cluster is ready to upgrade</a>
					</li></ul></div></section><section class="section" id="manually-maintained-credentials-upgrade_preparing-manual-creds-update"><div class="titlepage"><div><div><h2 class="title">7.4. Updating cloud provider resources with manually maintained credentials</h2></div></div></div><p>
				Before upgrading a cluster with manually maintained credentials, you must create any new credentials for the release image that you are upgrading to. You must also review the required permissions for existing credentials and accommodate any new permissions requirements in the new release for those components.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Extract and examine the <code class="literal">CredentialsRequest</code> custom resource for the new release.
					</p><p class="simpara">
						The "Manually creating IAM" section of the installation content for your cloud provider explains how to obtain and use the credentials required for your cloud.
					</p></li><li class="listitem"><p class="simpara">
						Update the manually maintained credentials on your cluster:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Create new secrets for any <code class="literal">CredentialsRequest</code> custom resources that are added by the new release image.
							</li><li class="listitem">
								If the <code class="literal">CredentialsRequest</code> custom resources for any existing credentials that are stored in secrets have changed permissions requirements, update the permissions as required.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						If your cluster uses cluster capabilities to disable one or more optional components, delete the <code class="literal">CredentialsRequest</code> custom resources for any disabled components.
					</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">credrequests</code> directory contents for OpenShift Container Platform 4.12 on AWS</strong></p><p>
							
<pre class="programlisting language-terminal">0000_30_machine-api-operator_00_credentials-request.yaml <span id="CO3-1"><!--Empty--></span><span class="callout">1</span>
0000_50_cloud-credential-operator_05-iam-ro-credentialsrequest.yaml <span id="CO3-2"><!--Empty--></span><span class="callout">2</span>
0000_50_cluster-image-registry-operator_01-registry-credentials-request.yaml <span id="CO3-3"><!--Empty--></span><span class="callout">3</span>
0000_50_cluster-ingress-operator_00-ingress-credentials-request.yaml <span id="CO3-4"><!--Empty--></span><span class="callout">4</span>
0000_50_cluster-network-operator_02-cncc-credentials.yaml <span id="CO3-5"><!--Empty--></span><span class="callout">5</span>
0000_50_cluster-storage-operator_03_credentials_request_aws.yaml <span id="CO3-6"><!--Empty--></span><span class="callout">6</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO3-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The Machine API Operator CR is required.
							</div></dd><dt><a href="#CO3-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The Cloud Credential Operator CR is required.
							</div></dd><dt><a href="#CO3-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								The Image Registry Operator CR is required.
							</div></dd><dt><a href="#CO3-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								The Ingress Operator CR is required.
							</div></dd><dt><a href="#CO3-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								The Network Operator CR is required.
							</div></dd><dt><a href="#CO3-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								The Storage Operator CR is an optional component and might be disabled in your cluster.
							</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example <code class="literal">credrequests</code> directory contents for OpenShift Container Platform 4.12 on GCP</strong></p><p>
							
<pre class="programlisting language-terminal">0000_26_cloud-controller-manager-operator_16_credentialsrequest-gcp.yaml <span id="CO4-1"><!--Empty--></span><span class="callout">1</span>
0000_30_machine-api-operator_00_credentials-request.yaml <span id="CO4-2"><!--Empty--></span><span class="callout">2</span>
0000_50_cloud-credential-operator_05-gcp-ro-credentialsrequest.yaml <span id="CO4-3"><!--Empty--></span><span class="callout">3</span>
0000_50_cluster-image-registry-operator_01-registry-credentials-request-gcs.yaml <span id="CO4-4"><!--Empty--></span><span class="callout">4</span>
0000_50_cluster-ingress-operator_00-ingress-credentials-request.yaml <span id="CO4-5"><!--Empty--></span><span class="callout">5</span>
0000_50_cluster-network-operator_02-cncc-credentials.yaml <span id="CO4-6"><!--Empty--></span><span class="callout">6</span>
0000_50_cluster-storage-operator_03_credentials_request_gcp.yaml <span id="CO4-7"><!--Empty--></span><span class="callout">7</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO4-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The Cloud Controller Manager Operator CR is required.
							</div></dd><dt><a href="#CO4-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The Machine API Operator CR is required.
							</div></dd><dt><a href="#CO4-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								The Cloud Credential Operator CR is required.
							</div></dd><dt><a href="#CO4-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								The Image Registry Operator CR is required.
							</div></dd><dt><a href="#CO4-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								The Ingress Operator CR is required.
							</div></dd><dt><a href="#CO4-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								The Network Operator CR is required.
							</div></dd><dt><a href="#CO4-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								The Storage Operator CR is an optional component and might be disabled in your cluster.
							</div></dd></dl></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Update the <code class="literal">upgradeable-to</code> annotation to indicate that the cluster is ready to upgrade.
					</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#manually-create-iam_manually-creating-iam-aws">Manually creating IAM for AWS</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#manually-create-iam_manually-creating-iam-azure">Manually creating IAM for Azure</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#manually-create-iam_installing-azure-stack-hub-default">Manually creating IAM for Azure Stack Hub</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#manually-create-iam_manually-creating-iam-gcp">Manually creating IAM for GCP</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#cco-manual-upgrade-annotation_preparing-manual-creds-update">Indicating that the cluster is ready to upgrade</a>
					</li></ul></div></section><section class="section" id="cco-manual-upgrade-annotation_preparing-manual-creds-update"><div class="titlepage"><div><div><h2 class="title">7.5. Indicating that the cluster is ready to upgrade</h2></div></div></div><p>
				The Cloud Credential Operator (CCO) <code class="literal">Upgradable</code> status for a cluster with manually maintained credentials is <code class="literal">False</code> by default.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						For the release image that you are upgrading to, you have processed any new credentials manually or by using the Cloud Credential Operator utility (<code class="literal">ccoctl</code>).
					</li><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to <code class="literal">oc</code> on the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li><li class="listitem"><p class="simpara">
						Edit the <code class="literal">CloudCredential</code> resource to add an <code class="literal">upgradeable-to</code> annotation within the <code class="literal">metadata</code> field by running the following command:
					</p><pre class="programlisting language-terminal">$ oc edit cloudcredential cluster</pre><div class="formalpara"><p class="title"><strong>Text to add</strong></p><p>
							
<pre class="programlisting language-yaml">...
  metadata:
    annotations:
      cloudcredential.openshift.io/upgradeable-to: &lt;version_number&gt;
...</pre>

						</p></div><p class="simpara">
						Where <code class="literal">&lt;version_number&gt;</code> is the version that you are upgrading to, in the format <code class="literal">x.y.z</code>. For example, use <code class="literal">4.12.2</code> for OpenShift Container Platform 4.12.2.
					</p><p class="simpara">
						It may take several minutes after adding the annotation for the upgradeable status to change.
					</p></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						In the <span class="strong strong"><strong>Administrator</strong></span> perspective of the web console, navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Cluster Settings</strong></span>.
					</li><li class="listitem"><p class="simpara">
						To view the CCO status details, click <span class="strong strong"><strong>cloud-credential</strong></span> in the <span class="strong strong"><strong>Cluster Operators</strong></span> list.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								If the <span class="strong strong"><strong>Upgradeable</strong></span> status in the <span class="strong strong"><strong>Conditions</strong></span> section is <span class="strong strong"><strong>False</strong></span>, verify that the <code class="literal">upgradeable-to</code> annotation is free of typographical errors.
							</li></ul></div></li><li class="listitem">
						When the <span class="strong strong"><strong>Upgradeable</strong></span> status in the <span class="strong strong"><strong>Conditions</strong></span> section is <span class="strong strong"><strong>True</strong></span>, begin the OpenShift Container Platform upgrade.
					</li></ol></div></section></section><section class="chapter" id="updating-cluster-within-minor"><div class="titlepage"><div><div><h1 class="title">Chapter 8. Updating a cluster using the web console</h1></div></div></div><p>
			You can update, or upgrade, an OpenShift Container Platform cluster by using the web console. The following steps update a cluster within a minor version. You can use the same instructions for updating a cluster between minor versions.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				Use the web console or <code class="literal">oc adm upgrade channel <span class="emphasis"><em>&lt;channel&gt;</em></span></code> to change the update channel. You can follow the steps in <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-cluster-cli">Updating a cluster using the CLI</a> to complete the update after you change to a 4.13 channel.
			</p></div></div><section class="section" id="prerequisites"><div class="titlepage"><div><div><h2 class="title">8.1. Prerequisites</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Have access to the cluster as a user with <code class="literal">admin</code> privileges. See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#using-rbac-to-define-and-apply-permissions">Using RBAC to define and apply permissions</a>.
					</li><li class="listitem">
						Have a recent <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/backup_and_restore/#backup-etcd">etcd backup</a> in case your update fails and you must <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/backup_and_restore/#dr-restoring-cluster-state">restore your cluster to a previous state</a>.
					</li><li class="listitem">
						Support for RHEL7 workers is removed in OpenShift Container Platform 4.13. You must replace RHEL7 workers with RHEL8 or RHCOS workers before upgrading to OpenShift Container Platform 4.13. Red Hat does not support in-place RHEL7 to RHEL8 updates for RHEL workers; those hosts must be replaced with a clean operating system install.
					</li><li class="listitem">
						Ensure all Operators previously installed through Operator Lifecycle Manager (OLM) are updated to their latest version in their latest channel. Updating the Operators ensures they have a valid update path when the default OperatorHub catalogs switch from the current minor version to the next during a cluster update. See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-upgrading-operators">Updating installed Operators</a> for more information.
					</li><li class="listitem">
						Ensure that all machine config pools (MCPs) are running and not paused. Nodes associated with a paused MCP are skipped during the update process. You can pause the MCPs if you are performing a canary rollout update strategy.
					</li><li class="listitem">
						To accommodate the time it takes to update, you are able to do a partial update by updating the worker or custom pool nodes. You can pause and resume within the progress bar of each pool.
					</li><li class="listitem">
						If your cluster uses manually maintained credentials, update the cloud provider resources for the new release. For more information, including how to determine if this is a requirement for your cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#preparing-manual-creds-update">Preparing to update a cluster with manually maintained credentials</a>.
					</li><li class="listitem">
						Review the list of APIs that were removed in Kubernetes 1.26, migrate any affected components to use the new API version, and provide the administrator acknowledgment. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-cluster-prepare">Preparing to update to OpenShift Container Platform 4.13</a>.
					</li><li class="listitem">
						If you run an Operator or you have configured any application with the pod disruption budget, you might experience an interruption during the upgrade process. If <code class="literal">minAvailable</code> is set to 1 in <code class="literal">PodDisruptionBudget</code>, the nodes are drained to apply pending machine configs which might block the eviction process. If several nodes are rebooted, all the pods might run on only one node, and the <code class="literal">PodDisruptionBudget</code> field can prevent the node drain.
					</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							When an update is failing to complete, the Cluster Version Operator (CVO) reports the status of any blocking components while attempting to reconcile the update. Rolling your cluster back to a previous version is not supported. If your update is failing to complete, contact Red Hat support.
						</li><li class="listitem">
							Using the <code class="literal">unsupportedConfigOverrides</code> section to modify the configuration of an Operator is unsupported and might block cluster updates. You must remove this setting before you can update your cluster.
						</li></ul></div></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/architecture/#unmanaged-operators_architecture-installation">Support policy for unmanaged Operators</a>
					</li></ul></div></section><section class="section" id="update-using-custom-machine-config-pools-canary_updating-cluster-within-minor"><div class="titlepage"><div><div><h2 class="title">8.2. Performing a canary rollout update</h2></div></div></div><p>
				In some specific use cases, you might want a more controlled update process where you do not want specific nodes updated concurrently with the rest of the cluster. These use cases include, but are not limited to:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						You have mission-critical applications that you do not want unavailable during the update. You can slowly test the applications on your nodes in small batches after the update.
					</li><li class="listitem">
						You have a small maintenance window that does not allow the time for all nodes to be updated, or you have multiple maintenance windows.
					</li></ul></div><p>
				The rolling update process is <span class="strong strong"><strong>not</strong></span> a typical update workflow. With larger clusters, it can be a time-consuming process that requires you execute multiple commands. This complexity can result in errors that can affect the entire cluster. It is recommended that you carefully consider whether your organization wants to use a rolling update and carefully plan the implementation of the process before you start.
			</p><p>
				The rolling update process described in this topic involves:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Creating one or more custom machine config pools (MCPs).
					</li><li class="listitem">
						Labeling each node that you do not want to update immediately to move those nodes to the custom MCPs.
					</li><li class="listitem">
						Pausing those custom MCPs, which prevents updates to those nodes.
					</li><li class="listitem">
						Performing the cluster update.
					</li><li class="listitem">
						Unpausing one custom MCP, which triggers the update on those nodes.
					</li><li class="listitem">
						Testing the applications on those nodes to make sure the applications work as expected on those newly-updated nodes.
					</li><li class="listitem">
						Optionally removing the custom labels from the remaining nodes in small batches and testing the applications on those nodes.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Pausing an MCP should be done with careful consideration and for short periods of time only.
				</p></div></div><p>
				If you want to use the canary rollout update process, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-using-custom-machine-config-pools">Performing a canary rollout update</a>.
			</p></section><section class="section" id="machine-health-checks-pausing-web-console_updating-cluster-within-minor"><div class="titlepage"><div><div><h2 class="title">8.3. Pausing a MachineHealthCheck resource by using the web console</h2></div></div></div><p>
				During the upgrade process, nodes in the cluster might become temporarily unavailable. In the case of worker nodes, the machine health check might identify such nodes as unhealthy and reboot them. To avoid rebooting such nodes, pause all the <code class="literal">MachineHealthCheck</code> resources before updating the cluster.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to the cluster with <code class="literal">cluster-admin</code> privileges.
					</li><li class="listitem">
						You have access to the OpenShift Container Platform web console.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to the OpenShift Container Platform web console.
					</li><li class="listitem">
						Navigate to <span class="strong strong"><strong>Compute</strong></span> → <span class="strong strong"><strong>MachineHealthChecks</strong></span>.
					</li><li class="listitem"><p class="simpara">
						To pause the machine health checks, add the <code class="literal">cluster.x-k8s.io/paused=""</code> annotation to each <code class="literal">MachineHealthCheck</code> resource. For example, to add the annotation to the <code class="literal">machine-api-termination-handler</code> resource, complete the following steps:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Click the Options menu 
								<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Updating_clusters-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
								 next to the <code class="literal">machine-api-termination-handler</code> and click <span class="strong strong"><strong>Edit annotations</strong></span>.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>Edit annotations</strong></span> dialog, click <span class="strong strong"><strong>Add more</strong></span>.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>Key</strong></span> and <span class="strong strong"><strong>Value</strong></span> fields, add <code class="literal">cluster.x-k8s.io/paused</code> and <code class="literal">""</code> values, respectively, and click <span class="strong strong"><strong>Save</strong></span>.
							</li></ol></div></li></ol></div></section><section class="section" id="update-single-node-openshift_updating-cluster-within-minor"><div class="titlepage"><div><div><h2 class="title">8.4. About updating single node OpenShift Container Platform</h2></div></div></div><p>
				You can update, or upgrade, a single-node OpenShift Container Platform cluster by using either the console or CLI.
			</p><p>
				However, note the following limitations:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The prerequisite to pause the <code class="literal">MachineHealthCheck</code> resources is not required because there is no other node to perform the health check.
					</li><li class="listitem">
						Restoring a single-node OpenShift Container Platform cluster using an etcd backup is not officially supported. However, it is good practice to perform the etcd backup in case your upgrade fails. If your control plane is healthy, you might be able to restore your cluster to a previous state by using the backup.
					</li><li class="listitem"><p class="simpara">
						Updating a single-node OpenShift Container Platform cluster requires downtime and can include an automatic reboot. The amount of downtime depends on the update payload, as described in the following scenarios:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								If the update payload contains an operating system update, which requires a reboot, the downtime is significant and impacts cluster management and user workloads.
							</li><li class="listitem">
								If the update contains machine configuration changes that do not require a reboot, the downtime is less, and the impact on the cluster management and user workloads is lessened. In this case, the node draining step is skipped with single-node OpenShift Container Platform because there is no other node in the cluster to reschedule the workloads to.
							</li><li class="listitem">
								If the update payload does not contain an operating system update or machine configuration changes, a short API outage occurs and resolves quickly.
							</li></ul></div></li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					There are conditions, such as bugs in an updated package, that can cause the single node to not restart after a reboot. In this case, the update does not rollback automatically.
				</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						For information on which machine configuration changes require a reboot, see the note in <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/architecture/#understanding-machine-config-operator_control-plane">Understanding the Machine Config Operator</a>.
					</li></ul></div></section><section class="section" id="update-upgrading-web_updating-cluster-within-minor"><div class="titlepage"><div><div><h2 class="title">8.5. Updating a cluster by using the web console</h2></div></div></div><p>
				If updates are available, you can update your cluster from the web console.
			</p><p>
				You can find information about available OpenShift Container Platform advisories and updates <a class="link" href="https://access.redhat.com/downloads/content/290">in the errata section</a> of the Customer Portal.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Have access to the web console as a user with <code class="literal">admin</code> privileges.
					</li><li class="listitem">
						Pause all <code class="literal">MachineHealthCheck</code> resources.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						From the web console, click <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Cluster Settings</strong></span> and review the contents of the <span class="strong strong"><strong>Details</strong></span> tab.
					</li><li class="listitem"><p class="simpara">
						For production clusters, ensure that the <span class="strong strong"><strong>Channel</strong></span> is set to the correct channel for the version that you want to update to, such as <code class="literal">stable-4.13</code>.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							For production clusters, you must subscribe to a <code class="literal">stable-*</code>, <code class="literal">eus-*</code> or <code class="literal">fast-*</code> channel.
						</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							When you are ready to move to the next minor version, choose the channel that corresponds to that minor version. The sooner the update channel is declared, the more effectively the cluster can recommend update paths to your target version. The cluster might take some time to evaluate all the possible updates that are available and offer the best update recommendations to choose from. Update recommendations can change over time, as they are based on what update options are available at the time.
						</p><p>
							If you cannot see an update path to your target minor version, keep updating your cluster to the latest patch release for your current version until the next minor version is available in the path.
						</p></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								If the <span class="strong strong"><strong>Update status</strong></span> is not <span class="strong strong"><strong>Updates available</strong></span>, you cannot update your cluster.
							</li><li class="listitem">
								<span class="strong strong"><strong>Select channel</strong></span> indicates the cluster version that your cluster is running or is updating to.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						Select a version to update to, and click <span class="strong strong"><strong>Save</strong></span>.
					</p><p class="simpara">
						The Input channel <span class="strong strong"><strong>Update status</strong></span> changes to <span class="strong strong"><strong>Update to &lt;product-version&gt; in progress</strong></span>, and you can review the progress of the cluster update by watching the progress bars for the Operators and nodes.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you are upgrading your cluster to the next minor version, like version 4.y to 4.(y+1), it is recommended to confirm your nodes are updated before deploying workloads that rely on a new feature. Any pools with worker nodes that are not yet updated are displayed on the <span class="strong strong"><strong>Cluster Settings</strong></span> page.
						</p></div></div></li><li class="listitem"><p class="simpara">
						After the update completes and the Cluster Version Operator refreshes the available updates, check if more updates are available in your current channel.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								If updates are available, continue to perform updates in the current channel until you can no longer update.
							</li><li class="listitem">
								If no updates are available, change the <span class="strong strong"><strong>Channel</strong></span> to the <code class="literal">stable-*</code>, <code class="literal">eus-*</code> or <code class="literal">fast-*</code> channel for the next minor version, and update to the version that you want in that channel.
							</li></ul></div><p class="simpara">
						You might need to perform several intermediate updates until you reach the version that you want.
					</p></li></ol></div></section><section class="section" id="update-changing-update-server-web_updating-cluster-within-minor"><div class="titlepage"><div><div><h2 class="title">8.6. Changing the update server by using the web console</h2></div></div></div><p>
				Changing the update server is optional. If you have an OpenShift Update Service (OSUS) installed and configured locally, you must set the URL for the server as the <code class="literal">upstream</code> to use the local server during updates.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Cluster Settings</strong></span>, click <span class="strong strong"><strong>version</strong></span>.
					</li><li class="listitem"><p class="simpara">
						Click the <span class="strong strong"><strong>YAML</strong></span> tab and then edit the <code class="literal">upstream</code> parameter value:
					</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-yaml">  ...
  spec:
    clusterID: db93436d-7b05-42cc-b856-43e11ad2d31a
    upstream: '&lt;update-server-url&gt;' <span id="CO5-1"><!--Empty--></span><span class="callout">1</span>
  ...</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO5-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The <code class="literal">&lt;update-server-url&gt;</code> variable specifies the URL for the update server.
							</div></dd></dl></div><p class="simpara">
						The default <code class="literal">upstream</code> is <code class="literal">https://api.openshift.com/api/upgrades_info/v1/graph</code>.
					</p></li><li class="listitem">
						Click <span class="strong strong"><strong>Save</strong></span>.
					</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#understanding-upgrade-channels-releases">Understanding update channels and releases</a>
					</li></ul></div></section></section><section class="chapter" id="updating-cluster-cli"><div class="titlepage"><div><div><h1 class="title">Chapter 9. Updating a cluster using the CLI</h1></div></div></div><p>
			You can update, or upgrade, an OpenShift Container Platform cluster within a minor version by using the OpenShift CLI (<code class="literal">oc</code>). You can also update a cluster between minor versions by following the same instructions.
		</p><section class="section" id="prerequisites-2"><div class="titlepage"><div><div><h2 class="title">9.1. Prerequisites</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Have access to the cluster as a user with <code class="literal">admin</code> privileges. See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#using-rbac-to-define-and-apply-permissions">Using RBAC to define and apply permissions</a>.
					</li><li class="listitem">
						Have a recent <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/backup_and_restore/#backup-etcd">etcd backup</a> in case your update fails and you must <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/backup_and_restore/#dr-restoring-cluster-state">restore your cluster to a previous state</a>.
					</li><li class="listitem">
						Support for RHEL7 workers is removed in OpenShift Container Platform 4.13. You must replace RHEL7 workers with RHEL8 or RHCOS workers before upgrading to OpenShift Container Platform 4.13. Red Hat does not support in-place RHEL7 to RHEL8 updates for RHEL workers; those hosts must be replaced with a clean operating system install.
					</li><li class="listitem">
						Ensure all Operators previously installed through Operator Lifecycle Manager (OLM) are updated to their latest version in their latest channel. Updating the Operators ensures they have a valid update path when the default OperatorHub catalogs switch from the current minor version to the next during a cluster update. See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-upgrading-operators">Updating installed Operators</a> for more information.
					</li><li class="listitem">
						Ensure that all machine config pools (MCPs) are running and not paused. Nodes associated with a paused MCP are skipped during the update process. You can pause the MCPs if you are performing a canary rollout update strategy.
					</li><li class="listitem">
						If your cluster uses manually maintained credentials, update the cloud provider resources for the new release. For more information, including how to determine if this is a requirement for your cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#preparing-manual-creds-update">Preparing to update a cluster with manually maintained credentials</a>.
					</li><li class="listitem">
						Ensure that you address all <code class="literal">Upgradeable=False</code> conditions so the cluster allows an update to the next minor version. An alert displays at the top of the <span class="strong strong"><strong>Cluster Settings</strong></span> page when you have one or more cluster Operators that cannot be upgraded. You can still update to the next available patch update for the minor release you are currently on.
					</li><li class="listitem">
						Review the list of APIs that were removed in Kubernetes 1.26, migrate any affected components to use the new API version, and provide the administrator acknowledgment. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-cluster-prepare">Preparing to update to OpenShift Container Platform 4.13</a>.
					</li><li class="listitem">
						If you run an Operator or you have configured any application with the pod disruption budget, you might experience an interruption during the upgrade process. If <code class="literal">minAvailable</code> is set to 1 in <code class="literal">PodDisruptionBudget</code>, the nodes are drained to apply pending machine configs which might block the eviction process. If several nodes are rebooted, all the pods might run on only one node, and the <code class="literal">PodDisruptionBudget</code> field can prevent the node drain.
					</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							When an update is failing to complete, the Cluster Version Operator (CVO) reports the status of any blocking components while attempting to reconcile the update. Rolling your cluster back to a previous version is not supported. If your update is failing to complete, contact Red Hat support.
						</li><li class="listitem">
							Using the <code class="literal">unsupportedConfigOverrides</code> section to modify the configuration of an Operator is unsupported and might block cluster updates. You must remove this setting before you can update your cluster.
						</li></ul></div></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/architecture/#unmanaged-operators_architecture-installation">Support policy for unmanaged Operators</a>
					</li></ul></div></section><section class="section" id="machine-health-checks-pausing_updating-cluster-cli"><div class="titlepage"><div><div><h2 class="title">9.2. Pausing a MachineHealthCheck resource</h2></div></div></div><p>
				During the upgrade process, nodes in the cluster might become temporarily unavailable. In the case of worker nodes, the machine health check might identify such nodes as unhealthy and reboot them. To avoid rebooting such nodes, pause all the <code class="literal">MachineHealthCheck</code> resources before updating the cluster.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install the OpenShift CLI (<code class="literal">oc</code>).
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To list all the available <code class="literal">MachineHealthCheck</code> resources that you want to pause, run the following command:
					</p><pre class="programlisting language-terminal">$ oc get machinehealthcheck -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
						To pause the machine health checks, add the <code class="literal">cluster.x-k8s.io/paused=""</code> annotation to the <code class="literal">MachineHealthCheck</code> resource. Run the following command:
					</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api annotate mhc &lt;mhc-name&gt; cluster.x-k8s.io/paused=""</pre><p class="simpara">
						The annotated <code class="literal">MachineHealthCheck</code> resource resembles the following YAML file:
					</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: example
  namespace: openshift-machine-api
  annotations:
    cluster.x-k8s.io/paused: ""
spec:
  selector:
    matchLabels:
      role: worker
  unhealthyConditions:
  - type:    "Ready"
    status:  "Unknown"
    timeout: "300s"
  - type:    "Ready"
    status:  "False"
    timeout: "300s"
  maxUnhealthy: "40%"
status:
  currentHealthy: 5
  expectedMachines: 5</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Resume the machine health checks after updating the cluster. To resume the check, remove the pause annotation from the <code class="literal">MachineHealthCheck</code> resource by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api annotate mhc &lt;mhc-name&gt; cluster.x-k8s.io/paused-</pre></div></div></li></ol></div></section><section class="section" id="update-single-node-openshift_updating-cluster-cli"><div class="titlepage"><div><div><h2 class="title">9.3. About updating single node OpenShift Container Platform</h2></div></div></div><p>
				You can update, or upgrade, a single-node OpenShift Container Platform cluster by using either the console or CLI.
			</p><p>
				However, note the following limitations:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The prerequisite to pause the <code class="literal">MachineHealthCheck</code> resources is not required because there is no other node to perform the health check.
					</li><li class="listitem">
						Restoring a single-node OpenShift Container Platform cluster using an etcd backup is not officially supported. However, it is good practice to perform the etcd backup in case your upgrade fails. If your control plane is healthy, you might be able to restore your cluster to a previous state by using the backup.
					</li><li class="listitem"><p class="simpara">
						Updating a single-node OpenShift Container Platform cluster requires downtime and can include an automatic reboot. The amount of downtime depends on the update payload, as described in the following scenarios:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								If the update payload contains an operating system update, which requires a reboot, the downtime is significant and impacts cluster management and user workloads.
							</li><li class="listitem">
								If the update contains machine configuration changes that do not require a reboot, the downtime is less, and the impact on the cluster management and user workloads is lessened. In this case, the node draining step is skipped with single-node OpenShift Container Platform because there is no other node in the cluster to reschedule the workloads to.
							</li><li class="listitem">
								If the update payload does not contain an operating system update or machine configuration changes, a short API outage occurs and resolves quickly.
							</li></ul></div></li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					There are conditions, such as bugs in an updated package, that can cause the single node to not restart after a reboot. In this case, the update does not rollback automatically.
				</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						For information on which machine configuration changes require a reboot, see the note in <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/architecture/#understanding-machine-config-operator_control-plane">Understanding the Machine Config Operator</a>.
					</li></ul></div></section><section class="section" id="update-upgrading-cli_updating-cluster-cli"><div class="titlepage"><div><div><h2 class="title">9.4. Updating a cluster by using the CLI</h2></div></div></div><p>
				If updates are available, you can update your cluster by using the OpenShift CLI (<code class="literal">oc</code>).
			</p><p>
				You can find information about available OpenShift Container Platform advisories and updates <a class="link" href="https://access.redhat.com/downloads/content/290">in the errata section</a> of the Customer Portal.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install the OpenShift CLI (<code class="literal">oc</code>) that matches the version for your updated version.
					</li><li class="listitem">
						Log in to the cluster as user with <code class="literal">cluster-admin</code> privileges.
					</li><li class="listitem">
						Pause all <code class="literal">MachineHealthCheck</code> resources.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						View the available updates and note the version number of the update that you want to apply:
					</p><pre class="programlisting language-terminal">$ oc adm upgrade</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">Cluster version is 4.9.23

Upstream is unset, so the cluster will use an appropriate default.
Channel: stable-4.9 (available channels: candidate-4.10, candidate-4.9, fast-4.10, fast-4.9, stable-4.10, stable-4.9, eus-4.10)

Recommended updates:

VERSION IMAGE
4.9.24  quay.io/openshift-release-dev/ocp-release@sha256:6a899c54dda6b844bb12a247e324a0f6cde367e880b73ba110c056df6d018032
4.9.25  quay.io/openshift-release-dev/ocp-release@sha256:2eafde815e543b92f70839972f585cc52aa7c37aa72d5f3c8bc886b0fd45707a
4.9.26  quay.io/openshift-release-dev/ocp-release@sha256:3ccd09dd08c303f27a543351f787d09b83979cd31cf0b4c6ff56cd68814ef6c8
4.9.27  quay.io/openshift-release-dev/ocp-release@sha256:1c7db78eec0cf05df2cead44f69c0e4b2c3234d5635c88a41e1b922c3bedae16
4.9.28  quay.io/openshift-release-dev/ocp-release@sha256:4084d94969b186e20189649b5affba7da59f7d1943e4e5bc7ef78b981eafb7a8
4.9.29  quay.io/openshift-release-dev/ocp-release@sha256:b04ca01d116f0134a102a57f86c67e5b1a3b5da1c4a580af91d521b8fa0aa6ec
4.9.31  quay.io/openshift-release-dev/ocp-release@sha256:2a28b8ebb53d67dd80594421c39e36d9896b1e65cb54af81fbb86ea9ac3bf2d7
4.9.32  quay.io/openshift-release-dev/ocp-release@sha256:ecdb6d0df547b857eaf0edb5574ddd64ca6d9aff1fa61fd1ac6fb641203bedfa</pre>

						</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							For details and information on how to perform an <code class="literal">EUS-to-EUS</code> channel upgrade, please refer to the <span class="emphasis"><em>Preparing to perform an EUS-to-EUS upgrade</em></span> page, listed in the Additional resources section.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Based on your organization requirements, set the appropriate upgrade channel. For example, you can set your channel to <code class="literal">stable-4.12</code>, <code class="literal">fast-4.12</code>, or <code class="literal">eus-4.12</code>. For more information about channels, refer to <span class="emphasis"><em>Understanding update channels and releases</em></span> listed in the Additional resources section.
					</p><pre class="programlisting language-terminal">$ oc adm upgrade channel &lt;channel&gt;</pre><p class="simpara">
						For example, to set the channel to <code class="literal">stable-4.13</code>:
					</p><pre class="programlisting language-terminal">$ oc adm upgrade channel stable-4.13</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							For production clusters, you must subscribe to a <code class="literal">stable-*</code>, <code class="literal">eus-*</code>, or <code class="literal">fast-*</code> channel.
						</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							When you are ready to move to the next minor version, choose the channel that corresponds to that minor version. The sooner the update channel is declared, the more effectively the cluster can recommend update paths to your target version. The cluster might take some time to evaluate all the possible updates that are available and offer the best update recommendations to choose from. Update recommendations can change over time, as they are based on what update options are available at the time.
						</p><p>
							If you cannot see an update path to your target minor version, keep updating your cluster to the latest patch release for your current version until the next minor version is available in the path.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Apply an update:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								To update to the latest version:
							</p><pre class="programlisting language-terminal">$ oc adm upgrade --to-latest=true <span id="CO6-1"><!--Empty--></span><span class="callout">1</span></pre></li><li class="listitem"><p class="simpara">
								To update to a specific version:
							</p><pre class="programlisting language-terminal">$ oc adm upgrade --to=&lt;version&gt; <span id="CO6-2"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO6-1"><span class="callout">1</span></a> <a href="#CO6-2"><span class="callout">1</span></a> </dt><dd><div class="para">
										<code class="literal">&lt;version&gt;</code> is the update version that you obtained from the output of the <code class="literal">oc adm upgrade</code> command.
									</div></dd></dl></div></li></ul></div></li><li class="listitem"><p class="simpara">
						Review the status of the Cluster Version Operator:
					</p><pre class="programlisting language-terminal">$ oc adm upgrade</pre></li><li class="listitem"><p class="simpara">
						After the update completes, you can confirm that the cluster version has updated to the new version:
					</p><pre class="programlisting language-terminal">$ oc get clusterversion</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">Cluster version is &lt;version&gt;

Upstream is unset, so the cluster will use an appropriate default.
Channel: stable-4.10 (available channels: candidate-4.10, candidate-4.11, eus-4.10, fast-4.10, fast-4.11, stable-4.10)

No updates available. You may force an upgrade to a specific release image, but doing so might not be supported and might result in downtime or data loss.</pre>

						</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If the <code class="literal">oc get clusterversion</code> command displays the following error while the <code class="literal">PROGRESSING</code> status is <code class="literal">True</code>, you can ignore the error.
						</p><pre class="programlisting language-terminal">NAME    VERSION AVAILABLE PROGRESSING SINCE STATUS
version 4.10.26 True      True        24m   Unable to apply 4.11.0-rc.7: an unknown error has occurred: MultipleErrors</pre></div></div></li><li class="listitem"><p class="simpara">
						If you are upgrading your cluster to the next minor version, such as version X.y to X.(y+1), it is recommended to confirm that your nodes are upgraded before deploying workloads that rely on a new feature:
					</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                           STATUS   ROLES    AGE   VERSION
ip-10-0-168-251.ec2.internal   Ready    master   82m   v1.26.0
ip-10-0-170-223.ec2.internal   Ready    master   82m   v1.26.0
ip-10-0-179-95.ec2.internal    Ready    worker   70m   v1.26.0
ip-10-0-182-134.ec2.internal   Ready    worker   70m   v1.26.0
ip-10-0-211-16.ec2.internal    Ready    master   82m   v1.26.0
ip-10-0-250-100.ec2.internal   Ready    worker   69m   v1.26.0</pre>

						</p></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#preparing-eus-eus-upgrade">Preparing to perform an EUS-to-EUS update</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#understanding-upgrade-channels-releases">Understanding update channels and releases</a>
					</li></ul></div></section><section class="section" id="update-conditional-upgrade-pathupdating-cluster-cli"><div class="titlepage"><div><div><h2 class="title">9.5. Updating along a conditional upgrade path</h2></div></div></div><p>
				You can update along a recommended conditional upgrade path using the web console or the OpenShift CLI (<code class="literal">oc</code>). When a conditional update is not recommended for your cluster, you can update along a conditional upgrade path using the OpenShift CLI (<code class="literal">oc</code>) 4.10 or later.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To view the description of the update when it is not recommended because a risk might apply, run the following command:
					</p><pre class="programlisting language-terminal">$ oc adm upgrade --include-not-recommended</pre></li><li class="listitem"><p class="simpara">
						If the cluster administrator evaluates the potential known risks and decides it is acceptable for the current cluster, then the administrator can waive the safety guards and proceed the update by running the following command:
					</p><pre class="programlisting language-terminal">$ oc adm upgrade --allow-not-recommended --to &lt;version&gt; &lt;.&gt;</pre><p class="simpara">
						&lt;.&gt; <code class="literal">&lt;version&gt;</code> is the supported but not recommended update version that you obtained from the output of the previous command.
					</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#understanding-upgrade-channels-releases">Understanding update channels and releases</a>
					</li></ul></div></section><section class="section" id="update-changing-update-server-cli_updating-cluster-cli"><div class="titlepage"><div><div><h2 class="title">9.6. Changing the update server by using the CLI</h2></div></div></div><p>
				Changing the update server is optional. If you have an OpenShift Update Service (OSUS) installed and configured locally, you must set the URL for the server as the <code class="literal">upstream</code> to use the local server during updates. The default value for <code class="literal">upstream</code> is <code class="literal">https://api.openshift.com/api/upgrades_info/v1/graph</code>.
			</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Change the <code class="literal">upstream</code> parameter value in the cluster version:
					</p><pre class="programlisting language-terminal">$ oc patch clusterversion/version --patch '{"spec":{"upstream":"&lt;update-server-url&gt;"}}' --type=merge</pre><p class="simpara">
						The <code class="literal">&lt;update-server-url&gt;</code> variable specifies the URL for the update server.
					</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">clusterversion.config.openshift.io/version patched</pre>

						</p></div></li></ul></div></section></section><section class="chapter" id="migrating-clusters-to-multi-payload"><div class="titlepage"><div><div><h1 class="title">Chapter 10. Migrating to a cluster with multi-architecture compute machines</h1></div></div></div><p>
			You can migrate your current cluster with single-architecture compute machines to a cluster with multi-architecture compute machines by updating to a multi-architecture, manifest-listed payload. This allows you to add mixed architecture compute nodes to your cluster.
		</p><p>
			For information about configuring your multi-architecture compute machines, see <span class="emphasis"><em>Configuring multi-architecture compute machines on an OpenShift Container Platform cluster</em></span>.
		</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				Migration from a multi-architecture payload to a single-architecture payload is not supported. Once a cluster has transitioned to using a multi-architecture payload, it can no longer accept a single-architecture upgrade payload.
			</p></div></div><section class="section" id="migrating-to-multi-arch-cli_updating-clusters-overview"><div class="titlepage"><div><div><h2 class="title">10.1. Migrating to a cluster with multi-architecture compute machines using the CLI</h2></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li><li class="listitem"><p class="simpara">
						Your OpenShift Container Platform version is up to date to at least version 4.13.0.
					</p><p class="simpara">
						For more information on how to update your cluster version, see <span class="emphasis"><em>Updating a cluster using the web console</em></span> or <span class="emphasis"><em>Updating a cluster using the CLI</em></span>.
					</p></li><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>) that matches the version for your current cluster.
					</li><li class="listitem">
						Your <code class="literal">oc</code> client is updated to at least verion 4.13.0.
					</li><li class="listitem"><p class="simpara">
						Your OpenShift Container Platform cluster is installed on either the AWS or Azure platform.
					</p><p class="simpara">
						For more information on selecting a supported platform for your cluster installation, see <span class="emphasis"><em>Selecting a cluster installation type</em></span>.
					</p></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Verify that the <code class="literal">RetrievedUpdates</code> condition is <code class="literal">True</code> in the Cluster Version Operator (CVO) by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get clusterversion/version -o=jsonpath="{.status.conditions[?(.type=='RetrievedUpdates')].status}"</pre><p class="simpara">
						If the <code class="literal">RetrievedUpates</code> condition is <code class="literal">False</code>, you can find supplemental information regarding the failure by using the following command:
					</p><pre class="programlisting language-terminal">$ oc adm upgrade</pre><p class="simpara">
						For more information about cluster version condition types, see <span class="emphasis"><em>Understanding cluster version condition types</em></span>.
					</p></li><li class="listitem"><p class="simpara">
						If the condition <code class="literal">RetrievedUpdates</code> is <code class="literal">False</code>, change the channel to <code class="literal">stable-&lt;4.y&gt;</code> or <code class="literal">fast-&lt;4.y&gt;</code> with the following command:
					</p><pre class="programlisting language-terminal">$ oc adm upgrade channel &lt;channel&gt;</pre><p class="simpara">
						After setting the channel, verify if <code class="literal">RetrievedUpdates</code> is <code class="literal">True</code>.
					</p><p class="simpara">
						For more information about channels, see <span class="emphasis"><em>Understanding update channels and releases</em></span>.
					</p></li><li class="listitem"><p class="simpara">
						Migrate to the multi-architecture payload with following command:
					</p><pre class="programlisting language-terminal">$ oc adm upgrade --to-multi-arch</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						You can monitor the migration by running the following command:
					</p><pre class="programlisting language-terminal">$ oc adm upgrade</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Machine launches may fail as the cluster settles into the new state. To notice and recover when machines fail to launch, we recommend deploying machine health checks. For more information about machine health checks and how to deploy them, see <span class="emphasis"><em>About machine health checks</em></span>.
						</p></div></div><p class="simpara">
						The migrations must be complete and all the cluster operators must be stable before you can add compute machine sets with different architectures to your cluster.
					</p></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#multi-architecture-creating-arm64-bootimage_multi-architecture-configuration">Configuring multi-architecture compute machines on an OpenShift Container Platform cluster</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-cluster-within-minor">Updating a cluster using the web console</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-cluster-cli">Updating a cluster using the CLI</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#understanding-clusterversion-conditiontypes_updating-clusters-overview">Understanding cluster version condition types</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#understanding-upgrade-channels-releases">Understanding update channels and releases</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-preparing-install-manage">Selecting a cluster installation type</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machine-health-checks-about_deploying-machine-health-checks">About machine health checks</a>
					</li></ul></div></section></section><section class="chapter" id="update-using-custom-machine-config-pools"><div class="titlepage"><div><div><h1 class="title">Chapter 11. Performing a canary rollout update</h1></div></div></div><p>
			There might be some scenarios where you want a more controlled rollout of an update to the worker nodes in order to ensure that mission-critical applications stay available during the whole update, even if the update process causes your applications to fail. Depending on your organizational needs, you might want to update a small subset of worker nodes, evaluate cluster and workload health over a period of time, then update the remaining nodes. This is commonly referred to as a <span class="emphasis"><em>canary</em></span> update. Or, you might also want to fit worker node updates, which often require a host reboot, into smaller defined maintenance windows when it is not possible to take a large maintenance window to update the entire cluster at one time.
		</p><p>
			In these scenarios, you can create multiple custom machine config pools (MCPs) to prevent certain worker nodes from updating when you update the cluster. After the rest of the cluster is updated, you can update those worker nodes in batches at appropriate times.
		</p><p>
			For example, if you have a cluster with 100 nodes with 10% excess capacity, maintenance windows that must not exceed 4 hours, and you know that it takes no longer than 8 minutes to drain and reboot a worker node, you can leverage MCPs to meet your goals. For example, you could define four MCPs, named <span class="strong strong"><strong>workerpool-canary</strong></span>, <span class="strong strong"><strong>workerpool-A</strong></span>, <span class="strong strong"><strong>workerpool-B</strong></span>, and <span class="strong strong"><strong>workerpool-C</strong></span>, with 10, 30, 30, and 30 nodes respectively.
		</p><p>
			During your first maintenance window, you would pause the MCP for <span class="strong strong"><strong>workerpool-A</strong></span>, <span class="strong strong"><strong>workerpool-B</strong></span>, and <span class="strong strong"><strong>workerpool-C</strong></span>, then initiate the cluster update. This updates components that run on top of OpenShift Container Platform and the 10 nodes which are members of the <span class="strong strong"><strong>workerpool-canary</strong></span> MCP, because that pool was not paused. The other three MCPs are not updated, because they were paused. If for some reason, you determine that your cluster or workload health was negatively affected by the <span class="strong strong"><strong>workerpool-canary</strong></span> update, you would then cordon and drain all nodes in that pool while still maintaining sufficient capacity until you have diagnosed the problem. When everything is working as expected, you would then evaluate the cluster and workload health before deciding to unpause, and thus update, <span class="strong strong"><strong>workerpool-A</strong></span>, <span class="strong strong"><strong>workerpool-B</strong></span>, and <span class="strong strong"><strong>workerpool-C</strong></span> in succession during each additional maintenance window.
		</p><p>
			While managing worker node updates using custom MCPs provides flexibility, it can be a time-consuming process that requires you execute multiple commands. This complexity can result in errors that can affect the entire cluster. It is recommended that you carefully consider your organizational needs and carefully plan the implemention of the process before you start.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				It is not recommended to update the MCPs to different OpenShift Container Platform versions. For example, do not update one MCP from 4.y.10 to 4.y.11 and another to 4.y.12. This scenario has not been tested and might result in an undefined cluster state.
			</p></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				Pausing a machine config pool prevents the Machine Config Operator from applying any configuration changes on the associated nodes. Pausing an MCP also prevents any automatically rotated certificates from being pushed to the associated nodes, including the automatic CA rotation of the <code class="literal">kube-apiserver-to-kubelet-signer</code> CA certificate.
			</p><p>
				If the MCP is paused when the <code class="literal">kube-apiserver-to-kubelet-signer</code> CA certificate expires and the MCO attempts to automatically renew the certificate, the MCO cannot push the newly rotated certificates to those nodes. This causes failure in multiple <code class="literal">oc</code> commands, including <code class="literal">oc debug</code>, <code class="literal">oc logs</code>, <code class="literal">oc exec</code>, and <code class="literal">oc attach</code>. You receive alerts in the Alerting UI of the OpenShift Container Platform web console if an MCP is paused when the certificates are rotated.
			</p><p>
				Pausing an MCP should be done with careful consideration about the <code class="literal">kube-apiserver-to-kubelet-signer</code> CA certificate expiration and for short periods of time only.
			</p></div></div><section class="section" id="update-using-custom-machine-config-pools-about-mcp_update-using-custom-machine-config-pools"><div class="titlepage"><div><div><h2 class="title">11.1. About the canary rollout update process and MCPs</h2></div></div></div><p>
				In OpenShift Container Platform, nodes are not considered individually. Nodes are grouped into machine config pools (MCP). There are two MCPs in a default OpenShift Container Platform cluster: one for the control plane nodes and one for the worker nodes. An OpenShift Container Platform update affects all MCPs concurrently.
			</p><p>
				During the update, the Machine Config Operator (MCO) drains and cordons all nodes within a MCP up to the specified <code class="literal">maxUnavailable</code> number of nodes (if specified), by default <code class="literal">1</code>. Draining and cordoning a node deschedules all pods on the node and marks the node as unschedulable. After the node is drained, the Machine Config Daemon applies a new machine configuration, which can include updating the operating system (OS). Updating the OS requires the host to reboot.
			</p><p>
				To prevent specific nodes from being updated, and thus, not drained, cordoned, and updated, you can create custom MCPs. Then, pause those MCPs to ensure that the nodes associated with those MCPs are not updated. The MCO does not update any paused MCPs. You can create one or more custom MCPs, which can give you more control over the sequence in which you update those nodes. After you update the nodes in the first MCP, you can verify the application compatibility, and then update the rest of the nodes gradually to the new version.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					To ensure the stability of the control plane, creating a custom MCP from the control plane nodes is not supported. The Machine Config Operator (MCO) ignores any custom MCP created for the control plane nodes.
				</p></div></div><p>
				You should give careful consideration to the number of MCPs you create and the number of nodes in each MCP, based on your workload deployment topology. For example, If you need to fit updates into specific maintenance windows, you need to know how many nodes that OpenShift Container Platform can update within a window. This number is dependent on your unique cluster and workload characteristics.
			</p><p>
				Also, you need to consider how much extra capacity you have available in your cluster. For example, in the case where your applications fail to work as expected on the updated nodes, you can cordon and drain those nodes in the pool, which moves the application pods to other nodes. You need to consider how much extra capacity you have available in order to determine the number of custom MCPs you need and how many nodes are in each MCP. For example, if you use two custom MCPs and 50% of your nodes are in each pool, you need to determine if running 50% of your nodes would provide sufficient quality-of-service (QoS) for your applications.
			</p><p>
				You can use this update process with all documented OpenShift Container Platform update processes. However, the process does not work with Red Hat Enterprise Linux (RHEL) machines, which are updated using Ansible playbooks.
			</p></section><section class="section" id="update-using-custom-machine-config-pools-about_update-using-custom-machine-config-pools"><div class="titlepage"><div><div><h2 class="title">11.2. About performing a canary rollout update</h2></div></div></div><p>
				This topic describes the general workflow of this canary rollout update process. The steps to perform each task in the workflow are described in the following sections.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create MCPs based on the worker pool. The number of nodes in each MCP depends on a few factors, such as your maintenance window duration for each MCP, and the amount of reserve capacity, meaning extra worker nodes, available in your cluster.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You can change the <code class="literal">maxUnavailable</code> setting in an MCP to specify the percentage or the number of machines that can be updating at any given time. The default is 1.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Add a node selector to the custom MCPs. For each node that you do not want to update simultaneously with the rest of the cluster, add a matching label to the nodes. This label associates the node to the MCP.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Do not remove the default worker label from the nodes. The nodes <span class="strong strong"><strong>must</strong></span> have a role label to function properly in the cluster.
						</p></div></div></li><li class="listitem">
						Pause the MCPs you do not want to update as part of the update process.
					</li><li class="listitem">
						Perform the cluster update. The update process updates the MCPs that are not paused, including the control plane nodes.
					</li><li class="listitem">
						Test the applications on the updated nodes to ensure they are working as expected.
					</li><li class="listitem">
						Unpause the remaining MCPs one-by-one and test the applications on those nodes until all worker nodes are updated. Unpausing an MCP starts the update process for the nodes associated with that MCP. You can check the progress of the update from the web console by clicking <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Cluster settings</strong></span>. Or, use the <code class="literal">oc get machineconfigpools</code> CLI command.
					</li><li class="listitem">
						Optionally, remove the custom label from updated nodes and delete the custom MCPs.
					</li></ol></div></section><section class="section" id="update-using-custom-machine-config-pools-mcp_update-using-custom-machine-config-pools"><div class="titlepage"><div><div><h2 class="title">11.3. Creating machine config pools to perform a canary rollout update</h2></div></div></div><p>
				The first task in performing this canary rollout update is to create one or more machine config pools (MCP).
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create an MCP from a worker node.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								List the worker nodes in your cluster.
							</p><pre class="programlisting language-terminal">$ oc get -l 'node-role.kubernetes.io/master!=' -o 'jsonpath={range .items[*]}{.metadata.name}{"\n"}{end}' nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">ci-ln-pwnll6b-f76d1-s8t9n-worker-a-s75z4
ci-ln-pwnll6b-f76d1-s8t9n-worker-b-dglj2
ci-ln-pwnll6b-f76d1-s8t9n-worker-c-lldbm</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								For the nodes you want to delay, add a custom label to the node:
							</p><pre class="programlisting language-terminal">$ oc label node &lt;node name&gt; node-role.kubernetes.io/&lt;custom-label&gt;=</pre><p class="simpara">
								For example:
							</p><pre class="programlisting language-terminal">$ oc label node ci-ln-0qv1yp2-f76d1-kl2tq-worker-a-j2ssz node-role.kubernetes.io/workerpool-canary=</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">node/ci-ln-gtrwm8t-f76d1-spbl7-worker-a-xk76k labeled</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Create the new MCP:
							</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: workerpool-canary <span id="CO7-1"><!--Empty--></span><span class="callout">1</span>
spec:
  machineConfigSelector:
    matchExpressions: <span id="CO7-2"><!--Empty--></span><span class="callout">2</span>
      - {
         key: machineconfiguration.openshift.io/role,
         operator: In,
         values: [worker,workerpool-canary]
        }
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/workerpool-canary: "" <span id="CO7-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO7-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify a name for the MCP.
									</div></dd><dt><a href="#CO7-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specify the <code class="literal">worker</code> and custom MCP name.
									</div></dd><dt><a href="#CO7-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Specify the custom label you added to the nodes that you want in this pool.
									</div></dd></dl></div><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">machineconfigpool.machineconfiguration.openshift.io/workerpool-canary created</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								View the list of MCPs in the cluster and their current state:
							</p><pre class="programlisting language-terminal">$ oc get machineconfigpool</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME              CONFIG                                                        UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master            rendered-master-b0bb90c4921860f2a5d8a2f8137c1867              True      False      False      3              3                   3                     0                      97m
workerpool-canary rendered-workerpool-canary-87ba3dec1ad78cb6aecebf7fbb476a36   True      False      False      1              1                   1                     0                      2m42s
worker            rendered-worker-87ba3dec1ad78cb6aecebf7fbb476a36              True      False      False      2              2                   2                     0                      97m</pre>

								</p></div><p class="simpara">
								The new machine config pool, <code class="literal">workerpool-canary</code>, is created and the number of nodes to which you added the custom label are shown in the machine counts. The worker MCP machine counts are reduced by the same number. It can take several minutes to update the machine counts. In this example, one node was moved from the <code class="literal">worker</code> MCP to the <code class="literal">workerpool-canary</code> MCP.
							</p></li></ol></div></li></ol></div></section><section class="section" id="update-using-custom-machine-config-pools-pause_update-using-custom-machine-config-pools"><div class="titlepage"><div><div><h2 class="title">11.4. Pausing the machine config pools</h2></div></div></div><p>
				In this canary rollout update process, after you label the nodes that you do not want to update with the rest of your OpenShift Container Platform cluster and create the machine config pools (MCPs), you pause those MCPs. Pausing an MCP prevents the Machine Config Operator (MCO) from updating the nodes associated with that MCP.
			</p><p>
				To pause an MCP:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Patch the MCP that you want paused:
					</p><pre class="programlisting language-terminal">$ oc patch mcp/&lt;mcp_name&gt; --patch '{"spec":{"paused":true}}' --type=merge</pre><p class="simpara">
						For example:
					</p><pre class="programlisting language-terminal">$  oc patch mcp/workerpool-canary --patch '{"spec":{"paused":true}}' --type=merge</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">machineconfigpool.machineconfiguration.openshift.io/workerpool-canary patched</pre>

						</p></div></li></ol></div></section><section class="section" id="update-using-custom-machine-config-pools-update_update-using-custom-machine-config-pools"><div class="titlepage"><div><div><h2 class="title">11.5. Performing the cluster update</h2></div></div></div><p>
				When the MCPs enter ready state, you can peform the cluster update. See one of the following update methods, as appropriate for your cluster:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-upgrading-web_updating-cluster-within-minor">Updating a cluster using the web console</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-upgrading-cli_updating-cluster-cli">Updating a cluster using the CLI</a>
					</li></ul></div><p>
				After the update is complete, you can start to unpause the MCPs one-by-one.
			</p></section><section class="section" id="update-using-custom-machine-config-pools-unpause_update-using-custom-machine-config-pools"><div class="titlepage"><div><div><h2 class="title">11.6. Unpausing the machine config pools</h2></div></div></div><p>
				In this canary rollout update process, after the OpenShift Container Platform update is complete, unpause your custom MCPs one-by-one. Unpausing an MCP allows the Machine Config Operator (MCO) to update the nodes associated with that MCP.
			</p><p>
				To unpause an MCP:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Patch the MCP that you want to unpause:
					</p><pre class="programlisting language-terminal">$ oc patch mcp/&lt;mcp_name&gt; --patch '{"spec":{"paused":false}}' --type=merge</pre><p class="simpara">
						For example:
					</p><pre class="programlisting language-terminal">$  oc patch mcp/workerpool-canary --patch '{"spec":{"paused":false}}' --type=merge</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">machineconfigpool.machineconfiguration.openshift.io/workerpool-canary patched</pre>

						</p></div><p class="simpara">
						You can check the progress of the update by using the <code class="literal">oc get machineconfigpools</code> command.
					</p></li><li class="listitem">
						Test your applications on the updated nodes to ensure that they are working as expected.
					</li><li class="listitem">
						Unpause any other paused MCPs one-by-one and verify that your applications work.
					</li></ol></div><section class="section" id="update-using-custom-machine-config-pools-fail_update-using-custom-machine-config-pools"><div class="titlepage"><div><div><h3 class="title">11.6.1. In case of application failure</h3></div></div></div><p>
					In case of a failure, such as your applications not working on the updated nodes, you can cordon and drain the nodes in the pool, which moves the application pods to other nodes to help maintain the quality-of-service for the applications. This first MCP should be no larger than the excess capacity.
				</p></section></section><section class="section" id="update-using-custom-machine-config-pools-mcp-remove_update-using-custom-machine-config-pools"><div class="titlepage"><div><div><h2 class="title">11.7. Moving a node to the original machine config pool</h2></div></div></div><p>
				In this canary rollout update process, after you have unpaused a custom machine config pool (MCP) and verified that the applications on the nodes associated with that MCP are working as expected, you should move the node back to its original MCP by removing the custom label you added to the node.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					A node must have a role to be properly functioning in the cluster.
				</p></div></div><p>
				To move a node to its original MCP:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Remove the custom label from the node.
					</p><pre class="programlisting language-terminal">$ oc label node &lt;node_name&gt; node-role.kubernetes.io/&lt;custom-label&gt;-</pre><p class="simpara">
						For example:
					</p><pre class="programlisting language-terminal">$ oc label node ci-ln-0qv1yp2-f76d1-kl2tq-worker-a-j2ssz node-role.kubernetes.io/workerpool-canary-</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">node/ci-ln-0qv1yp2-f76d1-kl2tq-worker-a-j2ssz labeled</pre>

						</p></div><p class="simpara">
						The MCO moves the nodes back to the original MCP and reconciles the node to the MCP configuration.
					</p></li><li class="listitem"><p class="simpara">
						View the list of MCPs in the cluster and their current state:
					</p><pre class="programlisting language-terminal">$oc get mcp</pre><pre class="programlisting language-terminal">NAME                CONFIG                                                   UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master              rendered-master-1203f157d053fd987c7cbd91e3fbc0ed         True      False      False      3              3                   3                     0                      61m
workerpool-canary   rendered-mcp-noupdate-5ad4791166c468f3a35cd16e734c9028   True      False      False      0              0                   0                     0                      21m
worker              rendered-worker-5ad4791166c468f3a35cd16e734c9028         True      False      False      3              3                   3                     0                      61m</pre><p class="simpara">
						The node is removed from the custom MCP and moved back to the original MCP. It can take several minutes to update the machine counts. In this example, one node was moved from the removed <code class="literal">workerpool-canary</code> MCP to the `worker`MCP.
					</p></li><li class="listitem"><p class="simpara">
						Optional: Delete the custom MCP:
					</p><pre class="programlisting language-terminal">$ oc delete mcp &lt;mcp_name&gt;</pre></li></ol></div></section></section><section class="chapter" id="updating-cluster-rhel-compute"><div class="titlepage"><div><div><h1 class="title">Chapter 12. Updating a cluster that includes RHEL compute machines</h1></div></div></div><p>
			You can update, or upgrade, an OpenShift Container Platform cluster. If your cluster contains Red Hat Enterprise Linux (RHEL) machines, you must perform more steps to update those machines.
		</p><section class="section" id="prerequisites-3"><div class="titlepage"><div><div><h2 class="title">12.1. Prerequisites</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Have access to the cluster as a user with <code class="literal">admin</code> privileges. See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#using-rbac-to-define-and-apply-permissions">Using RBAC to define and apply permissions</a>.
					</li><li class="listitem">
						Have a recent <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/backup_and_restore/#backup-etcd">etcd backup</a> in case your update fails and you must <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/backup_and_restore/#dr-restoring-cluster-state">restore your cluster to a previous state</a>.
					</li><li class="listitem">
						Support for RHEL7 workers is removed in OpenShift Container Platform 4.13. You must replace RHEL7 workers with RHEL8 or RHCOS workers before upgrading to OpenShift Container Platform 4.13. Red Hat does not support in-place RHEL7 to RHEL8 updates for RHEL workers; those hosts must be replaced with a clean operating system install.
					</li><li class="listitem">
						If your cluster uses manually maintained credentials, update the cloud provider resources for the new release. For more information, including how to determine if this is a requirement for your cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#preparing-manual-creds-update">Preparing to update a cluster with manually maintained credentials</a>.
					</li><li class="listitem">
						If you run an Operator or you have configured any application with the pod disruption budget, you might experience an interruption during the upgrade process. If <code class="literal">minAvailable</code> is set to 1 in <code class="literal">PodDisruptionBudget</code>, the nodes are drained to apply pending machine configs which might block the eviction process. If several nodes are rebooted, all the pods might run on only one node, and the <code class="literal">PodDisruptionBudget</code> field can prevent the node drain.
					</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/architecture/#unmanaged-operators_architecture-installation">Support policy for unmanaged Operators</a>
					</li></ul></div></section><section class="section" id="update-upgrading-web_updating-cluster-rhel-compute"><div class="titlepage"><div><div><h2 class="title">12.2. Updating a cluster by using the web console</h2></div></div></div><p>
				If updates are available, you can update your cluster from the web console.
			</p><p>
				You can find information about available OpenShift Container Platform advisories and updates <a class="link" href="https://access.redhat.com/downloads/content/290">in the errata section</a> of the Customer Portal.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Have access to the web console as a user with <code class="literal">admin</code> privileges.
					</li><li class="listitem">
						Pause all <code class="literal">MachineHealthCheck</code> resources.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						From the web console, click <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Cluster Settings</strong></span> and review the contents of the <span class="strong strong"><strong>Details</strong></span> tab.
					</li><li class="listitem"><p class="simpara">
						For production clusters, ensure that the <span class="strong strong"><strong>Channel</strong></span> is set to the correct channel for the version that you want to update to, such as <code class="literal">stable-4.13</code>.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							For production clusters, you must subscribe to a <code class="literal">stable-*</code>, <code class="literal">eus-*</code> or <code class="literal">fast-*</code> channel.
						</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							When you are ready to move to the next minor version, choose the channel that corresponds to that minor version. The sooner the update channel is declared, the more effectively the cluster can recommend update paths to your target version. The cluster might take some time to evaluate all the possible updates that are available and offer the best update recommendations to choose from. Update recommendations can change over time, as they are based on what update options are available at the time.
						</p><p>
							If you cannot see an update path to your target minor version, keep updating your cluster to the latest patch release for your current version until the next minor version is available in the path.
						</p></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								If the <span class="strong strong"><strong>Update status</strong></span> is not <span class="strong strong"><strong>Updates available</strong></span>, you cannot update your cluster.
							</li><li class="listitem">
								<span class="strong strong"><strong>Select channel</strong></span> indicates the cluster version that your cluster is running or is updating to.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						Select a version to update to, and click <span class="strong strong"><strong>Save</strong></span>.
					</p><p class="simpara">
						The Input channel <span class="strong strong"><strong>Update status</strong></span> changes to <span class="strong strong"><strong>Update to &lt;product-version&gt; in progress</strong></span>, and you can review the progress of the cluster update by watching the progress bars for the Operators and nodes.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you are upgrading your cluster to the next minor version, like version 4.y to 4.(y+1), it is recommended to confirm your nodes are updated before deploying workloads that rely on a new feature. Any pools with worker nodes that are not yet updated are displayed on the <span class="strong strong"><strong>Cluster Settings</strong></span> page.
						</p></div></div></li><li class="listitem"><p class="simpara">
						After the update completes and the Cluster Version Operator refreshes the available updates, check if more updates are available in your current channel.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								If updates are available, continue to perform updates in the current channel until you can no longer update.
							</li><li class="listitem">
								If no updates are available, change the <span class="strong strong"><strong>Channel</strong></span> to the <code class="literal">stable-*</code>, <code class="literal">eus-*</code> or <code class="literal">fast-*</code> channel for the next minor version, and update to the version that you want in that channel.
							</li></ul></div><p class="simpara">
						You might need to perform several intermediate updates until you reach the version that you want.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							When you update a cluster that contains Red Hat Enterprise Linux (RHEL) worker machines, those workers temporarily become unavailable during the update process. You must run the upgrade playbook against each RHEL machine as it enters the <code class="literal">NotReady</code> state for the cluster to finish updating.
						</p></div></div></li></ol></div></section><section class="section" id="updating-cluster-rhel-compute-hooks"><div class="titlepage"><div><div><h2 class="title">12.3. Optional: Adding hooks to perform Ansible tasks on RHEL machines</h2></div></div></div><p>
				You can use <span class="emphasis"><em>hooks</em></span> to run Ansible tasks on the RHEL compute machines during the OpenShift Container Platform update.
			</p><section class="section" id="rhel-compute-about-hooks_updating-cluster-rhel-compute"><div class="titlepage"><div><div><h3 class="title">12.3.1. About Ansible hooks for upgrades</h3></div></div></div><p>
					When you update OpenShift Container Platform, you can run custom tasks on your Red Hat Enterprise Linux (RHEL) nodes during specific operations by using <span class="emphasis"><em>hooks</em></span>. Hooks allow you to provide files that define tasks to run before or after specific update tasks. You can use hooks to validate or modify custom infrastructure when you update the RHEL compute nodes in you OpenShift Container Platform cluster.
				</p><p>
					Because when a hook fails, the operation fails, you must design hooks that are idempotent, or can run multiple times and provide the same results.
				</p><p>
					Hooks have the following important limitations: - Hooks do not have a defined or versioned interface. They can use internal <code class="literal">openshift-ansible</code> variables, but it is possible that the variables will be modified or removed in future OpenShift Container Platform releases. - Hooks do not have error handling, so an error in a hook halts the update process. If you get an error, you must address the problem and then start the upgrade again.
				</p></section><section class="section" id="rhel-compute-using-hooks_updating-cluster-rhel-compute"><div class="titlepage"><div><div><h3 class="title">12.3.2. Configuring the Ansible inventory file to use hooks</h3></div></div></div><p>
					You define the hooks to use when you update the Red Hat Enterprise Linux (RHEL) compute machines, which are also known as worker machines, in the <code class="literal">hosts</code> inventory file under the <code class="literal">all:vars</code> section.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the machine that you used to add the RHEL compute machines cluster. You must have access to the <code class="literal">hosts</code> Ansible inventory file that defines your RHEL machines.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							After you design the hook, create a YAML file that defines the Ansible tasks for it. This file must be a set of tasks and cannot be a playbook, as shown in the following example:
						</p><pre class="screen yaml yaml">---
# Trivial example forcing an operator to acknowledge the start of an upgrade
# file=/home/user/openshift-ansible/hooks/pre_compute.yml

- name: note the start of a compute machine update
  debug:
      msg: "Compute machine upgrade of {{ inventory_hostname }} is about to start"

- name: require the user agree to start an upgrade
  pause:
      prompt: "Press Enter to start the compute machine update"</pre></li><li class="listitem"><p class="simpara">
							Modify the <code class="literal">hosts</code> Ansible inventory file to specify the hook files. The hook files are specified as parameter values in the <code class="literal">[all:vars]</code> section, as shown:
						</p><div class="formalpara"><p class="title"><strong>Example hook definitions in an inventory file</strong></p><p>
								
<pre class="screen">[all:vars]
openshift_node_pre_upgrade_hook=/home/user/openshift-ansible/hooks/pre_node.yml
openshift_node_post_upgrade_hook=/home/user/openshift-ansible/hooks/post_node.yml</pre>

							</p></div><p class="simpara">
							To avoid ambiguity in the paths to the hook, use absolute paths instead of a relative paths in their definitions.
						</p></li></ol></div></section><section class="section" id="rhel-compute-available-hooks_updating-cluster-rhel-compute"><div class="titlepage"><div><div><h3 class="title">12.3.3. Available hooks for RHEL compute machines</h3></div></div></div><p>
					You can use the following hooks when you update the Red Hat Enterprise Linux (RHEL) compute machines in your OpenShift Container Platform cluster.
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140049050987536" scope="col">Hook name</th><th align="left" valign="top" id="idm140049050986448" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140049050987536"> <p>
									<code class="literal">openshift_node_pre_cordon_hook</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140049050986448"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Runs <span class="strong strong"><strong>before</strong></span> each node is cordoned.
										</li><li class="listitem">
											This hook runs against <span class="strong strong"><strong>each node</strong></span> in serial.
										</li><li class="listitem">
											If a task must run against a different host, the task must use <a class="link" href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_delegation.html"><code class="literal">delegate_to</code> or <code class="literal">local_action</code></a>.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140049050987536"> <p>
									<code class="literal">openshift_node_pre_upgrade_hook</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140049050986448"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Runs <span class="strong strong"><strong>after</strong></span> each node is cordoned but <span class="strong strong"><strong>before</strong></span> it is updated.
										</li><li class="listitem">
											This hook runs against <span class="strong strong"><strong>each node</strong></span> in serial.
										</li><li class="listitem">
											If a task must run against a different host, the task must use <a class="link" href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_delegation.html"><code class="literal">delegate_to</code> or <code class="literal">local_action</code></a>.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140049050987536"> <p>
									<code class="literal">openshift_node_pre_uncordon_hook</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140049050986448"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Runs <span class="strong strong"><strong>after</strong></span> each node is updated but <span class="strong strong"><strong>before</strong></span> it is uncordoned.
										</li><li class="listitem">
											This hook runs against <span class="strong strong"><strong>each node</strong></span> in serial.
										</li><li class="listitem">
											If a task must run against a different host, they task must use <a class="link" href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_delegation.html"><code class="literal">delegate_to</code> or <code class="literal">local_action</code></a>.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140049050987536"> <p>
									<code class="literal">openshift_node_post_upgrade_hook</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140049050986448"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Runs <span class="strong strong"><strong>after</strong></span> each node uncordoned. It is the <span class="strong strong"><strong>last</strong></span> node update action.
										</li><li class="listitem">
											This hook runs against <span class="strong strong"><strong>each node</strong></span> in serial.
										</li><li class="listitem">
											If a task must run against a different host, the task must use <a class="link" href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_delegation.html"><code class="literal">delegate_to</code> or <code class="literal">local_action</code></a>.
										</li></ul></div>
								 </td></tr></tbody></table></div></section></section><section class="section" id="rhel-compute-updating-minor_updating-cluster-rhel-compute"><div class="titlepage"><div><div><h2 class="title">12.4. Updating RHEL compute machines in your cluster</h2></div></div></div><p>
				After you update your cluster, you must update the Red Hat Enterprise Linux (RHEL) compute machines in your cluster.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Red Hat Enterprise Linux (RHEL) versions 8.6, 8.7, and 8.8 are supported for RHEL compute machines.
				</p></div></div><p>
				You can also update your compute machines to another minor version of OpenShift Container Platform if you are using RHEL as the operating system. You do not need to exclude any RPM packages from RHEL when performing a minor version update.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You cannot upgrade RHEL 7 compute machines to RHEL 8. You must deploy new RHEL 8 hosts, and the old RHEL 7 hosts should be removed.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						You updated your cluster.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Because the RHEL machines require assets that are generated by the cluster to complete the update process, you must update the cluster before you update the RHEL worker machines in it.
						</p></div></div></li><li class="listitem">
						You have access to the local machine that you used to add the RHEL compute machines to your cluster. You must have access to the <code class="literal">hosts</code> Ansible inventory file that defines your RHEL machines and the <code class="literal">upgrade</code> playbook.
					</li><li class="listitem">
						For updates to a minor version, the RPM repository is using the same version of OpenShift Container Platform that is running on your cluster.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Stop and disable firewalld on the host:
					</p><pre class="programlisting language-terminal"># systemctl disable --now firewalld.service</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							By default, the base OS RHEL with "Minimal" installation option enables firewalld service. Having the firewalld service enabled on your host prevents you from accessing OpenShift Container Platform logs on the worker. Do not enable firewalld later if you wish to continue accessing OpenShift Container Platform logs on the worker.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Enable the repositories that are required for OpenShift Container Platform 4.13:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								On the machine that you run the Ansible playbooks, update the required repositories:
							</p><pre class="programlisting language-terminal"># subscription-manager repos --disable=rhocp-4.12-for-rhel-8-x86_64-rpms \
                             --enable=rhocp-4.13-for-rhel-8-x86_64-rpms</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									As of OpenShift Container Platform 4.11, the Ansible playbooks are provided only for RHEL 8. If a RHEL 7 system was used as a host for the OpenShift Container Platform 4.10 Ansible playbooks, you must either upgrade the Ansible host to RHEL 8, or create a new Ansible host on a RHEL 8 system and copy over the inventories from the old Ansible host.
								</p></div></div></li><li class="listitem"><p class="simpara">
								On the machine that you run the Ansible playbooks, update the Ansible package:
							</p><pre class="programlisting language-terminal"># yum swap ansible ansible-core</pre></li><li class="listitem"><p class="simpara">
								On the machine that you run the Ansible playbooks, update the required packages, including <code class="literal">openshift-ansible</code>:
							</p><pre class="programlisting language-terminal"># yum update openshift-ansible openshift-clients</pre></li><li class="listitem"><p class="simpara">
								On each RHEL compute node, update the required repositories:
							</p><pre class="programlisting language-terminal"># subscription-manager repos --disable=rhocp-4.12-for-rhel-8-x86_64-rpms \
                             --enable=rhocp-4.13-for-rhel-8-x86_64-rpms</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Update a RHEL worker machine:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Review your Ansible inventory file at <code class="literal">/&lt;path&gt;/inventory/hosts</code> and update its contents so that the RHEL 8 machines are listed in the <code class="literal">[workers]</code> section, as shown in the following example:
							</p><pre class="screen">[all:vars]
ansible_user=root
#ansible_become=True

openshift_kubeconfig_path="~/.kube/config"

[workers]
mycluster-rhel8-0.example.com
mycluster-rhel8-1.example.com
mycluster-rhel8-2.example.com
mycluster-rhel8-3.example.com</pre></li><li class="listitem"><p class="simpara">
								Change to the <code class="literal">openshift-ansible</code> directory:
							</p><pre class="programlisting language-terminal">$ cd /usr/share/ansible/openshift-ansible</pre></li><li class="listitem"><p class="simpara">
								Run the <code class="literal">upgrade</code> playbook:
							</p><pre class="programlisting language-terminal">$ ansible-playbook -i /&lt;path&gt;/inventory/hosts playbooks/upgrade.yml <span id="CO8-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO8-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										For <code class="literal">&lt;path&gt;</code>, specify the path to the Ansible inventory file that you created.
									</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									The <code class="literal">upgrade</code> playbook only upgrades the OpenShift Container Platform packages. It does not update the operating system packages.
								</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
						After you update all of the workers, confirm that all of your cluster nodes have updated to the new version:
					</p><pre class="programlisting language-terminal"># oc get node</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                        STATUS                        ROLES    AGE    VERSION
mycluster-control-plane-0   Ready                         master   145m   v1.26.0
mycluster-control-plane-1   Ready                         master   145m   v1.26.0
mycluster-control-plane-2   Ready                         master   145m   v1.26.0
mycluster-rhel8-0           Ready                         worker   98m    v1.26.0
mycluster-rhel8-1           Ready                         worker   98m    v1.26.0
mycluster-rhel8-2           Ready                         worker   98m    v1.26.0
mycluster-rhel8-3           Ready                         worker   98m    v1.26.0</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Optional: Update the operating system packages that were not updated by the <code class="literal">upgrade</code> playbook. To update packages that are not on 4.13, use the following command:
					</p><pre class="programlisting language-terminal"># yum update</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You do not need to exclude RPM packages if you are using the same RPM repository that you used when you installed 4.13.
						</p></div></div></li></ol></div></section></section><section class="chapter" id="updating-a-cluster-in-a-disconnected-environment"><div class="titlepage"><div><div><h1 class="title">Chapter 13. Updating a cluster in a disconnected environment</h1></div></div></div><section class="section" id="about-restricted-network-updates"><div class="titlepage"><div><div><h2 class="title">13.1. About cluster updates in a disconnected environment</h2></div></div></div><p>
				A disconnected environment is one in which your cluster nodes cannot access the internet. For this reason, you must populate a registry with the installation images. If your registry host cannot access both the internet and the cluster, you can mirror the images to a file system that is disconnected from that environment and then bring that host or removable media across that gap. If the local container registry and the cluster are connected to the mirror registry’s host, you can directly push the release images to the local registry.
			</p><p>
				A single container image registry is sufficient to host mirrored images for several clusters in the disconnected network.
			</p><section class="section" id="about-disconnected-updates-mirroring"><div class="titlepage"><div><div><h3 class="title">13.1.1. Mirroring the OpenShift Container Platform image repository</h3></div></div></div><p>
					To update your cluster in a disconnected environment, your cluster environment must have access to a mirror registry that has the necessary images and resources for your targeted update. The following page has instructions for mirroring images onto a repository in your disconnected cluster:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#mirroring-ocp-image-repository">Mirroring the OpenShift Container Platform image repository</a>
						</li></ul></div></section><section class="section" id="about-disconnected-updates-update"><div class="titlepage"><div><div><h3 class="title">13.1.2. Performing a cluster update in a disconnected environment</h3></div></div></div><p>
					You can use one of the following procedures to update a disconnected OpenShift Container Platform cluster:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-restricted-network-cluster-OSUS">Updating a cluster in a disconnected environment using the OpenShift Update Service</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-restricted-network-cluster">Updating a cluster in a disconnected environment without the OpenShift Update Service</a>
						</li></ul></div></section><section class="section" id="about-disconnected-updates-uninstalling-osus"><div class="titlepage"><div><div><h3 class="title">13.1.3. Uninstalling the OpenShift Update Service from a cluster</h3></div></div></div><p>
					You can use the following procedure to uninstall a local copy of the OpenShift Update Service (OSUS) from your cluster:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#uninstalling-osus">Uninstalling the OpenShift Update Service from a cluster</a>
						</li></ul></div></section></section><section class="section" id="mirroring-ocp-image-repository"><div class="titlepage"><div><div><h2 class="title">13.2. Mirroring the OpenShift Container Platform image repository</h2></div></div></div><p>
				You must mirror container images onto a mirror registry before you can update a cluster in a disconnected environment. You can also use this procedure in connected environments to ensure your clusters run only approved container images that have satisfied your organizational controls for external content.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Your mirror registry must be running at all times while the cluster is running.
				</p></div></div><p>
				The following steps outline the high-level workflow on how to mirror images to a mirror registry:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Install the OpenShift CLI (<code class="literal">oc</code>) on all devices being used to retrieve and push release images.
					</li><li class="listitem">
						Download the registry pull secret and add it to your cluster.
					</li><li class="listitem"><p class="simpara">
						If you use the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#mirroring-ocp-resources-ocmirror">oc-mirror OpenShift CLI (<code class="literal">oc</code>) plugin</a>:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Install the oc-mirror plugin on all devices being used to retrieve and push release images.
							</li><li class="listitem">
								Create an image set configuration file for the plugin to use when determining which release images to mirror. You can edit this configuration file later to change which release images that the plugin mirrors.
							</li><li class="listitem">
								Mirror your targeted release images directly to a mirror registry, or to removable media and then to a mirror registry.
							</li><li class="listitem">
								Configure your cluster to use the resources generated by the oc-mirror plugin.
							</li><li class="listitem">
								Repeat these steps as needed to update your mirror registry.
							</li></ol></div></li><li class="listitem"><p class="simpara">
						If you use the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-mirror-repository-adm-release-mirror_mirroring-ocp-image-repository"><code class="literal">oc adm release mirror</code> command</a>:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Set environment variables that correspond to your environment and the release images you want to mirror.
							</li><li class="listitem">
								Mirror your targeted release images directly to a mirror registry, or to removable media and then to a mirror registry.
							</li><li class="listitem">
								Repeat these steps as needed to update your mirror registry.
							</li></ol></div></li></ol></div><p>
				Compared to using the <code class="literal">oc adm release mirror</code> command, the oc-mirror plugin has the following advantages:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						It can mirror content other than container images.
					</li><li class="listitem">
						After mirroring images for the first time, it is easier to update images in the registry.
					</li><li class="listitem">
						The oc-mirror plugin provides an automated way to mirror the release payload from Quay, and also builds the latest graph data image for the OpenShift Update Service running in the disconnected environment.
					</li></ul></div><section class="section" id="prerequisites_updating-mirroring-disconnected"><div class="titlepage"><div><div><h3 class="title">13.2.1. Prerequisites</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							You must have a container image registry that supports <a class="link" href="https://docs.docker.com/registry/spec/manifest-v2-2">Docker v2-2</a> in the location that will host the OpenShift Container Platform cluster, such as Red Hat Quay.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If you use Red Hat Quay, you must use version 3.6 or later with the oc-mirror plugin. If you have an entitlement to Red Hat Quay, see the documentation on deploying Red Hat Quay <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3.6/html/deploy_red_hat_quay_for_proof-of-concept_non-production_purposes/">for proof-of-concept purposes</a> or <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3.6/html/deploy_red_hat_quay_on_openshift_with_the_quay_operator/">by using the Quay Operator</a>. If you need additional assistance selecting and installing a registry, contact your sales representative or Red Hat Support.
							</p></div></div><p class="simpara">
							If you do not have an existing solution for a container image registry, the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-mirroring-creating-registry">mirror registry for Red Hat OpenShift</a> is included in OpenShift Container Platform subscriptions. The <span class="emphasis"><em>mirror registry for Red Hat OpenShift</em></span> is a small-scale container registry that you can use to mirror OpenShift Container Platform container images in disconnected installations and updates.
						</p></li></ul></div></section><section class="section" id="updating-restricted-network-mirror-host"><div class="titlepage"><div><div><h3 class="title">13.2.2. Preparing your mirror host</h3></div></div></div><p>
					Before you perform the mirror procedure, you must prepare the host to retrieve content and push it to the remote location.
				</p><section class="section" id="cli-installing-cli_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h4 class="title">13.2.2.1. Installing the OpenShift CLI by downloading the binary</h4></div></div></div><p>
						You can install the OpenShift CLI (<code class="literal">oc</code>) to interact with OpenShift Container Platform from a command-line interface. You can install <code class="literal">oc</code> on Linux, Windows, or macOS.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							If you installed an earlier version of <code class="literal">oc</code>, you cannot use it to complete all of the commands in OpenShift Container Platform 4.13. Download and install the new version of <code class="literal">oc</code>. If you are upgrading a cluster in a disconnected environment, install the <code class="literal">oc</code> version that you plan to upgrade to.
						</p></div></div><h6 id="installing-the-openshift-cli-on-linux">Installing the OpenShift CLI on Linux</h6><p>
						You can install the OpenShift CLI (<code class="literal">oc</code>) binary on Linux by using the following procedure.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Navigate to the <a class="link" href="https://access.redhat.com/downloads/content/290">OpenShift Container Platform downloads page</a> on the Red Hat Customer Portal.
							</li><li class="listitem">
								Select the architecture from the <span class="strong strong"><strong>Product Variant</strong></span> drop-down list.
							</li><li class="listitem">
								Select the appropriate version from the <span class="strong strong"><strong>Version</strong></span> drop-down list.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Download Now</strong></span> next to the <span class="strong strong"><strong>OpenShift v4.13 Linux Client</strong></span> entry and save the file.
							</li><li class="listitem"><p class="simpara">
								Unpack the archive:
							</p><pre class="programlisting language-terminal">$ tar xvf &lt;file&gt;</pre></li><li class="listitem"><p class="simpara">
								Place the <code class="literal">oc</code> binary in a directory that is on your <code class="literal">PATH</code>.
							</p><p class="simpara">
								To check your <code class="literal">PATH</code>, execute the following command:
							</p><pre class="programlisting language-terminal">$ echo $PATH</pre></li></ol></div><p>
						After you install the OpenShift CLI, it is available using the <code class="literal">oc</code> command:
					</p><pre class="programlisting language-terminal">$ oc &lt;command&gt;</pre><h6 id="installing-the-openshift-cli-on-windows">Installing the OpenShift CLI on Windows</h6><p>
						You can install the OpenShift CLI (<code class="literal">oc</code>) binary on Windows by using the following procedure.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Navigate to the <a class="link" href="https://access.redhat.com/downloads/content/290">OpenShift Container Platform downloads page</a> on the Red Hat Customer Portal.
							</li><li class="listitem">
								Select the appropriate version from the <span class="strong strong"><strong>Version</strong></span> drop-down list.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Download Now</strong></span> next to the <span class="strong strong"><strong>OpenShift v4.13 Windows Client</strong></span> entry and save the file.
							</li><li class="listitem">
								Unzip the archive with a ZIP program.
							</li><li class="listitem"><p class="simpara">
								Move the <code class="literal">oc</code> binary to a directory that is on your <code class="literal">PATH</code>.
							</p><p class="simpara">
								To check your <code class="literal">PATH</code>, open the command prompt and execute the following command:
							</p><pre class="programlisting language-terminal">C:\&gt; path</pre></li></ol></div><p>
						After you install the OpenShift CLI, it is available using the <code class="literal">oc</code> command:
					</p><pre class="programlisting language-terminal">C:\&gt; oc &lt;command&gt;</pre><h6 id="installing-the-openshift-cli-on-macos">Installing the OpenShift CLI on macOS</h6><p>
						You can install the OpenShift CLI (<code class="literal">oc</code>) binary on macOS by using the following procedure.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Navigate to the <a class="link" href="https://access.redhat.com/downloads/content/290">OpenShift Container Platform downloads page</a> on the Red Hat Customer Portal.
							</li><li class="listitem">
								Select the appropriate version from the <span class="strong strong"><strong>Version</strong></span> drop-down list.
							</li><li class="listitem"><p class="simpara">
								Click <span class="strong strong"><strong>Download Now</strong></span> next to the <span class="strong strong"><strong>OpenShift v4.13 macOS Client</strong></span> entry and save the file.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									For macOS arm64, choose the <span class="strong strong"><strong>OpenShift v4.13 macOS arm64 Client</strong></span> entry.
								</p></div></div></li><li class="listitem">
								Unpack and unzip the archive.
							</li><li class="listitem"><p class="simpara">
								Move the <code class="literal">oc</code> binary to a directory on your PATH.
							</p><p class="simpara">
								To check your <code class="literal">PATH</code>, open a terminal and execute the following command:
							</p><pre class="programlisting language-terminal">$ echo $PATH</pre></li></ol></div><p>
						After you install the OpenShift CLI, it is available using the <code class="literal">oc</code> command:
					</p><pre class="programlisting language-terminal">$ oc &lt;command&gt;</pre><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/cli_tools/#cli-installing-plugins_cli-extend-plugins">Installing and using CLI plugins</a>
							</li></ul></div></section><section class="section" id="installation-adding-registry-pull-secret_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h4 class="title">13.2.2.2. Configuring credentials that allow images to be mirrored</h4></div></div></div><p>
						Create a container image registry credentials file that allows mirroring images from Red Hat to your mirror.
					</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							Do not use this image registry credentials file as the pull secret when you install a cluster. If you provide this file when you install cluster, all of the machines in the cluster will have write access to your mirror registry.
						</p></div></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							This process requires that you have write access to a container image registry on the mirror registry and adds the credentials to a registry pull secret.
						</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You configured a mirror registry to use in your disconnected environment.
							</li><li class="listitem">
								You identified an image repository location on your mirror registry to mirror images into.
							</li><li class="listitem">
								You provisioned a mirror registry account that allows images to be uploaded to that image repository.
							</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							Complete the following steps on the installation host:
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								Download your <code class="literal">registry.redhat.io</code> <a class="link" href="https://console.redhat.com/openshift/install/pull-secret">pull secret from the Red Hat OpenShift Cluster Manager</a>.
							</li><li class="listitem"><p class="simpara">
								Make a copy of your pull secret in JSON format:
							</p><pre class="programlisting language-terminal">$ cat ./pull-secret | jq . &gt; &lt;path&gt;/&lt;pull_secret_file_in_json&gt; <span id="CO9-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO9-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify the path to the folder to store the pull secret in and a name for the JSON file that you create.
									</div></dd></dl></div><p class="simpara">
								The contents of the file resemble the following example:
							</p><pre class="programlisting language-json">{
  "auths": {
    "cloud.openshift.com": {
      "auth": "b3BlbnNo...",
      "email": "you@example.com"
    },
    "quay.io": {
      "auth": "b3BlbnNo...",
      "email": "you@example.com"
    },
    "registry.connect.redhat.com": {
      "auth": "NTE3Njg5Nj...",
      "email": "you@example.com"
    },
    "registry.redhat.io": {
      "auth": "NTE3Njg5Nj...",
      "email": "you@example.com"
    }
  }
}</pre></li><li class="listitem">
								Optional: If using the oc-mirror plugin, save the file either as <code class="literal">~/.docker/config.json</code> or <code class="literal">$XDG_RUNTIME_DIR/containers/auth.json</code>.
							</li><li class="listitem"><p class="simpara">
								Generate the base64-encoded user name and password or token for your mirror registry:
							</p><pre class="programlisting language-terminal">$ echo -n '&lt;user_name&gt;:&lt;password&gt;' | base64 -w0 <span id="CO10-1"><!--Empty--></span><span class="callout">1</span>
BGVtbYk3ZHAtqXs=</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO10-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										For <code class="literal">&lt;user_name&gt;</code> and <code class="literal">&lt;password&gt;</code>, specify the user name and password that you configured for your registry.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Edit the JSON file and add a section that describes your registry to it:
							</p><pre class="programlisting language-json">  "auths": {
    "&lt;mirror_registry&gt;": { <span id="CO11-1"><!--Empty--></span><span class="callout">1</span>
      "auth": "&lt;credentials&gt;", <span id="CO11-2"><!--Empty--></span><span class="callout">2</span>
      "email": "you@example.com"
    }
  },</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO11-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										For <code class="literal">&lt;mirror_registry&gt;</code>, specify the registry domain name, and optionally the port, that your mirror registry uses to serve content. For example, <code class="literal">registry.example.com</code> or <code class="literal">registry.example.com:8443</code>
									</div></dd><dt><a href="#CO11-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										For <code class="literal">&lt;credentials&gt;</code>, specify the base64-encoded user name and password for the mirror registry.
									</div></dd></dl></div><p class="simpara">
								The file resembles the following example:
							</p><pre class="programlisting language-json">{
  "auths": {
    "registry.example.com": {
      "auth": "BGVtbYk3ZHAtqXs=",
      "email": "you@example.com"
    },
    "cloud.openshift.com": {
      "auth": "b3BlbnNo...",
      "email": "you@example.com"
    },
    "quay.io": {
      "auth": "b3BlbnNo...",
      "email": "you@example.com"
    },
    "registry.connect.redhat.com": {
      "auth": "NTE3Njg5Nj...",
      "email": "you@example.com"
    },
    "registry.redhat.io": {
      "auth": "NTE3Njg5Nj...",
      "email": "you@example.com"
    }
  }
}</pre></li></ol></div></section></section><section class="section" id="mirroring-ocp-resources-ocmirror"><div class="titlepage"><div><div><h3 class="title">13.2.3. Mirroring resources using the oc-mirror plugin</h3></div></div></div><p>
					You can use the oc-mirror OpenShift CLI (<code class="literal">oc</code>) plugin to mirror images to a mirror registry in your fully or partially disconnected environments. You must run oc-mirror from a system with internet connectivity to download the required images from the official Red Hat registries.
				</p><section class="section" id="installation-oc-mirror-about_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h4 class="title">13.2.3.1. About the oc-mirror plugin</h4></div></div></div><p>
						You can use the oc-mirror OpenShift CLI (<code class="literal">oc</code>) plugin to mirror all required OpenShift Container Platform content and other images to your mirror registry by using a single tool. It provides the following features:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Provides a centralized method to mirror OpenShift Container Platform releases, Operators, helm charts, and other images.
							</li><li class="listitem">
								Maintains update paths for OpenShift Container Platform and Operators.
							</li><li class="listitem">
								Uses a declarative image set configuration file to include only the OpenShift Container Platform releases, Operators, and images that your cluster needs.
							</li><li class="listitem">
								Performs incremental mirroring, which reduces the size of future image sets.
							</li><li class="listitem">
								Prunes images from the target mirror registry that were excluded from the image set configuration since the previous execution.
							</li><li class="listitem">
								Optionally generates supporting artifacts for OpenShift Update Service (OSUS) usage.
							</li></ul></div><p>
						When using the oc-mirror plugin, you specify which content to mirror in an image set configuration file. In this YAML file, you can fine-tune the configuration to only include the OpenShift Container Platform releases and Operators that your cluster needs. This reduces the amount of data that you need to download and transfer. The oc-mirror plugin can also mirror arbitrary helm charts and additional container images to assist users in seamlessly synchronizing their workloads onto mirror registries.
					</p><p>
						The first time you run the oc-mirror plugin, it populates your mirror registry with the required content to perform your disconnected cluster installation or update. In order for your disconnected cluster to continue receiving updates, you must keep your mirror registry updated. To update your mirror registry, you run the oc-mirror plugin using the same configuration as the first time you ran it. The oc-mirror plugin references the metadata from the storage backend and only downloads what has been released since the last time you ran the tool. This provides update paths for OpenShift Container Platform and Operators and performs dependency resolution as required.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							When using the oc-mirror CLI plugin to populate a mirror registry, any further updates to the mirror registry must be made using the oc-mirror tool.
						</p></div></div></section><section class="section" id="oc-mirror-support_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h4 class="title">13.2.3.2. oc-mirror compatibility and support</h4></div></div></div><p>
						The oc-mirror plugin supports mirroring OpenShift Container Platform payload images and Operator catalogs for OpenShift Container Platform versions 4.10 and later.
					</p><p>
						Use the latest available version of the oc-mirror plugin regardless of which versions of OpenShift Container Platform you need to mirror.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							If you used the Technology Preview OCI local catalogs feature for the oc-mirror plugin for OpenShift Container Platform 4.12, you can no longer use the OCI local catalogs feature of the oc-mirror plugin to copy a catalog locally and convert it to OCI format as a first step to mirroring to a fully disconnected cluster.
						</p></div></div></section><section class="section" id="installation-about-mirror-registry_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h4 class="title">13.2.3.3. About the mirror registry</h4></div></div></div><p>
						You can mirror the images that are required for OpenShift Container Platform installation and subsequent product updates to a container mirror registry that supports <a class="link" href="https://docs.docker.com/registry/spec/manifest-v2-2">Docker v2-2</a>, such as Red Hat Quay. If you do not have access to a large-scale container registry, you can use the <span class="emphasis"><em>mirror registry for Red Hat OpenShift</em></span>, which is a small-scale container registry included with OpenShift Container Platform subscriptions.
					</p><p>
						Regardless of your chosen registry, the procedure to mirror content from Red Hat hosted sites on the internet to an isolated image registry is the same. After you mirror the content, you configure each cluster to retrieve this content from your mirror registry.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							The OpenShift image registry cannot be used as the target registry because it does not support pushing without a tag, which is required during the mirroring process.
						</p></div></div><p>
						If choosing a container registry that is not the <span class="emphasis"><em>mirror registry for Red Hat OpenShift</em></span>, it must be reachable by every machine in the clusters that you provision. If the registry is unreachable, installation, updating, or normal operations such as workload relocation might fail. For that reason, you must run mirror registries in a highly available way, and the mirror registries must at least match the production availability of your OpenShift Container Platform clusters.
					</p><p>
						When you populate your mirror registry with OpenShift Container Platform images, you can follow two scenarios. If you have a host that can access both the internet and your mirror registry, but not your cluster nodes, you can directly mirror the content from that machine. This process is referred to as <span class="emphasis"><em>connected mirroring</em></span>. If you have no such host, you must mirror the images to a file system and then bring that host or removable media into your restricted environment. This process is referred to as <span class="emphasis"><em>disconnected mirroring</em></span>.
					</p><p>
						For mirrored registries, to view the source of pulled images, you must review the <code class="literal">Trying to access</code> log entry in the CRI-O logs. Other methods to view the image pull source, such as using the <code class="literal">crictl images</code> command on a node, show the non-mirrored image name, even though the image is pulled from the mirrored location.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Red Hat does not test third party registries with OpenShift Container Platform.
						</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								For information about viewing the CRI-O logs to view the image source, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#viewing-the-image-pull-source_validating-an-installation">Viewing the image pull source</a>.
							</li></ul></div></section><section class="section" id="installation-oc-mirror-installing-plugin_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h4 class="title">13.2.3.4. Installing the oc-mirror OpenShift CLI plugin</h4></div></div></div><p>
						To use the oc-mirror OpenShift CLI plugin to mirror registry images, you must install the plugin. If you are mirroring image sets in a fully disconnected environment, ensure that you install the oc-mirror plugin on the host with internet access and the host in the disconnected environment with access to the mirror registry.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Download the oc-mirror CLI plugin.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Navigate to the <a class="link" href="https://console.redhat.com/openshift/downloads">Downloads</a> page of the <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager Hybrid Cloud Console</a>.
									</li><li class="listitem">
										Under the <span class="strong strong"><strong>OpenShift disconnected installation tools</strong></span> section, click <span class="strong strong"><strong>Download</strong></span> for <span class="strong strong"><strong>OpenShift Client (oc) mirror plugin</strong></span> and save the file.
									</li></ol></div></li><li class="listitem"><p class="simpara">
								Extract the archive:
							</p><pre class="programlisting language-terminal">$ tar xvzf oc-mirror.tar.gz</pre></li><li class="listitem"><p class="simpara">
								If necessary, update the plugin file to be executable:
							</p><pre class="programlisting language-terminal">$ chmod +x oc-mirror</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Do not rename the <code class="literal">oc-mirror</code> file.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Install the oc-mirror CLI plugin by placing the file in your <code class="literal">PATH</code>, for example, <code class="literal">/usr/local/bin</code>:
							</p><pre class="programlisting language-terminal">$ sudo mv oc-mirror /usr/local/bin/.</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Run <code class="literal">oc mirror help</code> to verify that the plugin was successfully installed:
							</p><pre class="programlisting language-terminal">$ oc mirror help</pre></li></ul></div></section><section class="section" id="oc-mirror-creating-image-set-config_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h4 class="title">13.2.3.5. Creating the image set configuration</h4></div></div></div><p>
						Before you can use the oc-mirror plugin to mirror image sets, you must create an image set configuration file. This image set configuration file defines which OpenShift Container Platform releases, Operators, and other images to mirror, along with other configuration settings for the oc-mirror plugin.
					</p><p>
						You must specify a storage backend in the image set configuration file. This storage backend can be a local directory or a registry that supports <a class="link" href="https://docs.docker.com/registry/spec/manifest-v2-2">Docker v2-2</a>. The oc-mirror plugin stores metadata in this storage backend during image set creation.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Do not delete or modify the metadata that is generated by the oc-mirror plugin. You must use the same storage backend every time you run the oc-mirror plugin for the same mirror registry.
						</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have created a container image registry credentials file. For instructions, see <span class="emphasis"><em>Configuring credentials that allow images to be mirrored</em></span>.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Use the <code class="literal">oc mirror init</code> command to create a template for the image set configuration and save it to a file called <code class="literal">imageset-config.yaml</code>:
							</p><pre class="programlisting language-terminal">$ oc mirror init --registry example.com/mirror/oc-mirror-metadata &gt; imageset-config.yaml <span id="CO12-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO12-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal">example.com/mirror/oc-mirror-metadata</code> with the location of your registry for the storage backend.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Edit the file and adjust the settings as necessary:
							</p><pre class="programlisting language-yaml">kind: ImageSetConfiguration
apiVersion: mirror.openshift.io/v1alpha2
archiveSize: 4                                                      <span id="CO13-1"><!--Empty--></span><span class="callout">1</span>
storageConfig:                                                      <span id="CO13-2"><!--Empty--></span><span class="callout">2</span>
  registry:
    imageURL: example.com/mirror/oc-mirror-metadata                 <span id="CO13-3"><!--Empty--></span><span class="callout">3</span>
    skipTLS: false
mirror:
  platform:
    channels:
    - name: stable-4.13                                             <span id="CO13-4"><!--Empty--></span><span class="callout">4</span>
      type: ocp
    graph: true                                                     <span id="CO13-5"><!--Empty--></span><span class="callout">5</span>
  operators:
  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.13  <span id="CO13-6"><!--Empty--></span><span class="callout">6</span>
    packages:
    - name: serverless-operator                                     <span id="CO13-7"><!--Empty--></span><span class="callout">7</span>
      channels:
      - name: stable                                                <span id="CO13-8"><!--Empty--></span><span class="callout">8</span>
  additionalImages:
  - name: registry.redhat.io/ubi9/ubi:latest                        <span id="CO13-9"><!--Empty--></span><span class="callout">9</span>
  helm: {}</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO13-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Add <code class="literal">archiveSize</code> to set the maximum size, in GiB, of each file within the image set.
									</div></dd><dt><a href="#CO13-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Set the back-end location to save the image set metadata to. This location can be a registry or local directory. It is required to specify <code class="literal">storageConfig</code> values.
									</div></dd><dt><a href="#CO13-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Set the registry URL for the storage backend.
									</div></dd><dt><a href="#CO13-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										Set the channel to retrieve the OpenShift Container Platform images from.
									</div></dd><dt><a href="#CO13-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										Add <code class="literal">graph: true</code> to build and push the graph-data image to the mirror registry. The graph-data image is required to create OpenShift Update Service (OSUS). The <code class="literal">graph: true</code> field also generates the <code class="literal">UpdateService</code> custom resource manifest. The <code class="literal">oc</code> command-line interface (CLI) can use the <code class="literal">UpdateService</code> custom resource manifest to create OSUS. For more information, see <span class="emphasis"><em>About the OpenShift Update Service</em></span>.
									</div></dd><dt><a href="#CO13-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										Set the Operator catalog to retrieve the OpenShift Container Platform images from.
									</div></dd><dt><a href="#CO13-7"><span class="callout">7</span></a> </dt><dd><div class="para">
										Specify only certain Operator packages to include in the image set. Remove this field to retrieve all packages in the catalog.
									</div></dd><dt><a href="#CO13-8"><span class="callout">8</span></a> </dt><dd><div class="para">
										Specify only certain channels of the Operator packages to include in the image set. You must always include the default channel for the Operator package even if you do not use the bundles in that channel. You can find the default channel by running the following command: <code class="literal">oc mirror list operators --catalog=&lt;catalog_name&gt; --package=&lt;package_name&gt;</code>.
									</div></dd><dt><a href="#CO13-9"><span class="callout">9</span></a> </dt><dd><div class="para">
										Specify any additional images to include in image set.
									</div></dd></dl></div><p class="simpara">
								See <span class="emphasis"><em>Image set configuration parameters</em></span> for the full list of parameters and <span class="emphasis"><em>Image set configuration examples</em></span> for various mirroring use cases.
							</p></li><li class="listitem"><p class="simpara">
								Save the updated file.
							</p><p class="simpara">
								This image set configuration file is required by the <code class="literal">oc mirror</code> command when mirroring content.
							</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#oc-mirror-imageset-config-params_mirroring-ocp-image-repository">Image set configuration parameters</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#oc-mirror-image-set-examples_mirroring-ocp-image-repository">Image set configuration examples</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-service-overview_updating-restricted-network-cluster-osus">About the OpenShift Update Service</a>
							</li></ul></div></section><section class="section" id="mirroring-image-set"><div class="titlepage"><div><div><h4 class="title">13.2.3.6. Mirroring an image set to a mirror registry</h4></div></div></div><p>
						You can use the oc-mirror CLI plugin to mirror images to a mirror registry in a <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#mirroring-image-set-partial">partially disconnected environment</a> or in a <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#mirroring-image-set-full">fully disconnected environment</a>.
					</p><p>
						The following procedures assume that you already have your mirror registry set up.
					</p><section class="section" id="mirroring-image-set-partial"><div class="titlepage"><div><div><h5 class="title">13.2.3.6.1. Mirroring an image set in a partially disconnected environment</h5></div></div></div><p>
							In a partially disconnected environment, you can mirror an image set directly to the target mirror registry.
						</p><section class="section" id="oc-mirror-mirror-to-mirror_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h6 class="title">13.2.3.6.1.1. Mirroring from mirror to mirror</h6></div></div></div><p>
								You can use the oc-mirror plugin to mirror an image set directly to a target mirror registry that is accessible during image set creation.
							</p><p>
								You are required to specify a storage backend in the image set configuration file. This storage backend can be a local directory or a Docker v2 registry. The oc-mirror plugin stores metadata in this storage backend during image set creation.
							</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									Do not delete or modify the metadata that is generated by the oc-mirror plugin. You must use the same storage backend every time you run the oc-mirror plugin for the same mirror registry.
								</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
										You have access to the internet to obtain the necessary container images.
									</li><li class="listitem">
										You have installed the OpenShift CLI (<code class="literal">oc</code>).
									</li><li class="listitem">
										You have installed the <code class="literal">oc-mirror</code> CLI plugin.
									</li><li class="listitem">
										You have created the image set configuration file.
									</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
										Run the <code class="literal">oc mirror</code> command to mirror the images from the specified image set configuration to a specified registry:
									</p><pre class="programlisting language-terminal">$ oc mirror --config=./imageset-config.yaml \<span id="CO14-1"><!--Empty--></span><span class="callout">1</span>
  docker://registry.example:5000             <span id="CO14-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO14-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Pass in the image set configuration file that was created. This procedure assumes that it is named <code class="literal">imageset-config.yaml</code>.
											</div></dd><dt><a href="#CO14-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Specify the registry to mirror the image set file to. The registry must start with <code class="literal">docker://</code>. If you specify a top-level namespace for the mirror registry, you must also use this same namespace on subsequent executions.
											</div></dd></dl></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem">
										Navigate into the <code class="literal">oc-mirror-workspace/</code> directory that was generated.
									</li><li class="listitem">
										Navigate into the results directory, for example, <code class="literal">results-1639608409/</code>.
									</li><li class="listitem">
										Verify that YAML files are present for the <code class="literal">ImageContentSourcePolicy</code> and <code class="literal">CatalogSource</code> resources.
									</li></ol></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
										Configure your cluster to use the resources generated by oc-mirror.
									</li></ul></div><div class="itemizedlist"><p class="title"><strong>Troubleshooting</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
										<a class="link" href="https://access.redhat.com/solutions/7032017">Unable to retrieve source image</a>.
									</li></ul></div></section></section><section class="section" id="mirroring-image-set-full"><div class="titlepage"><div><div><h5 class="title">13.2.3.6.2. Mirroring an image set in a fully disconnected environment</h5></div></div></div><p>
							To mirror an image set in a fully disconnected environment, you must first <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#oc-mirror-mirror-to-disk_mirroring-ocp-image-repository">mirror the image set to disk</a>, then <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#oc-mirror-disk-to-mirror_mirroring-ocp-image-repository">mirror the image set file on disk to a mirror</a>.
						</p><section class="section" id="oc-mirror-mirror-to-disk_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h6 class="title">13.2.3.6.2.1. Mirroring from mirror to disk</h6></div></div></div><p>
								You can use the oc-mirror plugin to generate an image set and save the contents to disk. The generated image set can then be transferred to the disconnected environment and mirrored to the target registry.
							</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									Depending on the configuration specified in the image set configuration file, using oc-mirror to mirror images might download several hundreds of gigabytes of data to disk.
								</p><p>
									The initial image set download when you populate the mirror registry is often the largest. Because you only download the images that changed since the last time you ran the command, when you run the oc-mirror plugin again, the generated image set is often smaller.
								</p></div></div><p>
								You are required to specify a storage backend in the image set configuration file. This storage backend can be a local directory or a docker v2 registry. The oc-mirror plugin stores metadata in this storage backend during image set creation.
							</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									Do not delete or modify the metadata that is generated by the oc-mirror plugin. You must use the same storage backend every time you run the oc-mirror plugin for the same mirror registry.
								</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
										You have access to the internet to obtain the necessary container images.
									</li><li class="listitem">
										You have installed the OpenShift CLI (<code class="literal">oc</code>).
									</li><li class="listitem">
										You have installed the <code class="literal">oc-mirror</code> CLI plugin.
									</li><li class="listitem">
										You have created the image set configuration file.
									</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
										Run the <code class="literal">oc mirror</code> command to mirror the images from the specified image set configuration to disk:
									</p><pre class="programlisting language-terminal">$ oc mirror --config=./imageset-config.yaml \<span id="CO15-1"><!--Empty--></span><span class="callout">1</span>
  file://&lt;path_to_output_directory&gt;          <span id="CO15-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO15-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Pass in the image set configuration file that was created. This procedure assumes that it is named <code class="literal">imageset-config.yaml</code>.
											</div></dd><dt><a href="#CO15-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Specify the target directory where you want to output the image set file. The target directory path must start with <code class="literal">file://</code>.
											</div></dd></dl></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
										Navigate to your output directory:
									</p><pre class="programlisting language-terminal">$ cd &lt;path_to_output_directory&gt;</pre></li><li class="listitem"><p class="simpara">
										Verify that an image set <code class="literal">.tar</code> file was created:
									</p><pre class="programlisting language-terminal">$ ls</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-text">mirror_seq1_000000.tar</pre>

										</p></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
										Transfer the image set .tar file to the disconnected environment.
									</li></ul></div><div class="itemizedlist"><p class="title"><strong>Troubleshooting</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
										<a class="link" href="https://access.redhat.com/solutions/7032017">Unable to retrieve source image</a>.
									</li></ul></div></section><section class="section" id="oc-mirror-disk-to-mirror_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h6 class="title">13.2.3.6.2.2. Mirroring from disk to mirror</h6></div></div></div><p>
								You can use the oc-mirror plugin to mirror the contents of a generated image set to the target mirror registry.
							</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
										You have installed the OpenShift CLI (<code class="literal">oc</code>) in the disconnected environment.
									</li><li class="listitem">
										You have installed the <code class="literal">oc-mirror</code> CLI plugin in the disconnected environment.
									</li><li class="listitem">
										You have generated the image set file by using the <code class="literal">oc mirror</code> command.
									</li><li class="listitem">
										You have transferred the image set file to the disconnected environment.
									</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
										Run the <code class="literal">oc mirror</code> command to process the image set file on disk and mirror the contents to a target mirror registry:
									</p><pre class="programlisting language-terminal">$ oc mirror --from=./mirror_seq1_000000.tar \<span id="CO16-1"><!--Empty--></span><span class="callout">1</span>
  docker://registry.example:5000             <span id="CO16-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO16-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Pass in the image set .tar file to mirror, named <code class="literal">mirror_seq1_000000.tar</code> in this example. If an <code class="literal">archiveSize</code> value was specified in the image set configuration file, the image set might be broken up into multiple .tar files. In this situation, you can pass in a directory that contains the image set .tar files.
											</div></dd><dt><a href="#CO16-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Specify the registry to mirror the image set file to. The registry must start with <code class="literal">docker://</code>. If you specify a top-level namespace for the mirror registry, you must also use this same namespace on subsequent executions.
											</div></dd></dl></div><p class="simpara">
										This command updates the mirror registry with the image set and generates the <code class="literal">ImageContentSourcePolicy</code> and <code class="literal">CatalogSource</code> resources.
									</p></li></ul></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem">
										Navigate into the <code class="literal">oc-mirror-workspace/</code> directory that was generated.
									</li><li class="listitem">
										Navigate into the results directory, for example, <code class="literal">results-1639608409/</code>.
									</li><li class="listitem">
										Verify that YAML files are present for the <code class="literal">ImageContentSourcePolicy</code> and <code class="literal">CatalogSource</code> resources.
									</li></ol></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
										Configure your cluster to use the resources generated by oc-mirror.
									</li></ul></div><div class="itemizedlist"><p class="title"><strong>Troubleshooting</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
										<a class="link" href="https://access.redhat.com/solutions/7032017">Unable to retrieve source image</a>.
									</li></ul></div></section></section></section><section class="section" id="oc-mirror-updating-cluster-manifests_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h4 class="title">13.2.3.7. Configuring your cluster to use the resources generated by oc-mirror</h4></div></div></div><p>
						After you have mirrored your image set to the mirror registry, you must apply the generated <code class="literal">ImageContentSourcePolicy</code>, <code class="literal">CatalogSource</code>, and release image signature resources into the cluster.
					</p><p>
						The <code class="literal">ImageContentSourcePolicy</code> resource associates the mirror registry with the source registry and redirects image pull requests from the online registries to the mirror registry. The <code class="literal">CatalogSource</code> resource is used by Operator Lifecycle Manager (OLM) to retrieve information about the available Operators in the mirror registry. The release image signatures are used to verify the mirrored release images.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have mirrored the image set to the registry mirror in the disconnected environment.
							</li><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Log in to the OpenShift CLI as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem"><p class="simpara">
								Apply the YAML files from the results directory to the cluster by running the following command:
							</p><pre class="programlisting language-terminal">$ oc apply -f ./oc-mirror-workspace/results-1639608409/</pre></li><li class="listitem"><p class="simpara">
								Apply the release image signatures to the cluster by running the following command:
							</p><pre class="programlisting language-terminal">$ oc apply -f ./oc-mirror-workspace/results-1639608409/release-signatures/</pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Verify that the <code class="literal">ImageContentSourcePolicy</code> resources were successfully installed by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get imagecontentsourcepolicy --all-namespaces</pre></li><li class="listitem"><p class="simpara">
								Verify that the <code class="literal">CatalogSource</code> resources were successfully installed by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get catalogsource --all-namespaces</pre></li></ol></div></section><section class="section" id="updating-mirror-registry-content"><div class="titlepage"><div><div><h4 class="title">13.2.3.8. Keeping your mirror registry content updated</h4></div></div></div><p>
						After you populate your target mirror registry with the initial image set, you must update it regularly so that it has the latest content. If possible, you can set up a cron job to update the mirror registry on a regular basis.
					</p><p>
						Update your image set configuration to add or remove OpenShift Container Platform and Operator releases as necessary. Removed images are pruned from the mirror registry.
					</p><section class="section" id="oc-mirror-updating-registry-about_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h5 class="title">13.2.3.8.1. About updating your mirror registry content</h5></div></div></div><p>
							When you run the oc-mirror plugin again, it generates an image set that only contains new and updated images since the previous execution. Because it only pulls in the differences since the previous image set was created, the generated image set is often smaller and faster to process than the initial image set.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Generated image sets are sequential and must be pushed to the target mirror registry in order. You can derive the sequence number from the file name of the generated image set archive file.
							</p></div></div><h6 id="adding-new-and-updated-images">Adding new and updated images</h6><p>
							Depending on the settings in your image set configuration, future executions of oc-mirror can mirror additional new and updated images. Review the settings in your image set configuration to ensure that you are retrieving new versions as necessary. For example, you can set the minimum and maximum versions of Operators to mirror if you want to restrict to specific versions. Alternatively, you can set the minimum version as a starting point to mirror, but keep the version range open so you keep receiving new Operator versions on future executions of oc-mirror. Omitting any minimum or maximum version gives you the full version history of an Operator in a channel. Omitting explicitly named channels gives you all releases in all channels of the specified Operator. Omitting any named Operator gives you the entire catalog of all Operators and all their versions ever released.
						</p><p>
							All these constraints and conditions are evaluated against the publicly released content by Red Hat on every invocation of oc-mirror. This way, it automatically picks up new releases and entirely new Operators. Constraints can be specified by only listing a desired set of Operators, which will not automatically add other newly released Operators into the mirror set. You can also specify a particular release channel, which limits mirroring to just this channel and not any new channels that have been added. This is important for Operator products, such as Red Hat Quay, that use different release channels for their minor releases. Lastly, you can specify a maximum version of a particular Operator, which causes the tool to only mirror the specified version range so that you do not automatically get any newer releases past the maximum version mirrored. In all these cases, you must update the image set configuration file to broaden the scope of the mirroring of Operators to get other Operators, new channels, and newer versions of Operators to be available in your target registry.
						</p><p>
							It is recommended to align constraints like channel specification or version ranges with the release strategy that a particular Operator has chosen. For example, when the Operator uses a <code class="literal">stable</code> channel, you should restrict mirroring to that channel and potentially a minimum version to find the right balance between download volume and getting stable updates regularly. If the Operator chooses a release version channel scheme, for example <code class="literal">stable-3.7</code>, you should mirror all releases in that channel. This allows you to keep receiving patch versions of the Operator, for example <code class="literal">3.7.1</code>. You can also regularly adjust the image set configuration to add channels for new product releases, for example <code class="literal">stable-3.8</code>.
						</p><h6 id="pruning-images">Pruning images</h6><p>
							Images are pruned automatically from the target mirror registry if they are no longer included in the latest image set that was generated and mirrored. This allows you to easily manage and clean up unneeded content and reclaim storage resources.
						</p><p>
							If there are OpenShift Container Platform releases or Operator versions that you no longer need, you can modify your image set configuration to exclude them, and they will be pruned from the mirror registry upon mirroring. This can be done by adjusting a minimum or maximum version range setting per Operator in the image set configuration file or by deleting the Operator from the list of Operators to mirror from the catalog. You can also remove entire Operator catalogs or entire OpenShift Container Platform releases from the configuration file.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								If there are no new or updated images to mirror, the excluded images are not pruned from the target mirror registry. Additionally, if an Operator publisher removes an Operator version from a channel, the removed versions are pruned from the target mirror registry.
							</p></div></div><p>
							To disable automatic pruning of images from the target mirror registry, pass the <code class="literal">--skip-pruning</code> flag to the <code class="literal">oc mirror</code> command.
						</p></section><section class="section" id="oc-mirror-differential-updates_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h5 class="title">13.2.3.8.2. Updating your mirror registry content</h5></div></div></div><p>
							After you publish the initial image set to the mirror registry, you can use the oc-mirror plugin to keep your disconnected clusters updated.
						</p><p>
							Depending on your image set configuration, oc-mirror automatically detects newer releases of OpenShift Container Platform and your selected Operators that have been released after you completed the inital mirror. It is recommended to run oc-mirror at regular intervals, for example in a nightly cron job, to receive product and security updates on a timely basis.
						</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
									You have used the oc-mirror plugin to mirror the initial image set to your mirror registry.
								</li><li class="listitem"><p class="simpara">
									You have access to the storage backend that was used for the initial execution of the oc-mirror plugin.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										You must use the same storage backend as the initial execution of oc-mirror for the same mirror registry. Do not delete or modify the metadata image that is generated by the oc-mirror plugin.
									</p></div></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
									If necessary, update your image set configuration file to pick up new OpenShift Container Platform and Operator versions. See <span class="emphasis"><em>Image set configuration examples</em></span> for example mirroring use cases.
								</li><li class="listitem"><p class="simpara">
									Follow the same steps that you used to mirror your initial image set to the mirror registry. For instructions, see <span class="emphasis"><em>Mirroring an image set in a partially disconnected environment</em></span> or <span class="emphasis"><em>Mirroring an image set in a fully disconnected environment</em></span>.
								</p><div class="admonition important"><div class="admonition_header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												You must provide the same storage backend so that only a differential image set is created and mirrored.
											</li><li class="listitem">
												If you specified a top-level namespace for the mirror registry during the initial image set creation, then you must use this same namespace every time you run the oc-mirror plugin for the same mirror registry.
											</li></ul></div></div></div></li><li class="listitem">
									Configure your cluster to use the resources generated by oc-mirror.
								</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#oc-mirror-image-set-examples_mirroring-ocp-image-repository">Image set configuration examples</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#mirroring-image-set-partial">Mirroring an image set in a partially disconnected environment</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#mirroring-image-set-full">Mirroring an image set in a fully disconnected environment</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#oc-mirror-updating-cluster-manifests_mirroring-ocp-image-repository">Configuring your cluster to use the resources generated by oc-mirror</a>
								</li></ul></div></section></section><section class="section" id="oc-mirror-dry-run_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h4 class="title">13.2.3.9. Performing a dry run</h4></div></div></div><p>
						You can use oc-mirror to perform a dry run, without actually mirroring any images. This allows you to review the list of images that would be mirrored, as well as any images that would be pruned from the mirror registry. It also allows you to catch any errors with your image set configuration early or use the generated list of images with other tools to carry out the mirroring operation.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the internet to obtain the necessary container images.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You have installed the <code class="literal">oc-mirror</code> CLI plugin.
							</li><li class="listitem">
								You have created the image set configuration file.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Run the <code class="literal">oc mirror</code> command with the <code class="literal">--dry-run</code> flag to perform a dry run:
							</p><pre class="programlisting language-terminal">$ oc mirror --config=./imageset-config.yaml \<span id="CO17-1"><!--Empty--></span><span class="callout">1</span>
  docker://registry.example:5000            \<span id="CO17-2"><!--Empty--></span><span class="callout">2</span>
  --dry-run                                  <span id="CO17-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO17-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Pass in the image set configuration file that was created. This procedure assumes that it is named <code class="literal">imageset-config.yaml</code>.
									</div></dd><dt><a href="#CO17-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specify the mirror registry. Nothing is mirrored to this registry as long as you use the <code class="literal">--dry-run</code> flag.
									</div></dd><dt><a href="#CO17-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Use the <code class="literal">--dry-run</code> flag to generate the dry run artifacts and not an actual image set file.
									</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Checking push permissions for registry.example:5000
Creating directory: oc-mirror-workspace/src/publish
Creating directory: oc-mirror-workspace/src/v2
Creating directory: oc-mirror-workspace/src/charts
Creating directory: oc-mirror-workspace/src/release-signatures
No metadata detected, creating new workspace
wrote mirroring manifests to oc-mirror-workspace/operators.1658342351/manifests-redhat-operator-index

...

info: Planning completed in 31.48s
info: Dry run complete
Writing image mapping to oc-mirror-workspace/mapping.txt</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Navigate into the workspace directory that was generated:
							</p><pre class="programlisting language-terminal">$ cd oc-mirror-workspace/</pre></li><li class="listitem"><p class="simpara">
								Review the <code class="literal">mapping.txt</code> file that was generated.
							</p><p class="simpara">
								This file contains a list of all images that would be mirrored.
							</p></li><li class="listitem"><p class="simpara">
								Review the <code class="literal">pruning-plan.json</code> file that was generated.
							</p><p class="simpara">
								This file contains a list of all images that would be pruned from the mirror registry when the image set is published.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									The <code class="literal">pruning-plan.json</code> file is only generated if your oc-mirror command points to your mirror registry and there are images to be pruned.
								</p></div></div></li></ol></div></section><section class="section" id="oc-mirror-oci-format_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h4 class="title">13.2.3.10. Including local OCI Operator catalogs</h4></div></div></div><p>
						While mirroring OpenShift Container Platform releases, Operator catalogs, and additional images from a registry to a partially disconnected cluster, you can include Operator catalog images from a local file-based catalog on disk. The local catalog must be in the Open Container Initiative (OCI) format.
					</p><p>
						The local catalog and its contents are mirrored to your target mirror registry based on the filtering information in the image set configuration file.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							When mirroring local OCI catalogs, any OpenShift Container Platform releases or additional images that you want to mirror along with the local OCI-formatted catalog must be pulled from a registry.
						</p><p>
							You cannot mirror OCI catalogs along with an oc-mirror image set file on disk.
						</p></div></div><p>
						One example use case for using the OCI feature is if you have a CI/CD system building an OCI catalog to a location on disk, and you want to mirror that OCI catalog along with an OpenShift Container Platform release to your mirror registry.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you used the Technology Preview OCI local catalogs feature for the oc-mirror plugin for OpenShift Container Platform 4.12, you can no longer use the OCI local catalogs feature of the oc-mirror plugin to copy a catalog locally and convert it to OCI format as a first step to mirroring to a fully disconnected cluster.
						</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the internet to obtain the necessary container images.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You have installed the <code class="literal">oc-mirror</code> CLI plugin.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create the image set configuration file and adjust the settings as necessary.
							</p><p class="simpara">
								The following example image set configuration mirrors an OCI catalog on disk along with an OpenShift Container Platform release and a UBI image from <code class="literal">registry.redhat.io</code>.
							</p><pre class="programlisting language-yaml">kind: ImageSetConfiguration
apiVersion: mirror.openshift.io/v1alpha2
storageConfig:
  local:
    path: /home/user/metadata                                                <span id="CO18-1"><!--Empty--></span><span class="callout">1</span>
mirror:
  platform:
    channels:
    - name: stable-4.13                                                      <span id="CO18-2"><!--Empty--></span><span class="callout">2</span>
      type: ocp
    graph: false
  operators:
  - catalog: oci:///home/user/oc-mirror/my-oci-catalog                       <span id="CO18-3"><!--Empty--></span><span class="callout">3</span>
    targetCatalog: my-namespace/redhat-operator-index                        <span id="CO18-4"><!--Empty--></span><span class="callout">4</span>
    packages:
    - name: aws-load-balancer-operator
  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.13           <span id="CO18-5"><!--Empty--></span><span class="callout">5</span>
    packages:
    - name: rhacs-operator
  additionalImages:
  - name: registry.redhat.io/ubi9/ubi:latest                                 <span id="CO18-6"><!--Empty--></span><span class="callout">6</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO18-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Set the back-end location to save the image set metadata to. This location can be a registry or local directory. It is required to specify <code class="literal">storageConfig</code> values.
									</div></dd><dt><a href="#CO18-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Optionally, include an OpenShift Container Platform release to mirror from <code class="literal">registry.redhat.io</code>.
									</div></dd><dt><a href="#CO18-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Specify the absolute path to the location of the OCI catalog on disk. The path must start with <code class="literal">oci://</code> when using the OCI feature.
									</div></dd><dt><a href="#CO18-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										Optionally, specify an alternative namespace and name to mirror the catalog as.
									</div></dd><dt><a href="#CO18-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										Optionally, specify additional Operator catalogs to pull from a registry.
									</div></dd><dt><a href="#CO18-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										Optionally, specify additional images to pull from a registry.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Run the <code class="literal">oc mirror</code> command to mirror the OCI catalog to a target mirror registry:
							</p><pre class="programlisting language-terminal">$ oc mirror --config=./imageset-config.yaml \ <span id="CO19-1"><!--Empty--></span><span class="callout">1</span>
  --include-local-oci-catalogs                <span id="CO19-2"><!--Empty--></span><span class="callout">2</span>
  docker://registry.example:5000              <span id="CO19-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO19-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Pass in the image set configuration file. This procedure assumes that it is named <code class="literal">imageset-config.yaml</code>.
									</div></dd><dt><a href="#CO19-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Use the <code class="literal">--include-local-oci-catalogs</code> flag to enable mirroring local OCI catalogs along with other remote content.
									</div></dd><dt><a href="#CO19-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Specify the registry to mirror the content to. The registry must start with <code class="literal">docker://</code>. If you specify a top-level namespace for the mirror registry, you must also use this same namespace on subsequent executions.
									</div></dd></dl></div><p class="simpara">
								Optionally, you can specify other flags to adjust the behavior of the OCI feature:
							</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">--oci-insecure-signature-policy</code></span></dt><dd>
											Do not push signatures to the target mirror registry.
										</dd><dt><span class="term"><code class="literal">--oci-registries-config</code></span></dt><dd><p class="simpara">
											Specify the path to a TOML-formatted <code class="literal">registries.conf</code> file. You can use this to mirror from a different registry, such as a pre-production location for testing, without having to change the image set configuration file. This flag only affects local OCI catalogs, not any other mirrored content.
										</p><div class="formalpara"><p class="title"><strong>Example registries.conf file</strong></p><p>
												
<pre class="programlisting language-toml">[[registry]]
 location = "registry.redhat.io:5000"
 insecure = false
 blocked = false
 mirror-by-digest-only = true
 prefix = ""
 [[registry.mirror]]
    location = "preprod-registry.example.com"
    insecure = false</pre>

											</p></div></dd></dl></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Configure your cluster to use the resources generated by oc-mirror.
							</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-managing-custom-catalogs-fb">File-based catalogs</a>
							</li></ul></div></section><section class="section" id="oc-mirror-imageset-config-params_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h4 class="title">13.2.3.11. Image set configuration parameters</h4></div></div></div><p>
						The oc-mirror plugin requires an image set configuration file that defines what images to mirror. The following table lists the available parameters for the <code class="literal">ImageSetConfiguration</code> resource.
					</p><div class="table" id="idm140049050075232"><p class="title"><strong>Table 13.1. <code class="literal">ImageSetConfiguration</code> parameters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 40%; " class="col_1"><!--Empty--></col><col style="width: 40%; " class="col_2"><!--Empty--></col><col style="width: 20%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140049046758752" scope="col">Parameter</th><th align="left" valign="top" id="idm140049046757664" scope="col">Description</th><th align="left" valign="top" id="idm140049046756576" scope="col">Values</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">apiVersion</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The API version for the <code class="literal">ImageSetConfiguration</code> content.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">mirror.openshift.io/v1alpha2</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">archiveSize</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The maximum size, in GiB, of each archive file within the image set.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Integer. For example: <code class="literal">4</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The configuration of the image set.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Object
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.additionalImages</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The additional images configuration of the image set.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Array of objects. For example:
									</p>
									 
<pre class="programlisting language-yaml">additionalImages:
  - name: registry.redhat.io/ubi8/ubi:latest</pre>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.additionalImages.name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The tag or digest of the image to mirror.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">registry.redhat.io/ubi8/ubi:latest</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.blockedImages</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The full tag, digest, or pattern of images to block from mirroring.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Array of strings. For example: <code class="literal">docker.io/library/alpine</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.helm</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The helm configuration of the image set. Note that the oc-mirror plugin supports only helm charts that do not require user input when rendered.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Object
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.helm.local</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The local helm charts to mirror.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Array of objects. For example:
									</p>
									 
<pre class="programlisting language-yaml">local:
  - name: podinfo
    path: /test/podinfo-5.0.0.tar.gz</pre>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.helm.local.name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The name of the local helm chart to mirror.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">podinfo</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.helm.local.path</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The path of the local helm chart to mirror.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">/test/podinfo-5.0.0.tar.gz</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.helm.repositories</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The remote helm repositories to mirror from.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Array of objects. For example:
									</p>
									 
<pre class="programlisting language-yaml">repositories:
  - name: podinfo
    url: https://example.github.io/podinfo
    charts:
      - name: podinfo
        version: 5.0.0</pre>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.helm.repositories.name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The name of the helm repository to mirror from.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">podinfo</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.helm.repositories.url</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The URL of the helm repository to mirror from.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">https://example.github.io/podinfo</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.helm.repositories.charts</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The remote helm charts to mirror.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Array of objects.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.helm.repositories.charts.name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The name of the helm chart to mirror.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">podinfo</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.helm.repositories.charts.version</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The version of the named helm chart to mirror.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">5.0.0</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.operators</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The Operators configuration of the image set.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Array of objects. For example:
									</p>
									 
<pre class="programlisting language-yaml">operators:
  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.13
    packages:
      - name: elasticsearch-operator
        minVersion: '2.4.0'</pre>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.operators.catalog</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The Operator catalog to include in the image set.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">registry.redhat.io/redhat/redhat-operator-index:v4.13</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.operators.full</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										When <code class="literal">true</code>, downloads the full catalog, Operator package, or Operator channel.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Boolean. The default value is <code class="literal">false</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.operators.packages</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The Operator packages configuration.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Array of objects. For example:
									</p>
									 
<pre class="programlisting language-yaml">operators:
  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.13
    packages:
      - name: elasticsearch-operator
        minVersion: '5.2.3-31'</pre>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.operators.packages.name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The Operator package name to include in the image set
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">elasticsearch-operator</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.operators.packages.channels</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The Operator package channel configuration.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Object
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.operators.packages.channels.name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The Operator channel name, unique within a package, to include in the image set.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">fast</code> or <code class="literal">stable-v4.13</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.operators.packages.channels.maxVersion</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The highest version of the Operator mirror across all channels in which it exists. See the following note for further information.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">5.2.3-31</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.operators.packages.channels.minBundle</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The name of the minimum bundle to include, plus all bundles in the upgrade graph to the channel head. Set this field only if the named bundle has no semantic version metadata.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">bundleName</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.operators.packages.channels.minVersion</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The lowest version of the Operator to mirror across all channels in which it exists. See the following note for further information.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">5.2.3-31</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.operators.packages.maxVersion</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The highest version of the Operator to mirror across all channels in which it exists. See the following note for further information.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">5.2.3-31</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.operators.packages.minVersion</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The lowest version of the Operator to mirror across all channels in which it exists. See the following note for further information.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">5.2.3-31</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.operators.skipDependencies</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										If <code class="literal">true</code>, dependencies of bundles are not included.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Boolean. The default value is <code class="literal">false</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.operators.targetCatalog</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										An alternative name and optional namespace hierarchy to mirror the referenced catalog as.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">my-namespace/my-operator-catalog</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.operators.targetName</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										An alternative name to mirror the referenced catalog as.
									</p>
									 <p>
										The <code class="literal">targetName</code> parameter is deprecated. Use the <code class="literal">targetCatalog</code> parameter instead.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">my-operator-catalog</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.operators.targetTag</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										An alternative tag to append to the <code class="literal">targetName</code> or <code class="literal">targetCatalog</code>.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">v1</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.platform</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The platform configuration of the image set.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Object
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.platform.architectures</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The architecture of the platform release payload to mirror.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Array of strings. For example:
									</p>
									 
<pre class="programlisting language-yaml">architectures:
  - amd64
  - arm64</pre>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.platform.channels</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The platform channel configuration of the image set.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Array of objects. For example:
									</p>
									 
<pre class="programlisting language-yaml">channels:
  - name: stable-4.10
  - name: stable-4.13</pre>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.platform.channels.full</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										When <code class="literal">true</code>, sets the <code class="literal">minVersion</code> to the first release in the channel and the <code class="literal">maxVersion</code> to the last release in the channel.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Boolean. The default value is <code class="literal">false</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.platform.channels.name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The name of the release channel.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">stable-4.13</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.platform.channels.minVersion</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The minimum version of the referenced platform to be mirrored.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">4.12.6</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.platform.channels.maxVersion</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The highest version of the referenced platform to be mirrored.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">4.13.1</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.platform.channels.shortestPath</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										Toggles shortest path mirroring or full range mirroring.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Boolean. The default value is <code class="literal">false</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.platform.channels.type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The type of the platform to be mirrored.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">ocp</code> or <code class="literal">okd</code>. The default is <code class="literal">ocp</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">mirror.platform.graph</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										Indicates whether the OSUS graph is added to the image set and subsequently published to the mirror.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Boolean. The default value is <code class="literal">false</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">storageConfig</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The back-end configuration of the image set.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Object
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">storageConfig.local</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The local back-end configuration of the image set.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Object
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">storageConfig.local.path</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The path of the directory to contain the image set metadata.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">./path/to/dir/</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">storageConfig.registry</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The registry back-end configuration of the image set.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Object
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">storageConfig.registry.imageURL</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										The back-end registry URI. Can optionally include a namespace reference in the URI.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										String. For example: <code class="literal">quay.io/myuser/imageset:metadata</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049046758752"> <p>
										<code class="literal">storageConfig.registry.skipTLS</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049046757664"> <p>
										Optionally skip TLS verification of the referenced back-end registry.
									</p>
									 </td><td align="left" valign="top" headers="idm140049046756576"> <p>
										Boolean. The default value is <code class="literal">false</code>.
									</p>
									 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Using the the <code class="literal">minVersion</code> and <code class="literal">maxVersion</code> properties to filter for a specific Operator version range can result in a multiple channel heads error. The error message will state that there are <code class="literal">multiple channel heads</code>. This is because when the filter is applied, the update graph of the operator is truncated.
						</p><p>
							The Operator Lifecycle Manager requires that every operator channel contains versions that form an update graph with exactly one end point, that is, the latest version of the operator. When applying the filter range that graph can turn into two or more separate graphs or a graph that has more than one end point.
						</p><p>
							To avoid this error, do not filter out the latest version of an operator. If you still run into the error, depending on the operator, either the <code class="literal">maxVersion</code> property needs to be increased or the <code class="literal">minVersion</code> property needs to be decreased. Because every operator graph can be different, you might need to adjust these values, according to the procedure, until the error is gone.
						</p></div></div></section><section class="section" id="oc-mirror-image-set-examples_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h4 class="title">13.2.3.12. Image set configuration examples</h4></div></div></div><p>
						The following <code class="literal">ImageSetConfiguration</code> file examples show the configuration for various mirroring use cases.
					</p><h6 id="oc-mirror-image-set-examples-shortest-upgrade-path_mirroring-ocp-image-repository">Use case: Including the shortest OpenShift Container Platform upgrade path</h6><p>
						The following <code class="literal">ImageSetConfiguration</code> file uses a local storage backend and includes all OpenShift Container Platform versions along the shortest upgrade path from the minimum version of <code class="literal">4.11.37</code> to the maximum version of <code class="literal">4.12.15</code>.
					</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">ImageSetConfiguration</code> file</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: mirror.openshift.io/v1alpha2
kind: ImageSetConfiguration
storageConfig:
  local:
    path: /home/user/metadata
mirror:
  platform:
    channels:
      - name: stable-4.12
        minVersion: 4.11.37
        maxVersion: 4.12.15
        shortestPath: true</pre>

						</p></div><h6 id="oc-mirror-image-set-examples-minimum-to-latest_mirroring-ocp-image-repository">Use case: Including all versions of OpenShift Container Platform from a minimum to the latest</h6><p>
						The following <code class="literal">ImageSetConfiguration</code> file uses a registry storage backend and includes all OpenShift Container Platform versions starting at a minimum version of <code class="literal">4.10.10</code> to the latest version in the channel.
					</p><p>
						On every invocation of oc-mirror with this image set configuration, the latest release of the <code class="literal">stable-4.10</code> channel is evaluated, so running oc-mirror at regular intervals ensures that you automatically receive the latest releases of OpenShift Container Platform images.
					</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">ImageSetConfiguration</code> file</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: mirror.openshift.io/v1alpha2
kind: ImageSetConfiguration
storageConfig:
  registry:
    imageURL: example.com/mirror/oc-mirror-metadata
    skipTLS: false
mirror:
  platform:
    channels:
      - name: stable-4.10
        minVersion: 4.10.10</pre>

						</p></div><h6 id="oc-mirror-image-set-examples-operator-versions_mirroring-ocp-image-repository">Use case: Including Operator versions from a minimum to the latest</h6><p>
						The following <code class="literal">ImageSetConfiguration</code> file uses a local storage backend and includes only the Red Hat Advanced Cluster Security for Kubernetes Operator, versions starting at 4.0.1 and later in the <code class="literal">stable</code> channel.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							When you specify a minimum or maximum version range, you might not receive all Operator versions in that range.
						</p><p>
							By default, oc-mirror excludes any versions that are skipped or replaced by a newer version in the Operator Lifecycle Manager (OLM) specification. Operator versions that are skipped might be affected by a CVE or contain bugs. Use a newer version instead. For more information on skipped and replaced versions, see <a class="link" href="https://olm.operatorframework.io/docs/concepts/olm-architecture/operator-catalog/creating-an-update-graph/">Creating an update graph with OLM</a>.
						</p><p>
							To receive all Operator versions in a specified range, you can set the <code class="literal">mirror.operators.full</code> field to <code class="literal">true</code>.
						</p></div></div><div class="formalpara"><p class="title"><strong>Example <code class="literal">ImageSetConfiguration</code> file</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: mirror.openshift.io/v1alpha2
kind: ImageSetConfiguration
storageConfig:
  local:
    path: /home/user/metadata
mirror:
  operators:
    - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.13
      packages:
        - name: rhacs-operator
          channels:
          - name: stable
            minVersion: 4.0.1</pre>

						</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							To specify a maximum version instead of the latest, set the <code class="literal">mirror.operators.packages.channels.maxVersion</code> field.
						</p></div></div><h6 id="oc-mirror-image-set-examples-nutanix-operator_mirroring-ocp-image-repository">Use case: Including the Nutanix CSI Operator</h6><p>
						The following <code class="literal">ImageSetConfiguration</code> file uses a local storage backend and includes the Nutanix CSI Operator, the OpenShift Update Service (OSUS) graph image, and an additional Red Hat Universal Base Image (UBI).
					</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">ImageSetConfiguration</code> file</strong></p><p>
							
<pre class="programlisting language-yaml">kind: ImageSetConfiguration
apiVersion: mirror.openshift.io/v1alpha2
storageConfig:
  registry:
    imageURL: mylocalregistry/ocp-mirror/openshift4
    skipTLS: false
mirror:
  platform:
    channels:
    - name: stable-4.11
      type: ocp
    graph: true
  operators:
  - catalog: registry.redhat.io/redhat/certified-operator-index:v4.11
    packages:
    - name: nutanixcsioperator
      channels:
      - name: stable
  additionalImages:
  - name: registry.redhat.io/ubi9/ubi:latest</pre>

						</p></div><h6 id="oc-mirror-image-set-examples-default-channel_mirroring-ocp-image-repository">Use case: Including the default Operator channel</h6><p>
						The following <code class="literal">ImageSetConfiguration</code> file includes the <code class="literal">stable-5.7</code> and <code class="literal">stable</code> channels for the OpenShift Elasticsearch Operator. Even if only the packages from the <code class="literal">stable-5.7</code> channel are needed, the <code class="literal">stable</code> channel must also be included in the <code class="literal">ImageSetConfiguration</code> file, because it is the default channel for the Operator. You must always include the default channel for the Operator package even if you do not use the bundles in that channel.
					</p><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
						You can find the default channel by running the following command: <code class="literal">oc mirror list operators --catalog=&lt;catalog_name&gt; --package=&lt;package_name&gt;</code>.
					</p></div></div><div class="formalpara"><p class="title"><strong>Example <code class="literal">ImageSetConfiguration</code> file</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: mirror.openshift.io/v1alpha2
kind: ImageSetConfiguration
storageConfig:
  registry:
    imageURL: example.com/mirror/oc-mirror-metadata
    skipTLS: false
mirror:
  operators:
  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.13
    packages:
    - name: elasticsearch-operator
      channels:
      - name: stable-5.7
      - name: stable</pre>

						</p></div><h6 id="oc-mirror-image-set-examples-entire-catalog-full_mirroring-ocp-image-repository">Use case: Including an entire catalog (all versions)</h6><p>
						The following <code class="literal">ImageSetConfiguration</code> file sets the <code class="literal">mirror.operators.full</code> field to <code class="literal">true</code> to include all versions for an entire Operator catalog.
					</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">ImageSetConfiguration</code> file</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: mirror.openshift.io/v1alpha2
kind: ImageSetConfiguration
storageConfig:
  registry:
    imageURL: example.com/mirror/oc-mirror-metadata
    skipTLS: false
mirror:
  operators:
    - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.13
      full: true</pre>

						</p></div><h6 id="oc-mirror-image-set-examples-entire-catalog-heads_mirroring-ocp-image-repository">Use case: Including an entire catalog (channel heads only)</h6><p>
						The following <code class="literal">ImageSetConfiguration</code> file includes the channel heads for an entire Operator catalog.
					</p><p>
						By default, for each Operator in the catalog, oc-mirror includes the latest Operator version (channel head) from the default channel. If you want to mirror all Operator versions, and not just the channel heads, you must set the <code class="literal">mirror.operators.full</code> field to <code class="literal">true</code>.
					</p><p>
						This example also uses the <code class="literal">targetCatalog</code> field to specify an alternative namespace and name to mirror the catalog as.
					</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">ImageSetConfiguration</code> file</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: mirror.openshift.io/v1alpha2
kind: ImageSetConfiguration
storageConfig:
  registry:
    imageURL: example.com/mirror/oc-mirror-metadata
    skipTLS: false
mirror:
  operators:
  - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.13
    targetCatalog: my-namespace/my-operator-catalog</pre>

						</p></div><h6 id="oc-mirror-image-set-examples-helm_mirroring-ocp-image-repository">Use case: Including arbitrary images and helm charts</h6><p>
						The following <code class="literal">ImageSetConfiguration</code> file uses a registry storage backend and includes helm charts and an additional Red Hat Universal Base Image (UBI).
					</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">ImageSetConfiguration</code> file</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: mirror.openshift.io/v1alpha2
kind: ImageSetConfiguration
archiveSize: 4
storageConfig:
  registry:
    imageURL: example.com/mirror/oc-mirror-metadata
    skipTLS: false
mirror:
 platform:
   architectures:
     - "s390x"
   channels:
     - name: stable-4.13
 operators:
   - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.13
 helm:
   repositories:
     - name: redhat-helm-charts
       url: https://raw.githubusercontent.com/redhat-developer/redhat-helm-charts/master
       charts:
         - name: ibm-mongodb-enterprise-helm
           version: 0.2.0
 additionalImages:
   - name: registry.redhat.io/ubi9/ubi:latest</pre>

						</p></div></section><section class="section" id="oc-mirror-command-reference_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h4 class="title">13.2.3.13. Command reference for oc-mirror</h4></div></div></div><p>
						The following tables describe the <code class="literal">oc mirror</code> subcommands and flags:
					</p><div class="table" id="idm140049047813776"><p class="title"><strong>Table 13.2. oc mirror subcommands</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 67%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140049047808944" scope="col">Subcommand</th><th align="left" valign="top" id="idm140049047807856" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140049047808944"> <p>
										<code class="literal">completion</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049047807856"> <p>
										Generate the autocompletion script for the specified shell.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049047808944"> <p>
										<code class="literal">describe</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049047807856"> <p>
										Output the contents of an image set.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049047808944"> <p>
										<code class="literal">help</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049047807856"> <p>
										Show help about any subcommand.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049047808944"> <p>
										<code class="literal">init</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049047807856"> <p>
										Output an initial image set configuration template.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049047808944"> <p>
										<code class="literal">list</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049047807856"> <p>
										List available platform and Operator content and their version.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049047808944"> <p>
										<code class="literal">version</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049047807856"> <p>
										Output the oc-mirror version.
									</p>
									 </td></tr></tbody></table></div></div><div class="table" id="idm140049051026832"><p class="title"><strong>Table 13.3. oc mirror flags</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 67%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140049051022000" scope="col">Flag</th><th align="left" valign="top" id="idm140049051020912" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">-c</code>, <code class="literal">--config</code> <code class="literal">&lt;string&gt;</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Specify the path to an image set configuration file.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--continue-on-error</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										If any non image-pull related error occurs, continue and attempt to mirror as much as possible.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--dest-skip-tls</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Disable TLS validation for the target registry.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--dest-use-http</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Use plain HTTP for the target registry.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--dry-run</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Print actions without mirroring images. Generates <code class="literal">mapping.txt</code> and <code class="literal">pruning-plan.json</code> files.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--from &lt;string&gt;</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Specify the path to an image set archive that was generated by an execution of oc-mirror to load into a target registry.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">-h</code>, <code class="literal">--help</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Show the help.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--ignore-history</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Ignore past mirrors when downloading images and packing layers. Disables incremental mirroring and might download more data.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--include-local-oci-catalogs</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Enable mirroring for local OCI catalogs on disk to the target mirror registry.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--manifests-only</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Generate manifests for <code class="literal">ImageContentSourcePolicy</code> objects to configure a cluster to use the mirror registry, but do not actually mirror any images. To use this flag, you must pass in an image set archive with the <code class="literal">--from</code> flag.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--max-nested-paths &lt;int&gt;</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Specify the maximum number of nested paths for destination registries that limit nested paths. The default is <code class="literal">0</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--max-per-registry &lt;int&gt;</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Specify the number of concurrent requests allowed per registry. The default is <code class="literal">6</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--oci-insecure-signature-policy</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Do not push signatures when mirroring local OCI catalogs (with <code class="literal">--include-local-oci-catalogs</code>).
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--oci-registries-config</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Provide a registries configuration file to specify an alternative registry location to copy from when mirroring local OCI catalogs (with <code class="literal">--include-local-oci-catalogs</code>).
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--skip-cleanup</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Skip removal of artifact directories.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--skip-image-pin</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Do not replace image tags with digest pins in Operator catalogs.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--skip-metadata-check</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Skip metadata when publishing an image set. This is only recommended when the image set was created with <code class="literal">--ignore-history</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--skip-missing</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										If an image is not found, skip it instead of reporting an error and aborting execution. Does not apply to custom images explicitly specified in the image set configuration.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--skip-pruning</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Disable automatic pruning of images from the target mirror registry.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--skip-verification</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Skip digest verification.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--source-skip-tls</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Disable TLS validation for the source registry.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--source-use-http</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Use plain HTTP for the source registry.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">--use-oci-feature</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Enable mirroring for local OCI catalogs on disk to the target mirror registry.
									</p>
									 <p>
										The <code class="literal">--use-oci-feature</code> flag is deprecated. Use the <code class="literal">--include-local-oci-catalogs</code> flag instead.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140049051022000"> <p>
										<code class="literal">-v</code>, <code class="literal">--verbose</code> <code class="literal">&lt;int&gt;</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140049051020912"> <p>
										Specify the number for the log level verbosity. Valid values are <code class="literal">0</code> - <code class="literal">9</code>. The default is <code class="literal">0</code>.
									</p>
									 </td></tr></tbody></table></div></div></section></section><section class="section" id="update-mirror-repository-adm-release-mirror_mirroring-ocp-image-repository"><div class="titlepage"><div><div><h3 class="title">13.2.4. Mirroring images using the oc adm release mirror command</h3></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						To avoid excessive memory usage by the OpenShift Update Service application, you must mirror release images to a separate repository as described in the following procedure.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You configured a mirror registry to use in your disconnected environment and can access the certificate and credentials that you configured.
						</li><li class="listitem">
							You downloaded the <a class="link" href="https://console.redhat.com/openshift/install/pull-secret">pull secret from the Red Hat OpenShift Cluster Manager</a> and modified it to include authentication to your mirror repository.
						</li><li class="listitem">
							If you use self-signed certificates, you have specified a Subject Alternative Name in the certificates.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Use the <a class="link" href="https://access.redhat.com/labs/ocpupgradegraph/update_channel">Red Hat OpenShift Container Platform Upgrade Graph visualizer and update planner</a> to plan an update from one version to another. The OpenShift Upgrade Graph provides channel graphs and a way to confirm that there is an update path between your current and intended cluster versions.
						</li><li class="listitem"><p class="simpara">
							Set the required environment variables:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Export the release version:
								</p><pre class="programlisting language-terminal">$ export OCP_RELEASE=&lt;release_version&gt;</pre><p class="simpara">
									For <code class="literal">&lt;release_version&gt;</code>, specify the tag that corresponds to the version of OpenShift Container Platform to which you want to update, such as <code class="literal">4.5.4</code>.
								</p></li><li class="listitem"><p class="simpara">
									Export the local registry name and host port:
								</p><pre class="programlisting language-terminal">$ LOCAL_REGISTRY='&lt;local_registry_host_name&gt;:&lt;local_registry_host_port&gt;'</pre><p class="simpara">
									For <code class="literal">&lt;local_registry_host_name&gt;</code>, specify the registry domain name for your mirror repository, and for <code class="literal">&lt;local_registry_host_port&gt;</code>, specify the port that it serves content on.
								</p></li><li class="listitem"><p class="simpara">
									Export the local repository name:
								</p><pre class="programlisting language-terminal">$ LOCAL_REPOSITORY='&lt;local_repository_name&gt;'</pre><p class="simpara">
									For <code class="literal">&lt;local_repository_name&gt;</code>, specify the name of the repository to create in your registry, such as <code class="literal">ocp4/openshift4</code>.
								</p></li><li class="listitem"><p class="simpara">
									If you are using the OpenShift Update Service, export an additional local repository name to contain the release images:
								</p><pre class="programlisting language-terminal">$ LOCAL_RELEASE_IMAGES_REPOSITORY='&lt;local_release_images_repository_name&gt;'</pre><p class="simpara">
									For <code class="literal">&lt;local_release_images_repository_name&gt;</code>, specify the name of the repository to create in your registry, such as <code class="literal">ocp4/openshift4-release-images</code>.
								</p></li><li class="listitem"><p class="simpara">
									Export the name of the repository to mirror:
								</p><pre class="programlisting language-terminal">$ PRODUCT_REPO='openshift-release-dev'</pre><p class="simpara">
									For a production release, you must specify <code class="literal">openshift-release-dev</code>.
								</p></li><li class="listitem"><p class="simpara">
									Export the path to your registry pull secret:
								</p><pre class="programlisting language-terminal">$ LOCAL_SECRET_JSON='&lt;path_to_pull_secret&gt;'</pre><p class="simpara">
									For <code class="literal">&lt;path_to_pull_secret&gt;</code>, specify the absolute path to and file name of the pull secret for your mirror registry that you created.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										If your cluster uses an <code class="literal">ImageContentSourcePolicy</code> object to configure repository mirroring, you can use only global pull secrets for mirrored registries. You cannot add a pull secret to a project.
									</p></div></div></li><li class="listitem"><p class="simpara">
									Export the release mirror:
								</p><pre class="programlisting language-terminal">$ RELEASE_NAME="ocp-release"</pre><p class="simpara">
									For a production release, you must specify <code class="literal">ocp-release</code>.
								</p></li><li class="listitem"><p class="simpara">
									Export the type of architecture for your cluster:
								</p><pre class="programlisting language-terminal">$ ARCHITECTURE=&lt;cluster_architecture&gt; <span id="CO20-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO20-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the architecture of the cluster, such as <code class="literal">x86_64</code>, <code class="literal">aarch64</code>, <code class="literal">s390x</code>, or <code class="literal">ppc64le</code>.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Export the path to the directory to host the mirrored images:
								</p><pre class="programlisting language-terminal">$ REMOVABLE_MEDIA_PATH=&lt;path&gt; <span id="CO21-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO21-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the full path, including the initial forward slash (/) character.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Review the images and configuration manifests to mirror:
						</p><pre class="programlisting language-terminal">$ oc adm release mirror -a ${LOCAL_SECRET_JSON} --to-dir=${REMOVABLE_MEDIA_PATH}/mirror quay.io/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE}-${ARCHITECTURE} --dry-run</pre></li><li class="listitem"><p class="simpara">
							Mirror the version images to the mirror registry.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									If your mirror host does not have internet access, take the following actions:
								</p><div class="orderedlist"><ol class="orderedlist" type="i"><li class="listitem">
											Connect the removable media to a system that is connected to the internet.
										</li><li class="listitem"><p class="simpara">
											Mirror the images and configuration manifests to a directory on the removable media:
										</p><pre class="programlisting language-terminal">$ oc adm release mirror -a ${LOCAL_SECRET_JSON} --to-dir=${REMOVABLE_MEDIA_PATH}/mirror quay.io/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE}-${ARCHITECTURE}</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												This command also generates and saves the mirrored release image signature config map onto the removable media.
											</p></div></div></li><li class="listitem"><p class="simpara">
											Take the media to the disconnected environment and upload the images to the local container registry.
										</p><pre class="programlisting language-terminal">$ oc image mirror  -a ${LOCAL_SECRET_JSON} --from-dir=${REMOVABLE_MEDIA_PATH}/mirror "file://openshift/release:${OCP_RELEASE}*" ${LOCAL_REGISTRY}/${LOCAL_REPOSITORY} <span id="CO22-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO22-1"><span class="callout">1</span></a> </dt><dd><div class="para">
													For <code class="literal">REMOVABLE_MEDIA_PATH</code>, you must use the same path that you specified when you mirrored the images.
												</div></dd></dl></div></li><li class="listitem">
											Use <code class="literal">oc</code> command-line interface (CLI) to log in to the cluster that you are upgrading.
										</li><li class="listitem"><p class="simpara">
											Apply the mirrored release image signature config map to the connected cluster:
										</p><pre class="programlisting language-terminal">$ oc apply -f ${REMOVABLE_MEDIA_PATH}/mirror/config/&lt;image_signature_file&gt; <span id="CO23-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO23-1"><span class="callout">1</span></a> </dt><dd><div class="para">
													For <code class="literal">&lt;image_signature_file&gt;</code>, specify the path and name of the file, for example, <code class="literal">signature-sha256-81154f5c03294534.yaml</code>.
												</div></dd></dl></div></li><li class="listitem"><p class="simpara">
											If you are using the OpenShift Update Service, mirror the release image to a separate repository:
										</p><pre class="programlisting language-terminal">$ oc image mirror -a ${LOCAL_SECRET_JSON} ${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}:${OCP_RELEASE}-${ARCHITECTURE} ${LOCAL_REGISTRY}/${LOCAL_RELEASE_IMAGES_REPOSITORY}:${OCP_RELEASE}-${ARCHITECTURE}</pre></li></ol></div></li><li class="listitem"><p class="simpara">
									If the local container registry and the cluster are connected to the mirror host, take the following actions:
								</p><div class="orderedlist"><ol class="orderedlist" type="i"><li class="listitem"><p class="simpara">
											Directly push the release images to the local registry and apply the config map to the cluster by using following command:
										</p><pre class="programlisting language-terminal">$ oc adm release mirror -a ${LOCAL_SECRET_JSON} --from=quay.io/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE}-${ARCHITECTURE} \
  --to=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY} --apply-release-image-signature</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												If you include the <code class="literal">--apply-release-image-signature</code> option, do not create the config map for image signature verification.
											</p></div></div></li><li class="listitem"><p class="simpara">
											If you are using the OpenShift Update Service, mirror the release image to a separate repository:
										</p><pre class="programlisting language-terminal">$ oc image mirror -a ${LOCAL_SECRET_JSON} ${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}:${OCP_RELEASE}-${ARCHITECTURE} ${LOCAL_REGISTRY}/${LOCAL_RELEASE_IMAGES_REPOSITORY}:${OCP_RELEASE}-${ARCHITECTURE}</pre></li></ol></div></li></ul></div></li></ol></div></section></section><section class="section" id="updating-restricted-network-cluster-OSUS"><div class="titlepage"><div><div><h2 class="title">13.3. Updating a cluster in a disconnected environment using the OpenShift Update Service</h2></div></div></div><p>
				To get an update experience similar to connected clusters, you can use the following procedures to install and configure the OpenShift Update Service (OSUS) in a disconnected environment.
			</p><p>
				The following steps outline the high-level workflow on how to update a cluster in a disconnected environment using OSUS:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Configure access to a secured registry.
					</li><li class="listitem">
						Update the global cluster pull secret to access your mirror registry.
					</li><li class="listitem">
						Install the OSUS Operator.
					</li><li class="listitem">
						Create a graph data container image for the OpenShift Update Service.
					</li><li class="listitem">
						Install the OSUS application and configure your clusters to use the local OpenShift Update Service.
					</li><li class="listitem">
						Perform a supported update procedure from the documentation as you would with a connected cluster.
					</li></ol></div><section class="section" id="update-service-overview_updating-restricted-network-cluster-osus"><div class="titlepage"><div><div><h3 class="title">13.3.1. Using the OpenShift Update Service in a disconnected environment</h3></div></div></div><p>
					The OpenShift Update Service (OSUS) provides update recommendations to OpenShift Container Platform clusters. Red Hat publicly hosts the OpenShift Update Service, and clusters in a connected environment can connect to the service through public APIs to retrieve update recommendations.
				</p><p>
					However, clusters in a disconnected environment cannot access these public APIs to retrieve update information. To have a similar update experience in a disconnected environment, you can install and configure the OpenShift Update Service locally so that it is available within the disconnected environment.
				</p><p>
					A single OSUS instance is capable of serving recommendations to thousands of clusters. OSUS can be scaled horizontally to cater to more clusters by changing the replica value. So for most disconnected use cases, one OSUS instance is enough. For example, Red Hat hosts just one OSUS instance for the entire fleet of connected clusters.
				</p><p>
					If you want to keep update recommendations separate in different environments, you can run one OSUS instance for each environment. For example, in a case where you have separate test and stage environments, you might not want a cluster in a stage environment to receive update recommendations to version A if that version has not been tested in the test environment yet.
				</p><p>
					The following sections describe how to install a local OSUS instance and configure it to provide update recommendations to a cluster.
				</p><div class="itemizedlist"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-service-about_understanding-openshift-updates">About the OpenShift Update Service</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#understanding-upgrade-channels-releases">Understanding update channels and releases</a>
						</li></ul></div></section><section class="section" id="update-service-prereqs"><div class="titlepage"><div><div><h3 class="title">13.3.2. Prerequisites</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							You must have the <code class="literal">oc</code> command-line interface (CLI) tool installed.
						</li><li class="listitem">
							You must provision a local container image registry with the container images for your update, as described in <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#mirroring-ocp-image-repository">Mirroring the OpenShift Container Platform image repository</a>.
						</li></ul></div></section><section class="section" id="registry-configuration-for-update-service"><div class="titlepage"><div><div><h3 class="title">13.3.3. Configuring access to a secured registry for the OpenShift Update Service</h3></div></div></div><p>
					If the release images are contained in a registry whose HTTPS X.509 certificate is signed by a custom certificate authority, complete the steps in <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/registry/#images-configuration-cas_configuring-registry-operator">Configuring additional trust stores for image registry access</a> along with following changes for the update service.
				</p><p>
					The OpenShift Update Service Operator needs the config map key name <code class="literal">updateservice-registry</code> in the registry CA cert.
				</p><div class="formalpara"><p class="title"><strong>Image registry CA config map example for the update service</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: my-registry-ca
data:
  updateservice-registry: | <span id="CO24-1"><!--Empty--></span><span class="callout">1</span>
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----
  registry-with-port.example.com..5000: | <span id="CO24-2"><!--Empty--></span><span class="callout">2</span>
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO24-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The OpenShift Update Service Operator requires the config map key name updateservice-registry in the registry CA cert.
						</div></dd><dt><a href="#CO24-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							If the registry has the port, such as <code class="literal">registry-with-port.example.com:5000</code>, <code class="literal">:</code> should be replaced with <code class="literal">..</code>.
						</div></dd></dl></div></section><section class="section" id="images-update-global-pull-secret_updating-restricted-network-cluster-osus"><div class="titlepage"><div><div><h3 class="title">13.3.4. Updating the global cluster pull secret</h3></div></div></div><p>
					You can update the global pull secret for your cluster by either replacing the current pull secret or appending a new pull secret.
				</p><p>
					The procedure is required when users use a separate registry to store images than the registry used during installation.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Optional: To append a new pull secret to the existing pull secret, complete the following steps:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Enter the following command to download the pull secret:
								</p><pre class="programlisting language-terminal">$ oc get secret/pull-secret -n openshift-config --template='{{index .data ".dockerconfigjson" | base64decode}}' &gt;&lt;pull_secret_location&gt; <span id="CO25-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO25-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Provide the path to the pull secret file.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Enter the following command to add the new pull secret:
								</p><pre class="programlisting language-terminal">$ oc registry login --registry="&lt;registry&gt;" \ <span id="CO26-1"><!--Empty--></span><span class="callout">1</span>
--auth-basic="&lt;username&gt;:&lt;password&gt;" \ <span id="CO26-2"><!--Empty--></span><span class="callout">2</span>
--to=&lt;pull_secret_location&gt; <span id="CO26-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO26-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Provide the new registry. You can include multiple repositories within the same registry, for example: <code class="literal">--registry="&lt;registry/my-namespace/my-repository&gt;"</code>.
										</div></dd><dt><a href="#CO26-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Provide the credentials of the new registry.
										</div></dd><dt><a href="#CO26-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Provide the path to the pull secret file.
										</div></dd></dl></div><p class="simpara">
									Alternatively, you can perform a manual update to the pull secret file.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Enter the following command to update the global pull secret for your cluster:
						</p><pre class="programlisting language-terminal">$ oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=&lt;pull_secret_location&gt; <span id="CO27-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO27-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Provide the path to the new pull secret file.
								</div></dd></dl></div><p class="simpara">
							This update is rolled out to all nodes, which can take some time depending on the size of your cluster.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								As of OpenShift Container Platform 4.7.4, changes to the global pull secret no longer trigger a node drain or reboot.
							</p></div></div></li></ol></div></section><section class="section" id="update-service-install"><div class="titlepage"><div><div><h3 class="title">13.3.5. Installing the OpenShift Update Service Operator</h3></div></div></div><p>
					To install the OpenShift Update Service, you must first install the OpenShift Update Service Operator by using the OpenShift Container Platform web console or CLI.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						For clusters that are installed in disconnected environments, also known as disconnected clusters, Operator Lifecycle Manager by default cannot access the Red Hat-provided OperatorHub sources hosted on remote registries because those remote sources require full internet connectivity. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</a>.
					</p></div></div><section class="section" id="update-service-install-web-console_updating-restricted-network-cluster-osus"><div class="titlepage"><div><div><h4 class="title">13.3.5.1. Installing the OpenShift Update Service Operator by using the web console</h4></div></div></div><p>
						You can use the web console to install the OpenShift Update Service Operator.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								In the web console, click <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span>.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Enter <code class="literal">Update Service</code> into the <span class="strong strong"><strong>Filter by keyword…​</strong></span> field to find the Operator faster.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Choose <span class="strong strong"><strong>OpenShift Update Service</strong></span> from the list of available Operators, and click <span class="strong strong"><strong>Install</strong></span>.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Channel <code class="literal">v1</code> is selected as the <span class="strong strong"><strong>Update Channel</strong></span> since it is the only channel available in this release.
									</li><li class="listitem">
										Select <span class="strong strong"><strong>A specific namespace on the cluster</strong></span> under <span class="strong strong"><strong>Installation Mode</strong></span>.
									</li><li class="listitem">
										Select a namespace for <span class="strong strong"><strong>Installed Namespace</strong></span> or accept the recommended namespace <code class="literal">openshift-update-service</code>.
									</li><li class="listitem"><p class="simpara">
										Select an <span class="strong strong"><strong>Approval Strategy</strong></span>:
									</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												The <span class="strong strong"><strong>Automatic</strong></span> strategy allows Operator Lifecycle Manager (OLM) to automatically update the Operator when a new version is available.
											</li><li class="listitem">
												The <span class="strong strong"><strong>Manual</strong></span> strategy requires a cluster administrator to approve the Operator update.
											</li></ul></div></li><li class="listitem">
										Click <span class="strong strong"><strong>Install</strong></span>.
									</li></ol></div></li><li class="listitem">
								Verify that the OpenShift Update Service Operator is installed by switching to the <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span> page.
							</li><li class="listitem">
								Ensure that <span class="strong strong"><strong>OpenShift Update Service</strong></span> is listed in the selected namespace with a <span class="strong strong"><strong>Status</strong></span> of <span class="strong strong"><strong>Succeeded</strong></span>.
							</li></ol></div></section><section class="section" id="update-service-install-cli_updating-restricted-network-cluster-osus"><div class="titlepage"><div><div><h4 class="title">13.3.5.2. Installing the OpenShift Update Service Operator by using the CLI</h4></div></div></div><p>
						You can use the OpenShift CLI (<code class="literal">oc</code>) to install the OpenShift Update Service Operator.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a namespace for the OpenShift Update Service Operator:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Create a <code class="literal">Namespace</code> object YAML file, for example, <code class="literal">update-service-namespace.yaml</code>, for the OpenShift Update Service Operator:
									</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: openshift-update-service
  annotations:
    openshift.io/node-selector: ""
  labels:
    openshift.io/cluster-monitoring: "true" <span id="CO28-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO28-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Set the <code class="literal">openshift.io/cluster-monitoring</code> label to enable Operator-recommended cluster monitoring on this namespace.
											</div></dd></dl></div></li><li class="listitem"><p class="simpara">
										Create the namespace:
									</p><pre class="programlisting language-terminal">$ oc create -f &lt;filename&gt;.yaml</pre><p class="simpara">
										For example:
									</p><pre class="programlisting language-terminal">$ oc create -f update-service-namespace.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Install the OpenShift Update Service Operator by creating the following objects:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Create an <code class="literal">OperatorGroup</code> object YAML file, for example, <code class="literal">update-service-operator-group.yaml</code>:
									</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: update-service-operator-group
spec:
  targetNamespaces:
  - openshift-update-service</pre></li><li class="listitem"><p class="simpara">
										Create an <code class="literal">OperatorGroup</code> object:
									</p><pre class="programlisting language-terminal">$ oc -n openshift-update-service create -f &lt;filename&gt;.yaml</pre><p class="simpara">
										For example:
									</p><pre class="programlisting language-terminal">$ oc -n openshift-update-service create -f update-service-operator-group.yaml</pre></li><li class="listitem"><p class="simpara">
										Create a <code class="literal">Subscription</code> object YAML file, for example, <code class="literal">update-service-subscription.yaml</code>:
									</p><div class="formalpara"><p class="title"><strong>Example Subscription</strong></p><p>
											
<pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: update-service-subscription
spec:
  channel: v1
  installPlanApproval: "Automatic"
  source: "redhat-operators" <span id="CO29-1"><!--Empty--></span><span class="callout">1</span>
  sourceNamespace: "openshift-marketplace"
  name: "cincinnati-operator"</pre>

										</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO29-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Specify the name of the catalog source that provides the Operator. For clusters that do not use a custom Operator Lifecycle Manager (OLM), specify <code class="literal">redhat-operators</code>. If your OpenShift Container Platform cluster is installed in a disconnected environment, specify the name of the <code class="literal">CatalogSource</code> object created when you configured Operator Lifecycle Manager (OLM).
											</div></dd></dl></div></li><li class="listitem"><p class="simpara">
										Create the <code class="literal">Subscription</code> object:
									</p><pre class="programlisting language-terminal">$ oc create -f &lt;filename&gt;.yaml</pre><p class="simpara">
										For example:
									</p><pre class="programlisting language-terminal">$ oc -n openshift-update-service create -f update-service-subscription.yaml</pre><p class="simpara">
										The OpenShift Update Service Operator is installed to the <code class="literal">openshift-update-service</code> namespace and targets the <code class="literal">openshift-update-service</code> namespace.
									</p></li></ol></div></li><li class="listitem"><p class="simpara">
								Verify the Operator installation:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-update-service get clusterserviceversions</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                             DISPLAY                    VERSION   REPLACES   PHASE
update-service-operator.v4.6.0   OpenShift Update Service   4.6.0                Succeeded
...</pre>

								</p></div><p class="simpara">
								If the OpenShift Update Service Operator is listed, the installation was successful. The version number might be different than shown.
							</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-installing-operators-in-namespace">Installing Operators in your namespace</a>.
							</li></ul></div></section></section><section class="section" id="update-service-graph-data_updating-restricted-network-cluster-osus"><div class="titlepage"><div><div><h3 class="title">13.3.6. Creating the OpenShift Update Service graph data container image</h3></div></div></div><p>
					The OpenShift Update Service requires a graph data container image, from which the OpenShift Update Service retrieves information about channel membership and blocked update edges. Graph data is typically fetched directly from the upgrade graph data repository. In environments where an internet connection is unavailable, loading this information from an init container is another way to make the graph data available to the OpenShift Update Service. The role of the init container is to provide a local copy of the graph data, and during pod initialization, the init container copies the data to a volume that is accessible by the service.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The oc-mirror OpenShift CLI (<code class="literal">oc</code>) plugin creates this graph data container image in addition to mirroring release images. If you used the oc-mirror plugin to mirror your release images, you can skip this procedure.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a Dockerfile, for example, <code class="literal">./Dockerfile</code>, containing the following:
						</p><pre class="programlisting language-terminal">FROM registry.access.redhat.com/ubi9/ubi:latest

RUN curl -L -o cincinnati-graph-data.tar.gz https://api.openshift.com/api/upgrades_info/graph-data

RUN mkdir -p /var/lib/cincinnati-graph-data &amp;&amp; tar xvzf cincinnati-graph-data.tar.gz -C /var/lib/cincinnati-graph-data/ --no-overwrite-dir --no-same-owner

CMD ["/bin/bash", "-c" ,"exec cp -rp /var/lib/cincinnati-graph-data/* /var/lib/cincinnati/graph-data"]</pre></li><li class="listitem"><p class="simpara">
							Use the docker file created in the above step to build a graph data container image, for example, <code class="literal">registry.example.com/openshift/graph-data:latest</code>:
						</p><pre class="programlisting language-terminal">$ podman build -f ./Dockerfile -t registry.example.com/openshift/graph-data:latest</pre></li><li class="listitem"><p class="simpara">
							Push the graph data container image created in the previous step to a repository that is accessible to the OpenShift Update Service, for example, <code class="literal">registry.example.com/openshift/graph-data:latest</code>:
						</p><pre class="programlisting language-terminal">$ podman push registry.example.com/openshift/graph-data:latest</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								To push a graph data image to a local registry in a disconnected environment, copy the graph data container image created in the previous step to a repository that is accessible to the OpenShift Update Service. Run <code class="literal">oc image mirror --help</code> for available options.
							</p></div></div></li></ol></div></section><section class="section" id="update-service-create-service"><div class="titlepage"><div><div><h3 class="title">13.3.7. Creating an OpenShift Update Service application</h3></div></div></div><p>
					You can create an OpenShift Update Service application by using the OpenShift Container Platform web console or CLI.
				</p><section class="section" id="update-service-create-service-web-console_updating-restricted-network-cluster-osus"><div class="titlepage"><div><div><h4 class="title">13.3.7.1. Creating an OpenShift Update Service application by using the web console</h4></div></div></div><p>
						You can use the OpenShift Container Platform web console to create an OpenShift Update Service application by using the OpenShift Update Service Operator.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								The OpenShift Update Service Operator has been installed.
							</li><li class="listitem">
								The OpenShift Update Service graph data container image has been created and pushed to a repository that is accessible to the OpenShift Update Service.
							</li><li class="listitem">
								The current release and update target releases have been mirrored to a locally accessible registry.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								In the web console, click <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
							</li><li class="listitem">
								Choose <span class="strong strong"><strong>OpenShift Update Service</strong></span> from the list of installed Operators.
							</li><li class="listitem">
								Click the <span class="strong strong"><strong>Update Service</strong></span> tab.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Create UpdateService</strong></span>.
							</li><li class="listitem">
								Enter a name in the <span class="strong strong"><strong>Name</strong></span> field, for example, <code class="literal">service</code>.
							</li><li class="listitem">
								Enter the local pullspec in the <span class="strong strong"><strong>Graph Data Image</strong></span> field to the graph data container image created in "Creating the OpenShift Update Service graph data container image", for example, <code class="literal">registry.example.com/openshift/graph-data:latest</code>.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>Releases</strong></span> field, enter the local registry and repository created to contain the release images in "Mirroring the OpenShift Container Platform image repository", for example, <code class="literal">registry.example.com/ocp4/openshift4-release-images</code>.
							</li><li class="listitem">
								Enter <code class="literal">2</code> in the <span class="strong strong"><strong>Replicas</strong></span> field.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Create</strong></span> to create the OpenShift Update Service application.
							</li><li class="listitem"><p class="simpara">
								Verify the OpenShift Update Service application:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										From the <span class="strong strong"><strong>UpdateServices</strong></span> list in the <span class="strong strong"><strong>Update Service</strong></span> tab, click the Update Service application just created.
									</li><li class="listitem">
										Click the <span class="strong strong"><strong>Resources</strong></span> tab.
									</li><li class="listitem">
										Verify each application resource has a status of <span class="strong strong"><strong>Created</strong></span>.
									</li></ul></div></li></ol></div></section><section class="section" id="update-service-create-service-cli_updating-restricted-network-cluster-osus"><div class="titlepage"><div><div><h4 class="title">13.3.7.2. Creating an OpenShift Update Service application by using the CLI</h4></div></div></div><p>
						You can use the OpenShift CLI (<code class="literal">oc</code>) to create an OpenShift Update Service application.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								The OpenShift Update Service Operator has been installed.
							</li><li class="listitem">
								The OpenShift Update Service graph data container image has been created and pushed to a repository that is accessible to the OpenShift Update Service.
							</li><li class="listitem">
								The current release and update target releases have been mirrored to a locally accessible registry.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Configure the OpenShift Update Service target namespace, for example, <code class="literal">openshift-update-service</code>:
							</p><pre class="programlisting language-terminal">$ NAMESPACE=openshift-update-service</pre><p class="simpara">
								The namespace must match the <code class="literal">targetNamespaces</code> value from the operator group.
							</p></li><li class="listitem"><p class="simpara">
								Configure the name of the OpenShift Update Service application, for example, <code class="literal">service</code>:
							</p><pre class="programlisting language-terminal">$ NAME=service</pre></li><li class="listitem"><p class="simpara">
								Configure the local registry and repository for the release images as configured in "Mirroring the OpenShift Container Platform image repository", for example, <code class="literal">registry.example.com/ocp4/openshift4-release-images</code>:
							</p><pre class="programlisting language-terminal">$ RELEASE_IMAGES=registry.example.com/ocp4/openshift4-release-images</pre></li><li class="listitem"><p class="simpara">
								Set the local pullspec for the graph data image to the graph data container image created in "Creating the OpenShift Update Service graph data container image", for example, <code class="literal">registry.example.com/openshift/graph-data:latest</code>:
							</p><pre class="programlisting language-terminal">$ GRAPH_DATA_IMAGE=registry.example.com/openshift/graph-data:latest</pre></li><li class="listitem"><p class="simpara">
								Create an OpenShift Update Service application object:
							</p><pre class="programlisting language-terminal">$ oc -n "${NAMESPACE}" create -f - &lt;&lt;EOF
apiVersion: updateservice.operator.openshift.io/v1
kind: UpdateService
metadata:
  name: ${NAME}
spec:
  replicas: 2
  releases: ${RELEASE_IMAGES}
  graphDataImage: ${GRAPH_DATA_IMAGE}
EOF</pre></li><li class="listitem"><p class="simpara">
								Verify the OpenShift Update Service application:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Use the following command to obtain a policy engine route:
									</p><pre class="programlisting language-terminal">$ while sleep 1; do POLICY_ENGINE_GRAPH_URI="$(oc -n "${NAMESPACE}" get -o jsonpath='{.status.policyEngineURI}/api/upgrades_info/v1/graph{"\n"}' updateservice "${NAME}")"; SCHEME="${POLICY_ENGINE_GRAPH_URI%%:*}"; if test "${SCHEME}" = http -o "${SCHEME}" = https; then break; fi; done</pre><p class="simpara">
										You might need to poll until the command succeeds.
									</p></li><li class="listitem"><p class="simpara">
										Retrieve a graph from the policy engine. Be sure to specify a valid version for <code class="literal">channel</code>. For example, if running in OpenShift Container Platform 4.13, use <code class="literal">stable-4.13</code>:
									</p><pre class="programlisting language-terminal">$ while sleep 10; do HTTP_CODE="$(curl --header Accept:application/json --output /dev/stderr --write-out "%{http_code}" "${POLICY_ENGINE_GRAPH_URI}?channel=stable-4.6")"; if test "${HTTP_CODE}" -eq 200; then break; fi; echo "${HTTP_CODE}"; done</pre><p class="simpara">
										This polls until the graph request succeeds; however, the resulting graph might be empty depending on which release images you have mirrored.
									</p></li></ol></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The policy engine route name must not be more than 63 characters based on RFC-1123. If you see <code class="literal">ReconcileCompleted</code> status as <code class="literal">false</code> with the reason <code class="literal">CreateRouteFailed</code> caused by <code class="literal">host must conform to DNS 1123 naming convention and must be no more than 63 characters</code>, try creating the Update Service with a shorter name.
						</p></div></div><section class="section" id="update-service-configure-cvo"><div class="titlepage"><div><div><h5 class="title">13.3.7.2.1. Configuring the Cluster Version Operator (CVO)</h5></div></div></div><p>
							After the OpenShift Update Service Operator has been installed and the OpenShift Update Service application has been created, the Cluster Version Operator (CVO) can be updated to pull graph data from the locally installed OpenShift Update Service.
						</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
									The OpenShift Update Service Operator has been installed.
								</li><li class="listitem">
									The OpenShift Update Service graph data container image has been created and pushed to a repository that is accessible to the OpenShift Update Service.
								</li><li class="listitem">
									The current release and update target releases have been mirrored to a locally accessible registry.
								</li><li class="listitem">
									The OpenShift Update Service application has been created.
								</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
									Set the OpenShift Update Service target namespace, for example, <code class="literal">openshift-update-service</code>:
								</p><pre class="programlisting language-terminal">$ NAMESPACE=openshift-update-service</pre></li><li class="listitem"><p class="simpara">
									Set the name of the OpenShift Update Service application, for example, <code class="literal">service</code>:
								</p><pre class="programlisting language-terminal">$ NAME=service</pre></li><li class="listitem"><p class="simpara">
									Obtain the policy engine route:
								</p><pre class="programlisting language-terminal">$ POLICY_ENGINE_GRAPH_URI="$(oc -n "${NAMESPACE}" get -o jsonpath='{.status.policyEngineURI}/api/upgrades_info/v1/graph{"\n"}' updateservice "${NAME}")"</pre></li><li class="listitem"><p class="simpara">
									Set the patch for the pull graph data:
								</p><pre class="programlisting language-terminal">$ PATCH="{\"spec\":{\"upstream\":\"${POLICY_ENGINE_GRAPH_URI}\"}}"</pre></li><li class="listitem"><p class="simpara">
									Patch the CVO to use the local OpenShift Update Service:
								</p><pre class="programlisting language-terminal">$ oc patch clusterversion version -p $PATCH --type merge</pre></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-proxy-configure-object">Enabling the cluster-wide proxy</a> to configure the CA to trust the update server.
							</p></div></div></section></section></section><section class="section" id="next-steps_updating-restricted-network-cluster-osus"><div class="titlepage"><div><div><h3 class="title">13.3.8. Next steps</h3></div></div></div><p>
					Before updating your cluster, confirm that the following conditions are met:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The Cluster Version Operator (CVO) is configured to use your locally-installed OpenShift Update Service application.
						</li><li class="listitem"><p class="simpara">
							The release image signature config map for the new release is applied to your cluster.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The release image signature config map allows the Cluster Version Operator (CVO) to ensure the integrity of release images by verifying that the actual image signatures match the expected signatures.
							</p></div></div></li><li class="listitem">
							The current release and update target release images are mirrored to a locally accessible registry.
						</li><li class="listitem">
							A recent graph data container image has been mirrored to your local registry.
						</li></ul></div><p>
					After you configure your cluster to use the locally-installed OpenShift Update Service and local mirror registry, you can use any of the following update methods:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-cluster-within-minor">Updating a cluster using the web console</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-cluster-cli">Updating a cluster using the CLI</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#preparing-eus-eus-upgrade">Preparing to perform an EUS-to-EUS update</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#update-using-custom-machine-config-pools">Performing a canary rollout update</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-cluster-rhel-compute">Updating a cluster that includes RHEL compute machines</a>
						</li></ul></div></section></section><section class="section" id="updating-restricted-network-cluster"><div class="titlepage"><div><div><h2 class="title">13.4. Updating a cluster in a disconnected environment without the OpenShift Update Service</h2></div></div></div><p>
				Use the following procedures to update a cluster in a disconnected environment without access to the OpenShift Update Service.
			</p><section class="section" id="prerequisites-4"><div class="titlepage"><div><div><h3 class="title">13.4.1. Prerequisites</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							You must have the <code class="literal">oc</code> command-line interface (CLI) tool installed.
						</li><li class="listitem">
							You must provision a local container image registry with the container images for your update, as described in <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#mirroring-ocp-image-repository">Mirroring the OpenShift Container Platform image repository</a>.
						</li><li class="listitem">
							You must have access to the cluster as a user with <code class="literal">admin</code> privileges. See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#using-rbac">Using RBAC to define and apply permissions</a>.
						</li><li class="listitem">
							You must have a recent <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/backup_and_restore/#backup-etcd">etcd backup</a> in case your update fails and you must <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/backup_and_restore/#dr-restoring-cluster-state">restore your cluster to a previous state</a>.
						</li><li class="listitem">
							You must ensure that all machine config pools (MCPs) are running and not paused. Nodes associated with a paused MCP are skipped during the update process. You can pause the MCPs if you are performing a canary rollout update strategy.
						</li><li class="listitem">
							If your cluster uses manually maintained credentials, update the cloud provider resources for the new release. For more information, including how to determine if this is a requirement for your cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#preparing-manual-creds-update">Preparing to update a cluster with manually maintained credentials</a>.
						</li><li class="listitem">
							If you run an Operator or you have configured any application with the pod disruption budget, you might experience an interruption during the upgrade process. If <code class="literal">minAvailable</code> is set to 1 in <code class="literal">PodDisruptionBudget</code>, the nodes are drained to apply pending machine configs which might block the eviction process. If several nodes are rebooted, all the pods might run on only one node, and the <code class="literal">PodDisruptionBudget</code> field can prevent the node drain.
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you run an Operator or you have configured any application with the pod disruption budget, you might experience an interruption during the upgrade process. If <code class="literal">minAvailable</code> is set to 1 in <code class="literal">PodDisruptionBudget</code>, the nodes are drained to apply pending machine configs which might block the eviction process. If several nodes are rebooted, all the pods might run on only one node, and the <code class="literal">PodDisruptionBudget</code> field can prevent the node drain.
					</p></div></div></section><section class="section" id="machine-health-checks-pausing_updating-restricted-network-cluster"><div class="titlepage"><div><div><h3 class="title">13.4.2. Pausing a MachineHealthCheck resource</h3></div></div></div><p>
					During the upgrade process, nodes in the cluster might become temporarily unavailable. In the case of worker nodes, the machine health check might identify such nodes as unhealthy and reboot them. To avoid rebooting such nodes, pause all the <code class="literal">MachineHealthCheck</code> resources before updating the cluster.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To list all the available <code class="literal">MachineHealthCheck</code> resources that you want to pause, run the following command:
						</p><pre class="programlisting language-terminal">$ oc get machinehealthcheck -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
							To pause the machine health checks, add the <code class="literal">cluster.x-k8s.io/paused=""</code> annotation to the <code class="literal">MachineHealthCheck</code> resource. Run the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api annotate mhc &lt;mhc-name&gt; cluster.x-k8s.io/paused=""</pre><p class="simpara">
							The annotated <code class="literal">MachineHealthCheck</code> resource resembles the following YAML file:
						</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: example
  namespace: openshift-machine-api
  annotations:
    cluster.x-k8s.io/paused: ""
spec:
  selector:
    matchLabels:
      role: worker
  unhealthyConditions:
  - type:    "Ready"
    status:  "Unknown"
    timeout: "300s"
  - type:    "Ready"
    status:  "False"
    timeout: "300s"
  maxUnhealthy: "40%"
status:
  currentHealthy: 5
  expectedMachines: 5</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Resume the machine health checks after updating the cluster. To resume the check, remove the pause annotation from the <code class="literal">MachineHealthCheck</code> resource by running the following command:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-machine-api annotate mhc &lt;mhc-name&gt; cluster.x-k8s.io/paused-</pre></div></div></li></ol></div></section><section class="section" id="update-restricted-image-digests_updating-restricted-network-cluster"><div class="titlepage"><div><div><h3 class="title">13.4.3. Retrieving a release image digest</h3></div></div></div><p>
					In order to update a cluster in a disconnected environment using the <code class="literal">oc adm upgrade</code> command with the <code class="literal">--to-image</code> option, you must reference the sha256 digest that corresponds to your targeted release image.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Run the following command on a device that is connected to the internet:
						</p><pre class="programlisting language-terminal">$ oc adm release info -o 'jsonpath={.digest}{"\n"}' quay.io/openshift-release-dev/ocp-release:${OCP_RELEASE_VERSION}-${ARCHITECTURE}</pre><p class="simpara">
							For <code class="literal">{OCP_RELEASE_VERSION}</code>, specify the version of OpenShift Container Platform to which you want to update, such as <code class="literal">4.10.16</code>.
						</p><p class="simpara">
							For <code class="literal">{ARCHITECTURE}</code>, specify the architecture of the cluster, such as <code class="literal">x86_64</code>, <code class="literal">aarch64</code>, <code class="literal">s390x</code>, or <code class="literal">ppc64le</code>.
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">sha256:a8bfba3b6dddd1a2fbbead7dac65fe4fb8335089e4e7cae327f3bad334add31d</pre>

							</p></div></li><li class="listitem">
							Copy the sha256 digest for use when updating your cluster.
						</li></ol></div></section><section class="section" id="update-restricted_updating-restricted-network-cluster"><div class="titlepage"><div><div><h3 class="title">13.4.4. Updating the disconnected cluster</h3></div></div></div><p>
					Update the disconnected cluster to the OpenShift Container Platform version that you downloaded the release images for.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you have a local OpenShift Update Service, you can update by using the connected web console or CLI instructions instead of this procedure.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You mirrored the images for the new release to your registry.
						</li><li class="listitem"><p class="simpara">
							You applied the release image signature ConfigMap for the new release to your cluster.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The release image signature config map allows the Cluster Version Operator (CVO) to ensure the integrity of release images by verifying that the actual image signatures match the expected signatures.
							</p></div></div></li><li class="listitem">
							You obtained the sha256 digest for your targeted release image.
						</li><li class="listitem">
							You installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You paused all <code class="literal">MachineHealthCheck</code> resources.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Update the cluster:
						</p><pre class="programlisting language-terminal">$ oc adm upgrade --allow-explicit-upgrade --to-image ${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}@&lt;digest&gt; <span id="CO30-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO30-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal">&lt;digest&gt;</code> value is the sha256 digest for the targeted release image, for example, <code class="literal">sha256:81154f5c03294534e1eaf0319bef7a601134f891689ccede5d705ef659aa8c92</code>
								</div></dd></dl></div><p class="simpara">
							If you use an <code class="literal">ImageContentSourcePolicy</code> for the mirror registry, you can use the canonical registry name instead of <code class="literal">LOCAL_REGISTRY</code>.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								You can only configure global pull secrets for clusters that have an <code class="literal">ImageContentSourcePolicy</code> object. You cannot add a pull secret to a project.
							</p></div></div></li></ul></div></section><section class="section" id="images-configuration-registry-mirror_updating-restricted-network-cluster"><div class="titlepage"><div><div><h3 class="title">13.4.5. Configuring image registry repository mirroring</h3></div></div></div><p>
					Setting up container registry repository mirroring enables you to perform the following tasks:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Configure your OpenShift Container Platform cluster to redirect requests to pull images from a repository on a source image registry and have it resolved by a repository on a mirrored image registry.
						</li><li class="listitem">
							Identify multiple mirrored repositories for each target repository, to make sure that if one mirror is down, another can be used.
						</li></ul></div><p>
					Repository mirroring in OpenShift Container Platform includes the following attributes:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Image pulls are resilient to registry downtimes.
						</li><li class="listitem">
							Clusters in disconnected environments can pull images from critical locations, such as quay.io, and have registries behind a company firewall provide the requested images.
						</li><li class="listitem">
							A particular order of registries is tried when an image pull request is made, with the permanent registry typically being the last one tried.
						</li><li class="listitem">
							The mirror information you enter is added to the <code class="literal">/etc/containers/registries.conf</code> file on every node in the OpenShift Container Platform cluster.
						</li><li class="listitem">
							When a node makes a request for an image from the source repository, it tries each mirrored repository in turn until it finds the requested content. If all mirrors fail, the cluster tries the source repository. If successful, the image is pulled to the node.
						</li></ul></div><p>
					Setting up repository mirroring can be done in the following ways:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							At OpenShift Container Platform installation:
						</p><p class="simpara">
							By pulling container images needed by OpenShift Container Platform and then bringing those images behind your company’s firewall, you can install OpenShift Container Platform into a datacenter that is in a disconnected environment.
						</p></li><li class="listitem"><p class="simpara">
							After OpenShift Container Platform installation:
						</p><p class="simpara">
							If you did not configure mirroring during OpenShift Container Platform installation, you can do so post-installation by using one of the following custom resource (CR) objects:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<code class="literal">ImageDigestMirrorSet</code>. This CR allows you to pull images from a mirrored registry by using digest specifications.
								</li><li class="listitem">
									<code class="literal">ImageTagMirrorSet</code>. This CR allows you to pull images from a mirrored registry by using image tags.
								</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Using an <code class="literal">ImageContentSourcePolicy</code> (ICSP) object to configure repository mirroring is a deprecated feature. Deprecated functionality is still included in OpenShift Container Platform and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. If you have existing YAML files that you used to create <code class="literal">ImageContentSourcePolicy</code> objects, you can use the <code class="literal">oc adm migrate icsp</code> command to convert those files to an <code class="literal">ImageDigestMirrorSet</code> YAML file. For more information, see "Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring" in the following section.
							</p></div></div></li></ul></div><p>
					Both of these custom resource objects identify the following information:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The source of the container image repository you want to mirror.
						</li><li class="listitem">
							A separate entry for each mirror repository you want to offer the content requested from the source repository.
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If your cluster uses an <code class="literal">ImageDigestMirrorSet</code> or <code class="literal">ImageTagMirrorSet</code> object to configure repository mirroring, you can use only global pull secrets for mirrored registries. You cannot add a pull secret to a project.
					</p></div></div><p>
					The following procedure creates a post-installation mirror configuration, where you create an <code class="literal">ImageDigestMirrorSet</code> object.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Ensure that you have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem"><p class="simpara">
							Ensure that there are no <code class="literal">ImageContentSourcePolicy</code> objects on your cluster. For example, you can use the following command:
						</p><pre class="programlisting language-terminal">$ oc get ImageContentSourcePolicy</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">No resources found</pre>

							</p></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Configure mirrored repositories, by either:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Setting up a mirrored repository with Red Hat Quay, as described in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/manage_red_hat_quay/repo-mirroring-in-red-hat-quay">Red Hat Quay Repository Mirroring</a>. Using Red Hat Quay allows you to copy images from one repository to another and also automatically sync those repositories repeatedly over time.
								</li><li class="listitem"><p class="simpara">
									Using a tool such as <code class="literal">skopeo</code> to copy images manually from the source directory to the mirrored repository.
								</p><p class="simpara">
									For example, after installing the skopeo RPM package on a Red Hat Enterprise Linux (RHEL) 7 or RHEL 8 system, use the <code class="literal">skopeo</code> command as shown in this example:
								</p><pre class="programlisting language-terminal">$ skopeo copy \
docker://registry.access.redhat.com/ubi9/ubi-minimal:latest@sha256:5cf... \
docker://example.io/example/ubi-minimal</pre><p class="simpara">
									In this example, you have a container image registry that is named <code class="literal">example.io</code> with an image repository named <code class="literal">example</code> to which you want to copy the <code class="literal">ubi9/ubi-minimal</code> image from <code class="literal">registry.access.redhat.com</code>. After you create the registry, you can configure your OpenShift Container Platform cluster to redirect requests made of the source repository to the mirrored repository.
								</p></li></ul></div></li><li class="listitem">
							Log in to your OpenShift Container Platform cluster.
						</li><li class="listitem"><p class="simpara">
							Create an <code class="literal">ImageDigestMirrorSet</code> or <code class="literal">ImageTagMirrorSet</code> CR, as needed, replacing the source and mirrors with your own registry and repository pairs and images:
						</p><pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1 <span id="CO31-1"><!--Empty--></span><span class="callout">1</span>
kind: ImageDigestMirrorSet <span id="CO31-2"><!--Empty--></span><span class="callout">2</span>
metadata:
  name: ubi9repo
spec:
  imageDigestMirrors: <span id="CO31-3"><!--Empty--></span><span class="callout">3</span>
  - mirrors:
    - example.io/example/ubi-minimal <span id="CO31-4"><!--Empty--></span><span class="callout">4</span>
    - example.com/example/ubi-minimal <span id="CO31-5"><!--Empty--></span><span class="callout">5</span>
    source: registry.access.redhat.com/ubi9/ubi-minimal <span id="CO31-6"><!--Empty--></span><span class="callout">6</span>
    mirrorSourcePolicy: AllowContactingSource <span id="CO31-7"><!--Empty--></span><span class="callout">7</span>
  - mirrors:
    - mirror.example.com/redhat
    source: registry.redhat.io/openshift4 <span id="CO31-8"><!--Empty--></span><span class="callout">8</span>
    mirrorSourcePolicy: AllowContactingSource
  - mirrors:
    - mirror.example.com
    source: registry.redhat.io <span id="CO31-9"><!--Empty--></span><span class="callout">9</span>
    mirrorSourcePolicy: AllowContactingSource
  - mirrors:
    - mirror.example.net/image
    source: registry.example.com/example/myimage <span id="CO31-10"><!--Empty--></span><span class="callout">10</span>
    mirrorSourcePolicy: AllowContactingSource
  - mirrors:
    - mirror.example.net
    source: registry.example.com/example <span id="CO31-11"><!--Empty--></span><span class="callout">11</span>
    mirrorSourcePolicy: AllowContactingSource
  - mirrors:
    - mirror.example.net/registry-example-com
    source: registry.example.com <span id="CO31-12"><!--Empty--></span><span class="callout">12</span>
    mirrorSourcePolicy: AllowContactingSource</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO31-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Indicates the API to use with this CR. This must be <code class="literal">config.openshift.io/v1</code>.
								</div></dd><dt><a href="#CO31-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Indicates the kind of object according to the pull type:
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">ImageDigestMirrorSet</code>: Pulls a digest reference image.
										</li><li class="listitem">
											<code class="literal">ImageTagMirrorSet</code>: Pulls a tag reference image.
										</li></ul></div></dd><dt><a href="#CO31-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Indicates the type of image pull method, either:
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">imageDigestMirrors</code>: Use for an <code class="literal">ImageDigestMirrorSet</code> CR.
										</li><li class="listitem">
											<code class="literal">imageTagMirrors</code>: Use for an <code class="literal">ImageTagMirrorSet</code> CR.
										</li></ul></div></dd><dt><a href="#CO31-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Indicates the name of the mirrored image registry and repository.
								</div></dd><dt><a href="#CO31-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Optional: Indicates a secondary mirror repository for each target repository. If one mirror is down, the target repository can use another mirror.
								</div></dd><dt><a href="#CO31-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Indicates the registry and repository source, which is the repository that is referred to in image pull specifications.
								</div></dd><dt><a href="#CO31-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									Optional: Indicates the fallback policy if the image pull fails:
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">AllowContactingSource</code>: Allows continued attempts to pull the image from the source repository. This is the default.
										</li><li class="listitem">
											<code class="literal">NeverContactSource</code>: Prevents continued attempts to pull the image from the source repository.
										</li></ul></div></dd><dt><a href="#CO31-8"><span class="callout">8</span></a> </dt><dd><div class="para">
									Optional: Indicates a namespace inside a registry, which allows you to use any image in that namespace. If you use a registry domain as a source, the object is applied to all repositories from the registry.
								</div></dd><dt><a href="#CO31-9"><span class="callout">9</span></a> </dt><dd><div class="para">
									Optional: Indicates a registry, which allows you to use any image in that registry. If you specify a registry name, the object is applied to all repositories from a source registry to a mirror registry.
								</div></dd><dt><a href="#CO31-10"><span class="callout">10</span></a> </dt><dd><div class="para">
									Pulls the image <code class="literal">registry.example.com/example/myimage@sha256:…​</code> from the mirror <code class="literal">mirror.example.net/image@sha256:..</code>.
								</div></dd><dt><a href="#CO31-11"><span class="callout">11</span></a> </dt><dd><div class="para">
									Pulls the image <code class="literal">registry.example.com/example/image@sha256:…​</code> in the source registry namespace from the mirror <code class="literal">mirror.example.net/image@sha256:…​</code>.
								</div></dd><dt><a href="#CO31-12"><span class="callout">12</span></a> </dt><dd><div class="para">
									Pulls the image <code class="literal">registry.example.com/myimage@sha256</code> from the mirror registry <code class="literal">example.net/registry-example-com/myimage@sha256:…​</code>. The <code class="literal">ImageContentSourcePolicy</code> resource is applied to all repositories from a source registry to a mirror registry <code class="literal">mirror.example.net/registry-example-com</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the new object:
						</p><pre class="programlisting language-terminal">$ oc create -f registryrepomirror.yaml</pre><p class="simpara">
							After the object is created, the Machine Config Operator (MCO) cordons the nodes as the new settings are deployed to each node. The MCO restarts the nodes for an <code class="literal">ImageTagMirrorSet</code> object only. The MCO does not restart the nodes for <code class="literal">ImageDigestMirrorSet</code> objects. When the nodes are uncordoned, the cluster starts using the mirrored repository for requests to the source repository.
						</p></li><li class="listitem"><p class="simpara">
							To check that the mirrored configuration settings are applied, do the following on one of the nodes.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									List your nodes:
								</p><pre class="programlisting language-terminal">$ oc get node</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                           STATUS                     ROLES    AGE  VERSION
ip-10-0-137-44.ec2.internal    Ready                      worker   7m   v1.26.0
ip-10-0-138-148.ec2.internal   Ready                      master   11m  v1.26.0
ip-10-0-139-122.ec2.internal   Ready                      master   11m  v1.26.0
ip-10-0-147-35.ec2.internal    Ready                      worker   7m   v1.26.0
ip-10-0-153-12.ec2.internal    Ready                      worker   7m   v1.26.0
ip-10-0-154-10.ec2.internal    Ready                      master   11m  v1.26.0</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Start the debugging process to access the node:
								</p><pre class="programlisting language-terminal">$ oc debug node/ip-10-0-147-35.ec2.internal</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">Starting pod/ip-10-0-147-35ec2internal-debug ...
To use host binaries, run `chroot /host`</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Change your root directory to <code class="literal">/host</code>:
								</p><pre class="programlisting language-terminal">sh-4.2# chroot /host</pre></li><li class="listitem"><p class="simpara">
									Check the <code class="literal">/etc/containers/registries.conf</code> file to make sure the changes were made:
								</p><pre class="programlisting language-terminal">sh-4.2# cat /etc/containers/registries.conf</pre><p class="simpara">
									The following output represents a <code class="literal">registries.conf</code> file where an <code class="literal">ImageDigestMirrorSet</code> object and an <code class="literal">ImageTagMirrorSet</code> object were applied. The final two entries are marked <code class="literal">digest-only</code> and <code class="literal">tag-only</code> respectively.
								</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">unqualified-search-registries = ["registry.access.redhat.com", "docker.io"]
short-name-mode = ""

[[registry]]
  prefix = ""
  location = "registry.access.redhat.com/ubi9/ubi-minimal" <span id="CO32-1"><!--Empty--></span><span class="callout">1</span>

  [[registry.mirror]]
    location = "example.io/example/ubi-minimal" <span id="CO32-2"><!--Empty--></span><span class="callout">2</span>
    pull-from-mirror = "digest-only" <span id="CO32-3"><!--Empty--></span><span class="callout">3</span>

  [[registry.mirror]]
    location = "example.com/example/ubi-minimal"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.example.com"

  [[registry.mirror]]
    location = "mirror.example.net/registry-example-com"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.example.com/example"

  [[registry.mirror]]
    location = "mirror.example.net"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.example.com/example/myimage"

  [[registry.mirror]]
    location = "mirror.example.net/image"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.redhat.io"

  [[registry.mirror]]
    location = "mirror.example.com"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.redhat.io/openshift4"

  [[registry.mirror]]
    location = "mirror.example.com/redhat"
    pull-from-mirror = "digest-only"
[[registry]]
  prefix = ""
  location = "registry.access.redhat.com/ubi9/ubi-minimal"
  blocked = true <span id="CO32-4"><!--Empty--></span><span class="callout">4</span>

  [[registry.mirror]]
    location = "example.io/example/ubi-minimal-tag"
    pull-from-mirror = "tag-only" <span id="CO32-5"><!--Empty--></span><span class="callout">5</span></pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO32-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Indicates the repository that is referred to in a pull spec.
										</div></dd><dt><a href="#CO32-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Indicates the mirror for that repository.
										</div></dd><dt><a href="#CO32-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Indicates that the image pull from the mirror is a digest reference image.
										</div></dd><dt><a href="#CO32-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Indicates that the <code class="literal">NeverContactSource</code> parameter is set for this repository.
										</div></dd><dt><a href="#CO32-5"><span class="callout">5</span></a> </dt><dd><div class="para">
											Indicates that the image pull from the mirror is a tag reference image.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Pull an image to the node from the source and check if it is resolved by the mirror.
								</p><pre class="programlisting language-terminal">sh-4.2# podman pull --log-level=debug registry.access.redhat.com/ubi9/ubi-minimal@sha256:5cf...</pre></li></ol></div></li></ol></div><div class="formalpara"><p class="title"><strong>Troubleshooting repository mirroring</strong></p><p>
						If the repository mirroring procedure does not work as described, use the following information about how repository mirroring works to help troubleshoot the problem.
					</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The first working mirror is used to supply the pulled image.
						</li><li class="listitem">
							The main registry is only used if no other mirror works.
						</li><li class="listitem">
							From the system context, the <code class="literal">Insecure</code> flags are used as fallback.
						</li><li class="listitem">
							The format of the <code class="literal">/etc/containers/registries.conf</code> file has changed recently. It is now version 2 and in TOML format.
						</li><li class="listitem">
							You cannot add the same repository to both an <code class="literal">ImageDigestMirrorSet</code> and an <code class="literal">ImageTagMirrorSet</code> object.
						</li></ul></div><section class="section" id="images-configuration-registry-mirror-convert_updating-restricted-network-cluster"><div class="titlepage"><div><div><h4 class="title">13.4.5.1. Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring</h4></div></div></div><p>
						Using an <code class="literal">ImageContentSourcePolicy</code> (ICSP) object to configure repository mirroring is a deprecated feature. This functionality is still included in OpenShift Container Platform and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments.
					</p><p>
						ICSP objects are being replaced by <code class="literal">ImageDigestMirrorSet</code> and <code class="literal">ImageTagMirrorSet</code> objects to configure repository mirroring. If you have existing YAML files that you used to create <code class="literal">ImageContentSourcePolicy</code> objects, you can use the <code class="literal">oc adm migrate icsp</code> command to convert those files to an <code class="literal">ImageDigestMirrorSet</code> YAML file. The command updates the API to the current version, changes the <code class="literal">kind</code> value to <code class="literal">ImageDigestMirrorSet</code>, and changes <code class="literal">spec.repositoryDigestMirrors</code> to <code class="literal">spec.imageDigestMirrors</code>. The rest of the file is not changed.
					</p><p>
						For more information about <code class="literal">ImageDigestMirrorSet</code> or <code class="literal">ImageTagMirrorSet</code> objects, see "Configuring image registry repository mirroring" in the previous section.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Ensure that you have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								Ensure that you have <code class="literal">ImageContentSourcePolicy</code> objects on your cluster.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Use the following command to convert one or more <code class="literal">ImageContentSourcePolicy</code> YAML files to an <code class="literal">ImageDigestMirrorSet</code> YAML file:
							</p><pre class="programlisting language-terminal">$ oc adm migrate icsp &lt;file_name&gt;.yaml &lt;file_name&gt;.yaml &lt;file_name&gt;.yaml --dest-dir &lt;path_to_the_directory&gt;</pre><p class="simpara">
								where:
							</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;file_name&gt;</code></span></dt><dd>
											Specifies the name of the source <code class="literal">ImageContentSourcePolicy</code> YAML. You can list multiple file names.
										</dd><dt><span class="term"><code class="literal">--dest-dir</code></span></dt><dd>
											Optional: Specifies a directory for the output <code class="literal">ImageDigestMirrorSet</code> YAML. If unset, the file is written to the current directory.
										</dd></dl></div><p class="simpara">
								For example, the following command converts the <code class="literal">icsp.yaml</code> and <code class="literal">icsp-2.yaml</code> file and saves the new YAML files to the <code class="literal">idms-files</code> directory.
							</p><pre class="programlisting language-terminal">$ oc adm migrate icsp icsp.yaml icsp-2.yaml --dest-dir idms-files</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">wrote ImageDigestMirrorSet to idms-files/imagedigestmirrorset_ubi8repo.5911620242173376087.yaml
wrote ImageDigestMirrorSet to idms-files/imagedigestmirrorset_ubi9repo.6456931852378115011.yaml</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Create the CR object by running the following command:
							</p><pre class="programlisting language-terminal">$ oc create -f &lt;path_to_the_directory&gt;/&lt;file-name&gt;.yaml</pre><p class="simpara">
								where:
							</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;path_to_the_directory&gt;</code></span></dt><dd>
											Specifies the path to the directory, if you used the <code class="literal">--dest-dir</code> flag.
										</dd><dt><span class="term"><code class="literal">&lt;file_name&gt;</code></span></dt><dd>
											Specifies the name of the <code class="literal">ImageDigestMirrorSet</code> YAML.
										</dd></dl></div></li></ol></div></section></section><section class="section" id="generating-icsp-object-scoped-to-a-registry_updating-restricted-network-cluster"><div class="titlepage"><div><div><h3 class="title">13.4.6. Widening the scope of the mirror image catalog to reduce the frequency of cluster node reboots</h3></div></div></div><p>
					You can scope the mirrored image catalog at the repository level or the wider registry level. A widely scoped <code class="literal">ImageContentSourcePolicy</code> resource reduces the number of times the nodes need to reboot in response to changes to the resource.
				</p><p>
					To widen the scope of the mirror image catalog in the <code class="literal">ImageContentSourcePolicy</code> resource, perform the following procedure.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift Container Platform CLI <code class="literal">oc</code>.
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							Configure a mirrored image catalog for use in your disconnected cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Run the following command, specifying values for <code class="literal">&lt;local_registry&gt;</code>, <code class="literal">&lt;pull_spec&gt;</code>, and <code class="literal">&lt;pull_secret_file&gt;</code>:
						</p><pre class="programlisting language-terminal">$ oc adm catalog mirror &lt;local_registry&gt;/&lt;pull_spec&gt; &lt;local_registry&gt; -a &lt;pull_secret_file&gt; --icsp-scope=registry</pre><p class="simpara">
							where:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;local_registry&gt;</span></dt><dd>
										is the local registry you have configured for your disconnected cluster, for example, <code class="literal">local.registry:5000</code>.
									</dd><dt><span class="term">&lt;pull_spec&gt;</span></dt><dd>
										is the pull specification as configured in your disconnected registry, for example, <code class="literal">redhat/redhat-operator-index:v4.13</code>
									</dd><dt><span class="term">&lt;pull_secret_file&gt;</span></dt><dd>
										is the <code class="literal">registry.redhat.io</code> pull secret in <code class="literal">.json</code> file format. You can download the <a class="link" href="https://console.redhat.com/openshift/install/pull-secret">pull secret from the Red Hat OpenShift Cluster Manager</a>.
									</dd></dl></div><p class="simpara">
							The <code class="literal">oc adm catalog mirror</code> command creates a <code class="literal">/redhat-operator-index-manifests</code> directory and generates <code class="literal">imageContentSourcePolicy.yaml</code>, <code class="literal">catalogSource.yaml</code>, and <code class="literal">mapping.txt</code> files.
						</p></li><li class="listitem"><p class="simpara">
							Apply the new <code class="literal">ImageContentSourcePolicy</code> resource to the cluster:
						</p><pre class="programlisting language-terminal">$ oc apply -f imageContentSourcePolicy.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Verify that <code class="literal">oc apply</code> successfully applied the change to <code class="literal">ImageContentSourcePolicy</code>:
						</p><pre class="programlisting language-terminal">$ oc get ImageContentSourcePolicy -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
items:
- apiVersion: operator.openshift.io/v1alpha1
  kind: ImageContentSourcePolicy
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"operator.openshift.io/v1alpha1","kind":"ImageContentSourcePolicy","metadata":{"annotations":{},"name":"redhat-operator-index"},"spec":{"repositoryDigestMirrors":[{"mirrors":["local.registry:5000"],"source":"registry.redhat.io"}]}}
...</pre>

							</p></div></li></ul></div><p>
					After you update the <code class="literal">ImageContentSourcePolicy</code> resource, OpenShift Container Platform deploys the new settings to each node and the cluster starts using the mirrored repository for requests to the source repository.
				</p></section><section class="section _additional-resources" id="additional-resources_security-container-signature"><div class="titlepage"><div><div><h3 class="title">13.4.7. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#machine-config-overview-post-install-machine-configuration-tasks">Machine Config Overview</a>
						</li></ul></div></section></section><section class="section" id="uninstalling-osus"><div class="titlepage"><div><div><h2 class="title">13.5. Uninstalling the OpenShift Update Service from a cluster</h2></div></div></div><p>
				To remove a local copy of the OpenShift Update Service (OSUS) from your cluster, you must first delete the OSUS application and then uninstall the OSUS Operator.
			</p><section class="section" id="update-service-delete-service"><div class="titlepage"><div><div><h3 class="title">13.5.1. Deleting an OpenShift Update Service application</h3></div></div></div><p>
					You can delete an OpenShift Update Service application by using the OpenShift Container Platform web console or CLI.
				</p><section class="section" id="update-service-delete-service-web-console_uninstalling-osus"><div class="titlepage"><div><div><h4 class="title">13.5.1.1. Deleting an OpenShift Update Service application by using the web console</h4></div></div></div><p>
						You can use the OpenShift Container Platform web console to delete an OpenShift Update Service application by using the OpenShift Update Service Operator.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								The OpenShift Update Service Operator has been installed.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								In the web console, click <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
							</li><li class="listitem">
								Choose <span class="strong strong"><strong>OpenShift Update Service</strong></span> from the list of installed Operators.
							</li><li class="listitem">
								Click the <span class="strong strong"><strong>Update Service</strong></span> tab.
							</li><li class="listitem">
								From the list of installed OpenShift Update Service applications, select the application to be deleted and then click <span class="strong strong"><strong>Delete UpdateService</strong></span>.
							</li><li class="listitem">
								From the <span class="strong strong"><strong>Delete UpdateService?</strong></span> confirmation dialog, click <span class="strong strong"><strong>Delete</strong></span> to confirm the deletion.
							</li></ol></div></section><section class="section" id="update-service-delete-service-cli_uninstalling-osus"><div class="titlepage"><div><div><h4 class="title">13.5.1.2. Deleting an OpenShift Update Service application by using the CLI</h4></div></div></div><p>
						You can use the OpenShift CLI (<code class="literal">oc</code>) to delete an OpenShift Update Service application.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Get the OpenShift Update Service application name using the namespace the OpenShift Update Service application was created in, for example, <code class="literal">openshift-update-service</code>:
							</p><pre class="programlisting language-terminal">$ oc get updateservice -n openshift-update-service</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME      AGE
service   6s</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Delete the OpenShift Update Service application using the <code class="literal">NAME</code> value from the previous step and the namespace the OpenShift Update Service application was created in, for example, <code class="literal">openshift-update-service</code>:
							</p><pre class="programlisting language-terminal">$ oc delete updateservice service -n openshift-update-service</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">updateservice.updateservice.operator.openshift.io "service" deleted</pre>

								</p></div></li></ol></div></section></section><section class="section" id="update-service-uninstall"><div class="titlepage"><div><div><h3 class="title">13.5.2. Uninstalling the OpenShift Update Service Operator</h3></div></div></div><p>
					You can uninstall the OpenShift Update Service Operator by using the OpenShift Container Platform web console or CLI.
				</p><section class="section" id="update-service-uninstall-web-console_uninstalling-osus"><div class="titlepage"><div><div><h4 class="title">13.5.2.1. Uninstalling the OpenShift Update Service Operator by using the web console</h4></div></div></div><p>
						You can use the OpenShift Container Platform web console to uninstall the OpenShift Update Service Operator.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								All OpenShift Update Service applications have been deleted.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								In the web console, click <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
							</li><li class="listitem">
								Select <span class="strong strong"><strong>OpenShift Update Service</strong></span> from the list of installed Operators and click <span class="strong strong"><strong>Uninstall Operator</strong></span>.
							</li><li class="listitem">
								From the <span class="strong strong"><strong>Uninstall Operator?</strong></span> confirmation dialog, click <span class="strong strong"><strong>Uninstall</strong></span> to confirm the uninstallation.
							</li></ol></div></section><section class="section" id="update-service-uninstall-cli_uninstalling-osus"><div class="titlepage"><div><div><h4 class="title">13.5.2.2. Uninstalling the OpenShift Update Service Operator by using the CLI</h4></div></div></div><p>
						You can use the OpenShift CLI (<code class="literal">oc</code>) to uninstall the OpenShift Update Service Operator.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								All OpenShift Update Service applications have been deleted.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Change to the project containing the OpenShift Update Service Operator, for example, <code class="literal">openshift-update-service</code>:
							</p><pre class="programlisting language-terminal">$ oc project openshift-update-service</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Now using project "openshift-update-service" on server "https://example.com:6443".</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Get the name of the OpenShift Update Service Operator operator group:
							</p><pre class="programlisting language-terminal">$ oc get operatorgroup</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                             AGE
openshift-update-service-fprx2   4m41s</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Delete the operator group, for example, <code class="literal">openshift-update-service-fprx2</code>:
							</p><pre class="programlisting language-terminal">$ oc delete operatorgroup openshift-update-service-fprx2</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">operatorgroup.operators.coreos.com "openshift-update-service-fprx2" deleted</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Get the name of the OpenShift Update Service Operator subscription:
							</p><pre class="programlisting language-terminal">$ oc get subscription</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                      PACKAGE                   SOURCE                        CHANNEL
update-service-operator   update-service-operator   updateservice-index-catalog   v1</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Using the <code class="literal">Name</code> value from the previous step, check the current version of the subscribed OpenShift Update Service Operator in the <code class="literal">currentCSV</code> field:
							</p><pre class="programlisting language-terminal">$ oc get subscription update-service-operator -o yaml | grep " currentCSV"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">  currentCSV: update-service-operator.v0.0.1</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Delete the subscription, for example, <code class="literal">update-service-operator</code>:
							</p><pre class="programlisting language-terminal">$ oc delete subscription update-service-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">subscription.operators.coreos.com "update-service-operator" deleted</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Delete the CSV for the OpenShift Update Service Operator using the <code class="literal">currentCSV</code> value from the previous step:
							</p><pre class="programlisting language-terminal">$ oc delete clusterserviceversion update-service-operator.v0.0.1</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">clusterserviceversion.operators.coreos.com "update-service-operator.v0.0.1" deleted</pre>

								</p></div></li></ol></div></section></section></section></section><section class="chapter" id="updating-hardware-on-nodes-running-on-vsphere"><div class="titlepage"><div><div><h1 class="title">Chapter 14. Updating hardware on nodes running on vSphere</h1></div></div></div><p>
			You must ensure that your nodes running in vSphere are running on the hardware version supported by OpenShift Container Platform. Currently, hardware version 15 or later is supported for vSphere virtual machines in a cluster.
		</p><p>
			You can update your virtual hardware immediately or schedule an update in vCenter.
		</p><div class="admonition important"><div class="admonition_header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Version 4.13 of OpenShift Container Platform requires VMware virtual hardware version 15 or later.
					</li><li class="listitem">
						Before upgrading OpenShift 4.12 to OpenShift 4.13, you must update vSphere to <span class="strong strong"><strong>v7.0.2 or later</strong></span>; otherwise, the OpenShift 4.12 cluster is marked <span class="strong strong"><strong>un-upgradeable</strong></span>.
					</li></ul></div></div></div><section class="section" id="updating-virtual-hardware-on-vsphere_updating-hardware-on-nodes-running-in-vsphere"><div class="titlepage"><div><div><h2 class="title">14.1. Updating virtual hardware on vSphere</h2></div></div></div><p>
				To update the hardware of your virtual machines (VMs) on VMware vSphere, update your virtual machines separately to reduce the risk of downtime for your cluster.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					As of OpenShift Container Platform 4.13, VMware virtual hardware version 13 is no longer supported. You need to update to VMware version 15 or later for supporting functionality.
				</p></div></div><section class="section" id="update-vsphere-virtual-hardware-on-control-plane-nodes_updating-hardware-on-nodes-running-in-vsphere"><div class="titlepage"><div><div><h3 class="title">14.1.1. Updating the virtual hardware for control plane nodes on vSphere</h3></div></div></div><p>
					To reduce the risk of downtime, it is recommended that control plane nodes be updated serially. This ensures that the Kubernetes API remains available and etcd retains quorum.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have cluster administrator permissions to execute the required permissions in the vCenter instance hosting your OpenShift Container Platform cluster.
						</li><li class="listitem">
							Your vSphere ESXi hosts are version 7.0U2 or later.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							List the control plane nodes in your cluster.
						</p><pre class="programlisting language-terminal">$ oc get nodes -l node-role.kubernetes.io/master</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                    STATUS   ROLES    AGE   VERSION
control-plane-node-0    Ready    master   75m   v1.26.0
control-plane-node-1    Ready    master   75m   v1.26.0
control-plane-node-2    Ready    master   75m   v1.26.0</pre>

							</p></div><p class="simpara">
							Note the names of your control plane nodes.
						</p></li><li class="listitem"><p class="simpara">
							Mark the control plane node as unschedulable.
						</p><pre class="programlisting language-terminal">$ oc adm cordon &lt;control_plane_node&gt;</pre></li><li class="listitem">
							Shut down the virtual machine (VM) associated with the control plane node. Do this in the vSphere client by right-clicking the VM and selecting <span class="strong strong"><strong>Power</strong></span> → <span class="strong strong"><strong>Shut Down Guest OS</strong></span>. Do not shut down the VM using <span class="strong strong"><strong>Power Off</strong></span> because it might not shut down safely.
						</li><li class="listitem">
							Update the VM in the vSphere client. Follow <a class="link" href="https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vm_admin.doc/GUID-60768C2F-72E1-42E0-8A17-CA76849F2950.html">Upgrade the Compatibility of a Virtual Machine Manually</a> in the VMware documentation for more information.
						</li><li class="listitem">
							Power on the VM associated with the control plane node. Do this in the vSphere client by right-clicking the VM and selecting <span class="strong strong"><strong>Power On</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Wait for the node to report as <code class="literal">Ready</code>:
						</p><pre class="programlisting language-terminal">$ oc wait --for=condition=Ready node/&lt;control_plane_node&gt;</pre></li><li class="listitem"><p class="simpara">
							Mark the control plane node as schedulable again:
						</p><pre class="programlisting language-terminal">$ oc adm uncordon &lt;control_plane_node&gt;</pre></li><li class="listitem">
							Repeat this procedure for each control plane node in your cluster.
						</li></ol></div></section><section class="section" id="update-vsphere-virtual-hardware-on-compute-nodes_updating-hardware-on-nodes-running-in-vsphere"><div class="titlepage"><div><div><h3 class="title">14.1.2. Updating the virtual hardware for compute nodes on vSphere</h3></div></div></div><p>
					To reduce the risk of downtime, it is recommended that compute nodes be updated serially.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Multiple compute nodes can be updated in parallel given workloads are tolerant of having multiple nodes in a <code class="literal">NotReady</code> state. It is the responsibility of the administrator to ensure that the required compute nodes are available.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have cluster administrator permissions to execute the required permissions in the vCenter instance hosting your OpenShift Container Platform cluster.
						</li><li class="listitem">
							Your vSphere ESXi hosts are version 7.0U2 or later.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							List the compute nodes in your cluster.
						</p><pre class="programlisting language-terminal">$ oc get nodes -l node-role.kubernetes.io/worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME              STATUS   ROLES    AGE   VERSION
compute-node-0    Ready    worker   30m   v1.26.0
compute-node-1    Ready    worker   30m   v1.26.0
compute-node-2    Ready    worker   30m   v1.26.0</pre>

							</p></div><p class="simpara">
							Note the names of your compute nodes.
						</p></li><li class="listitem"><p class="simpara">
							Mark the compute node as unschedulable:
						</p><pre class="programlisting language-terminal">$ oc adm cordon &lt;compute_node&gt;</pre></li><li class="listitem"><p class="simpara">
							Evacuate the pods from the compute node. There are several ways to do this. For example, you can evacuate all or selected pods on a node:
						</p><pre class="programlisting language-terminal">$ oc adm drain &lt;compute_node&gt; [--pod-selector=&lt;pod_selector&gt;]</pre><p class="simpara">
							See the "Understanding how to evacuate pods on nodes" section for other options to evacuate pods from a node.
						</p></li><li class="listitem">
							Shut down the virtual machine (VM) associated with the compute node. Do this in the vSphere client by right-clicking the VM and selecting <span class="strong strong"><strong>Power</strong></span> → <span class="strong strong"><strong>Shut Down Guest OS</strong></span>. Do not shut down the VM using <span class="strong strong"><strong>Power Off</strong></span> because it might not shut down safely.
						</li><li class="listitem">
							Update the VM in the vSphere client. Follow <a class="link" href="https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vm_admin.doc/GUID-60768C2F-72E1-42E0-8A17-CA76849F2950.html">Upgrade the Compatibility of a Virtual Machine Manually</a> in the VMware documentation for more information.
						</li><li class="listitem">
							Power on the VM associated with the compute node. Do this in the vSphere client by right-clicking the VM and selecting <span class="strong strong"><strong>Power On</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Wait for the node to report as <code class="literal">Ready</code>:
						</p><pre class="programlisting language-terminal">$ oc wait --for=condition=Ready node/&lt;compute_node&gt;</pre></li><li class="listitem"><p class="simpara">
							Mark the compute node as schedulable again:
						</p><pre class="programlisting language-terminal">$ oc adm uncordon &lt;compute_node&gt;</pre></li><li class="listitem">
							Repeat this procedure for each compute node in your cluster.
						</li></ol></div></section><section class="section" id="update-vsphere-virtual-hardware-on-template_updating-hardware-on-nodes-running-in-vsphere"><div class="titlepage"><div><div><h3 class="title">14.1.3. Updating the virtual hardware for template on vSphere</h3></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have cluster administrator permissions to execute the required permissions in the vCenter instance hosting your OpenShift Container Platform cluster.
						</li><li class="listitem">
							Your vSphere ESXi hosts are version 7.0U2 or later.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							If the RHCOS template is configured as a vSphere template follow <a class="link" href="https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vm_admin.doc/GUID-D632CAC5-BA5E-4A1E-959B-382D9ACB1DD0_copy.html">Convert a Template to a Virtual Machine</a> in the VMware documentation prior to the next step.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Once converted from a template, do not power on the virtual machine.
							</p></div></div></li><li class="listitem">
							Update the VM in the vSphere client. Follow <a class="link" href="https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vm_admin.doc/GUID-60768C2F-72E1-42E0-8A17-CA76849F2950.html">Upgrade the Compatibility of a Virtual Machine Manually</a> in the VMware documentation for more information.
						</li><li class="listitem">
							Convert the VM in the vSphere client from a VM to template. Follow <a class="link" href="https://docs.vmware.com/en/VMware-vSphere/6.0/com.vmware.vsphere.hostclient.doc/GUID-846238E4-A1E3-4A28-B230-33BDD1D57454.html">Convert a Virtual Machine to a Template in the vSphere Client</a> in the VMware documentation for more information.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-working-evacuating_nodes-nodes-working">Understanding how to evacuate pods on nodes</a>
						</li></ul></div></section></section><section class="section" id="scheduling-virtual-hardware-update-on-vsphere_updating-hardware-on-nodes-running-in-vsphere"><div class="titlepage"><div><div><h2 class="title">14.2. Scheduling an update for virtual hardware on vSphere</h2></div></div></div><p>
				Virtual hardware updates can be scheduled to occur when a virtual machine is powered on or rebooted. You can schedule your virtual hardware updates exclusively in vCenter by following <a class="link" href="https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vm_admin.doc/GUID-96C06236-C271-4CFE-857E-22D1FDEECC95.html">Schedule a Compatibility Upgrade for a Virtual Machine</a> in the VMware documentation.
			</p><p>
				When scheduling an upgrade prior to performing an upgrade of OpenShift Container Platform, the virtual hardware update occurs when the nodes are rebooted during the course of the OpenShift Container Platform upgrade.
			</p></section></section><section class="chapter" id="kmm-preflight-validation"><div class="titlepage"><div><div><h1 class="title">Chapter 15. Preflight validation for Kernel Module Management (KMM) Modules</h1></div></div></div><p>
			Before performing an upgrade on the cluster with applied KMM modules, the administrator must verify that kernel modules installed using KMM are able to be installed on the nodes after the cluster upgrade and possible kernel upgrade. Preflight attempts to validate every <code class="literal">Module</code> loaded in the cluster, in parallel. Preflight does not wait for validation of one <code class="literal">Module</code> to complete before starting validation of another <code class="literal">Module</code>.
		</p><section class="section" id="kmm-validation-kickoff_kmm-preflight-validation"><div class="titlepage"><div><div><h2 class="title">15.1. Validation kickoff</h2></div></div></div><p>
				Preflight validation is triggered by creating a <code class="literal">PreflightValidationOCP</code> resource in the cluster. This spec contains two fields:
			</p><pre class="programlisting language-terminal">type PreflightValidationOCPSpec struct {
	// releaseImage describes the OCP release image that all Modules need to be checked against.
	// +kubebuilder:validation:Required
	ReleaseImage string `json:"releaseImage"` <span id="CO33-1"><!--Empty--></span><span class="callout">1</span>
	// Boolean flag that determines whether images build during preflight must also
	// be pushed to a defined repository
	// +optional
	PushBuiltImage bool `json:"pushBuiltImage"` <span id="CO33-2"><!--Empty--></span><span class="callout">2</span>
}</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO33-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						<code class="literal">ReleaseImage</code> - Mandatory field that provides the name of the release image for the OpenShift Container Platform version the cluster is upgraded to.
					</div></dd><dt><a href="#CO33-2"><span class="callout">2</span></a> </dt><dd><div class="para">
						<code class="literal">PushBuiltImage</code> - If <code class="literal">true</code>, then the images created during the Build and Sign validation are pushed to their repositories (<code class="literal">false</code> by default).
					</div></dd></dl></div></section><section class="section" id="kmm-validation-lifecycle_kmm-preflight-validation"><div class="titlepage"><div><div><h2 class="title">15.2. Validation lifecycle</h2></div></div></div><p>
				Preflight validation attempts to validate every module loaded in the cluster. Preflight will stop running validation on a <code class="literal">Module</code> resource after the validation is successful. In case module validation has failed, you can change the module definitions and Preflight will try to validate the module again in the next loop.
			</p><p>
				If you want to run Preflight validation for an additional kernel, then you should create another <code class="literal">PreflightValidationOCP</code> resource for that kernel. After all the modules have been validated, it is recommended to delete the <code class="literal">PreflightValidationOCP</code> resource.
			</p></section><section class="section" id="kmm-validation-status_kmm-preflight-validation"><div class="titlepage"><div><div><h2 class="title">15.3. Validation status</h2></div></div></div><p>
				Preflight reports the status and progress of each module in the cluster that it attempts to validate.
			</p><pre class="programlisting language-terminal">type CRStatus struct {
	// Status of Module CR verification: true (verified), false (verification failed),
	// error (error during verification process), unknown (verification has not started yet)
	// +required
	// +kubebuilder:validation:Required
	// +kubebuilder:validation:Enum=True;False
	VerificationStatus string `json:"verificationStatus"` <span id="CO34-1"><!--Empty--></span><span class="callout">1</span>
	// StatusReason contains a string describing the status source.
	// +optional
	StatusReason string `json:"statusReason,omitempty"` <span id="CO34-2"><!--Empty--></span><span class="callout">2</span>
	// Current stage of the verification process:
	// image (image existence verification), build(build process verification)
	// +required
	// +kubebuilder:validation:Required
	// +kubebuilder:validation:Enum=Image;Build;Sign;Requeued;Done
	VerificationStage string `json:"verificationStage"` <span id="CO34-3"><!--Empty--></span><span class="callout">3</span>
	// LastTransitionTime is the last time the CR status transitioned from one status to another.
	// This should be when the underlying status changed.  If that is not known, then using the time when the API field changed is acceptable.
	// +required
	// +kubebuilder:validation:Required
	// +kubebuilder:validation:Type=string
	// +kubebuilder:validation:Format=date-time
	LastTransitionTime metav1.Time `json:"lastTransitionTime" protobuf:"bytes,4,opt,name=lastTransitionTime"` <span id="CO34-4"><!--Empty--></span><span class="callout">4</span>
}</pre><p>
				The following fields apply to each module:
			</p><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO34-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						<code class="literal">VerificationStatus</code> - <code class="literal">true</code> or <code class="literal">false</code>, validated or not.
					</div></dd><dt><a href="#CO34-2"><span class="callout">2</span></a> </dt><dd><div class="para">
						<code class="literal">StatusReason</code> - Verbal explanation regarding the status.
					</div></dd><dt><a href="#CO34-3"><span class="callout">3</span></a> </dt><dd><div class="para">
						<code class="literal">VerificationStage</code> - Describes the validation stage being executed (Image, Build, Sign).
					</div></dd><dt><a href="#CO34-4"><span class="callout">4</span></a> </dt><dd><div class="para">
						<code class="literal">LastTransitionTime</code> - The time of the last update to the status.
					</div></dd></dl></div></section><section class="section" id="kmm-preflight-validation-stages-per-module_kmm-preflight-validation"><div class="titlepage"><div><div><h2 class="title">15.4. Preflight validation stages per Module</h2></div></div></div><p>
				Preflight runs the following validations on every KMM Module present in the cluster:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Image validation stage
					</li><li class="listitem">
						Build validation stage
					</li><li class="listitem">
						Sign validation stage
					</li></ol></div><section class="section" id="kmm-image-validation-stage_kmm-preflight-validation"><div class="titlepage"><div><div><h3 class="title">15.4.1. Image validation stage</h3></div></div></div><p>
					Image validation is always the first stage of the preflight validation to be executed. If image validation is successful, no other validations are run on that specific module.
				</p><p>
					Image validation consists of two stages:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Image existence and accessibility. The code tries to access the image defined for the upgraded kernel in the module and get its manifests.
						</li><li class="listitem">
							Verify the presence of the kernel module defined in the <code class="literal">Module</code> in the correct path for future <code class="literal">modprobe</code> execution. The correct path is <code class="literal">&lt;dirname&gt;/lib/modules/&lt;upgraded_kernel&gt;/</code>.
						</li></ol></div><p>
					If this validation is successful, it probably means that the kernel module was compiled with the correct Linux headers.
				</p></section><section class="section" id="kmm-build-validation-stage_kmm-preflight-validation"><div class="titlepage"><div><div><h3 class="title">15.4.2. Build validation stage</h3></div></div></div><p>
					Build validation is executed only when image validation has failed and there is a <code class="literal">build</code> section in the <code class="literal">Module</code> that is relevant for the upgraded kernel. Build validation attempts to run the build job and validate that it finishes successfully.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You must specify the kernel version when running <code class="literal">depmod</code>, as shown here:
					</p><pre class="programlisting language-terminal">$ RUN depmod -b /opt ${KERNEL_VERSION}</pre></div></div><p>
					If the <code class="literal">PushBuiltImage</code> flag is defined in the <code class="literal">PreflightValidationOCP</code> custom resource (CR), it will also try to push the resulting image into its repository. The resulting image name is taken from the definition of the <code class="literal">containerImage</code> field of the <code class="literal">Module</code> CR.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If the <code class="literal">sign</code> section is defined for the upgraded kernel, then the resulting image will not be the <code class="literal">containerImage</code> field of the <code class="literal">Module</code> CR, but a temporary image name, because the resulting image should be the product of Sign flow.
					</p></div></div></section><section class="section" id="kmm-sign-validation-stage_kmm-preflight-validation"><div class="titlepage"><div><div><h3 class="title">15.4.3. Sign validation stage</h3></div></div></div><p>
					Sign validation is executed only when image validation has failed, there is a <code class="literal">sign</code> section in the <code class="literal">Module</code> that is relevant for the upgrade kernel, and build validation finished successfully in the event there was a <code class="literal">build</code> section in the <code class="literal">Module</code> relevant for the upgraded kernel. Sign validation will try to run the sign job and validate that it finishes successfully.
				</p><p>
					If the <code class="literal">PushBuiltImage</code> flag is defined in the <code class="literal">PreflightValidationOCP</code> CR, sign validation will also try to push the resulting image to its registry.
				</p><p>
					The resulting image is always the image defined in the <code class="literal">containerImage</code> field of the <code class="literal">Module</code>. The input image is either the output of the Build stage, or an image defined in the <code class="literal">UnsignedImage</code> field.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If a <code class="literal">build</code> section exists, the <code class="literal">sign</code> section input image is the <code class="literal">build</code> section’s output image. Therefore, in order for the input image to be available for the <code class="literal">sign</code> section, the <code class="literal">PushBuiltImage</code> flag must be defined in the <code class="literal">PreflightValidationOCP</code> CR.
					</p></div></div></section></section><section class="section" id="kmm-example-cr_kmm-preflight-validation"><div class="titlepage"><div><div><h2 class="title">15.5. Example PreflightValidationOCP resource</h2></div></div></div><p>
				This section shows an example of the <code class="literal">PreflightValidationOCP</code> resource in the YAML format.
			</p><p>
				The example verifies all the currently present modules against the upcoming kernel version included in the OpenShift Container Platform release 4.11.18, which the following release image points to:
			</p><pre class="programlisting language-terminal">quay.io/openshift-release-dev/ocp-release@sha256:22e149142517dfccb47be828f012659b1ccf71d26620e6f62468c264a7ce7863</pre><p>
				Because <code class="literal">.spec.pushBuiltImage</code> is set to <code class="literal">true</code>, KMM pushes the resulting images of Build/Sign into the defined repositories.
			</p><pre class="programlisting language-yaml">apiVersion: kmm.sigs.x-k8s.io/v1beta1
kind: PreflightValidationOCP
metadata:
 name: preflight
spec:
 releaseImage: quay.io/openshift-release-dev/ocp-release@sha256:22e149142517dfccb47be828f012659b1ccf71d26620e6f62468c264a7ce7863
 pushBuiltImage: true</pre></section></section><section class="chapter" id="updating-hosted-control-planes"><div class="titlepage"><div><div><h1 class="title">Chapter 16. Updating hosted control planes</h1></div></div></div><p>
			On hosted control planes for OpenShift Container Platform, updates are decoupled between the control plane and the nodes. Your service cluster provider, which is the user that hosts the cluster control planes, can manage the updates as needed. The hosted cluster handles control plane updates, and node pools handle node upgrades.
		</p><section class="section" id="updates-for-hosted-control-planes_updating-hosted-control-planes"><div class="titlepage"><div><div><h2 class="title">16.1. Updates for hosted control planes</h2></div></div></div><p>
				Updates for hosted control planes involve updating the hosted cluster and the node pools. For a cluster to remain fully operational during an update process, you must meet the requirements of the <a class="link" href="https://kubernetes.io/releases/version-skew-policy/">Kubernetes version skew policy</a> while completing the control plane and node updates.
			</p><section class="section" id="updates-for-hosted-control-planes-hostedcluster_updating-hosted-control-planes"><div class="titlepage"><div><div><h3 class="title">16.1.1. Updates for the hosted cluster</h3></div></div></div><p>
					The <code class="literal">spec.release</code> value dictates the version of the control plane. The <code class="literal">HostedCluster</code> object transmits the intended <code class="literal">spec.release</code> value to the <code class="literal">HostedControlPlane.spec.release</code> value and runs the appropriate Control Plane Operator version.
				</p><p>
					The hosted control plane manages the rollout of the new version of the control plane components along with any OpenShift Container Platform components through the new version of the Cluster Version Operator (CVO).
				</p></section><section class="section" id="updates-for-hosted-control-planes-nodepools_updating-hosted-control-planes"><div class="titlepage"><div><div><h3 class="title">16.1.2. Updates for node pools</h3></div></div></div><p>
					With node pools, you can configure the software that is running in the nodes by exposing the <code class="literal">spec.release</code> and <code class="literal">spec.config</code> values. You can start a rolling node pool update in the following ways:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Changing the <code class="literal">spec.release</code> or <code class="literal">spec.config</code> values.
						</li><li class="listitem">
							Changing any platform-specific field, such as the AWS instance type. The result is a set of new instances with the new type.
						</li><li class="listitem">
							Changing the cluster configuration, if the change propagates to the node.
						</li></ul></div><p>
					Node pools support replace updates and in-place updates. The <code class="literal">nodepool.spec.release</code> value dictates the version of any particular node pool. A <code class="literal">NodePool</code> object completes a replace or an in-place rolling update according to the <code class="literal">.spec.management.upgradeType</code> value.
				</p><p>
					After you create a node pool, you cannot change the update type. If you want to change the update type, you must create a node pool and delete the other one.
				</p><section class="section" id="updates-for-nodepools-replace_updating-hosted-control-planes"><div class="titlepage"><div><div><h4 class="title">16.1.2.1. Replace updates for node pools</h4></div></div></div><p>
						A <span class="emphasis"><em>replace</em></span> update creates instances in the new version while it removes old instances from the previous version. This update type is effective in cloud environments where this level of immutability is cost effective.
					</p><p>
						Replace updates do not preserve any manual changes because the node is entirely re-provisioned.
					</p></section><section class="section" id="updates-for-nodepools-inplace_updating-hosted-control-planes"><div class="titlepage"><div><div><h4 class="title">16.1.2.2. In place updates for node pools</h4></div></div></div><p>
						An <span class="emphasis"><em>in-place</em></span> update directly updates the operating systems of the instances. This type is suitable for environments where the infrastructure constraints are higher, such as bare metal.
					</p><p>
						In-place updates can preserve manual changes, but will report errors if you make manual changes to any file system or operating system configuration that the cluster directly manages, such as kubelet certificates.
					</p></section></section></section><section class="section" id="updating-node-pools-for-hcp_updating-hosted-control-planes"><div class="titlepage"><div><div><h2 class="title">16.2. Updating node pools for hosted control planes</h2></div></div></div><p>
				On hosted control planes, you update your version of OpenShift Container Platform by updating the node pools. The node pool version must not surpass the hosted control plane version.
			</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						To start the process to update to a new version of OpenShift Container Platform, change the <code class="literal">spec.release.image</code> value of the node pool by entering the following command:
					</p><pre class="programlisting language-terminal">$ oc -n NAMESPACE patch HC HCNAME --patch '{"spec":{"release":{"image": "example"}}}' --type=merge</pre></li></ul></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						To verify that the new version was rolled out, check the <code class="literal">.status.version</code> value and the status conditions.
					</li></ul></div></section><section class="section" id="configuring-node-pools-for-hcp_updating-hosted-control-planes"><div class="titlepage"><div><div><h2 class="title">16.3. Configuring node pools for hosted control planes</h2></div></div></div><p>
				On hosted control planes, you can configure node pools by creating a <code class="literal">MachineConfig</code> object inside of a config map in the management cluster.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To create a <code class="literal">MachineConfig</code> object inside of a config map in the management cluster, enter the following information:
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: &lt;configmap-name&gt;
  namespace: clusters
data:
  config: |
    apiVersion: machineconfiguration.openshift.io/v1
    kind: MachineConfig
    metadata:
      labels:
        machineconfiguration.openshift.io/role: worker
      name: &lt;machineconfig-name&gt;
    spec:
      config:
        ignition:
          version: 3.2.0
        storage:
          files:
          - contents:
              source: data:...
            mode: 420
            overwrite: true
            path: ${PATH} <span id="CO35-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO35-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Sets the path on the node where the <code class="literal">MachineConfig</code> object is stored.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						After you add the object to the config map, you can apply the config map to the node pool as follows:
					</p><pre class="programlisting language-yaml">spec:
  config:
    - name: ${CONFIGMAP_NAME}</pre></li></ol></div></section></section><div><div xml:lang="en-US" class="legalnotice" id="idm140049057122752"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"><!--Empty--></span>© 2023 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div>


  <nav class="pvof-doc__book-nav">
  <ol class="book-nav__list">
              </ol>
</nav>


          </div>
              </div>
              <div id="comments-footer" class="book-comments">
          

  

        </div>
          </div>
  </article>
<meta itemscope="" itemref="md1">



    </div>
      <!-- CP_PRIMER_FOOTER -->            </div>
        </main>
    </div>
    <!--googleoff: all-->
    <div id="to-top"><a class="btn_slideto" href="#masthead" aria-label="Back to Top"><span class="web-icon-upload"></span></a></div>
    <footer class="footer-main">
        <div class="footer-top">
            <div class="container">

              <div class="brand">
                <a href="https://redhat.com">
                  <svg class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 613 145">
                    <defs>
                      <style>
                        .rh-logo-hat {
                          fill: #e00;
                        }
                        .rh-logo-type {
                          fill: #fff;
                        }
                      </style>
                    </defs>
                    <title>Red Hat</title>
                    <path
                      class="rh-logo-hat"
                      d="M127.47,83.49c12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42l-7.45-32.36c-1.72-7.12-3.23-10.35-15.73-16.6C124.89,8.69,103.76.5,97.51.5,91.69.5,90,8,83.06,8c-6.68,0-11.64-5.6-17.89-5.6-6,0-9.91,4.09-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33C22.27,52,.5,55,.5,74.22c0,31.48,74.59,70.28,133.65,70.28,45.28,0,56.7-20.48,56.7-36.65,0-12.72-11-27.16-30.83-35.78"/>
                      <path class="rh-logo-band"
                      d="M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45,12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/>
                      <path
                      class="rh-logo-type"
                      d="M579.74,92.8c0,11.89,7.15,17.67,20.19,17.67a52.11,52.11,0,0,0,11.89-1.68V95a24.84,24.84,0,0,1-7.68,1.16c-5.37,0-7.36-1.68-7.36-6.73V68.3h15.56V54.1H596.78v-18l-17,3.68V54.1H568.49V68.3h11.25Zm-53,.32c0-3.68,3.69-5.47,9.26-5.47a43.12,43.12,0,0,1,10.1,1.26v7.15a21.51,21.51,0,0,1-10.63,2.63c-5.46,0-8.73-2.1-8.73-5.57m5.2,17.56c6,0,10.84-1.26,15.36-4.31v3.37h16.82V74.08c0-13.56-9.14-21-24.39-21-8.52,0-16.94,2-26,6.1l6.1,12.52c6.52-2.74,12-4.42,16.83-4.42,7,0,10.62,2.73,10.62,8.31v2.73a49.53,49.53,0,0,0-12.62-1.58c-14.31,0-22.93,6-22.93,16.73,0,9.78,7.78,17.24,20.19,17.24m-92.44-.94h18.09V80.92h30.29v28.82H506V36.12H487.93V64.41H457.64V36.12H439.55ZM370.62,81.87c0-8,6.31-14.1,14.62-14.1A17.22,17.22,0,0,1,397,72.09V91.54A16.36,16.36,0,0,1,385.24,96c-8.2,0-14.62-6.1-14.62-14.09m26.61,27.87h16.83V32.44l-17,3.68V57.05a28.3,28.3,0,0,0-14.2-3.68c-16.19,0-28.92,12.51-28.92,28.5a28.25,28.25,0,0,0,28.4,28.6,25.12,25.12,0,0,0,14.93-4.83ZM320,67c5.36,0,9.88,3.47,11.67,8.83H308.47C310.15,70.3,314.36,67,320,67M291.33,82c0,16.2,13.25,28.82,30.28,28.82,9.36,0,16.2-2.53,23.25-8.42l-11.26-10c-2.63,2.74-6.52,4.21-11.14,4.21a14.39,14.39,0,0,1-13.68-8.83h39.65V83.55c0-17.67-11.88-30.39-28.08-30.39a28.57,28.57,0,0,0-29,28.81M262,51.58c6,0,9.36,3.78,9.36,8.31S268,68.2,262,68.2H244.11V51.58Zm-36,58.16h18.09V82.92h13.77l13.89,26.82H292l-16.2-29.45a22.27,22.27,0,0,0,13.88-20.72c0-13.25-10.41-23.45-26-23.45H226Z"/>
                  </svg>
                </a>
              </div>

              <div role="navigation" aria-label="quick">
                  <h3>Quick Links</h3>
                  <ul>
                      <li><a class="download-software" href="https://access.redhat.com/downloads/">Downloads</a></li>
                      <li><a class="manage-subscriptions" href="https://access.redhat.com/management">Subscriptions</a></li>
                      <li><a class="support-cases" href="https://access.redhat.com/support">Support Cases</a></li>
                      <li><a class="customer-service" href="https://access.redhat.com/support/customer-service">Customer Service</a></li>
                      <li><a class="quick-docs" href="https://access.redhat.com/documentation">Product Documentation</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="help">
                  <h3>Help</h3>
                  <ul>
                      <li><a class="contact-us" href="https://access.redhat.com/support/contact/">Contact Us</a></li>
                      <li><a class="cp-faqs" href="https://access.redhat.com/articles/33844">Customer Portal FAQ</a></li>
                      <li><a class="login-problems" href="https://access.redhat.com/help/login_assistance">Log-in Assistance</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="site">
                  <h3>Site Info</h3>
                  <ul>
                      <li><a class="trust-red-hat" href="https://www.redhat.com/en/trust">Trust Red Hat</a></li>
                      <li><a class="browser-support-policy" href="https://www.redhat.com/en/about/browser-support">Browser Support Policy</a></li>
                      <li><a class="accessibility" href="https://www.redhat.com/en/about/digital-accessibility">Accessibility</a></li>
                      <li><a class="recognition" href="https://access.redhat.com/recognition/">Awards and Recognition</a></li>
                      <li><a class="colophon" href="https://access.redhat.com/help/colophon/">Colophon</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="other">
                  <h3>Related Sites</h3>
                  <ul>
                      <li><a href="https://www.redhat.com/" class="red-hat-com">redhat.com</a></li>
                      <li><a href="http://developers.redhat.com/" class="red-hat-developers">developers.redhat.com</a></li>
                      <li><a href="https://connect.redhat.com/" class="partner-connect">connect.redhat.com</a></li>
                      <li><a href="https://cloud.redhat.com/" class="cloud-com">cloud.redhat.com</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="about">
                  <h3>About</h3>
                  <ul>
                      <li><a href="https://access.redhat.com/subscription-value" class="subscription-value">Red Hat Subscription Value</a></li>
                      <li><a href="https://www.redhat.com/about/" class="about-red-hat">About Red Hat</a></li>
                      <li><a href="http://jobs.redhat.com" class="about-jobs">Red Hat Jobs</a></li>
                  </ul>
              </div>

            </div>
        </div>

        <div class="anchor">
            <div class="container">
                <div class="status-legal">
                    <a hidden href="https://status.redhat.com" class="status-page-widget">
                        <span class="status-description"></span>
                        <span class="status-dot shape-circle"></span>
                    </a>
                    <div class="legal-copyright">
                        <div class="copyright">Copyright © 2023 Red Hat, Inc.</div>
                        <div role="navigation" aria-label="legal" class="legal">
                            <ul>
                                <li><a href="http://www.redhat.com/en/about/privacy-policy" class="privacy-policy">Privacy Statement</a></li>
                                <li><a href="https://www.redhat.com/en/about/terms-use" class="terms-of-use">Terms of Use</a></li>
                                <li><a href="http://www.redhat.com/en/about/all-policies-guidelines" class="all-policies">All Policies and Guidelines</a></li>
                                <li><a id="teconsent"></a></li>
                            </ul>
                            <div id="privacy_policy">We've updated our <a href='http://www.redhat.com/en/about/privacy-policy' class='privacy-policy'>Privacy Statement</a> effective September 15, 2023.
                            </div>
                          </div>
                        </div>
                </div>
                <div class="social">
                    <a href="http://www.redhat.com/summit/" class="summit">
                        <img src="https://access.redhat.com/chrome_themes/nimbus/img/rh-summit-red-a.svg" alt="Red Hat Summit" /> <span class="offscreen">Red Hat Summit</span>
                    </a>

                    <div class="social-media">
                        <a href="https://twitter.com/RedHat" class="sm-icon twitter"><span class="nicon-twitter"></span><span class="offscreen">Twitter</span></a>                        
                    </div>
                </div>
            </div>
        </div>
    </footer>
    <!-- TrustArc -->
    <div id="consent_blackbar"></div> 
    <!--googleon: all-->
</div>
<!-- /CP_PRIMER_FOOTER -->


  </div>

    
  </body>
</html>
