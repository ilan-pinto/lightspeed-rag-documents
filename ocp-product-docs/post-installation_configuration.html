<!DOCTYPE html>
<html lang="en" dir="ltr" prefix="og: https://ogp.me/ns#">
  <head>
    <meta charset="utf-8" />
<link rel="canonical" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/index" />
<link rel="shortlink" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/index" />
<meta property="og:site_name" content="Red Hat Customer Portal" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/index" />
<meta property="og:title" content="Post-installation configuration OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<meta property="og:description" content="This document provides instructions and guidance on post installation activities for OpenShift Container Platform." />
<meta property="og:image" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:description" content="This document provides instructions and guidance on post installation activities for OpenShift Container Platform." />
<meta name="twitter:title" content="Post-installation configuration OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<meta name="twitter:url" content="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/index" />
<meta name="twitter:image" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
<meta name="title" content="Post-installation configuration OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<link rel="alternate" hreflang="en" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/index" />
<link rel="alternate" hreflang="ko" href="https://access.redhat.com/documentation/ko-kr/openshift_container_platform/4.13/html-single/post-installation_configuration/index" />
<link rel="alternate" hreflang="zh-hans" href="https://access.redhat.com/documentation/zh-cn/openshift_container_platform/4.13/html-single/post-installation_configuration/index" />
<link rel="alternate" hreflang="ja" href="https://access.redhat.com/documentation/ja-jp/openshift_container_platform/4.13/html-single/post-installation_configuration/index" />
<link rel="alternate" hreflang="es" href="https://access.redhat.com/documentation/es-es/openshift_container_platform/4.13/html-single/post-installation_configuration/index" />
<link rel="alternate" hreflang="ru" href="https://access.redhat.com/documentation/ru-ru/openshift_container_platform/4.13/html-single/post-installation_configuration/index" />
<link rel="alternate" hreflang="pt-br" href="https://access.redhat.com/documentation/pt-br/openshift_container_platform/4.13/html-single/post-installation_configuration/index" />
<link rel="alternate" hreflang="it" href="https://access.redhat.com/documentation/it-it/openshift_container_platform/4.13/html-single/post-installation_configuration/index" />
<link rel="alternate" hreflang="de" href="https://access.redhat.com/documentation/de-de/openshift_container_platform/4.13/html-single/post-installation_configuration/index" />
<link rel="alternate" hreflang="fr" href="https://access.redhat.com/documentation/fr-fr/openshift_container_platform/4.13/html-single/post-installation_configuration/index" />
<link rel="alternate" hreflang="zh-hant" href="https://access.redhat.com/documentation/zh-tw/openshift_container_platform/4.13/html-single/post-installation_configuration/index" />
<link rel="alternate" hreflang="id" href="https://access.redhat.com/documentation/id-id/openshift_container_platform/4.13/html-single/post-installation_configuration/index" />
<link rel="alternate" hreflang="th" href="https://access.redhat.com/documentation/th-th/openshift_container_platform/4.13/html-single/post-installation_configuration/index" />
<link rel="alternate" hreflang="vi" href="https://access.redhat.com/documentation/vi-vn/openshift_container_platform/4.13/html-single/post-installation_configuration/index" />
<meta name="Generator" content="Drupal 9 (https://www.drupal.org)" />
<meta name="MobileOptimized" content="width" />
<meta name="HandheldFriendly" content="true" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="revision" product="b0738f19-59ac-47eb-9512-8a439cd6dfb0" title="3896fe67-0dea-452e-b3a1-f9fd4af28885" page="2af193c6-c99a-4a5d-b44b-5d8a19cc9c7b" revision="dccd48b1f252cc22dad43e9ae6252b9287d98ca1:en-us" body="db1cd7baf81f47fedc4cab3f63227d0b.html" toc="7ad44a0b2aa7aa9ea6c553021a5bfcf2.json" />

    <title>Post-installation configuration OpenShift Container Platform 4.13 | Red Hat Customer Portal</title>
    <link rel="stylesheet" media="all" href="/sites/dxp-docs/files/css/css_87GMcmxT1ib8ziQiU2KUAnTDFtZQV6iP-KGslA9LigM.css" />
<link rel="stylesheet" media="all" href="/sites/dxp-docs/files/css/css__Xq4GfgPDJw9K_yYJFmlRZGJeCENu3R3r4s0K7Tr_9g.css" />

    
    <script type="application/json" data-drupal-selector="drupal-settings-json">{"path":{"baseUrl":"\/","scriptPath":null,"pathPrefix":"","currentPath":"documentation\/en-us\/openshift_container_platform\/4.13\/html-single\/post-installation_configuration\/index","currentPathIsAdmin":false,"isFront":false,"currentLanguage":"en"},"pluralDelimiter":"\u0003","suppressDeprecationErrors":true,"red_hat_jwt":{"client_id":"customer-portal","cookie_name":"rh_jwt","leeway":"0","realm":"redhat-external","sso_host":"https:\/\/sso.redhat.com\/","user_integration":1,"user_plugin":"drupal_user_auth","use_external_js":0,"use_internal_js":0,"use_in_admin":0},"user":{"uid":0,"permissionsHash":"d8ea0bce2d740dacbdfe0257cf55baa0e33f7fb8468a26d055ce75daaaa2d315"}}</script>
<script src="/sites/dxp-docs/files/js/js_EQWKo9EokWkWS99x_e1oM-NEM0zlKyTkp_83mGdm5Ks.js"></script>

    <!-- CP_PRIMER_HEAD -->  <!-- TrustArc & DTM -->
  <script src="//static.redhat.com/libs/redhat/marketing/latest/trustarc/trustarc.js"></script>
  <script src="//www.redhat.com/dtm.js"></script><meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<!--[if IEMobile]><meta http-equiv="cleartype" content="on"><![endif]-->

<!-- metaInclude -->
<meta name="avalon-host-info" content="dxp-kbase-prod-139-77b4fb8768-25dr9" />
<meta name="avalon-version" content="27861f77" />
<meta name="cp-chrome-build-date" content="2023-10-06T19:17:59.039Z" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<!-- Chrome, Firefox OS and Opera -->
<meta name="theme-color" content="#000000">
<!-- Windows Phone -->
<meta name="msapplication-navbutton-color" content="#000000">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-status-bar-style" content="#000000">
<link rel="manifest" href="https://access.redhat.com/webassets/avalon/j/manifest.json">
<!-- Open Search - Tap to search -->
<link rel="search" type="application/opensearchdescription+xml" title="Red Hat Customer Portal" href="https://access.redhat.com/webassets/avalon/j/opensearch.xml" />
<!-- title -->
<title>Red Hat Customer Portal - Access to 24x7 support and knowledge</title>
<!-- /title -->
<script type="text/javascript">
    window.portal = {
        analytics : {},
        host      : "https://access.redhat.com",
        idp_url   : "https://sso.redhat.com",
        lang      : "en", 
        version   : "27861f77",
        builddate : "2023-10-06T19:17:59.039Z",        fetchdate : "2023-10-10T17:45:08-0400",        nrid      : "NOLONGERSUPPORTED",
        nrlk      : "NOLONGERSUPPORTED"
    };
</script>
<script type="text/javascript">
    if (!/\/logout.*/.test(location.pathname) && portal.host === location.origin && document.cookie.indexOf('rh_sso_session') >= 0 && !(document.cookie.indexOf('rh_jwt') >= 0)) window.location = '/login?redirectTo=' + encodeURIComponent(window.location.href);
</script>
<!-- cssInclude -->

<link rel="shortcut icon" href="https://access.redhat.com/webassets/avalon/g/favicon.ico" /><link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/bootstrap.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/bootstrap-grid.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/main.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/components.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/pages.css?v=27861f77" />

<link href="https://access.redhat.com/webassets/avalon/s/chosen.css?v=27861f77" rel="stylesheet" type="text/css" />

<!--[if lte IE 9]>
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/ie.css" />
<![endif]--><noscript>
    <style type="text/css" media="screen"> .primary-nav { display: block; } </style>
</noscript>
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/webassets/avalon/j/public_modules/node_modules/@cpelements/pfe-navigation/dist/pfe-navigation--lightdom.min.css" />
<!-- /cssInclude -->
<script src="https://access.redhat.com/webassets/avalon/j/public_modules/node_modules/@cpelements/pfe-navigation/dist/ie-polyfills.js?v=27861f77"></script>

<script async>
  if (!HTMLScriptElement.supports || !HTMLScriptElement.supports('importmap')) {
    import("https://www.redhatstatic.com/dx/v1-alpha/es-module-shims@1.7.3.js");
  }
</script>
<script type="importmap">
{
  "imports": {
    "@patternfly/elements/" : "https://www.redhatstatic.com/dx/v1-alpha/@patternfly/elements@2.2.2/",
    "@rhds/elements/":"https://www.redhatstatic.com/dx/v1-alpha/@rhds/elements@1.1.0/elements/",
    "@rhds/elements/lib/":"https://www.redhatstatic.com/dx/v1-alpha/@rhds/elements@1.1.0/lib/",
    "@cpelements/elements/":"https://www.redhatstatic.com/dx/v1-alpha/@cpelements/elements@2.0.0-alpha.7/elements/"
  }
}
</script><script type="text/javascript" src="https://access.redhat.com/webassets/avalon/j/lib/require.js?v=27861f77" data-main="/webassets/avalon/j/"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<script src="https://access.redhat.com/chrome_themes/nimbus/js/ie8.js"></script>
<![endif]-->
<script type="text/javascript" src="https://access.redhat.com/chrome_themes/nimbus/js/new-nav.js?v=27861f77" ></script>
<!-- /CP_PRIMER_HEAD -->

  </head>
  <body>
    
      <div class="dialog-off-canvas-main-canvas" data-off-canvas-main-canvas>
      <!-- CP_PRIMER_HEADER -->
<div id="page-wrap" class="page-wrap">
    <div id="pers-top-page-wrap" class="top-page-wrap pers-loader-bg">

      <div id="hero-bg-top-left" class="summit-bg-shapes"></div>
      <div id="hero-bg-top-right" class="summit-bg-shapes"></div>

        <!--googleoff: all-->
        <header class="masthead" id="masthead">

            <a href="#pfe-navigation" id="global-skip-to-nav" class="skip-link visually-hidden">Skip to navigation</a>
            <a href="#cp-main" class="skip-link visually-hidden">Skip to main content</a>            <nav id="portal-utility-nav" class="utility-navigation utility-navigation--bar hidden-at-mobile" data-analytics-region="utility" aria-labelledby="nav__utility-nav--desktop">
                <h3 id="nav__utility-nav--desktop" class="element-invisible">Utilities
                </h3>
                <ul aria-labelledby="nav__utility-nav--desktop">
                    <li id="nav-subscription" data-portal-tour-1="1">
                        <a class="top-nav-subscriptions" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Subscription" href="https://access.redhat.com/management/" >Subscriptions
                        </a>
                    </li>
                    <li id="nav-downloads" data-portal-tour-1="2">
                        <a class="top-nav-downloads" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Downloads" href="https://access.redhat.com/downloads/" >Downloads
                        </a>
                    </li>
                    <li id="nav-containers">
                        <a class="top-nav-containers" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Containers" href="https://catalog.redhat.com/software/containers/explore/" >Containers
                        </a>
                    </li>
                    <li id="nav-support" data-portal-tour-1="3">
                        <a class="top-nav-support-cases" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Support Cases" href="https://access.redhat.com/support/cases/" >Support Cases
                        </a>
                    </li>
                </ul>
            </nav>

            <pfe-navigation id="pfe-navigation" data-analytics-region="mega menu">
                <div class="pfe-navigation__logo-wrapper" id="pfe-navigation__logo-wrapper">
                    <a href="https://access.redhat.com/" class="pfe-navigation__logo-link" data-analytics-text="logo" data-analytics-category="MM|logo">
                        <img class="pfe-navigation__logo-image" alt="Red Hat Customer Portal" src="https://access.redhat.com/chrome_themes/nimbus/img/red-hat-customer-portal.svg" />
                    </a>
                </div>

                <nav class="pfe-navigation" aria-label="Main Navigation" data-analytics-region="main nav">
                    <ul class="pfe-navigation__menu" id="pfe-navigation__menu">                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-subscription--mobile" data-portal-tour-1="1">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Subscription" href="https://access.redhat.com/management/" >Subscriptions
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-downloads--mobile" data-portal-tour-1="2">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Downloads" href="https://access.redhat.com/downloads/" >Downloads
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-containers--mobile">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Containers" href="https://catalog.redhat.com/software/containers/explore/" >Containers
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-support--mobile" data-portal-tour-1="3">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Support Cases" href="https://access.redhat.com/support/cases/" >Support Cases
                            </a>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a href="https://access.redhat.com/products/" class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Products and Services">Products &amp; Services
                            </a>
                            <div class="pfe-navigation__dropdown has-primary-detail">                                <div class="desktop-col-span-2 tablet-col-span-all">
                                    <h3>
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Products" href="https://access.redhat.com/products/">Products
                                        </a>
                                    </h3>
                                    <slot name="main-menu__dropdown--product__product-listing"></slot>
                                </div>                                <div>
                                    <h3 id="nav__products__support">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Support" href="https://access.redhat.com/support">Support
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__support">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Production Support" href="https://access.redhat.com/support/offerings/production/">Production Support
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Development Support" href="https://access.redhat.com/support/offerings/developer/">Development Support
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Product Life Cycles" href="https://access.redhat.com/product-life-cycles/">Product Life Cycles
                                                    </a></li>
                                    </ul>

                                    <h3 id="nav__products__services">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Services" href="https://www.redhat.com/en/services">Services
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__services">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Consulting" href="https://www.redhat.com/en/services/consulting">Consulting
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Technical Account Management" href="https://access.redhat.com/support/offerings/tam/">Technical Account Management
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Training and Certifications" href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications
                                                    </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__products__documentation">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Documentation" href="https://access.redhat.com/documentation">Documentation
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__documentation">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat Enterprise Linux" href="https://access.redhat.com/documentation/en/red_hat_enterprise_linux">Red Hat Enterprise Linux
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat JBoss Enterprise Application Platform" href="https://access.redhat.com/documentation/en/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat OpenStack Platform" href="https://access.redhat.com/documentation/en/red_hat_openstack_platform">Red Hat OpenStack Platform
                                                    </a></li>
                                                    <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat OpenShift Container Platform" href="https://access.redhat.com/documentation/en/openshift_container_platform">Red Hat OpenShift Container Platform
                                                        </a></li>
                                    </ul>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="All Documentation" data-analytics-linkType="cta" href="https://access.redhat.com/documentation">All Documentation
                                        </a>
                                    </pfe-cta>

                                    <h3 id="nav__products__catalog"><a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Ecosystem Catalog" href="https://catalog.redhat.com/">Ecosystem Catalog
                                        </a></h3>
                                        <ul aria-labelledby="nav__products__catalog">
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Ecosystem Catalog" data-analytics-text="Red Hat Partner Ecosystem" href="https://access.redhat.com/ecosystem/">Red Hat Partner Ecosystem
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Ecosystem Catalog" data-analytics-text="Partner Resources" href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources
                                                    </a></li>
                                        </ul>
                                </div>
                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Tools" href="https://access.redhat.com/labs/">Tools
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="nav__tools__tools" data-analytics-level="2" data-analytics-text="Tools" data-analytics-category="Tools">Tools
                                    </h3>
                                    <ul aria-labelledby="nav__tools__tools">
                                        <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Solution Engine" href="https://access.redhat.com/support/cases/#/troubleshoot">Troubleshoot a product issue
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Packages" href="https://access.redhat.com/downloads/content/package-browser">Packages
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Errata" href="https://access.redhat.com/errata/">Errata
                                                    </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__tools__labs">
                                        <a data-analytics-level="2" data-analytics-category="Tools" data-analytics-text="Customer Portal Labs" href="https://access.redhat.com/labs/">Customer Portal Labs
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__tools__labs">
                                        <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Configuration" href="https://access.redhat.com/labs/#!?type=config">Configuration
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Deployment" href="https://access.redhat.com/labs/#!?type=deploy">Deployment
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Security" href="https://access.redhat.com/labs/#!?type=security">Security
                                                    </a></li>                                                    <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Troubleshooting" href="https://access.redhat.com/labs/#!?type=troubleshoot">Troubleshoot
                                                        </a></li>
                                    </ul>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="All Labs" data-analytics-linkType="cta" href="https://access.redhat.com/labs/">All labs
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h4 id="nav__tools__red-hat-insights">
                                        <a data-analytics-level="2" data-analytics-category="Tools" data-analytics-text="Red Hat Insights" href="//www.redhat.com/en/technologies/management/insights">Red Hat Insights
                                        </a>
                                    </h4>
                                    <p>Increase visibility into IT operations to detect and resolve technical issues before they impact your business.</p>
                                    <a data-analytics-level="3" data-analytics-category="Tools|Red Hat Insights" data-analytics-text="Learn more" href="https://www.redhat.com/en/technologies/management/insights">Learn More
                                    </a>
                                    <br>
                                    <a data-analytics-level="3" data-analytics-category="Tools|Red Hat Insights" data-analytics-text="Go to Insights" href="https://cloud.redhat.com/insights">Go to Insights
                                    </a>
                                </div>
                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Security" href="https://access.redhat.com/security/">Security
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="security__security-center">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Red Hat Product Security Center" href="https://access.redhat.com/security">Red Hat Product Security Center
                                        </a>
                                    </h3>
                                    <p>Engage with our Red Hat Product Security team, access security updates, and ensure your environments are not exposed to any known security vulnerabilities.
                                    </p>
                                    <pfe-cta pfe-priority="primary">
                                        <a data-analytics-level="3" data-analytics-category="Security|Red Hat Product Security Center" data-analytics-text="Product Security Center" data-analytics-linkType="cta" href="https://access.redhat.com/security/">Product Security Center
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__security__updates" data-analytics-level="2" data-analytics-text="Security Updates" data-analytics-category="Security">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Security Updates" href="/security">Security Updates
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__security__updates">
                                        <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Security Advisories" href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Red Hat CVE Database" href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Security Labs" href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs
                                                    </a></li>
                                    </ul>
                                    <p class="margin-top-xl">Keep your systems secure with Red Hat&#039;s specialized responses to security vulnerabilities.
                                    </p>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="View Responses" data-analytics-linkType="cta" href="https://access.redhat.com/security/vulnerability">View Responses
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__security__resources">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Resources" href="https://access.redhat.com/security/overview">Resources
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__security__resources">                                            <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Security Blog" href="//redhat.com/en/blog/channel/security">Security Blog
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Security Measurement" href="https://www.redhat.com/security/data/metrics/">Security Measurement
                                                    </a></li>
                                                    <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Severity Ratings" href="https://access.redhat.com/security/updates/classification/">Severity Ratings
                                                        </a></li>
                                                        <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Backporting Policies" href="https://access.redhat.com/security/updates/backporting/">Backporting Policies
                                                            </a></li>
                                                            <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Product Signing (GPG) Keys" href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys
                                                                </a></li>
                                    </ul>
                                </div>

                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a href="https://access.redhat.com/community/" class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Community">Community
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="nav__community__cp-community">
                                        <a href="https://access.redhat.com/community" data-analytics-level="2" data-analytics-text="Customer Portal Community" data-analytics-text="Customer Portal Community" data-analytics-category="Community">Customer Portal Community
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__community__cp-community">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Discussions" href="https://access.redhat.com/discussions">Discussions
                                            </a></li>                                                <li><a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Private Groups" href="https://access.redhat.com/groups/">Private Groups
                                                    </a></li>
                                    </ul>

                                    <pfe-cta pfe-priority="primary">
                                        <a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Community Activity" data-analytics-linkType="cta" href="https://access.redhat.com/community/">Community Activity
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__community__events" data-analytics-level="2" data-analytics-text="Customer Events" data-analytics-category="Community">Customer Events
                                    </h3>
                                    <ul aria-labelledby="nav__community__events">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Customer Events" data-analytics-text="Red Hat Convergence" href="https://access.redhat.com/convergence/">Red Hat Convergence
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Community|Customer Events" data-analytics-text="Red Hat Summit" href="http://www.redhat.com/summit/">Red Hat Summit
                                                </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__community__stories" data-analytics-level="2" data-analytics-text="Stories" data-analytics-category="Community">Stories
                                    </h3>
                                    <ul aria-labelledby="nav__community__stories">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Stories" data-analytics-text="Red Hat Subscription Value" href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-text="You Asked. We Acted." data-analytics-category="Community|Stories" href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Community|Stories" data-analytics-text="Open Source Communities" href="http://www.redhat.com/en/open-source">Open Source Communities
                                                    </a></li>
                                    </ul>
                                </div>
                            </div>
                        </li>
                    </ul>                </nav>                <div id="site-search" slot="search" class="utility-link site-search">
                    <div class="content">
                        <form class="ng-pristine ng-valid topSearchForm" id="topSearchForm" name="topSearchForm" action="/search/browse/search/" method="get" enctype="application/x-www-form-urlencoded">
                            <cp-search-autocomplete class="push-bottom" path="/webassets/avalon/j/data.json"></cp-search-autocomplete>                            <div>Or <a href="/support/cases/#/troubleshoot">troubleshoot an issue</a>.
                            </div>
                        </form>
                    </div>
                </div>


                <div slot="secondary-links" id="localesMenu">
                    <button class="pfe-navigation__secondary-link">
                        <pfe-icon icon="web-icon-globe" size="sm" aria-hidden="true"></pfe-icon>English
                    </button>

                    <pfe-navigation-dropdown dropdown-width="single">
                        <h2 class="utility-header">Select Your Language
                        </h2>
                        <ul class="reset">
                            <li><a href="https://access.redhat.com/changeLanguage?language=en" data-lang="en" id="en" data-analytics-text="English">English</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=ko" data-lang="ko" id="ko" data-analytics-text="Korean">한국어</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=ja"    data-lang="ja"    id="ja" data-analytics-text="Japanese">日本語</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" data-lang="zh_CN" id="zh_CN" data-analytics-text="Chinese">中文 (中国)</a></li>
                        </ul>

                    </pfe-navigation-dropdown>
                </div>                <rh-account-dropdown slot="account"></rh-account-dropdown>                <pfe-primary-detail breakpoint-width="600" class="main-menu__dropdown--product__product-listing" slot="main-menu__dropdown--product__product-listing" consistent-height>
                    <h3 slot="details-nav">Infrastructure and Management                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Enterprise Linux" href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Satellite" href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Subscription Management" href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Insights" href="https://access.redhat.com/products/red-hat-insights/">Red Hat Insights
                                </a>
                            </li>
                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Ansible Automation Platform" href="https://access.redhat.com/products/red-hat-ansible-automation-platform/">Red Hat Ansible Automation Platform
                                </a></li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Cloud Computing                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift" href="https://access.redhat.com/products/openshift">Red Hat OpenShift
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenStack Platform" href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat OpenShift Container Platform" href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat OpenShift Data Science" href="https://access.redhat.com/products/red-hat-openshift-data-science/">Red Hat OpenShift Data Science
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift Dedicated" href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat Advanced Cluster Security for Kubernetes" href="https://access.redhat.com/products/red-hat-advanced-cluster-security-for-kubernetes/">Red Hat Advanced Cluster Security for Kubernetes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat Advanced Cluster Management for Kubernetes" href="https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes/">Red Hat Advanced Cluster Management for Kubernetes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat Quay" href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat CodeReady Workspaces" href="https://access.redhat.com/products/red-hat-codeready-workspaces/">OpenShift Dev Spaces
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift Service on AWS" href="https://access.redhat.com/products/red-hat-openshift-service-aws">Red Hat OpenShift Service on AWS
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Storage                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Gluster Storage" href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Hyperconverged Infrastructure" href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Ceph Storage" href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Openshift Container Storage" href="https://access.redhat.com/products/red-hat-openshift-data-foundation">Red Hat OpenShift Data Foundation
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Runtimes                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Runtimes" href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat JBoss Enterprise Application Platform" href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Data Grid" href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat JBoss Web Server" href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Single Sign On" href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat support for Spring Boot" href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat build of Node.js" href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js
                                </a>
                            </li>                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat build of Quarkus" href="https://access.redhat.com/products/quarkus/">Red Hat build of Quarkus
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Integration and Automation                    </h3>
                    <div slot="details">
                        <ul class="border-bottom" id="portal-menu-border-bottom">
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat Application Foundations" href="https://access.redhat.com/products/red-hat-application-foundations/">Red Hat Application Foundations
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat Fuse" href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat AMQ" href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat 3scale API Management" href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management
                                </a>
                            </li>
                        </ul>
                    </div>
                    <div slot="details-nav--footer">
                        <pfe-cta pfe-priority="primary">
                            <a href="https://access.redhat.com/products/" class="pfe-navigation__menu-link" data-analytics-level="2" data-analytics-text="All Products" data-analytics-category="Products and Services|Products:" data-analytics-linkType="cta">All Products
                            </a>
                        </pfe-cta>
                    </div>
                </pfe-primary-detail>

            </pfe-navigation>

            <div id="scroll-anchor"></div>

            <!--[if IE 8]>
                <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                <span class="icon-warning alert-icon" aria-hidden="true"></span>
                You are using an unsupported web browser. Update to a supported browser for the best experience. <a href="/announcements/2120951">Read the announcement</a>.
                </div>
                </div>
            <![endif]-->
            <!--[if IE 9]>
                <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                <span class="icon-warning alert-icon" aria-hidden="true"></span>As of March 1, 2016, the Red Hat Customer Portal will no longer support Internet Explorer 9. See our new <a href="/help/browsers">browser support policy</a> for more information.
                </div>
                </div>
            <![endif]-->
            <div id="site-section"></div>
        </header>
        <!--googleon: all-->

        <main id="cp-main" class="portal-content-area">
            <div id="cp-content" class="main-content">                            <!-- /CP_PRIMER_HEADER -->

      <div class="container">
        

                                                                                                        <script>breadcrumbs = [["Products & Services","\/products\/"],["Product Documentation","\/documentation"],["OpenShift Container Platform","\/documentation\/en-us\/openshift_container_platform"],["4.13","\/documentation\/en-us\/openshift_container_platform\/4.13"],["Post-installation configuration","\/documentation\/en-us\/openshift_container_platform\/4.13\/html\/post-installation_configuration"],["Post-installation configuration","\/documentation\/en-us\/openshift_container_platform\/4.13\/html\/post-installation_configuration\/--single-page-document--"]]</script>

<div data-drupal-messages-fallback class="hidden"></div>


    </div>
        <div class="container">
        

  

  


  <article class="pvof-doc__content-wrapper__outer pvof-doc__content-wrapper__outer--css-not-removed">
    <script>
      'use strict';

            var $outerWrapper = document.querySelector('.pvof-doc__content-wrapper__outer');
      if ($outerWrapper && $outerWrapper.closest) {
        var $containerWrapper = $outerWrapper.closest('.container');
        if ($containerWrapper) {
          $containerWrapper.classList.remove('container');
          $containerWrapper.classList.add('j-chrome-content-container');
        }
      }

            var cssRemoved = false;
      try {
        var $crapCss = document.querySelectorAll(
          'link[href*="/chrome_themes/nimbus/css/pages.css"], link[href*="/chrome_themes/nimbus/css/components.css"]'
        );
        if ($crapCss.length) {
          for (let index = 0; index < $crapCss.length; index++) {
            const $stylesheet = $crapCss[index];
            $stylesheet.remove();
          }
        }
        cssRemoved = true;
      }
      catch (error) {
        console.error('Ran into an issue while trying to retheme page', error);
        cssRemoved = false;
      }

            if (cssRemoved) {
        var $pvofOuterWrapper = document.querySelector('.pvof-doc__content-wrapper__outer--css-not-removed');
        if ($pvofOuterWrapper) {
          $pvofOuterWrapper.classList.remove('pvof-doc__content-wrapper__outer--css-not-removed');
        }
      }
    </script>
    <div itemscope="" itemtype="https://schema.org/TechArticle" itemref="techArticle-md1 techArticle-md2 techArticle-md3"></div>
    <div itemscope="" itemtype="https://schema.org/SoftwareApplication" itemref="softwareApplication-md1 softwareApplication-md2 softwareApplication-md3 softwareApplication-md4"></div>
    <div class="pvof-doc__content-wrapper pvof-doc__content-wrapper--has-sidebar">
                                <div class="pvof-doc__content-wrapper__inner j-superdoc j-superdoc--has-nav">
                            <div class="pvof-sidebar__wrapper j-doc-nav j-superdoc__nav">
            <div class="j-sidebar__menu-container">
              <button class="j-sidebar__menu-trigger content-expander__trigger">
                <span class="j-sidebar__menu-trigger__open-text">Jump To</span>
                <span class="j-sidebar__menu-trigger__close-text">Close</span>
              </button>

              <div class="pvof-sidebar__inner-wrapper j-doc-nav__wrapper content-expander">
                <div class="j-sidebar__menu-details-container">
                  <button class="j-sidebar__menu-details-button j-sidebar__menu-details-button--expand">
                    Expand all
                  </button>
                  <button class="j-sidebar__menu-details-button j-sidebar__menu-details-button--collapse">
                    Collapse all
                  </button>
                </div>
                

  <nav id="pvof-doc__toc" class="pvof-doc__toc">
  <h2 class="j-doc-nav__title" id="j-doc-nav__title">
    Table of contents
  </h2>
  <div class="pvof-doc__toc-inner">
              <ol class="j-doc-nav__list" aria-labelledby="j-doc-nav__title">
                  <li class="j-doc-nav__list-item">
                                    

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration" class="j-doc-nav__link ">
    Post-installation configuration
  </a>
  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-configuration-overview" class="j-doc-nav__link j-doc-nav__link--has-children">
    1. Post-installation configuration overview
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "1. Post-installation configuration overview"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "1. Post-installation configuration overview"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-tasks" class="j-doc-nav__link ">
    1.1. Configuration tasks to perform after installation
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#configuring-private-cluster" class="j-doc-nav__link j-doc-nav__link--has-children">
    2. Configuring a private cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2. Configuring a private cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2. Configuring a private cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#private-clusters-about_configuring-private-cluster" class="j-doc-nav__link ">
    2.1. About private clusters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#private-clusters-setting-dns-private_configuring-private-cluster" class="j-doc-nav__link ">
    2.2. Setting DNS to private
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#private-clusters-setting-ingress-private_configuring-private-cluster" class="j-doc-nav__link ">
    2.3. Setting the Ingress Controller to private
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#private-clusters-setting-api-private_configuring-private-cluster" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.4. Restricting the API server to private
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.4. Restricting the API server to private"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.4. Restricting the API server to private"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nw-ingresscontroller-change-internal_configuring-private-cluster" class="j-doc-nav__link ">
    2.4.1. Configuring the Ingress Controller endpoint publishing scope to Internal
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-bare-metal-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    3. Bare metal configuration
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3. Bare metal configuration"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3. Bare metal configuration"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#bmo-about-the-bare-metal-operator_post-install-bare-metal-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.1. About the Bare Metal Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.1. About the Bare Metal Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.1. About the Bare Metal Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#bmo-bare-metal-operator-architecture_post-install-bare-metal-configuration" class="j-doc-nav__link ">
    3.1.1. Bare Metal Operator architecture
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#about-the-baremetalhost-resource_post-install-bare-metal-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.2. About the BareMetalHost resource
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.2. About the BareMetalHost resource"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.2. About the BareMetalHost resource"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#the-baremetalhost-spec" class="j-doc-nav__link ">
    3.2.1. The BareMetalHost spec
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#the-baremetalhost-status" class="j-doc-nav__link ">
    3.2.2. The BareMetalHost status
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#getting-the-baremetalhost-resource_post-install-bare-metal-configuration" class="j-doc-nav__link ">
    3.3. Getting the BareMetalHost resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#about-the-hostfirmwaresettings-resource_post-install-bare-metal-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.4. About the HostFirmwareSettings resource
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.4. About the HostFirmwareSettings resource"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.4. About the HostFirmwareSettings resource"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#the-literal-hostfirmwaresettings-literal-spec" class="j-doc-nav__link ">
    3.4.1. The HostFirmwareSettings spec
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#the-literal-hostfirmwaresettings-literal-status" class="j-doc-nav__link ">
    3.4.2. The HostFirmwareSettings status
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#getting-the-hostfirmwaresettings-resource_post-install-bare-metal-configuration" class="j-doc-nav__link ">
    3.5. Getting the HostFirmwareSettings resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#editing-the-hostfirmwaresettings-resource_post-install-bare-metal-configuration" class="j-doc-nav__link ">
    3.6. Editing the HostFirmwareSettings resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#verifying-the-hostfirmware-settings-resource-is-valid_post-install-bare-metal-configuration" class="j-doc-nav__link ">
    3.7. Verifying the HostFirmware Settings resource is valid
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#about-the-firmwareschema-resource_post-install-bare-metal-configuration" class="j-doc-nav__link ">
    3.8. About the FirmwareSchema resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#getting-the-firmwareschema-resource_post-install-bare-metal-configuration" class="j-doc-nav__link ">
    3.9. Getting the FirmwareSchema resource
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-multi-architecture-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    4. Configuring multi-architecture compute machines on an OpenShift Container Platform cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4. Configuring multi-architecture compute machines on an OpenShift Container Platform cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4. Configuring multi-architecture compute machines on an OpenShift Container Platform cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#multi-architecture-verifying-cluster-compatibility_multi-architecture-configuration" class="j-doc-nav__link ">
    4.1. Verifying cluster compatibility
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#creating-a-cluster-with-multi-architecture-compute-machine-on-azure" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.2. Creating a cluster with multi-architecture compute machine on Azure
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.2. Creating a cluster with multi-architecture compute machine on Azure"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.2. Creating a cluster with multi-architecture compute machine on Azure"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#multi-architecture-creating-arm64-bootimage_multi-architecture-configuration" class="j-doc-nav__link ">
    4.2.1. Creating an ARM64 boot image using the Azure image gallery
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#multi-architecture-modify-machine-set_multi-architecture-configuration" class="j-doc-nav__link ">
    4.2.2. Adding a multi-architecture compute machine set to your cluster
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#creating-a-cluster-with-multi-architecture-compute-machines-on-aws" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.3. Creating a cluster with multi-architecture compute machines on AWS
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.3. Creating a cluster with multi-architecture compute machines on AWS"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.3. Creating a cluster with multi-architecture compute machines on AWS"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#multi-architecture-modify-machine-set-aws_multi-architecture-configuration" class="j-doc-nav__link ">
    4.3.1. Adding an ARM64 compute machine set to your cluster
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#creating-a-cluster-with-multi-architecture-compute-machine-on-bare-metal-technology-preview" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.4. Creating a cluster with multi-architecture compute machine on bare metal (Technology Preview)
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.4. Creating a cluster with multi-architecture compute machine on bare metal (Technology Preview)"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.4. Creating a cluster with multi-architecture compute machine on bare metal (Technology Preview)"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machine-user-infra-machines-iso_multi-architecture-configuration" class="j-doc-nav__link ">
    4.4.1. Creating RHCOS machines using an ISO image
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machine-user-infra-machines-pxe_multi-architecture-configuration" class="j-doc-nav__link ">
    4.4.2. Creating RHCOS machines by PXE or iPXE booting
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#installation-approve-csrs_multi-architecture-configuration" class="j-doc-nav__link ">
    4.4.3. Approving the certificate signing requests for your machines
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#multi-architecture-import-imagestreams_multi-architecture-configuration" class="j-doc-nav__link ">
    4.5. Importing manifest lists in image streams on your multi-architecture compute machines
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#vsphere-post-installation-encryption" class="j-doc-nav__link j-doc-nav__link--has-children">
    5. Enabling encryption on a vSphere cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "5. Enabling encryption on a vSphere cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "5. Enabling encryption on a vSphere cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#encrypting-virtual-machines_vsphere-post-installation-encryption" class="j-doc-nav__link ">
    5.1. Encrypting virtual machines
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#additional-resources_enabling-encryption-installation" class="j-doc-nav__link ">
    5.2. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-machine-configuration-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    6. Post-installation machine configuration tasks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6. Post-installation machine configuration tasks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6. Post-installation machine configuration tasks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#understanding-the-machine-config-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.1. Understanding the Machine Config Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.1. Understanding the Machine Config Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.1. Understanding the Machine Config Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machine-config-operator_post-install-machine-configuration-tasks" class="j-doc-nav__link ">
    6.1.1. Machine Config Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machine-config-overview-post-install-machine-configuration-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.1.2. Machine config overview
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.1.2. Machine config overview"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.1.2. Machine config overview"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#what-can-you-change-with-machine-configs" class="j-doc-nav__link ">
    6.1.2.1. What can you change with machine configs?
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#project-2" class="j-doc-nav__link ">
    6.1.2.2. Project
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machine-config-drift-detection_post-install-machine-configuration-tasks" class="j-doc-nav__link ">
    6.1.3. Understanding configuration drift detection
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#checking-mco-status_post-install-machine-configuration-tasks" class="j-doc-nav__link ">
    6.1.4. Checking machine config pool status
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#using-machineconfigs-to-change-machines" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.2. Using MachineConfig objects to configure nodes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.2. Using MachineConfig objects to configure nodes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.2. Using MachineConfig objects to configure nodes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#installation-special-config-chrony_post-install-machine-configuration-tasks" class="j-doc-nav__link ">
    6.2.1. Configuring chrony time service
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#cnf-disable-chronyd_post-install-machine-configuration-tasks" class="j-doc-nav__link ">
    6.2.2. Disabling the chrony time service
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-nodes-kernel-arguments_post-install-machine-configuration-tasks" class="j-doc-nav__link ">
    6.2.3. Adding kernel arguments to nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#rhcos-enabling-multipath-day-2_post-install-machine-configuration-tasks" class="j-doc-nav__link ">
    6.2.4. Enabling multipathing with kernel arguments on RHCOS
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-nodes-rtkernel-arguments_post-install-machine-configuration-tasks" class="j-doc-nav__link ">
    6.2.5. Adding a real-time kernel to nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machineconfig-modify-journald_post-install-machine-configuration-tasks" class="j-doc-nav__link ">
    6.2.6. Configuring journald settings
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#rhcos-add-extensions_post-install-machine-configuration-tasks" class="j-doc-nav__link ">
    6.2.7. Adding extensions to RHCOS
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#rhcos-load-firmware-blobs_post-install-machine-configuration-tasks" class="j-doc-nav__link ">
    6.2.8. Loading custom firmware blobs in the machine config manifest
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#core-user-password_post-install-machine-configuration-tasks" class="j-doc-nav__link ">
    6.2.9. Changing the core user password for node access
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#configuring-machines-with-custom-resources" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.3. Configuring MCO-related custom resources
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.3. Configuring MCO-related custom resources"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.3. Configuring MCO-related custom resources"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#create-a-kubeletconfig-crd-to-edit-kubelet-parameters_post-install-machine-configuration-tasks" class="j-doc-nav__link ">
    6.3.1. Creating a KubeletConfig CRD to edit kubelet parameters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#create-a-containerruntimeconfig_post-install-machine-configuration-tasks" class="j-doc-nav__link ">
    6.3.2. Creating a ContainerRuntimeConfig CR to edit CRI-O parameters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#set-the-default-max-container-root-partition-size-for-overlay-with-crio_post-install-machine-configuration-tasks" class="j-doc-nav__link ">
    6.3.3. Setting the default maximum container root partition size for Overlay with CRI-O
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-cluster-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    7. Post-installation cluster tasks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7. Post-installation cluster tasks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7. Post-installation cluster tasks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#available_cluster_customizations" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.1. Available cluster customizations
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.1. Available cluster customizations"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.1. Available cluster customizations"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#configuration-resources_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.1.1. Cluster configuration resources
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#operator-configuration-resources_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.1.2. Operator configuration resources
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#additional-configuration-resources_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.1.3. Additional configuration resources
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#informational-resources_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.1.4. Informational Resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#images-update-global-pull-secret_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.2. Updating the global cluster pull secret
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#adding-worker-nodes_post-install-cluster-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.3. Adding worker nodes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.3. Adding worker nodes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.3. Adding worker nodes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#adding-worker-nodes-to-installer-provisioned-infrastructure-clusters" class="j-doc-nav__link ">
    7.3.1. Adding worker nodes to installer-provisioned infrastructure clusters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#adding-worker-nodes-to-user-provisioned-infrastructure-clusters" class="j-doc-nav__link ">
    7.3.2. Adding worker nodes to user-provisioned infrastructure clusters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#adding-worker-nodes-to-clusters-managed-by-the-assisted-installer" class="j-doc-nav__link ">
    7.3.3. Adding worker nodes to clusters managed by the Assisted Installer
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#adding-worker-nodes-to-clusters-managed-by-the-multicluster-engine-for-kubernetes" class="j-doc-nav__link ">
    7.3.4. Adding worker nodes to clusters managed by the multicluster engine for Kubernetes
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-adjust-worker-nodes" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.4. Adjust worker nodes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.4. Adjust worker nodes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.4. Adjust worker nodes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#differences-between-machinesets-and-machineconfigpool_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.4.1. Understanding the difference between compute machine sets and the machine config pool
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machineset-manually-scaling_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.4.2. Scaling a compute machine set manually
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machineset-delete-policy_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.4.3. The compute machine set deletion policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-scheduler-node-selectors-cluster_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.4.4. Creating default cluster-wide node selectors
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#installation-extend-edge-nodes-aws-local-zones_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.4.5. Creating user workloads in AWS Local Zones
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-worker-latency-profiles" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.5. Improving cluster stability in high latency environments using worker latency profiles
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.5. Improving cluster stability in high latency environments using worker latency profiles"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.5. Improving cluster stability in high latency environments using worker latency profiles"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-worker-latency-profiles-about_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.5.1. Understanding worker latency profiles
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-worker-latency-profiles-using_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.5.2. Using worker latency profiles
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-cpms-setup" class="j-doc-nav__link ">
    7.6. Managing control plane machines
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-creating-infrastructure-machinesets-production" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.7. Creating infrastructure machine sets for production environments
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.7. Creating infrastructure machine sets for production environments"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.7. Creating infrastructure machine sets for production environments"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machineset-creating_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.7.1. Creating a compute machine set
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#creating-an-infra-node_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.7.2. Creating an infrastructure node
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#creating-infra-machines_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.7.3. Creating a machine config pool for infrastructure machines
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#assigning-machine-set-resources-to-infra-nodes" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.8. Assigning machine set resources to infrastructure nodes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.8. Assigning machine set resources to infrastructure nodes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.8. Assigning machine set resources to infrastructure nodes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#binding-infra-node-workloads-using-taints-tolerations_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.8.1. Binding infrastructure node workloads using taints and tolerations
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#moving-resources-to-infrastructure-machinesets" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.9. Moving resources to infrastructure machine sets
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.9. Moving resources to infrastructure machine sets"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.9. Moving resources to infrastructure machine sets"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#infrastructure-moving-router_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.9.1. Moving the router
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#infrastructure-moving-registry_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.9.2. Moving the default registry
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#infrastructure-moving-monitoring_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.9.3. Moving the monitoring solution
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#infrastructure-moving-logging_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.9.4. Moving OpenShift Logging resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#cluster-autoscaler-about_post-install-cluster-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.10. About the cluster autoscaler
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.10. About the cluster autoscaler"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.10. About the cluster autoscaler"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#cluster-autoscaler-cr_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.10.1. Cluster autoscaler resource definition
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#ClusterAutoscaler-deploying_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.10.2. Deploying a cluster autoscaler
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machine-autoscaler-about_post-install-cluster-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.11. About the machine autoscaler
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.11. About the machine autoscaler"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.11. About the machine autoscaler"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machine-autoscaler-cr_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.11.1. Machine autoscaler resource definition
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#MachineAutoscaler-deploying_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.11.2. Deploying a machine autoscaler
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-clusters-cgroups-2_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.12. Configuring Linux cgroup
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-tp-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.13. Enabling Technology Preview features using FeatureGates
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.13. Enabling Technology Preview features using FeatureGates"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.13. Enabling Technology Preview features using FeatureGates"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-enabling-features-about_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.13.1. Understanding feature gates
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-enabling-features-console_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.13.2. Enabling feature sets using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-enabling-features-cli_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.13.3. Enabling feature sets using the CLI
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-etcd-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.14. etcd tasks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.14. etcd tasks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.14. etcd tasks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#about-etcd_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.14.1. About etcd encryption
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#etcd-encryption-types_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.14.2. Supported encryption types
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#enabling-etcd-encryption_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.14.3. Enabling etcd encryption
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#disabling-etcd-encryption_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.14.4. Disabling etcd encryption
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#backing-up-etcd-data_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.14.5. Backing up etcd data
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#etcd-defrag_post-install-cluster-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.14.6. Defragmenting etcd data
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.14.6. Defragmenting etcd data"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.14.6. Defragmenting etcd data"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#automatic-defrag-etcd-data_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.14.6.1. Automatic defragmentation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#manual-defrag-etcd-data_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.14.6.2. Manual defragmentation
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#dr-scenario-2-restoring-cluster-state_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.14.7. Restoring to a previous cluster state
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#dr-scenario-cluster-state-issues_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.14.8. Issues and workarounds for restoring a persistent storage state
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-pod-disruption-budgets" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.15. Pod disruption budgets
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.15. Pod disruption budgets"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.15. Pod disruption budgets"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-pods-pod-distruption-about_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.15.1. Understanding how to use pod disruption budgets to specify the number of pods that must be up
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-pods-pod-disruption-configuring_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.15.2. Specifying the number of pods that must be up with pod disruption budgets
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#pod-disruption-eviction-policy_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.15.3. Specifying the eviction policy for unhealthy pods
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-rotate-remove-cloud-creds" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.16. Rotating or removing cloud provider credentials
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.16. Rotating or removing cloud provider credentials"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.16. Rotating or removing cloud provider credentials"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#ccoctl-rotate-remove-cloud-creds" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.16.1. Rotating cloud provider credentials with the Cloud Credential Operator utility
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.16.1. Rotating cloud provider credentials with the Cloud Credential Operator utility"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.16.1. Rotating cloud provider credentials with the Cloud Credential Operator utility"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#refreshing-service-ids-ibm-cloud_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.16.1.1. Rotating API keys
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#manually-rotating-cloud-creds_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.16.2. Rotating cloud provider credentials manually
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#manually-removing-cloud-creds_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.16.3. Removing cloud provider credentials
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-must-gather-disconnected" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.17. Configuring image streams for a disconnected cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.17. Configuring image streams for a disconnected cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.17. Configuring image streams for a disconnected cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#installation-images-samples-disconnected-mirroring-assist_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.17.1. Cluster Samples Operator assistance for mirroring
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#installation-restricted-network-samples_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.17.2. Using Cluster Samples Operator image streams with alternate or mirrored registries
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#installation-preparing-restricted-cluster-to-gather-support-data_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.17.3. Preparing your cluster to gather support data
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#images-cluster-sample-imagestream-import_post-install-cluster-tasks" class="j-doc-nav__link ">
    7.18. Configuring periodic importing of Cluster Sample Operator image stream tags
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-node-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    8. Post-installation node tasks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8. Post-installation node tasks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8. Post-installation node tasks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-config-adding-rhel-compute" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.1. Adding RHEL compute machines to an OpenShift Container Platform cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.1. Adding RHEL compute machines to an OpenShift Container Platform cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.1. Adding RHEL compute machines to an OpenShift Container Platform cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#rhel-compute-overview_post-install-node-tasks" class="j-doc-nav__link ">
    8.1.1. About adding RHEL compute nodes to a cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#rhel-compute-requirements_post-install-node-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.1.2. System requirements for RHEL compute nodes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.1.2. System requirements for RHEL compute nodes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.1.2. System requirements for RHEL compute nodes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#csr-management_post-install-node-tasks" class="j-doc-nav__link ">
    8.1.2.1. Certificate signing requests management
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#rhel-preparing-playbook-machine_post-install-node-tasks" class="j-doc-nav__link ">
    8.1.3. Preparing the machine to run the playbook
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#rhel-preparing-node_post-install-node-tasks" class="j-doc-nav__link ">
    8.1.4. Preparing a RHEL compute node
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#rhel-adding-node_post-install-node-tasks" class="j-doc-nav__link ">
    8.1.5. Adding a RHEL compute machine to your cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#rhel-ansible-parameters_post-install-node-tasks" class="j-doc-nav__link ">
    8.1.6. Required parameters for the Ansible hosts file
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#rhel-removing-rhcos_post-install-node-tasks" class="j-doc-nav__link ">
    8.1.7. Optional: Removing RHCOS compute machines from a cluster
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-config-adding-fcos-compute" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.2. Adding RHCOS compute machines to an OpenShift Container Platform cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.2. Adding RHCOS compute machines to an OpenShift Container Platform cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.2. Adding RHCOS compute machines to an OpenShift Container Platform cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#prerequisites" class="j-doc-nav__link ">
    8.2.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machine-user-infra-machines-iso_post-install-node-tasks" class="j-doc-nav__link ">
    8.2.2. Creating RHCOS machines using an ISO image
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machine-user-infra-machines-pxe_post-install-node-tasks" class="j-doc-nav__link ">
    8.2.3. Creating RHCOS machines by PXE or iPXE booting
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#installation-approve-csrs_post-install-node-tasks" class="j-doc-nav__link ">
    8.2.4. Approving the certificate signing requests for your machines
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machine-node-custom-partition_post-install-node-tasks" class="j-doc-nav__link ">
    8.2.5. Adding a new RHCOS worker node with a custom /var partition in AWS
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-installation-config-deploying-machine-health-checks" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.3. Deploying machine health checks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.3. Deploying machine health checks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.3. Deploying machine health checks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machine-health-checks-about_post-install-node-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.3.1. About machine health checks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.3.1. About machine health checks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.3.1. About machine health checks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machine-health-checks-limitations_post-install-node-tasks" class="j-doc-nav__link ">
    8.3.1.1. Limitations when deploying machine health checks
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machine-health-checks-resource_post-install-node-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.3.2. Sample MachineHealthCheck resource
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.3.2. Sample MachineHealthCheck resource"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.3.2. Sample MachineHealthCheck resource"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machine-health-checks-short-circuiting_post-install-node-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.3.2.1. Short-circuiting machine health check remediation
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.3.2.1. Short-circuiting machine health check remediation"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.3.2.1. Short-circuiting machine health check remediation"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#setting-maxunhealthy-by-using-an-absolute-value" class="j-doc-nav__link ">
    8.3.2.1.1. Setting maxUnhealthy by using an absolute value
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#setting-maxunhealthy-by-using-percentages" class="j-doc-nav__link ">
    8.3.2.1.2. Setting maxUnhealthy by using percentages
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machine-health-checks-creating_post-install-node-tasks" class="j-doc-nav__link ">
    8.3.3. Creating a machine health check resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#machineset-manually-scaling_post-install-node-tasks" class="j-doc-nav__link ">
    8.3.4. Scaling a compute machine set manually
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#differences-between-machinesets-and-machineconfigpool_post-install-node-tasks" class="j-doc-nav__link ">
    8.3.5. Understanding the difference between compute machine sets and the machine config pool
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#recommended-node-host-practices_post-install-node-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.4. Recommended node host practices
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.4. Recommended node host practices"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.4. Recommended node host practices"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#create-a-kubeletconfig-crd-to-edit-kubelet-parameters_post-install-node-tasks" class="j-doc-nav__link ">
    8.4.1. Creating a KubeletConfig CRD to edit kubelet parameters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#modify-unavailable-workers_post-install-node-tasks" class="j-doc-nav__link ">
    8.4.2. Modifying the number of unavailable worker nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#master-node-sizing_post-install-node-tasks" class="j-doc-nav__link ">
    8.4.3. Control plane node sizing
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#seting_up_cpu_manager_post-install-node-tasks" class="j-doc-nav__link ">
    8.4.4. Setting up CPU Manager
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-huge-pages" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.5. Huge pages
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.5. Huge pages"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.5. Huge pages"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#what-huge-pages-do_post-install-node-tasks" class="j-doc-nav__link ">
    8.5.1. What huge pages do
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#how-huge-pages-are-consumed-by-apps_post-install-node-tasks" class="j-doc-nav__link ">
    8.5.2. How huge pages are consumed by apps
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#configuring-huge-pages_post-install-node-tasks" class="j-doc-nav__link ">
    8.5.3. Configuring huge pages at boot time
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-pods-plugins-about_post-install-node-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.6. Understanding device plugins
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.6. Understanding device plugins"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.6. Understanding device plugins"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#methods-for-deploying-a-device-plugin_post-install-node-tasks" class="j-doc-nav__link ">
    8.6.1. Methods for deploying a device plugin
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-pods-plugins-device-mgr_post-install-node-tasks" class="j-doc-nav__link ">
    8.6.2. Understanding the Device Manager
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-pods-plugins-install_post-install-node-tasks" class="j-doc-nav__link ">
    8.6.3. Enabling Device Manager
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-taints-tolerations" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.7. Taints and tolerations
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.7. Taints and tolerations"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.7. Taints and tolerations"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-scheduler-taints-tolerations-about_post-install-node-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.7.1. Understanding taints and tolerations
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.7.1. Understanding taints and tolerations"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.7.1. Understanding taints and tolerations"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-scheduler-taints-tolerations-about-seconds_post-install-node-tasks" class="j-doc-nav__link ">
    8.7.1.1. Understanding how to use toleration seconds to delay pod evictions
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-scheduler-taints-tolerations-about-multiple_post-install-node-tasks" class="j-doc-nav__link ">
    8.7.1.2. Understanding how to use multiple taints
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-scheduler-taints-tolerations-about-taintNodesByCondition_post-install-node-tasks" class="j-doc-nav__link ">
    8.7.1.3. Understanding pod scheduling and node conditions (taint node by condition)
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-scheduler-taints-tolerations-about-taintBasedEvictions_post-install-node-tasks" class="j-doc-nav__link ">
    8.7.1.4. Understanding evicting pods by condition (taint-based evictions)
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-scheduler-taints-tolerations-all_post-install-node-tasks" class="j-doc-nav__link ">
    8.7.1.5. Tolerating all taints
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-scheduler-taints-tolerations-adding_post-install-node-tasks" class="j-doc-nav__link ">
    8.7.2. Adding taints and tolerations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-scheduler-taints-tolerations-adding-machineset_post-install-node-tasks" class="j-doc-nav__link ">
    8.7.3. Adding taints and tolerations using a compute machine set
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-scheduler-taints-tolerations-bindings_post-install-node-tasks" class="j-doc-nav__link ">
    8.7.4. Binding a user to a node using taints and tolerations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-scheduler-taints-tolerations-special_post-install-node-tasks" class="j-doc-nav__link ">
    8.7.5. Controlling nodes with special hardware using taints and tolerations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-scheduler-taints-tolerations-removing_post-install-node-tasks" class="j-doc-nav__link ">
    8.7.6. Removing taints and tolerations
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-topology-manager" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.8. Topology Manager
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.8. Topology Manager"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.8. Topology Manager"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#topology_manager_policies_post-install-node-tasks" class="j-doc-nav__link ">
    8.8.1. Topology Manager policies
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#seting_up_topology_manager_post-install-node-tasks" class="j-doc-nav__link ">
    8.8.2. Setting up Topology Manager
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#pod-interactions-with-topology-manager_post-install-node-tasks" class="j-doc-nav__link ">
    8.8.3. Pod interactions with Topology Manager policies
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-overcommit-resource-requests_post-install-node-tasks" class="j-doc-nav__link ">
    8.9. Resource requests and overcommitment
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-resource-override_post-install-node-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.10. Cluster-level overcommit using the Cluster Resource Override Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.10. Cluster-level overcommit using the Cluster Resource Override Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.10. Cluster-level overcommit using the Cluster Resource Override Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-resource-override-deploy-console_post-install-node-tasks" class="j-doc-nav__link ">
    8.10.1. Installing the Cluster Resource Override Operator using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-resource-override-deploy-cli_post-install-node-tasks" class="j-doc-nav__link ">
    8.10.2. Installing the Cluster Resource Override Operator using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-resource-configure_post-install-node-tasks" class="j-doc-nav__link ">
    8.10.3. Configuring cluster-level overcommit
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-node-overcommit_post-install-node-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.11. Node-level overcommit
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.11. Node-level overcommit"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.11. Node-level overcommit"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-overcommit-reserving-memory_post-install-node-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.11.1. Understanding compute resources and containers
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.11.1. Understanding compute resources and containers"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.11.1. Understanding compute resources and containers"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#understanding-container-CPU-requests_post-install-node-tasks" class="j-doc-nav__link ">
    8.11.1.1. Understanding container CPU requests
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#understanding-memory-requests-container_post-install-node-tasks" class="j-doc-nav__link ">
    8.11.1.2. Understanding container memory requests
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-overcommit-qos-about_post-install-node-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.11.2. Understanding overcomitment and quality of service classes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.11.2. Understanding overcomitment and quality of service classes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.11.2. Understanding overcomitment and quality of service classes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#qos-about-reserve_post-install-node-tasks" class="j-doc-nav__link ">
    8.11.2.1. Understanding how to reserve memory across quality of service tiers
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-qos-about-swap_post-install-node-tasks" class="j-doc-nav__link ">
    8.11.3. Understanding swap memory and QOS
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-overcommit-configure-nodes_post-install-node-tasks" class="j-doc-nav__link ">
    8.11.4. Understanding nodes overcommitment
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-overcommit-node-enforcing_post-install-node-tasks" class="j-doc-nav__link ">
    8.11.5. Disabling or enforcing CPU limits using CPU CFS quotas
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-overcommit-node-resources_post-install-node-tasks" class="j-doc-nav__link ">
    8.11.6. Reserving resources for system processes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-overcommit-node-disable_post-install-node-tasks" class="j-doc-nav__link ">
    8.11.7. Disabling overcommitment for a node
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-project-overcommit_post-install-node-tasks" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.12. Project-level limits
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.12. Project-level limits"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.12. Project-level limits"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-cluster-overcommit-project-disable_post-install-node-tasks" class="j-doc-nav__link ">
    8.12.1. Disabling overcommitment for a project
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-garbage-collection" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.13. Freeing node resources using garbage collection
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.13. Freeing node resources using garbage collection"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.13. Freeing node resources using garbage collection"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-nodes-garbage-collection-containers_post-install-node-tasks" class="j-doc-nav__link ">
    8.13.1. Understanding how terminated containers are removed through garbage collection
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-nodes-garbage-collection-images_post-install-node-tasks" class="j-doc-nav__link ">
    8.13.2. Understanding how images are removed through garbage collection
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-nodes-garbage-collection-configuring_post-install-node-tasks" class="j-doc-nav__link ">
    8.13.3. Configuring garbage collection for containers and images
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-using-node-tuning-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.14. Using the Node Tuning Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.14. Using the Node Tuning Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.14. Using the Node Tuning Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#accessing-an-example-node-tuning-operator-specification_post-install-node-tasks" class="j-doc-nav__link ">
    8.14.1. Accessing an example Node Tuning Operator specification
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#custom-tuning-specification_post-install-node-tasks" class="j-doc-nav__link ">
    8.14.2. Custom tuning specification
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#custom-tuning-default-profiles-set_post-install-node-tasks" class="j-doc-nav__link ">
    8.14.3. Default profiles set on a cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#supported-tuned-daemon-plug-ins_post-install-node-tasks" class="j-doc-nav__link ">
    8.14.4. Supported TuneD daemon plugins
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nodes-nodes-managing-max-pods-proc_post-install-node-tasks" class="j-doc-nav__link ">
    8.15. Configuring the maximum number of pods per node
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-network-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    9. Post-installation network configuration
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9. Post-installation network configuration"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9. Post-installation network configuration"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nw-operator-cr_post-install-network-configuration" class="j-doc-nav__link ">
    9.1. Cluster Network Operator configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nw-proxy-configure-object_post-install-network-configuration" class="j-doc-nav__link ">
    9.2. Enabling the cluster-wide proxy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#private-clusters-setting-dns-private_post-install-network-configuration" class="j-doc-nav__link ">
    9.3. Setting DNS to private
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-configuring_ingress_cluster_traffic" class="j-doc-nav__link ">
    9.4. Configuring ingress cluster traffic
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-configuring-node-port-service-range" class="j-doc-nav__link j-doc-nav__link--has-children">
    9.5. Configuring the node port service range
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9.5. Configuring the node port service range"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9.5. Configuring the node port service range"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-configuring-node-port-service-range-prerequisites" class="j-doc-nav__link j-doc-nav__link--has-children">
    9.5.1. Prerequisites
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9.5.1. Prerequisites"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9.5.1. Prerequisites"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nw-nodeport-service-range-edit_post-install-network-configuration" class="j-doc-nav__link ">
    9.5.1.1. Expanding the node port range
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-configuring-ipsec-ovn" class="j-doc-nav__link j-doc-nav__link--has-children">
    9.6. Configuring IPsec encryption
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9.6. Configuring IPsec encryption"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9.6. Configuring IPsec encryption"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-configuring-ipsec-ovn-prerequisites" class="j-doc-nav__link j-doc-nav__link--has-children">
    9.6.1. Prerequisites
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9.6.1. Prerequisites"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9.6.1. Prerequisites"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nw-ovn-ipsec-enable_post-install-network-configuration" class="j-doc-nav__link ">
    9.6.1.1. Enabling IPsec encryption
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nw-ovn-ipsec-verification_post-install-network-configuration" class="j-doc-nav__link ">
    9.6.1.2. Verifying that IPsec is enabled
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-configuring-network-policy" class="j-doc-nav__link j-doc-nav__link--has-children">
    9.7. Configuring network policy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9.7. Configuring network policy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9.7. Configuring network policy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nw-networkpolicy-about_post-install-network-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    9.7.1. About network policy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9.7.1. About network policy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9.7.1. About network policy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nw-networkpolicy-allow-from-router_post-install-network-configuration" class="j-doc-nav__link ">
    9.7.1.1. Using the allow-from-router network policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nw-networkpolicy-allow-from-hostnetwork_post-install-network-configuration" class="j-doc-nav__link ">
    9.7.1.2. Using the allow-from-hostnetwork network policy
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nw-networkpolicy-object_post-install-network-configuration" class="j-doc-nav__link ">
    9.7.2. Example NetworkPolicy object
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nw-networkpolicy-create-cli_post-install-network-configuration" class="j-doc-nav__link ">
    9.7.3. Creating a network policy using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nw-networkpolicy-multitenant-isolation_post-install-network-configuration" class="j-doc-nav__link ">
    9.7.4. Configuring multitenant isolation by using network policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-nw-networkpolicy-creating-default-networkpolicy-objects-for-a-new-project" class="j-doc-nav__link ">
    9.7.5. Creating default network policies for a new project
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#modifying-template-for-new-projects_post-install-network-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    9.7.6. Modifying the template for new projects
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9.7.6. Modifying the template for new projects"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9.7.6. Modifying the template for new projects"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nw-networkpolicy-project-defaults_post-install-network-configuration" class="j-doc-nav__link ">
    9.7.6.1. Adding network policies to the new project template
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#ossm-supported-configurations_post-install-network-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    9.8. Supported configurations
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9.8. Supported configurations"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9.8. Supported configurations"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#ossm-supported-platforms_post-install-network-configuration" class="j-doc-nav__link ">
    9.8.1. Supported platforms
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#ossm-unsupported-configurations_post-install-network-configuration" class="j-doc-nav__link ">
    9.8.2. Unsupported configurations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#ossm-supported-configurations-networks_post-install-network-configuration" class="j-doc-nav__link ">
    9.8.3. Supported network configurations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#ossm-supported-configurations-sm_post-install-network-configuration" class="j-doc-nav__link ">
    9.8.4. Supported configurations for Service Mesh
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#ossm-supported-configurations-kiali_post-install-network-configuration" class="j-doc-nav__link ">
    9.8.5. Supported configurations for Kiali
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#ossm-supported-configurations-jaeger_post-install-network-configuration" class="j-doc-nav__link ">
    9.8.6. Supported configurations for Distributed Tracing
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#ossm-supported-configurations-webassembly_post-install-network-configuration" class="j-doc-nav__link ">
    9.8.7. Supported WebAssembly module
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#ossm-installation-activities_post-install-network-configuration" class="j-doc-nav__link ">
    9.8.8. Operator overview
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-installationrouting-optimization" class="j-doc-nav__link j-doc-nav__link--has-children">
    9.9. Optimizing routing
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9.9. Optimizing routing"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9.9. Optimizing routing"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#baseline-router-performance_post-install-network-configuration" class="j-doc-nav__link ">
    9.9.1. Baseline Ingress Controller (router) performance
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#ingress-liveness-readiness-startup-probes_post-install-network-configuration" class="j-doc-nav__link ">
    9.9.2. Configuring Ingress Controller liveness, readiness, and startup probes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#configuring-haproxy-interval_post-install-network-configuration" class="j-doc-nav__link ">
    9.9.3. Configuring HAProxy reload interval
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-installation-osp-fips" class="j-doc-nav__link j-doc-nav__link--has-children">
    9.10. Post-installation RHOSP network configuration
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9.10. Post-installation RHOSP network configuration"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9.10. Post-installation RHOSP network configuration"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#installation-osp-configuring-api-floating-ip_post-install-network-configuration" class="j-doc-nav__link ">
    9.10.1. Configuring application access with floating IP addresses
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#installation-osp-kuryr-port-pools_post-install-network-configuration" class="j-doc-nav__link ">
    9.10.2. Kuryr ports pools
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#installation-osp-kuryr-settings-active_post-install-network-configuration" class="j-doc-nav__link ">
    9.10.3. Adjusting Kuryr ports pool settings in active deployments on RHOSP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nw-osp-enabling-ovs-offload_post-install-network-configuration" class="j-doc-nav__link ">
    9.10.4. Enabling OVS hardware offloading
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nw-osp-hardware-offload-attaching-network_post-install-network-configuration" class="j-doc-nav__link ">
    9.10.5. Attaching an OVS hardware offloading network
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nw-osp-pod-connections-ipv6_post-install-network-configuration" class="j-doc-nav__link ">
    9.10.6. Enabling IPv6 connectivity to pods on RHOSP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nw-osp-pod-adding-connections-ipv6_post-install-network-configuration" class="j-doc-nav__link ">
    9.10.7. Adding IPv6 connectivity to pods on RHOSP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#nw-osp-pod-creating-ipv6_post-install-network-configuration" class="j-doc-nav__link ">
    9.10.8. Create pods that have IPv6 connectivity on RHOSP
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-storage-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    10. Post-installation storage configuration
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10. Post-installation storage configuration"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10. Post-installation storage configuration"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-dynamic-provisioning" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.1. Dynamic provisioning
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.1. Dynamic provisioning"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.1. Dynamic provisioning"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#about_post-install-storage-configuration" class="j-doc-nav__link ">
    10.1.1. About dynamic provisioning
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#available-plug-ins_post-install-storage-configuration" class="j-doc-nav__link ">
    10.1.2. Available dynamic provisioning plugins
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#defining-storage-classes_post-install-storage-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.2. Defining a storage class
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.2. Defining a storage class"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.2. Defining a storage class"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#basic-storage-class-definition_post-install-storage-configuration" class="j-doc-nav__link ">
    10.2.1. Basic StorageClass object definition
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#storage-class-annotations_post-install-storage-configuration" class="j-doc-nav__link ">
    10.2.2. Storage class annotations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#openstack-cinder-storage-class_post-install-storage-configuration" class="j-doc-nav__link ">
    10.2.3. RHOSP Cinder object definition
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#aws-definition_post-install-storage-configuration" class="j-doc-nav__link ">
    10.2.4. AWS Elastic Block Store (EBS) object definition
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#azure-disk-definition_post-install-storage-configuration" class="j-doc-nav__link ">
    10.2.5. Azure Disk object definition
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#azure-file-definition_post-install-storage-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.2.6. Azure File object definition
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.2.6. Azure File object definition"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.2.6. Azure File object definition"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#azure-file-considerations_post-install-storage-configuration" class="j-doc-nav__link ">
    10.2.6.1. Considerations when using Azure File
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#gce-persistentdisk-storage-class_post-install-storage-configuration" class="j-doc-nav__link ">
    10.2.7. GCE PersistentDisk (gcePD) object definition
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#vsphere-definition_post-install-storage-configuration" class="j-doc-nav__link ">
    10.2.8. VMware vSphere object definition
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#ovirt-csi-driver-storage-class_post-install-storage-configuration" class="j-doc-nav__link ">
    10.2.9. Red Hat Virtualization (RHV) object definition
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#change-default-storage-class_post-install-storage-configuration" class="j-doc-nav__link ">
    10.3. Changing the default storage class
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-optimizing-storage" class="j-doc-nav__link ">
    10.4. Optimizing storage
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#available-persistent-storage-options_post-install-storage-configuration" class="j-doc-nav__link ">
    10.5. Available persistent storage options
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#recommended-configurable-storage-technology_post-install-storage-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.6. Recommended configurable storage technology
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.6. Recommended configurable storage technology"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.6. Recommended configurable storage technology"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#specific-application-storage-recommendations" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.6.1. Specific application storage recommendations
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.6.1. Specific application storage recommendations"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.6.1. Specific application storage recommendations"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#registry" class="j-doc-nav__link ">
    10.6.1.1. Registry
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#scaled-registry" class="j-doc-nav__link ">
    10.6.1.2. Scaled registry
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#metrics" class="j-doc-nav__link ">
    10.6.1.3. Metrics
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#logging" class="j-doc-nav__link ">
    10.6.1.4. Logging
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#applications" class="j-doc-nav__link ">
    10.6.1.5. Applications
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#other-specific-application-storage-recommendations" class="j-doc-nav__link ">
    10.6.2. Other specific application storage recommendations
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-deploy-OCS" class="j-doc-nav__link ">
    10.7. Deploy Red Hat OpenShift Data Foundation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#admission-plug-ins-additional-resources" class="j-doc-nav__link ">
    10.8. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-preparing-for-users" class="j-doc-nav__link j-doc-nav__link--has-children">
    11. Preparing for users
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11. Preparing for users"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11. Preparing for users"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-understanding-identity-provider" class="j-doc-nav__link j-doc-nav__link--has-children">
    11.1. Understanding identity provider configuration
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11.1. Understanding identity provider configuration"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11.1. Understanding identity provider configuration"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#identity-provider-overview_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.1.1. About identity providers in OpenShift Container Platform
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-supported-identity-providers" class="j-doc-nav__link ">
    11.1.2. Supported identity providers
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#identity-provider-parameters_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.1.3. Identity provider parameters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#identity-provider-default-CR_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.1.4. Sample identity provider CR
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-using-rbac-to-define-and-apply-permissions" class="j-doc-nav__link j-doc-nav__link--has-children">
    11.2. Using RBAC to define and apply permissions
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11.2. Using RBAC to define and apply permissions"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11.2. Using RBAC to define and apply permissions"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#authorization-overview_post-install-preparing-for-users" class="j-doc-nav__link j-doc-nav__link--has-children">
    11.2.1. RBAC overview
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11.2.1. RBAC overview"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11.2.1. RBAC overview"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#default-roles_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.2.1.1. Default cluster roles
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#evaluating-authorization_post-install-preparing-for-users" class="j-doc-nav__link j-doc-nav__link--has-children">
    11.2.1.2. Evaluating authorization
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11.2.1.2. Evaluating authorization"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11.2.1.2. Evaluating authorization"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#cluster-role-aggregations_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.2.1.2.1. Cluster role aggregation
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#rbac-projects-namespaces_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.2.2. Projects and namespaces
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#rbac-default-projects_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.2.3. Default projects
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#viewing-cluster-roles_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.2.4. Viewing cluster roles and bindings
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#viewing-local-roles_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.2.5. Viewing local roles and bindings
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#adding-roles_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.2.6. Adding roles to users
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#creating-local-role_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.2.7. Creating a local role
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#creating-cluster-role_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.2.8. Creating a cluster role
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#local-role-binding-commands_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.2.9. Local role binding commands
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#cluster-role-binding-commands_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.2.10. Cluster role binding commands
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#creating-cluster-admin_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.2.11. Creating a cluster admin
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#understanding-kubeadmin_post-install-preparing-for-users" class="j-doc-nav__link j-doc-nav__link--has-children">
    11.3. The kubeadmin user
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11.3. The kubeadmin user"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11.3. The kubeadmin user"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#removing-kubeadmin_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.3.1. Removing the kubeadmin user
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-image-configuration-resources" class="j-doc-nav__link j-doc-nav__link--has-children">
    11.4. Image configuration
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11.4. Image configuration"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11.4. Image configuration"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#images-configuration-parameters_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.4.1. Image controller configuration parameters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#images-configuration-file_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.4.2. Configuring image registry settings
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#images-configuration-cas_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.4.3. Configuring additional trust stores for image registry access
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#images-configuration-registry-mirror_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.4.4. Configuring image registry repository mirroring
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#images-configuration-registry-mirror-convert_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.4.5. Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-mirrored-catalogs" class="j-doc-nav__link j-doc-nav__link--has-children">
    11.5. Populating OperatorHub from mirrored Operator catalogs
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11.5. Populating OperatorHub from mirrored Operator catalogs"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11.5. Populating OperatorHub from mirrored Operator catalogs"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#prerequisites_post-install-mirrored-catalogs" class="j-doc-nav__link ">
    11.5.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#olm-mirror-catalog-icsp_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.5.2. Creating the ImageContentSourcePolicy object
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#olm-creating-catalog-from-index_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.5.3. Adding a catalog source to a cluster
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#olm-installing-operators-from-operatorhub_post-install-preparing-for-users" class="j-doc-nav__link j-doc-nav__link--has-children">
    11.6. About Operator installation with OperatorHub
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11.6. About Operator installation with OperatorHub"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11.6. About Operator installation with OperatorHub"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#olm-installing-from-operatorhub-using-web-console_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.6.1. Installing from OperatorHub using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#olm-installing-operator-from-operatorhub-using-cli_post-install-preparing-for-users" class="j-doc-nav__link ">
    11.6.2. Installing from OperatorHub using the CLI
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#configuring-alert-notifications" class="j-doc-nav__link j-doc-nav__link--has-children">
    12. Configuring alert notifications
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12. Configuring alert notifications"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12. Configuring alert notifications"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#sending-notifications-to-external-systems_configuring-alert-notifications" class="j-doc-nav__link j-doc-nav__link--has-children">
    12.1. Sending notifications to external systems
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12.1. Sending notifications to external systems"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12.1. Sending notifications to external systems"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#configuring-alert-receivers_configuring-alert-notifications" class="j-doc-nav__link ">
    12.1.1. Configuring alert receivers
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#configuring-alert-notifications-additional-resources" class="j-doc-nav__link ">
    12.2. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#connected-to-disconnected" class="j-doc-nav__link j-doc-nav__link--has-children">
    13. Converting a connected cluster to a disconnected cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13. Converting a connected cluster to a disconnected cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13. Converting a connected cluster to a disconnected cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#installation-about-mirror-registry_connected-to-disconnected" class="j-doc-nav__link ">
    13.1. About the mirror registry
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#prerequisites_connected-to-disconnected" class="j-doc-nav__link ">
    13.2. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#connected-to-disconnected-prepare-mirror_connected-to-disconnected" class="j-doc-nav__link ">
    13.3. Preparing the cluster for mirroring
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#connected-to-disconnected-mirror-images_connected-to-disconnected" class="j-doc-nav__link ">
    13.4. Mirroring the images
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#connected-to-disconnected-config-registry_connected-to-disconnected" class="j-doc-nav__link ">
    13.5. Configuring the cluster for the mirror registry
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#connected-to-disconnected-verify_connected-to-disconnected" class="j-doc-nav__link ">
    13.6. Ensure applications continue to work
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#connected-to-disconnected-disconnect_connected-to-disconnected" class="j-doc-nav__link ">
    13.7. Disconnect the cluster from the network
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#connected-to-disconnected-restore-insights_connected-to-disconnected" class="j-doc-nav__link ">
    13.8. Restoring a degraded Insights Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#connected-to-disconnected-restore_connected-to-disconnected" class="j-doc-nav__link ">
    13.9. Restoring the network
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#enabling-cluster-capabilities" class="j-doc-nav__link j-doc-nav__link--has-children">
    14. Enabling cluster capabilities
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "14. Enabling cluster capabilities"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "14. Enabling cluster capabilities"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#viewing_the_cluster_capabilities_enabling-cluster-capabilities" class="j-doc-nav__link ">
    14.1. Viewing the cluster capabilities
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#setting_baseline_capability_set_enabling-cluster-capabilities" class="j-doc-nav__link ">
    14.2. Enabling the cluster capabilities by setting baseline capability set
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#setting_additional_enabled_capabilities_enabling-cluster-capabilities" class="j-doc-nav__link ">
    14.3. Enabling the cluster capabilities by setting additional enabled capabilities
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#additional-resources_enabling-cluster-capabilities" class="j-doc-nav__link ">
    14.4. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-configure-additional-devices-ibmz" class="j-doc-nav__link j-doc-nav__link--has-children">
    15. Configuring additional devices in an IBM Z or IBM(R) LinuxONE environment
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "15. Configuring additional devices in an IBM Z or IBM(R) LinuxONE environment"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "15. Configuring additional devices in an IBM Z or IBM(R) LinuxONE environment"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#configure-additional-devices-using-mco_post-install-configure-additional-devices-ibmz" class="j-doc-nav__link j-doc-nav__link--has-children">
    15.1. Configuring additional devices using the Machine Config Operator (MCO)
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "15.1. Configuring additional devices using the Machine Config Operator (MCO)"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "15.1. Configuring additional devices using the Machine Config Operator (MCO)"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#configuring-fcp-host" class="j-doc-nav__link ">
    15.1.1. Configuring a Fibre Channel Protocol (FCP) host
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#configuring-fcp-lun" class="j-doc-nav__link ">
    15.1.2. Configuring an FCP LUN
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#configuring-dasd" class="j-doc-nav__link ">
    15.1.3. Configuring DASD
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#configuring-qeth" class="j-doc-nav__link ">
    15.1.4. Configuring qeth
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#configure-additional-devices-manually_post-install-configure-additional-devices-ibmz" class="j-doc-nav__link ">
    15.2. Configuring additional devices manually
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#roce-network-cards" class="j-doc-nav__link ">
    15.3. RoCE network Cards
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#enabling-multipathing-fcp-luns_post-install-configure-additional-devices-ibmz" class="j-doc-nav__link ">
    15.4. Enabling multipathing for FCP LUNs
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#post-install-vsphere-zones-regions-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    16. Multiple regions and zones configuration for a cluster on vSphere
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16. Multiple regions and zones configuration for a cluster on vSphere"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16. Multiple regions and zones configuration for a cluster on vSphere"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#specifying-regions-zones-infrastructure-vsphere_post-install-vsphere-zones-regions-configuration" class="j-doc-nav__link ">
    16.1. Specifying multiple regions and zones for your cluster on vSphere
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#vsphere-enabling-multiple-layer2-network_post-install-vsphere-zones-regions-configuration" class="j-doc-nav__link ">
    16.2. Enabling a multiple layer 2 network for your cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#references-regions-zones-infrastructure-vsphere_post-install-vsphere-zones-regions-configuration" class="j-doc-nav__link ">
    16.3. Parameters for the cluster-wide infrastructure CRD
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#coreos-layering" class="j-doc-nav__link j-doc-nav__link--has-children">
    17. RHCOS image layering
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "17. RHCOS image layering"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "17. RHCOS image layering"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#coreos-layering-configuring_coreos-layering" class="j-doc-nav__link ">
    17.1. Applying a RHCOS custom layered image
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#coreos-layering-removing_coreos-layering" class="j-doc-nav__link ">
    17.2. Removing a RHCOS custom layered image
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#coreos-layering-updating_coreos-layering" class="j-doc-nav__link ">
    17.3. Updating with a RHCOS custom layered image
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                    

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration#idm140031663487472" class="j-doc-nav__link ">
    Legal Notice
  </a>
  
          </li>
              </ol>
    
  </div>
</nav>


              </div>
            </div>
            <div class="j-options-container j-options-container--mobile">
              <button class="j-sidebar__menu-trigger j-sidebar__menu-trigger--options content-expander__trigger">
                <span class="j-sidebar__menu-trigger__open-headline">
                  Settings
                </span>
                <span class="j-sidebar__menu-trigger__close-text">Close</span>
              </button>
              

  <ul class="j-doc-options__list content-expander">
    <li class="j-doc-options__item">
          <label class="j-doc-option__label j-doc-option__label--language" for="j-doc-language">
        Language:
      </label>
      <select id="j-doc-language" class="j-doc-option__select">
                  <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration" selected=''>
            English
          </option>
                  <option value="/documentation/ko-kr/openshift_container_platform/4.13/html-single/post-installation_configuration" >
            한국어
          </option>
                  <option value="/documentation/ja-jp/openshift_container_platform/4.13/html-single/post-installation_configuration" >
            日本語
          </option>
                  <option value="/documentation/zh-cn/openshift_container_platform/4.13/html-single/post-installation_configuration" >
            简体中文
          </option>
              </select>

            <noscript>
        <div class="j-doc-option__label j-doc-option__label--language" id="j-doc-option__label--language--nojs">
          Language:
        </div>
        <ul aria-labelledby="j-doc-option__label--language--nojs" class="j-doc-option__languages-list">
                      <li>
              <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration">English</a>
            </li>
                      <li>
              <a href="/documentation/ko-kr/openshift_container_platform/4.13/html-single/post-installation_configuration">한국어</a>
            </li>
                      <li>
              <a href="/documentation/ja-jp/openshift_container_platform/4.13/html-single/post-installation_configuration">日本語</a>
            </li>
                      <li>
              <a href="/documentation/zh-cn/openshift_container_platform/4.13/html-single/post-installation_configuration">简体中文</a>
            </li>
                  </ul>
      </noscript>

      </li>

    <li class="j-doc-options__item">
    <label for="j-doc-mode" class="j-doc-option__label j-doc-option__label--format">
      Format:
    </label>
    <select id="j-doc-mode" class="j-doc-option__select">
              <option value="/documentation/en-us/openshift_container_platform/4.13/html/post-installation_configuration"  class="j-doc-options__option j-doc-options__option--multi-page">
          Multi-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration" selected='' class="j-doc-options__option j-doc-options__option--single-page">
          Single-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/pdf/post-installation_configuration/OpenShift_Container_Platform-4.13-Post-installation_configuration-en-US.pdf"  class="j-doc-options__option j-doc-options__option--pdf">
          PDF
        </option>
          </select>

        <noscript>
      <div class="j-doc-option__label j-doc-option__label--format" id="j-doc-option__label--format--nojs">
        Format:
      </div>
      <ul class="j-doc-option__format-list" aria-labelledby="j-doc-option__label--format--nojs">
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--multi-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html/post-installation_configuration">Multi-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--single-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration">Single-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--pdf"><a href="/documentation/en-us/openshift_container_platform/4.13/pdf/post-installation_configuration/OpenShift_Container_Platform-4.13-Post-installation_configuration-en-US.pdf">PDF</a></li>
              </ul>
    </noscript>
  </li>
</ul>


              </div>
          </div>
                <div class="pvof-doc__tertiary-sidebar j-doc__tertiary-sidebar">
          <div class="pvof-doc__tertiary-sidebar__inner j-doc__tertiary-sidebar__inner">
            <div class="j-doc__doc-options">
              <div class="j-options-container j-options-container--desktop">
                <button class="j-sidebar__menu-trigger j-sidebar__menu-trigger--tablet content-expander__trigger">
                  <span class="j-sidebar__menu-trigger-icon"></span>
                  <h2 class="visually-hidden">Language and Page Formatting Options</h2>
                </button>
                  

  <ul class="j-doc-options__list content-expander">
    <li class="j-doc-options__item">
          <label class="j-doc-option__label j-doc-option__label--language" for="j-doc-language--2">
        Language:
      </label>
      <select id="j-doc-language--2" class="j-doc-option__select">
                  <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration" selected=''>
            English
          </option>
                  <option value="/documentation/ko-kr/openshift_container_platform/4.13/html-single/post-installation_configuration" >
            한국어
          </option>
                  <option value="/documentation/ja-jp/openshift_container_platform/4.13/html-single/post-installation_configuration" >
            日本語
          </option>
                  <option value="/documentation/zh-cn/openshift_container_platform/4.13/html-single/post-installation_configuration" >
            简体中文
          </option>
              </select>

            <noscript>
        <div class="j-doc-option__label j-doc-option__label--language" id="j-doc-option__label--language--nojs">
          Language:
        </div>
        <ul aria-labelledby="j-doc-option__label--language--nojs" class="j-doc-option__languages-list">
                      <li>
              <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration">English</a>
            </li>
                      <li>
              <a href="/documentation/ko-kr/openshift_container_platform/4.13/html-single/post-installation_configuration">한국어</a>
            </li>
                      <li>
              <a href="/documentation/ja-jp/openshift_container_platform/4.13/html-single/post-installation_configuration">日本語</a>
            </li>
                      <li>
              <a href="/documentation/zh-cn/openshift_container_platform/4.13/html-single/post-installation_configuration">简体中文</a>
            </li>
                  </ul>
      </noscript>

      </li>

    <li class="j-doc-options__item">
    <label for="j-doc-mode--2" class="j-doc-option__label j-doc-option__label--format">
      Format:
    </label>
    <select id="j-doc-mode--2" class="j-doc-option__select">
              <option value="/documentation/en-us/openshift_container_platform/4.13/html/post-installation_configuration"  class="j-doc-options__option j-doc-options__option--multi-page">
          Multi-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration" selected='' class="j-doc-options__option j-doc-options__option--single-page">
          Single-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/pdf/post-installation_configuration/OpenShift_Container_Platform-4.13-Post-installation_configuration-en-US.pdf"  class="j-doc-options__option j-doc-options__option--pdf">
          PDF
        </option>
          </select>

        <noscript>
      <div class="j-doc-option__label j-doc-option__label--format" id="j-doc-option__label--format--nojs">
        Format:
      </div>
      <ul class="j-doc-option__format-list" aria-labelledby="j-doc-option__label--format--nojs">
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--multi-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html/post-installation_configuration">Multi-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--single-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration">Single-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--pdf"><a href="/documentation/en-us/openshift_container_platform/4.13/pdf/post-installation_configuration/OpenShift_Container_Platform-4.13-Post-installation_configuration-en-US.pdf">PDF</a></li>
              </ul>
    </noscript>
  </li>
</ul>


                </div>
              </div>
          </div>
        </div>

                  <div class="doc-wrapper pvof-doc__wrapper j-superdoc__content-wrapper" id="doc-wrapper">
            

  <div class="pane-page-title">
    <h1 class="title" itemprop="name">Post-installation configuration</h1>
  </div>


  <div xml:lang="en-US" class="book" id="idm140031651630208"><div class="titlepage"><div><div class="producttitle"><span class="productname">OpenShift Container Platform</span> <span class="productnumber">4.13</span></div><div><h2 class="subtitle">Day 2 operations for OpenShift Container Platform </h2></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat OpenShift Documentation Team</span></div></div><div><a href="#idm140031663487472">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				This document provides instructions and guidance on post installation activities for OpenShift Container Platform.
			</div></div></div></div><hr/></div><section class="chapter" id="post-install-configuration-overview"><div class="titlepage"><div><div><h1 class="title">Chapter 1. Post-installation configuration overview</h1></div></div></div><p>
			After installing OpenShift Container Platform, a cluster administrator can configure and customize the following components:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Machine
				</li><li class="listitem">
					Bare metal
				</li><li class="listitem">
					Cluster
				</li><li class="listitem">
					Node
				</li><li class="listitem">
					Network
				</li><li class="listitem">
					Storage
				</li><li class="listitem">
					Users
				</li><li class="listitem">
					Alerts and notifications
				</li></ul></div><section class="section" id="post-install-tasks"><div class="titlepage"><div><div><h2 class="title">1.1. Configuration tasks to perform after installation</h2></div></div></div><p>
				Cluster administrators can perform the following post-installation configuration tasks:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#post-install-machine-configuration-tasks">Configure operating system features</a>: Machine Config Operator (MCO) manages <code class="literal">MachineConfig</code> objects. By using MCO, you can perform the following tasks on an OpenShift Container Platform cluster:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Configure nodes by using <code class="literal">MachineConfig</code> objects
							</li><li class="listitem">
								Configure MCO-related custom resources
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#post-install-bare-metal-configuration">Configure bare metal nodes</a>: The Bare Metal Operator (BMO) implements a Kubernetes API for managing bare metal hosts. It maintains an inventory of available bare metal hosts as instances of the BareMetalHost Custom Resource Definition (CRD). The Bare Metal Operator can:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Inspect the host’s hardware details and report them on the corresponding BareMetalHost. This includes information about CPUs, RAM, disks, NICs, and more.
							</li><li class="listitem">
								Inspect the host’s firmware and configure BIOS settings.
							</li><li class="listitem">
								Provision hosts with a desired image.
							</li><li class="listitem">
								Clean a host’s disk contents before or after provisioning.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#post-install-cluster-tasks">Configure cluster features</a>: As a cluster administrator, you can modify the configuration resources of the major features of an OpenShift Container Platform cluster. These features include:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Image registry
							</li><li class="listitem">
								Networking configuration
							</li><li class="listitem">
								Image build behavior
							</li><li class="listitem">
								Identity provider
							</li><li class="listitem">
								The etcd configuration
							</li><li class="listitem">
								Machine set creation to handle the workloads
							</li><li class="listitem">
								Cloud provider credential management
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#configuring-private-cluster">Configure cluster components to be private</a>: By default, the installation program provisions OpenShift Container Platform by using a publicly accessible DNS and endpoints. If you want your cluster to be accessible only from within an internal network, configure the following components to be private:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								DNS
							</li><li class="listitem">
								Ingress Controller
							</li><li class="listitem">
								API server
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#post-install-node-tasks">Perform node operations</a>: By default, OpenShift Container Platform uses Red Hat Enterprise Linux CoreOS (RHCOS) compute machines. As a cluster administrator, you can perform the following operations with the machines in your OpenShift Container Platform cluster:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Add and remove compute machines
							</li><li class="listitem">
								Add and remove taints and tolerations to the nodes
							</li><li class="listitem">
								Configure the maximum number of pods per node
							</li><li class="listitem">
								Enable Device Manager
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#post-install-network-configuration">Configure network</a>: After installing OpenShift Container Platform, you can configure the following:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Ingress cluster traffic
							</li><li class="listitem">
								Node port service range
							</li><li class="listitem">
								Network policy
							</li><li class="listitem">
								Enabling the cluster-wide proxy
							</li></ul></div></li><li class="listitem"><p class="simpara">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#post-install-storage-configuration">Configure storage</a>: By default, containers operate using ephemeral storage or transient local storage. The ephemeral storage has a lifetime limitation. TO store the data for a long time, you must configure persistent storage. You can configure storage by using one of the following methods:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								<span class="strong strong"><strong>Dynamic provisioning</strong></span>: You can dynamically provision storage on demand by defining and creating storage classes that control different levels of storage, including storage access.
							</li><li class="listitem">
								<span class="strong strong"><strong>Static provisioning</strong></span>: You can use Kubernetes persistent volumes to make existing storage available to a cluster. Static provisioning can support various device configurations and mount options.
							</li></ul></div></li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#post-install-preparing-for-users">Configure users</a>: OAuth access tokens allow users to authenticate themselves to the API. As a cluster administrator, you can configure OAuth to perform the following tasks:
					</li><li class="listitem">
						Specify an identity provider
					</li><li class="listitem">
						Use role-based access control to define and supply permissions to users
					</li><li class="listitem">
						Install an Operator from OperatorHub
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#configuring-alert-notifications">Manage alerts and notifications</a>: By default, firing alerts are displayed on the Alerting UI of the web console. You can also configure OpenShift Container Platform to send alert notifications to external systems.
					</li></ul></div></section></section><section class="chapter" id="configuring-private-cluster"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Configuring a private cluster</h1></div></div></div><p>
			After you install an OpenShift Container Platform version 4.13 cluster, you can set some of its core components to be private.
		</p><section class="section" id="private-clusters-about_configuring-private-cluster"><div class="titlepage"><div><div><h2 class="title">2.1. About private clusters</h2></div></div></div><p>
				By default, OpenShift Container Platform is provisioned using publicly-accessible DNS and endpoints. You can set the DNS, Ingress Controller, and API server to private after you deploy your private cluster.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					If the cluster has any public subnets, load balancer services created by administrators might be publicly accessible. To ensure cluster security, verify that these services are explicitly annotated as private.
				</p></div></div><h4 id="private-clusters-about-dns_configuring-private-cluster">DNS</h4><p>
				If you install OpenShift Container Platform on installer-provisioned infrastructure, the installation program creates records in a pre-existing public zone and, where possible, creates a private zone for the cluster’s own DNS resolution. In both the public zone and the private zone, the installation program or cluster creates DNS entries for <code class="literal">*.apps</code>, for the <code class="literal">Ingress</code> object, and <code class="literal">api</code>, for the API server.
			</p><p>
				The <code class="literal">*.apps</code> records in the public and private zone are identical, so when you delete the public zone, the private zone seamlessly provides all DNS resolution for the cluster.
			</p><h4 id="private-clusters-about-ingress-controller_configuring-private-cluster">Ingress Controller</h4><p>
				Because the default <code class="literal">Ingress</code> object is created as public, the load balancer is internet-facing and in the public subnets. You can replace the default Ingress Controller with an internal one.
			</p><h4 id="private-clusters-about-api-server_configuring-private-cluster">API server</h4><p>
				By default, the installation program creates appropriate network load balancers for the API server to use for both internal and external traffic.
			</p><p>
				On Amazon Web Services (AWS), separate public and private load balancers are created. The load balancers are identical except that an additional port is available on the internal one for use within the cluster. Although the installation program automatically creates or destroys the load balancer based on API server requirements, the cluster does not manage or maintain them. As long as you preserve the cluster’s access to the API server, you can manually modify or move the load balancers. For the public load balancer, port 6443 is open and the health check is configured for HTTPS against the <code class="literal">/readyz</code> path.
			</p><p>
				On Google Cloud Platform, a single load balancer is created to manage both internal and external API traffic, so you do not need to modify the load balancer.
			</p><p>
				On Microsoft Azure, both public and private load balancers are created. However, because of limitations in current implementation, you just retain both load balancers in a private cluster.
			</p></section><section class="section" id="private-clusters-setting-dns-private_configuring-private-cluster"><div class="titlepage"><div><div><h2 class="title">2.2. Setting DNS to private</h2></div></div></div><p>
				After you deploy a cluster, you can modify its DNS to use only a private zone.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Review the <code class="literal">DNS</code> custom resource for your cluster:
					</p><pre class="programlisting language-terminal">$ oc get dnses.config.openshift.io/cluster -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: DNS
metadata:
  creationTimestamp: "2019-10-25T18:27:09Z"
  generation: 2
  name: cluster
  resourceVersion: "37966"
  selfLink: /apis/config.openshift.io/v1/dnses/cluster
  uid: 0e714746-f755-11f9-9cb1-02ff55d8f976
spec:
  baseDomain: &lt;base_domain&gt;
  privateZone:
    tags:
      Name: &lt;infrastructure_id&gt;-int
      kubernetes.io/cluster/&lt;infrastructure_id&gt;: owned
  publicZone:
    id: Z2XXXXXXXXXXA4
status: {}</pre>

						</p></div><p class="simpara">
						Note that the <code class="literal">spec</code> section contains both a private and a public zone.
					</p></li><li class="listitem"><p class="simpara">
						Patch the <code class="literal">DNS</code> custom resource to remove the public zone:
					</p><pre class="programlisting language-terminal">$ oc patch dnses.config.openshift.io/cluster --type=merge --patch='{"spec": {"publicZone": null}}'
dns.config.openshift.io/cluster patched</pre><p class="simpara">
						Because the Ingress Controller consults the <code class="literal">DNS</code> definition when it creates <code class="literal">Ingress</code> objects, when you create or modify <code class="literal">Ingress</code> objects, only private records are created.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							DNS records for the existing Ingress objects are not modified when you remove the public zone.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Optional: Review the <code class="literal">DNS</code> custom resource for your cluster and confirm that the public zone was removed:
					</p><pre class="programlisting language-terminal">$ oc get dnses.config.openshift.io/cluster -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: DNS
metadata:
  creationTimestamp: "2019-10-25T18:27:09Z"
  generation: 2
  name: cluster
  resourceVersion: "37966"
  selfLink: /apis/config.openshift.io/v1/dnses/cluster
  uid: 0e714746-f755-11f9-9cb1-02ff55d8f976
spec:
  baseDomain: &lt;base_domain&gt;
  privateZone:
    tags:
      Name: &lt;infrastructure_id&gt;-int
      kubernetes.io/cluster/&lt;infrastructure_id&gt;-wfpg4: owned
status: {}</pre>

						</p></div></li></ol></div></section><section class="section" id="private-clusters-setting-ingress-private_configuring-private-cluster"><div class="titlepage"><div><div><h2 class="title">2.3. Setting the Ingress Controller to private</h2></div></div></div><p>
				After you deploy a cluster, you can modify its Ingress Controller to use only a private zone.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Modify the default Ingress Controller to use only an internal endpoint:
					</p><pre class="programlisting language-terminal">$ oc replace --force --wait --filename - &lt;&lt;EOF
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  namespace: openshift-ingress-operator
  name: default
spec:
  endpointPublishingStrategy:
    type: LoadBalancerService
    loadBalancer:
      scope: Internal
EOF</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">ingresscontroller.operator.openshift.io "default" deleted
ingresscontroller.operator.openshift.io/default replaced</pre>

						</p></div><p class="simpara">
						The public DNS entry is removed, and the private zone entry is updated.
					</p></li></ol></div></section><section class="section" id="private-clusters-setting-api-private_configuring-private-cluster"><div class="titlepage"><div><div><h2 class="title">2.4. Restricting the API server to private</h2></div></div></div><p>
				After you deploy a cluster to Amazon Web Services (AWS) or Microsoft Azure, you can reconfigure the API server to use only the private zone.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						Have access to the web console as a user with <code class="literal">admin</code> privileges.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						In the web portal or console for your cloud provider, take the following actions:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Locate and delete the appropriate load balancer component:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										For AWS, delete the external load balancer. The API DNS entry in the private zone already points to the internal load balancer, which uses an identical configuration, so you do not need to modify the internal load balancer.
									</li><li class="listitem">
										For Azure, delete the <code class="literal">api-internal</code> rule for the load balancer.
									</li></ul></div></li><li class="listitem">
								Delete the <code class="literal">api.$clustername.$yourdomain</code> DNS entry in the public zone.
							</li></ol></div></li><li class="listitem"><p class="simpara">
						Remove the external load balancers:
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							You can run the following steps only for an installer-provisioned infrastructure (IPI) cluster. For a user-provisioned infrastructure (UPI) cluster, you must manually remove or disable the external load balancers.
						</p></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								If your cluster uses a control plane machine set, delete the following lines in the control plane machine set custom resource:
							</p><pre class="programlisting language-yaml">providerSpec:
  value:
    loadBalancers:
    - name: lk4pj-ext <span id="CO1-1"><!--Empty--></span><span class="callout">1</span>
      type: network <span id="CO1-2"><!--Empty--></span><span class="callout">2</span>
    - name: lk4pj-int
      type: network</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO1-1"><span class="callout">1</span></a> <a href="#CO1-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Delete this line.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								If your cluster does not use a control plane machine set, you must delete the external load balancers from each control plane machine.
							</p><div class="orderedlist"><ol class="orderedlist" type="i"><li class="listitem"><p class="simpara">
										From your terminal, list the cluster machines by running the following command:
									</p><pre class="programlisting language-terminal">$ oc get machine -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">NAME                            STATE     TYPE        REGION      ZONE         AGE
lk4pj-master-0                  running   m4.xlarge   us-east-1   us-east-1a   17m
lk4pj-master-1                  running   m4.xlarge   us-east-1   us-east-1b   17m
lk4pj-master-2                  running   m4.xlarge   us-east-1   us-east-1a   17m
lk4pj-worker-us-east-1a-5fzfj   running   m4.xlarge   us-east-1   us-east-1a   15m
lk4pj-worker-us-east-1a-vbghs   running   m4.xlarge   us-east-1   us-east-1a   15m
lk4pj-worker-us-east-1b-zgpzg   running   m4.xlarge   us-east-1   us-east-1b   15m</pre>

										</p></div><p class="simpara">
										The control plane machines contain <code class="literal">master</code> in the name.
									</p></li><li class="listitem"><p class="simpara">
										Remove the external load balancer from each control plane machine:
									</p><div class="orderedlist"><ol class="orderedlist" type="A"><li class="listitem"><p class="simpara">
												Edit a control plane machine object to by running the following command:
											</p><pre class="programlisting language-terminal">$ oc edit machines -n openshift-machine-api &lt;control_plane_name&gt; <span id="CO2-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO2-1"><span class="callout">1</span></a> </dt><dd><div class="para">
														Specify the name of the control plane machine object to modify.
													</div></dd></dl></div></li><li class="listitem"><p class="simpara">
												Remove the lines that describe the external load balancer, which are marked in the following example:
											</p><pre class="programlisting language-yaml">providerSpec:
  value:
    loadBalancers:
    - name: lk4pj-ext <span id="CO3-1"><!--Empty--></span><span class="callout">1</span>
      type: network <span id="CO3-2"><!--Empty--></span><span class="callout">2</span>
    - name: lk4pj-int
      type: network</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO3-1"><span class="callout">1</span></a> <a href="#CO3-2"><span class="callout">2</span></a> </dt><dd><div class="para">
														Delete this line.
													</div></dd></dl></div></li><li class="listitem">
												Save your changes and exit the object specification.
											</li><li class="listitem">
												Repeat this process for each of the control plane machines.
											</li></ol></div></li></ol></div></li></ul></div></li></ol></div><section class="section" id="nw-ingresscontroller-change-internal_configuring-private-cluster"><div class="titlepage"><div><div><h3 class="title">2.4.1. Configuring the Ingress Controller endpoint publishing scope to Internal</h3></div></div></div><p>
					When a cluster administrator installs a new cluster without specifying that the cluster is private, the default Ingress Controller is created with a <code class="literal">scope</code> set to <code class="literal">External</code>. Cluster administrators can change an <code class="literal">External</code> scoped Ingress Controller to <code class="literal">Internal</code>.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You installed the <code class="literal">oc</code> CLI.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To change an <code class="literal">External</code> scoped Ingress Controller to <code class="literal">Internal</code>, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress-operator patch ingresscontrollers/default --type=merge --patch='{"spec":{"endpointPublishingStrategy":{"type":"LoadBalancerService","loadBalancer":{"scope":"Internal"}}}}'</pre></li><li class="listitem"><p class="simpara">
							To check the status of the Ingress Controller, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress-operator get ingresscontrollers/default -o yaml</pre><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
									The <code class="literal">Progressing</code> status condition indicates whether you must take further action. For example, the status condition can indicate that you need to delete the service by entering the following command:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress delete services/router-default</pre><p class="simpara">
									If you delete the service, the Ingress Operator recreates it as <code class="literal">Internal</code>.
								</p></li></ul></div></li></ul></div></section></section></section><section class="chapter" id="post-install-bare-metal-configuration"><div class="titlepage"><div><div><h1 class="title">Chapter 3. Bare metal configuration</h1></div></div></div><p>
			When deploying OpenShift Container Platform on bare metal hosts, there are times when you need to make changes to the host either before or after provisioning. This can include inspecting the host’s hardware, firmware, and firmware details. It can also include formatting disks or changing modifiable firmware settings.
		</p><section class="section" id="bmo-about-the-bare-metal-operator_post-install-bare-metal-configuration"><div class="titlepage"><div><div><h2 class="title">3.1. About the Bare Metal Operator</h2></div></div></div><p>
				Use the Bare Metal Operator (BMO) to provision, manage, and inspect bare-metal hosts in your cluster.
			</p><p>
				The BMO uses three resources to complete these tasks:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">BareMetalHost</code>
					</li><li class="listitem">
						<code class="literal">HostFirmwareSettings</code>
					</li><li class="listitem">
						<code class="literal">FirmwareSchema</code>
					</li></ul></div><p>
				The BMO maintains an inventory of the physical hosts in the cluster by mapping each bare-metal host to an instance of the <code class="literal">BareMetalHost</code> custom resource definition. Each <code class="literal">BareMetalHost</code> resource features hardware, software, and firmware details. The BMO continually inspects the bare-metal hosts in the cluster to ensure each <code class="literal">BareMetalHost</code> resource accurately details the components of the corresponding host.
			</p><p>
				The BMO also uses the <code class="literal">HostFirmwareSettings</code> resource and the <code class="literal">FirmwareSchema</code> resource to detail firmware specifications for the bare-metal host.
			</p><p>
				The BMO interfaces with bare-metal hosts in the cluster by using the Ironic API service. The Ironic service uses the Baseboard Management Controller (BMC) on the host to interface with the machine.
			</p><p>
				Some common tasks you can complete by using the BMO include the following:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Provision bare-metal hosts to the cluster with a specific image
					</li><li class="listitem">
						Format a host’s disk contents before provisioning or after deprovisioning
					</li><li class="listitem">
						Turn on or off a host
					</li><li class="listitem">
						Change firmware settings
					</li><li class="listitem">
						View the host’s hardware details
					</li></ul></div><section class="section" id="bmo-bare-metal-operator-architecture_post-install-bare-metal-configuration"><div class="titlepage"><div><div><h3 class="title">3.1.1. Bare Metal Operator architecture</h3></div></div></div><p>
					The Bare Metal Operator (BMO) uses three resources to provision, manage, and inspect bare-metal hosts in your cluster. The following diagram illustrates the architecture of these resources:
				</p><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Post-installation_configuration-en-US/images/c5fdd5b713e31083d81ea4a39d08942c/302_OpenShift_Bare_Metal_Operator_0223.png" alt="BMO architecture overview"/></div></div><div class="formalpara"><p class="title"><strong>BareMetalHost</strong></p><p>
						The <code class="literal">BareMetalHost</code> resource defines a physical host and its properties. When you provision a bare-metal host to the cluster, you must define a <code class="literal">BareMetalHost</code> resource for that host. For ongoing management of the host, you can inspect the information in the <code class="literal">BareMetalHost</code> or update this information.
					</p></div><p>
					The <code class="literal">BareMetalHost</code> resource features provisioning information such as the following:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Deployment specifications such as the operating system boot image or the custom RAM disk
						</li><li class="listitem">
							Provisioning state
						</li><li class="listitem">
							Baseboard Management Controller (BMC) address
						</li><li class="listitem">
							Desired power state
						</li></ul></div><p>
					The <code class="literal">BareMetalHost</code> resource features hardware information such as the following:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Number of CPUs
						</li><li class="listitem">
							MAC address of a NIC
						</li><li class="listitem">
							Size of the host’s storage device
						</li><li class="listitem">
							Current power state
						</li></ul></div><div class="formalpara"><p class="title"><strong>HostFirmwareSettings</strong></p><p>
						You can use the <code class="literal">HostFirmwareSettings</code> resource to retrieve and manage the firmware settings for a host. When a host moves to the <code class="literal">Available</code> state, the Ironic service reads the host’s firmware settings and creates the <code class="literal">HostFirmwareSettings</code> resource. There is a one-to-one mapping between the <code class="literal">BareMetalHost</code> resource and the <code class="literal">HostFirmwareSettings</code> resource.
					</p></div><p>
					You can use the <code class="literal">HostFirmwareSettings</code> resource to inspect the firmware specifications for a host or to update a host’s firmware specifications.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You must adhere to the schema specific to the vendor firmware when you edit the <code class="literal">spec</code> field of the <code class="literal">HostFirmwareSettings</code> resource. This schema is defined in the read-only <code class="literal">FirmwareSchema</code> resource.
					</p></div></div><div class="formalpara"><p class="title"><strong>FirmwareSchema</strong></p><p>
						Firmware settings vary among hardware vendors and host models. A <code class="literal">FirmwareSchema</code> resource is a read-only resource that contains the types and limits for each firmware setting on each host model. The data comes directly from the BMC by using the Ironic service. The <code class="literal">FirmwareSchema</code> resource enables you to identify valid values you can specify in the <code class="literal">spec</code> field of the <code class="literal">HostFirmwareSettings</code> resource.
					</p></div><p>
					A <code class="literal">FirmwareSchema</code> resource can apply to many <code class="literal">BareMetalHost</code> resources if the schema is the same.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://metal3.io/">Metal³ API service for provisioning bare-metal hosts</a>
						</li><li class="listitem">
							<a class="link" href="https://ironicbaremetal.org/">Ironic API service for managing bare-metal infrastructure</a>
						</li></ul></div></section></section><section class="section" id="about-the-baremetalhost-resource_post-install-bare-metal-configuration"><div class="titlepage"><div><div><h2 class="title">3.2. About the BareMetalHost resource</h2></div></div></div><p>
				Metal<sup>3</sup> introduces the concept of the <code class="literal">BareMetalHost</code> resource, which defines a physical host and its properties. The <code class="literal">BareMetalHost</code> resource contains two sections:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						The <code class="literal">BareMetalHost</code> spec
					</li><li class="listitem">
						The <code class="literal">BareMetalHost</code> status
					</li></ol></div><section class="section" id="the-baremetalhost-spec"><div class="titlepage"><div><div><h3 class="title">3.2.1. The BareMetalHost spec</h3></div></div></div><p>
					The <code class="literal">spec</code> section of the <code class="literal">BareMetalHost</code> resource defines the desired state of the host.
				</p><div class="table" id="idm140031662817664"><p class="title"><strong>Table 3.1. BareMetalHost spec</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031650024928" scope="col">Parameters</th><th align="left" valign="top" id="idm140031650023840" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031650024928"> <p>
									<code class="literal">automatedCleaningMode</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031650023840"> <p>
									An interface to enable or disable automated cleaning during provisioning and de-provisioning. When set to <code class="literal">disabled</code>, it skips automated cleaning. When set to <code class="literal">metadata</code>, automated cleaning is enabled. The default setting is <code class="literal">metadata</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031650024928">
<pre class="screen">bmc:
  address:
  credentialsName:
  disableCertificateVerification:</pre>
								 </td><td align="left" valign="top" headers="idm140031650023840"> <p>
									The <code class="literal">bmc</code> configuration setting contains the connection information for the baseboard management controller (BMC) on the host. The fields are:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">address</code>: The URL for communicating with the host’s BMC controller.
										</li><li class="listitem">
											<code class="literal">credentialsName</code>: A reference to a secret containing the username and password for the BMC.
										</li><li class="listitem">
											<code class="literal">disableCertificateVerification</code>: A boolean to skip certificate validation when set to <code class="literal">true</code>.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031650024928"> <p>
									<code class="literal">bootMACAddress</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031650023840"> <p>
									The MAC address of the NIC used for provisioning the host.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031650024928"> <p>
									<code class="literal">bootMode</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031650023840"> <p>
									The boot mode of the host. It defaults to <code class="literal">UEFI</code>, but it can also be set to <code class="literal">legacy</code> for BIOS boot, or <code class="literal">UEFISecureBoot</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031650024928"> <p>
									<code class="literal">consumerRef</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031650023840"> <p>
									A reference to another resource that is using the host. It could be empty if another resource is not currently using the host. For example, a <code class="literal">Machine</code> resource might use the host when the <code class="literal">machine-api</code> is using the host.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031650024928"> <p>
									<code class="literal">description</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031650023840"> <p>
									A human-provided string to help identify the host.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031650024928"> <p>
									<code class="literal">externallyProvisioned</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031650023840"> <p>
									A boolean indicating whether the host provisioning and deprovisioning are managed externally. When set:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Power status can still be managed using the online field.
										</li><li class="listitem">
											Hardware inventory will be monitored, but no provisioning or deprovisioning operations are performed on the host.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031650024928"> <p>
									<code class="literal">firmware</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031650023840"> <p>
									Contains information about the BIOS configuration of bare metal hosts. Currently, <code class="literal">firmware</code> is only supported by iRMC, iDRAC, iLO4 and iLO5 BMCs. The sub fields are:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">simultaneousMultithreadingEnabled</code>: Allows a single physical processor core to appear as several logical processors. Valid settings are <code class="literal">true</code> or <code class="literal">false</code>.
										</li><li class="listitem">
											<code class="literal">sriovEnabled</code>: SR-IOV support enables a hypervisor to create virtual instances of a PCI-express device, potentially increasing performance. Valid settings are <code class="literal">true</code> or <code class="literal">false</code>.
										</li><li class="listitem">
											<code class="literal">virtualizationEnabled</code>: Supports the virtualization of platform hardware. Valid settings are <code class="literal">true</code> or <code class="literal">false</code>.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031650024928">
<pre class="screen">image:
  url:
  checksum:
  checksumType:
  format:</pre>
								 </td><td align="left" valign="top" headers="idm140031650023840"> <p>
									The <code class="literal">image</code> configuration setting holds the details for the image to be deployed on the host. Ironic requires the image fields. However, when the <code class="literal">externallyProvisioned</code> configuration setting is set to <code class="literal">true</code> and the external management doesn’t require power control, the fields can be empty. The fields are:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">url</code>: The URL of an image to deploy to the host.
										</li><li class="listitem">
											<code class="literal">checksum</code>: The actual checksum or a URL to a file containing the checksum for the image at <code class="literal">image.url</code>.
										</li><li class="listitem">
											<code class="literal">checksumType</code>: You can specify checksum algorithms. Currently <code class="literal">image.checksumType</code> only supports <code class="literal">md5</code>, <code class="literal">sha256</code>, and <code class="literal">sha512</code>. The default checksum type is <code class="literal">md5</code>.
										</li><li class="listitem">
											<code class="literal">format</code>: This is the disk format of the image. It can be one of <code class="literal">raw</code>, <code class="literal">qcow2</code>, <code class="literal">vdi</code>, <code class="literal">vmdk</code>, <code class="literal">live-iso</code> or be left unset. Setting it to <code class="literal">raw</code> enables raw image streaming in the Ironic agent for that image. Setting it to <code class="literal">live-iso</code> enables iso images to live boot without deploying to disk, and it ignores the <code class="literal">checksum</code> fields.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031650024928"> <p>
									<code class="literal">networkData</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031650023840"> <p>
									A reference to the secret containing the network configuration data and its namespace, so that it can be attached to the host before the host boots to set up the network.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031650024928"> <p>
									<code class="literal">online</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031650023840"> <p>
									A boolean indicating whether the host should be powered on (<code class="literal">true</code>) or off (<code class="literal">false</code>). Changing this value will trigger a change in the power state of the physical host.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031650024928">
<pre class="screen">raid:
  hardwareRAIDVolumes:
  softwareRAIDVolumes:</pre>
								 </td><td align="left" valign="top" headers="idm140031650023840"> <p>
									(Optional) Contains the information about the RAID configuration for bare metal hosts. If not specified, it retains the current configuration.
								</p>
								 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
										OpenShift Container Platform 4.13 supports hardware RAID for BMCs using the iRMC protocol only. OpenShift Container Platform 4.13 does not support software RAID.
									</p></div></div>
								 <p>
									See the following configuration settings:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
											<code class="literal">hardwareRAIDVolumes</code>: Contains the list of logical drives for hardware RAID, and defines the desired volume configuration in the hardware RAID. If you don’t specify <code class="literal">rootDeviceHints</code>, the first volume is the root volume. The sub-fields are:
										</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
													<code class="literal">level</code>: The RAID level for the logical drive. The following levels are supported: <code class="literal">0</code>,<code class="literal">1</code>,<code class="literal">2</code>,<code class="literal">5</code>,<code class="literal">6</code>,<code class="literal">1+0</code>,<code class="literal">5+0</code>,<code class="literal">6+0</code>.
												</li><li class="listitem">
													<code class="literal">name</code>: The name of the volume as a string. It should be unique within the server. If not specified, the volume name will be auto-generated.
												</li><li class="listitem">
													<code class="literal">numberOfPhysicalDisks</code>: The number of physical drives as an integer to use for the logical drove. Defaults to the minimum number of disk drives required for the particular RAID level.
												</li><li class="listitem">
													<code class="literal">physicalDisks</code>: The list of names of physical disk drives as a string. This is an optional field. If specified, the controller field must be specified too.
												</li><li class="listitem">
													<code class="literal">controller</code>: (Optional) The name of the RAID controller as a string to use in the hardware RAID volume.
												</li><li class="listitem">
													<code class="literal">rotational</code>: If set to <code class="literal">true</code>, it will only select rotational disk drives. If set to <code class="literal">false</code>, it will only select solid-state and NVMe drives. If not set, it selects any drive types, which is the default behavior.
												</li><li class="listitem">
													<code class="literal">sizeGibibytes</code>: The size of the logical drive as an integer to create in GiB. If unspecified or set to <code class="literal">0</code>, it will use the maximum capacity of physical drive for the logical drive.
												</li></ul></div></li><li class="listitem"><p class="simpara">
											<code class="literal">softwareRAIDVolumes</code>: OpenShift Container Platform 4.13 does not support software RAID. The following information is for reference only. This configuration contains the list of logical disks for software RAID. If you don’t specify <code class="literal">rootDeviceHints</code>, the first volume is the root volume. If you set <code class="literal">HardwareRAIDVolumes</code>, this item will be invalid. Software RAIDs will always be deleted. The number of created software RAID devices must be <code class="literal">1</code> or <code class="literal">2</code>. If there is only one software RAID device, it must be <code class="literal">RAID-1</code>. If there are two RAID devices, the first device must be <code class="literal">RAID-1</code>, while the RAID level for the second device can be <code class="literal">0</code>, <code class="literal">1</code>, or <code class="literal">1+0</code>. The first RAID device will be the deployment device. Therefore, enforcing <code class="literal">RAID-1</code> reduces the risk of a non-booting node in case of a device failure. The <code class="literal">softwareRAIDVolume</code> field defines the desired configuration of the volume in the software RAID. The sub-fields are:
										</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
													<code class="literal">level</code>: The RAID level for the logical drive. The following levels are supported: <code class="literal">0</code>,<code class="literal">1</code>,<code class="literal">1+0</code>.
												</li><li class="listitem">
													<code class="literal">physicalDisks</code>: A list of device hints. The number of items should be greater than or equal to <code class="literal">2</code>.
												</li><li class="listitem">
													<code class="literal">sizeGibibytes</code>: The size of the logical disk drive as an integer to be created in GiB. If unspecified or set to <code class="literal">0</code>, it will use the maximum capacity of physical drive for logical drive.
												</li></ul></div></li></ul></div>
								 <p>
									You can set the <code class="literal">hardwareRAIDVolume</code> as an empty slice to clear the hardware RAID configuration. For example:
								</p>
								 
<pre class="screen">spec:
   raid:
     hardwareRAIDVolume: []</pre>
								 <p>
									If you receive an error message indicating that the driver does not support RAID, set the <code class="literal">raid</code>, <code class="literal">hardwareRAIDVolumes</code> or <code class="literal">softwareRAIDVolumes</code> to nil. You might need to ensure the host has a RAID controller.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031650024928">
<pre class="screen">rootDeviceHints:
  deviceName:
  hctl:
  model:
  vendor:
  serialNumber:
  minSizeGigabytes:
  wwn:
  wwnWithExtension:
  wwnVendorExtension:
  rotational:</pre>
								 </td><td align="left" valign="top" headers="idm140031650023840"> <p>
									The <code class="literal">rootDeviceHints</code> parameter enables provisioning of the RHCOS image to a particular device. It examines the devices in the order it discovers them, and compares the discovered values with the hint values. It uses the first discovered device that matches the hint value. The configuration can combine multiple hints, but a device must match all hints to get selected. The fields are:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">deviceName</code>: A string containing a Linux device name like <code class="literal">/dev/vda</code>. The hint must match the actual value exactly.
										</li><li class="listitem">
											<code class="literal">hctl</code>: A string containing a SCSI bus address like <code class="literal">0:0:0:0</code>. The hint must match the actual value exactly.
										</li><li class="listitem">
											<code class="literal">model</code>: A string containing a vendor-specific device identifier. The hint can be a substring of the actual value.
										</li><li class="listitem">
											<code class="literal">vendor</code>: A string containing the name of the vendor or manufacturer of the device. The hint can be a sub-string of the actual value.
										</li><li class="listitem">
											<code class="literal">serialNumber</code>: A string containing the device serial number. The hint must match the actual value exactly.
										</li><li class="listitem">
											<code class="literal">minSizeGigabytes</code>: An integer representing the minimum size of the device in gigabytes.
										</li><li class="listitem">
											<code class="literal">wwn</code>: A string containing the unique storage identifier. The hint must match the actual value exactly.
										</li><li class="listitem">
											<code class="literal">wwnWithExtension</code>: A string containing the unique storage identifier with the vendor extension appended. The hint must match the actual value exactly.
										</li><li class="listitem">
											<code class="literal">wwnVendorExtension</code>: A string containing the unique vendor storage identifier. The hint must match the actual value exactly.
										</li><li class="listitem">
											<code class="literal">rotational</code>: A boolean indicating whether the device should be a rotating disk (true) or not (false).
										</li></ul></div>
								 </td></tr></tbody></table></div></div></section><section class="section" id="the-baremetalhost-status"><div class="titlepage"><div><div><h3 class="title">3.2.2. The BareMetalHost status</h3></div></div></div><p>
					The <code class="literal">BareMetalHost</code> status represents the host’s current state, and includes tested credentials, current hardware details, and other information.
				</p><div class="table" id="idm140031659382064"><p class="title"><strong>Table 3.2. BareMetalHost status</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031658159072" scope="col">Parameters</th><th align="left" valign="top" id="idm140031658157984" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031658159072"> <p>
									<code class="literal">goodCredentials</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031658157984"> <p>
									A reference to the secret and its namespace holding the last set of baseboard management controller (BMC) credentials the system was able to validate as working.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031658159072"> <p>
									<code class="literal">errorMessage</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031658157984"> <p>
									Details of the last error reported by the provisioning backend, if any.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031658159072"> <p>
									<code class="literal">errorType</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031658157984"> <p>
									Indicates the class of problem that has caused the host to enter an error state. The error types are:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">provisioned registration error</code>: Occurs when the controller is unable to re-register an already provisioned host.
										</li><li class="listitem">
											<code class="literal">registration error</code>: Occurs when the controller is unable to connect to the host’s baseboard management controller.
										</li><li class="listitem">
											<code class="literal">inspection error</code>: Occurs when an attempt to obtain hardware details from the host fails.
										</li><li class="listitem">
											<code class="literal">preparation error</code>: Occurs when cleaning fails.
										</li><li class="listitem">
											<code class="literal">provisioning error</code>: Occurs when the controller fails to provision or deprovision the host.
										</li><li class="listitem">
											<code class="literal">power management error</code>: Occurs when the controller is unable to modify the power state of the host.
										</li><li class="listitem">
											<code class="literal">detach error</code>: Occurs when the controller is unable to detatch the host from the provisioner.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031658159072">
<pre class="screen">hardware:
  cpu
    arch:
    model:
    clockMegahertz:
    flags:
    count:</pre>
								 </td><td align="left" valign="top" headers="idm140031658157984"> <p>
									The <code class="literal">hardware.cpu</code> field details of the CPU(s) in the system. The fields include:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">arch</code>: The architecture of the CPU.
										</li><li class="listitem">
											<code class="literal">model</code>: The CPU model as a string.
										</li><li class="listitem">
											<code class="literal">clockMegahertz</code>: The speed in MHz of the CPU.
										</li><li class="listitem">
											<code class="literal">flags</code>: The list of CPU flags. For example, <code class="literal">'mmx','sse','sse2','vmx'</code> etc.
										</li><li class="listitem">
											<code class="literal">count</code>: The number of CPUs available in the system.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031658159072">
<pre class="screen">hardware:
  firmware:</pre>
								 </td><td align="left" valign="top" headers="idm140031658157984"> <p>
									Contains BIOS firmware information. For example, the hardware vendor and version.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031658159072">
<pre class="screen">hardware:
  nics:
  - ip:
    name:
    mac:
    speedGbps:
    vlans:
    vlanId:
    pxe:</pre>
								 </td><td align="left" valign="top" headers="idm140031658157984"> <p>
									The <code class="literal">hardware.nics</code> field contains a list of network interfaces for the host. The fields include:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">ip</code>: The IP address of the NIC, if one was assigned when the discovery agent ran.
										</li><li class="listitem">
											<code class="literal">name</code>: A string identifying the network device. For example, <code class="literal">nic-1</code>.
										</li><li class="listitem">
											<code class="literal">mac</code>: The MAC address of the NIC.
										</li><li class="listitem">
											<code class="literal">speedGbps</code>: The speed of the device in Gbps.
										</li><li class="listitem">
											<code class="literal">vlans</code>: A list holding all the VLANs available for this NIC.
										</li><li class="listitem">
											<code class="literal">vlanId</code>: The untagged VLAN ID.
										</li><li class="listitem">
											<code class="literal">pxe</code>: Whether the NIC is able to boot using PXE.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031658159072">
<pre class="screen">hardware:
  ramMebibytes:</pre>
								 </td><td align="left" valign="top" headers="idm140031658157984"> <p>
									The host’s amount of memory in Mebibytes (MiB).
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031658159072">
<pre class="screen">hardware:
  storage:
  - name:
    rotational:
    sizeBytes:
    serialNumber:</pre>
								 </td><td align="left" valign="top" headers="idm140031658157984"> <p>
									The <code class="literal">hardware.storage</code> field contains a list of storage devices available to the host. The fields include:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">name</code>: A string identifying the storage device. For example, <code class="literal">disk 1 (boot)</code>.
										</li><li class="listitem">
											<code class="literal">rotational</code>: Indicates whether the disk is rotational, and returns either <code class="literal">true</code> or <code class="literal">false</code>.
										</li><li class="listitem">
											<code class="literal">sizeBytes</code>: The size of the storage device.
										</li><li class="listitem">
											<code class="literal">serialNumber</code>: The device’s serial number.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031658159072">
<pre class="screen">hardware:
  systemVendor:
    manufacturer:
    productName:
    serialNumber:</pre>
								 </td><td align="left" valign="top" headers="idm140031658157984"> <p>
									Contains information about the host’s <code class="literal">manufacturer</code>, the <code class="literal">productName</code>, and the <code class="literal">serialNumber</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031658159072"> <p>
									<code class="literal">lastUpdated</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031658157984"> <p>
									The timestamp of the last time the status of the host was updated.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031658159072"> <p>
									<code class="literal">operationalStatus</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031658157984"> <p>
									The status of the server. The status is one of the following:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">OK</code>: Indicates all the details for the host are known, correctly configured, working, and manageable.
										</li><li class="listitem">
											<code class="literal">discovered</code>: Implies some of the host’s details are either not working correctly or missing. For example, the BMC address is known but the login credentials are not.
										</li><li class="listitem">
											<code class="literal">error</code>: Indicates the system found some sort of irrecoverable error. Refer to the <code class="literal">errorMessage</code> field in the status section for more details.
										</li><li class="listitem">
											<code class="literal">delayed</code>: Indicates that provisioning is delayed to limit simultaneous provisioning of multiple hosts.
										</li><li class="listitem">
											<code class="literal">detached</code>: Indicates the host is marked <code class="literal">unmanaged</code>.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031658159072"> <p>
									<code class="literal">poweredOn</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031658157984"> <p>
									Boolean indicating whether the host is powered on.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031658159072">
<pre class="screen">provisioning:
  state:
  id:
  image:
  raid:
  firmware:
  rootDeviceHints:</pre>
								 </td><td align="left" valign="top" headers="idm140031658157984"> <p>
									The <code class="literal">provisioning</code> field contains values related to deploying an image to the host. The sub-fields include:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
											<code class="literal">state</code>: The current state of any ongoing provisioning operation. The states include:
										</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
													<code class="literal">&lt;empty string&gt;</code>: There is no provisioning happening at the moment.
												</li><li class="listitem">
													<code class="literal">unmanaged</code>: There is insufficient information available to register the host.
												</li><li class="listitem">
													<code class="literal">registering</code>: The agent is checking the host’s BMC details.
												</li><li class="listitem">
													<code class="literal">match profile</code>: The agent is comparing the discovered hardware details on the host against known profiles.
												</li><li class="listitem">
													<code class="literal">available</code>: The host is available for provisioning. This state was previously known as <code class="literal">ready</code>.
												</li><li class="listitem">
													<code class="literal">preparing</code>: The existing configuration will be removed, and the new configuration will be set on the host.
												</li><li class="listitem">
													<code class="literal">provisioning</code>: The provisioner is writing an image to the host’s storage.
												</li><li class="listitem">
													<code class="literal">provisioned</code>: The provisioner wrote an image to the host’s storage.
												</li><li class="listitem">
													<code class="literal">externally provisioned</code>: Metal<sup>3</sup> does not manage the image on the host.
												</li><li class="listitem">
													<code class="literal">deprovisioning</code>: The provisioner is wiping the image from the host’s storage.
												</li><li class="listitem">
													<code class="literal">inspecting</code>: The agent is collecting hardware details for the host.
												</li><li class="listitem">
													<code class="literal">deleting</code>: The agent is deleting the from the cluster.
												</li></ul></div></li><li class="listitem">
											<code class="literal">id</code>: The unique identifier for the service in the underlying provisioning tool.
										</li><li class="listitem">
											<code class="literal">image</code>: The image most recently provisioned to the host.
										</li><li class="listitem">
											<code class="literal">raid</code>: The list of hardware or software RAID volumes recently set.
										</li><li class="listitem">
											<code class="literal">firmware</code>: The BIOS configuration for the bare metal server.
										</li><li class="listitem">
											<code class="literal">rootDeviceHints</code>: The root device selection instructions used for the most recent provisioning operation.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031658159072"> <p>
									<code class="literal">triedCredentials</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031658157984"> <p>
									A reference to the secret and its namespace holding the last set of BMC credentials that were sent to the provisioning backend.
								</p>
								 </td></tr></tbody></table></div></div></section></section><section class="section" id="getting-the-baremetalhost-resource_post-install-bare-metal-configuration"><div class="titlepage"><div><div><h2 class="title">3.3. Getting the BareMetalHost resource</h2></div></div></div><p>
				The <code class="literal">BareMetalHost</code> resource contains the properties of a physical host. You must get the <code class="literal">BareMetalHost</code> resource for a physical host to review its properties.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Get the list of <code class="literal">BareMetalHost</code> resources:
					</p><pre class="programlisting language-terminal">$ oc get bmh -n openshift-machine-api -o yaml</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You can use <code class="literal">baremetalhost</code> as the long form of <code class="literal">bmh</code> with <code class="literal">oc get</code> command.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Get the list of hosts:
					</p><pre class="programlisting language-terminal">$ oc get bmh -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
						Get the <code class="literal">BareMetalHost</code> resource for a specific host:
					</p><pre class="programlisting language-terminal">$ oc get bmh &lt;host_name&gt; -n openshift-machine-api -o yaml</pre><p class="simpara">
						Where <code class="literal">&lt;host_name&gt;</code> is the name of the host.
					</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  creationTimestamp: "2022-06-16T10:48:33Z"
  finalizers:
  - baremetalhost.metal3.io
  generation: 2
  name: openshift-worker-0
  namespace: openshift-machine-api
  resourceVersion: "30099"
  uid: 1513ae9b-e092-409d-be1b-ad08edeb1271
spec:
  automatedCleaningMode: metadata
  bmc:
    address: redfish://10.46.61.19:443/redfish/v1/Systems/1
    credentialsName: openshift-worker-0-bmc-secret
    disableCertificateVerification: true
  bootMACAddress: 48:df:37:c7:f7:b0
  bootMode: UEFI
  consumerRef:
    apiVersion: machine.openshift.io/v1beta1
    kind: Machine
    name: ocp-edge-958fk-worker-0-nrfcg
    namespace: openshift-machine-api
  customDeploy:
    method: install_coreos
  hardwareProfile: unknown
  online: true
  rootDeviceHints:
    deviceName: /dev/disk/by-id/scsi-&lt;serial_number&gt;
  userData:
    name: worker-user-data-managed
    namespace: openshift-machine-api
status:
  errorCount: 0
  errorMessage: ""
  goodCredentials:
    credentials:
      name: openshift-worker-0-bmc-secret
      namespace: openshift-machine-api
    credentialsVersion: "16120"
  hardware:
    cpu:
      arch: x86_64
      clockMegahertz: 2300
      count: 64
      flags:
      - 3dnowprefetch
      - abm
      - acpi
      - adx
      - aes
      model: Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz
    firmware:
      bios:
        date: 10/26/2020
        vendor: HPE
        version: U30
    hostname: openshift-worker-0
    nics:
    - mac: 48:df:37:c7:f7:b3
      model: 0x8086 0x1572
      name: ens1f3
    ramMebibytes: 262144
    storage:
    - hctl: "0:0:0:0"
      model: VK000960GWTTB
      name: /dev/disk/by-id/scsi-&lt;serial_number&gt;
      sizeBytes: 960197124096
      type: SSD
      vendor: ATA
    systemVendor:
      manufacturer: HPE
      productName: ProLiant DL380 Gen10 (868703-B21)
      serialNumber: CZ200606M3
  hardwareProfile: unknown
  lastUpdated: "2022-06-16T11:41:42Z"
  operationalStatus: OK
  poweredOn: true
  provisioning:
    ID: 217baa14-cfcf-4196-b764-744e184a3413
    bootMode: UEFI
    customDeploy:
      method: install_coreos
    image:
      url: ""
    raid:
      hardwareRAIDVolumes: null
      softwareRAIDVolumes: []
    rootDeviceHints:
      deviceName: /dev/disk/by-id/scsi-&lt;serial_number&gt;
    state: provisioned
  triedCredentials:
    credentials:
      name: openshift-worker-0-bmc-secret
      namespace: openshift-machine-api
    credentialsVersion: "16120"</pre>

						</p></div></li></ol></div></section><section class="section" id="about-the-hostfirmwaresettings-resource_post-install-bare-metal-configuration"><div class="titlepage"><div><div><h2 class="title">3.4. About the HostFirmwareSettings resource</h2></div></div></div><p>
				You can use the <code class="literal">HostFirmwareSettings</code> resource to retrieve and manage the BIOS settings for a host. When a host moves to the <code class="literal">Available</code> state, Ironic reads the host’s BIOS settings and creates the <code class="literal">HostFirmwareSettings</code> resource. The resource contains the complete BIOS configuration returned from the baseboard management controller (BMC). Whereas, the <code class="literal">firmware</code> field in the <code class="literal">BareMetalHost</code> resource returns three vendor-independent fields, the <code class="literal">HostFirmwareSettings</code> resource typically comprises many BIOS settings of vendor-specific fields per host.
			</p><p>
				The <code class="literal">HostFirmwareSettings</code> resource contains two sections:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						The <code class="literal">HostFirmwareSettings</code> spec.
					</li><li class="listitem">
						The <code class="literal">HostFirmwareSettings</code> status.
					</li></ol></div><section class="section" id="the-literal-hostfirmwaresettings-literal-spec"><div class="titlepage"><div><div><h3 class="title">3.4.1. The <code class="literal">HostFirmwareSettings</code> spec</h3></div></div></div><p>
					The <code class="literal">spec</code> section of the <code class="literal">HostFirmwareSettings</code> resource defines the desired state of the host’s BIOS, and it is empty by default. Ironic uses the settings in the <code class="literal">spec.settings</code> section to update the baseboard management controller (BMC) when the host is in the <code class="literal">Preparing</code> state. Use the <code class="literal">FirmwareSchema</code> resource to ensure that you do not send invalid name/value pairs to hosts. See "About the FirmwareSchema resource" for additional details.
				</p><div class="formalpara"><p class="title"><strong>Example</strong></p><p>
						
<pre class="programlisting language-terminal">spec:
  settings:
    ProcTurboMode: Disabled<span id="CO4-1"><!--Empty--></span><span class="callout">1</span></pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO4-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							In the foregoing example, the <code class="literal">spec.settings</code> section contains a name/value pair that will set the <code class="literal">ProcTurboMode</code> BIOS setting to <code class="literal">Disabled</code>.
						</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Integer parameters listed in the <code class="literal">status</code> section appear as strings. For example, <code class="literal">"1"</code>. When setting integers in the <code class="literal">spec.settings</code> section, the values should be set as integers without quotes. For example, <code class="literal">1</code>.
					</p></div></div></section><section class="section" id="the-literal-hostfirmwaresettings-literal-status"><div class="titlepage"><div><div><h3 class="title">3.4.2. The <code class="literal">HostFirmwareSettings</code> status</h3></div></div></div><p>
					The <code class="literal">status</code> represents the current state of the host’s BIOS.
				</p><div class="table" id="idm140031650662832"><p class="title"><strong>Table 3.3. HostFirmwareSettings</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031652391504" scope="col">Parameters</th><th align="left" valign="top" id="idm140031652390416" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031652391504">
<pre class="screen">status:
  conditions:
  - lastTransitionTime:
    message:
    observedGeneration:
    reason:
    status:
    type:</pre>
								 </td><td align="left" valign="top" headers="idm140031652390416"> <p>
									The <code class="literal">conditions</code> field contains a list of state changes. The sub-fields include:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">lastTransitionTime</code>: The last time the state changed.
										</li><li class="listitem">
											<code class="literal">message</code>: A description of the state change.
										</li><li class="listitem">
											<code class="literal">observedGeneration</code>: The current generation of the <code class="literal">status</code>. If <code class="literal">metadata.generation</code> and this field are not the same, the <code class="literal">status.conditions</code> might be out of date.
										</li><li class="listitem">
											<code class="literal">reason</code>: The reason for the state change.
										</li><li class="listitem">
											<code class="literal">status</code>: The status of the state change. The status can be <code class="literal">True</code>, <code class="literal">False</code> or <code class="literal">Unknown</code>.
										</li><li class="listitem">
											<code class="literal">type</code>: The type of state change. The types are <code class="literal">Valid</code> and <code class="literal">ChangeDetected</code>.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031652391504">
<pre class="screen">status:
  schema:
    name:
    namespace:
    lastUpdated:</pre>
								 </td><td align="left" valign="top" headers="idm140031652390416"> <p>
									The <code class="literal">FirmwareSchema</code> for the firmware settings. The fields include:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">name</code>: The name or unique identifier referencing the schema.
										</li><li class="listitem">
											<code class="literal">namespace</code>: The namespace where the schema is stored.
										</li><li class="listitem">
											<code class="literal">lastUpdated</code>: The last time the resource was updated.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031652391504">
<pre class="screen">status:
  settings:</pre>
								 </td><td align="left" valign="top" headers="idm140031652390416"> <p>
									The <code class="literal">settings</code> field contains a list of name/value pairs of a host’s current BIOS settings.
								</p>
								 </td></tr></tbody></table></div></div></section></section><section class="section" id="getting-the-hostfirmwaresettings-resource_post-install-bare-metal-configuration"><div class="titlepage"><div><div><h2 class="title">3.5. Getting the HostFirmwareSettings resource</h2></div></div></div><p>
				The <code class="literal">HostFirmwareSettings</code> resource contains the vendor-specific BIOS properties of a physical host. You must get the <code class="literal">HostFirmwareSettings</code> resource for a physical host to review its BIOS properties.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Get the detailed list of <code class="literal">HostFirmwareSettings</code> resources:
					</p><pre class="programlisting language-terminal">$ oc get hfs -n openshift-machine-api -o yaml</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You can use <code class="literal">hostfirmwaresettings</code> as the long form of <code class="literal">hfs</code> with the <code class="literal">oc get</code> command.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Get the list of <code class="literal">HostFirmwareSettings</code> resources:
					</p><pre class="programlisting language-terminal">$ oc get hfs -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
						Get the <code class="literal">HostFirmwareSettings</code> resource for a particular host
					</p><pre class="programlisting language-terminal">$ oc get hfs &lt;host_name&gt; -n openshift-machine-api -o yaml</pre><p class="simpara">
						Where <code class="literal">&lt;host_name&gt;</code> is the name of the host.
					</p></li></ol></div></section><section class="section" id="editing-the-hostfirmwaresettings-resource_post-install-bare-metal-configuration"><div class="titlepage"><div><div><h2 class="title">3.6. Editing the HostFirmwareSettings resource</h2></div></div></div><p>
				You can edit the <code class="literal">HostFirmwareSettings</code> of provisioned hosts.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You can only edit hosts when they are in the <code class="literal">provisioned</code> state, excluding read-only values. You cannot edit hosts in the <code class="literal">externally provisioned</code> state.
				</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Get the list of <code class="literal">HostFirmwareSettings</code> resources:
					</p><pre class="programlisting language-terminal">$ oc get hfs -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
						Edit a host’s <code class="literal">HostFirmwareSettings</code> resource:
					</p><pre class="programlisting language-terminal">$ oc edit hfs &lt;host_name&gt; -n openshift-machine-api</pre><p class="simpara">
						Where <code class="literal">&lt;host_name&gt;</code> is the name of a provisioned host. The <code class="literal">HostFirmwareSettings</code> resource will open in the default editor for your terminal.
					</p></li><li class="listitem"><p class="simpara">
						Add name/value pairs to the <code class="literal">spec.settings</code> section:
					</p><div class="formalpara"><p class="title"><strong>Example</strong></p><p>
							
<pre class="programlisting language-terminal">spec:
  settings:
    name: value <span id="CO5-1"><!--Empty--></span><span class="callout">1</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO5-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Use the <code class="literal">FirmwareSchema</code> resource to identify the available settings for the host. You cannot set values that are read-only.
							</div></dd></dl></div></li><li class="listitem">
						Save the changes and exit the editor.
					</li><li class="listitem"><p class="simpara">
						Get the host’s machine name:
					</p><pre class="programlisting language-terminal"> $ oc get bmh &lt;host_name&gt; -n openshift-machine name</pre><p class="simpara">
						Where <code class="literal">&lt;host_name&gt;</code> is the name of the host. The machine name appears under the <code class="literal">CONSUMER</code> field.
					</p></li><li class="listitem"><p class="simpara">
						Annotate the machine to delete it from the machineset:
					</p><pre class="programlisting language-terminal">$ oc annotate machine &lt;machine_name&gt; machine.openshift.io/delete-machine=true -n openshift-machine-api</pre><p class="simpara">
						Where <code class="literal">&lt;machine_name&gt;</code> is the name of the machine to delete.
					</p></li><li class="listitem"><p class="simpara">
						Get a list of nodes and count the number of worker nodes:
					</p><pre class="programlisting language-terminal">$ oc get nodes</pre></li><li class="listitem"><p class="simpara">
						Get the machineset:
					</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
						Scale the machineset:
					</p><pre class="programlisting language-terminal">$ oc scale machineset &lt;machineset_name&gt; -n openshift-machine-api --replicas=&lt;n-1&gt;</pre><p class="simpara">
						Where <code class="literal">&lt;machineset_name&gt;</code> is the name of the machineset and <code class="literal">&lt;n-1&gt;</code> is the decremented number of worker nodes.
					</p></li><li class="listitem"><p class="simpara">
						When the host enters the <code class="literal">Available</code> state, scale up the machineset to make the <code class="literal">HostFirmwareSettings</code> resource changes take effect:
					</p><pre class="programlisting language-terminal">$ oc scale machineset &lt;machineset_name&gt; -n openshift-machine-api --replicas=&lt;n&gt;</pre><p class="simpara">
						Where <code class="literal">&lt;machineset_name&gt;</code> is the name of the machineset and <code class="literal">&lt;n&gt;</code> is the number of worker nodes.
					</p></li></ol></div></section><section class="section" id="verifying-the-hostfirmware-settings-resource-is-valid_post-install-bare-metal-configuration"><div class="titlepage"><div><div><h2 class="title">3.7. Verifying the HostFirmware Settings resource is valid</h2></div></div></div><p>
				When the user edits the <code class="literal">spec.settings</code> section to make a change to the <code class="literal">HostFirmwareSetting</code>(HFS) resource, the Bare Metal Operator (BMO) validates the change against the <code class="literal">FimwareSchema</code> resource, which is a read-only resource. If the setting is invalid, the BMO will set the <code class="literal">Type</code> value of the <code class="literal">status.Condition</code> setting to <code class="literal">False</code> and also generate an event and store it in the HFS resource. Use the following procedure to verify that the resource is valid.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Get a list of <code class="literal">HostFirmwareSetting</code> resources:
					</p><pre class="programlisting language-terminal">$ oc get hfs -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
						Verify that the <code class="literal">HostFirmwareSettings</code> resource for a particular host is valid:
					</p><pre class="programlisting language-terminal">$ oc describe hfs &lt;host_name&gt; -n openshift-machine-api</pre><p class="simpara">
						Where <code class="literal">&lt;host_name&gt;</code> is the name of the host.
					</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">Events:
  Type    Reason            Age    From                                    Message
  ----    ------            ----   ----                                    -------
  Normal  ValidationFailed  2m49s  metal3-hostfirmwaresettings-controller  Invalid BIOS setting: Setting ProcTurboMode is invalid, unknown enumeration value - Foo</pre>

						</p></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							If the response returns <code class="literal">ValidationFailed</code>, there is an error in the resource configuration and you must update the values to conform to the <code class="literal">FirmwareSchema</code> resource.
						</p></div></div></li></ol></div></section><section class="section" id="about-the-firmwareschema-resource_post-install-bare-metal-configuration"><div class="titlepage"><div><div><h2 class="title">3.8. About the FirmwareSchema resource</h2></div></div></div><p>
				BIOS settings vary among hardware vendors and host models. A <code class="literal">FirmwareSchema</code> resource is a read-only resource that contains the types and limits for each BIOS setting on each host model. The data comes directly from the BMC through Ironic. The <code class="literal">FirmwareSchema</code> enables you to identify valid values you can specify in the <code class="literal">spec</code> field of the <code class="literal">HostFirmwareSettings</code> resource. The <code class="literal">FirmwareSchema</code> resource has a unique identifier derived from its settings and limits. Identical host models use the same <code class="literal">FirmwareSchema</code> identifier. It is likely that multiple instances of <code class="literal">HostFirmwareSettings</code> use the same <code class="literal">FirmwareSchema</code>.
			</p><div class="table" id="idm140031657880256"><p class="title"><strong>Table 3.4. FirmwareSchema specification</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031646040096" scope="col">Parameters</th><th align="left" valign="top" id="idm140031646039008" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031646040096">
<pre class="screen">&lt;BIOS_setting_name&gt;
  attribute_type:
  allowable_values:
  lower_bound:
  upper_bound:
  min_length:
  max_length:
  read_only:
  unique:</pre>
							 </td><td align="left" valign="top" headers="idm140031646039008"> <p>
								The <code class="literal">spec</code> is a simple map consisting of the BIOS setting name and the limits of the setting. The fields include:
							</p>
							 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
										<code class="literal">attribute_type</code>: The type of setting. The supported types are:
									</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
												<code class="literal">Enumeration</code>
											</li><li class="listitem">
												<code class="literal">Integer</code>
											</li><li class="listitem">
												<code class="literal">String</code>
											</li><li class="listitem">
												<code class="literal">Boolean</code>
											</li></ul></div></li><li class="listitem">
										<code class="literal">allowable_values</code>: A list of allowable values when the <code class="literal">attribute_type</code> is <code class="literal">Enumeration</code>.
									</li><li class="listitem">
										<code class="literal">lower_bound</code>: The lowest allowed value when <code class="literal">attribute_type</code> is <code class="literal">Integer</code>.
									</li><li class="listitem">
										<code class="literal">upper_bound</code>: The highest allowed value when <code class="literal">attribute_type</code> is <code class="literal">Integer</code>.
									</li><li class="listitem">
										<code class="literal">min_length</code>: The shortest string length that the value can have when <code class="literal">attribute_type</code> is <code class="literal">String</code>.
									</li><li class="listitem">
										<code class="literal">max_length</code>: The longest string length that the value can have when <code class="literal">attribute_type</code> is <code class="literal">String</code>.
									</li><li class="listitem">
										<code class="literal">read_only</code>: The setting is read only and cannot be modified.
									</li><li class="listitem">
										<code class="literal">unique</code>: The setting is specific to this host.
									</li></ul></div>
							 </td></tr></tbody></table></div></div></section><section class="section" id="getting-the-firmwareschema-resource_post-install-bare-metal-configuration"><div class="titlepage"><div><div><h2 class="title">3.9. Getting the FirmwareSchema resource</h2></div></div></div><p>
				Each host model from each vendor has different BIOS settings. When editing the <code class="literal">HostFirmwareSettings</code> resource’s <code class="literal">spec</code> section, the name/value pairs you set must conform to that host’s firmware schema. To ensure you are setting valid name/value pairs, get the <code class="literal">FirmwareSchema</code> for the host and review it.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To get a list of <code class="literal">FirmwareSchema</code> resource instances, execute the following:
					</p><pre class="programlisting language-terminal">$ oc get firmwareschema -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
						To get a particular <code class="literal">FirmwareSchema</code> instance, execute:
					</p><pre class="programlisting language-terminal">$ oc get firmwareschema &lt;instance_name&gt; -n openshift-machine-api -o yaml</pre><p class="simpara">
						Where <code class="literal">&lt;instance_name&gt;</code> is the name of the schema instance stated in the <code class="literal">HostFirmwareSettings</code> resource (see Table 3).
					</p></li></ol></div></section></section><section class="chapter" id="post-install-multi-architecture-configuration"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Configuring multi-architecture compute machines on an OpenShift Container Platform cluster</h1></div></div></div><p>
			An OpenShift Container Platform cluster with multi-architecture compute machines is a cluster that supports compute machines with different architectures. Clusters with multi-architecture compute machines are available only on AWS or Azure installer-provisioned infrastructures and bare metal user-provisioned infrastructures with x86_64 control plane machines.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				When there are nodes with multiple architectures in your cluster, the architecture of your image must be consistent with the architecture of the node. You need to ensure that the pod is assigned to the node with the appropriate architecture and that it matches the image architecture. For more information on assigning pods to nodes, see <a class="link" href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">Assigning pods to nodes</a>.
			</p></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				The Cluster Samples Operator is not supported on clusters with multi-architecture compute machines. Your cluster can be created without this capability. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#enabling-cluster-capabilities">Enabling cluster capabilities</a>
			</p></div></div><p>
			For information on migrating your single-architecture cluster to a cluster that supports multi-architecture compute machines, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#migrating-to-multi-payload">Migrating to a cluster with multi-architecture compute machines</a>.
		</p><section class="section" id="multi-architecture-verifying-cluster-compatibility_multi-architecture-configuration"><div class="titlepage"><div><div><h2 class="title">4.1. Verifying cluster compatibility</h2></div></div></div><p>
				Before you can start adding compute nodes of different architectures to your cluster, you must verify that your cluster is multi-architecture compatible.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You installed the OpenShift CLI (<code class="literal">oc</code>)
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						You can check that your cluster uses the architecture payload by running the following command:
					</p><pre class="programlisting language-terminal">$ oc adm release info -o json | jq .metadata.metadata</pre></li></ul></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						If you see the following output, then your cluster is using the multi-architecture payload:
					</p><pre class="programlisting language-terminal">$ "release.openshift.io/architecture": "multi"</pre><p class="simpara">
						You can then begin adding multi-arch compute nodes to your cluster.
					</p></li><li class="listitem"><p class="simpara">
						If you see the following output, then your cluster is not using the multi-architecture payload:
					</p><pre class="programlisting language-terminal">$ null</pre><p class="simpara">
						To migrate your cluster to one that supports multi-architecture compute machines, follow the procedure in "Migrating to a cluster with multi-architecture compute machines".
					</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#migrating-to-multi-payload">Migrating to a cluster with multi-architecture compute machines</a>.
					</li></ul></div></section><section class="section" id="creating-a-cluster-with-multi-architecture-compute-machine-on-azure"><div class="titlepage"><div><div><h2 class="title">4.2. Creating a cluster with multi-architecture compute machine on Azure</h2></div></div></div><p>
				To deploy an Azure cluster with multi-architecture compute machines, you must first create a single-architecture Azure installer-provisioned cluster that uses the multi-architecture installer binary. For more information on Azure installations, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-a-cluster-on-azure-with-customizations">Installing a cluster on Azure with customizations</a>. You can then add an ARM64 compute machine set to your cluster to create a cluster with multi-architecture compute machines.
			</p><p>
				The following procedures explain how to generate an ARM64 boot image and create an Azure compute machine set that uses the ARM64 boot image. This adds ARM64 compute nodes to your cluster and deploys the amount of ARM64 virtual machines (VM) that you need.
			</p><section class="section" id="multi-architecture-creating-arm64-bootimage_multi-architecture-configuration"><div class="titlepage"><div><div><h3 class="title">4.2.1. Creating an ARM64 boot image using the Azure image gallery</h3></div></div></div><p>
					The following procedure describes how to manually generate an ARM64 boot image.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You installed the Azure CLI (<code class="literal">az</code>).
						</li><li class="listitem">
							You created a single-architecture Azure installer-provisioned cluster with the multi-architecture installer binary.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Log in to your Azure account:
						</p><pre class="programlisting language-terminal">$ az login</pre></li><li class="listitem"><p class="simpara">
							Create a storage account and upload the <code class="literal">arm64</code> virtual hard disk (VHD) to your storage account. The OpenShift Container Platform installation program creates a resource group, however, the boot image can also be uploaded to a custom named resource group:
						</p><pre class="programlisting language-terminal">$ az storage account create -n ${STORAGE_ACCOUNT_NAME} -g ${RESOURCE_GROUP} -l westus --sku Standard_LRS <span id="CO6-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO6-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal">westus</code> object is an example region.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create a storage container using the storage account you generated:
						</p><pre class="programlisting language-terminal">$ az storage container create -n ${CONTAINER_NAME} --account-name ${STORAGE_ACCOUNT_NAME}</pre></li><li class="listitem"><p class="simpara">
							You must use the OpenShift Container Platform installation program JSON file to extract the URL and <code class="literal">aarch64</code> VHD name:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Extract the <code class="literal">URL</code> field and set it to <code class="literal">RHCOS_VHD_ORIGIN_URL</code> as the file name by running the following command:
								</p><pre class="programlisting language-terminal">$ RHCOS_VHD_ORIGIN_URL=$(oc -n openshift-machine-config-operator get configmap/coreos-bootimages -o jsonpath='{.data.stream}' | jq -r '.architectures.aarch64."rhel-coreos-extensions"."azure-disk".url')</pre></li><li class="listitem"><p class="simpara">
									Extract the <code class="literal">aarch64</code> VHD name and set it to <code class="literal">BLOB_NAME</code> as the file name by running the following command:
								</p><pre class="programlisting language-terminal">$ BLOB_NAME=rhcos-$(oc -n openshift-machine-config-operator get configmap/coreos-bootimages -o jsonpath='{.data.stream}' | jq -r '.architectures.aarch64."rhel-coreos-extensions"."azure-disk".release')-azure.aarch64.vhd</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Generate a shared access signature (SAS) token. Use this token to upload the RHCOS VHD to your storage container with the following commands:
						</p><pre class="programlisting language-terminal">$ end=`date -u -d "30 minutes" '+%Y-%m-%dT%H:%MZ'`</pre><pre class="programlisting language-terminal">$ sas=`az storage container generate-sas -n ${CONTAINER_NAME} --account-name ${STORAGE_ACCOUNT_NAME} --https-only --permissions dlrw --expiry $end -o tsv`</pre></li><li class="listitem"><p class="simpara">
							Copy the RHCOS VHD into the storage container:
						</p><pre class="programlisting language-terminal">$ az storage blob copy start --account-name ${STORAGE_ACCOUNT_NAME} --sas-token "$sas" \
 --source-uri "${RHCOS_VHD_ORIGIN_URL}" \
 --destination-blob "${BLOB_NAME}" --destination-container ${CONTAINER_NAME}</pre><p class="simpara">
							You can check the status of the copying process with the following command:
						</p><pre class="programlisting language-terminal">$ az storage blob show -c ${CONTAINER_NAME} -n ${BLOB_NAME} --account-name ${STORAGE_ACCOUNT_NAME} | jq .properties.copy</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">{
 "completionTime": null,
 "destinationSnapshot": null,
 "id": "1fd97630-03ca-489a-8c4e-cfe839c9627d",
 "incrementalCopy": null,
 "progress": "17179869696/17179869696",
 "source": "https://rhcos.blob.core.windows.net/imagebucket/rhcos-411.86.202207130959-0-azure.aarch64.vhd",
 "status": "success", <span id="CO7-1"><!--Empty--></span><span class="callout">1</span>
 "statusDescription": null
}</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO7-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									If the status parameter displays the <code class="literal">success</code> object, the copying process is complete.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create an image gallery using the following command:
						</p><pre class="programlisting language-terminal">$ az sig create --resource-group ${RESOURCE_GROUP} --gallery-name ${GALLERY_NAME}</pre><p class="simpara">
							Use the image gallery to create an image definition. In the following example command, <code class="literal">rhcos-arm64</code> is the name of the image definition.
						</p><pre class="programlisting language-terminal">$ az sig image-definition create --resource-group ${RESOURCE_GROUP} --gallery-name ${GALLERY_NAME} --gallery-image-definition rhcos-arm64 --publisher RedHat --offer arm --sku arm64 --os-type linux --architecture Arm64 --hyper-v-generation V2</pre></li><li class="listitem"><p class="simpara">
							To get the URL of the VHD and set it to <code class="literal">RHCOS_VHD_URL</code> as the file name, run the following command:
						</p><pre class="programlisting language-terminal">$ RHCOS_VHD_URL=$(az storage blob url --account-name ${STORAGE_ACCOUNT_NAME} -c ${CONTAINER_NAME} -n "${BLOB_NAME}" -o tsv)</pre></li><li class="listitem"><p class="simpara">
							Use the <code class="literal">RHCOS_VHD_URL</code> file, your storage account, resource group, and image gallery to create an image version. In the following example, <code class="literal">1.0.0</code> is the image version.
						</p><pre class="programlisting language-terminal">$ az sig image-version create --resource-group ${RESOURCE_GROUP} --gallery-name ${GALLERY_NAME} --gallery-image-definition rhcos-arm64 --gallery-image-version 1.0.0 --os-vhd-storage-account ${STORAGE_ACCOUNT_NAME} --os-vhd-uri ${RHCOS_VHD_URL}</pre></li><li class="listitem"><p class="simpara">
							Your <code class="literal">arm64</code> boot image is now generated. You can access the ID of your image with the following command:
						</p><pre class="programlisting language-terminal">$ az sig image-version show -r $GALLERY_NAME -g $RESOURCE_GROUP -i rhcos-arm64 -e 1.0.0</pre><p class="simpara">
							The following example image ID is used in the <code class="literal">recourseID</code> parameter of the compute machine set:
						</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">resourceID</code></strong></p><p>
								
<pre class="programlisting language-terminal">/resourceGroups/${RESOURCE_GROUP}/providers/Microsoft.Compute/galleries/${GALLERY_NAME}/images/rhcos-arm64/versions/1.0.0</pre>

							</p></div></li></ol></div></section><section class="section" id="multi-architecture-modify-machine-set_multi-architecture-configuration"><div class="titlepage"><div><div><h3 class="title">4.2.2. Adding a multi-architecture compute machine set to your cluster</h3></div></div></div><p>
					To add ARM64 compute nodes to your cluster, you must create an Azure compute machine set that uses the ARM64 boot image. To create your own custom compute machine set on Azure, see "Creating a compute machine set on Azure".
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Create a compute machine set and modify the <code class="literal">resourceID</code> and <code class="literal">vmSize</code> parameters with the following command. This compute machine set will control the <code class="literal">arm64</code> worker nodes in your cluster:
						</p><pre class="programlisting language-terminal">$ oc create -f arm64-machine-set-0.yaml</pre><div class="formalpara"><p class="title"><strong>Sample YAML compute machine set with <code class="literal">arm64</code> boot image</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
    machine.openshift.io/cluster-api-machine-role: worker
    machine.openshift.io/cluster-api-machine-type: worker
  name: &lt;infrastructure_id&gt;-arm64-machine-set-0
  namespace: openshift-machine-api
spec:
  replicas: 2
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-arm64-machine-set-0
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-arm64-machine-set-0
    spec:
      lifecycleHooks: {}
      metadata: {}
      providerSpec:
        value:
          acceleratedNetworking: true
          apiVersion: machine.openshift.io/v1beta1
          credentialsSecret:
            name: azure-cloud-credentials
            namespace: openshift-machine-api
          image:
            offer: ""
            publisher: ""
            resourceID: /resourceGroups/${RESOURCE_GROUP}/providers/Microsoft.Compute/galleries/${GALLERY_NAME}/images/rhcos-arm64/versions/1.0.0 <span id="CO8-1"><!--Empty--></span><span class="callout">1</span>
            sku: ""
            version: ""
          kind: AzureMachineProviderSpec
          location: &lt;region&gt;
          managedIdentity: &lt;infrastructure_id&gt;-identity
          networkResourceGroup: &lt;infrastructure_id&gt;-rg
          osDisk:
            diskSettings: {}
            diskSizeGB: 128
            managedDisk:
              storageAccountType: Premium_LRS
            osType: Linux
          publicIP: false
          publicLoadBalancer: &lt;infrastructure_id&gt;
          resourceGroup: &lt;infrastructure_id&gt;-rg
          subnet: &lt;infrastructure_id&gt;-worker-subnet
          userDataSecret:
            name: worker-user-data
          vmSize: Standard_D4ps_v5 <span id="CO8-2"><!--Empty--></span><span class="callout">2</span>
          vnet: &lt;infrastructure_id&gt;-vnet
          zone: "&lt;zone&gt;"</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO8-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Set the <code class="literal">resourceID</code> parameter to the <code class="literal">arm64</code> boot image.
								</div></dd><dt><a href="#CO8-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Set the <code class="literal">vmSize</code> parameter to the instance type used in your installation. Some example instance types are <code class="literal">Standard_D4ps_v5</code> or <code class="literal">D8ps</code>.
								</div></dd></dl></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Verify that the new ARM64 machines are running by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                DESIRED  CURRENT  READY  AVAILABLE  AGE
&lt;infrastructure_id&gt;-arm64-machine-set-0                   2        2      2          2  10m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							You can check that the nodes are ready and scheduable with the following command:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-a-compute-machine-set-on-azure">Creating a compute machine set on Azure</a>
						</li></ul></div></section></section><section class="section" id="creating-a-cluster-with-multi-architecture-compute-machines-on-aws"><div class="titlepage"><div><div><h2 class="title">4.3. Creating a cluster with multi-architecture compute machines on AWS</h2></div></div></div><p>
				To create an AWS cluster with multi-architecture compute machines, you must first create a single-architecture AWS installer-provisioned cluster with the multi-architecture installer binary. For more information on AWS installations, refer to <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-a-cluster-on-aws-with-customizations">Installing a cluster on AWS with customizations</a>. You can then add a ARM64 compute machine set to your AWS cluster.
			</p><section class="section" id="multi-architecture-modify-machine-set-aws_multi-architecture-configuration"><div class="titlepage"><div><div><h3 class="title">4.3.1. Adding an ARM64 compute machine set to your cluster</h3></div></div></div><p>
					To configure a cluster with multi-architecture compute machines, you must create a AWS ARM64 compute machine set. This adds ARM64 compute nodes to your cluster so that your cluster has multi-architecture compute machines.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You used the installation program to create an AMD64 single-architecture AWS cluster with the multi-architecture installer binary.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Create and modify a compute machine set, this will control the ARM64 compute nodes in your cluster.
						</p><pre class="programlisting language-terminal">$ oc create -f aws-arm64-machine-set-0.yaml</pre><div class="formalpara"><p class="title"><strong>Sample YAML compute machine set to deploy an ARM64 compute node</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO9-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-aws-arm64-machine-set-0 <span id="CO9-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO9-3"><!--Empty--></span><span class="callout">3</span>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;zone&gt; <span id="CO9-4"><!--Empty--></span><span class="callout">4</span>
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO9-5"><!--Empty--></span><span class="callout">5</span>
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO9-6"><!--Empty--></span><span class="callout">6</span>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;zone&gt; <span id="CO9-7"><!--Empty--></span><span class="callout">7</span>
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/&lt;role&gt;: ""
      providerSpec:
        value:
          ami:
            id: ami-02a574449d4f4d280 <span id="CO9-8"><!--Empty--></span><span class="callout">8</span>
          apiVersion: awsproviderconfig.openshift.io/v1beta1
          blockDevices:
            - ebs:
                iops: 0
                volumeSize: 120
                volumeType: gp2
          credentialsSecret:
            name: aws-cloud-credentials
          deviceIndex: 0
          iamInstanceProfile:
            id: &lt;infrastructure_id&gt;-worker-profile <span id="CO9-9"><!--Empty--></span><span class="callout">9</span>
          instanceType: m6g.xlarge <span id="CO9-10"><!--Empty--></span><span class="callout">10</span>
          kind: AWSMachineProviderConfig
          placement:
            availabilityZone: us-east-1a <span id="CO9-11"><!--Empty--></span><span class="callout">11</span>
            region: &lt;region&gt; <span id="CO9-12"><!--Empty--></span><span class="callout">12</span>
          securityGroups:
            - filters:
                - name: tag:Name
                  values:
                    - &lt;infrastructure_id&gt;-worker-sg <span id="CO9-13"><!--Empty--></span><span class="callout">13</span>
          subnet:
            filters:
              - name: tag:Name
                values:
                  - &lt;infrastructure_id&gt;-private-&lt;zone&gt;
          tags:
            - name: kubernetes.io/cluster/&lt;infrastructure_id&gt; <span id="CO9-14"><!--Empty--></span><span class="callout">14</span>
              value: owned
            - name: &lt;custom_tag_name&gt;
              value: &lt;custom_tag_value&gt;
          userDataSecret:
            name: worker-user-data</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO9-1"><span class="callout">1</span></a> <a href="#CO9-2"><span class="callout">2</span></a> <a href="#CO9-3"><span class="callout">3</span></a> <a href="#CO9-9"><span class="callout">9</span></a> <a href="#CO9-13"><span class="callout">13</span></a> <a href="#CO9-14"><span class="callout">14</span></a> </dt><dd><div class="para">
									Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:
								</div><pre class="programlisting language-terminal">$ oc get -o jsonpath=‘{.status.infrastructureName}{“\n”}’ infrastructure cluster</pre></dd><dt><a href="#CO9-4"><span class="callout">4</span></a> <a href="#CO9-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									Specify the infrastructure ID, role node label, and zone.
								</div></dd><dt><a href="#CO9-5"><span class="callout">5</span></a> <a href="#CO9-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Specify the role node label to add.
								</div></dd><dt><a href="#CO9-8"><span class="callout">8</span></a> </dt><dd><div class="para">
									Specify an ARM64 supported Red Hat Enterprise Linux CoreOS (RHCOS) Amazon Machine Image (AMI) for your AWS zone for your OpenShift Container Platform nodes.
								</div><pre class="programlisting language-terminal">$ oc get configmap/coreos-bootimages /
	  -n openshift-machine-config-operator /
	  -o jsonpath='{.data.stream}' | jq /
	  -r '.architectures.&lt;arch&gt;.images.aws.regions."&lt;region&gt;".image'</pre></dd><dt><a href="#CO9-10"><span class="callout">10</span></a> </dt><dd><div class="para">
									Specify an ARM64 supported machine type. For more information, refer to "Tested instance types for AWS 64-bit ARM"
								</div></dd><dt><a href="#CO9-11"><span class="callout">11</span></a> </dt><dd><div class="para">
									Specify the zone, for example <code class="literal">us-east-1a</code>. Ensure that the zone you select offers 64-bit ARM machines.
								</div></dd><dt><a href="#CO9-12"><span class="callout">12</span></a> </dt><dd><div class="para">
									Specify the region, for example, <code class="literal">us-east-1</code>. Ensure that the zone you select offers 64-bit ARM machines.
								</div></dd></dl></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the list of compute machine sets by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><p class="simpara">
							You can then see your created ARM64 machine set.
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                DESIRED  CURRENT  READY  AVAILABLE  AGE
&lt;infrastructure_id&gt;-aws-arm64-machine-set-0                   2        2      2          2  10m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							You can check that the nodes are ready and scheduable with the following command:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-aws-arm-tested-machine-types_installing-aws-customizations">Tested instance types for AWS 64-bit ARM</a>
						</li></ul></div></section></section><section class="section" id="creating-a-cluster-with-multi-architecture-compute-machine-on-bare-metal-technology-preview"><div class="titlepage"><div><div><h2 class="title">4.4. Creating a cluster with multi-architecture compute machine on bare metal (Technology Preview)</h2></div></div></div><p>
				To create a cluster with multi-architecture compute machines on bare metal, you must have an existing single-architecture bare metal cluster. For more information on bare metal installations, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-bare-metal">Installing a user provisioned cluster on bare metal</a>. You can then add 64-bit ARM compute machines to your OpenShift Container Platform cluster on bare metal.
			</p><p>
				Before you can add 64-bit ARM nodes to your bare metal cluster, you must upgrade your cluster to one that uses the multi-architecture payload. For more information on migrating to the multi-architecture payload, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#migrating-to-multi-payload">Migrating to a cluster with multi-architecture compute machines</a>.
			</p><p>
				The following procedures explain how to create a RHCOS compute machine using an ISO image or network PXE booting. This will allow you to add ARM64 nodes to your bare metal cluster and deploy a cluster with multi-architecture compute machines.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Clusters with multi-architecture compute machines on bare metal user-provisioned installations is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
				</p><p>
					For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
				</p></div></div><section class="section" id="machine-user-infra-machines-iso_multi-architecture-configuration"><div class="titlepage"><div><div><h3 class="title">4.4.1. Creating RHCOS machines using an ISO image</h3></div></div></div><p>
					You can create more Red Hat Enterprise Linux CoreOS (RHCOS) compute machines for your bare metal cluster by using an ISO image to create the machines.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Obtain the URL of the Ignition config file for the compute machines for your cluster. You uploaded this file to your HTTP server during installation.
						</li><li class="listitem">
							You must have the OpenShift CLI (<code class="literal">oc</code>) installed.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Extract the Ignition config file from the cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc extract -n openshift-machine-api secret/worker-user-data-managed --keys=userData --to=- &gt; worker.ign</pre></li><li class="listitem">
							Upload the <code class="literal">worker.ign</code> Ignition config file you exported from your cluster to your HTTP server. Note the URLs of these files.
						</li><li class="listitem"><p class="simpara">
							You can validate that the ignition files are available on the URLs. The following example gets the Ignition config files for the compute node:
						</p><pre class="programlisting language-terminal">$ curl -k http://&lt;HTTP_server&gt;/worker.ign</pre></li><li class="listitem"><p class="simpara">
							You can access the ISO image for booting your new machine by running to following command:
						</p><pre class="programlisting language-terminal">RHCOS_VHD_ORIGIN_URL=$(oc -n openshift-machine-config-operator get configmap/coreos-bootimages -o jsonpath='{.data.stream}' | jq -r '.architectures.&lt;architecture&gt;.artifacts.metal.formats.iso.disk.location')</pre></li><li class="listitem"><p class="simpara">
							Use the ISO file to install RHCOS on more compute machines. Use the same method that you used when you created machines before you installed the cluster:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Burn the ISO image to a disk and boot it directly.
								</li><li class="listitem">
									Use ISO redirection with a LOM interface.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							Boot the RHCOS ISO image without specifying any options, or interrupting the live boot sequence. Wait for the installer to boot into a shell prompt in the RHCOS live environment.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								You can interrupt the RHCOS installation boot process to add kernel arguments. However, for this ISO procedure you must use the <code class="literal">coreos-installer</code> command as outlined in the following steps, instead of adding kernel arguments.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">coreos-installer</code> command and specify the options that meet your installation requirements. At a minimum, you must specify the URL that points to the Ignition config file for the node type, and the device that you are installing to:
						</p><pre class="programlisting language-terminal">$ sudo coreos-installer install --ignition-url=http://&lt;HTTP_server&gt;/&lt;node_type&gt;.ign &lt;device&gt; --ignition-hash=sha512-&lt;digest&gt; <span id="CO10-1"><!--Empty--></span><span class="callout">1</span><span id="CO10-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO10-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									You must run the <code class="literal">coreos-installer</code> command by using <code class="literal">sudo</code>, because the <code class="literal">core</code> user does not have the required root privileges to perform the installation.
								</div></dd><dt><a href="#CO10-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The <code class="literal">--ignition-hash</code> option is required when the Ignition config file is obtained through an HTTP URL to validate the authenticity of the Ignition config file on the cluster node. <code class="literal">&lt;digest&gt;</code> is the Ignition config file SHA512 digest obtained in a preceding step.
								</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If you want to provide your Ignition config files through an HTTPS server that uses TLS, you can add the internal certificate authority (CA) to the system trust store before running <code class="literal">coreos-installer</code>.
							</p></div></div><p class="simpara">
							The following example initializes a bootstrap node installation to the <code class="literal">/dev/sda</code> device. The Ignition config file for the bootstrap node is obtained from an HTTP web server with the IP address 192.168.1.2:
						</p><pre class="programlisting language-terminal">$ sudo coreos-installer install --ignition-url=http://192.168.1.2:80/installation_directory/bootstrap.ign /dev/sda --ignition-hash=sha512-a5a2d43879223273c9b60af66b44202a1d1248fc01cf156c46d4a79f552b6bad47bc8cc78ddf0116e80c59d2ea9e32ba53bc807afbca581aa059311def2c3e3b</pre></li><li class="listitem"><p class="simpara">
							Monitor the progress of the RHCOS installation on the console of the machine.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Ensure that the installation is successful on each node before commencing with the OpenShift Container Platform installation. Observing the installation process can also help to determine the cause of RHCOS installation issues that might arise.
							</p></div></div></li><li class="listitem">
							Continue to create more compute machines for your cluster.
						</li></ol></div></section><section class="section" id="machine-user-infra-machines-pxe_multi-architecture-configuration"><div class="titlepage"><div><div><h3 class="title">4.4.2. Creating RHCOS machines by PXE or iPXE booting</h3></div></div></div><p>
					You can create more Red Hat Enterprise Linux CoreOS (RHCOS) compute machines for your bare metal cluster by using PXE or iPXE booting.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Obtain the URL of the Ignition config file for the compute machines for your cluster. You uploaded this file to your HTTP server during installation.
						</li><li class="listitem">
							Obtain the URLs of the RHCOS ISO image, compressed metal BIOS, <code class="literal">kernel</code>, and <code class="literal">initramfs</code> files that you uploaded to your HTTP server during cluster installation.
						</li><li class="listitem">
							You have access to the PXE booting infrastructure that you used to create the machines for your OpenShift Container Platform cluster during installation. The machines must boot from their local disks after RHCOS is installed on them.
						</li><li class="listitem">
							If you use UEFI, you have access to the <code class="literal">grub.conf</code> file that you modified during OpenShift Container Platform installation.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Confirm that your PXE or iPXE installation for the RHCOS images is correct.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									For PXE:
								</p><pre class="screen">DEFAULT pxeboot
TIMEOUT 20
PROMPT 0
LABEL pxeboot
    KERNEL http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-kernel-&lt;architecture&gt; <span id="CO11-1"><!--Empty--></span><span class="callout">1</span>
    APPEND initrd=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-initramfs.&lt;architecture&gt;.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://&lt;HTTP_server&gt;/worker.ign coreos.live.rootfs_url=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-rootfs.&lt;architecture&gt;.img <span id="CO11-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO11-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the location of the live <code class="literal">kernel</code> file that you uploaded to your HTTP server.
										</div></dd><dt><a href="#CO11-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Specify locations of the RHCOS files that you uploaded to your HTTP server. The <code class="literal">initrd</code> parameter value is the location of the live <code class="literal">initramfs</code> file, the <code class="literal">coreos.inst.ignition_url</code> parameter value is the location of the worker Ignition config file, and the <code class="literal">coreos.live.rootfs_url</code> parameter value is the location of the live <code class="literal">rootfs</code> file. The <code class="literal">coreos.inst.ignition_url</code> and <code class="literal">coreos.live.rootfs_url</code> parameters only support HTTP and HTTPS.
										</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										This configuration does not enable serial console access on machines with a graphical console. To configure a different console, add one or more <code class="literal">console=</code> arguments to the <code class="literal">APPEND</code> line. For example, add <code class="literal">console=tty0 console=ttyS0</code> to set the first PC serial port as the primary console and the graphical console as a secondary console. For more information, see <a class="link" href="https://access.redhat.com/articles/7212">How does one set up a serial terminal and/or console in Red Hat Enterprise Linux?</a>.
									</p></div></div></li><li class="listitem"><p class="simpara">
									For iPXE (<code class="literal">x86_64</code> + <code class="literal">aarch64</code>):
								</p><pre class="screen">kernel http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-kernel-&lt;architecture&gt; initrd=main coreos.live.rootfs_url=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-rootfs.&lt;architecture&gt;.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://&lt;HTTP_server&gt;/worker.ign <span id="CO12-1"><!--Empty--></span><span class="callout">1</span> <span id="CO12-2"><!--Empty--></span><span class="callout">2</span>
initrd --name main http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-initramfs.&lt;architecture&gt;.img <span id="CO12-3"><!--Empty--></span><span class="callout">3</span>
boot</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO12-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the locations of the RHCOS files that you uploaded to your HTTP server. The <code class="literal">kernel</code> parameter value is the location of the <code class="literal">kernel</code> file, the <code class="literal">initrd=main</code> argument is needed for booting on UEFI systems, the <code class="literal">coreos.live.rootfs_url</code> parameter value is the location of the <code class="literal">rootfs</code> file, and the <code class="literal">coreos.inst.ignition_url</code> parameter value is the location of the worker Ignition config file.
										</div></dd><dt><a href="#CO12-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											If you use multiple NICs, specify a single interface in the <code class="literal">ip</code> option. For example, to use DHCP on a NIC that is named <code class="literal">eno1</code>, set <code class="literal">ip=eno1:dhcp</code>.
										</div></dd><dt><a href="#CO12-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Specify the location of the <code class="literal">initramfs</code> file that you uploaded to your HTTP server.
										</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										This configuration does not enable serial console access on machines with a graphical console To configure a different console, add one or more <code class="literal">console=</code> arguments to the <code class="literal">kernel</code> line. For example, add <code class="literal">console=tty0 console=ttyS0</code> to set the first PC serial port as the primary console and the graphical console as a secondary console. For more information, see <a class="link" href="https://access.redhat.com/articles/7212">How does one set up a serial terminal and/or console in Red Hat Enterprise Linux?</a> and "Enabling the serial console for PXE and ISO installation" in the "Advanced RHCOS installation configuration" section.
									</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										To network boot the CoreOS <code class="literal">kernel</code> on <code class="literal">aarch64</code> architecture, you need to use a version of iPXE build with the <code class="literal">IMAGE_GZIP</code> option enabled. See <a class="link" href="https://ipxe.org/buildcfg/image_gzip"><code class="literal">IMAGE_GZIP</code> option in iPXE</a>.
									</p></div></div></li><li class="listitem"><p class="simpara">
									For PXE (with UEFI and GRUB as second stage) on <code class="literal">aarch64</code>:
								</p><pre class="screen">menuentry 'Install CoreOS' {
    linux rhcos-&lt;version&gt;-live-kernel-&lt;architecture&gt;  coreos.live.rootfs_url=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-rootfs.&lt;architecture&gt;.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://&lt;HTTP_server&gt;/worker.ign <span id="CO13-1"><!--Empty--></span><span class="callout">1</span> <span id="CO13-2"><!--Empty--></span><span class="callout">2</span>
    initrd rhcos-&lt;version&gt;-live-initramfs.&lt;architecture&gt;.img <span id="CO13-3"><!--Empty--></span><span class="callout">3</span>
}</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO13-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the locations of the RHCOS files that you uploaded to your HTTP/TFTP server. The <code class="literal">kernel</code> parameter value is the location of the <code class="literal">kernel</code> file on your TFTP server. The <code class="literal">coreos.live.rootfs_url</code> parameter value is the location of the <code class="literal">rootfs</code> file, and the <code class="literal">coreos.inst.ignition_url</code> parameter value is the location of the worker Ignition config file on your HTTP Server.
										</div></dd><dt><a href="#CO13-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											If you use multiple NICs, specify a single interface in the <code class="literal">ip</code> option. For example, to use DHCP on a NIC that is named <code class="literal">eno1</code>, set <code class="literal">ip=eno1:dhcp</code>.
										</div></dd><dt><a href="#CO13-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Specify the location of the <code class="literal">initramfs</code> file that you uploaded to your TFTP server.
										</div></dd></dl></div></li></ul></div></li><li class="listitem">
							Use the PXE or iPXE infrastructure to create the required compute machines for your cluster.
						</li></ol></div></section><section class="section" id="installation-approve-csrs_multi-architecture-configuration"><div class="titlepage"><div><div><h3 class="title">4.4.3. Approving the certificate signing requests for your machines</h3></div></div></div><p>
					When you add machines to a cluster, two pending certificate signing requests (CSRs) are generated for each machine that you added. You must confirm that these CSRs are approved or, if necessary, approve them yourself. The client requests must be approved first, followed by the server requests.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You added machines to your cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Confirm that the cluster recognizes the machines:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  63m  v1.26.0
master-1  Ready     master  63m  v1.26.0
master-2  Ready     master  64m  v1.26.0</pre>

							</p></div><p class="simpara">
							The output lists all of the machines that you created.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The preceding output might not include the compute nodes, also known as worker nodes, until some CSRs are approved.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Review the pending CSRs and ensure that you see the client requests with the <code class="literal">Pending</code> or <code class="literal">Approved</code> status for each machine that you added to the cluster:
						</p><pre class="programlisting language-terminal">$ oc get csr</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-8b2br   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-8vnps   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
...</pre>

							</p></div><p class="simpara">
							In this example, two machines are joining the cluster. You might see more approved CSRs in the list.
						</p></li><li class="listitem"><p class="simpara">
							If the CSRs were not approved, after all of the pending CSRs for the machines you added are in <code class="literal">Pending</code> status, approve the CSRs for your cluster machines:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Because the CSRs rotate automatically, approve your CSRs within an hour of adding the machines to the cluster. If you do not approve them within an hour, the certificates will rotate, and more than two certificates will be present for each node. You must approve all of these certificates. After the client CSR is approved, the Kubelet creates a secondary CSR for the serving certificate, which requires manual approval. Then, subsequent serving certificate renewal requests are automatically approved by the <code class="literal">machine-approver</code> if the Kubelet requests a new certificate with identical parameters.
							</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								For clusters running on platforms that are not machine API enabled, such as bare metal and other user-provisioned infrastructure, you must implement a method of automatically approving the kubelet serving certificate requests (CSRs). If a request is not approved, then the <code class="literal">oc exec</code>, <code class="literal">oc rsh</code>, and <code class="literal">oc logs</code> commands cannot succeed, because a serving certificate is required when the API server connects to the kubelet. Any operation that contacts the Kubelet endpoint requires this certificate approval to be in place. The method must watch for new CSRs, confirm that the CSR was submitted by the <code class="literal">node-bootstrapper</code> service account in the <code class="literal">system:node</code> or <code class="literal">system:admin</code> groups, and confirm the identity of the node.
							</p></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To approve them individually, run the following command for each valid CSR:
								</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt; <span id="CO14-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO14-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											<code class="literal">&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To approve all pending CSRs, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs --no-run-if-empty oc adm certificate approve</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Some Operators might not become available until some CSRs are approved.
									</p></div></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Now that your client requests are approved, you must review the server requests for each machine that you added to the cluster:
						</p><pre class="programlisting language-terminal">$ oc get csr</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-bfd72   5m26s   system:node:ip-10-0-50-126.us-east-2.compute.internal                       Pending
csr-c57lv   5m26s   system:node:ip-10-0-95-157.us-east-2.compute.internal                       Pending
...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							If the remaining CSRs are not approved, and are in the <code class="literal">Pending</code> status, approve the CSRs for your cluster machines:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To approve them individually, run the following command for each valid CSR:
								</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt; <span id="CO15-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO15-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											<code class="literal">&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To approve all pending CSRs, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs oc adm certificate approve</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							After all client and server CSRs have been approved, the machines have the <code class="literal">Ready</code> status. Verify this by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  73m  v1.26.0
master-1  Ready     master  73m  v1.26.0
master-2  Ready     master  74m  v1.26.0
worker-0  Ready     worker  11m  v1.26.0
worker-1  Ready     worker  11m  v1.26.0</pre>

							</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								It can take a few minutes after approval of the server CSRs for the machines to transition to the <code class="literal">Ready</code> status.
							</p></div></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Additional information</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							For more information on CSRs, see <a class="link" href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/">Certificate Signing Requests</a>.
						</li></ul></div></section></section><section class="section" id="multi-architecture-import-imagestreams_multi-architecture-configuration"><div class="titlepage"><div><div><h2 class="title">4.5. Importing manifest lists in image streams on your multi-architecture compute machines</h2></div></div></div><p>
				On an OpenShift Container Platform 4.13 cluster with multi-architecture compute machines, the image streams in the cluster do not import manifest lists automatically. You must manually change the default <code class="literal">importMode</code> option to the <code class="literal">PreserveOriginal</code> option in order to import the manifest list.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You installed the OpenShift Container Platform CLI (<code class="literal">oc</code>).
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						The following example command shows how to patch the <code class="literal">ImageStream</code> cli-artifacts so that the <code class="literal">cli-artifacts:latest</code> image stream tag is imported as a manifest list.
					</p><pre class="programlisting language-terminal">$ oc patch is/cli-artifacts -n openshift -p '{"spec":{"tags":[{"name":"latest","importPolicy":{"importMode":"PreserveOriginal"}}]}}'</pre></li></ul></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						You can check that the manifest lists imported properly by inspecting the image stream tag. The following command will list the individual architecture manifests for a particular tag.
					</p><pre class="programlisting language-terminal">$ oc get istag cli-artifacts:latest -n openshift -oyaml</pre><p class="simpara">
						If the <code class="literal">dockerImageManifests</code> object is present, then the manifest list import was successful.
					</p><div class="formalpara"><p class="title"><strong>Example output of the <code class="literal">dockerImageManifests</code> object</strong></p><p>
							
<pre class="programlisting language-yaml">dockerImageManifests:
  - architecture: amd64
    digest: sha256:16d4c96c52923a9968fbfa69425ec703aff711f1db822e4e9788bf5d2bee5d77
    manifestSize: 1252
    mediaType: application/vnd.docker.distribution.manifest.v2+json
    os: linux
  - architecture: arm64
    digest: sha256:6ec8ad0d897bcdf727531f7d0b716931728999492709d19d8b09f0d90d57f626
    manifestSize: 1252
    mediaType: application/vnd.docker.distribution.manifest.v2+json
    os: linux
  - architecture: ppc64le
    digest: sha256:65949e3a80349cdc42acd8c5b34cde6ebc3241eae8daaeea458498fedb359a6a
    manifestSize: 1252
    mediaType: application/vnd.docker.distribution.manifest.v2+json
    os: linux
  - architecture: s390x
    digest: sha256:75f4fa21224b5d5d511bea8f92dfa8e1c00231e5c81ab95e83c3013d245d1719
    manifestSize: 1252
    mediaType: application/vnd.docker.distribution.manifest.v2+json
    os: linux</pre>

						</p></div></li></ul></div></section></section><section class="chapter" id="vsphere-post-installation-encryption"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Enabling encryption on a vSphere cluster</h1></div></div></div><p>
			You can encrypt your virtual machines after installing OpenShift Container Platform 4.13 on vSphere by draining and shutting down your nodes one at a time. While each virtual machine is shutdown, you can enable encryption in the vCenter web interface.
		</p><section class="section" id="encrypting-virtual-machines_vsphere-post-installation-encryption"><div class="titlepage"><div><div><h2 class="title">5.1. Encrypting virtual machines</h2></div></div></div><p>
				You can encrypt your virtual machines with the following process. You can drain your virtual machines, power them down and encrypt them using the vCenter interface. Finally, you can create a storage class to use the encrypted storage.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						You have configured a Standard key provider in vSphere. For more information, see <a class="link" href="https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vsan.doc/GUID-AC06B3C3-901F-402E-B25F-1EE7809D1264.html">Adding a KMS to vCenter Server</a>.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							The Native key provider in vCenter is not supported. For more information, see <a class="link" href="https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.security.doc/GUID-54B9FBA2-FDB1-400B-A6AE-81BF3AC9DF97.html">vSphere Native Key Provider Overview</a>.
						</p></div></div></li><li class="listitem">
						You have enabled host encryption mode on all of the ESXi hosts that are hosting the cluster. For more information, see <a class="link" href="https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.security.doc/GUID-A9E1F016-51B3-472F-B8DE-803F6BDB70BC.html">Enabling host encryption mode</a>.
					</li><li class="listitem">
						You have a vSphere account which has all cryptographic privileges enabled. For more information, see <a class="link" href="https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.security.doc/GUID-660CCB35-847F-46B3-81CA-10DDDB9D7AA9.html">Cryptographic Operations Privileges</a>.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Drain and cordon one of your nodes. For detailed instructions on node management, see "Working with Nodes".
					</li><li class="listitem">
						Shutdown the virtual machine associated with that node in the vCenter interface.
					</li><li class="listitem">
						Right-click on the virtual machine in the vCenter interface and select <span class="strong strong"><strong>VM Policies</strong></span> → <span class="strong strong"><strong>Edit VM Storage Policies</strong></span>.
					</li><li class="listitem">
						Select an encrypted storage policy and select <span class="strong strong"><strong>OK</strong></span>.
					</li><li class="listitem">
						Start the encrypted virtual machine in the vCenter interface.
					</li><li class="listitem">
						Repeat steps 1-5 for all nodes that you want to encrypt.
					</li><li class="listitem">
						Configure a storage class that uses the encrypted storage policy. For more information about configuring an encrypted storage class, see "VMware vSphere CSI Driver Operator".
					</li></ol></div></section><section class="section _additional-resources" id="additional-resources_enabling-encryption-installation"><div class="titlepage"><div><div><h2 class="title">5.2. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-working-evacuating_nodes-nodes-working">Working with nodes</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/storage/#vsphere-pv-encryption">vSphere encryption</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-vsphere-encrypted-vms_installing-vsphere">Installing a cluster on vSphere with user-provisioned infrastructure</a>
					</li></ul></div></section></section><section class="chapter" id="post-install-machine-configuration-tasks"><div class="titlepage"><div><div><h1 class="title">Chapter 6. Post-installation machine configuration tasks</h1></div></div></div><p>
			There are times when you need to make changes to the operating systems running on OpenShift Container Platform nodes. This can include changing settings for network time service, adding kernel arguments, or configuring journaling in a specific way.
		</p><p>
			Aside from a few specialized features, most changes to operating systems on OpenShift Container Platform nodes can be done by creating what are referred to as <code class="literal">MachineConfig</code> objects that are managed by the Machine Config Operator.
		</p><p>
			Tasks in this section describe how to use features of the Machine Config Operator to configure operating system features on OpenShift Container Platform nodes.
		</p><section class="section" id="understanding-the-machine-config-operator"><div class="titlepage"><div><div><h2 class="title">6.1. Understanding the Machine Config Operator</h2></div></div></div><section class="section" id="machine-config-operator_post-install-machine-configuration-tasks"><div class="titlepage"><div><div><h3 class="title">6.1.1. Machine Config Operator</h3></div></div></div><h5 id="purpose">Purpose</h5><p>
					The Machine Config Operator manages and applies configuration and updates of the base operating system and container runtime, including everything between the kernel and kubelet.
				</p><p>
					There are four components:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">machine-config-server</code>: Provides Ignition configuration to new machines joining the cluster.
						</li><li class="listitem">
							<code class="literal">machine-config-controller</code>: Coordinates the upgrade of machines to the desired configurations defined by a <code class="literal">MachineConfig</code> object. Options are provided to control the upgrade for sets of machines individually.
						</li><li class="listitem">
							<code class="literal">machine-config-daemon</code>: Applies new machine configuration during update. Validates and verifies the state of the machine to the requested machine configuration.
						</li><li class="listitem">
							<code class="literal">machine-config</code>: Provides a complete source of machine configuration at installation, first start up, and updates for a machine.
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Currently, there is no supported way to block or restrict the machine config server endpoint. The machine config server must be exposed to the network so that newly-provisioned machines, which have no existing configuration or state, are able to fetch their configuration. In this model, the root of trust is the certificate signing requests (CSR) endpoint, which is where the kubelet sends its certificate signing request for approval to join the cluster. Because of this, machine configs should not be used to distribute sensitive information, such as secrets and certificates.
					</p><p>
						To ensure that the machine config server endpoints, ports 22623 and 22624, are secured in bare metal scenarios, customers must configure proper network policies.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-openshift-sdn">About the OpenShift SDN network plugin</a>.
						</li></ul></div><h5 id="project">Project</h5><p>
					<a class="link" href="https://github.com/openshift/machine-config-operator">openshift-machine-config-operator</a>
				</p></section><section class="section" id="machine-config-overview-post-install-machine-configuration-tasks"><div class="titlepage"><div><div><h3 class="title">6.1.2. Machine config overview</h3></div></div></div><p>
					The Machine Config Operator (MCO) manages updates to systemd, CRI-O and Kubelet, the kernel, Network Manager and other system features. It also offers a <code class="literal">MachineConfig</code> CRD that can write configuration files onto the host (see <a class="link" href="https://github.com/openshift/machine-config-operator#machine-config-operator">machine-config-operator</a>). Understanding what MCO does and how it interacts with other components is critical to making advanced, system-level changes to an OpenShift Container Platform cluster. Here are some things you should know about MCO, machine configs, and how they are used:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Machine configs are processed alphabetically, in lexicographically increasing order, of their name. The render controller uses the first machine config in the list as the base and appends the rest to the base machine config.
						</li><li class="listitem">
							A machine config can make a specific change to a file or service on the operating system of each system representing a pool of OpenShift Container Platform nodes.
						</li><li class="listitem"><p class="simpara">
							MCO applies changes to operating systems in pools of machines. All OpenShift Container Platform clusters start with worker and control plane node pools. By adding more role labels, you can configure custom pools of nodes. For example, you can set up a custom pool of worker nodes that includes particular hardware features needed by an application. However, examples in this section focus on changes to the default pool types.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								A node can have multiple labels applied that indicate its type, such as <code class="literal">master</code> or <code class="literal">worker</code>, however it can be a member of only a <span class="strong strong"><strong>single</strong></span> machine config pool.
							</p></div></div></li><li class="listitem">
							After a machine config change, the MCO updates the affected nodes alphabetically by zone, based on the <code class="literal">topology.kubernetes.io/zone</code> label. If a zone has more than one node, the oldest nodes are updated first. For nodes that do not use zones, such as in bare metal deployments, the nodes are upgraded by age, with the oldest nodes updated first. The MCO updates the number of nodes as specified by the <code class="literal">maxUnavailable</code> field on the machine configuration pool at a time.
						</li><li class="listitem">
							Some machine configuration must be in place before OpenShift Container Platform is installed to disk. In most cases, this can be accomplished by creating a machine config that is injected directly into the OpenShift Container Platform installer process, instead of running as a post-installation machine config. In other cases, you might need to do bare metal installation where you pass kernel arguments at OpenShift Container Platform installer startup, to do such things as setting per-node individual IP addresses or advanced disk partitioning.
						</li><li class="listitem">
							MCO manages items that are set in machine configs. Manual changes you do to your systems will not be overwritten by MCO, unless MCO is explicitly told to manage a conflicting file. In other words, MCO only makes specific updates you request, it does not claim control over the whole node.
						</li><li class="listitem">
							Manual changes to nodes are strongly discouraged. If you need to decommission a node and start a new one, those direct changes would be lost.
						</li><li class="listitem">
							MCO is only supported for writing to files in <code class="literal">/etc</code> and <code class="literal">/var</code> directories, although there are symbolic links to some directories that can be writeable by being symbolically linked to one of those areas. The <code class="literal">/opt</code> and <code class="literal">/usr/local</code> directories are examples.
						</li><li class="listitem">
							Ignition is the configuration format used in MachineConfigs. See the <a class="link" href="https://coreos.github.io/ignition/configuration-v3_2/">Ignition Configuration Specification v3.2.0</a> for details.
						</li><li class="listitem">
							Although Ignition config settings can be delivered directly at OpenShift Container Platform installation time, and are formatted in the same way that MCO delivers Ignition configs, MCO has no way of seeing what those original Ignition configs are. Therefore, you should wrap Ignition config settings into a machine config before deploying them.
						</li><li class="listitem">
							When a file managed by MCO changes outside of MCO, the Machine Config Daemon (MCD) sets the node as <code class="literal">degraded</code>. It will not overwrite the offending file, however, and should continue to operate in a <code class="literal">degraded</code> state.
						</li><li class="listitem">
							A key reason for using a machine config is that it will be applied when you spin up new nodes for a pool in your OpenShift Container Platform cluster. The <code class="literal">machine-api-operator</code> provisions a new machine and MCO configures it.
						</li></ul></div><p>
					MCO uses <a class="link" href="https://coreos.github.io/ignition/">Ignition</a> as the configuration format. OpenShift Container Platform 4.6 moved from Ignition config specification version 2 to version 3.
				</p><section class="section" id="what-can-you-change-with-machine-configs"><div class="titlepage"><div><div><h4 class="title">6.1.2.1. What can you change with machine configs?</h4></div></div></div><p>
						The kinds of components that MCO can change include:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								<span class="strong strong"><strong>config</strong></span>: Create Ignition config objects (see the <a class="link" href="https://coreos.github.io/ignition/configuration-v3_2/">Ignition configuration specification</a>) to do things like modify files, systemd services, and other features on OpenShift Container Platform machines, including:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										<span class="strong strong"><strong>Configuration files</strong></span>: Create or overwrite files in the <code class="literal">/var</code> or <code class="literal">/etc</code> directory.
									</li><li class="listitem">
										<span class="strong strong"><strong>systemd units</strong></span>: Create and set the status of a systemd service or add to an existing systemd service by dropping in additional settings.
									</li><li class="listitem"><p class="simpara">
										<span class="strong strong"><strong>users and groups</strong></span>: Change SSH keys in the passwd section post-installation.
									</p><div class="admonition important"><div class="admonition_header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="square"><li class="listitem">
													Changing SSH keys by using a machine config is supported only for the <code class="literal">core</code> user.
												</li><li class="listitem">
													Adding new users by using a machine config is not supported.
												</li></ul></div></div></div></li></ul></div></li><li class="listitem">
								<span class="strong strong"><strong>kernelArguments</strong></span>: Add arguments to the kernel command line when OpenShift Container Platform nodes boot.
							</li><li class="listitem">
								<span class="strong strong"><strong>kernelType</strong></span>: Optionally identify a non-standard kernel to use instead of the standard kernel. Use <code class="literal">realtime</code> to use the RT kernel (for RAN). This is only supported on select platforms.
							</li></ul></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<span class="strong strong"><strong>extensions</strong></span>: Extend RHCOS features by adding selected pre-packaged software. For this feature, available extensions include <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/security_hardening/index#protecting-systems-against-intrusive-usb-devices_security-hardening">usbguard</a> and kernel modules.
							</li><li class="listitem">
								<span class="strong strong"><strong>Custom resources (for <code class="literal">ContainerRuntime</code> and <code class="literal">Kubelet</code>)</strong></span>: Outside of machine configs, MCO manages two special custom resources for modifying CRI-O container runtime settings (<code class="literal">ContainerRuntime</code> CR) and the Kubelet service (<code class="literal">Kubelet</code> CR).
							</li></ul></div><p>
						The MCO is not the only Operator that can change operating system components on OpenShift Container Platform nodes. Other Operators can modify operating system-level features as well. One example is the Node Tuning Operator, which allows you to do node-level tuning through Tuned daemon profiles.
					</p><p>
						Tasks for the MCO configuration that can be done post-installation are included in the following procedures. See descriptions of RHCOS bare metal installation for system configuration tasks that must be done during or before OpenShift Container Platform installation.
					</p><p>
						There might be situations where the configuration on a node does not fully match what the currently-applied machine config specifies. This state is called <span class="emphasis"><em>configuration drift</em></span>. The Machine Config Daemon (MCD) regularly checks the nodes for configuration drift. If the MCD detects configuration drift, the MCO marks the node <code class="literal">degraded</code> until an administrator corrects the node configuration. A degraded node is online and operational, but, it cannot be updated. For more information on configuration drift, see <span class="emphasis"><em>Understanding configuration drift detection</em></span>.
					</p></section><section class="section" id="project-2"><div class="titlepage"><div><div><h4 class="title">6.1.2.2. Project</h4></div></div></div><p>
						See the <a class="link" href="https://github.com/openshift/machine-config-operator">openshift-machine-config-operator</a> GitHub site for details.
					</p></section></section><section class="section" id="machine-config-drift-detection_post-install-machine-configuration-tasks"><div class="titlepage"><div><div><h3 class="title">6.1.3. Understanding configuration drift detection</h3></div></div></div><p>
					There might be situations when the on-disk state of a node differs from what is configured in the machine config. This is known as <span class="emphasis"><em>configuration drift</em></span>. For example, a cluster admin might manually modify a file, a systemd unit file, or a file permission that was configured through a machine config. This causes configuration drift. Configuration drift can cause problems between nodes in a Machine Config Pool or when the machine configs are updated.
				</p><p>
					The Machine Config Operator (MCO) uses the Machine Config Daemon (MCD) to check nodes for configuration drift on a regular basis. If detected, the MCO sets the node and the machine config pool (MCP) to <code class="literal">Degraded</code> and reports the error. A degraded node is online and operational, but, it cannot be updated.
				</p><p>
					The MCD performs configuration drift detection upon each of the following conditions:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							When a node boots.
						</li><li class="listitem">
							After any of the files (Ignition files and systemd drop-in units) specified in the machine config are modified outside of the machine config.
						</li><li class="listitem"><p class="simpara">
							Before a new machine config is applied.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If you apply a new machine config to the nodes, the MCD temporarily shuts down configuration drift detection. This shutdown is needed because the new machine config necessarily differs from the machine config on the nodes. After the new machine config is applied, the MCD restarts detecting configuration drift using the new machine config.
							</p></div></div></li></ul></div><p>
					When performing configuration drift detection, the MCD validates that the file contents and permissions fully match what the currently-applied machine config specifies. Typically, the MCD detects configuration drift in less than a second after the detection is triggered.
				</p><p>
					If the MCD detects configuration drift, the MCD performs the following tasks:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Emits an error to the console logs
						</li><li class="listitem">
							Emits a Kubernetes event
						</li><li class="listitem">
							Stops further detection on the node
						</li><li class="listitem">
							Sets the node and MCP to <code class="literal">degraded</code>
						</li></ul></div><p>
					You can check if you have a degraded node by listing the MCPs:
				</p><pre class="programlisting language-terminal">$ oc get mcp worker</pre><p>
					If you have a degraded MCP, the <code class="literal">DEGRADEDMACHINECOUNT</code> field is non-zero, similar to the following output:
				</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
worker   rendered-worker-404caf3180818d8ac1f50c32f14b57c3   False     True       True       2              1                   1                     1                      5h51m</pre>

					</p></div><p>
					You can determine if the problem is caused by configuration drift by examining the machine config pool:
				</p><pre class="programlisting language-terminal">$ oc describe mcp worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal"> ...
    Last Transition Time:  2021-12-20T18:54:00Z
    Message:               Node ci-ln-j4h8nkb-72292-pxqxz-worker-a-fjks4 is reporting: "content mismatch for file \"/etc/mco-test-file\"" <span id="CO16-1"><!--Empty--></span><span class="callout">1</span>
    Reason:                1 nodes are reporting degraded status on sync
    Status:                True
    Type:                  NodeDegraded <span id="CO16-2"><!--Empty--></span><span class="callout">2</span>
 ...</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO16-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							This message shows that a node’s <code class="literal">/etc/mco-test-file</code> file, which was added by the machine config, has changed outside of the machine config.
						</div></dd><dt><a href="#CO16-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							The state of the node is <code class="literal">NodeDegraded</code>.
						</div></dd></dl></div><p>
					Or, if you know which node is degraded, examine that node:
				</p><pre class="programlisting language-terminal">$ oc describe node/ci-ln-j4h8nkb-72292-pxqxz-worker-a-fjks4</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal"> ...

Annotations:        cloud.network.openshift.io/egress-ipconfig: [{"interface":"nic0","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ip":10}}]
                    csi.volume.kubernetes.io/nodeid:
                      {"pd.csi.storage.gke.io":"projects/openshift-gce-devel-ci/zones/us-central1-a/instances/ci-ln-j4h8nkb-72292-pxqxz-worker-a-fjks4"}
                    machine.openshift.io/machine: openshift-machine-api/ci-ln-j4h8nkb-72292-pxqxz-worker-a-fjks4
                    machineconfiguration.openshift.io/controlPlaneTopology: HighlyAvailable
                    machineconfiguration.openshift.io/currentConfig: rendered-worker-67bd55d0b02b0f659aef33680693a9f9
                    machineconfiguration.openshift.io/desiredConfig: rendered-worker-67bd55d0b02b0f659aef33680693a9f9
                    machineconfiguration.openshift.io/reason: content mismatch for file "/etc/mco-test-file" <span id="CO17-1"><!--Empty--></span><span class="callout">1</span>
                    machineconfiguration.openshift.io/state: Degraded <span id="CO17-2"><!--Empty--></span><span class="callout">2</span>
 ...</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO17-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The error message indicating that configuration drift was detected between the node and the listed machine config. Here the error message indicates that the contents of the <code class="literal">/etc/mco-test-file</code>, which was added by the machine config, has changed outside of the machine config.
						</div></dd><dt><a href="#CO17-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							The state of the node is <code class="literal">Degraded</code>.
						</div></dd></dl></div><p>
					You can correct configuration drift and return the node to the <code class="literal">Ready</code> state by performing one of the following remediations:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Ensure that the contents and file permissions of the files on the node match what is configured in the machine config. You can manually rewrite the file contents or change the file permissions.
						</li><li class="listitem"><p class="simpara">
							Generate a <a class="link" href="https://access.redhat.com/solutions/5414371">force file</a> on the degraded node. The force file causes the MCD to bypass the usual configuration drift detection and reapplies the current machine config.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Generating a force file on a node causes that node to reboot.
							</p></div></div></li></ul></div></section><section class="section" id="checking-mco-status_post-install-machine-configuration-tasks"><div class="titlepage"><div><div><h3 class="title">6.1.4. Checking machine config pool status</h3></div></div></div><p>
					To see the status of the Machine Config Operator (MCO), its sub-components, and the resources it manages, use the following <code class="literal">oc</code> commands:
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To see the number of MCO-managed nodes available on your cluster for each machine config pool (MCP), run the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineconfigpool</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      CONFIG                    UPDATED  UPDATING   DEGRADED  MACHINECOUNT  READYMACHINECOUNT  UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT  AGE
master    rendered-master-06c9c4…   True     False      False     3             3                  3                   0                     4h42m
worker    rendered-worker-f4b64…    False    True       False     3             2                  2                   0                     4h42m</pre>

							</p></div><p class="simpara">
							where:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">UPDATED</span></dt><dd>
										The <code class="literal">True</code> status indicates that the MCO has applied the current machine config to the nodes in that MCP. The current machine config is specified in the <code class="literal">STATUS</code> field in the <code class="literal">oc get mcp</code> output. The <code class="literal">False</code> status indicates a node in the MCP is updating.
									</dd><dt><span class="term">UPDATING</span></dt><dd>
										The <code class="literal">True</code> status indicates that the MCO is applying the desired machine config, as specified in the <code class="literal">MachineConfigPool</code> custom resource, to at least one of the nodes in that MCP. The desired machine config is the new, edited machine config. Nodes that are updating might not be available for scheduling. The <code class="literal">False</code> status indicates that all nodes in the MCP are updated.
									</dd><dt><span class="term">DEGRADED</span></dt><dd>
										A <code class="literal">True</code> status indicates the MCO is blocked from applying the current or desired machine config to at least one of the nodes in that MCP, or the configuration is failing. Nodes that are degraded might not be available for scheduling. A <code class="literal">False</code> status indicates that all nodes in the MCP are ready.
									</dd><dt><span class="term">MACHINECOUNT</span></dt><dd>
										Indicates the total number of machines in that MCP.
									</dd><dt><span class="term">READYMACHINECOUNT</span></dt><dd>
										Indicates the total number of machines in that MCP that are ready for scheduling.
									</dd><dt><span class="term">UPDATEDMACHINECOUNT</span></dt><dd>
										Indicates the total number of machines in that MCP that have the current machine config.
									</dd><dt><span class="term">DEGRADEDMACHINECOUNT</span></dt><dd>
										Indicates the total number of machines in that MCP that are marked as degraded or unreconcilable.
									</dd></dl></div><p class="simpara">
							In the previous output, there are three control plane (master) nodes and three worker nodes. The control plane MCP and the associated nodes are updated to the current machine config. The nodes in the worker MCP are being updated to the desired machine config. Two of the nodes in the worker MCP are updated and one is still updating, as indicated by the <code class="literal">UPDATEDMACHINECOUNT</code> being <code class="literal">2</code>. There are no issues, as indicated by the <code class="literal">DEGRADEDMACHINECOUNT</code> being <code class="literal">0</code> and <code class="literal">DEGRADED</code> being <code class="literal">False</code>.
						</p><p class="simpara">
							While the nodes in the MCP are updating, the machine config listed under <code class="literal">CONFIG</code> is the current machine config, which the MCP is being updated from. When the update is complete, the listed machine config is the desired machine config, which the MCP was updated to.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If a node is being cordoned, that node is not included in the <code class="literal">READYMACHINECOUNT</code>, but is included in the <code class="literal">MACHINECOUNT</code>. Also, the MCP status is set to <code class="literal">UPDATING</code>. Because the node has the current machine config, it is counted in the <code class="literal">UPDATEDMACHINECOUNT</code> total:
							</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME      CONFIG                    UPDATED  UPDATING   DEGRADED  MACHINECOUNT  READYMACHINECOUNT  UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT  AGE
master    rendered-master-06c9c4…   True     False      False     3             3                  3                   0                     4h42m
worker    rendered-worker-c1b41a…   False    True       False     3             2                  3                   0                     4h42m</pre>

								</p></div></div></div></li><li class="listitem"><p class="simpara">
							To check the status of the nodes in an MCP by examining the <code class="literal">MachineConfigPool</code> custom resource, run the following command: :
						</p><pre class="programlisting language-terminal">$ oc describe mcp worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">...
  Degraded Machine Count:     0
  Machine Count:              3
  Observed Generation:        2
  Ready Machine Count:        3
  Unavailable Machine Count:  0
  Updated Machine Count:      3
Events:                       &lt;none&gt;</pre>

							</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If a node is being cordoned, the node is not included in the <code class="literal">Ready Machine Count</code>. It is included in the <code class="literal">Unavailable Machine Count</code>:
							</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">...
  Degraded Machine Count:     0
  Machine Count:              3
  Observed Generation:        2
  Ready Machine Count:        2
  Unavailable Machine Count:  1
  Updated Machine Count:      3</pre>

								</p></div></div></div></li><li class="listitem"><p class="simpara">
							To see each existing <code class="literal">MachineConfig</code> object, run the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineconfigs</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                             GENERATEDBYCONTROLLER          IGNITIONVERSION  AGE
00-master                        2c9371fbb673b97a6fe8b1c52...   3.2.0            5h18m
00-worker                        2c9371fbb673b97a6fe8b1c52...   3.2.0            5h18m
01-master-container-runtime      2c9371fbb673b97a6fe8b1c52...   3.2.0            5h18m
01-master-kubelet                2c9371fbb673b97a6fe8b1c52…     3.2.0            5h18m
...
rendered-master-dde...           2c9371fbb673b97a6fe8b1c52...   3.2.0            5h18m
rendered-worker-fde...           2c9371fbb673b97a6fe8b1c52...   3.2.0            5h18m</pre>

							</p></div><p class="simpara">
							Note that the <code class="literal">MachineConfig</code> objects listed as <code class="literal">rendered</code> are not meant to be changed or deleted.
						</p></li><li class="listitem"><p class="simpara">
							To view the contents of a particular machine config (in this case, <code class="literal">01-master-kubelet</code>), run the following command:
						</p><pre class="programlisting language-terminal">$ oc describe machineconfigs 01-master-kubelet</pre><p class="simpara">
							The output from the command shows that this <code class="literal">MachineConfig</code> object contains both configuration files (<code class="literal">cloud.conf</code> and <code class="literal">kubelet.conf</code>) and a systemd service (Kubernetes Kubelet):
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Name:         01-master-kubelet
...
Spec:
  Config:
    Ignition:
      Version:  3.2.0
    Storage:
      Files:
        Contents:
          Source:   data:,
        Mode:       420
        Overwrite:  true
        Path:       /etc/kubernetes/cloud.conf
        Contents:
          Source:   data:,kind%3A%20KubeletConfiguration%0AapiVersion%3A%20kubelet.config.k8s.io%2Fv1beta1%0Aauthentication%3A%0A%20%20x509%3A%0A%20%20%20%20clientCAFile%3A%20%2Fetc%2Fkubernetes%2Fkubelet-ca.crt%0A%20%20anonymous...
        Mode:       420
        Overwrite:  true
        Path:       /etc/kubernetes/kubelet.conf
    Systemd:
      Units:
        Contents:  [Unit]
Description=Kubernetes Kubelet
Wants=rpc-statd.service network-online.target crio.service
After=network-online.target crio.service

ExecStart=/usr/bin/hyperkube \
    kubelet \
      --config=/etc/kubernetes/kubelet.conf \ ...</pre>

							</p></div></li></ol></div><p>
					If something goes wrong with a machine config that you apply, you can always back out that change. For example, if you had run <code class="literal">oc create -f ./myconfig.yaml</code> to apply a machine config, you could remove that machine config by running the following command:
				</p><pre class="programlisting language-terminal">$ oc delete -f ./myconfig.yaml</pre><p>
					If that was the only problem, the nodes in the affected pool should return to a non-degraded state. This actually causes the rendered configuration to roll back to its previously rendered state.
				</p><p>
					If you add your own machine configs to your cluster, you can use the commands shown in the previous example to check their status and the related status of the pool to which they are applied.
				</p></section></section><section class="section" id="using-machineconfigs-to-change-machines"><div class="titlepage"><div><div><h2 class="title">6.2. Using MachineConfig objects to configure nodes</h2></div></div></div><p>
				You can use the tasks in this section to create <code class="literal">MachineConfig</code> objects that modify files, systemd unit files, and other operating system features running on OpenShift Container Platform nodes. For more ideas on working with machine configs, see content related to <a class="link" href="https://access.redhat.com/solutions/3868301">updating</a> SSH authorized keys, <a class="link" href="https://access.redhat.com/verify-images-ocp4">verifying image signatures</a>, <a class="link" href="https://access.redhat.com/solutions/4727321">enabling SCTP</a>, and <a class="link" href="https://access.redhat.com/solutions/5170251">configuring iSCSI initiatornames</a> for OpenShift Container Platform.
			</p><p>
				OpenShift Container Platform supports <a class="link" href="https://coreos.github.io/ignition/configuration-v3_2/">Ignition specification version 3.2</a>. All new machine configs you create going forward should be based on Ignition specification version 3.2. If you are upgrading your OpenShift Container Platform cluster, any existing Ignition specification version 2.x machine configs will be translated automatically to specification version 3.2.
			</p><p>
				There might be situations where the configuration on a node does not fully match what the currently-applied machine config specifies. This state is called <span class="emphasis"><em>configuration drift</em></span>. The Machine Config Daemon (MCD) regularly checks the nodes for configuration drift. If the MCD detects configuration drift, the MCO marks the node <code class="literal">degraded</code> until an administrator corrects the node configuration. A degraded node is online and operational, but, it cannot be updated. For more information on configuration drift, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#machine-config-drift-detection_post-install-machine-configuration-tasks">Understanding configuration drift detection</a>.
			</p><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
				Use the following "Configuring chrony time service" procedure as a model for how to go about adding other configuration files to OpenShift Container Platform nodes.
			</p></div></div><section class="section" id="installation-special-config-chrony_post-install-machine-configuration-tasks"><div class="titlepage"><div><div><h3 class="title">6.2.1. Configuring chrony time service</h3></div></div></div><p>
					You can set the time server and related settings used by the chrony time service (<code class="literal">chronyd</code>) by modifying the contents of the <code class="literal">chrony.conf</code> file and passing those contents to your nodes as a machine config.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a Butane config including the contents of the <code class="literal">chrony.conf</code> file. For example, to configure chrony on worker nodes, create a <code class="literal">99-worker-chrony.bu</code> file.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								See "Creating machine configs with Butane" for information about Butane.
							</p></div></div><pre class="programlisting language-yaml">variant: openshift
version: 4.13.0
metadata:
  name: 99-worker-chrony <span id="CO18-1"><!--Empty--></span><span class="callout">1</span>
  labels:
    machineconfiguration.openshift.io/role: worker <span id="CO18-2"><!--Empty--></span><span class="callout">2</span>
storage:
  files:
  - path: /etc/chrony.conf
    mode: 0644 <span id="CO18-3"><!--Empty--></span><span class="callout">3</span>
    overwrite: true
    contents:
      inline: |
        pool 0.rhel.pool.ntp.org iburst <span id="CO18-4"><!--Empty--></span><span class="callout">4</span>
        driftfile /var/lib/chrony/drift
        makestep 1.0 3
        rtcsync
        logdir /var/log/chrony</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO18-1"><span class="callout">1</span></a> <a href="#CO18-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									On control plane nodes, substitute <code class="literal">master</code> for <code class="literal">worker</code> in both of these locations.
								</div></dd><dt><a href="#CO18-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify an octal value mode for the <code class="literal">mode</code> field in the machine config file. After creating the file and applying the changes, the <code class="literal">mode</code> is converted to a decimal value. You can check the YAML file with the command <code class="literal">oc get mc &lt;mc-name&gt; -o yaml</code>.
								</div></dd><dt><a href="#CO18-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specify any valid, reachable time source, such as the one provided by your DHCP server. Alternately, you can specify any of the following NTP servers: <code class="literal">1.rhel.pool.ntp.org</code>, <code class="literal">2.rhel.pool.ntp.org</code>, or <code class="literal">3.rhel.pool.ntp.org</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Use Butane to generate a <code class="literal">MachineConfig</code> object file, <code class="literal">99-worker-chrony.yaml</code>, containing the configuration to be delivered to the nodes:
						</p><pre class="programlisting language-terminal">$ butane 99-worker-chrony.bu -o 99-worker-chrony.yaml</pre></li><li class="listitem"><p class="simpara">
							Apply the configurations in one of two ways:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									If the cluster is not running yet, after you generate manifest files, add the <code class="literal">MachineConfig</code> object file to the <code class="literal">&lt;installation_directory&gt;/openshift</code> directory, and then continue to create the cluster.
								</li><li class="listitem"><p class="simpara">
									If the cluster is already running, apply the file:
								</p><pre class="programlisting language-terminal">$ oc apply -f ./99-worker-chrony.yaml</pre></li></ul></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-special-config-butane_installing-customizing">Creating machine configs with Butane</a>
						</li></ul></div></section><section class="section" id="cnf-disable-chronyd_post-install-machine-configuration-tasks"><div class="titlepage"><div><div><h3 class="title">6.2.2. Disabling the chrony time service</h3></div></div></div><p>
					You can disable the chrony time service (<code class="literal">chronyd</code>) for nodes with a specific role by using a <code class="literal">MachineConfig</code> custom resource (CR).
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the <code class="literal">MachineConfig</code> CR that disables <code class="literal">chronyd</code> for the specified node role.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Save the following YAML in the <code class="literal">disable-chronyd.yaml</code> file:
								</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: &lt;node_role&gt; <span id="CO19-1"><!--Empty--></span><span class="callout">1</span>
  name: disable-chronyd
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
        - contents: |
            [Unit]
            Description=NTP client/server
            Documentation=man:chronyd(8) man:chrony.conf(5)
            After=ntpdate.service sntp.service ntpd.service
            Conflicts=ntpd.service systemd-timesyncd.service
            ConditionCapability=CAP_SYS_TIME
            [Service]
            Type=forking
            PIDFile=/run/chrony/chronyd.pid
            EnvironmentFile=-/etc/sysconfig/chronyd
            ExecStart=/usr/sbin/chronyd $OPTIONS
            ExecStartPost=/usr/libexec/chrony-helper update-daemon
            PrivateTmp=yes
            ProtectHome=yes
            ProtectSystem=full
            [Install]
            WantedBy=multi-user.target
          enabled: false
          name: "chronyd.service"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO19-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Node role where you want to disable <code class="literal">chronyd</code>, for example, <code class="literal">master</code>.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">MachineConfig</code> CR by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f disable-chronyd.yaml</pre></li></ol></div></li></ol></div></section><section class="section" id="nodes-nodes-kernel-arguments_post-install-machine-configuration-tasks"><div class="titlepage"><div><div><h3 class="title">6.2.3. Adding kernel arguments to nodes</h3></div></div></div><p>
					In some special cases, you might want to add kernel arguments to a set of nodes in your cluster. This should only be done with caution and clear understanding of the implications of the arguments you set.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Improper use of kernel arguments can result in your systems becoming unbootable.
					</p></div></div><p>
					Examples of kernel arguments you could set include:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<span class="strong strong"><strong>enforcing=0</strong></span>: Configures Security Enhanced Linux (SELinux) to run in permissive mode. In permissive mode, the system acts as if SELinux is enforcing the loaded security policy, including labeling objects and emitting access denial entries in the logs, but it does not actually deny any operations. While not supported for production systems, permissive mode can be helpful for debugging.
						</li><li class="listitem">
							<span class="strong strong"><strong>nosmt</strong></span>: Disables symmetric multithreading (SMT) in the kernel. Multithreading allows multiple logical threads for each CPU. You could consider <code class="literal">nosmt</code> in multi-tenant environments to reduce risks from potential cross-thread attacks. By disabling SMT, you essentially choose security over performance.
						</li><li class="listitem">
							<span class="strong strong"><strong>systemd.unified_cgroup_hierarchy</strong></span>: Enables <a class="link" href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html">Linux control group version 2</a> (cgroup v2). cgroup v2 is the next version of the kernel <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/ch01">control group</a> and offers multiple improvements.
						</li></ul></div><p>
					See <a class="link" href="https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt">Kernel.org kernel parameters</a> for a list and descriptions of kernel arguments.
				</p><p>
					In the following procedure, you create a <code class="literal">MachineConfig</code> object that identifies:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							A set of machines to which you want to add the kernel argument. In this case, machines with a worker role.
						</li><li class="listitem">
							Kernel arguments that are appended to the end of the existing kernel arguments.
						</li><li class="listitem">
							A label that indicates where in the list of machine configs the change is applied.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Have administrative privilege to a working OpenShift Container Platform cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							List existing <code class="literal">MachineConfig</code> objects for your OpenShift Container Platform cluster to determine how to label your machine config:
						</p><pre class="programlisting language-terminal">$ oc get MachineConfig</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
00-master                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
00-worker                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-ssh                                                                                 3.2.0             40m
99-worker-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-worker-ssh                                                                                 3.2.0             40m
rendered-master-23e785de7587df95a4b517e0647e5ab7   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
rendered-worker-5d596d9293ca3ea80c896a1191735bb1   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">MachineConfig</code> object file that identifies the kernel argument (for example, <code class="literal">05-worker-kernelarg-selinuxpermissive.yaml</code>)
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker<span id="CO20-1"><!--Empty--></span><span class="callout">1</span>
  name: 05-worker-kernelarg-selinuxpermissive<span id="CO20-2"><!--Empty--></span><span class="callout">2</span>
spec:
  kernelArguments:
    - enforcing=0<span id="CO20-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO20-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Applies the new kernel argument only to worker nodes.
								</div></dd><dt><a href="#CO20-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Named to identify where it fits among the machine configs (05) and what it does (adds a kernel argument to configure SELinux permissive mode).
								</div></dd><dt><a href="#CO20-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Identifies the exact kernel argument as <code class="literal">enforcing=0</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the new machine config:
						</p><pre class="programlisting language-terminal">$ oc create -f 05-worker-kernelarg-selinuxpermissive.yaml</pre></li><li class="listitem"><p class="simpara">
							Check the machine configs to see that the new one was added:
						</p><pre class="programlisting language-terminal">$ oc get MachineConfig</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
00-master                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
00-worker                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
05-worker-kernelarg-selinuxpermissive                                                         3.2.0             105s
99-master-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-ssh                                                                                 3.2.0             40m
99-worker-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-worker-ssh                                                                                 3.2.0             40m
rendered-master-23e785de7587df95a4b517e0647e5ab7   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
rendered-worker-5d596d9293ca3ea80c896a1191735bb1   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check the nodes:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                           STATUS                     ROLES    AGE   VERSION
ip-10-0-136-161.ec2.internal   Ready                      worker   28m   v1.26.0
ip-10-0-136-243.ec2.internal   Ready                      master   34m   v1.26.0
ip-10-0-141-105.ec2.internal   Ready,SchedulingDisabled   worker   28m   v1.26.0
ip-10-0-142-249.ec2.internal   Ready                      master   34m   v1.26.0
ip-10-0-153-11.ec2.internal    Ready                      worker   28m   v1.26.0
ip-10-0-153-150.ec2.internal   Ready                      master   34m   v1.26.0</pre>

							</p></div><p class="simpara">
							You can see that scheduling on each worker node is disabled as the change is being applied.
						</p></li><li class="listitem"><p class="simpara">
							Check that the kernel argument worked by going to one of the worker nodes and listing the kernel command line arguments (in <code class="literal">/proc/cmdline</code> on the host):
						</p><pre class="programlisting language-terminal">$ oc debug node/ip-10-0-141-105.ec2.internal</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Starting pod/ip-10-0-141-105ec2internal-debug ...
To use host binaries, run `chroot /host`

sh-4.2# cat /host/proc/cmdline
BOOT_IMAGE=/ostree/rhcos-... console=tty0 console=ttyS0,115200n8
rootflags=defaults,prjquota rw root=UUID=fd0... ostree=/ostree/boot.0/rhcos/16...
coreos.oem.id=qemu coreos.oem.id=ec2 ignition.platform.id=ec2 enforcing=0

sh-4.2# exit</pre>

							</p></div><p class="simpara">
							You should see the <code class="literal">enforcing=0</code> argument added to the other kernel arguments.
						</p></li></ol></div></section><section class="section" id="rhcos-enabling-multipath-day-2_post-install-machine-configuration-tasks"><div class="titlepage"><div><div><h3 class="title">6.2.4. Enabling multipathing with kernel arguments on RHCOS</h3></div></div></div><p>
					Red Hat Enterprise Linux CoreOS (RHCOS) supports multipathing on the primary disk, allowing stronger resilience to hardware failure to achieve higher host availability. Post-installation support is available by activating multipathing via the machine config.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Enabling multipathing during installation is supported and recommended for nodes provisioned in OpenShift Container Platform 4.8 or higher. In setups where any I/O to non-optimized paths results in I/O system errors, you must enable multipathing at installation time. For more information about enabling multipathing during installation time, see "Enabling multipathing with kernel arguments on RHCOS" in the <span class="emphasis"><em>Installing on bare metal</em></span> documentation.
					</p></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						On IBM Z and IBM® LinuxONE, you can enable multipathing only if you configured your cluster for it during installation. For more information, see "Installing RHCOS and starting the OpenShift Container Platform bootstrap process" in <span class="emphasis"><em>Installing a cluster with z/VM on IBM Z and IBM® LinuxONE</em></span>.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have a running OpenShift Container Platform cluster that uses version 4.7 or later.
						</li><li class="listitem">
							You are logged in to the cluster as a user with administrative privileges.
						</li><li class="listitem">
							You have confirmed that the disk is enabled for multipathing. Multipathing is only supported on hosts that are connected to a SAN via an HBA adapter.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To enable multipathing post-installation on control plane nodes:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Create a machine config file, such as <code class="literal">99-master-kargs-mpath.yaml</code>, that instructs the cluster to add the <code class="literal">master</code> label and that identifies the multipath kernel argument, for example:
								</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: "master"
  name: 99-master-kargs-mpath
spec:
  kernelArguments:
    - 'rd.multipath=default'
    - 'root=/dev/disk/by-label/dm-mpath-root'</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							To enable multipathing post-installation on worker nodes:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Create a machine config file, such as <code class="literal">99-worker-kargs-mpath.yaml</code>, that instructs the cluster to add the <code class="literal">worker</code> label and that identifies the multipath kernel argument, for example:
								</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: "worker"
  name: 99-worker-kargs-mpath
spec:
  kernelArguments:
    - 'rd.multipath=default'
    - 'root=/dev/disk/by-label/dm-mpath-root'</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							Create the new machine config by using either the master or worker YAML file you previously created:
						</p><pre class="programlisting language-terminal">$ oc create -f ./99-worker-kargs-mpath.yaml</pre></li><li class="listitem"><p class="simpara">
							Check the machine configs to see that the new one was added:
						</p><pre class="programlisting language-terminal">$ oc get MachineConfig</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
00-master                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
00-worker                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-ssh                                                                                 3.2.0             40m
99-worker-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-worker-kargs-mpath                              52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             105s
99-worker-ssh                                                                                 3.2.0             40m
rendered-master-23e785de7587df95a4b517e0647e5ab7   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
rendered-worker-5d596d9293ca3ea80c896a1191735bb1   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check the nodes:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                           STATUS                     ROLES    AGE   VERSION
ip-10-0-136-161.ec2.internal   Ready                      worker   28m   v1.26.0
ip-10-0-136-243.ec2.internal   Ready                      master   34m   v1.26.0
ip-10-0-141-105.ec2.internal   Ready,SchedulingDisabled   worker   28m   v1.26.0
ip-10-0-142-249.ec2.internal   Ready                      master   34m   v1.26.0
ip-10-0-153-11.ec2.internal    Ready                      worker   28m   v1.26.0
ip-10-0-153-150.ec2.internal   Ready                      master   34m   v1.26.0</pre>

							</p></div><p class="simpara">
							You can see that scheduling on each worker node is disabled as the change is being applied.
						</p></li><li class="listitem"><p class="simpara">
							Check that the kernel argument worked by going to one of the worker nodes and listing the kernel command line arguments (in <code class="literal">/proc/cmdline</code> on the host):
						</p><pre class="programlisting language-terminal">$ oc debug node/ip-10-0-141-105.ec2.internal</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Starting pod/ip-10-0-141-105ec2internal-debug ...
To use host binaries, run `chroot /host`

sh-4.2# cat /host/proc/cmdline
...
rd.multipath=default root=/dev/disk/by-label/dm-mpath-root
...

sh-4.2# exit</pre>

							</p></div><p class="simpara">
							You should see the added kernel arguments.
						</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#rhcos-enabling-multipath_installing-bare-metal">Enabling multipathing with kernel arguments on RHCOS</a> for more information about enabling multipathing during installation time.
						</li></ul></div></section><section class="section" id="nodes-nodes-rtkernel-arguments_post-install-machine-configuration-tasks"><div class="titlepage"><div><div><h3 class="title">6.2.5. Adding a real-time kernel to nodes</h3></div></div></div><p>
					Some OpenShift Container Platform workloads require a high degree of determinism.While Linux is not a real-time operating system, the Linux real-time kernel includes a preemptive scheduler that provides the operating system with real-time characteristics.
				</p><p>
					If your OpenShift Container Platform workloads require these real-time characteristics, you can switch your machines to the Linux real-time kernel. For OpenShift Container Platform, 4.13 you can make this switch using a <code class="literal">MachineConfig</code> object. Although making the change is as simple as changing a machine config <code class="literal">kernelType</code> setting to <code class="literal">realtime</code>, there are a few other considerations before making the change:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Currently, real-time kernel is supported only on worker nodes, and only for radio access network (RAN) use.
						</li><li class="listitem">
							The following procedure is fully supported with bare metal installations that use systems that are certified for Red Hat Enterprise Linux for Real Time 8.
						</li><li class="listitem">
							Real-time support in OpenShift Container Platform is limited to specific subscriptions.
						</li><li class="listitem">
							The following procedure is also supported for use with Google Cloud Platform.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Have a running OpenShift Container Platform cluster (version 4.4 or later).
						</li><li class="listitem">
							Log in to the cluster as a user with administrative privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a machine config for the real-time kernel: Create a YAML file (for example, <code class="literal">99-worker-realtime.yaml</code>) that contains a <code class="literal">MachineConfig</code> object for the <code class="literal">realtime</code> kernel type. This example tells the cluster to use a real-time kernel for all worker nodes:
						</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF &gt; 99-worker-realtime.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: "worker"
  name: 99-worker-realtime
spec:
  kernelType: realtime
EOF</pre></li><li class="listitem"><p class="simpara">
							Add the machine config to the cluster. Type the following to add the machine config to the cluster:
						</p><pre class="programlisting language-terminal">$ oc create -f 99-worker-realtime.yaml</pre></li><li class="listitem"><p class="simpara">
							Check the real-time kernel: Once each impacted node reboots, log in to the cluster and run the following commands to make sure that the real-time kernel has replaced the regular kernel for the set of nodes you configured:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                        STATUS  ROLES    AGE   VERSION
ip-10-0-143-147.us-east-2.compute.internal  Ready   worker   103m  v1.26.0
ip-10-0-146-92.us-east-2.compute.internal   Ready   worker   101m  v1.26.0
ip-10-0-169-2.us-east-2.compute.internal    Ready   worker   102m  v1.26.0</pre>

							</p></div><pre class="programlisting language-terminal">$ oc debug node/ip-10-0-143-147.us-east-2.compute.internal</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Starting pod/ip-10-0-143-147us-east-2computeinternal-debug ...
To use host binaries, run `chroot /host`

sh-4.4# uname -a
Linux &lt;worker_node&gt; 4.18.0-147.3.1.rt24.96.el8_1.x86_64 #1 SMP PREEMPT RT
        Wed Nov 27 18:29:55 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux</pre>

							</p></div><p class="simpara">
							The kernel name contains <code class="literal">rt</code> and text “PREEMPT RT” indicates that this is a real-time kernel.
						</p></li><li class="listitem"><p class="simpara">
							To go back to the regular kernel, delete the <code class="literal">MachineConfig</code> object:
						</p><pre class="programlisting language-terminal">$ oc delete -f 99-worker-realtime.yaml</pre></li></ol></div></section><section class="section" id="machineconfig-modify-journald_post-install-machine-configuration-tasks"><div class="titlepage"><div><div><h3 class="title">6.2.6. Configuring journald settings</h3></div></div></div><p>
					If you need to configure settings for the <code class="literal">journald</code> service on OpenShift Container Platform nodes, you can do that by modifying the appropriate configuration file and passing the file to the appropriate pool of nodes as a machine config.
				</p><p>
					This procedure describes how to modify <code class="literal">journald</code> rate limiting settings in the <code class="literal">/etc/systemd/journald.conf</code> file and apply them to worker nodes. See the <code class="literal">journald.conf</code> man page for information on how to use that file.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Have a running OpenShift Container Platform cluster.
						</li><li class="listitem">
							Log in to the cluster as a user with administrative privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a Butane config file, <code class="literal">40-worker-custom-journald.bu</code>, that includes an <code class="literal">/etc/systemd/journald.conf</code> file with the required settings.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								See "Creating machine configs with Butane" for information about Butane.
							</p></div></div><pre class="programlisting language-yaml">variant: openshift
version: 4.13.0
metadata:
  name: 40-worker-custom-journald
  labels:
    machineconfiguration.openshift.io/role: worker
storage:
  files:
  - path: /etc/systemd/journald.conf
    mode: 0644
    overwrite: true
    contents:
      inline: |
        # Disable rate limiting
        RateLimitInterval=1s
        RateLimitBurst=10000
        Storage=volatile
        Compress=no
        MaxRetentionSec=30s</pre></li><li class="listitem"><p class="simpara">
							Use Butane to generate a <code class="literal">MachineConfig</code> object file, <code class="literal">40-worker-custom-journald.yaml</code>, containing the configuration to be delivered to the worker nodes:
						</p><pre class="programlisting language-terminal">$ butane 40-worker-custom-journald.bu -o 40-worker-custom-journald.yaml</pre></li><li class="listitem"><p class="simpara">
							Apply the machine config to the pool:
						</p><pre class="programlisting language-terminal">$ oc apply -f 40-worker-custom-journald.yaml</pre></li><li class="listitem"><p class="simpara">
							Check that the new machine config is applied and that the nodes are not in a degraded state. It might take a few minutes. The worker pool will show the updates in progress, as each node successfully has the new machine config applied:
						</p><pre class="programlisting language-terminal">$ oc get machineconfigpool
NAME   CONFIG             UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE
master rendered-master-35 True    False    False    3            3                 3                   0                    34m
worker rendered-worker-d8 False   True     False    3            1                 1                   0                    34m</pre></li><li class="listitem"><p class="simpara">
							To check that the change was applied, you can log in to a worker node:
						</p><pre class="programlisting language-terminal">$ oc get node | grep worker
ip-10-0-0-1.us-east-2.compute.internal   Ready    worker   39m   v0.0.0-master+$Format:%h$
$ oc debug node/ip-10-0-0-1.us-east-2.compute.internal
Starting pod/ip-10-0-141-142us-east-2computeinternal-debug ...
...
sh-4.2# chroot /host
sh-4.4# cat /etc/systemd/journald.conf
# Disable rate limiting
RateLimitInterval=1s
RateLimitBurst=10000
Storage=volatile
Compress=no
MaxRetentionSec=30s
sh-4.4# exit</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-special-config-butane_installing-customizing">Creating machine configs with Butane</a>
						</li></ul></div></section><section class="section" id="rhcos-add-extensions_post-install-machine-configuration-tasks"><div class="titlepage"><div><div><h3 class="title">6.2.7. Adding extensions to RHCOS</h3></div></div></div><p>
					RHCOS is a minimal container-oriented RHEL operating system, designed to provide a common set of capabilities to OpenShift Container Platform clusters across all platforms. While adding software packages to RHCOS systems is generally discouraged, the MCO provides an <code class="literal">extensions</code> feature you can use to add a minimal set of features to RHCOS nodes.
				</p><p>
					Currently, the following extensions are available:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<span class="strong strong"><strong>usbguard</strong></span>: Adding the <code class="literal">usbguard</code> extension protects RHCOS systems from attacks from intrusive USB devices. See <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/security_hardening/index#usbguard_protecting-systems-against-intrusive-usb-devices">USBGuard</a> for details.
						</li><li class="listitem">
							<span class="strong strong"><strong>kerberos</strong></span>: Adding the <code class="literal">kerberos</code> extension provides a mechanism that allows both users and machines to identify themselves to the network to receive defined, limited access to the areas and services that an administrator has configured. See <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system-level_authentication_guide/using_kerberos">Using Kerberos</a> for details, including how to set up a Kerberos client and mount a Kerberized NFS share.
						</li></ul></div><p>
					The following procedure describes how to use a machine config to add one or more extensions to your RHCOS nodes.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Have a running OpenShift Container Platform cluster (version 4.6 or later).
						</li><li class="listitem">
							Log in to the cluster as a user with administrative privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a machine config for extensions: Create a YAML file (for example, <code class="literal">80-extensions.yaml</code>) that contains a <code class="literal">MachineConfig</code> <code class="literal">extensions</code> object. This example tells the cluster to add the <code class="literal">usbguard</code> extension.
						</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF &gt; 80-extensions.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 80-worker-extensions
spec:
  config:
    ignition:
      version: 3.2.0
  extensions:
    - usbguard
EOF</pre></li><li class="listitem"><p class="simpara">
							Add the machine config to the cluster. Type the following to add the machine config to the cluster:
						</p><pre class="programlisting language-terminal">$ oc create -f 80-extensions.yaml</pre><p class="simpara">
							This sets all worker nodes to have rpm packages for <code class="literal">usbguard</code> installed.
						</p></li><li class="listitem"><p class="simpara">
							Check that the extensions were applied:
						</p><pre class="programlisting language-terminal">$ oc get machineconfig 80-worker-extensions</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                 GENERATEDBYCONTROLLER IGNITIONVERSION AGE
80-worker-extensions                       3.2.0           57s</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check that the new machine config is now applied and that the nodes are not in a degraded state. It may take a few minutes. The worker pool will show the updates in progress, as each machine successfully has the new machine config applied:
						</p><pre class="programlisting language-terminal">$ oc get machineconfigpool</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME   CONFIG             UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE
master rendered-master-35 True    False    False    3            3                 3                   0                    34m
worker rendered-worker-d8 False   True     False    3            1                 1                   0                    34m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check the extensions. To check that the extension was applied, run:
						</p><pre class="programlisting language-terminal">$ oc get node | grep worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                        STATUS  ROLES    AGE   VERSION
ip-10-0-169-2.us-east-2.compute.internal    Ready   worker   102m  v1.26.0</pre>

							</p></div><pre class="programlisting language-terminal">$ oc debug node/ip-10-0-169-2.us-east-2.compute.internal</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">...
To use host binaries, run `chroot /host`
sh-4.4# chroot /host
sh-4.4# rpm -q usbguard
usbguard-0.7.4-4.el8.x86_64.rpm</pre>

							</p></div></li></ol></div></section><section class="section" id="rhcos-load-firmware-blobs_post-install-machine-configuration-tasks"><div class="titlepage"><div><div><h3 class="title">6.2.8. Loading custom firmware blobs in the machine config manifest</h3></div></div></div><p>
					Because the default location for firmware blobs in <code class="literal">/usr/lib</code> is read-only, you can locate a custom firmware blob by updating the search path. This enables you to load local firmware blobs in the machine config manifest when the blobs are not managed by RHCOS.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a Butane config file, <code class="literal">98-worker-firmware-blob.bu</code>, that updates the search path so that it is root-owned and writable to local storage. The following example places the custom blob file from your local workstation onto nodes under <code class="literal">/var/lib/firmware</code>.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								See "Creating machine configs with Butane" for information about Butane.
							</p></div></div><div class="formalpara"><p class="title"><strong>Butane config file for custom firmware blob</strong></p><p>
								
<pre class="programlisting language-yaml">variant: openshift
version: 4.13.0
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 98-worker-firmware-blob
storage:
  files:
  - path: /var/lib/firmware/&lt;package_name&gt; <span id="CO21-1"><!--Empty--></span><span class="callout">1</span>
    contents:
      local: &lt;package_name&gt; <span id="CO21-2"><!--Empty--></span><span class="callout">2</span>
    mode: 0644 <span id="CO21-3"><!--Empty--></span><span class="callout">3</span>
openshift:
  kernel_arguments:
    - 'firmware_class.path=/var/lib/firmware' <span id="CO21-4"><!--Empty--></span><span class="callout">4</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO21-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Sets the path on the node where the firmware package is copied to.
								</div></dd><dt><a href="#CO21-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specifies a file with contents that are read from a local file directory on the system running Butane. The path of the local file is relative to a <code class="literal">files-dir</code> directory, which must be specified by using the <code class="literal">--files-dir</code> option with Butane in the following step.
								</div></dd><dt><a href="#CO21-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Sets the permissions for the file on the RHCOS node. It is recommended to set <code class="literal">0644</code> permissions.
								</div></dd><dt><a href="#CO21-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The <code class="literal">firmware_class.path</code> parameter customizes the kernel search path of where to look for the custom firmware blob that was copied from your local workstation onto the root file system of the node. This example uses <code class="literal">/var/lib/firmware</code> as the customized path.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Run Butane to generate a <code class="literal">MachineConfig</code> object file that uses a copy of the firmware blob on your local workstation named <code class="literal">98-worker-firmware-blob.yaml</code>. The firmware blob contains the configuration to be delivered to the nodes. The following example uses the <code class="literal">--files-dir</code> option to specify the directory on your workstation where the local file or files are located:
						</p><pre class="programlisting language-terminal">$ butane 98-worker-firmware-blob.bu -o 98-worker-firmware-blob.yaml --files-dir &lt;directory_including_package_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Apply the configurations to the nodes in one of two ways:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									If the cluster is not running yet, after you generate manifest files, add the <code class="literal">MachineConfig</code> object file to the <code class="literal">&lt;installation_directory&gt;/openshift</code> directory, and then continue to create the cluster.
								</li><li class="listitem"><p class="simpara">
									If the cluster is already running, apply the file:
								</p><pre class="programlisting language-terminal">$ oc apply -f 98-worker-firmware-blob.yaml</pre><p class="simpara">
									A <code class="literal">MachineConfig</code> object YAML file is created for you to finish configuring your machines.
								</p></li></ul></div></li><li class="listitem">
							Save the Butane config in case you need to update the <code class="literal">MachineConfig</code> object in the future.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-special-config-butane_installing-customizing">Creating machine configs with Butane</a>
						</li></ul></div></section><section class="section" id="core-user-password_post-install-machine-configuration-tasks"><div class="titlepage"><div><div><h3 class="title">6.2.9. Changing the core user password for node access</h3></div></div></div><p>
					By default, Red Hat Enterprise Linux CoreOS (RHCOS) creates a user named <code class="literal">core</code> on the nodes in your cluster. You can use the <code class="literal">core</code> user to access the node through a cloud provider serial console or a bare metal baseboard controller manager (BMC). This can be helpful, for example, if a node is down and you cannot access that node by using SSH or the <code class="literal">oc debug node</code> command. However, by default, there is no password for this user, so you cannot log in without creating one.
				</p><p>
					You can create a password for the <code class="literal">core</code> user by using a machine config. The Machine Config Operator (MCO) assigns the password and injects the password into the <code class="literal">/etc/shadow</code> file, allowing you to log in with the <code class="literal">core</code> user. The MCO does not examine the password hash. As such, the MCO cannot report if there is a problem with the password.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								The password works only through a cloud provider serial console or a BMC. It does not work with SSH.
							</li><li class="listitem">
								If you have a machine config that includes an <code class="literal">/etc/shadow</code> file or a systemd unit that sets a password, it takes precedence over the password hash.
							</li></ul></div></div></div><p>
					You can change the password, if needed, by editing the machine config you used to create the password. Also, you can remove the password by deleting the machine config. Deleting the machine config does not remove the user account.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Create a hashed password by using a tool that is supported by your operating system.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a machine config file that contains the <code class="literal">core</code> username and the hashed password:
						</p><pre class="programlisting language-terminal">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: set-core-user-password
spec:
  config:
    ignition:
      version: 3.2.0
    passwd:
      users:
      - name: core <span id="CO22-1"><!--Empty--></span><span class="callout">1</span>
        passwordHash: &lt;password&gt; <span id="CO22-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO22-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									This must be <code class="literal">core</code>.
								</div></dd><dt><a href="#CO22-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The hashed password to use with the <code class="literal">core</code> account.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the machine config by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre><p class="simpara">
							The nodes do not reboot and should become available in a few moments. You can use the <code class="literal">oc get mcp</code> to watch for the machine config pools to be updated, as shown in the following example:
						</p><pre class="screen">NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master   rendered-master-d686a3ffc8fdec47280afec446fce8dd   True      False      False      3              3                   3                     0                      64m
worker   rendered-worker-4605605a5b1f9de1d061e9d350f251e5   False     True       False      3              0                   0                     0                      64m</pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							After the nodes return to the <code class="literal">UPDATED=True</code> state, start a debug session for a node by running the following command:
						</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Set <code class="literal">/host</code> as the root directory within the debug shell by running the following command:
						</p><pre class="programlisting language-terminal">sh-4.4# chroot /host</pre></li><li class="listitem"><p class="simpara">
							Check the contents of the <code class="literal">/etc/shadow</code> file:
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">...
core:$6$2sE/010goDuRSxxv$o18K52wor.wIwZp:19418:0:99999:7:::
...</pre>

							</p></div><p class="simpara">
							The hashed password is assigned to the <code class="literal">core</code> user.
						</p></li></ol></div></section></section><section class="section" id="configuring-machines-with-custom-resources"><div class="titlepage"><div><div><h2 class="title">6.3. Configuring MCO-related custom resources</h2></div></div></div><p>
				Besides managing <code class="literal">MachineConfig</code> objects, the MCO manages two custom resources (CRs): <code class="literal">KubeletConfig</code> and <code class="literal">ContainerRuntimeConfig</code>. Those CRs let you change node-level settings impacting how the Kubelet and CRI-O container runtime services behave.
			</p><section class="section" id="create-a-kubeletconfig-crd-to-edit-kubelet-parameters_post-install-machine-configuration-tasks"><div class="titlepage"><div><div><h3 class="title">6.3.1. Creating a KubeletConfig CRD to edit kubelet parameters</h3></div></div></div><p>
					The kubelet configuration is currently serialized as an Ignition configuration, so it can be directly edited. However, there is also a new <code class="literal">kubelet-config-controller</code> added to the Machine Config Controller (MCC). This lets you use a <code class="literal">KubeletConfig</code> custom resource (CR) to edit the kubelet parameters.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						As the fields in the <code class="literal">kubeletConfig</code> object are passed directly to the kubelet from upstream Kubernetes, the kubelet validates those values directly. Invalid values in the <code class="literal">kubeletConfig</code> object might cause cluster nodes to become unavailable. For valid values, see the <a class="link" href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/">Kubernetes documentation</a>.
					</p></div></div><p>
					Consider the following guidance:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Create one <code class="literal">KubeletConfig</code> CR for each machine config pool with all the config changes you want for that pool. If you are applying the same content to all of the pools, you need only one <code class="literal">KubeletConfig</code> CR for all of the pools.
						</li><li class="listitem">
							Edit an existing <code class="literal">KubeletConfig</code> CR to modify existing settings or add new settings, instead of creating a CR for each change. It is recommended that you create a CR only to modify a different machine config pool, or for changes that are intended to be temporary, so that you can revert the changes.
						</li><li class="listitem">
							As needed, create multiple <code class="literal">KubeletConfig</code> CRs with a limit of 10 per cluster. For the first <code class="literal">KubeletConfig</code> CR, the Machine Config Operator (MCO) creates a machine config appended with <code class="literal">kubelet</code>. With each subsequent CR, the controller creates another <code class="literal">kubelet</code> machine config with a numeric suffix. For example, if you have a <code class="literal">kubelet</code> machine config with a <code class="literal">-2</code> suffix, the next <code class="literal">kubelet</code> machine config is appended with <code class="literal">-3</code>.
						</li></ul></div><p>
					If you want to delete the machine configs, delete them in reverse order to avoid exceeding the limit. For example, you delete the <code class="literal">kubelet-3</code> machine config before deleting the <code class="literal">kubelet-2</code> machine config.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you have a machine config with a <code class="literal">kubelet-9</code> suffix, and you create another <code class="literal">KubeletConfig</code> CR, a new machine config is not created, even if there are fewer than 10 <code class="literal">kubelet</code> machine configs.
					</p></div></div><div class="formalpara"><p class="title"><strong>Example <code class="literal">KubeletConfig</code> CR</strong></p><p>
						
<pre class="programlisting language-terminal">$ oc get kubeletconfig</pre>

					</p></div><pre class="programlisting language-terminal">NAME                AGE
set-max-pods        15m</pre><div class="formalpara"><p class="title"><strong>Example showing a <code class="literal">KubeletConfig</code> machine config</strong></p><p>
						
<pre class="programlisting language-terminal">$ oc get mc | grep kubelet</pre>

					</p></div><pre class="programlisting language-terminal">...
99-worker-generated-kubelet-1                  b5c5119de007945b6fe6fb215db3b8e2ceb12511   3.2.0             26m
...</pre><p>
					The following procedure is an example to show how to configure the maximum number of pods per node on the worker nodes.
				</p><div class="orderedlist"><p class="title"><strong>Prerequisites</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the label associated with the static <code class="literal">MachineConfigPool</code> CR for the type of node you want to configure. Perform one of the following steps:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									View the machine config pool:
								</p><pre class="programlisting language-terminal">$ oc describe machineconfigpool &lt;name&gt;</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc describe machineconfigpool worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: 2019-02-08T14:52:39Z
  generation: 1
  labels:
    custom-kubelet: set-max-pods <span id="CO23-1"><!--Empty--></span><span class="callout">1</span></pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO23-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											If a label has been added it appears under <code class="literal">labels</code>.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									If the label is not present, add a key/value pair:
								</p><pre class="programlisting language-terminal">$ oc label machineconfigpool worker custom-kubelet=set-max-pods</pre></li></ol></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the available machine configuration objects that you can select:
						</p><pre class="programlisting language-terminal">$ oc get machineconfig</pre><p class="simpara">
							By default, the two kubelet-related configs are <code class="literal">01-master-kubelet</code> and <code class="literal">01-worker-kubelet</code>.
						</p></li><li class="listitem"><p class="simpara">
							Check the current value for the maximum pods per node:
						</p><pre class="programlisting language-terminal">$ oc describe node &lt;node_name&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc describe node ci-ln-5grqprb-f76d1-ncnqq-worker-a-mdv94</pre><p class="simpara">
							Look for <code class="literal">value: pods: &lt;value&gt;</code> in the <code class="literal">Allocatable</code> stanza:
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Allocatable:
 attachable-volumes-aws-ebs:  25
 cpu:                         3500m
 hugepages-1Gi:               0
 hugepages-2Mi:               0
 memory:                      15341844Ki
 pods:                        250</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Set the maximum pods per node on the worker nodes by creating a custom resource file that contains the kubelet configuration:
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-max-pods
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: set-max-pods <span id="CO24-1"><!--Empty--></span><span class="callout">1</span>
  kubeletConfig:
    maxPods: 500 <span id="CO24-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO24-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Enter the label from the machine config pool.
								</div></dd><dt><a href="#CO24-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Add the kubelet configuration. In this example, use <code class="literal">maxPods</code> to set the maximum pods per node.
								</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The rate at which the kubelet talks to the API server depends on queries per second (QPS) and burst values. The default values, <code class="literal">50</code> for <code class="literal">kubeAPIQPS</code> and <code class="literal">100</code> for <code class="literal">kubeAPIBurst</code>, are sufficient if there are limited pods running on each node. It is recommended to update the kubelet QPS and burst rates if there are enough CPU and memory resources on the node.
							</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-max-pods
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: set-max-pods
  kubeletConfig:
    maxPods: &lt;pod_count&gt;
    kubeAPIBurst: &lt;burst_rate&gt;
    kubeAPIQPS: &lt;QPS&gt;</pre></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Update the machine config pool for workers with the label:
								</p><pre class="programlisting language-terminal">$ oc label machineconfigpool worker custom-kubelet=set-max-pods</pre></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">KubeletConfig</code> object:
								</p><pre class="programlisting language-terminal">$ oc create -f change-maxPods-cr.yaml</pre></li><li class="listitem"><p class="simpara">
									Verify that the <code class="literal">KubeletConfig</code> object is created:
								</p><pre class="programlisting language-terminal">$ oc get kubeletconfig</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                AGE
set-max-pods        15m</pre>

									</p></div><p class="simpara">
									Depending on the number of worker nodes in the cluster, wait for the worker nodes to be rebooted one by one. For a cluster with 3 worker nodes, this could take about 10 to 15 minutes.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Verify that the changes are applied to the node:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Check on a worker node that the <code class="literal">maxPods</code> value changed:
								</p><pre class="programlisting language-terminal">$ oc describe node &lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
									Locate the <code class="literal">Allocatable</code> stanza:
								</p><pre class="programlisting language-terminal"> ...
Allocatable:
  attachable-volumes-gce-pd:  127
  cpu:                        3500m
  ephemeral-storage:          123201474766
  hugepages-1Gi:              0
  hugepages-2Mi:              0
  memory:                     14225400Ki
  pods:                       500 <span id="CO25-1"><!--Empty--></span><span class="callout">1</span>
 ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO25-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											In this example, the <code class="literal">pods</code> parameter should report the value you set in the <code class="literal">KubeletConfig</code> object.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Verify the change in the <code class="literal">KubeletConfig</code> object:
						</p><pre class="programlisting language-terminal">$ oc get kubeletconfigs set-max-pods -o yaml</pre><p class="simpara">
							This should show a status of <code class="literal">True</code> and <code class="literal">type:Success</code>, as shown in the following example:
						</p><pre class="programlisting language-yaml">spec:
  kubeletConfig:
    maxPods: 500
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: set-max-pods
status:
  conditions:
  - lastTransitionTime: "2021-06-30T17:04:07Z"
    message: Success
    status: "True"
    type: Success</pre></li></ol></div></section><section class="section" id="create-a-containerruntimeconfig_post-install-machine-configuration-tasks"><div class="titlepage"><div><div><h3 class="title">6.3.2. Creating a ContainerRuntimeConfig CR to edit CRI-O parameters</h3></div></div></div><p>
					You can change some of the settings associated with the OpenShift Container Platform CRI-O runtime for the nodes associated with a specific machine config pool (MCP). Using a <code class="literal">ContainerRuntimeConfig</code> custom resource (CR), you set the configuration values and add a label to match the MCP. The MCO then rebuilds the <code class="literal">crio.conf</code> and <code class="literal">storage.conf</code> configuration files on the associated nodes with the updated values.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						To revert the changes implemented by using a <code class="literal">ContainerRuntimeConfig</code> CR, you must delete the CR. Removing the label from the machine config pool does not revert the changes.
					</p></div></div><p>
					You can modify the following settings by using a <code class="literal">ContainerRuntimeConfig</code> CR:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							<span class="strong strong"><strong>PIDs limit</strong></span>: Setting the PIDs limit in the <code class="literal">ContainerRuntimeConfig</code> is expected to be deprecated. If PIDs limits are required, it is recommended to use the <code class="literal">podPidsLimit</code> field in the <code class="literal">KubeletConfig</code> CR instead. The default value of the <code class="literal">podPidsLimit</code> field is <code class="literal">4096</code>.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The CRI-O flag is applied on the cgroup of the container, while the Kubelet flag is set on the cgroup of the pod. Please adjust the PIDs limit accordingly.
							</p></div></div></li><li class="listitem">
							<span class="strong strong"><strong>Log level</strong></span>: The <code class="literal">logLevel</code> parameter sets the CRI-O <code class="literal">log_level</code> parameter, which is the level of verbosity for log messages. The default is <code class="literal">info</code> (<code class="literal">log_level = info</code>). Other options include <code class="literal">fatal</code>, <code class="literal">panic</code>, <code class="literal">error</code>, <code class="literal">warn</code>, <code class="literal">debug</code>, and <code class="literal">trace</code>.
						</li><li class="listitem">
							<span class="strong strong"><strong>Overlay size</strong></span>: The <code class="literal">overlaySize</code> parameter sets the CRI-O Overlay storage driver <code class="literal">size</code> parameter, which is the maximum size of a container image.
						</li><li class="listitem">
							<span class="strong strong"><strong>Maximum log size</strong></span>: Setting the maximum log size in the <code class="literal">ContainerRuntimeConfig</code> is expected to be deprecated. If a maximum log size is required, it is recommended to use the <code class="literal">containerLogMaxSize</code> field in the <code class="literal">KubeletConfig</code> CR instead.
						</li><li class="listitem">
							<span class="strong strong"><strong>Container runtime</strong></span>: The <code class="literal">defaultRuntime</code> parameter sets the container runtime to either <code class="literal">runc</code> or <code class="literal">crun</code>. The default is <code class="literal">runc</code>.
						</li></ul></div><p>
					You should have one <code class="literal">ContainerRuntimeConfig</code> CR for each machine config pool with all the config changes you want for that pool. If you are applying the same content to all the pools, you only need one <code class="literal">ContainerRuntimeConfig</code> CR for all the pools.
				</p><p>
					You should edit an existing <code class="literal">ContainerRuntimeConfig</code> CR to modify existing settings or add new settings instead of creating a new CR for each change. It is recommended to create a new <code class="literal">ContainerRuntimeConfig</code> CR only to modify a different machine config pool, or for changes that are intended to be temporary so that you can revert the changes.
				</p><p>
					You can create multiple <code class="literal">ContainerRuntimeConfig</code> CRs, as needed, with a limit of 10 per cluster. For the first <code class="literal">ContainerRuntimeConfig</code> CR, the MCO creates a machine config appended with <code class="literal">containerruntime</code>. With each subsequent CR, the controller creates a new <code class="literal">containerruntime</code> machine config with a numeric suffix. For example, if you have a <code class="literal">containerruntime</code> machine config with a <code class="literal">-2</code> suffix, the next <code class="literal">containerruntime</code> machine config is appended with <code class="literal">-3</code>.
				</p><p>
					If you want to delete the machine configs, you should delete them in reverse order to avoid exceeding the limit. For example, you should delete the <code class="literal">containerruntime-3</code> machine config before deleting the <code class="literal">containerruntime-2</code> machine config.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you have a machine config with a <code class="literal">containerruntime-9</code> suffix, and you create another <code class="literal">ContainerRuntimeConfig</code> CR, a new machine config is not created, even if there are fewer than 10 <code class="literal">containerruntime</code> machine configs.
					</p></div></div><div class="formalpara"><p class="title"><strong>Example showing multiple <code class="literal">ContainerRuntimeConfig</code> CRs</strong></p><p>
						
<pre class="programlisting language-terminal">$ oc get ctrcfg</pre>

					</p></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">NAME         AGE
ctr-pid      24m
ctr-overlay  15m
ctr-level    5m45s</pre>

					</p></div><div class="formalpara"><p class="title"><strong>Example showing multiple <code class="literal">containerruntime</code> machine configs</strong></p><p>
						
<pre class="programlisting language-terminal">$ oc get mc | grep container</pre>

					</p></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">...
01-master-container-runtime                        b5c5119de007945b6fe6fb215db3b8e2ceb12511   3.2.0             57m
...
01-worker-container-runtime                        b5c5119de007945b6fe6fb215db3b8e2ceb12511   3.2.0             57m
...
99-worker-generated-containerruntime               b5c5119de007945b6fe6fb215db3b8e2ceb12511   3.2.0             26m
99-worker-generated-containerruntime-1             b5c5119de007945b6fe6fb215db3b8e2ceb12511   3.2.0             17m
99-worker-generated-containerruntime-2             b5c5119de007945b6fe6fb215db3b8e2ceb12511   3.2.0             7m26s
...</pre>

					</p></div><p>
					The following example raises the <code class="literal">pids_limit</code> to 2048, sets the <code class="literal">log_level</code> to <code class="literal">debug</code>, sets the overlay size to 8 GB, and sets the <code class="literal">log_size_max</code> to unlimited:
				</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">ContainerRuntimeConfig</code> CR</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: ContainerRuntimeConfig
metadata:
 name: overlay-size
spec:
 machineConfigPoolSelector:
   matchLabels:
     pools.operator.machineconfiguration.openshift.io/worker: '' <span id="CO26-1"><!--Empty--></span><span class="callout">1</span>
 containerRuntimeConfig:
   pidsLimit: 2048 <span id="CO26-2"><!--Empty--></span><span class="callout">2</span>
   logLevel: debug <span id="CO26-3"><!--Empty--></span><span class="callout">3</span>
   overlaySize: 8G <span id="CO26-4"><!--Empty--></span><span class="callout">4</span>
   logSizeMax: "-1" <span id="CO26-5"><!--Empty--></span><span class="callout">5</span>
   defaultRuntime: "crun" <span id="CO26-6"><!--Empty--></span><span class="callout">6</span></pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO26-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Specifies the machine config pool label.
						</div></dd><dt><a href="#CO26-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Optional: Specifies the maximum number of processes allowed in a container.
						</div></dd><dt><a href="#CO26-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Optional: Specifies the level of verbosity for log messages.
						</div></dd><dt><a href="#CO26-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Optional: Specifies the maximum size of a container image.
						</div></dd><dt><a href="#CO26-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Optional: Specifies the maximum size allowed for the container log file. If set to a positive number, it must be at least 8192.
						</div></dd><dt><a href="#CO26-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							Optional: Specifies the container runtime to deploy to new containers. The default is <code class="literal">runc</code>.
						</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To change CRI-O settings using the <code class="literal">ContainerRuntimeConfig</code> CR:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a YAML file for the <code class="literal">ContainerRuntimeConfig</code> CR:
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: ContainerRuntimeConfig
metadata:
 name: overlay-size
spec:
 machineConfigPoolSelector:
   matchLabels:
     pools.operator.machineconfiguration.openshift.io/worker: '' <span id="CO27-1"><!--Empty--></span><span class="callout">1</span>
 containerRuntimeConfig: <span id="CO27-2"><!--Empty--></span><span class="callout">2</span>
   pidsLimit: 2048
   logLevel: debug
   overlaySize: 8G
   logSizeMax: "-1"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO27-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify a label for the machine config pool that you want you want to modify.
								</div></dd><dt><a href="#CO27-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Set the parameters as needed.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">ContainerRuntimeConfig</code> CR:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li><li class="listitem"><p class="simpara">
							Verify that the CR is created:
						</p><pre class="programlisting language-terminal">$ oc get ContainerRuntimeConfig</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME           AGE
overlay-size   3m19s</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check that a new <code class="literal">containerruntime</code> machine config is created:
						</p><pre class="programlisting language-terminal">$ oc get machineconfigs | grep containerrun</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">99-worker-generated-containerruntime   2c9371fbb673b97a6fe8b1c52691999ed3a1bfc2  3.2.0  31s</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Monitor the machine config pool until all are shown as ready:
						</p><pre class="programlisting language-terminal">$ oc get mcp worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME    CONFIG               UPDATED  UPDATING  DEGRADED  MACHINECOUNT  READYMACHINECOUNT  UPDATEDMACHINECOUNT  DEGRADEDMACHINECOUNT  AGE
worker  rendered-worker-169  False    True      False     3             1                  1                    0                     9h</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify that the settings were applied in CRI-O:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Open an <code class="literal">oc debug</code> session to a node in the machine config pool and run <code class="literal">chroot /host</code>.
								</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt;</pre><pre class="programlisting language-terminal">sh-4.4# chroot /host</pre></li><li class="listitem"><p class="simpara">
									Verify the changes in the <code class="literal">crio.conf</code> file:
								</p><pre class="programlisting language-terminal">sh-4.4# crio config | egrep 'log_level|pids_limit|log_size_max'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">pids_limit = 2048
log_size_max = -1
log_level = "debug"</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Verify the changes in the `storage.conf`file:
								</p><pre class="programlisting language-terminal">sh-4.4# head -n 7 /etc/containers/storage.conf</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="screen">[storage]
  driver = "overlay"
  runroot = "/var/run/containers/storage"
  graphroot = "/var/lib/containers/storage"
  [storage.options]
    additionalimagestores = []
    size = "8G"</pre>

									</p></div></li></ol></div></li></ol></div></section><section class="section" id="set-the-default-max-container-root-partition-size-for-overlay-with-crio_post-install-machine-configuration-tasks"><div class="titlepage"><div><div><h3 class="title">6.3.3. Setting the default maximum container root partition size for Overlay with CRI-O</h3></div></div></div><p>
					The root partition of each container shows all of the available disk space of the underlying host. Follow this guidance to set a maximum partition size for the root disk of all containers.
				</p><p>
					To configure the maximum Overlay size, as well as other CRI-O options like the log level and PID limit, you can create the following <code class="literal">ContainerRuntimeConfig</code> custom resource definition (CRD):
				</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: ContainerRuntimeConfig
metadata:
 name: overlay-size
spec:
 machineConfigPoolSelector:
   matchLabels:
     custom-crio: overlay-size
 containerRuntimeConfig:
   pidsLimit: 2048
   logLevel: debug
   overlaySize: 8G</pre><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the configuration object:
						</p><pre class="programlisting language-terminal">$ oc apply -f overlaysize.yml</pre></li><li class="listitem"><p class="simpara">
							To apply the new CRI-O configuration to your worker nodes, edit the worker machine config pool:
						</p><pre class="programlisting language-terminal">$ oc edit machineconfigpool worker</pre></li><li class="listitem"><p class="simpara">
							Add the <code class="literal">custom-crio</code> label based on the <code class="literal">matchLabels</code> name you set in the <code class="literal">ContainerRuntimeConfig</code> CRD:
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: "2020-07-09T15:46:34Z"
  generation: 3
  labels:
    custom-crio: overlay-size
    machineconfiguration.openshift.io/mco-built-in: ""</pre></li><li class="listitem"><p class="simpara">
							Save the changes, then view the machine configs:
						</p><pre class="programlisting language-terminal">$ oc get machineconfigs</pre><p class="simpara">
							New <code class="literal">99-worker-generated-containerruntime</code> and <code class="literal">rendered-worker-xyz</code> objects are created:
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">99-worker-generated-containerruntime  4173030d89fbf4a7a0976d1665491a4d9a6e54f1   3.2.0             7m42s
rendered-worker-xyz                   4173030d89fbf4a7a0976d1665491a4d9a6e54f1   3.2.0             7m36s</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							After those objects are created, monitor the machine config pool for the changes to be applied:
						</p><pre class="programlisting language-terminal">$ oc get mcp worker</pre><p class="simpara">
							The worker nodes show <code class="literal">UPDATING</code> as <code class="literal">True</code>, as well as the number of machines, the number updated, and other details:
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME   CONFIG              UPDATED   UPDATING   DEGRADED  MACHINECOUNT  READYMACHINECOUNT  UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
worker rendered-worker-xyz False True False     3             2                   2                    0                      20h</pre>

							</p></div><p class="simpara">
							When complete, the worker nodes transition back to <code class="literal">UPDATING</code> as <code class="literal">False</code>, and the <code class="literal">UPDATEDMACHINECOUNT</code> number matches the <code class="literal">MACHINECOUNT</code>:
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME   CONFIG              UPDATED   UPDATING   DEGRADED  MACHINECOUNT  READYMACHINECOUNT  UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
worker   rendered-worker-xyz   True      False      False      3         3            3             0           20h</pre>

							</p></div><p class="simpara">
							Looking at a worker machine, you see that the new 8 GB max size configuration is applied to all of the workers:
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">head -n 7 /etc/containers/storage.conf
[storage]
  driver = "overlay"
  runroot = "/var/run/containers/storage"
  graphroot = "/var/lib/containers/storage"
  [storage.options]
    additionalimagestores = []
    size = "8G"</pre>

							</p></div><p class="simpara">
							Looking inside a container, you see that the root partition is now 8 GB:
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">~ $ df -h
Filesystem                Size      Used Available Use% Mounted on
overlay                   8.0G      8.0K      8.0G   0% /</pre>

							</p></div></li></ol></div></section></section></section><section class="chapter" id="post-install-cluster-tasks"><div class="titlepage"><div><div><h1 class="title">Chapter 7. Post-installation cluster tasks</h1></div></div></div><p>
			After installing OpenShift Container Platform, you can further expand and customize your cluster to your requirements.
		</p><section class="section" id="available_cluster_customizations"><div class="titlepage"><div><div><h2 class="title">7.1. Available cluster customizations</h2></div></div></div><p>
				You complete most of the cluster configuration and customization after you deploy your OpenShift Container Platform cluster. A number of <span class="emphasis"><em>configuration resources</em></span> are available.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					If you install your cluster on IBM Z, not all features and functions are available.
				</p></div></div><p>
				You modify the configuration resources to configure the major features of the cluster, such as the image registry, networking configuration, image build behavior, and the identity provider.
			</p><p>
				For current documentation of the settings that you control by using these resources, use the <code class="literal">oc explain</code> command, for example <code class="literal">oc explain builds --api-version=config.openshift.io/v1</code>
			</p><section class="section" id="configuration-resources_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.1.1. Cluster configuration resources</h3></div></div></div><p>
					All cluster configuration resources are globally scoped (not namespaced) and named <code class="literal">cluster</code>.
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031640002336" scope="col">Resource name</th><th align="left" valign="top" id="idm140031640001248" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031640002336"> <p>
									<code class="literal">apiserver.config.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031640001248"> <p>
									Provides API server configuration such as <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/security_and_compliance/#api-server-certificates">certificates and certificate authorities</a>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031640002336"> <p>
									<code class="literal">authentication.config.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031640001248"> <p>
									Controls the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#understanding-identity-provider">identity provider</a> and authentication configuration for the cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031640002336"> <p>
									<code class="literal">build.config.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031640001248"> <p>
									Controls default and enforced <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/cicd/#build-configuration">configuration</a> for all builds on the cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031640002336"> <p>
									<code class="literal">console.config.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031640001248"> <p>
									Configures the behavior of the web console interface, including the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/web_console/#configuring-web-console">logout behavior</a>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031640002336"> <p>
									<code class="literal">featuregate.config.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031640001248"> <p>
									Enables <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-cluster-enabling">FeatureGates</a> so that you can use Tech Preview features.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031640002336"> <p>
									<code class="literal">image.config.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031640001248"> <p>
									Configures how specific <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/images/#image-configuration">image registries</a> should be treated (allowed, disallowed, insecure, CA details).
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031640002336"> <p>
									<code class="literal">ingress.config.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031640001248"> <p>
									Configuration details related to <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-installation-ingress-config-asset_configuring-ingress">routing</a> such as the default domain for routes.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031640002336"> <p>
									<code class="literal">oauth.config.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031640001248"> <p>
									Configures identity providers and other behavior related to <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#configuring-internal-oauth">internal OAuth server</a> flows.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031640002336"> <p>
									<code class="literal">project.config.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031640001248"> <p>
									Configures <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/building_applications/#configuring-project-creation">how projects are created</a> including the project template.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031640002336"> <p>
									<code class="literal">proxy.config.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031640001248"> <p>
									Defines proxies to be used by components needing external network access. Note: not all components currently consume this value.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031640002336"> <p>
									<code class="literal">scheduler.config.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031640001248"> <p>
									Configures <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-profiles">scheduler</a> behavior such as profiles and default node selectors.
								</p>
								 </td></tr></tbody></table></div></section><section class="section" id="operator-configuration-resources_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.1.2. Operator configuration resources</h3></div></div></div><p>
					These configuration resources are cluster-scoped instances, named <code class="literal">cluster</code>, which control the behavior of a specific component as owned by a particular Operator.
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031656969904" scope="col">Resource name</th><th align="left" valign="top" id="idm140031656968816" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031656969904"> <p>
									<code class="literal">consoles.operator.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031656968816"> <p>
									Controls console appearance such as branding customizations
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031656969904"> <p>
									<code class="literal">config.imageregistry.operator.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031656968816"> <p>
									Configures <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/registry/#registry-operator-configuration-resource-overview_configuring-registry-operator">OpenShift image registry settings</a> such as public routing, log levels, proxy settings, resource constraints, replica counts, and storage type.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031656969904"> <p>
									<code class="literal">config.samples.operator.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031656968816"> <p>
									Configures the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/images/#configuring-samples-operator">Samples Operator</a> to control which example image streams and templates are installed on the cluster.
								</p>
								 </td></tr></tbody></table></div></section><section class="section" id="additional-configuration-resources_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.1.3. Additional configuration resources</h3></div></div></div><p>
					These configuration resources represent a single instance of a particular component. In some cases, you can request multiple instances by creating multiple instances of the resource. In other cases, the Operator can use only a specific resource instance name in a specific namespace. Reference the component-specific documentation for details on how and when you can create additional resource instances.
				</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 14%; " class="col_1"><!--Empty--></col><col style="width: 14%; " class="col_2"><!--Empty--></col><col style="width: 14%; " class="col_3"><!--Empty--></col><col style="width: 58%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031648580832" scope="col">Resource name</th><th align="left" valign="top" id="idm140031648579744" scope="col">Instance name</th><th align="left" valign="top" id="idm140031648578656" scope="col">Namespace</th><th align="left" valign="top" id="idm140031652020720" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031648580832"> <p>
									<code class="literal">alertmanager.monitoring.coreos.com</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031648579744"> <p>
									<code class="literal">main</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031648578656"> <p>
									<code class="literal">openshift-monitoring</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031652020720"> <p>
									Controls the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/monitoring/#managing-alerts">Alertmanager</a> deployment parameters.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031648580832"> <p>
									<code class="literal">ingresscontroller.operator.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031648579744"> <p>
									<code class="literal">default</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031648578656"> <p>
									<code class="literal">openshift-ingress-operator</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031652020720"> <p>
									Configures <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-ingress">Ingress Operator</a> behavior such as domain, number of replicas, certificates, and controller placement.
								</p>
								 </td></tr></tbody></table></div></section><section class="section" id="informational-resources_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.1.4. Informational Resources</h3></div></div></div><p>
					You use these resources to retrieve information about the cluster. Some configurations might require you to edit these resources directly.
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 17%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 66%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031642558224" scope="col">Resource name</th><th align="left" valign="top" id="idm140031642557136" scope="col">Instance name</th><th align="left" valign="top" id="idm140031642556048" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031642558224"> <p>
									<code class="literal">clusterversion.config.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031642557136"> <p>
									<code class="literal">version</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031642556048"> <p>
									In OpenShift Container Platform 4.13, you must not customize the <code class="literal">ClusterVersion</code> resource for production clusters. Instead, follow the process to <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#updating-cluster-within-minor">update a cluster</a>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031642558224"> <p>
									<code class="literal">dns.config.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031642557136"> <p>
									<code class="literal">cluster</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031642556048"> <p>
									You cannot modify the DNS settings for your cluster. You can <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#dns-operator">view the DNS Operator status</a>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031642558224"> <p>
									<code class="literal">infrastructure.config.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031642557136"> <p>
									<code class="literal">cluster</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031642556048"> <p>
									Configuration details allowing the cluster to interact with its cloud provider.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031642558224"> <p>
									<code class="literal">network.config.openshift.io</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031642557136"> <p>
									<code class="literal">cluster</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031642556048"> <p>
									You cannot modify your cluster networking after installation. To customize your network, follow the process to <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-aws-network-customizations">customize networking during installation</a>.
								</p>
								 </td></tr></tbody></table></div></section></section><section class="section" id="images-update-global-pull-secret_post-install-cluster-tasks"><div class="titlepage"><div><div><h2 class="title">7.2. Updating the global cluster pull secret</h2></div></div></div><p>
				You can update the global pull secret for your cluster by either replacing the current pull secret or appending a new pull secret.
			</p><p>
				The procedure is required when users use a separate registry to store images than the registry used during installation.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Optional: To append a new pull secret to the existing pull secret, complete the following steps:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Enter the following command to download the pull secret:
							</p><pre class="programlisting language-terminal">$ oc get secret/pull-secret -n openshift-config --template='{{index .data ".dockerconfigjson" | base64decode}}' &gt;&lt;pull_secret_location&gt; <span id="CO28-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO28-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Provide the path to the pull secret file.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Enter the following command to add the new pull secret:
							</p><pre class="programlisting language-terminal">$ oc registry login --registry="&lt;registry&gt;" \ <span id="CO29-1"><!--Empty--></span><span class="callout">1</span>
--auth-basic="&lt;username&gt;:&lt;password&gt;" \ <span id="CO29-2"><!--Empty--></span><span class="callout">2</span>
--to=&lt;pull_secret_location&gt; <span id="CO29-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO29-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Provide the new registry. You can include multiple repositories within the same registry, for example: <code class="literal">--registry="&lt;registry/my-namespace/my-repository&gt;"</code>.
									</div></dd><dt><a href="#CO29-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Provide the credentials of the new registry.
									</div></dd><dt><a href="#CO29-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Provide the path to the pull secret file.
									</div></dd></dl></div><p class="simpara">
								Alternatively, you can perform a manual update to the pull secret file.
							</p></li></ol></div></li><li class="listitem"><p class="simpara">
						Enter the following command to update the global pull secret for your cluster:
					</p><pre class="programlisting language-terminal">$ oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=&lt;pull_secret_location&gt; <span id="CO30-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO30-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Provide the path to the new pull secret file.
							</div></dd></dl></div><p class="simpara">
						This update is rolled out to all nodes, which can take some time depending on the size of your cluster.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							As of OpenShift Container Platform 4.7.4, changes to the global pull secret no longer trigger a node drain or reboot.
						</p></div></div></li></ol></div></section><section class="section" id="adding-worker-nodes_post-install-cluster-tasks"><div class="titlepage"><div><div><h2 class="title">7.3. Adding worker nodes</h2></div></div></div><p>
				After you deploy your OpenShift Container Platform cluster, you can add worker nodes to scale cluster resources. There are different ways you can add worker nodes depending on the installation method and the environment of your cluster.
			</p><section class="section" id="adding-worker-nodes-to-installer-provisioned-infrastructure-clusters"><div class="titlepage"><div><div><h3 class="title">7.3.1. Adding worker nodes to installer-provisioned infrastructure clusters</h3></div></div></div><p>
					For installer-provisioned infrastructure clusters, you can manually or automatically scale the <code class="literal">MachineSet</code> object to match the number of available bare-metal hosts.
				</p><p>
					To add a bare-metal host, you must configure all network prerequisites, configure an associated <code class="literal">baremetalhost</code> object, then provision the worker node to the cluster. You can add a bare-metal host manually or by using the web console.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#adding-bare-metal-host-to-cluster-using-web-console_managing-bare-metal-hosts">Adding worker nodes using the web console</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#adding-bare-metal-host-to-cluster-using-yaml_managing-bare-metal-hosts">Adding worker nodes using YAML in the web console</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#preparing-the-bare-metal-node_ipi-install-expanding">Manually adding a worker node to an installer-provisioned infrastructure cluster</a>
						</li></ul></div></section><section class="section" id="adding-worker-nodes-to-user-provisioned-infrastructure-clusters"><div class="titlepage"><div><div><h3 class="title">7.3.2. Adding worker nodes to user-provisioned infrastructure clusters</h3></div></div></div><p>
					For user-provisioned infrastructure clusters, you can add worker nodes by using a RHEL or RHCOS ISO image and connecting it to your cluster using cluster Ignition config files. For RHEL worker nodes, the following example uses Ansible playbooks to add worker nodes to the cluster. For RHCOS worker nodes, the following example uses an ISO image and network booting to add worker nodes to the cluster.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#post-install-config-adding-fcos-compute">Adding RHCOS worker nodes to a user-provisioned infrastructure cluster</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#post-install-config-adding-rhel-compute">Adding RHEL worker nodes to a user-provisioned infrastructure cluster</a>
						</li></ul></div></section><section class="section" id="adding-worker-nodes-to-clusters-managed-by-the-assisted-installer"><div class="titlepage"><div><div><h3 class="title">7.3.3. Adding worker nodes to clusters managed by the Assisted Installer</h3></div></div></div><p>
					For clusters managed by the Assisted Installer, you can add worker nodes by using the Red Hat OpenShift Cluster Manager console, the Assisted Installer REST API or you can manually add worker nodes using an ISO image and cluster Ignition config files.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#sno-adding-worker-nodes-to-sno-clusters_add-workers">Adding worker nodes using the OpenShift Cluster Manager</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#adding-worker-nodes-using-the-assisted-installer-api">Adding worker nodes using the Assisted Installer REST API</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#sno-adding-worker-nodes-to-single-node-clusters-manually_add-workers">Manually adding worker nodes to a SNO cluster</a>
						</li></ul></div></section><section class="section" id="adding-worker-nodes-to-clusters-managed-by-the-multicluster-engine-for-kubernetes"><div class="titlepage"><div><div><h3 class="title">7.3.4. Adding worker nodes to clusters managed by the multicluster engine for Kubernetes</h3></div></div></div><p>
					For clusters managed by the multicluster engine for Kubernetes, you can add worker nodes by using the dedicated multicluster engine console.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.8/html/clusters/cluster_mce_overview#scale-hosts-infrastructure-env">Scaling hosts to an infrastructure environment</a>
						</li></ul></div></section></section><section class="section" id="post-install-adjust-worker-nodes"><div class="titlepage"><div><div><h2 class="title">7.4. Adjust worker nodes</h2></div></div></div><p>
				If you incorrectly sized the worker nodes during deployment, adjust them by creating one or more new compute machine sets, scale them up, then scale the original compute machine set down before removing them.
			</p><section class="section" id="differences-between-machinesets-and-machineconfigpool_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.4.1. Understanding the difference between compute machine sets and the machine config pool</h3></div></div></div><p>
					<code class="literal">MachineSet</code> objects describe OpenShift Container Platform nodes with respect to the cloud or machine provider.
				</p><p>
					The <code class="literal">MachineConfigPool</code> object allows <code class="literal">MachineConfigController</code> components to define and provide the status of machines in the context of upgrades.
				</p><p>
					The <code class="literal">MachineConfigPool</code> object allows users to configure how upgrades are rolled out to the OpenShift Container Platform nodes in the machine config pool.
				</p><p>
					The <code class="literal">NodeSelector</code> object can be replaced with a reference to the <code class="literal">MachineSet</code> object.
				</p></section><section class="section" id="machineset-manually-scaling_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.4.2. Scaling a compute machine set manually</h3></div></div></div><p>
					To add or remove an instance of a machine in a compute machine set, you can manually scale the compute machine set.
				</p><p>
					This guidance is relevant to fully automated, installer-provisioned infrastructure installations. Customized, user-provisioned infrastructure installations do not have compute machine sets.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install an OpenShift Container Platform cluster and the <code class="literal">oc</code> command line.
						</li><li class="listitem">
							Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the compute machine sets that are in the cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><p class="simpara">
							The compute machine sets are listed in the form of <code class="literal">&lt;clusterid&gt;-worker-&lt;aws-region-az&gt;</code>.
						</p></li><li class="listitem"><p class="simpara">
							View the compute machines that are in the cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machine -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
							Set the annotation on the compute machine that you want to delete by running the following command:
						</p><pre class="programlisting language-terminal">$ oc annotate machine/&lt;machine_name&gt; -n openshift-machine-api machine.openshift.io/delete-machine="true"</pre></li><li class="listitem"><p class="simpara">
							Scale the compute machine set by running one of the following commands:
						</p><pre class="programlisting language-terminal">$ oc scale --replicas=2 machineset &lt;machineset&gt; -n openshift-machine-api</pre><p class="simpara">
							Or:
						</p><pre class="programlisting language-terminal">$ oc edit machineset &lt;machineset&gt; -n openshift-machine-api</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can alternatively apply the following YAML to scale the compute machine set:
						</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: &lt;machineset&gt;
  namespace: openshift-machine-api
spec:
  replicas: 2</pre></div></div><p class="simpara">
							You can scale the compute machine set up or down. It takes several minutes for the new machines to be available.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								By default, the machine controller tries to drain the node that is backed by the machine until it succeeds. In some situations, such as with a misconfigured pod disruption budget, the drain operation might not be able to succeed. If the drain operation fails, the machine controller cannot proceed removing the machine.
							</p><p>
								You can skip draining the node by annotating <code class="literal">machine.openshift.io/exclude-node-draining</code> in a specific machine.
							</p></div></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Verify the deletion of the intended machine by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machines</pre></li></ul></div></section><section class="section" id="machineset-delete-policy_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.4.3. The compute machine set deletion policy</h3></div></div></div><p>
					<code class="literal">Random</code>, <code class="literal">Newest</code>, and <code class="literal">Oldest</code> are the three supported deletion options. The default is <code class="literal">Random</code>, meaning that random machines are chosen and deleted when scaling compute machine sets down. The deletion policy can be set according to the use case by modifying the particular compute machine set:
				</p><pre class="programlisting language-yaml">spec:
  deletePolicy: &lt;delete_policy&gt;
  replicas: &lt;desired_replica_count&gt;</pre><p>
					Specific machines can also be prioritized for deletion by adding the annotation <code class="literal">machine.openshift.io/delete-machine=true</code> to the machine of interest, regardless of the deletion policy.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						By default, the OpenShift Container Platform router pods are deployed on workers. Because the router is required to access some cluster resources, including the web console, do not scale the worker compute machine set to <code class="literal">0</code> unless you first relocate the router pods.
					</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Custom compute machine sets can be used for use cases requiring that services run on specific nodes and that those services are ignored by the controller when the worker compute machine sets are scaling down. This prevents service disruption.
					</p></div></div></section><section class="section" id="nodes-scheduler-node-selectors-cluster_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.4.4. Creating default cluster-wide node selectors</h3></div></div></div><p>
					You can use default cluster-wide node selectors on pods together with labels on nodes to constrain all pods created in a cluster to specific nodes.
				</p><p>
					With cluster-wide node selectors, when you create a pod in that cluster, OpenShift Container Platform adds the default node selectors to the pod and schedules the pod on nodes with matching labels.
				</p><p>
					You configure cluster-wide node selectors by editing the Scheduler Operator custom resource (CR). You add labels to a node, a compute machine set, or a machine config. Adding the label to the compute machine set ensures that if the node or machine goes down, new nodes have the label. Labels added to a node or machine config do not persist if the node or machine goes down.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can add additional key/value pairs to a pod. But you cannot add a different value for a default key.
					</p></div></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To add a default cluster-wide node selector:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the Scheduler Operator CR to add the default cluster-wide node selectors:
						</p><pre class="programlisting language-terminal">$ oc edit scheduler cluster</pre><div class="formalpara"><p class="title"><strong>Example Scheduler Operator CR with a node selector</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  name: cluster
...
spec:
  defaultNodeSelector: type=user-node,region=east <span id="CO31-1"><!--Empty--></span><span class="callout">1</span>
  mastersSchedulable: false</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO31-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add a node selector with the appropriate <code class="literal">&lt;key&gt;:&lt;value&gt;</code> pairs.
								</div></dd></dl></div><p class="simpara">
							After making this change, wait for the pods in the <code class="literal">openshift-kube-apiserver</code> project to redeploy. This can take several minutes. The default cluster-wide node selector does not take effect until the pods redeploy.
						</p></li><li class="listitem"><p class="simpara">
							Add labels to a node by using a compute machine set or editing the node directly:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Use a compute machine set to add labels to nodes managed by the compute machine set when a node is created:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
											Run the following command to add labels to a <code class="literal">MachineSet</code> object:
										</p><pre class="programlisting language-terminal">$ oc patch MachineSet &lt;name&gt; --type='json' -p='[{"op":"add","path":"/spec/template/spec/metadata/labels", "value":{"&lt;key&gt;"="&lt;value&gt;","&lt;key&gt;"="&lt;value&gt;"}}]'  -n openshift-machine-api <span id="CO32-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO32-1"><span class="callout">1</span></a> </dt><dd><div class="para">
													Add a <code class="literal">&lt;key&gt;/&lt;value&gt;</code> pair for each label.
												</div></dd></dl></div><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc patch MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c --type='json' -p='[{"op":"add","path":"/spec/template/spec/metadata/labels", "value":{"type":"user-node","region":"east"}}]'  -n openshift-machine-api</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
											You can alternatively apply the following YAML to add labels to a compute machine set:
										</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: &lt;machineset&gt;
  namespace: openshift-machine-api
spec:
  template:
    spec:
      metadata:
        labels:
          region: "east"
          type: "user-node"</pre></div></div></li><li class="listitem"><p class="simpara">
											Verify that the labels are added to the <code class="literal">MachineSet</code> object by using the <code class="literal">oc edit</code> command:
										</p><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc edit MachineSet abc612-msrtw-worker-us-east-1c -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example <code class="literal">MachineSet</code> object</strong></p><p>
												
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
  ...
spec:
  ...
  template:
    metadata:
  ...
    spec:
      metadata:
        labels:
          region: east
          type: user-node
  ...</pre>

											</p></div></li><li class="listitem"><p class="simpara">
											Redeploy the nodes associated with that compute machine set by scaling down to <code class="literal">0</code> and scaling up the nodes:
										</p><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc scale --replicas=0 MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c -n openshift-machine-api</pre><pre class="programlisting language-terminal">$ oc scale --replicas=1 MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
											When the nodes are ready and available, verify that the label is added to the nodes by using the <code class="literal">oc get</code> command:
										</p><pre class="programlisting language-terminal">$ oc get nodes -l &lt;key&gt;=&lt;value&gt;</pre><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc get nodes -l type=user-node</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
												
<pre class="programlisting language-terminal">NAME                                       STATUS   ROLES    AGE   VERSION
ci-ln-l8nry52-f76d1-hl7m7-worker-c-vmqzp   Ready    worker   61s   v1.26.0</pre>

											</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
									Add labels directly to a node:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
											Edit the <code class="literal">Node</code> object for the node:
										</p><pre class="programlisting language-terminal">$ oc label nodes &lt;name&gt; &lt;key&gt;=&lt;value&gt;</pre><p class="simpara">
											For example, to label a node:
										</p><pre class="programlisting language-terminal">$ oc label nodes ci-ln-l8nry52-f76d1-hl7m7-worker-b-tgq49 type=user-node region=east</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
											You can alternatively apply the following YAML to add labels to a node:
										</p><pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  name: &lt;node_name&gt;
  labels:
    type: "user-node"
    region: "east"</pre></div></div></li><li class="listitem"><p class="simpara">
											Verify that the labels are added to the node using the <code class="literal">oc get</code> command:
										</p><pre class="programlisting language-terminal">$ oc get nodes -l &lt;key&gt;=&lt;value&gt;,&lt;key&gt;=&lt;value&gt;</pre><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc get nodes -l type=user-node,region=east</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
												
<pre class="programlisting language-terminal">NAME                                       STATUS   ROLES    AGE   VERSION
ci-ln-l8nry52-f76d1-hl7m7-worker-b-tgq49   Ready    worker   17m   v1.26.0</pre>

											</p></div></li></ol></div></li></ul></div></li></ol></div></section><section class="section" id="installation-extend-edge-nodes-aws-local-zones_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.4.5. Creating user workloads in AWS Local Zones</h3></div></div></div><p>
					After you create an Amazon Web Service (AWS) Local Zone environment, and you deploy your cluster, you can use edge worker nodes to create user workloads in Local Zone subnets.
				</p><p>
					After the <code class="literal">openshift-installer</code> creates the cluster, the installation program automatically specifies a taint effect of <code class="literal">NoSchedule</code> to each edge worker node. This means that a scheduler does not add a new pod, or deployment, to a node if the pod does not match the specified tolerations for a taint. You can modify the taint for better control over how each node creates a workload in each Local Zone subnet.
				</p><p>
					The <code class="literal">openshift-installer</code> creates the compute machine set manifests file with <code class="literal">node-role.kubernetes.io/edge</code> and <code class="literal">node-role.kubernetes.io/worker</code> labels applied to each edge worker node that is located in a Local Zone subnet.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You deployed your cluster in a Virtual Private Cloud (VPC) with defined Local Zone subnets.
						</li><li class="listitem">
							You ensured that the compute machine set for the edge workers on Local Zone subnets specifies the taints for <code class="literal">node-role.kubernetes.io/edge</code>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">deployment</code> resource YAML file for an example application to be deployed in the edge worker node that operates in a Local Zone subnet. Ensure that you specify the correct tolerations that match the taints for the edge worker node.
						</p><div class="formalpara"><p class="title"><strong>Example of a configured <code class="literal">deployment</code> resource for an edge worker node that operates in a Local Zone subnet</strong></p><p>
								
<pre class="programlisting language-yaml">kind: Namespace
apiVersion: v1
metadata:
  name: &lt;local_zone_application_namespace&gt;
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: &lt;pvc_name&gt;
  namespace: &lt;local_zone_application_namespace&gt;
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: gp2-csi <span id="CO33-1"><!--Empty--></span><span class="callout">1</span>
  volumeMode: Filesystem
---
apiVersion: apps/v1
kind: Deployment <span id="CO33-2"><!--Empty--></span><span class="callout">2</span>
metadata:
  name: &lt;local_zone_application&gt; <span id="CO33-3"><!--Empty--></span><span class="callout">3</span>
  namespace: &lt;local_zone_application_namespace&gt; <span id="CO33-4"><!--Empty--></span><span class="callout">4</span>
spec:
  selector:
    matchLabels:
      app: &lt;local_zone_application&gt;
  replicas: 1
  template:
    metadata:
      labels:
        app: &lt;local_zone_application&gt;
        zone-group: ${ZONE_GROUP_NAME} <span id="CO33-5"><!--Empty--></span><span class="callout">5</span>
    spec:
      securityContext:
        seccompProfile:
          type: RuntimeDefault
      nodeSelector: <span id="CO33-6"><!--Empty--></span><span class="callout">6</span>
        machine.openshift.io/zone-group: ${ZONE_GROUP_NAME}
      tolerations: <span id="CO33-7"><!--Empty--></span><span class="callout">7</span>
      - key: "node-role.kubernetes.io/edge"
        operator: "Equal"
        value: ""
        effect: "NoSchedule"
      containers:
        - image: openshift/origin-node
          command:
           - "/bin/socat"
          args:
            - TCP4-LISTEN:8080,reuseaddr,fork
            - EXEC:'/bin/bash -c \"printf \\\"HTTP/1.0 200 OK\r\n\r\n\\\"; sed -e \\\"/^\r/q\\\"\"'
          imagePullPolicy: Always
          name: echoserver
          ports:
            - containerPort: 8080
          volumeMounts:
            - mountPath: "/mnt/storage"
              name: data
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: &lt;pvc_name&gt;</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO33-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal">storageClassName</code>: For the Local Zone configuration, you must specify <code class="literal">gp2-csi</code>.
								</div></dd><dt><a href="#CO33-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									<code class="literal">kind</code>: Defines the <code class="literal">deployment</code> resource.
								</div></dd><dt><a href="#CO33-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									<code class="literal">name</code>: Specifies the name of your Local Zone application. For example, <code class="literal">local-zone-demo-app-nyc-1</code>.
								</div></dd><dt><a href="#CO33-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									<code class="literal">namespace:</code> Defines the namespace for the AWS Local Zone where you want to run the user workload. For example: <code class="literal">local-zone-app-nyc-1a</code>.
								</div></dd><dt><a href="#CO33-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									<code class="literal">zone-group</code>: Defines the group to where a zone belongs. For example, <code class="literal">us-east-1-iah-1</code>.
								</div></dd><dt><a href="#CO33-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									<code class="literal">nodeSelector</code>: Targets edge worker nodes that match the specified labels.
								</div></dd><dt><a href="#CO33-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									<code class="literal">tolerations</code>: Sets the values that match with the <code class="literal">taints</code> defined on the <code class="literal">MachineSet</code> manifest for the Local Zone node.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">service</code> resource YAML file for the node. This resource exposes a pod from a targeted edge worker node to services that run inside your Local Zone network.
						</p><div class="formalpara"><p class="title"><strong>Example of a configured <code class="literal">service</code> resource for an edge worker node that operates in a Local Zone subnet</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Service <span id="CO34-1"><!--Empty--></span><span class="callout">1</span>
metadata:
  name:  &lt;local_zone_application&gt;
  namespace: &lt;local_zone_application_namespace&gt;
spec:
  ports:
    - port: 80
      targetPort: 8080
      protocol: TCP
  type: NodePort
  selector: <span id="CO34-2"><!--Empty--></span><span class="callout">2</span>
    app: &lt;local_zone_application&gt;</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO34-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal">kind</code>: Defines the <code class="literal">service</code> resource.
								</div></dd><dt><a href="#CO34-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									<code class="literal">selector:</code> Specifies the label type applied to managed pods.
								</div></dd></dl></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Optional: Use the AWS Load Balancer (ALB) Operator to expose a pod from a targeted edge worker node to services that run inside a Local Zone subnet from a public network. See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-installing-aws-load-balancer-operator_aws-load-balancer-operator">Installing the AWS Load Balancer Operator</a>.
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-a-cluster-using-aws-local-zones">Installing a cluster using AWS Local Zones</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#controlling-pod-placement-using-node-taints">Understanding taints and tolerations</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/logging/#using-tolerations-to-control-logging-pod-placement">Using tolerations to control OpenShift Logging pod placement</a>
						</li></ul></div></section></section><section class="section" id="post-worker-latency-profiles"><div class="titlepage"><div><div><h2 class="title">7.5. Improving cluster stability in high latency environments using worker latency profiles</h2></div></div></div><p>
				All nodes send heartbeats to the Kubernetes Controller Manager Operator (kube controller) in the OpenShift Container Platform cluster every 10 seconds, by default. If the cluster does not receive heartbeats from a node, OpenShift Container Platform responds using several default mechanisms.
			</p><p>
				For example, if the Kubernetes Controller Manager Operator loses contact with a node after a configured period:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						The node controller on the control plane updates the node health to <code class="literal">Unhealthy</code> and marks the node <code class="literal">Ready</code> condition as <code class="literal">Unknown</code>.
					</li><li class="listitem">
						In response, the scheduler stops scheduling pods to that node.
					</li><li class="listitem">
						The on-premise node controller adds a <code class="literal">node.kubernetes.io/unreachable</code> taint with a <code class="literal">NoExecute</code> effect to the node and schedules any pods on the node for eviction after five minutes, by default.
					</li></ol></div><p>
				This behavior can cause problems if your network is prone to latency issues, especially if you have nodes at the network edge. In some cases, the Kubernetes Controller Manager Operator might not receive an update from a healthy node due to network latency. The Kubernetes Controller Manager Operator would then evict pods from the node even though the node is healthy. To avoid this problem, you can use <span class="emphasis"><em>worker latency profiles</em></span> to adjust the frequency that the kubelet and the Kubernetes Controller Manager Operator wait for status updates before taking action. These adjustments help to ensure that your cluster runs properly in the event that network latency between the control plane and the worker nodes is not optimal.
			</p><p>
				These worker latency profiles are three sets of parameters that are pre-defined with carefully tuned values that let you control the reaction of the cluster to latency issues without needing to determine the best values manually.
			</p><section class="section" id="nodes-cluster-worker-latency-profiles-about_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.5.1. Understanding worker latency profiles</h3></div></div></div><p>
					Worker latency profiles are multiple sets of carefully-tuned values for the <code class="literal">node-status-update-frequency</code>, <code class="literal">node-monitor-grace-period</code>, <code class="literal">default-not-ready-toleration-seconds</code> and <code class="literal">default-unreachable-toleration-seconds</code> parameters. These parameters let you control the reaction of the cluster to latency issues without needing to determine the best values manually.
				</p><p>
					All worker latency profiles configure the following parameters:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">node-status-update-frequency</code>. Specifies the amount of time in seconds that a kubelet updates its status to the Kubernetes Controller Manager Operator.
						</li><li class="listitem">
							<code class="literal">node-monitor-grace-period</code>. Specifies the amount of time in seconds that the Kubernetes Controller Manager Operator waits for an update from a kubelet before marking the node unhealthy and adding the <code class="literal">node.kubernetes.io/not-ready</code> or <code class="literal">node.kubernetes.io/unreachable</code> taint to the node.
						</li><li class="listitem">
							<code class="literal">default-not-ready-toleration-seconds</code>. Specifies the amount of time in seconds after marking a node unhealthy that the Kubernetes Controller Manager Operator waits before evicting pods from that node.
						</li><li class="listitem">
							<code class="literal">default-unreachable-toleration-seconds</code>. Specifies the amount of time in seconds after marking a node unreachable that the Kubernetes Controller Manager Operator waits before evicting pods from that node.
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Manually modifying the <code class="literal">node-monitor-grace-period</code> parameter is not supported.
					</p></div></div><p>
					The following Operators monitor the changes to the worker latency profiles and respond accordingly:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The Machine Config Operator (MCO) updates the <code class="literal">node-status-update-frequency</code> parameter on the worker nodes.
						</li><li class="listitem">
							The Kubernetes Controller Manager Operator updates the <code class="literal">node-monitor-grace-period</code> parameter on the control plane nodes.
						</li><li class="listitem">
							The Kubernetes API Server Operator updates the <code class="literal">default-not-ready-toleration-seconds</code> and <code class="literal">default-unreachable-toleration-seconds</code> parameters on the control plance nodes.
						</li></ul></div><p>
					While the default configuration works in most cases, OpenShift Container Platform offers two other worker latency profiles for situations where the network is experiencing higher latency than usual. The three worker latency profiles are described in the following sections:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Default worker latency profile</span></dt><dd><p class="simpara">
								With the <code class="literal">Default</code> profile, each kubelet reports its node status to the Kubelet Controller Manager Operator (kube controller) every 10 seconds. The Kubelet Controller Manager Operator checks the kubelet for a status every 5 seconds.
							</p><p class="simpara">
								The Kubernetes Controller Manager Operator waits 40 seconds for a status update before considering that node unhealthy. It marks the node with the <code class="literal">node.kubernetes.io/not-ready</code> or <code class="literal">node.kubernetes.io/unreachable</code> taint and evicts the pods on that node. If a pod on that node has the <code class="literal">NoExecute</code> toleration, the pod gets evicted in 300 seconds. If the pod has the <code class="literal">tolerationSeconds</code> parameter, the eviction waits for the period specified by that parameter.
							</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col><col style="width: 17%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th style="text-align: left; vertical-align: top; " id="idm140031649034064" scope="col">Profile</th><th style="text-align: left; vertical-align: top; " id="idm140031661356976" scope="col">Component</th><th style="text-align: left; vertical-align: top; " id="idm140031661355888" scope="col">Parameter</th><th style="text-align: left; vertical-align: top; " id="idm140031661354800" scope="col">Value</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; " rowspan="4" headers="idm140031649034064"> <p>
												Default
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031661356976"> <p>
												kubelet
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031661355888"> <p>
												<code class="literal">node-status-update-frequency</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031661354800"> <p>
												10s
											</p>
											 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm140031661356976"> <p>
												Kubelet Controller Manager
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031661355888"> <p>
												<code class="literal">node-monitor-grace-period</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031661354800"> <p>
												40s
											</p>
											 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm140031661356976"> <p>
												Kubernetes API Server
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031661355888"> <p>
												<code class="literal">default-not-ready-toleration-seconds</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031661354800"> <p>
												300s
											</p>
											 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm140031661356976"> <p>
												Kubernetes API Server
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031661355888"> <p>
												<code class="literal">default-unreachable-toleration-seconds</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031661354800"> <p>
												300s
											</p>
											 </td></tr></tbody></table></div></dd><dt><span class="term">Medium worker latency profile</span></dt><dd><p class="simpara">
								Use the <code class="literal">MediumUpdateAverageReaction</code> profile if the network latency is slightly higher than usual.
							</p><p class="simpara">
								The <code class="literal">MediumUpdateAverageReaction</code> profile reduces the frequency of kubelet updates to 20 seconds and changes the period that the Kubernetes Controller Manager Operator waits for those updates to 2 minutes. The pod eviction period for a pod on that node is reduced to 60 seconds. If the pod has the <code class="literal">tolerationSeconds</code> parameter, the eviction waits for the period specified by that parameter.
							</p><p class="simpara">
								The Kubernetes Controller Manager Operator waits for 2 minutes to consider a node unhealthy. In another minute, the eviction process starts.
							</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col><col style="width: 17%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th style="text-align: left; vertical-align: top; " id="idm140031649451344" scope="col">Profile</th><th style="text-align: left; vertical-align: top; " id="idm140031649450256" scope="col">Component</th><th style="text-align: left; vertical-align: top; " id="idm140031646027648" scope="col">Parameter</th><th style="text-align: left; vertical-align: top; " id="idm140031646026560" scope="col">Value</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; " rowspan="4" headers="idm140031649451344"> <p>
												MediumUpdateAverageReaction
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031649450256"> <p>
												kubelet
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031646027648"> <p>
												<code class="literal">node-status-update-frequency</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031646026560"> <p>
												20s
											</p>
											 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm140031649450256"> <p>
												Kubelet Controller Manager
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031646027648"> <p>
												<code class="literal">node-monitor-grace-period</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031646026560"> <p>
												2m
											</p>
											 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm140031649450256"> <p>
												Kubernetes API Server
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031646027648"> <p>
												<code class="literal">default-not-ready-toleration-seconds</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031646026560"> <p>
												60s
											</p>
											 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm140031649450256"> <p>
												Kubernetes API Server
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031646027648"> <p>
												<code class="literal">default-unreachable-toleration-seconds</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031646026560"> <p>
												60s
											</p>
											 </td></tr></tbody></table></div></dd><dt><span class="term">Low worker latency profile</span></dt><dd><p class="simpara">
								Use the <code class="literal">LowUpdateSlowReaction</code> profile if the network latency is extremely high.
							</p><p class="simpara">
								The <code class="literal">LowUpdateSlowReaction</code> profile reduces the frequency of kubelet updates to 1 minute and changes the period that the Kubernetes Controller Manager Operator waits for those updates to 5 minutes. The pod eviction period for a pod on that node is reduced to 60 seconds. If the pod has the <code class="literal">tolerationSeconds</code> parameter, the eviction waits for the period specified by that parameter.
							</p><p class="simpara">
								The Kubernetes Controller Manager Operator waits for 5 minutes to consider a node unhealthy. In another minute, the eviction process starts.
							</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col><col style="width: 17%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th style="text-align: left; vertical-align: top; " id="idm140031660782960" scope="col">Profile</th><th style="text-align: left; vertical-align: top; " id="idm140031660781872" scope="col">Component</th><th style="text-align: left; vertical-align: top; " id="idm140031660780784" scope="col">Parameter</th><th style="text-align: left; vertical-align: top; " id="idm140031651159936" scope="col">Value</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; " rowspan="4" headers="idm140031660782960"> <p>
												LowUpdateSlowReaction
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031660781872"> <p>
												kubelet
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031660780784"> <p>
												<code class="literal">node-status-update-frequency</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031651159936"> <p>
												1m
											</p>
											 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm140031660781872"> <p>
												Kubelet Controller Manager
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031660780784"> <p>
												<code class="literal">node-monitor-grace-period</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031651159936"> <p>
												5m
											</p>
											 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm140031660781872"> <p>
												Kubernetes API Server
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031660780784"> <p>
												<code class="literal">default-not-ready-toleration-seconds</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031651159936"> <p>
												60s
											</p>
											 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm140031660781872"> <p>
												Kubernetes API Server
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031660780784"> <p>
												<code class="literal">default-unreachable-toleration-seconds</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140031651159936"> <p>
												60s
											</p>
											 </td></tr></tbody></table></div></dd></dl></div></section><section class="section" id="nodes-cluster-worker-latency-profiles-using_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.5.2. Using worker latency profiles</h3></div></div></div><p>
					To implement a worker latency profile to deal with network latency, edit the <code class="literal">node.config</code> object to add the name of the profile. You can change the profile at any time as latency increases or decreases.
				</p><p>
					You must move one worker latency profile at a time. For example, you cannot move directly from the <code class="literal">Default</code> profile to the <code class="literal">LowUpdateSlowReaction</code> worker latency profile. You must move from the <code class="literal">default</code> worker latency profile to the <code class="literal">MediumUpdateAverageReaction</code> profile first, then to <code class="literal">LowUpdateSlowReaction</code>. Similarly, when returning to the default profile, you must move from the low profile to the medium profile first, then to the default.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can also configure worker latency profiles upon installing an OpenShift Container Platform cluster.
					</p></div></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To move from the default worker latency profile:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Move to the medium worker latency profile:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Edit the <code class="literal">node.config</code> object:
								</p><pre class="programlisting language-terminal">$ oc edit nodes.config/cluster</pre></li><li class="listitem"><p class="simpara">
									Add <code class="literal">spec.workerLatencyProfile: MediumUpdateAverageReaction</code>:
								</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">node.config</code> object</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Node
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2022-07-08T16:02:51Z"
  generation: 1
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    kind: ClusterVersion
    name: version
    uid: 36282574-bf9f-409e-a6cd-3032939293eb
  resourceVersion: "1865"
  uid: 0c0f7a4c-4307-4187-b591-6155695ac85b
spec:
  workerLatencyProfile: MediumUpdateAverageReaction <span id="CO35-1"><!--Empty--></span><span class="callout">1</span>

# ...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO35-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specifies the medium worker latency policy.
										</div></dd></dl></div><p class="simpara">
									Scheduling on each worker node is disabled as the change is being applied.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Optional: Move to the low worker latency profile:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Edit the <code class="literal">node.config</code> object:
								</p><pre class="programlisting language-terminal">$ oc edit nodes.config/cluster</pre></li><li class="listitem"><p class="simpara">
									Change the <code class="literal">spec.workerLatencyProfile</code> value to <code class="literal">LowUpdateSlowReaction</code>:
								</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">node.config</code> object</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Node
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2022-07-08T16:02:51Z"
  generation: 1
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    kind: ClusterVersion
    name: version
    uid: 36282574-bf9f-409e-a6cd-3032939293eb
  resourceVersion: "1865"
  uid: 0c0f7a4c-4307-4187-b591-6155695ac85b
spec:
  workerLatencyProfile: LowUpdateSlowReaction <span id="CO36-1"><!--Empty--></span><span class="callout">1</span>

# ...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO36-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specifies to use the low worker latency policy.
										</div></dd></dl></div><p class="simpara">
									Scheduling on each worker node is disabled as the change is being applied.
								</p></li></ol></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							When all nodes return to the <code class="literal">Ready</code> condition, you can use the following command to look in the Kubernetes Controller Manager to ensure it was applied:
						</p><pre class="programlisting language-terminal">$ oc get KubeControllerManager -o yaml | grep -i workerlatency -A 5 -B 5</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal"># ...
    - lastTransitionTime: "2022-07-11T19:47:10Z"
      reason: ProfileUpdated
      status: "False"
      type: WorkerLatencyProfileProgressing
    - lastTransitionTime: "2022-07-11T19:47:10Z" <span id="CO37-1"><!--Empty--></span><span class="callout">1</span>
      message: all static pod revision(s) have updated latency profile
      reason: ProfileUpdated
      status: "True"
      type: WorkerLatencyProfileComplete
    - lastTransitionTime: "2022-07-11T19:20:11Z"
      reason: AsExpected
      status: "False"
      type: WorkerLatencyProfileDegraded
    - lastTransitionTime: "2022-07-11T19:20:36Z"
      status: "False"
# ...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO37-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specifies that the profile is applied and active.
								</div></dd></dl></div></li></ul></div><p>
					To change the low profile to medium or change the medium to low, edit the <code class="literal">node.config</code> object and set the <code class="literal">spec.workerLatencyProfile</code> parameter to the appropriate value.
				</p></section></section><section class="section" id="post-install-cpms-setup"><div class="titlepage"><div><div><h2 class="title">7.6. Managing control plane machines</h2></div></div></div><p>
				<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-about">Control plane machine sets</a> provide management capabilities for control plane machines that are similar to what compute machine sets provide for compute machines. The availability and initial status of control plane machine sets on your cluster depend on your cloud provider and the version of OpenShift Container Platform that you installed. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-getting-started">Getting started with control plane machine sets</a>.
			</p></section><section class="section" id="post-install-creating-infrastructure-machinesets-production"><div class="titlepage"><div><div><h2 class="title">7.7. Creating infrastructure machine sets for production environments</h2></div></div></div><p>
				You can create a compute machine set to create machines that host only infrastructure components, such as the default router, the integrated container image registry, and components for cluster metrics and monitoring. These infrastructure machines are not counted toward the total number of subscriptions that are required to run the environment.
			</p><p>
				In a production deployment, it is recommended that you deploy at least three compute machine sets to hold infrastructure components. Both OpenShift Logging and Red Hat OpenShift Service Mesh deploy Elasticsearch, which requires three instances to be installed on different nodes. Each of these nodes can be deployed to different availability zones for high availability. A configuration like this requires three different compute machine sets, one for each availability zone. In global Azure regions that do not have multiple availability zones, you can use availability sets to ensure high availability.
			</p><p>
				For information on infrastructure nodes and which components can run on infrastructure nodes, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-infrastructure-machinesets">Creating infrastructure machine sets</a>.
			</p><p>
				To create an infrastructure node, you can <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#machineset-creating_post-install-cluster-tasks">use a machine set</a>, <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#creating-an-infra-node_post-install-cluster-tasks">assign a label to the nodes</a>, or <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#creating-infra-machines_post-install-cluster-tasks">use a machine config pool</a>.
			</p><p>
				For sample machine sets that you can use with these procedures, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-infrastructure-machinesets-clouds">Creating machine sets for different clouds</a>.
			</p><p>
				Applying a specific node selector to all infrastructure components causes OpenShift Container Platform to <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#moving-resources-to-infrastructure-machinesets">schedule those workloads on nodes with that label</a>.
			</p><section class="section" id="machineset-creating_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.7.1. Creating a compute machine set</h3></div></div></div><p>
					In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Deploy an OpenShift Container Platform cluster.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <code class="literal">&lt;file_name&gt;.yaml</code>.
						</p><p class="simpara">
							Ensure that you set the <code class="literal">&lt;clusterID&gt;</code> and <code class="literal">&lt;role&gt;</code> parameter values.
						</p></li><li class="listitem"><p class="simpara">
							Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									To list the compute machine sets in your cluster, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									To view values of a specific compute machine set custom resource (CR), run the following command:
								</p><pre class="programlisting language-terminal">$ oc get machineset &lt;machineset_name&gt; \
  -n openshift-machine-api -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <span id="CO38-1"><!--Empty--></span><span class="callout">1</span>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <span id="CO38-2"><!--Empty--></span><span class="callout">2</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
    spec:
      providerSpec: <span id="CO38-3"><!--Empty--></span><span class="callout">3</span>
        ...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO38-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The cluster infrastructure ID.
										</div></dd><dt><a href="#CO38-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											A default node label.
										</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												For clusters that have user-provisioned infrastructure, a compute machine set can only create <code class="literal">worker</code> and <code class="literal">infra</code> type machines.
											</p></div></div></dd><dt><a href="#CO38-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											The values in the <code class="literal">&lt;providerSpec&gt;</code> section of the compute machine set CR are platform-specific. For more information about <code class="literal">&lt;providerSpec&gt;</code> parameters in the CR, see the sample compute machine set CR configuration for your provider.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">MachineSet</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							View the list of compute machine sets by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineset -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-infra-us-east-1a    1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</pre>

							</p></div><p class="simpara">
							When the new compute machine set is available, the <code class="literal">DESIRED</code> and <code class="literal">CURRENT</code> values match. If the compute machine set is not available, wait a few minutes and run the command again.
						</p></li></ul></div></section><section class="section" id="creating-an-infra-node_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.7.2. Creating an infrastructure node</h3></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						See Creating infrastructure machine sets for installer-provisioned infrastructure environments or for any cluster where the control plane nodes are managed by the machine API.
					</p></div></div><p>
					Requirements of the cluster dictate that infrastructure, also called <code class="literal">infra</code> nodes, be provisioned. The installer only provides provisions for control plane and worker nodes. Worker nodes can be designated as infrastructure nodes or application, also called <code class="literal">app</code>, nodes through labeling.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add a label to the worker node that you want to act as application node:
						</p><pre class="programlisting language-terminal">$ oc label node &lt;node-name&gt; node-role.kubernetes.io/app=""</pre></li><li class="listitem"><p class="simpara">
							Add a label to the worker nodes that you want to act as infrastructure nodes:
						</p><pre class="programlisting language-terminal">$ oc label node &lt;node-name&gt; node-role.kubernetes.io/infra=""</pre></li><li class="listitem"><p class="simpara">
							Check to see if applicable nodes now have the <code class="literal">infra</code> role and <code class="literal">app</code> roles:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre></li><li class="listitem"><p class="simpara">
							Create a default cluster-wide node selector. The default node selector is applied to pods created in all namespaces. This creates an intersection with any existing node selectors on a pod, which additionally constrains the pod’s selector.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								If the default node selector key conflicts with the key of a pod’s label, then the default node selector is not applied.
							</p><p>
								However, do not set a default node selector that might cause a pod to become unschedulable. For example, setting the default node selector to a specific node role, such as <code class="literal">node-role.kubernetes.io/infra=""</code>, when a pod’s label is set to a different node role, such as <code class="literal">node-role.kubernetes.io/master=""</code>, can cause the pod to become unschedulable. For this reason, use caution when setting the default node selector to specific node roles.
							</p><p>
								You can alternatively use a project node selector to avoid cluster-wide node selector key conflicts.
							</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Edit the <code class="literal">Scheduler</code> object:
								</p><pre class="programlisting language-terminal">$ oc edit scheduler cluster</pre></li><li class="listitem"><p class="simpara">
									Add the <code class="literal">defaultNodeSelector</code> field with the appropriate node selector:
								</p><pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  name: cluster
spec:
  defaultNodeSelector: topology.kubernetes.io/region=us-east-1 <span id="CO39-1"><!--Empty--></span><span class="callout">1</span>
# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO39-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											This example node selector deploys pods on nodes in the <code class="literal">us-east-1</code> region by default.
										</div></dd></dl></div></li><li class="listitem">
									Save the file to apply the changes.
								</li></ol></div></li></ol></div><p>
					You can now move infrastructure resources to the newly labeled <code class="literal">infra</code> nodes.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For information on how to configure project node selectors to avoid cluster-wide node selector key conflicts, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#project-node-selectors_nodes-scheduler-node-selectors">Project node selectors</a>.
						</li></ul></div></section><section class="section" id="creating-infra-machines_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.7.3. Creating a machine config pool for infrastructure machines</h3></div></div></div><p>
					If you need infrastructure machines to have dedicated configurations, you must create an infra pool.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add a label to the node you want to assign as the infra node with a specific label:
						</p><pre class="programlisting language-terminal">$ oc label node &lt;node_name&gt; &lt;label&gt;</pre><pre class="programlisting language-terminal">$ oc label node ci-ln-n8mqwr2-f76d1-xscn2-worker-c-6fmtx node-role.kubernetes.io/infra=</pre></li><li class="listitem"><p class="simpara">
							Create a machine config pool that contains both the worker role and your custom role as machine config selector:
						</p><pre class="programlisting language-terminal">$ cat infra.mcp.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: infra
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,infra]} <span id="CO40-1"><!--Empty--></span><span class="callout">1</span>
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/infra: "" <span id="CO40-2"><!--Empty--></span><span class="callout">2</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO40-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add the worker role and your custom role.
								</div></dd><dt><a href="#CO40-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Add the label you added to the node as a <code class="literal">nodeSelector</code>.
								</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Custom machine config pools inherit machine configs from the worker pool. Custom pools use any machine config targeted for the worker pool, but add the ability to also deploy changes that are targeted at only the custom pool. Because a custom pool inherits resources from the worker pool, any change to the worker pool also affects the custom pool.
							</p></div></div></li><li class="listitem"><p class="simpara">
							After you have the YAML file, you can create the machine config pool:
						</p><pre class="programlisting language-terminal">$ oc create -f infra.mcp.yaml</pre></li><li class="listitem"><p class="simpara">
							Check the machine configs to ensure that the infrastructure configuration rendered successfully:
						</p><pre class="programlisting language-terminal">$ oc get machineconfig</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                        GENERATEDBYCONTROLLER                      IGNITIONVERSION   CREATED
00-master                                                   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
00-worker                                                   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
01-master-container-runtime                                 365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
01-master-kubelet                                           365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
01-worker-container-runtime                                 365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
01-worker-kubelet                                           365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
99-master-1ae2a1e0-a115-11e9-8f14-005056899d54-registries   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
99-master-ssh                                                                                          3.2.0             31d
99-worker-1ae64748-a115-11e9-8f14-005056899d54-registries   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
99-worker-ssh                                                                                          3.2.0             31d
rendered-infra-4e48906dca84ee702959c71a53ee80e7             365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             23m
rendered-master-072d4b2da7f88162636902b074e9e28e            5b6fb8349a29735e48446d435962dec4547d3090   3.2.0             31d
rendered-master-3e88ec72aed3886dec061df60d16d1af            02c07496ba0417b3e12b78fb32baf6293d314f79   3.2.0             31d
rendered-master-419bee7de96134963a15fdf9dd473b25            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             17d
rendered-master-53f5c91c7661708adce18739cc0f40fb            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             13d
rendered-master-a6a357ec18e5bce7f5ac426fc7c5ffcd            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             7d3h
rendered-master-dc7f874ec77fc4b969674204332da037            5b6fb8349a29735e48446d435962dec4547d3090   3.2.0             31d
rendered-worker-1a75960c52ad18ff5dfa6674eb7e533d            5b6fb8349a29735e48446d435962dec4547d3090   3.2.0             31d
rendered-worker-2640531be11ba43c61d72e82dc634ce6            5b6fb8349a29735e48446d435962dec4547d3090   3.2.0             31d
rendered-worker-4e48906dca84ee702959c71a53ee80e7            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             7d3h
rendered-worker-4f110718fe88e5f349987854a1147755            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             17d
rendered-worker-afc758e194d6188677eb837842d3b379            02c07496ba0417b3e12b78fb32baf6293d314f79   3.2.0             31d
rendered-worker-daa08cc1e8f5fcdeba24de60cd955cc3            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             13d</pre>

							</p></div><p class="simpara">
							You should see a new machine config, with the <code class="literal">rendered-infra-*</code> prefix.
						</p></li><li class="listitem"><p class="simpara">
							Optional: To deploy changes to a custom pool, create a machine config that uses the custom pool name as the label, such as <code class="literal">infra</code>. Note that this is not required and only shown for instructional purposes. In this manner, you can apply any custom configurations specific to only your infra nodes.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								After you create the new machine config pool, the MCO generates a new rendered config for that pool, and associated nodes of that pool reboot to apply the new configuration.
							</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a machine config:
								</p><pre class="programlisting language-terminal">$ cat infra.mc.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  name: 51-infra
  labels:
    machineconfiguration.openshift.io/role: infra <span id="CO41-1"><!--Empty--></span><span class="callout">1</span>
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
      - path: /etc/infratest
        mode: 0644
        contents:
          source: data:,infra</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO41-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Add the label you added to the node as a <code class="literal">nodeSelector</code>.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Apply the machine config to the infra-labeled nodes:
								</p><pre class="programlisting language-terminal">$ oc create -f infra.mc.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Confirm that your new machine config pool is available:
						</p><pre class="programlisting language-terminal">$ oc get mcp</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
infra    rendered-infra-60e35c2e99f42d976e084fa94da4d0fc    True      False      False      1              1                   1                     0                      4m20s
master   rendered-master-9360fdb895d4c131c7c4bebbae099c90   True      False      False      3              3                   3                     0                      91m
worker   rendered-worker-60e35c2e99f42d976e084fa94da4d0fc   True      False      False      2              2                   2                     0                      91m</pre>

							</p></div><p class="simpara">
							In this example, a worker node was changed to an infra node.
						</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/architecture/#architecture-machine-config-pools_control-plane">Node configuration management with machine config pools</a> for more information on grouping infra machines in a custom pool.
						</li></ul></div></section></section><section class="section" id="assigning-machine-set-resources-to-infra-nodes"><div class="titlepage"><div><div><h2 class="title">7.8. Assigning machine set resources to infrastructure nodes</h2></div></div></div><p>
				After creating an infrastructure machine set, the <code class="literal">worker</code> and <code class="literal">infra</code> roles are applied to new infra nodes. Nodes with the <code class="literal">infra</code> role are not counted toward the total number of subscriptions that are required to run the environment, even when the <code class="literal">worker</code> role is also applied.
			</p><p>
				However, when an infra node is assigned the worker role, there is a chance that user workloads can get assigned inadvertently to the infra node. To avoid this, you can apply a taint to the infra node and tolerations for the pods that you want to control.
			</p><section class="section" id="binding-infra-node-workloads-using-taints-tolerations_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.8.1. Binding infrastructure node workloads using taints and tolerations</h3></div></div></div><p>
					If you have an infra node that has the <code class="literal">infra</code> and <code class="literal">worker</code> roles assigned, you must configure the node so that user workloads are not assigned to it.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						It is recommended that you preserve the dual <code class="literal">infra,worker</code> label that is created for infra nodes and use taints and tolerations to manage nodes that user workloads are scheduled on. If you remove the <code class="literal">worker</code> label from the node, you must create a custom pool to manage it. A node with a label other than <code class="literal">master</code> or <code class="literal">worker</code> is not recognized by the MCO without a custom pool. Maintaining the <code class="literal">worker</code> label allows the node to be managed by the default worker machine config pool, if no custom pools that select the custom label exists. The <code class="literal">infra</code> label communicates to the cluster that it does not count toward the total number of subscriptions.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Configure additional <code class="literal">MachineSet</code> objects in your OpenShift Container Platform cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add a taint to the infra node to prevent scheduling user workloads on it:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Determine if the node has the taint:
								</p><pre class="programlisting language-terminal">$ oc describe nodes &lt;node_name&gt;</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
										
<pre class="programlisting language-text">oc describe node ci-ln-iyhx092-f76d1-nvdfm-worker-b-wln2l
Name:               ci-ln-iyhx092-f76d1-nvdfm-worker-b-wln2l
Roles:              worker
 ...
Taints:             node-role.kubernetes.io/infra:NoSchedule
 ...</pre>

									</p></div><p class="simpara">
									This example shows that the node has a taint. You can proceed with adding a toleration to your pod in the next step.
								</p></li><li class="listitem"><p class="simpara">
									If you have not configured a taint to prevent scheduling user workloads on it:
								</p><pre class="programlisting language-terminal">$ oc adm taint nodes &lt;node_name&gt; &lt;key&gt;=&lt;value&gt;:&lt;effect&gt;</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc adm taint nodes node1 node-role.kubernetes.io/infra=reserved:NoExecute</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
									You can alternatively apply the following YAML to add the taint:
								</p><pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  name: &lt;node_name&gt;
  labels:
    ...
spec:
  taints:
    - key: node-role.kubernetes.io/infra
      effect: NoExecute
      value: reserved
  ...</pre></div></div><p class="simpara">
									This example places a taint on <code class="literal">node1</code> that has key <code class="literal">node-role.kubernetes.io/infra</code> and taint effect <code class="literal">NoSchedule</code>. Nodes with the <code class="literal">NoSchedule</code> effect schedule only pods that tolerate the taint, but allow existing pods to remain scheduled on the node.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										If a descheduler is used, pods violating node taints could be evicted from the cluster.
									</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Add tolerations for the pod configurations you want to schedule on the infra node, like router, registry, and monitoring workloads. Add the following code to the <code class="literal">Pod</code> object specification:
						</p><pre class="programlisting language-yaml">tolerations:
  - effect: NoExecute <span id="CO42-1"><!--Empty--></span><span class="callout">1</span>
    key: node-role.kubernetes.io/infra <span id="CO42-2"><!--Empty--></span><span class="callout">2</span>
    operator: Exists <span id="CO42-3"><!--Empty--></span><span class="callout">3</span>
    value: reserved <span id="CO42-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO42-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the effect that you added to the node.
								</div></dd><dt><a href="#CO42-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the key that you added to the node.
								</div></dd><dt><a href="#CO42-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">Exists</code> Operator to require a taint with the key <code class="literal">node-role.kubernetes.io/infra</code> to be present on the node.
								</div></dd><dt><a href="#CO42-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specify the value of the key-value pair taint that you added to the node.
								</div></dd></dl></div><p class="simpara">
							This toleration matches the taint created by the <code class="literal">oc adm taint</code> command. A pod with this toleration can be scheduled onto the infra node.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Moving pods for an Operator installed via OLM to an infra node is not always possible. The capability to move Operator pods depends on the configuration of each Operator.
							</p></div></div></li><li class="listitem">
							Schedule the pod to the infra node using a scheduler. See the documentation for <span class="emphasis"><em>Controlling pod placement onto nodes</em></span> for details.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-about">Controlling pod placement using the scheduler</a> for general information on scheduling a pod to a node.
						</li></ul></div></section></section><section class="section" id="moving-resources-to-infrastructure-machinesets"><div class="titlepage"><div><div><h2 class="title">7.9. Moving resources to infrastructure machine sets</h2></div></div></div><p>
				Some of the infrastructure resources are deployed in your cluster by default. You can move them to the infrastructure machine sets that you created.
			</p><section class="section" id="infrastructure-moving-router_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.9.1. Moving the router</h3></div></div></div><p>
					You can deploy the router pod to a different compute machine set. By default, the pod is deployed to a worker node.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Configure additional compute machine sets in your OpenShift Container Platform cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the <code class="literal">IngressController</code> custom resource for the router Operator:
						</p><pre class="programlisting language-terminal">$ oc get ingresscontroller default -n openshift-ingress-operator -o yaml</pre><p class="simpara">
							The command output resembles the following text:
						</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: 2019-04-18T12:35:39Z
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 1
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "11341"
  selfLink: /apis/operator.openshift.io/v1/namespaces/openshift-ingress-operator/ingresscontrollers/default
  uid: 79509e05-61d6-11e9-bc55-02ce4781844a
spec: {}
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2019-04-18T12:36:15Z
    status: "True"
    type: Available
  domain: apps.&lt;cluster&gt;.example.com
  endpointPublishingStrategy:
    type: LoadBalancerService
  selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default</pre></li><li class="listitem"><p class="simpara">
							Edit the <code class="literal">ingresscontroller</code> resource and change the <code class="literal">nodeSelector</code> to use the <code class="literal">infra</code> label:
						</p><pre class="programlisting language-terminal">$ oc edit ingresscontroller default -n openshift-ingress-operator</pre><pre class="programlisting language-yaml">  spec:
    nodePlacement:
      nodeSelector: <span id="CO43-1"><!--Empty--></span><span class="callout">1</span>
        matchLabels:
          node-role.kubernetes.io/infra: ""
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/infra
        value: reserved
      - effect: NoExecute
        key: node-role.kubernetes.io/infra
        value: reserved</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO43-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add a <code class="literal">nodeSelector</code> parameter with the appropriate value to the component you want to move. You can use a <code class="literal">nodeSelector</code> in the format shown or use <code class="literal">&lt;key&gt;: &lt;value&gt;</code> pairs, based on the value specified for the node. If you added a taint to the infrastructure node, also add a matching toleration.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Confirm that the router pod is running on the <code class="literal">infra</code> node.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									View the list of router pods and note the node name of the running pod:
								</p><pre class="programlisting language-terminal">$ oc get pod -n openshift-ingress -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                              READY     STATUS        RESTARTS   AGE       IP           NODE                           NOMINATED NODE   READINESS GATES
router-default-86798b4b5d-bdlvd   1/1      Running       0          28s       10.130.2.4   ip-10-0-217-226.ec2.internal   &lt;none&gt;           &lt;none&gt;
router-default-955d875f4-255g8    0/1      Terminating   0          19h       10.129.2.4   ip-10-0-148-172.ec2.internal   &lt;none&gt;           &lt;none&gt;</pre>

									</p></div><p class="simpara">
									In this example, the running pod is on the <code class="literal">ip-10-0-217-226.ec2.internal</code> node.
								</p></li><li class="listitem"><p class="simpara">
									View the node status of the running pod:
								</p><pre class="programlisting language-terminal">$ oc get node &lt;node_name&gt; <span id="CO44-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO44-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the <code class="literal">&lt;node_name&gt;</code> that you obtained from the pod list.
										</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                          STATUS  ROLES         AGE   VERSION
ip-10-0-217-226.ec2.internal  Ready   infra,worker  17h   v1.26.0</pre>

									</p></div><p class="simpara">
									Because the role list includes <code class="literal">infra</code>, the pod is running on the correct node.
								</p></li></ol></div></li></ol></div></section><section class="section" id="infrastructure-moving-registry_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.9.2. Moving the default registry</h3></div></div></div><p>
					You configure the registry Operator to deploy its pods to different nodes.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Configure additional compute machine sets in your OpenShift Container Platform cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the <code class="literal">config/instance</code> object:
						</p><pre class="programlisting language-terminal">$ oc get configs.imageregistry.operator.openshift.io/cluster -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: imageregistry.operator.openshift.io/v1
kind: Config
metadata:
  creationTimestamp: 2019-02-05T13:52:05Z
  finalizers:
  - imageregistry.operator.openshift.io/finalizer
  generation: 1
  name: cluster
  resourceVersion: "56174"
  selfLink: /apis/imageregistry.operator.openshift.io/v1/configs/cluster
  uid: 36fd3724-294d-11e9-a524-12ffeee2931b
spec:
  httpSecret: d9a012ccd117b1e6616ceccb2c3bb66a5fed1b5e481623
  logging: 2
  managementState: Managed
  proxy: {}
  replicas: 1
  requests:
    read: {}
    write: {}
  storage:
    s3:
      bucket: image-registry-us-east-1-c92e88cad85b48ec8b312344dff03c82-392c
      region: us-east-1
status:
...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Edit the <code class="literal">config/instance</code> object:
						</p><pre class="programlisting language-terminal">$ oc edit configs.imageregistry.operator.openshift.io/cluster</pre><pre class="programlisting language-yaml">spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          namespaces:
          - openshift-image-registry
          topologyKey: kubernetes.io/hostname
        weight: 100
  logLevel: Normal
  managementState: Managed
  nodeSelector: <span id="CO45-1"><!--Empty--></span><span class="callout">1</span>
    node-role.kubernetes.io/infra: ""
  tolerations:
  - effect: NoSchedule
    key: node-role.kubernetes.io/infra
    value: reserved
  - effect: NoExecute
    key: node-role.kubernetes.io/infra
    value: reserved</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO45-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add a <code class="literal">nodeSelector</code> parameter with the appropriate value to the component you want to move. You can use a <code class="literal">nodeSelector</code> in the format shown or use <code class="literal">&lt;key&gt;: &lt;value&gt;</code> pairs, based on the value specified for the node. If you added a taint to the infrasructure node, also add a matching toleration.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Verify the registry pod has been moved to the infrastructure node.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Run the following command to identify the node where the registry pod is located:
								</p><pre class="programlisting language-terminal">$ oc get pods -o wide -n openshift-image-registry</pre></li><li class="listitem"><p class="simpara">
									Confirm the node has the label you specified:
								</p><pre class="programlisting language-terminal">$ oc describe node &lt;node_name&gt;</pre><p class="simpara">
									Review the command output and confirm that <code class="literal">node-role.kubernetes.io/infra</code> is in the <code class="literal">LABELS</code> list.
								</p></li></ol></div></li></ol></div></section><section class="section" id="infrastructure-moving-monitoring_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.9.3. Moving the monitoring solution</h3></div></div></div><p>
					The monitoring stack includes multiple components, including Prometheus, Thanos Querier, and Alertmanager. The Cluster Monitoring Operator manages this stack. To redeploy the monitoring stack to infrastructure nodes, you can create and apply a custom config map.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">cluster-monitoring-config</code> config map and change the <code class="literal">nodeSelector</code> to use the <code class="literal">infra</code> label:
						</p><pre class="programlisting language-terminal">$ oc edit configmap cluster-monitoring-config -n openshift-monitoring</pre><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |+
    alertmanagerMain:
      nodeSelector: <span id="CO46-1"><!--Empty--></span><span class="callout">1</span>
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    prometheusK8s:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    prometheusOperator:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    kubeStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    telemeterClient:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    openshiftStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    thanosQuerier:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO46-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add a <code class="literal">nodeSelector</code> parameter with the appropriate value to the component you want to move. You can use a <code class="literal">nodeSelector</code> in the format shown or use <code class="literal">&lt;key&gt;: &lt;value&gt;</code> pairs, based on the value specified for the node. If you added a taint to the infrasructure node, also add a matching toleration.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Watch the monitoring pods move to the new machines:
						</p><pre class="programlisting language-terminal">$ watch 'oc get pod -n openshift-monitoring -o wide'</pre></li><li class="listitem"><p class="simpara">
							If a component has not moved to the <code class="literal">infra</code> node, delete the pod with this component:
						</p><pre class="programlisting language-terminal">$ oc delete pod -n openshift-monitoring &lt;pod&gt;</pre><p class="simpara">
							The component from the deleted pod is re-created on the <code class="literal">infra</code> node.
						</p></li></ol></div></section><section class="section" id="infrastructure-moving-logging_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.9.4. Moving OpenShift Logging resources</h3></div></div></div><p>
					You can configure the Cluster Logging Operator to deploy the pods for logging subsystem components, such as Elasticsearch and Kibana, to different nodes. You cannot move the Cluster Logging Operator pod from its installed location.
				</p><p>
					For example, you can move the Elasticsearch pods to a separate node because of high CPU, memory, and disk requirements.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The Red Hat OpenShift Logging and Elasticsearch Operators must be installed. These features are not installed by default.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">ClusterLogging</code> custom resource (CR) in the <code class="literal">openshift-logging</code> project:
						</p><pre class="programlisting language-terminal">$ oc edit ClusterLogging instance</pre><pre class="programlisting language-yaml">apiVersion: logging.openshift.io/v1
kind: ClusterLogging

...

spec:
  collection:
    logs:
      fluentd:
        resources: null
      type: fluentd
  logStore:
    elasticsearch:
      nodeCount: 3
      nodeSelector: <span id="CO47-1"><!--Empty--></span><span class="callout">1</span>
        node-role.kubernetes.io/infra: ''
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/infra
        value: reserved
      - effect: NoExecute
        key: node-role.kubernetes.io/infra
        value: reserved
      redundancyPolicy: SingleRedundancy
      resources:
        limits:
          cpu: 500m
          memory: 16Gi
        requests:
          cpu: 500m
          memory: 16Gi
      storage: {}
    type: elasticsearch
  managementState: Managed
  visualization:
    kibana:
      nodeSelector: <span id="CO47-2"><!--Empty--></span><span class="callout">2</span>
        node-role.kubernetes.io/infra: ''
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/infra
        value: reserved
      - effect: NoExecute
        key: node-role.kubernetes.io/infra
        value: reserved
      proxy:
        resources: null
      replicas: 1
      resources: null
    type: kibana

...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO47-1"><span class="callout">1</span></a> <a href="#CO47-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Add a <code class="literal">nodeSelector</code> parameter with the appropriate value to the component you want to move. You can use a <code class="literal">nodeSelector</code> in the format shown or use <code class="literal">&lt;key&gt;: &lt;value&gt;</code> pairs, based on the value specified for the node. If you added a taint to the infrasructure node, also add a matching toleration.
								</div></dd></dl></div></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						To verify that a component has moved, you can use the <code class="literal">oc get pod -o wide</code> command.
					</p></div><p>
					For example:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							You want to move the Kibana pod from the <code class="literal">ip-10-0-147-79.us-east-2.compute.internal</code> node:
						</p><pre class="programlisting language-terminal">$ oc get pod kibana-5b8bdf44f9-ccpq9 -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                      READY   STATUS    RESTARTS   AGE   IP            NODE                                        NOMINATED NODE   READINESS GATES
kibana-5b8bdf44f9-ccpq9   2/2     Running   0          27s   10.129.2.18   ip-10-0-147-79.us-east-2.compute.internal   &lt;none&gt;           &lt;none&gt;</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							You want to move the Kibana pod to the <code class="literal">ip-10-0-139-48.us-east-2.compute.internal</code> node, a dedicated infrastructure node:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                         STATUS   ROLES          AGE   VERSION
ip-10-0-133-216.us-east-2.compute.internal   Ready    master         60m   v1.26.0
ip-10-0-139-146.us-east-2.compute.internal   Ready    master         60m   v1.26.0
ip-10-0-139-192.us-east-2.compute.internal   Ready    worker         51m   v1.26.0
ip-10-0-139-241.us-east-2.compute.internal   Ready    worker         51m   v1.26.0
ip-10-0-147-79.us-east-2.compute.internal    Ready    worker         51m   v1.26.0
ip-10-0-152-241.us-east-2.compute.internal   Ready    master         60m   v1.26.0
ip-10-0-139-48.us-east-2.compute.internal    Ready    infra          51m   v1.26.0</pre>

							</p></div><p class="simpara">
							Note that the node has a <code class="literal">node-role.kubernetes.io/infra: ''</code> label:
						</p><pre class="programlisting language-terminal">$ oc get node ip-10-0-139-48.us-east-2.compute.internal -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  name: ip-10-0-139-48.us-east-2.compute.internal
  selfLink: /api/v1/nodes/ip-10-0-139-48.us-east-2.compute.internal
  uid: 62038aa9-661f-41d7-ba93-b5f1b6ef8751
  resourceVersion: '39083'
  creationTimestamp: '2020-04-13T19:07:55Z'
  labels:
    node-role.kubernetes.io/infra: ''
...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To move the Kibana pod, edit the <code class="literal">ClusterLogging</code> CR to add a node selector:
						</p><pre class="programlisting language-yaml">apiVersion: logging.openshift.io/v1
kind: ClusterLogging

...

spec:

...

  visualization:
    kibana:
      nodeSelector: <span id="CO48-1"><!--Empty--></span><span class="callout">1</span>
        node-role.kubernetes.io/infra: ''
      proxy:
        resources: null
      replicas: 1
      resources: null
    type: kibana</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO48-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add a node selector to match the label in the node specification.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							After you save the CR, the current Kibana pod is terminated and new pod is deployed:
						</p><pre class="programlisting language-terminal">$ oc get pods</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                            READY   STATUS        RESTARTS   AGE
cluster-logging-operator-84d98649c4-zb9g7       1/1     Running       0          29m
elasticsearch-cdm-hwv01pf7-1-56588f554f-kpmlg   2/2     Running       0          28m
elasticsearch-cdm-hwv01pf7-2-84c877d75d-75wqj   2/2     Running       0          28m
elasticsearch-cdm-hwv01pf7-3-f5d95b87b-4nx78    2/2     Running       0          28m
fluentd-42dzz                                   1/1     Running       0          28m
fluentd-d74rq                                   1/1     Running       0          28m
fluentd-m5vr9                                   1/1     Running       0          28m
fluentd-nkxl7                                   1/1     Running       0          28m
fluentd-pdvqb                                   1/1     Running       0          28m
fluentd-tflh6                                   1/1     Running       0          28m
kibana-5b8bdf44f9-ccpq9                         2/2     Terminating   0          4m11s
kibana-7d85dcffc8-bfpfp                         2/2     Running       0          33s</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							The new pod is on the <code class="literal">ip-10-0-139-48.us-east-2.compute.internal</code> node:
						</p><pre class="programlisting language-terminal">$ oc get pod kibana-7d85dcffc8-bfpfp -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                      READY   STATUS        RESTARTS   AGE   IP            NODE                                        NOMINATED NODE   READINESS GATES
kibana-7d85dcffc8-bfpfp   2/2     Running       0          43s   10.131.0.22   ip-10-0-139-48.us-east-2.compute.internal   &lt;none&gt;           &lt;none&gt;</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							After a few moments, the original Kibana pod is removed.
						</p><pre class="programlisting language-terminal">$ oc get pods</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                            READY   STATUS    RESTARTS   AGE
cluster-logging-operator-84d98649c4-zb9g7       1/1     Running   0          30m
elasticsearch-cdm-hwv01pf7-1-56588f554f-kpmlg   2/2     Running   0          29m
elasticsearch-cdm-hwv01pf7-2-84c877d75d-75wqj   2/2     Running   0          29m
elasticsearch-cdm-hwv01pf7-3-f5d95b87b-4nx78    2/2     Running   0          29m
fluentd-42dzz                                   1/1     Running   0          29m
fluentd-d74rq                                   1/1     Running   0          29m
fluentd-m5vr9                                   1/1     Running   0          29m
fluentd-nkxl7                                   1/1     Running   0          29m
fluentd-pdvqb                                   1/1     Running   0          29m
fluentd-tflh6                                   1/1     Running   0          29m
kibana-7d85dcffc8-bfpfp                         2/2     Running   0          62s</pre>

							</p></div></li></ul></div></section></section><section class="section" id="cluster-autoscaler-about_post-install-cluster-tasks"><div class="titlepage"><div><div><h2 class="title">7.10. About the cluster autoscaler</h2></div></div></div><p>
				The cluster autoscaler adjusts the size of an OpenShift Container Platform cluster to meet its current deployment needs. It uses declarative, Kubernetes-style arguments to provide infrastructure management that does not rely on objects of a specific cloud provider. The cluster autoscaler has a cluster scope, and is not associated with a particular namespace.
			</p><p>
				The cluster autoscaler increases the size of the cluster when there are pods that fail to schedule on any of the current worker nodes due to insufficient resources or when another node is necessary to meet deployment needs. The cluster autoscaler does not increase the cluster resources beyond the limits that you specify.
			</p><p>
				The cluster autoscaler computes the total memory, CPU, and GPU on all nodes the cluster, even though it does not manage the control plane nodes. These values are not single-machine oriented. They are an aggregation of all the resources in the entire cluster. For example, if you set the maximum memory resource limit, the cluster autoscaler includes all the nodes in the cluster when calculating the current memory usage. That calculation is then used to determine if the cluster autoscaler has the capacity to add more worker resources.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Ensure that the <code class="literal">maxNodesTotal</code> value in the <code class="literal">ClusterAutoscaler</code> resource definition that you create is large enough to account for the total possible number of machines in your cluster. This value must encompass the number of control plane machines and the possible number of compute machines that you might scale to.
				</p></div></div><p>
				Every 10 seconds, the cluster autoscaler checks which nodes are unnecessary in the cluster and removes them. The cluster autoscaler considers a node for removal if the following conditions apply:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The node utilization is less than the <span class="emphasis"><em>node utilization level</em></span> threshold for the cluster. The node utilization level is the sum of the requested resources divided by the allocated resources for the node. If you do not specify a value in the <code class="literal">ClusterAutoscaler</code> custom resource, the cluster autoscaler uses a default value of <code class="literal">0.5</code>, which corresponds to 50% utilization.
					</li><li class="listitem">
						The cluster autoscaler can move all pods running on the node to the other nodes. The Kubernetes scheduler is responsible for scheduling pods on the nodes.
					</li><li class="listitem">
						The cluster autoscaler does not have scale down disabled annotation.
					</li></ul></div><p>
				If the following types of pods are present on a node, the cluster autoscaler will not remove the node:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Pods with restrictive pod disruption budgets (PDBs).
					</li><li class="listitem">
						Kube-system pods that do not run on the node by default.
					</li><li class="listitem">
						Kube-system pods that do not have a PDB or have a PDB that is too restrictive.
					</li><li class="listitem">
						Pods that are not backed by a controller object such as a deployment, replica set, or stateful set.
					</li><li class="listitem">
						Pods with local storage.
					</li><li class="listitem">
						Pods that cannot be moved elsewhere because of a lack of resources, incompatible node selectors or affinity, matching anti-affinity, and so on.
					</li><li class="listitem">
						Unless they also have a <code class="literal">"cluster-autoscaler.kubernetes.io/safe-to-evict": "true"</code> annotation, pods that have a <code class="literal">"cluster-autoscaler.kubernetes.io/safe-to-evict": "false"</code> annotation.
					</li></ul></div><p>
				For example, you set the maximum CPU limit to 64 cores and configure the cluster autoscaler to only create machines that have 8 cores each. If your cluster starts with 30 cores, the cluster autoscaler can add up to 4 more nodes with 32 cores, for a total of 62.
			</p><p>
				If you configure the cluster autoscaler, additional usage restrictions apply:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Do not modify the nodes that are in autoscaled node groups directly. All nodes within the same node group have the same capacity and labels and run the same system pods.
					</li><li class="listitem">
						Specify requests for your pods.
					</li><li class="listitem">
						If you have to prevent pods from being deleted too quickly, configure appropriate PDBs.
					</li><li class="listitem">
						Confirm that your cloud provider quota is large enough to support the maximum node pools that you configure.
					</li><li class="listitem">
						Do not run additional node group autoscalers, especially the ones offered by your cloud provider.
					</li></ul></div><p>
				The horizontal pod autoscaler (HPA) and the cluster autoscaler modify cluster resources in different ways. The HPA changes the deployment’s or replica set’s number of replicas based on the current CPU load. If the load increases, the HPA creates new replicas, regardless of the amount of resources available to the cluster. If there are not enough resources, the cluster autoscaler adds resources so that the HPA-created pods can run. If the load decreases, the HPA stops some replicas. If this action causes some nodes to be underutilized or completely empty, the cluster autoscaler deletes the unnecessary nodes.
			</p><p>
				The cluster autoscaler takes pod priorities into account. The Pod Priority and Preemption feature enables scheduling pods based on priorities if the cluster does not have enough resources, but the cluster autoscaler ensures that the cluster has resources to run all pods. To honor the intention of both features, the cluster autoscaler includes a priority cutoff function. You can use this cutoff to schedule "best-effort" pods, which do not cause the cluster autoscaler to increase resources but instead run only when spare resources are available.
			</p><p>
				Pods with priority lower than the cutoff value do not cause the cluster to scale up or prevent the cluster from scaling down. No new nodes are added to run the pods, and nodes running these pods might be deleted to free resources.
			</p><p>
				Cluster autoscaling is supported for the platforms that have machine API available on it.
			</p><section class="section" id="cluster-autoscaler-cr_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.10.1. Cluster autoscaler resource definition</h3></div></div></div><p>
					This <code class="literal">ClusterAutoscaler</code> resource definition shows the parameters and sample values for the cluster autoscaler.
				</p><pre class="programlisting language-yaml">apiVersion: "autoscaling.openshift.io/v1"
kind: "ClusterAutoscaler"
metadata:
  name: "default"
spec:
  podPriorityThreshold: -10 <span id="CO49-1"><!--Empty--></span><span class="callout">1</span>
  resourceLimits:
    maxNodesTotal: 24 <span id="CO49-2"><!--Empty--></span><span class="callout">2</span>
    cores:
      min: 8 <span id="CO49-3"><!--Empty--></span><span class="callout">3</span>
      max: 128 <span id="CO49-4"><!--Empty--></span><span class="callout">4</span>
    memory:
      min: 4 <span id="CO49-5"><!--Empty--></span><span class="callout">5</span>
      max: 256 <span id="CO49-6"><!--Empty--></span><span class="callout">6</span>
    gpus:
      - type: nvidia.com/gpu <span id="CO49-7"><!--Empty--></span><span class="callout">7</span>
        min: 0 <span id="CO49-8"><!--Empty--></span><span class="callout">8</span>
        max: 16 <span id="CO49-9"><!--Empty--></span><span class="callout">9</span>
      - type: amd.com/gpu
        min: 0
        max: 4
  logVerbosity: 4 <span id="CO49-10"><!--Empty--></span><span class="callout">10</span>
  scaleDown: <span id="CO49-11"><!--Empty--></span><span class="callout">11</span>
    enabled: true <span id="CO49-12"><!--Empty--></span><span class="callout">12</span>
    delayAfterAdd: 10m <span id="CO49-13"><!--Empty--></span><span class="callout">13</span>
    delayAfterDelete: 5m <span id="CO49-14"><!--Empty--></span><span class="callout">14</span>
    delayAfterFailure: 30s <span id="CO49-15"><!--Empty--></span><span class="callout">15</span>
    unneededTime: 5m <span id="CO49-16"><!--Empty--></span><span class="callout">16</span>
    utilizationThreshold: "0.4" <span id="CO49-17"><!--Empty--></span><span class="callout">17</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO49-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Specify the priority that a pod must exceed to cause the cluster autoscaler to deploy additional nodes. Enter a 32-bit integer value. The <code class="literal">podPriorityThreshold</code> value is compared to the value of the <code class="literal">PriorityClass</code> that you assign to each pod.
						</div></dd><dt><a href="#CO49-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Specify the maximum number of nodes to deploy. This value is the total number of machines that are deployed in your cluster, not just the ones that the autoscaler controls. Ensure that this value is large enough to account for all of your control plane and compute machines and the total number of replicas that you specify in your <code class="literal">MachineAutoscaler</code> resources.
						</div></dd><dt><a href="#CO49-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Specify the minimum number of cores to deploy in the cluster.
						</div></dd><dt><a href="#CO49-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Specify the maximum number of cores to deploy in the cluster.
						</div></dd><dt><a href="#CO49-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Specify the minimum amount of memory, in GiB, in the cluster.
						</div></dd><dt><a href="#CO49-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							Specify the maximum amount of memory, in GiB, in the cluster.
						</div></dd><dt><a href="#CO49-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							Optional: Specify the type of GPU node to deploy. Only <code class="literal">nvidia.com/gpu</code> and <code class="literal">amd.com/gpu</code> are valid types.
						</div></dd><dt><a href="#CO49-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							Specify the minimum number of GPUs to deploy in the cluster.
						</div></dd><dt><a href="#CO49-9"><span class="callout">9</span></a> </dt><dd><div class="para">
							Specify the maximum number of GPUs to deploy in the cluster.
						</div></dd><dt><a href="#CO49-10"><span class="callout">10</span></a> </dt><dd><div class="para">
							Specify the logging verbosity level between <code class="literal">0</code> and <code class="literal">10</code>. The following log level thresholds are provided for guidance:
						</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">1</code>: (Default) Basic information about changes.
								</li><li class="listitem">
									<code class="literal">4</code>: Debug-level verbosity for troubleshooting typical issues.
								</li><li class="listitem">
									<code class="literal">9</code>: Extensive, protocol-level debugging information.
								</li></ul></div><p>
							If you do not specify a value, the default value of <code class="literal">1</code> is used.
						</p></dd><dt><a href="#CO49-11"><span class="callout">11</span></a> </dt><dd><div class="para">
							In this section, you can specify the period to wait for each action by using any valid <a class="link" href="https://golang.org/pkg/time/#ParseDuration">ParseDuration</a> interval, including <code class="literal">ns</code>, <code class="literal">us</code>, <code class="literal">ms</code>, <code class="literal">s</code>, <code class="literal">m</code>, and <code class="literal">h</code>.
						</div></dd><dt><a href="#CO49-12"><span class="callout">12</span></a> </dt><dd><div class="para">
							Specify whether the cluster autoscaler can remove unnecessary nodes.
						</div></dd><dt><a href="#CO49-13"><span class="callout">13</span></a> </dt><dd><div class="para">
							Optional: Specify the period to wait before deleting a node after a node has recently been <span class="emphasis"><em>added</em></span>. If you do not specify a value, the default value of <code class="literal">10m</code> is used.
						</div></dd><dt><a href="#CO49-14"><span class="callout">14</span></a> </dt><dd><div class="para">
							Optional: Specify the period to wait before deleting a node after a node has recently been <span class="emphasis"><em>deleted</em></span>. If you do not specify a value, the default value of <code class="literal">0s</code> is used.
						</div></dd><dt><a href="#CO49-15"><span class="callout">15</span></a> </dt><dd><div class="para">
							Optional: Specify the period to wait before deleting a node after a scale down failure occurred. If you do not specify a value, the default value of <code class="literal">3m</code> is used.
						</div></dd><dt><a href="#CO49-16"><span class="callout">16</span></a> </dt><dd><div class="para">
							Optional: Specify a period of time before an unnecessary node is eligible for deletion. If you do not specify a value, the default value of <code class="literal">10m</code> is used.
						</div></dd><dt><a href="#CO49-17"><span class="callout">17</span></a> </dt><dd><div class="para">
							Optional: Specify the <span class="emphasis"><em>node utilization level</em></span>. Nodes below this utilization level are eligible for deletion. If you do not specify a value, the default value of <code class="literal">10m</code> is used.. The node utilization level is the sum of the requested resources divided by the allocated resources for the node, and must be a value greater than <code class="literal">"0"</code> but less than <code class="literal">"1"</code>. If you do not specify a value, the cluster autoscaler uses a default value of <code class="literal">"0.5"</code>, which corresponds to 50% utilization. This value must be expressed as a string.
						</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When performing a scaling operation, the cluster autoscaler remains within the ranges set in the <code class="literal">ClusterAutoscaler</code> resource definition, such as the minimum and maximum number of cores to deploy or the amount of memory in the cluster. However, the cluster autoscaler does not correct the current values in your cluster to be within those ranges.
					</p><p>
						The minimum and maximum CPUs, memory, and GPU values are determined by calculating those resources on all nodes in the cluster, even if the cluster autoscaler does not manage the nodes. For example, the control plane nodes are considered in the total memory in the cluster, even though the cluster autoscaler does not manage the control plane nodes.
					</p></div></div></section><section class="section" id="ClusterAutoscaler-deploying_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.10.2. Deploying a cluster autoscaler</h3></div></div></div><p>
					To deploy a cluster autoscaler, you create an instance of the <code class="literal">ClusterAutoscaler</code> resource.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Create a YAML file for a <code class="literal">ClusterAutoscaler</code> resource that contains the custom resource definition.
						</li><li class="listitem"><p class="simpara">
							Create the custom resource in the cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;filename&gt;.yaml <span id="CO50-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO50-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal">&lt;filename&gt;</code> is the name of the custom resource file.
								</div></dd></dl></div></li></ol></div></section></section><section class="section" id="machine-autoscaler-about_post-install-cluster-tasks"><div class="titlepage"><div><div><h2 class="title">7.11. About the machine autoscaler</h2></div></div></div><p>
				The machine autoscaler adjusts the number of Machines in the compute machine sets that you deploy in an OpenShift Container Platform cluster. You can scale both the default <code class="literal">worker</code> compute machine set and any other compute machine sets that you create. The machine autoscaler makes more Machines when the cluster runs out of resources to support more deployments. Any changes to the values in <code class="literal">MachineAutoscaler</code> resources, such as the minimum or maximum number of instances, are immediately applied to the compute machine set they target.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You must deploy a machine autoscaler for the cluster autoscaler to scale your machines. The cluster autoscaler uses the annotations on compute machine sets that the machine autoscaler sets to determine the resources that it can scale. If you define a cluster autoscaler without also defining machine autoscalers, the cluster autoscaler will never scale your cluster.
				</p></div></div><section class="section" id="machine-autoscaler-cr_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.11.1. Machine autoscaler resource definition</h3></div></div></div><p>
					This <code class="literal">MachineAutoscaler</code> resource definition shows the parameters and sample values for the machine autoscaler.
				</p><pre class="programlisting language-yaml">apiVersion: "autoscaling.openshift.io/v1beta1"
kind: "MachineAutoscaler"
metadata:
  name: "worker-us-east-1a" <span id="CO51-1"><!--Empty--></span><span class="callout">1</span>
  namespace: "openshift-machine-api"
spec:
  minReplicas: 1 <span id="CO51-2"><!--Empty--></span><span class="callout">2</span>
  maxReplicas: 12 <span id="CO51-3"><!--Empty--></span><span class="callout">3</span>
  scaleTargetRef: <span id="CO51-4"><!--Empty--></span><span class="callout">4</span>
    apiVersion: machine.openshift.io/v1beta1
    kind: MachineSet <span id="CO51-5"><!--Empty--></span><span class="callout">5</span>
    name: worker-us-east-1a <span id="CO51-6"><!--Empty--></span><span class="callout">6</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO51-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Specify the machine autoscaler name. To make it easier to identify which compute machine set this machine autoscaler scales, specify or include the name of the compute machine set to scale. The compute machine set name takes the following form: <code class="literal">&lt;clusterid&gt;-&lt;machineset&gt;-&lt;region&gt;</code>.
						</div></dd><dt><a href="#CO51-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Specify the minimum number machines of the specified type that must remain in the specified zone after the cluster autoscaler initiates cluster scaling. If running in AWS, GCP, Azure, RHOSP, or vSphere, this value can be set to <code class="literal">0</code>. For other providers, do not set this value to <code class="literal">0</code>.
						</div><p>
							You can save on costs by setting this value to <code class="literal">0</code> for use cases such as running expensive or limited-usage hardware that is used for specialized workloads, or by scaling a compute machine set with extra large machines. The cluster autoscaler scales the compute machine set down to zero if the machines are not in use.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Do not set the <code class="literal">spec.minReplicas</code> value to <code class="literal">0</code> for the three compute machine sets that are created during the OpenShift Container Platform installation process for an installer provisioned infrastructure.
							</p></div></div></dd><dt><a href="#CO51-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Specify the maximum number machines of the specified type that the cluster autoscaler can deploy in the specified zone after it initiates cluster scaling. Ensure that the <code class="literal">maxNodesTotal</code> value in the <code class="literal">ClusterAutoscaler</code> resource definition is large enough to allow the machine autoscaler to deploy this number of machines.
						</div></dd><dt><a href="#CO51-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							In this section, provide values that describe the existing compute machine set to scale.
						</div></dd><dt><a href="#CO51-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							The <code class="literal">kind</code> parameter value is always <code class="literal">MachineSet</code>.
						</div></dd><dt><a href="#CO51-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							The <code class="literal">name</code> value must match the name of an existing compute machine set, as shown in the <code class="literal">metadata.name</code> parameter value.
						</div></dd></dl></div></section><section class="section" id="MachineAutoscaler-deploying_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.11.2. Deploying a machine autoscaler</h3></div></div></div><p>
					To deploy a machine autoscaler, you create an instance of the <code class="literal">MachineAutoscaler</code> resource.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Create a YAML file for a <code class="literal">MachineAutoscaler</code> resource that contains the custom resource definition.
						</li><li class="listitem"><p class="simpara">
							Create the custom resource in the cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;filename&gt;.yaml <span id="CO52-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO52-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal">&lt;filename&gt;</code> is the name of the custom resource file.
								</div></dd></dl></div></li></ol></div></section></section><section class="section" id="nodes-clusters-cgroups-2_post-install-cluster-tasks"><div class="titlepage"><div><div><h2 class="title">7.12. Configuring Linux cgroup</h2></div></div></div><p>
				<a class="link" href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html">Linux control group version 1</a> (cgroup v1) is enabled by default. You can enable <a class="link" href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html">Linux control group version 2</a> (cgroup v2) in your cluster by editing the <code class="literal">node.config</code> object. Enabling cgroup v2 in OpenShift Container Platform disables all cgroup version 1 controllers and hierarchies in your cluster.
			</p><p>
				cgroup v2 is the next version of the Linux cgroup API. cgroup v2 offers several improvements over cgroup v1, including a unified hierarchy, safer sub-tree delegation, new features such as <a class="link" href="https://www.kernel.org/doc/html/latest/accounting/psi.html">Pressure Stall Information</a>, and enhanced resource management and isolation.
			</p><p>
				You can change between cgroup v1 and cgroup v2, as needed. For more information, see "Configuring the Linux cgroup on your nodes" in the "Additional resources" of this section.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Currently, disabling CPU load balancing is not supported by cgroup v2. As a result, you might not get the desired behavior from performance profiles if you have cgroup v2 enabled. Enabling cgroup v2 is not recommended if you are using performance profiles.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have a running OpenShift Container Platform cluster that uses version 4.12 or later.
					</li><li class="listitem">
						You are logged in to the cluster as a user with administrative privileges.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Enable cgroup v2 on nodes:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">node.config</code> object:
							</p><pre class="programlisting language-terminal">$ oc edit nodes.config/cluster</pre></li><li class="listitem"><p class="simpara">
								Add <code class="literal">spec.cgroupMode: "v2"</code>:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">node.config</code> object</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Node
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2022-07-08T16:02:51Z"
  generation: 1
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    kind: ClusterVersion
    name: version
    uid: 36282574-bf9f-409e-a6cd-3032939293eb
  resourceVersion: "1865"
  uid: 0c0f7a4c-4307-4187-b591-6155695ac85b
spec:
  cgroupMode: "v2" <span id="CO53-1"><!--Empty--></span><span class="callout">1</span>
...</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO53-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Enables cgroup v2.
									</div></dd></dl></div></li></ol></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Check the machine configs to see that the new machine configs were added:
					</p><pre class="programlisting language-terminal">$ oc get mc</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
00-master                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
00-worker                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
97-master-generated-kubelet                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-worker-generated-kubelet                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-ssh                                                                                 3.2.0             40m
99-worker-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-worker-ssh                                                                                 3.2.0             40m
rendered-master-23d4317815a5f854bd3553d689cfe2e9   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             10s <span id="CO54-1"><!--Empty--></span><span class="callout">1</span>
rendered-master-23e785de7587df95a4b517e0647e5ab7   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
rendered-worker-5d596d9293ca3ea80c896a1191735bb1   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
rendered-worker-dcc7f1b92892d34db74d6832bcc9ccd4   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             10s</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO54-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								New machine configs are created, as expected.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Check that the new <code class="literal">kernelArguments</code> were added to the new machine configs:
					</p><pre class="programlisting language-terminal">$ oc describe mc &lt;name&gt;</pre><div class="formalpara"><p class="title"><strong>Example output for cgroup v2</strong></p><p>
							
<pre class="programlisting language-terminal">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 05-worker-kernelarg-selinuxpermissive
spec:
  kernelArguments:
  - systemd_unified_cgroup_hierarchy=1 <span id="CO55-1"><!--Empty--></span><span class="callout">1</span>
  - cgroup_no_v1="all" <span id="CO55-2"><!--Empty--></span><span class="callout">2</span>
  - psi=1 <span id="CO55-3"><!--Empty--></span><span class="callout">3</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO55-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Enables cgroup v2 in systemd.
							</div></dd><dt><a href="#CO55-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Disables cgroup v1.
							</div></dd><dt><a href="#CO55-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Enables the Linux Pressure Stall Information (PSI) feature.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Check the nodes to see that scheduling on the nodes is disabled. This indicates that the change is being applied:
					</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                                       STATUS                     ROLES    AGE   VERSION
ci-ln-fm1qnwt-72292-99kt6-master-0         Ready,SchedulingDisabled   master   58m   v1.26.0
ci-ln-fm1qnwt-72292-99kt6-master-1         Ready                      master   58m   v1.26.0
ci-ln-fm1qnwt-72292-99kt6-master-2         Ready                      master   58m   v1.26.0
ci-ln-fm1qnwt-72292-99kt6-worker-a-h5gt4   Ready,SchedulingDisabled   worker   48m   v1.26.0
ci-ln-fm1qnwt-72292-99kt6-worker-b-7vtmd   Ready                      worker   48m   v1.26.0
ci-ln-fm1qnwt-72292-99kt6-worker-c-rhzkv   Ready                      worker   48m   v1.26.0</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						After a node returns to the <code class="literal">Ready</code> state, start a debug session for that node:
					</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
						Set <code class="literal">/host</code> as the root directory within the debug shell:
					</p><pre class="programlisting language-terminal">sh-4.4# chroot /host</pre></li><li class="listitem"><p class="simpara">
						Check that the <code class="literal">sys/fs/cgroup/cgroup2fs</code> file is present on your nodes. This file is created by cgroup v2:
					</p><pre class="programlisting language-terminal">$ stat -c %T -f /sys/fs/cgroup</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">cgroup2fs</pre>

						</p></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-cluster-cgroups-2">Configuring the Linux cgroup version on your nodes</a>
					</li></ul></div></section><section class="section" id="post-install-tp-tasks"><div class="titlepage"><div><div><h2 class="title">7.13. Enabling Technology Preview features using FeatureGates</h2></div></div></div><p>
				You can turn on a subset of the current Technology Preview features on for all nodes in the cluster by editing the <code class="literal">FeatureGate</code> custom resource (CR).
			</p><section class="section" id="nodes-cluster-enabling-features-about_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.13.1. Understanding feature gates</h3></div></div></div><p>
					You can use the <code class="literal">FeatureGate</code> custom resource (CR) to enable specific feature sets in your cluster. A feature set is a collection of OpenShift Container Platform features that are not enabled by default.
				</p><p>
					You can activate the following feature set by using the <code class="literal">FeatureGate</code> CR:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							<code class="literal">TechPreviewNoUpgrade</code>. This feature set is a subset of the current Technology Preview features. This feature set allows you to enable these Technology Preview features on test clusters, where you can fully test them, while leaving the features disabled on production clusters.
						</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
								Enabling the <code class="literal">TechPreviewNoUpgrade</code> feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.
							</p></div></div><p class="simpara">
							The following Technology Preview features are enabled by this feature set:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									External cloud providers. Enables support for external cloud providers for clusters on vSphere, AWS, Azure, and GCP. Support for OpenStack is GA. This is an internal feature that most users do not need to interact with. (<code class="literal">ExternalCloudProvider</code>)
								</li><li class="listitem">
									Shared Resources CSI Driver and Build CSI Volumes in OpenShift Builds. Enables the Container Storage Interface (CSI). (<code class="literal">CSIDriverSharedResource</code>)
								</li><li class="listitem">
									CSI volumes. Enables CSI volume support for the OpenShift Container Platform build system. (<code class="literal">BuildCSIVolumes</code>)
								</li><li class="listitem">
									Swap memory on nodes. Enables swap memory use for OpenShift Container Platform workloads on a per-node basis. (<code class="literal">NodeSwap</code>)
								</li><li class="listitem">
									OpenStack Machine API Provider. This gate has no effect and is planned to be removed from this feature set in a future release. (<code class="literal">MachineAPIProviderOpenStack</code>)
								</li><li class="listitem">
									Insights Operator. Enables the Insights Operator, which gathers OpenShift Container Platform configuration data and sends it to Red Hat. (<code class="literal">InsightsConfigAPI</code>)
								</li><li class="listitem">
									Pod topology spread constraints. Enables the <code class="literal">matchLabelKeys</code> parameter for pod topology constraints. The parameter is list of pod label keys to select the pods over which spreading will be calculated. (<code class="literal">MatchLabelKeysInPodTopologySpread</code>)
								</li><li class="listitem">
									Retroactive Default Storage Class. Enables OpenShift Container Platform to retroactively assign the default storage class to PVCs if there was no default storage class when the PVC was created.(<code class="literal">RetroactiveDefaultStorageClass</code>)
								</li><li class="listitem">
									Pod disruption budget (PDB) unhealthy pod eviction policy. Enables support for specifying how unhealthy pods are considered for eviction when using PDBs. (<code class="literal">PDBUnhealthyPodEvictionPolicy</code>)
								</li><li class="listitem">
									Dynamic Resource Allocation API. Enables a new API for requesting and sharing resources between pods and containers. This is an internal feature that most users do not need to interact with. (<code class="literal">DynamicResourceAllocation</code>)
								</li><li class="listitem">
									Pod security admission enforcement. Enables the restricted enforcement mode for pod security admission. Instead of only logging a warning, pods are rejected if they violate pod security standards. (<code class="literal">OpenShiftPodSecurityAdmission</code>)
								</li></ul></div></li></ul></div></section><section class="section" id="nodes-cluster-enabling-features-console_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.13.2. Enabling feature sets using the web console</h3></div></div></div><p>
					You can use the OpenShift Container Platform web console to enable feature sets for all of the nodes in a cluster by editing the <code class="literal">FeatureGate</code> custom resource (CR).
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To enable feature sets:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							In the OpenShift Container Platform web console, switch to the <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Custom Resource Definitions</strong></span> page.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>Custom Resource Definitions</strong></span> page, click <span class="strong strong"><strong>FeatureGate</strong></span>.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>Custom Resource Definition Details</strong></span> page, click the <span class="strong strong"><strong>Instances</strong></span> tab.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>cluster</strong></span> feature gate, then click the <span class="strong strong"><strong>YAML</strong></span> tab.
						</li><li class="listitem"><p class="simpara">
							Edit the <span class="strong strong"><strong>cluster</strong></span> instance to add specific feature sets:
						</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
								Enabling the <code class="literal">TechPreviewNoUpgrade</code> feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.
							</p></div></div><div class="formalpara"><p class="title"><strong>Sample Feature Gate custom resource</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: FeatureGate
metadata:
  name: cluster <span id="CO56-1"><!--Empty--></span><span class="callout">1</span>
# ...
spec:
  featureSet: TechPreviewNoUpgrade <span id="CO56-2"><!--Empty--></span><span class="callout">2</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO56-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the <code class="literal">FeatureGate</code> CR must be <code class="literal">cluster</code>.
								</div></dd><dt><a href="#CO56-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Add the feature set that you want to enable:
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">TechPreviewNoUpgrade</code> enables specific Technology Preview features.
										</li></ul></div></dd></dl></div><p class="simpara">
							After you save the changes, new machine configs are created, the machine config pools are updated, and scheduling on each node is disabled while the change is being applied.
						</p></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						You can verify that the feature gates are enabled by looking at the <code class="literal">kubelet.conf</code> file on a node after the nodes return to the ready state.
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							From the <span class="strong strong"><strong>Administrator</strong></span> perspective in the web console, navigate to <span class="strong strong"><strong>Compute</strong></span> → <span class="strong strong"><strong>Nodes</strong></span>.
						</li><li class="listitem">
							Select a node.
						</li><li class="listitem">
							In the <span class="strong strong"><strong>Node details</strong></span> page, click <span class="strong strong"><strong>Terminal</strong></span>.
						</li><li class="listitem"><p class="simpara">
							In the terminal window, change your root directory to <code class="literal">/host</code>:
						</p><pre class="programlisting language-terminal">sh-4.2# chroot /host</pre></li><li class="listitem"><p class="simpara">
							View the <code class="literal">kubelet.conf</code> file:
						</p><pre class="programlisting language-terminal">sh-4.2# cat /etc/kubernetes/kubelet.conf</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
								
<pre class="programlisting language-terminal"># ...
featureGates:
  InsightsOperatorPullingSCA: true,
  LegacyNodeRoleBehavior: false
# ...</pre>

							</p></div><p class="simpara">
							The features that are listed as <code class="literal">true</code> are enabled on your cluster.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The features listed vary depending upon the OpenShift Container Platform version.
							</p></div></div></li></ol></div></section><section class="section" id="nodes-cluster-enabling-features-cli_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.13.3. Enabling feature sets using the CLI</h3></div></div></div><p>
					You can use the OpenShift CLI (<code class="literal">oc</code>) to enable feature sets for all of the nodes in a cluster by editing the <code class="literal">FeatureGate</code> custom resource (CR).
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To enable feature sets:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">FeatureGate</code> CR named <code class="literal">cluster</code>:
						</p><pre class="programlisting language-terminal">$ oc edit featuregate cluster</pre><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
								Enabling the <code class="literal">TechPreviewNoUpgrade</code> feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.
							</p></div></div><div class="formalpara"><p class="title"><strong>Sample FeatureGate custom resource</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: FeatureGate
metadata:
  name: cluster <span id="CO57-1"><!--Empty--></span><span class="callout">1</span>
# ...
spec:
  featureSet: TechPreviewNoUpgrade <span id="CO57-2"><!--Empty--></span><span class="callout">2</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO57-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the <code class="literal">FeatureGate</code> CR must be <code class="literal">cluster</code>.
								</div></dd><dt><a href="#CO57-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Add the feature set that you want to enable:
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">TechPreviewNoUpgrade</code> enables specific Technology Preview features.
										</li></ul></div></dd></dl></div><p class="simpara">
							After you save the changes, new machine configs are created, the machine config pools are updated, and scheduling on each node is disabled while the change is being applied.
						</p></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						You can verify that the feature gates are enabled by looking at the <code class="literal">kubelet.conf</code> file on a node after the nodes return to the ready state.
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							From the <span class="strong strong"><strong>Administrator</strong></span> perspective in the web console, navigate to <span class="strong strong"><strong>Compute</strong></span> → <span class="strong strong"><strong>Nodes</strong></span>.
						</li><li class="listitem">
							Select a node.
						</li><li class="listitem">
							In the <span class="strong strong"><strong>Node details</strong></span> page, click <span class="strong strong"><strong>Terminal</strong></span>.
						</li><li class="listitem"><p class="simpara">
							In the terminal window, change your root directory to <code class="literal">/host</code>:
						</p><pre class="programlisting language-terminal">sh-4.2# chroot /host</pre></li><li class="listitem"><p class="simpara">
							View the <code class="literal">kubelet.conf</code> file:
						</p><pre class="programlisting language-terminal">sh-4.2# cat /etc/kubernetes/kubelet.conf</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
								
<pre class="programlisting language-terminal"># ...
featureGates:
  InsightsOperatorPullingSCA: true,
  LegacyNodeRoleBehavior: false
# ...</pre>

							</p></div><p class="simpara">
							The features that are listed as <code class="literal">true</code> are enabled on your cluster.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The features listed vary depending upon the OpenShift Container Platform version.
							</p></div></div></li></ol></div></section></section><section class="section" id="post-install-etcd-tasks"><div class="titlepage"><div><div><h2 class="title">7.14. etcd tasks</h2></div></div></div><p>
				Back up etcd, enable or disable etcd encryption, or defragment etcd data.
			</p><section class="section" id="about-etcd_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.14.1. About etcd encryption</h3></div></div></div><p>
					By default, etcd data is not encrypted in OpenShift Container Platform. You can enable etcd encryption for your cluster to provide an additional layer of data security. For example, it can help protect the loss of sensitive data if an etcd backup is exposed to the incorrect parties.
				</p><p>
					When you enable etcd encryption, the following OpenShift API server and Kubernetes API server resources are encrypted:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Secrets
						</li><li class="listitem">
							Config maps
						</li><li class="listitem">
							Routes
						</li><li class="listitem">
							OAuth access tokens
						</li><li class="listitem">
							OAuth authorize tokens
						</li></ul></div><p>
					When you enable etcd encryption, encryption keys are created. You must have these keys to restore from an etcd backup.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Etcd encryption only encrypts values, not keys. Resource types, namespaces, and object names are unencrypted.
					</p><p>
						If etcd encryption is enabled during a backup, the <code class="literal"><span class="emphasis"><em>static_kuberesources_&lt;datetimestamp&gt;.tar.gz</em></span></code> file contains the encryption keys for the etcd snapshot. For security reasons, store this file separately from the etcd snapshot. However, this file is required to restore a previous state of etcd from the respective etcd snapshot.
					</p></div></div></section><section class="section" id="etcd-encryption-types_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.14.2. Supported encryption types</h3></div></div></div><p>
					The following encryption types are supported for encrypting etcd data in OpenShift Container Platform:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">AES-CBC</span></dt><dd>
								Uses AES-CBC with PKCS#7 padding and a 32 byte key to perform the encryption. The encryption keys are rotated weekly.
							</dd><dt><span class="term">AES-GCM</span></dt><dd>
								Uses AES-GCM with a random nonce and a 32 byte key to perform the encryption. The encryption keys are rotated weekly.
							</dd></dl></div></section><section class="section" id="enabling-etcd-encryption_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.14.3. Enabling etcd encryption</h3></div></div></div><p>
					You can enable etcd encryption to encrypt sensitive resources in your cluster.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Do not back up etcd resources until the initial encryption process is completed. If the encryption process is not completed, the backup might be only partially encrypted.
					</p><p>
						After you enable etcd encryption, several changes can occur:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								The etcd encryption might affect the memory consumption of a few resources.
							</li><li class="listitem">
								You might notice a transient affect on backup performance because the leader must serve the backup.
							</li><li class="listitem">
								A disk I/O can affect the node that receives the backup state.
							</li></ul></div></div></div><p>
					You can encrypt the etcd database in either AES-GCM or AES-CBC encryption.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						To migrate your etcd database from one encryption type to the other, you can modify the API server’s <code class="literal">spec.encryption.type</code> field. Migration of the etcd data to the new encryption type occurs automatically.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Modify the <code class="literal">APIServer</code> object:
						</p><pre class="programlisting language-terminal">$ oc edit apiserver</pre></li><li class="listitem"><p class="simpara">
							Set the <code class="literal">spec.encryption.type</code> field to <code class="literal">aesgcm</code> or <code class="literal">aescbc</code>:
						</p><pre class="programlisting language-yaml">spec:
  encryption:
    type: aesgcm <span id="CO58-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO58-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Set to <code class="literal">aesgcm</code> for AES-GCM encryption or <code class="literal">aescbc</code> for AES-CBC encryption.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Save the file to apply the changes.
						</p><p class="simpara">
							The encryption process starts. It can take 20 minutes or longer for this process to complete, depending on the size of the etcd database.
						</p></li><li class="listitem"><p class="simpara">
							Verify that etcd encryption was successful.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Review the <code class="literal">Encrypted</code> status condition for the OpenShift API server to verify that its resources were successfully encrypted:
								</p><pre class="programlisting language-terminal">$ oc get openshiftapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'</pre><p class="simpara">
									The output shows <code class="literal">EncryptionCompleted</code> upon successful encryption:
								</p><pre class="programlisting language-terminal">EncryptionCompleted
All resources encrypted: routes.route.openshift.io</pre><p class="simpara">
									If the output shows <code class="literal">EncryptionInProgress</code>, encryption is still in progress. Wait a few minutes and try again.
								</p></li><li class="listitem"><p class="simpara">
									Review the <code class="literal">Encrypted</code> status condition for the Kubernetes API server to verify that its resources were successfully encrypted:
								</p><pre class="programlisting language-terminal">$ oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'</pre><p class="simpara">
									The output shows <code class="literal">EncryptionCompleted</code> upon successful encryption:
								</p><pre class="programlisting language-terminal">EncryptionCompleted
All resources encrypted: secrets, configmaps</pre><p class="simpara">
									If the output shows <code class="literal">EncryptionInProgress</code>, encryption is still in progress. Wait a few minutes and try again.
								</p></li><li class="listitem"><p class="simpara">
									Review the <code class="literal">Encrypted</code> status condition for the OpenShift OAuth API server to verify that its resources were successfully encrypted:
								</p><pre class="programlisting language-terminal">$ oc get authentication.operator.openshift.io -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'</pre><p class="simpara">
									The output shows <code class="literal">EncryptionCompleted</code> upon successful encryption:
								</p><pre class="programlisting language-terminal">EncryptionCompleted
All resources encrypted: oauthaccesstokens.oauth.openshift.io, oauthauthorizetokens.oauth.openshift.io</pre><p class="simpara">
									If the output shows <code class="literal">EncryptionInProgress</code>, encryption is still in progress. Wait a few minutes and try again.
								</p></li></ol></div></li></ol></div></section><section class="section" id="disabling-etcd-encryption_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.14.4. Disabling etcd encryption</h3></div></div></div><p>
					You can disable encryption of etcd data in your cluster.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Modify the <code class="literal">APIServer</code> object:
						</p><pre class="programlisting language-terminal">$ oc edit apiserver</pre></li><li class="listitem"><p class="simpara">
							Set the <code class="literal">encryption</code> field type to <code class="literal">identity</code>:
						</p><pre class="programlisting language-yaml">spec:
  encryption:
    type: identity <span id="CO59-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO59-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal">identity</code> type is the default value and means that no encryption is performed.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Save the file to apply the changes.
						</p><p class="simpara">
							The decryption process starts. It can take 20 minutes or longer for this process to complete, depending on the size of your cluster.
						</p></li><li class="listitem"><p class="simpara">
							Verify that etcd decryption was successful.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Review the <code class="literal">Encrypted</code> status condition for the OpenShift API server to verify that its resources were successfully decrypted:
								</p><pre class="programlisting language-terminal">$ oc get openshiftapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'</pre><p class="simpara">
									The output shows <code class="literal">DecryptionCompleted</code> upon successful decryption:
								</p><pre class="programlisting language-terminal">DecryptionCompleted
Encryption mode set to identity and everything is decrypted</pre><p class="simpara">
									If the output shows <code class="literal">DecryptionInProgress</code>, decryption is still in progress. Wait a few minutes and try again.
								</p></li><li class="listitem"><p class="simpara">
									Review the <code class="literal">Encrypted</code> status condition for the Kubernetes API server to verify that its resources were successfully decrypted:
								</p><pre class="programlisting language-terminal">$ oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'</pre><p class="simpara">
									The output shows <code class="literal">DecryptionCompleted</code> upon successful decryption:
								</p><pre class="programlisting language-terminal">DecryptionCompleted
Encryption mode set to identity and everything is decrypted</pre><p class="simpara">
									If the output shows <code class="literal">DecryptionInProgress</code>, decryption is still in progress. Wait a few minutes and try again.
								</p></li><li class="listitem"><p class="simpara">
									Review the <code class="literal">Encrypted</code> status condition for the OpenShift OAuth API server to verify that its resources were successfully decrypted:
								</p><pre class="programlisting language-terminal">$ oc get authentication.operator.openshift.io -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'</pre><p class="simpara">
									The output shows <code class="literal">DecryptionCompleted</code> upon successful decryption:
								</p><pre class="programlisting language-terminal">DecryptionCompleted
Encryption mode set to identity and everything is decrypted</pre><p class="simpara">
									If the output shows <code class="literal">DecryptionInProgress</code>, decryption is still in progress. Wait a few minutes and try again.
								</p></li></ol></div></li></ol></div></section><section class="section" id="backing-up-etcd-data_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.14.5. Backing up etcd data</h3></div></div></div><p>
					Follow these steps to back up etcd data by creating an etcd snapshot and backing up the resources for the static pods. This backup can be saved and used at a later time if you need to restore etcd.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Only save a backup from a single control plane host. Do not take a backup from each control plane host in the cluster.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem"><p class="simpara">
							You have checked whether the cluster-wide proxy is enabled.
						</p><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can check whether the proxy is enabled by reviewing the output of <code class="literal">oc get proxy cluster -o yaml</code>. The proxy is enabled if the <code class="literal">httpProxy</code>, <code class="literal">httpsProxy</code>, and <code class="literal">noProxy</code> fields have values set.
						</p></div></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Start a debug session as root for a control plane node:
						</p><pre class="programlisting language-terminal">$ oc debug --as-root node/&lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Change your root directory to <code class="literal">/host</code> in the debug shell:
						</p><pre class="programlisting language-terminal">sh-4.4# chroot /host</pre></li><li class="listitem">
							If the cluster-wide proxy is enabled, be sure that you have exported the <code class="literal">NO_PROXY</code>, <code class="literal">HTTP_PROXY</code>, and <code class="literal">HTTPS_PROXY</code> environment variables.
						</li><li class="listitem"><p class="simpara">
							Run the <code class="literal">cluster-backup.sh</code> script in the debug shell and pass in the location to save the backup to.
						</p><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							The <code class="literal">cluster-backup.sh</code> script is maintained as a component of the etcd Cluster Operator and is a wrapper around the <code class="literal">etcdctl snapshot save</code> command.
						</p></div></div><pre class="programlisting language-terminal">sh-4.4# /usr/local/bin/cluster-backup.sh /home/core/assets/backup</pre><div class="formalpara"><p class="title"><strong>Example script output</strong></p><p>
								
<pre class="programlisting language-terminal">found latest kube-apiserver: /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6
found latest kube-controller-manager: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7
found latest kube-scheduler: /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6
found latest etcd: /etc/kubernetes/static-pod-resources/etcd-pod-3
ede95fe6b88b87ba86a03c15e669fb4aa5bf0991c180d3c6895ce72eaade54a1
etcdctl version: 3.4.14
API version: 3.4
{"level":"info","ts":1624647639.0188997,"caller":"snapshot/v3_snapshot.go:119","msg":"created temporary db file","path":"/home/core/assets/backup/snapshot_2021-06-25_190035.db.part"}
{"level":"info","ts":"2021-06-25T19:00:39.030Z","caller":"clientv3/maintenance.go:200","msg":"opened snapshot stream; downloading"}
{"level":"info","ts":1624647639.0301006,"caller":"snapshot/v3_snapshot.go:127","msg":"fetching snapshot","endpoint":"https://10.0.0.5:2379"}
{"level":"info","ts":"2021-06-25T19:00:40.215Z","caller":"clientv3/maintenance.go:208","msg":"completed snapshot read; closing"}
{"level":"info","ts":1624647640.6032252,"caller":"snapshot/v3_snapshot.go:142","msg":"fetched snapshot","endpoint":"https://10.0.0.5:2379","size":"114 MB","took":1.584090459}
{"level":"info","ts":1624647640.6047094,"caller":"snapshot/v3_snapshot.go:152","msg":"saved","path":"/home/core/assets/backup/snapshot_2021-06-25_190035.db"}
Snapshot saved at /home/core/assets/backup/snapshot_2021-06-25_190035.db
{"hash":3866667823,"revision":31407,"totalKey":12828,"totalSize":114446336}
snapshot db and kube resources are successfully saved to /home/core/assets/backup</pre>

							</p></div><p class="simpara">
							In this example, two files are created in the <code class="literal">/home/core/assets/backup/</code> directory on the control plane host:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">snapshot_&lt;datetimestamp&gt;.db</code>: This file is the etcd snapshot. The <code class="literal">cluster-backup.sh</code> script confirms its validity.
								</li><li class="listitem"><p class="simpara">
									<code class="literal">static_kuberesources_&lt;datetimestamp&gt;.tar.gz</code>: This file contains the resources for the static pods. If etcd encryption is enabled, it also contains the encryption keys for the etcd snapshot.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										If etcd encryption is enabled, it is recommended to store this second file separately from the etcd snapshot for security reasons. However, this file is required to restore from the etcd snapshot.
									</p><p>
										Keep in mind that etcd encryption only encrypts values, not keys. This means that resource types, namespaces, and object names are unencrypted.
									</p></div></div></li></ul></div></li></ol></div></section><section class="section" id="etcd-defrag_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.14.6. Defragmenting etcd data</h3></div></div></div><p>
					For large and dense clusters, etcd can suffer from poor performance if the keyspace grows too large and exceeds the space quota. Periodically maintain and defragment etcd to free up space in the data store. Monitor Prometheus for etcd metrics and defragment it when required; otherwise, etcd can raise a cluster-wide alarm that puts the cluster into a maintenance mode that accepts only key reads and deletes.
				</p><p>
					Monitor these key metrics:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">etcd_server_quota_backend_bytes</code>, which is the current quota limit
						</li><li class="listitem">
							<code class="literal">etcd_mvcc_db_total_size_in_use_in_bytes</code>, which indicates the actual database usage after a history compaction
						</li><li class="listitem">
							<code class="literal">etcd_mvcc_db_total_size_in_bytes</code>, which shows the database size, including free space waiting for defragmentation
						</li></ul></div><p>
					Defragment etcd data to reclaim disk space after events that cause disk fragmentation, such as etcd history compaction.
				</p><p>
					History compaction is performed automatically every five minutes and leaves gaps in the back-end database. This fragmented space is available for use by etcd, but is not available to the host file system. You must defragment etcd to make this space available to the host file system.
				</p><p>
					Defragmentation occurs automatically, but you can also trigger it manually.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Automatic defragmentation is good for most cases, because the etcd operator uses cluster information to determine the most efficient operation for the user.
					</p></div></div><section class="section" id="automatic-defrag-etcd-data_post-install-cluster-tasks"><div class="titlepage"><div><div><h4 class="title">7.14.6.1. Automatic defragmentation</h4></div></div></div><p>
						The etcd Operator automatically defragments disks. No manual intervention is needed.
					</p><p>
						Verify that the defragmentation process is successful by viewing one of these logs:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								etcd logs
							</li><li class="listitem">
								cluster-etcd-operator pod
							</li><li class="listitem">
								operator status error log
							</li></ul></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							Automatic defragmentation can cause leader election failure in various OpenShift core components, such as the Kubernetes controller manager, which triggers a restart of the failing component. The restart is harmless and either triggers failover to the next running instance or the component resumes work again after the restart.
						</p></div></div><div class="formalpara"><p class="title"><strong>Example log output for successful defragmentation</strong></p><p>
							
<pre class="programlisting language-terminal">etcd member has been defragmented: <span class="emphasis"><em>&lt;member_name&gt;</em></span>, memberID: <span class="emphasis"><em>&lt;member_id&gt;</em></span></pre>

						</p></div><div class="formalpara"><p class="title"><strong>Example log output for unsuccessful defragmentation</strong></p><p>
							
<pre class="programlisting language-terminal">failed defrag on member: <span class="emphasis"><em>&lt;member_name&gt;</em></span>, memberID: <span class="emphasis"><em>&lt;member_id&gt;</em></span>: <span class="emphasis"><em>&lt;error_message&gt;</em></span></pre>

						</p></div></section><section class="section" id="manual-defrag-etcd-data_post-install-cluster-tasks"><div class="titlepage"><div><div><h4 class="title">7.14.6.2. Manual defragmentation</h4></div></div></div><p>
						A Prometheus alert indicates when you need to use manual defragmentation. The alert is displayed in two cases:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								When etcd uses more than 50% of its available space for more than 10 minutes
							</li><li class="listitem">
								When etcd is actively using less than 50% of its total database size for more than 10 minutes
							</li></ul></div><p>
						You can also determine whether defragmentation is needed by checking the etcd database size in MB that will be freed by defragmentation with the PromQL expression: <code class="literal">(etcd_mvcc_db_total_size_in_bytes - etcd_mvcc_db_total_size_in_use_in_bytes)/1024/1024</code>
					</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							Defragmenting etcd is a blocking action. The etcd member will not respond until defragmentation is complete. For this reason, wait at least one minute between defragmentation actions on each of the pods to allow the cluster to recover.
						</p></div></div><p>
						Follow this procedure to defragment etcd data on each etcd member.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Determine which etcd member is the leader, because the leader should be defragmented last.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Get the list of etcd pods:
									</p><pre class="programlisting language-terminal">$ oc -n openshift-etcd get pods -l k8s-app=etcd -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">etcd-ip-10-0-159-225.example.redhat.com                3/3     Running     0          175m   10.0.159.225   ip-10-0-159-225.example.redhat.com   &lt;none&gt;           &lt;none&gt;
etcd-ip-10-0-191-37.example.redhat.com                 3/3     Running     0          173m   10.0.191.37    ip-10-0-191-37.example.redhat.com    &lt;none&gt;           &lt;none&gt;
etcd-ip-10-0-199-170.example.redhat.com                3/3     Running     0          176m   10.0.199.170   ip-10-0-199-170.example.redhat.com   &lt;none&gt;           &lt;none&gt;</pre>

										</p></div></li><li class="listitem"><p class="simpara">
										Choose a pod and run the following command to determine which etcd member is the leader:
									</p><pre class="programlisting language-terminal">$ oc rsh -n openshift-etcd etcd-ip-10-0-159-225.example.redhat.com etcdctl endpoint status --cluster -w table</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">Defaulting container name to etcdctl.
Use 'oc describe pod/etcd-ip-10-0-159-225.example.redhat.com -n openshift-etcd' to see all of the containers in this pod.
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|  https://10.0.191.37:2379 | 251cd44483d811c3 |   3.5.9 |  104 MB |     false |      false |         7 |      91624 |              91624 |        |
| https://10.0.159.225:2379 | 264c7c58ecbdabee |   3.5.9 |  104 MB |     false |      false |         7 |      91624 |              91624 |        |
| https://10.0.199.170:2379 | 9ac311f93915cc79 |   3.5.9 |  104 MB |      true |      false |         7 |      91624 |              91624 |        |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</pre>

										</p></div><p class="simpara">
										Based on the <code class="literal">IS LEADER</code> column of this output, the <code class="literal">https://10.0.199.170:2379</code> endpoint is the leader. Matching this endpoint with the output of the previous step, the pod name of the leader is <code class="literal">etcd-ip-10-0-199-170.example.redhat.com</code>.
									</p></li></ol></div></li><li class="listitem"><p class="simpara">
								Defragment an etcd member.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Connect to the running etcd container, passing in the name of a pod that is <span class="emphasis"><em>not</em></span> the leader:
									</p><pre class="programlisting language-terminal">$ oc rsh -n openshift-etcd etcd-ip-10-0-159-225.example.redhat.com</pre></li><li class="listitem"><p class="simpara">
										Unset the <code class="literal">ETCDCTL_ENDPOINTS</code> environment variable:
									</p><pre class="programlisting language-terminal">sh-4.4# unset ETCDCTL_ENDPOINTS</pre></li><li class="listitem"><p class="simpara">
										Defragment the etcd member:
									</p><pre class="programlisting language-terminal">sh-4.4# etcdctl --command-timeout=30s --endpoints=https://localhost:2379 defrag</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">Finished defragmenting etcd member[https://localhost:2379]</pre>

										</p></div><p class="simpara">
										If a timeout error occurs, increase the value for <code class="literal">--command-timeout</code> until the command succeeds.
									</p></li><li class="listitem"><p class="simpara">
										Verify that the database size was reduced:
									</p><pre class="programlisting language-terminal">sh-4.4# etcdctl endpoint status -w table --cluster</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|  https://10.0.191.37:2379 | 251cd44483d811c3 |   3.5.9 |  104 MB |     false |      false |         7 |      91624 |              91624 |        |
| https://10.0.159.225:2379 | 264c7c58ecbdabee |   3.5.9 |   41 MB |     false |      false |         7 |      91624 |              91624 |        | <span id="CO60-1"><!--Empty--></span><span class="callout">1</span>
| https://10.0.199.170:2379 | 9ac311f93915cc79 |   3.5.9 |  104 MB |      true |      false |         7 |      91624 |              91624 |        |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</pre>

										</p></div><p class="simpara">
										This example shows that the database size for this etcd member is now 41 MB as opposed to the starting size of 104 MB.
									</p></li><li class="listitem"><p class="simpara">
										Repeat these steps to connect to each of the other etcd members and defragment them. Always defragment the leader last.
									</p><p class="simpara">
										Wait at least one minute between defragmentation actions to allow the etcd pod to recover. Until the etcd pod recovers, the etcd member will not respond.
									</p></li></ol></div></li><li class="listitem"><p class="simpara">
								If any <code class="literal">NOSPACE</code> alarms were triggered due to the space quota being exceeded, clear them.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Check if there are any <code class="literal">NOSPACE</code> alarms:
									</p><pre class="programlisting language-terminal">sh-4.4# etcdctl alarm list</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">memberID:12345678912345678912 alarm:NOSPACE</pre>

										</p></div></li><li class="listitem"><p class="simpara">
										Clear the alarms:
									</p><pre class="programlisting language-terminal">sh-4.4# etcdctl alarm disarm</pre></li></ol></div></li></ol></div></section></section><section class="section" id="dr-scenario-2-restoring-cluster-state_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.14.7. Restoring to a previous cluster state</h3></div></div></div><p>
					You can use a saved etcd backup to restore a previous cluster state or restore a cluster that has lost the majority of control plane hosts.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If your cluster uses a control plane machine set, see "Troubleshooting the control plane machine set" for a more simple etcd recovery procedure.
					</p></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						When you restore your cluster, you must use an etcd backup that was taken from the same z-stream release. For example, an OpenShift Container Platform 4.7.2 cluster must use an etcd backup that was taken from 4.7.2.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal">cluster-admin</code> role through a certificate-based <code class="literal">kubeconfig</code> file, like the one that was used during installation.
						</li><li class="listitem">
							A healthy control plane host to use as the recovery host.
						</li><li class="listitem">
							SSH access to control plane hosts.
						</li><li class="listitem">
							A backup directory containing both the etcd snapshot and the resources for the static pods, which were from the same backup. The file names in the directory must be in the following formats: <code class="literal">snapshot_&lt;datetimestamp&gt;.db</code> and <code class="literal">static_kuberesources_&lt;datetimestamp&gt;.tar.gz</code>.
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						For non-recovery control plane nodes, it is not required to establish SSH connectivity or to stop the static pods. You can delete and recreate other non-recovery, control plane machines, one by one.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Select a control plane host to use as the recovery host. This is the host that you will run the restore operation on.
						</li><li class="listitem"><p class="simpara">
							Establish SSH connectivity to each of the control plane nodes, including the recovery host.
						</p><p class="simpara">
							The Kubernetes API server becomes inaccessible after the restore process starts, so you cannot access the control plane nodes. For this reason, it is recommended to establish SSH connectivity to each control plane host in a separate terminal.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								If you do not complete this step, you will not be able to access the control plane hosts to complete the restore procedure, and you will be unable to recover your cluster from this state.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Copy the etcd backup directory to the recovery control plane host.
						</p><p class="simpara">
							This procedure assumes that you copied the <code class="literal">backup</code> directory containing the etcd snapshot and the resources for the static pods to the <code class="literal">/home/core/</code> directory of your recovery control plane host.
						</p></li><li class="listitem"><p class="simpara">
							Stop the static pods on any other control plane nodes.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								You do not need to stop the static pods on the recovery host.
							</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Access a control plane host that is not the recovery host.
								</li><li class="listitem"><p class="simpara">
									Move the existing etcd pod file out of the kubelet manifest directory:
								</p><pre class="programlisting language-terminal">$ sudo mv /etc/kubernetes/manifests/etcd-pod.yaml /tmp</pre></li><li class="listitem"><p class="simpara">
									Verify that the etcd pods are stopped.
								</p><pre class="programlisting language-terminal">$ sudo crictl ps | grep etcd | egrep -v "operator|etcd-guard"</pre><p class="simpara">
									The output of this command should be empty. If it is not empty, wait a few minutes and check again.
								</p></li><li class="listitem"><p class="simpara">
									Move the existing Kubernetes API server pod file out of the kubelet manifest directory:
								</p><pre class="programlisting language-terminal">$ sudo mv /etc/kubernetes/manifests/kube-apiserver-pod.yaml /tmp</pre></li><li class="listitem"><p class="simpara">
									Verify that the Kubernetes API server pods are stopped.
								</p><pre class="programlisting language-terminal">$ sudo crictl ps | grep kube-apiserver | egrep -v "operator|guard"</pre><p class="simpara">
									The output of this command should be empty. If it is not empty, wait a few minutes and check again.
								</p></li><li class="listitem"><p class="simpara">
									Move the etcd data directory to a different location:
								</p><pre class="programlisting language-terminal">$ sudo mv /var/lib/etcd/ /tmp</pre></li><li class="listitem">
									Repeat this step on each of the other control plane hosts that is not the recovery host.
								</li></ol></div></li><li class="listitem">
							Access the recovery control plane host.
						</li><li class="listitem"><p class="simpara">
							If the cluster-wide proxy is enabled, be sure that you have exported the <code class="literal">NO_PROXY</code>, <code class="literal">HTTP_PROXY</code>, and <code class="literal">HTTPS_PROXY</code> environment variables.
						</p><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can check whether the proxy is enabled by reviewing the output of <code class="literal">oc get proxy cluster -o yaml</code>. The proxy is enabled if the <code class="literal">httpProxy</code>, <code class="literal">httpsProxy</code>, and <code class="literal">noProxy</code> fields have values set.
						</p></div></div></li><li class="listitem"><p class="simpara">
							Run the restore script on the recovery control plane host and pass in the path to the etcd backup directory:
						</p><pre class="programlisting language-terminal">$ sudo -E /usr/local/bin/cluster-restore.sh /home/core/backup</pre><div class="formalpara"><p class="title"><strong>Example script output</strong></p><p>
								
<pre class="programlisting language-terminal">...stopping kube-scheduler-pod.yaml
...stopping kube-controller-manager-pod.yaml
...stopping etcd-pod.yaml
...stopping kube-apiserver-pod.yaml
Waiting for container etcd to stop
.complete
Waiting for container etcdctl to stop
.............................complete
Waiting for container etcd-metrics to stop
complete
Waiting for container kube-controller-manager to stop
complete
Waiting for container kube-apiserver to stop
..........................................................................................complete
Waiting for container kube-scheduler to stop
complete
Moving etcd data-dir /var/lib/etcd/member to /var/lib/etcd-backup
starting restore-etcd static pod
starting kube-apiserver-pod.yaml
static-pod-resources/kube-apiserver-pod-7/kube-apiserver-pod.yaml
starting kube-controller-manager-pod.yaml
static-pod-resources/kube-controller-manager-pod-7/kube-controller-manager-pod.yaml
starting kube-scheduler-pod.yaml
static-pod-resources/kube-scheduler-pod-8/kube-scheduler-pod.yaml</pre>

							</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The restore process can cause nodes to enter the <code class="literal">NotReady</code> state if the node certificates were updated after the last etcd backup.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Check the nodes to ensure they are in the <code class="literal">Ready</code> state.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Run the following command:
								</p><pre class="programlisting language-terminal">$ oc get nodes -w</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                STATUS  ROLES          AGE     VERSION
host-172-25-75-28   Ready   master         3d20h   v1.26.0
host-172-25-75-38   Ready   infra,worker   3d20h   v1.26.0
host-172-25-75-40   Ready   master         3d20h   v1.26.0
host-172-25-75-65   Ready   master         3d20h   v1.26.0
host-172-25-75-74   Ready   infra,worker   3d20h   v1.26.0
host-172-25-75-79   Ready   worker         3d20h   v1.26.0
host-172-25-75-86   Ready   worker         3d20h   v1.26.0
host-172-25-75-98   Ready   infra,worker   3d20h   v1.26.0</pre>

									</p></div><p class="simpara">
									It can take several minutes for all nodes to report their state.
								</p></li><li class="listitem"><p class="simpara">
									If any nodes are in the <code class="literal">NotReady</code> state, log in to the nodes and remove all of the PEM files from the <code class="literal">/var/lib/kubelet/pki</code> directory on each node. You can SSH into the nodes or use the terminal window in the web console.
								</p><pre class="programlisting language-terminal">$  ssh -i &lt;ssh-key-path&gt; core@&lt;master-hostname&gt;</pre><div class="formalpara"><p class="title"><strong>Sample <code class="literal">pki</code> directory</strong></p><p>
										
<pre class="programlisting language-terminal">sh-4.4# pwd
/var/lib/kubelet/pki
sh-4.4# ls
kubelet-client-2022-04-28-11-24-09.pem  kubelet-server-2022-04-28-11-24-15.pem
kubelet-client-current.pem              kubelet-server-current.pem</pre>

									</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Restart the kubelet service on all control plane hosts.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									From the recovery host, run the following command:
								</p><pre class="programlisting language-terminal">$ sudo systemctl restart kubelet.service</pre></li><li class="listitem">
									Repeat this step on all other control plane hosts.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Approve the pending CSRs:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Clusters with no worker nodes, such as single-node clusters or clusters consisting of three schedulable control plane nodes, will not have any pending CSRs to approve. You can skip all the commands listed in this step.
							</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Get the list of current CSRs:
								</p><pre class="programlisting language-terminal">$ oc get csr</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="screen">NAME        AGE    SIGNERNAME                                    REQUESTOR                                                                   CONDITION
csr-2s94x   8m3s   kubernetes.io/kubelet-serving                 system:node:&lt;node_name&gt;                                                     Pending <span id="CO60-2"><!--Empty--></span><span class="callout">1</span>
csr-4bd6t   8m3s   kubernetes.io/kubelet-serving                 system:node:&lt;node_name&gt;                                                     Pending <span id="CO60-3"><!--Empty--></span><span class="callout">2</span>
csr-4hl85   13m    kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending <span id="CO60-4"><!--Empty--></span><span class="callout">3</span>
csr-zhhhp   3m8s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending <span id="CO60-5"><!--Empty--></span><span class="callout">4</span>
...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO60-1"><span class="callout">1</span></a> <a href="#CO60-2"><span class="callout">1</span></a> <a href="#CO60-3"><span class="callout">2</span></a> </dt><dd><div class="para">
											A pending kubelet service CSR (for user-provisioned installations).
										</div></dd><dt><a href="#CO60-4"><span class="callout">3</span></a> <a href="#CO60-5"><span class="callout">4</span></a> </dt><dd><div class="para">
											A pending <code class="literal">node-bootstrapper</code> CSR.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Review the details of a CSR to verify that it is valid:
								</p><pre class="programlisting language-terminal">$ oc describe csr &lt;csr_name&gt; <span id="CO61-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO61-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											<code class="literal">&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Approve each valid <code class="literal">node-bootstrapper</code> CSR:
								</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt;</pre></li><li class="listitem"><p class="simpara">
									For user-provisioned installations, approve each valid kubelet service CSR:
								</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Verify that the single member control plane has started successfully.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									From the recovery host, verify that the etcd container is running.
								</p><pre class="programlisting language-terminal">$ sudo crictl ps | grep etcd | egrep -v "operator|etcd-guard"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">3ad41b7908e32       36f86e2eeaaffe662df0d21041eb22b8198e0e58abeeae8c743c3e6e977e8009                                                         About a minute ago   Running             etcd                                          0                   7c05f8af362f0</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									From the recovery host, verify that the etcd pod is running.
								</p><pre class="programlisting language-terminal">$ oc -n openshift-etcd get pods -l k8s-app=etcd</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                             READY   STATUS      RESTARTS   AGE
etcd-ip-10-0-143-125.ec2.internal                1/1     Running     1          2m47s</pre>

									</p></div><p class="simpara">
									If the status is <code class="literal">Pending</code>, or the output lists more than one running etcd pod, wait a few minutes and check again.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							If you are using the <code class="literal">OVNKubernetes</code> network plugin, delete the node objects that are associated with control plane hosts that are not the recovery control plane host.
						</p><pre class="programlisting language-terminal">$ oc delete node &lt;non-recovery-controlplane-host-1&gt; &lt;non-recovery-controlplane-host-2&gt;</pre></li><li class="listitem"><p class="simpara">
							Verify that the Cluster Network Operator (CNO) redeploys the OVN-Kubernetes control plane and that it no longer references the non-recovery controller IP addresses. To verify this result, regularly check the output of the following command. Wait until it returns an empty result before you proceed to restart the Open Virtual Network (OVN) Kubernetes pods on all of the hosts in the next step.
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ovn-kubernetes get ds/ovnkube-master -o yaml | grep -E '&lt;non-recovery_controller_ip_1&gt;|&lt;non-recovery_controller_ip_2&gt;'</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								It can take at least 5-10 minutes for the OVN-Kubernetes control plane to be redeployed and the previous command to return empty output.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Restart the Open Virtual Network (OVN) Kubernetes pods on all the hosts.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Validating and mutating admission webhooks can reject pods. If you add any additional webhooks with the <code class="literal">failurePolicy</code> set to <code class="literal">Fail</code>, then they can reject pods and the restoration process can fail. You can avoid this by saving and deleting webhooks while restoring the cluster state. After the cluster state is restored successfully, you can enable the webhooks again.
							</p><p>
								Alternatively, you can temporarily set the <code class="literal">failurePolicy</code> to <code class="literal">Ignore</code> while restoring the cluster state. After the cluster state is restored successfully, you can set the <code class="literal">failurePolicy</code> to <code class="literal">Fail</code>.
							</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Remove the northbound database (nbdb) and southbound database (sbdb). Access the recovery host and the remaining control plane nodes by using Secure Shell (SSH) and run the following command:
								</p><pre class="programlisting language-terminal">$ sudo rm -f /var/lib/ovn/etc/*.db</pre></li><li class="listitem"><p class="simpara">
									Delete all OVN-Kubernetes control plane pods by running the following command:
								</p><pre class="programlisting language-terminal">$ oc delete pods -l app=ovnkube-master -n openshift-ovn-kubernetes</pre></li><li class="listitem"><p class="simpara">
									Ensure that any OVN-Kubernetes control plane pods are deployed again and are in a <code class="literal">Running</code> state by running the following command:
								</p><pre class="programlisting language-terminal">$ oc get pods -l app=ovnkube-master -n openshift-ovn-kubernetes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                   READY   STATUS    RESTARTS   AGE
ovnkube-master-nb24h   4/4     Running   0          48s</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Delete all <code class="literal">ovnkube-node</code> pods by running the following command:
								</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-ovn-kubernetes -o name | grep ovnkube-node | while read p ; do oc delete $p -n openshift-ovn-kubernetes ; done</pre></li><li class="listitem"><p class="simpara">
									Ensure that all the <code class="literal">ovnkube-node</code> pods are deployed again and are in a <code class="literal">Running</code> state by running the following command:
								</p><pre class="programlisting language-terminal">$ oc get  pods -n openshift-ovn-kubernetes | grep ovnkube-node</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Delete and re-create other non-recovery, control plane machines, one by one. After the machines are re-created, a new revision is forced and etcd automatically scales up.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									If you use a user-provisioned bare metal installation, you can re-create a control plane machine by using the same method that you used to originally create it. For more information, see "Installing a user-provisioned cluster on bare metal".
								</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
										Do not delete and re-create the machine for the recovery host.
									</p></div></div></li><li class="listitem"><p class="simpara">
									If you are running installer-provisioned infrastructure, or you used the Machine API to create your machines, follow these steps:
								</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
										Do not delete and re-create the machine for the recovery host.
									</p><p>
										For bare metal installations on installer-provisioned infrastructure, control plane machines are not re-created. For more information, see "Replacing a bare-metal control plane node".
									</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
											Obtain the machine for one of the lost control plane hosts.
										</p><p class="simpara">
											In a terminal that has access to the cluster as a cluster-admin user, run the following command:
										</p><pre class="programlisting language-terminal">$ oc get machines -n openshift-machine-api -o wide</pre><p class="simpara">
											Example output:
										</p><pre class="programlisting language-terminal">NAME                                        PHASE     TYPE        REGION      ZONE         AGE     NODE                           PROVIDERID                              STATE
clustername-8qw5l-master-0                  Running   m4.xlarge   us-east-1   us-east-1a   3h37m   ip-10-0-131-183.ec2.internal   aws:///us-east-1a/i-0ec2782f8287dfb7e   stopped <span id="CO62-1"><!--Empty--></span><span class="callout">1</span>
clustername-8qw5l-master-1                  Running   m4.xlarge   us-east-1   us-east-1b   3h37m   ip-10-0-143-125.ec2.internal   aws:///us-east-1b/i-096c349b700a19631   running
clustername-8qw5l-master-2                  Running   m4.xlarge   us-east-1   us-east-1c   3h37m   ip-10-0-154-194.ec2.internal    aws:///us-east-1c/i-02626f1dba9ed5bba  running
clustername-8qw5l-worker-us-east-1a-wbtgd   Running   m4.large    us-east-1   us-east-1a   3h28m   ip-10-0-129-226.ec2.internal   aws:///us-east-1a/i-010ef6279b4662ced   running
clustername-8qw5l-worker-us-east-1b-lrdxb   Running   m4.large    us-east-1   us-east-1b   3h28m   ip-10-0-144-248.ec2.internal   aws:///us-east-1b/i-0cb45ac45a166173b   running
clustername-8qw5l-worker-us-east-1c-pkg26   Running   m4.large    us-east-1   us-east-1c   3h28m   ip-10-0-170-181.ec2.internal   aws:///us-east-1c/i-06861c00007751b0a   running</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO62-1"><span class="callout">1</span></a> </dt><dd><div class="para">
													This is the control plane machine for the lost control plane host, <code class="literal">ip-10-0-131-183.ec2.internal</code>.
												</div></dd></dl></div></li><li class="listitem"><p class="simpara">
											Save the machine configuration to a file on your file system:
										</p><pre class="programlisting language-terminal">$ oc get machine clustername-8qw5l-master-0 \ <span id="CO63-1"><!--Empty--></span><span class="callout">1</span>
    -n openshift-machine-api \
    -o yaml \
    &gt; new-master-machine.yaml</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO63-1"><span class="callout">1</span></a> </dt><dd><div class="para">
													Specify the name of the control plane machine for the lost control plane host.
												</div></dd></dl></div></li><li class="listitem"><p class="simpara">
											Edit the <code class="literal">new-master-machine.yaml</code> file that was created in the previous step to assign a new name and remove unnecessary fields.
										</p><div class="orderedlist"><ol class="orderedlist" type="i"><li class="listitem"><p class="simpara">
													Remove the entire <code class="literal">status</code> section:
												</p><pre class="programlisting language-terminal">status:
  addresses:
  - address: 10.0.131.183
    type: InternalIP
  - address: ip-10-0-131-183.ec2.internal
    type: InternalDNS
  - address: ip-10-0-131-183.ec2.internal
    type: Hostname
  lastUpdated: "2020-04-20T17:44:29Z"
  nodeRef:
    kind: Node
    name: ip-10-0-131-183.ec2.internal
    uid: acca4411-af0d-4387-b73e-52b2484295ad
  phase: Running
  providerStatus:
    apiVersion: awsproviderconfig.openshift.io/v1beta1
    conditions:
    - lastProbeTime: "2020-04-20T16:53:50Z"
      lastTransitionTime: "2020-04-20T16:53:50Z"
      message: machine successfully created
      reason: MachineCreationSucceeded
      status: "True"
      type: MachineCreation
    instanceId: i-0fdb85790d76d0c3f
    instanceState: stopped
    kind: AWSMachineProviderStatus</pre></li><li class="listitem"><p class="simpara">
													Change the <code class="literal">metadata.name</code> field to a new name.
												</p><p class="simpara">
													It is recommended to keep the same base name as the old machine and change the ending number to the next available number. In this example, <code class="literal">clustername-8qw5l-master-0</code> is changed to <code class="literal">clustername-8qw5l-master-3</code>:
												</p><pre class="programlisting language-terminal">apiVersion: machine.openshift.io/v1beta1
kind: Machine
metadata:
  ...
  name: clustername-8qw5l-master-3
  ...</pre></li><li class="listitem"><p class="simpara">
													Remove the <code class="literal">spec.providerID</code> field:
												</p><pre class="programlisting language-terminal">providerID: aws:///us-east-1a/i-0fdb85790d76d0c3f</pre></li><li class="listitem"><p class="simpara">
													Remove the <code class="literal">metadata.annotations</code> and <code class="literal">metadata.generation</code> fields:
												</p><pre class="programlisting language-terminal">annotations:
  machine.openshift.io/instance-state: running
...
generation: 2</pre></li><li class="listitem"><p class="simpara">
													Remove the <code class="literal">metadata.resourceVersion</code> and <code class="literal">metadata.uid</code> fields:
												</p><pre class="programlisting language-terminal">resourceVersion: "13291"
uid: a282eb70-40a2-4e89-8009-d05dd420d31a</pre></li></ol></div></li><li class="listitem"><p class="simpara">
											Delete the machine of the lost control plane host:
										</p><pre class="programlisting language-terminal">$ oc delete machine -n openshift-machine-api clustername-8qw5l-master-0 <span id="CO64-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO64-1"><span class="callout">1</span></a> </dt><dd><div class="para">
													Specify the name of the control plane machine for the lost control plane host.
												</div></dd></dl></div></li><li class="listitem"><p class="simpara">
											Verify that the machine was deleted:
										</p><pre class="programlisting language-terminal">$ oc get machines -n openshift-machine-api -o wide</pre><p class="simpara">
											Example output:
										</p><pre class="programlisting language-terminal">NAME                                        PHASE     TYPE        REGION      ZONE         AGE     NODE                           PROVIDERID                              STATE
clustername-8qw5l-master-1                  Running   m4.xlarge   us-east-1   us-east-1b   3h37m   ip-10-0-143-125.ec2.internal   aws:///us-east-1b/i-096c349b700a19631   running
clustername-8qw5l-master-2                  Running   m4.xlarge   us-east-1   us-east-1c   3h37m   ip-10-0-154-194.ec2.internal   aws:///us-east-1c/i-02626f1dba9ed5bba  running
clustername-8qw5l-worker-us-east-1a-wbtgd   Running   m4.large    us-east-1   us-east-1a   3h28m   ip-10-0-129-226.ec2.internal   aws:///us-east-1a/i-010ef6279b4662ced   running
clustername-8qw5l-worker-us-east-1b-lrdxb   Running   m4.large    us-east-1   us-east-1b   3h28m   ip-10-0-144-248.ec2.internal   aws:///us-east-1b/i-0cb45ac45a166173b   running
clustername-8qw5l-worker-us-east-1c-pkg26   Running   m4.large    us-east-1   us-east-1c   3h28m   ip-10-0-170-181.ec2.internal   aws:///us-east-1c/i-06861c00007751b0a   running</pre></li><li class="listitem"><p class="simpara">
											Create a machine by using the <code class="literal">new-master-machine.yaml</code> file:
										</p><pre class="programlisting language-terminal">$ oc apply -f new-master-machine.yaml</pre></li><li class="listitem"><p class="simpara">
											Verify that the new machine has been created:
										</p><pre class="programlisting language-terminal">$ oc get machines -n openshift-machine-api -o wide</pre><p class="simpara">
											Example output:
										</p><pre class="programlisting language-terminal">NAME                                        PHASE          TYPE        REGION      ZONE         AGE     NODE                           PROVIDERID                              STATE
clustername-8qw5l-master-1                  Running        m4.xlarge   us-east-1   us-east-1b   3h37m   ip-10-0-143-125.ec2.internal   aws:///us-east-1b/i-096c349b700a19631   running
clustername-8qw5l-master-2                  Running        m4.xlarge   us-east-1   us-east-1c   3h37m   ip-10-0-154-194.ec2.internal    aws:///us-east-1c/i-02626f1dba9ed5bba  running
clustername-8qw5l-master-3                  Provisioning   m4.xlarge   us-east-1   us-east-1a   85s     ip-10-0-173-171.ec2.internal    aws:///us-east-1a/i-015b0888fe17bc2c8  running <span id="CO65-1"><!--Empty--></span><span class="callout">1</span>
clustername-8qw5l-worker-us-east-1a-wbtgd   Running        m4.large    us-east-1   us-east-1a   3h28m   ip-10-0-129-226.ec2.internal   aws:///us-east-1a/i-010ef6279b4662ced   running
clustername-8qw5l-worker-us-east-1b-lrdxb   Running        m4.large    us-east-1   us-east-1b   3h28m   ip-10-0-144-248.ec2.internal   aws:///us-east-1b/i-0cb45ac45a166173b   running
clustername-8qw5l-worker-us-east-1c-pkg26   Running        m4.large    us-east-1   us-east-1c   3h28m   ip-10-0-170-181.ec2.internal   aws:///us-east-1c/i-06861c00007751b0a   running</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO65-1"><span class="callout">1</span></a> </dt><dd><div class="para">
													The new machine, <code class="literal">clustername-8qw5l-master-3</code> is being created and is ready after the phase changes from <code class="literal">Provisioning</code> to <code class="literal">Running</code>.
												</div></dd></dl></div><p class="simpara">
											It might take a few minutes for the new machine to be created. The etcd cluster Operator will automatically sync when the machine or node returns to a healthy state.
										</p></li><li class="listitem">
											Repeat these steps for each lost control plane host that is not the recovery host.
										</li></ol></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Turn off the quorum guard by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": {"useUnsupportedUnsafeNonHANonProductionUnstableEtcd": true}}}'</pre><p class="simpara">
							This command ensures that you can successfully re-create secrets and roll out the static pods.
						</p></li><li class="listitem"><p class="simpara">
							In a separate terminal window within the recovery host, export the recovery <code class="literal">kubeconfig</code> file by running the following command:
						</p><pre class="programlisting language-terminal">$ export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost-recovery.kubeconfig</pre></li><li class="listitem"><p class="simpara">
							Force etcd redeployment.
						</p><p class="simpara">
							In the same terminal window where you exported the recovery <code class="literal">kubeconfig</code> file, run the following command:
						</p><pre class="programlisting language-terminal">$ oc patch etcd cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge <span id="CO66-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO66-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal">forceRedeploymentReason</code> value must be unique, which is why a timestamp is appended.
								</div></dd></dl></div><p class="simpara">
							When the etcd cluster Operator performs a redeployment, the existing nodes are started with new pods similar to the initial bootstrap scale up.
						</p></li><li class="listitem"><p class="simpara">
							Turn the quorum guard back on by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": null}}'</pre></li><li class="listitem"><p class="simpara">
							You can verify that the <code class="literal">unsupportedConfigOverrides</code> section is removed from the object by entering this command:
						</p><pre class="programlisting language-terminal">$ oc get etcd/cluster -oyaml</pre></li><li class="listitem"><p class="simpara">
							Verify all nodes are updated to the latest revision.
						</p><p class="simpara">
							In a terminal that has access to the cluster as a <code class="literal">cluster-admin</code> user, run the following command:
						</p><pre class="programlisting language-terminal">$ oc get etcd -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'</pre><p class="simpara">
							Review the <code class="literal">NodeInstallerProgressing</code> status condition for etcd to verify that all nodes are at the latest revision. The output shows <code class="literal">AllNodesAtLatestRevision</code> upon successful update:
						</p><pre class="programlisting language-terminal">AllNodesAtLatestRevision
3 nodes are at revision 7 <span id="CO67-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO67-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									In this example, the latest revision number is <code class="literal">7</code>.
								</div></dd></dl></div><p class="simpara">
							If the output includes multiple revision numbers, such as <code class="literal">2 nodes are at revision 6; 1 nodes are at revision 7</code>, this means that the update is still in progress. Wait a few minutes and try again.
						</p></li><li class="listitem"><p class="simpara">
							After etcd is redeployed, force new rollouts for the control plane. The Kubernetes API server will reinstall itself on the other nodes because the kubelet is connected to API servers using an internal load balancer.
						</p><p class="simpara">
							In a terminal that has access to the cluster as a <code class="literal">cluster-admin</code> user, run the following commands.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Force a new rollout for the Kubernetes API server:
								</p><pre class="programlisting language-terminal">$ oc patch kubeapiserver cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge</pre><p class="simpara">
									Verify all nodes are updated to the latest revision.
								</p><pre class="programlisting language-terminal">$ oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'</pre><p class="simpara">
									Review the <code class="literal">NodeInstallerProgressing</code> status condition to verify that all nodes are at the latest revision. The output shows <code class="literal">AllNodesAtLatestRevision</code> upon successful update:
								</p><pre class="programlisting language-terminal">AllNodesAtLatestRevision
3 nodes are at revision 7 <span id="CO68-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO68-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											In this example, the latest revision number is <code class="literal">7</code>.
										</div></dd></dl></div><p class="simpara">
									If the output includes multiple revision numbers, such as <code class="literal">2 nodes are at revision 6; 1 nodes are at revision 7</code>, this means that the update is still in progress. Wait a few minutes and try again.
								</p></li><li class="listitem"><p class="simpara">
									Force a new rollout for the Kubernetes controller manager:
								</p><pre class="programlisting language-terminal">$ oc patch kubecontrollermanager cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge</pre><p class="simpara">
									Verify all nodes are updated to the latest revision.
								</p><pre class="programlisting language-terminal">$ oc get kubecontrollermanager -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'</pre><p class="simpara">
									Review the <code class="literal">NodeInstallerProgressing</code> status condition to verify that all nodes are at the latest revision. The output shows <code class="literal">AllNodesAtLatestRevision</code> upon successful update:
								</p><pre class="programlisting language-terminal">AllNodesAtLatestRevision
3 nodes are at revision 7 <span id="CO69-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO69-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											In this example, the latest revision number is <code class="literal">7</code>.
										</div></dd></dl></div><p class="simpara">
									If the output includes multiple revision numbers, such as <code class="literal">2 nodes are at revision 6; 1 nodes are at revision 7</code>, this means that the update is still in progress. Wait a few minutes and try again.
								</p></li><li class="listitem"><p class="simpara">
									Force a new rollout for the Kubernetes scheduler:
								</p><pre class="programlisting language-terminal">$ oc patch kubescheduler cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge</pre><p class="simpara">
									Verify all nodes are updated to the latest revision.
								</p><pre class="programlisting language-terminal">$ oc get kubescheduler -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'</pre><p class="simpara">
									Review the <code class="literal">NodeInstallerProgressing</code> status condition to verify that all nodes are at the latest revision. The output shows <code class="literal">AllNodesAtLatestRevision</code> upon successful update:
								</p><pre class="programlisting language-terminal">AllNodesAtLatestRevision
3 nodes are at revision 7 <span id="CO70-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO70-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											In this example, the latest revision number is <code class="literal">7</code>.
										</div></dd></dl></div><p class="simpara">
									If the output includes multiple revision numbers, such as <code class="literal">2 nodes are at revision 6; 1 nodes are at revision 7</code>, this means that the update is still in progress. Wait a few minutes and try again.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Verify that all control plane hosts have started and joined the cluster.
						</p><p class="simpara">
							In a terminal that has access to the cluster as a <code class="literal">cluster-admin</code> user, run the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-etcd get pods -l k8s-app=etcd</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">etcd-ip-10-0-143-125.ec2.internal                2/2     Running     0          9h
etcd-ip-10-0-154-194.ec2.internal                2/2     Running     0          9h
etcd-ip-10-0-173-171.ec2.internal                2/2     Running     0          9h</pre>

							</p></div></li></ol></div><p>
					To ensure that all workloads return to normal operation following a recovery procedure, restart each pod that stores Kubernetes API information. This includes OpenShift Container Platform components such as routers, Operators, and third-party components.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						On completion of the previous procedural steps, you might need to wait a few minutes for all services to return to their restored state. For example, authentication by using <code class="literal">oc login</code> might not immediately work until the OAuth server pods are restarted.
					</p><p>
						Consider using the <code class="literal">system:admin</code> <code class="literal">kubeconfig</code> file for immediate authentication. This method basis its authentication on SSL/TLS client certificates as against OAuth tokens. You can authenticate with this file by issuing the following command:
					</p><pre class="programlisting language-terminal">$ export KUBECONFIG=&lt;installation_directory&gt;/auth/kubeconfig</pre><p>
						Issue the following command to display your authenticated user name:
					</p><pre class="programlisting language-terminal">$ oc whoami</pre></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-bare-metal">Installing a user-provisioned cluster on bare metal</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#replacing-a-bare-metal-control-plane-node_ipi-install-expanding">Replacing a bare-metal control plane node</a>
						</li></ul></div></section><section class="section" id="dr-scenario-cluster-state-issues_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.14.8. Issues and workarounds for restoring a persistent storage state</h3></div></div></div><p>
					If your OpenShift Container Platform cluster uses persistent storage of any form, a state of the cluster is typically stored outside etcd. It might be an Elasticsearch cluster running in a pod or a database running in a <code class="literal">StatefulSet</code> object. When you restore from an etcd backup, the status of the workloads in OpenShift Container Platform is also restored. However, if the etcd snapshot is old, the status might be invalid or outdated.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The contents of persistent volumes (PVs) are never part of the etcd snapshot. When you restore an OpenShift Container Platform cluster from an etcd snapshot, non-critical workloads might gain access to critical data, or vice-versa.
					</p></div></div><p>
					The following are some example scenarios that produce an out-of-date status:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							MySQL database is running in a pod backed up by a PV object. Restoring OpenShift Container Platform from an etcd snapshot does not bring back the volume on the storage provider, and does not produce a running MySQL pod, despite the pod repeatedly attempting to start. You must manually restore this pod by restoring the volume on the storage provider, and then editing the PV to point to the new volume.
						</li><li class="listitem">
							Pod P1 is using volume A, which is attached to node X. If the etcd snapshot is taken while another pod uses the same volume on node Y, then when the etcd restore is performed, pod P1 might not be able to start correctly due to the volume still being attached to node Y. OpenShift Container Platform is not aware of the attachment, and does not automatically detach it. When this occurs, the volume must be manually detached from node Y so that the volume can attach on node X, and then pod P1 can start.
						</li><li class="listitem">
							Cloud provider or storage provider credentials were updated after the etcd snapshot was taken. This causes any CSI drivers or Operators that depend on the those credentials to not work. You might have to manually update the credentials required by those drivers or Operators.
						</li><li class="listitem"><p class="simpara">
							A device is removed or renamed from OpenShift Container Platform nodes after the etcd snapshot is taken. The Local Storage Operator creates symlinks for each PV that it manages from <code class="literal">/dev/disk/by-id</code> or <code class="literal">/dev</code> directories. This situation might cause the local PVs to refer to devices that no longer exist.
						</p><p class="simpara">
							To fix this problem, an administrator must:
						</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
									Manually remove the PVs with invalid devices.
								</li><li class="listitem">
									Remove symlinks from respective nodes.
								</li><li class="listitem">
									Delete <code class="literal">LocalVolume</code> or <code class="literal">LocalVolumeSet</code> objects (see <span class="emphasis"><em>Storage</em></span> → <span class="emphasis"><em>Configuring persistent storage</em></span> → <span class="emphasis"><em>Persistent storage using local volumes</em></span> → <span class="emphasis"><em>Deleting the Local Storage Operator Resources</em></span>).
								</li></ol></div></li></ul></div></section></section><section class="section" id="post-install-pod-disruption-budgets"><div class="titlepage"><div><div><h2 class="title">7.15. Pod disruption budgets</h2></div></div></div><p>
				Understand and configure pod disruption budgets.
			</p><section class="section" id="nodes-pods-pod-distruption-about_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.15.1. Understanding how to use pod disruption budgets to specify the number of pods that must be up</h3></div></div></div><p>
					A <span class="emphasis"><em>pod disruption budget</em></span> allows the specification of safety constraints on pods during operations, such as draining a node for maintenance.
				</p><p>
					<code class="literal">PodDisruptionBudget</code> is an API object that specifies the minimum number or percentage of replicas that must be up at a time. Setting these in projects can be helpful during node maintenance (such as scaling a cluster down or a cluster upgrade) and is only honored on voluntary evictions (not on node failures).
				</p><p>
					A <code class="literal">PodDisruptionBudget</code> object’s configuration consists of the following key parts:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							A label selector, which is a label query over a set of pods.
						</li><li class="listitem"><p class="simpara">
							An availability level, which specifies the minimum number of pods that must be available simultaneously, either:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<code class="literal">minAvailable</code> is the number of pods must always be available, even during a disruption.
								</li><li class="listitem">
									<code class="literal">maxUnavailable</code> is the number of pods can be unavailable during a disruption.
								</li></ul></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						<code class="literal">Available</code> refers to the number of pods that has condition <code class="literal">Ready=True</code>. <code class="literal">Ready=True</code> refers to the pod that is able to serve requests and should be added to the load balancing pools of all matching services.
					</p><p>
						A <code class="literal">maxUnavailable</code> of <code class="literal">0%</code> or <code class="literal">0</code> or a <code class="literal">minAvailable</code> of <code class="literal">100%</code> or equal to the number of replicas is permitted but can block nodes from being drained.
					</p></div></div><p>
					You can check for pod disruption budgets across all projects with the following:
				</p><pre class="programlisting language-terminal">$ oc get poddisruptionbudget --all-namespaces</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">NAMESPACE                              NAME                                    MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
openshift-apiserver                    openshift-apiserver-pdb                 N/A             1                 1                     121m
openshift-cloud-controller-manager     aws-cloud-controller-manager            1               N/A               1                     125m
openshift-cloud-credential-operator    pod-identity-webhook                    1               N/A               1                     117m
openshift-cluster-csi-drivers          aws-ebs-csi-driver-controller-pdb       N/A             1                 1                     121m
openshift-cluster-storage-operator     csi-snapshot-controller-pdb             N/A             1                 1                     122m
openshift-cluster-storage-operator     csi-snapshot-webhook-pdb                N/A             1                 1                     122m
openshift-console                      console                                 N/A             1                 1                     116m
#...</pre>

					</p></div><p>
					The <code class="literal">PodDisruptionBudget</code> is considered healthy when there are at least <code class="literal">minAvailable</code> pods running in the system. Every pod above that limit can be evicted.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Depending on your pod priority and preemption settings, lower-priority pods might be removed despite their pod disruption budget requirements.
					</p></div></div></section><section class="section" id="nodes-pods-pod-disruption-configuring_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.15.2. Specifying the number of pods that must be up with pod disruption budgets</h3></div></div></div><p>
					You can use a <code class="literal">PodDisruptionBudget</code> object to specify the minimum number or percentage of replicas that must be up at a time.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To configure a pod disruption budget:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a YAML file with the an object definition similar to the following:
						</p><pre class="programlisting language-yaml">apiVersion: policy/v1 <span id="CO71-1"><!--Empty--></span><span class="callout">1</span>
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  minAvailable: 2  <span id="CO71-2"><!--Empty--></span><span class="callout">2</span>
  selector:  <span id="CO71-3"><!--Empty--></span><span class="callout">3</span>
    matchLabels:
      name: my-pod</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO71-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal">PodDisruptionBudget</code> is part of the <code class="literal">policy/v1</code> API group.
								</div></dd><dt><a href="#CO71-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The minimum number of pods that must be available simultaneously. This can be either an integer or a string specifying a percentage, for example, <code class="literal">20%</code>.
								</div></dd><dt><a href="#CO71-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									A label query over a set of resources. The result of <code class="literal">matchLabels</code> and <code class="literal">matchExpressions</code> are logically conjoined. Leave this parameter blank, for example <code class="literal">selector {}</code>, to select all pods in the project.
								</div></dd></dl></div><p class="simpara">
							Or:
						</p><pre class="programlisting language-yaml">apiVersion: policy/v1 <span id="CO72-1"><!--Empty--></span><span class="callout">1</span>
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  maxUnavailable: 25% <span id="CO72-2"><!--Empty--></span><span class="callout">2</span>
  selector: <span id="CO72-3"><!--Empty--></span><span class="callout">3</span>
    matchLabels:
      name: my-pod</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO72-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal">PodDisruptionBudget</code> is part of the <code class="literal">policy/v1</code> API group.
								</div></dd><dt><a href="#CO72-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The maximum number of pods that can be unavailable simultaneously. This can be either an integer or a string specifying a percentage, for example, <code class="literal">20%</code>.
								</div></dd><dt><a href="#CO72-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									A label query over a set of resources. The result of <code class="literal">matchLabels</code> and <code class="literal">matchExpressions</code> are logically conjoined. Leave this parameter blank, for example <code class="literal">selector {}</code>, to select all pods in the project.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Run the following command to add the object to project:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;/path/to/file&gt; -n &lt;project_name&gt;</pre></li></ol></div></section><section class="section" id="pod-disruption-eviction-policy_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.15.3. Specifying the eviction policy for unhealthy pods</h3></div></div></div><p>
					When you use pod disruption budgets (PDBs) to specify how many pods must be available simultaneously, you can also define the criteria for how unhealthy pods are considered for eviction.
				</p><p>
					You can choose one of the following policies:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">IfHealthyBudget</span></dt><dd>
								Running pods that are not yet healthy can be evicted only if the guarded application is not disrupted.
							</dd><dt><span class="term">AlwaysAllow</span></dt><dd>
								Running pods that are not yet healthy can be evicted regardless of whether the criteria in the pod disruption budget is met. This policy can help evict malfunctioning applications, such as ones with pods stuck in the <code class="literal">CrashLoopBackOff</code> state or failing to report the <code class="literal">Ready</code> status.
							</dd></dl></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Specifying the unhealthy pod eviction policy for pod disruption budgets is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
					</p><p>
						For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
					</p></div></div><p>
					To use this Technology Preview feature, you must have enabled the <code class="literal">TechPreviewNoUpgrade</code> feature set.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Enabling the <code class="literal">TechPreviewNoUpgrade</code> feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a YAML file that defines a <code class="literal">PodDisruptionBudget</code> object and specify the unhealthy pod eviction policy:
						</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">pod-disruption-budget.yaml</code> file</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      name: my-pod
  unhealthyPodEvictionPolicy: AlwaysAllow <span id="CO73-1"><!--Empty--></span><span class="callout">1</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO73-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Choose either <code class="literal">IfHealthyBudget</code> or <code class="literal">AlwaysAllow</code> as the unhealthy pod eviction policy. The default is <code class="literal">IfHealthyBudget</code> when the <code class="literal">unhealthyPodEvictionPolicy</code> field is empty.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">PodDisruptionBudget</code> object by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f pod-disruption-budget.yaml</pre></li></ol></div><p>
					With a PDB that has the <code class="literal">AlwaysAllow</code> unhealthy pod eviction policy set, you can now drain nodes and evict the pods for a malfunctioning application guarded by this PDB.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-cluster-enabling">Enabling features using feature gates</a>
						</li><li class="listitem">
							<a class="link" href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy">Unhealthy Pod Eviction Policy</a> in the Kubernetes documentation
						</li></ul></div></section></section><section class="section" id="post-install-rotate-remove-cloud-creds"><div class="titlepage"><div><div><h2 class="title">7.16. Rotating or removing cloud provider credentials</h2></div></div></div><p>
				After installing OpenShift Container Platform, some organizations require the rotation or removal of the cloud provider credentials that were used during the initial installation.
			</p><p>
				To allow the cluster to use the new credentials, you must update the secrets that the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#cloud-credential-operator_cluster-operators-ref">Cloud Credential Operator (CCO)</a> uses to manage cloud provider credentials.
			</p><section class="section" id="ccoctl-rotate-remove-cloud-creds"><div class="titlepage"><div><div><h3 class="title">7.16.1. Rotating cloud provider credentials with the Cloud Credential Operator utility</h3></div></div></div><p>
					The Cloud Credential Operator (CCO) utility <code class="literal">ccoctl</code> supports updating secrets for clusters installed on IBM Cloud.
				</p><section class="section" id="refreshing-service-ids-ibm-cloud_post-install-cluster-tasks"><div class="titlepage"><div><div><h4 class="title">7.16.1.1. Rotating API keys</h4></div></div></div><p>
						You can rotate API keys for your existing service IDs and update the corresponding secrets.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have configured the <code class="literal">ccoctl</code> binary.
							</li><li class="listitem">
								You have existing service IDs in a live OpenShift Container Platform cluster installed.
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Use the <code class="literal">ccoctl</code> utility to rotate your API keys for the service IDs and update the secrets:
							</p><pre class="programlisting language-terminal">$ ccoctl &lt;provider_name&gt; refresh-keys \ <span id="CO74-1"><!--Empty--></span><span class="callout">1</span>
    --kubeconfig &lt;openshift_kubeconfig_file&gt; \ <span id="CO74-2"><!--Empty--></span><span class="callout">2</span>
    --credentials-requests-dir &lt;path_to_credential_requests_directory&gt; \ <span id="CO74-3"><!--Empty--></span><span class="callout">3</span>
    --name &lt;name&gt; <span id="CO74-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO74-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The name of the provider. For example: <code class="literal">ibmcloud</code> or <code class="literal">powervs</code>.
									</div></dd><dt><a href="#CO74-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The <code class="literal">kubeconfig</code> file associated with the cluster. For example, <code class="literal">&lt;installation_directory&gt;/auth/kubeconfig</code>.
									</div></dd><dt><a href="#CO74-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The directory where the credential requests are stored.
									</div></dd><dt><a href="#CO74-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										The name of the OpenShift Container Platform cluster.
									</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									If your cluster uses Technology Preview features that are enabled by the <code class="literal">TechPreviewNoUpgrade</code> feature set, you must include the <code class="literal">--enable-tech-preview</code> parameter.
								</p></div></div></li></ul></div></section></section><section class="section" id="manually-rotating-cloud-creds_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.16.2. Rotating cloud provider credentials manually</h3></div></div></div><p>
					If your cloud provider credentials are changed for any reason, you must manually update the secret that the Cloud Credential Operator (CCO) uses to manage cloud provider credentials.
				</p><p>
					The process for rotating cloud credentials depends on the mode that the CCO is configured to use. After you rotate credentials for a cluster that is using mint mode, you must manually remove the component credentials that were created by the removed credential.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Your cluster is installed on a platform that supports rotating cloud credentials manually with the CCO mode that you are using:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									For mint mode, Amazon Web Services (AWS) and Google Cloud Platform (GCP) are supported.
								</li><li class="listitem">
									For passthrough mode, Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), Red Hat OpenStack Platform (RHOSP), Red Hat Virtualization (RHV), and VMware vSphere are supported.
								</li></ul></div></li><li class="listitem">
							You have changed the credentials that are used to interface with your cloud provider.
						</li><li class="listitem">
							The new credentials have sufficient permissions for the mode CCO is configured to use in your cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the <span class="strong strong"><strong>Administrator</strong></span> perspective of the web console, navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Secrets</strong></span>.
						</li><li class="listitem"><p class="simpara">
							In the table on the <span class="strong strong"><strong>Secrets</strong></span> page, find the root secret for your cloud provider.
						</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031633661456" scope="col">Platform</th><th align="left" valign="top" id="idm140031633660368" scope="col">Secret name</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031633661456"> <p>
											AWS
										</p>
										 </td><td align="left" valign="top" headers="idm140031633660368"> <p>
											<code class="literal">aws-creds</code>
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140031633661456"> <p>
											Azure
										</p>
										 </td><td align="left" valign="top" headers="idm140031633660368"> <p>
											<code class="literal">azure-credentials</code>
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140031633661456"> <p>
											GCP
										</p>
										 </td><td align="left" valign="top" headers="idm140031633660368"> <p>
											<code class="literal">gcp-credentials</code>
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140031633661456"> <p>
											RHOSP
										</p>
										 </td><td align="left" valign="top" headers="idm140031633660368"> <p>
											<code class="literal">openstack-credentials</code>
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140031633661456"> <p>
											RHV
										</p>
										 </td><td align="left" valign="top" headers="idm140031633660368"> <p>
											<code class="literal">ovirt-credentials</code>
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140031633661456"> <p>
											VMware vSphere
										</p>
										 </td><td align="left" valign="top" headers="idm140031633660368"> <p>
											<code class="literal">vsphere-creds</code>
										</p>
										 </td></tr></tbody></table></div></li><li class="listitem">
							Click the <span class="strong strong"><strong>Options</strong></span> menu 
							<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Post-installation_configuration-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
							 in the same row as the secret and select <span class="strong strong"><strong>Edit Secret</strong></span>.
						</li><li class="listitem">
							Record the contents of the <span class="strong strong"><strong>Value</strong></span> field or fields. You can use this information to verify that the value is different after updating the credentials.
						</li><li class="listitem">
							Update the text in the <span class="strong strong"><strong>Value</strong></span> field or fields with the new authentication information for your cloud provider, and then click <span class="strong strong"><strong>Save</strong></span>.
						</li><li class="listitem"><p class="simpara">
							If you are updating the credentials for a vSphere cluster that does not have the vSphere CSI Driver Operator enabled, you must force a rollout of the Kubernetes controller manager to apply the updated credentials.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If the vSphere CSI Driver Operator is enabled, this step is not required.
							</p></div></div><p class="simpara">
							To apply the updated vSphere credentials, log in to the OpenShift Container Platform CLI as a user with the <code class="literal">cluster-admin</code> role and run the following command:
						</p><pre class="programlisting language-terminal">$ oc patch kubecontrollermanager cluster \
  -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date )"'"}}' \
  --type=merge</pre><p class="simpara">
							While the credentials are rolling out, the status of the Kubernetes Controller Manager Operator reports <code class="literal">Progressing=true</code>. To view the status, run the following command:
						</p><pre class="programlisting language-terminal">$ oc get co kube-controller-manager</pre></li><li class="listitem"><p class="simpara">
							If the CCO for your cluster is configured to use mint mode, delete each component secret that is referenced by the individual <code class="literal">CredentialsRequest</code> objects.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Log in to the OpenShift Container Platform CLI as a user with the <code class="literal">cluster-admin</code> role.
								</li><li class="listitem"><p class="simpara">
									Get the names and namespaces of all referenced component secrets:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-cloud-credential-operator get CredentialsRequest \
  -o json | jq -r '.items[] | select (.spec.providerSpec.kind=="&lt;provider_spec&gt;") | .spec.secretRef'</pre><p class="simpara">
									where <code class="literal">&lt;provider_spec&gt;</code> is the corresponding value for your cloud provider:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											AWS: <code class="literal">AWSProviderSpec</code>
										</li><li class="listitem">
											GCP: <code class="literal">GCPProviderSpec</code>
										</li></ul></div><div class="formalpara"><p class="title"><strong>Partial example output for AWS</strong></p><p>
										
<pre class="programlisting language-json">{
  "name": "ebs-cloud-credentials",
  "namespace": "openshift-cluster-csi-drivers"
}
{
  "name": "cloud-credential-operator-iam-ro-creds",
  "namespace": "openshift-cloud-credential-operator"
}</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Delete each of the referenced component secrets:
								</p><pre class="programlisting language-terminal">$ oc delete secret &lt;secret_name&gt; \<span id="CO75-1"><!--Empty--></span><span class="callout">1</span>
  -n &lt;secret_namespace&gt; <span id="CO75-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO75-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the name of a secret.
										</div></dd><dt><a href="#CO75-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Specify the namespace that contains the secret.
										</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example deletion of an AWS secret</strong></p><p>
										
<pre class="programlisting language-terminal">$ oc delete secret ebs-cloud-credentials -n openshift-cluster-csi-drivers</pre>

									</p></div><p class="simpara">
									You do not need to manually delete the credentials from your provider console. Deleting the referenced component secrets will cause the CCO to delete the existing credentials from the platform and create new ones.
								</p></li></ol></div></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						To verify that the credentials have changed:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							In the <span class="strong strong"><strong>Administrator</strong></span> perspective of the web console, navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Secrets</strong></span>.
						</li><li class="listitem">
							Verify that the contents of the <span class="strong strong"><strong>Value</strong></span> field or fields have changed.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/storage/#vmware-vsphere-csi-driver-operator">vSphere CSI Driver Operator</a>
						</li></ul></div></section><section class="section" id="manually-removing-cloud-creds_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.16.3. Removing cloud provider credentials</h3></div></div></div><p>
					After installing an OpenShift Container Platform cluster with the Cloud Credential Operator (CCO) in mint mode, you can remove the administrator-level credential secret from the <code class="literal">kube-system</code> namespace in the cluster. The administrator-level credential is required only during changes that require its elevated permissions, such as upgrades.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Prior to a non z-stream upgrade, you must reinstate the credential secret with the administrator-level credential. If the credential is not present, the upgrade might be blocked.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Your cluster is installed on a platform that supports removing cloud credentials from the CCO. Supported platforms are AWS and GCP.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the <span class="strong strong"><strong>Administrator</strong></span> perspective of the web console, navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Secrets</strong></span>.
						</li><li class="listitem"><p class="simpara">
							In the table on the <span class="strong strong"><strong>Secrets</strong></span> page, find the root secret for your cloud provider.
						</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031635983712" scope="col">Platform</th><th align="left" valign="top" id="idm140031634943248" scope="col">Secret name</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031635983712"> <p>
											AWS
										</p>
										 </td><td align="left" valign="top" headers="idm140031634943248"> <p>
											<code class="literal">aws-creds</code>
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140031635983712"> <p>
											GCP
										</p>
										 </td><td align="left" valign="top" headers="idm140031634943248"> <p>
											<code class="literal">gcp-credentials</code>
										</p>
										 </td></tr></tbody></table></div></li><li class="listitem">
							Click the <span class="strong strong"><strong>Options</strong></span> menu 
							<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Post-installation_configuration-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
							 in the same row as the secret and select <span class="strong strong"><strong>Delete Secret</strong></span>.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#about-cloud-credential-operator">About the Cloud Credential Operator</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#admin-credentials-root-secret-formats_manually-creating-iam-aws">Amazon Web Services (AWS) secret format</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#admin-credentials-root-secret-formats_manually-creating-iam-azure">Microsoft Azure secret format</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#admin-credentials-root-secret-formats_manually-creating-iam-gcp">Google Cloud Platform (GCP) secret format</a>
						</li></ul></div></section></section><section class="section" id="post-install-must-gather-disconnected"><div class="titlepage"><div><div><h2 class="title">7.17. Configuring image streams for a disconnected cluster</h2></div></div></div><p>
				After installing OpenShift Container Platform in a disconnected environment, configure the image streams for the Cluster Samples Operator and the <code class="literal">must-gather</code> image stream.
			</p><section class="section" id="installation-images-samples-disconnected-mirroring-assist_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.17.1. Cluster Samples Operator assistance for mirroring</h3></div></div></div><p>
					During installation, OpenShift Container Platform creates a config map named <code class="literal">imagestreamtag-to-image</code> in the <code class="literal">openshift-cluster-samples-operator</code> namespace. The <code class="literal">imagestreamtag-to-image</code> config map contains an entry, the populating image, for each image stream tag.
				</p><p>
					The format of the key for each entry in the data field in the config map is <code class="literal">&lt;image_stream_name&gt;_&lt;image_stream_tag_name&gt;</code>.
				</p><p>
					During a disconnected installation of OpenShift Container Platform, the status of the Cluster Samples Operator is set to <code class="literal">Removed</code>. If you choose to change it to <code class="literal">Managed</code>, it installs samples.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The use of samples in a network-restricted or discontinued environment may require access to services external to your network. Some example services include: Github, Maven Central, npm, RubyGems, PyPi and others. There might be additional steps to take that allow the cluster samples operators’s objects to reach the services they require.
					</p></div></div><p>
					You can use this config map as a reference for which images need to be mirrored for your image streams to import.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							While the Cluster Samples Operator is set to <code class="literal">Removed</code>, you can create your mirrored registry, or determine which existing mirrored registry you want to use.
						</li><li class="listitem">
							Mirror the samples you want to the mirrored registry using the new config map as your guide.
						</li><li class="listitem">
							Add any of the image streams you did not mirror to the <code class="literal">skippedImagestreams</code> list of the Cluster Samples Operator configuration object.
						</li><li class="listitem">
							Set <code class="literal">samplesRegistry</code> of the Cluster Samples Operator configuration object to the mirrored registry.
						</li><li class="listitem">
							Then set the Cluster Samples Operator to <code class="literal">Managed</code> to install the image streams you have mirrored.
						</li></ul></div></section><section class="section" id="installation-restricted-network-samples_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.17.2. Using Cluster Samples Operator image streams with alternate or mirrored registries</h3></div></div></div><p>
					Most image streams in the <code class="literal">openshift</code> namespace managed by the Cluster Samples Operator point to images located in the Red Hat registry at <a class="link" href="https://registry.redhat.io">registry.redhat.io</a>. Mirroring will not apply to these image streams.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The <code class="literal">cli</code>, <code class="literal">installer</code>, <code class="literal">must-gather</code>, and <code class="literal">tests</code> image streams, while part of the install payload, are not managed by the Cluster Samples Operator. These are not addressed in this procedure.
					</p></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The Cluster Samples Operator must be set to <code class="literal">Managed</code> in a disconnected environment. To install the image streams, you have a mirrored registry.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Create a pull secret for your mirror registry.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Access the images of a specific image stream to mirror, for example:
						</p><pre class="programlisting language-terminal">$ oc get is &lt;imagestream&gt; -n openshift -o json | jq .spec.tags[].from.name | grep registry.redhat.io</pre></li><li class="listitem"><p class="simpara">
							Mirror images from <a class="link" href="https://registry.redhat.io">registry.redhat.io</a> associated with any image streams you need in the restricted network environment into one of the defined mirrors, for example:
						</p><pre class="programlisting language-terminal">$ oc image mirror registry.redhat.io/rhscl/ruby-25-rhel7:latest ${MIRROR_ADDR}/rhscl/ruby-25-rhel7:latest</pre></li><li class="listitem"><p class="simpara">
							Create the cluster’s image configuration object:
						</p><pre class="programlisting language-terminal">$ oc create configmap registry-config --from-file=${MIRROR_ADDR_HOSTNAME}..5000=$path/ca.crt -n openshift-config</pre></li><li class="listitem"><p class="simpara">
							Add the required trusted CAs for the mirror in the cluster’s image configuration object:
						</p><pre class="programlisting language-terminal">$ oc patch image.config.openshift.io/cluster --patch '{"spec":{"additionalTrustedCA":{"name":"registry-config"}}}' --type=merge</pre></li><li class="listitem"><p class="simpara">
							Update the <code class="literal">samplesRegistry</code> field in the Cluster Samples Operator configuration object to contain the <code class="literal">hostname</code> portion of the mirror location defined in the mirror configuration:
						</p><pre class="programlisting language-terminal">$ oc edit configs.samples.operator.openshift.io -n openshift-cluster-samples-operator</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								This is required because the image stream import process does not use the mirror or search mechanism at this time.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Add any image streams that are not mirrored into the <code class="literal">skippedImagestreams</code> field of the Cluster Samples Operator configuration object. Or if you do not want to support any of the sample image streams, set the Cluster Samples Operator to <code class="literal">Removed</code> in the Cluster Samples Operator configuration object.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The Cluster Samples Operator issues alerts if image stream imports are failing but the Cluster Samples Operator is either periodically retrying or does not appear to be retrying them.
							</p></div></div><p class="simpara">
							Many of the templates in the <code class="literal">openshift</code> namespace reference the image streams. So using <code class="literal">Removed</code> to purge both the image streams and templates will eliminate the possibility of attempts to use them if they are not functional because of any missing image streams.
						</p></li></ol></div></section><section class="section" id="installation-preparing-restricted-cluster-to-gather-support-data_post-install-cluster-tasks"><div class="titlepage"><div><div><h3 class="title">7.17.3. Preparing your cluster to gather support data</h3></div></div></div><p>
					Clusters using a restricted network must import the default must-gather image to gather debugging data for Red Hat support. The must-gather image is not imported by default, and clusters on a restricted network do not have access to the internet to pull the latest image from a remote repository.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							If you have not added your mirror registry’s trusted CA to your cluster’s image configuration object as part of the Cluster Samples Operator configuration, perform the following steps:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create the cluster’s image configuration object:
								</p><pre class="programlisting language-terminal">$ oc create configmap registry-config --from-file=${MIRROR_ADDR_HOSTNAME}..5000=$path/ca.crt -n openshift-config</pre></li><li class="listitem"><p class="simpara">
									Add the required trusted CAs for the mirror in the cluster’s image configuration object:
								</p><pre class="programlisting language-terminal">$ oc patch image.config.openshift.io/cluster --patch '{"spec":{"additionalTrustedCA":{"name":"registry-config"}}}' --type=merge</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Import the default must-gather image from your installation payload:
						</p><pre class="programlisting language-terminal">$ oc import-image is/must-gather -n openshift</pre></li></ol></div><p>
					When running the <code class="literal">oc adm must-gather</code> command, use the <code class="literal">--image</code> flag and point to the payload image, as in the following example:
				</p><pre class="programlisting language-terminal">$ oc adm must-gather --image=$(oc adm release info --image-for must-gather)</pre></section></section><section class="section" id="images-cluster-sample-imagestream-import_post-install-cluster-tasks"><div class="titlepage"><div><div><h2 class="title">7.18. Configuring periodic importing of Cluster Sample Operator image stream tags</h2></div></div></div><p>
				You can ensure that you always have access to the latest versions of the Cluster Sample Operator images by periodically importing the image stream tags when new versions become available.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Fetch all the imagestreams in the <code class="literal">openshift</code> namespace by running the following command:
					</p><pre class="programlisting language-terminal">oc get imagestreams -nopenshift</pre></li><li class="listitem"><p class="simpara">
						Fetch the tags for every imagestream in the <code class="literal">openshift</code> namespace by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get is &lt;image-stream-name&gt; -o jsonpath="{range .spec.tags[*]}{.name}{'\t'}{.from.name}{'\n'}{end}" -nopenshift</pre><p class="simpara">
						For example:
					</p><pre class="programlisting language-terminal">$ oc get is ubi8-openjdk-17 -o jsonpath="{range .spec.tags[*]}{.name}{'\t'}{.from.name}{'\n'}{end}" -nopenshift</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">1.11	registry.access.redhat.com/ubi8/openjdk-17:1.11
1.12	registry.access.redhat.com/ubi8/openjdk-17:1.12</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Schedule periodic importing of images for each tag present in the image stream by running the following command:
					</p><pre class="programlisting language-terminal">$ oc tag &lt;repository/image&gt; &lt;image-stream-name:tag&gt; --scheduled -nopenshift</pre><p class="simpara">
						For example:
					</p><pre class="programlisting language-terminal">$ oc tag registry.access.redhat.com/ubi8/openjdk-17:1.11 ubi8-openjdk-17:1.11 --scheduled -nopenshift
$ oc tag registry.access.redhat.com/ubi8/openjdk-17:1.12 ubi8-openjdk-17:1.12 --scheduled -nopenshift</pre><p class="simpara">
						This command causes OpenShift Container Platform to periodically update this particular image stream tag. This period is a cluster-wide setting set to 15 minutes by default.
					</p></li><li class="listitem"><p class="simpara">
						Verify the scheduling status of the periodic import by running the following command:
					</p><pre class="programlisting language-terminal">oc get imagestream &lt;image-stream-name&gt; -o jsonpath="{range .spec.tags[*]}Tag: {.name}{'\t'}Scheduled: {.importPolicy.scheduled}{'\n'}{end}" -nopenshift</pre><p class="simpara">
						For example:
					</p><pre class="programlisting language-terminal">oc get imagestream ubi8-openjdk-17 -o jsonpath="{range .spec.tags[*]}Tag: {.name}{'\t'}Scheduled: {.importPolicy.scheduled}{'\n'}{end}" -nopenshift</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">Tag: 1.11	Scheduled: true
Tag: 1.12	Scheduled: true</pre>

						</p></div></li></ol></div></section></section><section class="chapter" id="post-install-node-tasks"><div class="titlepage"><div><div><h1 class="title">Chapter 8. Post-installation node tasks</h1></div></div></div><p>
			After installing OpenShift Container Platform, you can further expand and customize your cluster to your requirements through certain node tasks.
		</p><section class="section" id="post-install-config-adding-rhel-compute"><div class="titlepage"><div><div><h2 class="title">8.1. Adding RHEL compute machines to an OpenShift Container Platform cluster</h2></div></div></div><p>
				Understand and work with RHEL compute nodes.
			</p><section class="section" id="rhel-compute-overview_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.1.1. About adding RHEL compute nodes to a cluster</h3></div></div></div><p>
					In OpenShift Container Platform 4.13, you have the option of using Red Hat Enterprise Linux (RHEL) machines as compute machines in your cluster if you use a user-provisioned or installer-provisioned infrastructure installation on the <code class="literal">x86_64</code> architecture. You must use Red Hat Enterprise Linux CoreOS (RHCOS) machines for the control plane machines in your cluster.
				</p><p>
					If you choose to use RHEL compute machines in your cluster, you are responsible for all operating system life cycle management and maintenance. You must perform system updates, apply patches, and complete all other required tasks.
				</p><p>
					For installer-provisioned infrastructure clusters, you must manually add RHEL compute machines because automatic scaling in installer-provisioned infrastructure clusters adds Red Hat Enterprise Linux CoreOS (RHCOS) compute machines by default.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Because removing OpenShift Container Platform from a machine in the cluster requires destroying the operating system, you must use dedicated hardware for any RHEL machines that you add to the cluster.
							</li><li class="listitem">
								Swap memory is disabled on all RHEL machines that you add to your OpenShift Container Platform cluster. You cannot enable swap memory on these machines.
							</li></ul></div></div></div><p>
					You must add any RHEL compute machines to the cluster after you initialize the control plane.
				</p></section><section class="section" id="rhel-compute-requirements_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.1.2. System requirements for RHEL compute nodes</h3></div></div></div><p>
					The Red Hat Enterprise Linux (RHEL) compute machine hosts in your OpenShift Container Platform environment must meet the following minimum hardware specifications and system-level requirements:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							You must have an active OpenShift Container Platform subscription on your Red Hat account. If you do not, contact your sales representative for more information.
						</li><li class="listitem">
							Production environments must provide compute machines to support your expected workloads. As a cluster administrator, you must calculate the expected workload and add about 10% for overhead. For production environments, allocate enough resources so that a node host failure does not affect your maximum capacity.
						</li><li class="listitem"><p class="simpara">
							Each system must meet the following hardware requirements:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									Physical or virtual system, or an instance running on a public or private IaaS.
								</li><li class="listitem"><p class="simpara">
									Base OS: <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/index">RHEL 8.6, 8.7, or 8.8</a> with "Minimal" installation option.
								</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
										Adding RHEL 7 compute machines to an OpenShift Container Platform cluster is not supported.
									</p><p>
										If you have RHEL 7 compute machines that were previously supported in a past OpenShift Container Platform version, you cannot upgrade them to RHEL 8. You must deploy new RHEL 8 hosts, and the old RHEL 7 hosts should be removed. See the "Deleting nodes" section for more information.
									</p><p>
										For the most recent list of major functionality that has been deprecated or removed within OpenShift Container Platform, refer to the <span class="emphasis"><em>Deprecated and removed features</em></span> section of the OpenShift Container Platform release notes.
									</p></div></div></li></ul></div></li></ul></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							NetworkManager 1.0 or later.
						</li><li class="listitem">
							1 vCPU.
						</li><li class="listitem">
							Minimum 8 GB RAM.
						</li><li class="listitem">
							Minimum 15 GB hard disk space for the file system containing <code class="literal">/var/</code>.
						</li><li class="listitem">
							Minimum 1 GB hard disk space for the file system containing <code class="literal">/usr/local/bin/</code>.
						</li><li class="listitem"><p class="simpara">
							Minimum 1 GB hard disk space for the file system containing its temporary directory. The temporary system directory is determined according to the rules defined in the tempfile module in the Python standard library.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									Each system must meet any additional requirements for your system provider. For example, if you installed your cluster on VMware vSphere, your disks must be configured according to its <a class="link" href="https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/index.html">storage guidelines</a> and the <code class="literal">disk.enableUUID=true</code> attribute must be set.
								</li><li class="listitem">
									Each system must be able to access the cluster’s API endpoints by using DNS-resolvable hostnames. Any network security access control that is in place must allow system access to the cluster’s API service endpoints.
								</li></ul></div></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-working-deleting_nodes-nodes-working">Deleting nodes</a>
						</li></ul></div><section class="section" id="csr-management_post-install-node-tasks"><div class="titlepage"><div><div><h4 class="title">8.1.2.1. Certificate signing requests management</h4></div></div></div><p>
						Because your cluster has limited access to automatic machine management when you use infrastructure that you provision, you must provide a mechanism for approving cluster certificate signing requests (CSRs) after installation. The <code class="literal">kube-controller-manager</code> only approves the kubelet client CSRs. The <code class="literal">machine-approver</code> cannot guarantee the validity of a serving certificate that is requested by using kubelet credentials because it cannot confirm that the correct machine issued the request. You must determine and implement a method of verifying the validity of the kubelet serving certificate requests and approving them.
					</p></section></section><section class="section" id="rhel-preparing-playbook-machine_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.1.3. Preparing the machine to run the playbook</h3></div></div></div><p>
					Before you can add compute machines that use Red Hat Enterprise Linux (RHEL) as the operating system to an OpenShift Container Platform 4.13 cluster, you must prepare a RHEL 8 machine to run an Ansible playbook that adds the new node to the cluster. This machine is not part of the cluster but must be able to access it.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>) on the machine that you run the playbook on.
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> permission.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Ensure that the <code class="literal">kubeconfig</code> file for the cluster and the installation program that you used to install the cluster are on the RHEL 8 machine. One way to accomplish this is to use the same machine that you used to install the cluster.
						</li><li class="listitem">
							Configure the machine to access all of the RHEL hosts that you plan to use as compute machines. You can use any method that your company allows, including a bastion with an SSH proxy or a VPN.
						</li><li class="listitem"><p class="simpara">
							Configure a user on the machine that you run the playbook on that has SSH access to all of the RHEL hosts.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								If you use SSH key-based authentication, you must manage the key with an SSH agent.
							</p></div></div></li><li class="listitem"><p class="simpara">
							If you have not already done so, register the machine with RHSM and attach a pool with an <code class="literal">OpenShift</code> subscription to it:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Register the machine with RHSM:
								</p><pre class="programlisting language-terminal"># subscription-manager register --username=&lt;user_name&gt; --password=&lt;password&gt;</pre></li><li class="listitem"><p class="simpara">
									Pull the latest subscription data from RHSM:
								</p><pre class="programlisting language-terminal"># subscription-manager refresh</pre></li><li class="listitem"><p class="simpara">
									List the available subscriptions:
								</p><pre class="programlisting language-terminal"># subscription-manager list --available --matches '*OpenShift*'</pre></li><li class="listitem"><p class="simpara">
									In the output for the previous command, find the pool ID for an OpenShift Container Platform subscription and attach it:
								</p><pre class="programlisting language-terminal"># subscription-manager attach --pool=&lt;pool_id&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Enable the repositories required by OpenShift Container Platform 4.13:
						</p><pre class="programlisting language-terminal"># subscription-manager repos \
    --enable="rhel-8-for-x86_64-baseos-rpms" \
    --enable="rhel-8-for-x86_64-appstream-rpms" \
    --enable="rhocp-4.13-for-rhel-8-x86_64-rpms"</pre></li><li class="listitem"><p class="simpara">
							Install the required packages, including <code class="literal">openshift-ansible</code>:
						</p><pre class="programlisting language-terminal"># yum install openshift-ansible openshift-clients jq</pre><p class="simpara">
							The <code class="literal">openshift-ansible</code> package provides installation program utilities and pulls in other packages that you require to add a RHEL compute node to your cluster, such as Ansible, playbooks, and related configuration files. The <code class="literal">openshift-clients</code> provides the <code class="literal">oc</code> CLI, and the <code class="literal">jq</code> package improves the display of JSON output on your command line.
						</p></li></ol></div></section><section class="section" id="rhel-preparing-node_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.1.4. Preparing a RHEL compute node</h3></div></div></div><p>
					Before you add a Red Hat Enterprise Linux (RHEL) machine to your OpenShift Container Platform cluster, you must register each host with Red Hat Subscription Manager (RHSM), attach an active OpenShift Container Platform subscription, and enable the required repositories.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							On each host, register with RHSM:
						</p><pre class="programlisting language-terminal"># subscription-manager register --username=&lt;user_name&gt; --password=&lt;password&gt;</pre></li><li class="listitem"><p class="simpara">
							Pull the latest subscription data from RHSM:
						</p><pre class="programlisting language-terminal"># subscription-manager refresh</pre></li><li class="listitem"><p class="simpara">
							List the available subscriptions:
						</p><pre class="programlisting language-terminal"># subscription-manager list --available --matches '*OpenShift*'</pre></li><li class="listitem"><p class="simpara">
							In the output for the previous command, find the pool ID for an OpenShift Container Platform subscription and attach it:
						</p><pre class="programlisting language-terminal"># subscription-manager attach --pool=&lt;pool_id&gt;</pre></li><li class="listitem"><p class="simpara">
							Disable all yum repositories:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Disable all the enabled RHSM repositories:
								</p><pre class="programlisting language-terminal"># subscription-manager repos --disable="*"</pre></li><li class="listitem"><p class="simpara">
									List the remaining yum repositories and note their names under <code class="literal">repo id</code>, if any:
								</p><pre class="programlisting language-terminal"># yum repolist</pre></li><li class="listitem"><p class="simpara">
									Use <code class="literal">yum-config-manager</code> to disable the remaining yum repositories:
								</p><pre class="programlisting language-terminal"># yum-config-manager --disable &lt;repo_id&gt;</pre><p class="simpara">
									Alternatively, disable all repositories:
								</p><pre class="programlisting language-terminal"># yum-config-manager --disable \*</pre><p class="simpara">
									Note that this might take a few minutes if you have a large number of available repositories
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Enable only the repositories required by OpenShift Container Platform 4.13:
						</p><pre class="programlisting language-terminal"># subscription-manager repos \
    --enable="rhel-8-for-x86_64-baseos-rpms" \
    --enable="rhel-8-for-x86_64-appstream-rpms" \
    --enable="rhocp-4.13-for-rhel-8-x86_64-rpms" \
    --enable="fast-datapath-for-rhel-8-x86_64-rpms"</pre></li><li class="listitem"><p class="simpara">
							Stop and disable firewalld on the host:
						</p><pre class="programlisting language-terminal"># systemctl disable --now firewalld.service</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								You must not enable firewalld later. If you do, you cannot access OpenShift Container Platform logs on the worker.
							</p></div></div></li></ol></div></section><section class="section" id="rhel-adding-node_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.1.5. Adding a RHEL compute machine to your cluster</h3></div></div></div><p>
					You can add compute machines that use Red Hat Enterprise Linux as the operating system to an OpenShift Container Platform 4.13 cluster.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You installed the required packages and performed the necessary configuration on the machine that you run the playbook on.
						</li><li class="listitem">
							You prepared the RHEL hosts for installation.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						Perform the following steps on the machine that you prepared to run the playbook:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an Ansible inventory file that is named <code class="literal">/&lt;path&gt;/inventory/hosts</code> that defines your compute machine hosts and required variables:
						</p><pre class="screen">[all:vars]
ansible_user=root <span id="CO76-1"><!--Empty--></span><span class="callout">1</span>
#ansible_become=True <span id="CO76-2"><!--Empty--></span><span class="callout">2</span>

openshift_kubeconfig_path="~/.kube/config" <span id="CO76-3"><!--Empty--></span><span class="callout">3</span>

[new_workers] <span id="CO76-4"><!--Empty--></span><span class="callout">4</span>
mycluster-rhel8-0.example.com
mycluster-rhel8-1.example.com</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO76-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the user name that runs the Ansible tasks on the remote compute machines.
								</div></dd><dt><a href="#CO76-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									If you do not specify <code class="literal">root</code> for the <code class="literal">ansible_user</code>, you must set <code class="literal">ansible_become</code> to <code class="literal">True</code> and assign the user sudo permissions.
								</div></dd><dt><a href="#CO76-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify the path and file name of the <code class="literal">kubeconfig</code> file for your cluster.
								</div></dd><dt><a href="#CO76-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									List each RHEL machine to add to your cluster. You must provide the fully-qualified domain name for each host. This name is the hostname that the cluster uses to access the machine, so set the correct public or private name to access the machine.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Navigate to the Ansible playbook directory:
						</p><pre class="programlisting language-terminal">$ cd /usr/share/ansible/openshift-ansible</pre></li><li class="listitem"><p class="simpara">
							Run the playbook:
						</p><pre class="programlisting language-terminal">$ ansible-playbook -i /&lt;path&gt;/inventory/hosts playbooks/scaleup.yml <span id="CO77-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO77-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									For <code class="literal">&lt;path&gt;</code>, specify the path to the Ansible inventory file that you created.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="rhel-ansible-parameters_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.1.6. Required parameters for the Ansible hosts file</h3></div></div></div><p>
					You must define the following parameters in the Ansible hosts file before you add Red Hat Enterprise Linux (RHEL) compute machines to your cluster.
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 40%; " class="col_2"><!--Empty--></col><col style="width: 40%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031636942576" scope="col">Parameter</th><th align="left" valign="top" id="idm140031636941488" scope="col">Description</th><th align="left" valign="top" id="idm140031636940400" scope="col">Values</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031636942576"> <p>
									<code class="literal">ansible_user</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636941488"> <p>
									The SSH user that allows SSH-based authentication without requiring a password. If you use SSH key-based authentication, then you must manage the key with an SSH agent.
								</p>
								 </td><td align="left" valign="top" headers="idm140031636940400"> <p>
									A user name on the system. The default value is <code class="literal">root</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031636942576"> <p>
									<code class="literal">ansible_become</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636941488"> <p>
									If the values of <code class="literal">ansible_user</code> is not root, you must set <code class="literal">ansible_become</code> to <code class="literal">True</code>, and the user that you specify as the <code class="literal">ansible_user</code> must be configured for passwordless sudo access.
								</p>
								 </td><td align="left" valign="top" headers="idm140031636940400"> <p>
									<code class="literal">True</code>. If the value is not <code class="literal">True</code>, do not specify and define this parameter.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031636942576"> <p>
									<code class="literal">openshift_kubeconfig_path</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636941488"> <p>
									Specifies a path and file name to a local directory that contains the <code class="literal">kubeconfig</code> file for your cluster.
								</p>
								 </td><td align="left" valign="top" headers="idm140031636940400"> <p>
									The path and name of the configuration file.
								</p>
								 </td></tr></tbody></table></div></section><section class="section" id="rhel-removing-rhcos_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.1.7. Optional: Removing RHCOS compute machines from a cluster</h3></div></div></div><p>
					After you add the Red Hat Enterprise Linux (RHEL) compute machines to your cluster, you can optionally remove the Red Hat Enterprise Linux CoreOS (RHCOS) compute machines to free up resources.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have added RHEL compute machines to your cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the list of machines and record the node names of the RHCOS compute machines:
						</p><pre class="programlisting language-terminal">$ oc get nodes -o wide</pre></li><li class="listitem"><p class="simpara">
							For each RHCOS compute machine, delete the node:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Mark the node as unschedulable by running the <code class="literal">oc adm cordon</code> command:
								</p><pre class="programlisting language-terminal">$ oc adm cordon &lt;node_name&gt; <span id="CO78-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO78-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the node name of one of the RHCOS compute machines.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Drain all the pods from the node:
								</p><pre class="programlisting language-terminal">$ oc adm drain &lt;node_name&gt; --force --delete-emptydir-data --ignore-daemonsets <span id="CO79-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO79-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the node name of the RHCOS compute machine that you isolated.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Delete the node:
								</p><pre class="programlisting language-terminal">$ oc delete nodes &lt;node_name&gt; <span id="CO80-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO80-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the node name of the RHCOS compute machine that you drained.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Review the list of compute machines to ensure that only the RHEL nodes remain:
						</p><pre class="programlisting language-terminal">$ oc get nodes -o wide</pre></li><li class="listitem">
							Remove the RHCOS machines from the load balancer for your cluster’s compute machines. You can delete the virtual machines or reimage the physical hardware for the RHCOS compute machines.
						</li></ol></div></section></section><section class="section" id="post-install-config-adding-fcos-compute"><div class="titlepage"><div><div><h2 class="title">8.2. Adding RHCOS compute machines to an OpenShift Container Platform cluster</h2></div></div></div><p>
				You can add more Red Hat Enterprise Linux CoreOS (RHCOS) compute machines to your OpenShift Container Platform cluster on bare metal.
			</p><p>
				Before you add more compute machines to a cluster that you installed on bare metal infrastructure, you must create RHCOS machines for it to use. You can either use an ISO image or network PXE booting to create the machines.
			</p><section class="section" id="prerequisites"><div class="titlepage"><div><div><h3 class="title">8.2.1. Prerequisites</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							You installed a cluster on bare metal.
						</li><li class="listitem">
							You have installation media and Red Hat Enterprise Linux CoreOS (RHCOS) images that you used to create your cluster. If you do not have these files, you must obtain them by following the instructions in the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-bare-metal">installation procedure</a>.
						</li></ul></div></section><section class="section" id="machine-user-infra-machines-iso_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.2.2. Creating RHCOS machines using an ISO image</h3></div></div></div><p>
					You can create more Red Hat Enterprise Linux CoreOS (RHCOS) compute machines for your bare metal cluster by using an ISO image to create the machines.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Obtain the URL of the Ignition config file for the compute machines for your cluster. You uploaded this file to your HTTP server during installation.
						</li><li class="listitem">
							You must have the OpenShift CLI (<code class="literal">oc</code>) installed.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Extract the Ignition config file from the cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc extract -n openshift-machine-api secret/worker-user-data-managed --keys=userData --to=- &gt; worker.ign</pre></li><li class="listitem">
							Upload the <code class="literal">worker.ign</code> Ignition config file you exported from your cluster to your HTTP server. Note the URLs of these files.
						</li><li class="listitem"><p class="simpara">
							You can validate that the ignition files are available on the URLs. The following example gets the Ignition config files for the compute node:
						</p><pre class="programlisting language-terminal">$ curl -k http://&lt;HTTP_server&gt;/worker.ign</pre></li><li class="listitem"><p class="simpara">
							You can access the ISO image for booting your new machine by running to following command:
						</p><pre class="programlisting language-terminal">RHCOS_VHD_ORIGIN_URL=$(oc -n openshift-machine-config-operator get configmap/coreos-bootimages -o jsonpath='{.data.stream}' | jq -r '.architectures.&lt;architecture&gt;.artifacts.metal.formats.iso.disk.location')</pre></li><li class="listitem"><p class="simpara">
							Use the ISO file to install RHCOS on more compute machines. Use the same method that you used when you created machines before you installed the cluster:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Burn the ISO image to a disk and boot it directly.
								</li><li class="listitem">
									Use ISO redirection with a LOM interface.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							Boot the RHCOS ISO image without specifying any options, or interrupting the live boot sequence. Wait for the installer to boot into a shell prompt in the RHCOS live environment.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								You can interrupt the RHCOS installation boot process to add kernel arguments. However, for this ISO procedure you must use the <code class="literal">coreos-installer</code> command as outlined in the following steps, instead of adding kernel arguments.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">coreos-installer</code> command and specify the options that meet your installation requirements. At a minimum, you must specify the URL that points to the Ignition config file for the node type, and the device that you are installing to:
						</p><pre class="programlisting language-terminal">$ sudo coreos-installer install --ignition-url=http://&lt;HTTP_server&gt;/&lt;node_type&gt;.ign &lt;device&gt; --ignition-hash=sha512-&lt;digest&gt; <span id="CO81-1"><!--Empty--></span><span class="callout">1</span><span id="CO81-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO81-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									You must run the <code class="literal">coreos-installer</code> command by using <code class="literal">sudo</code>, because the <code class="literal">core</code> user does not have the required root privileges to perform the installation.
								</div></dd><dt><a href="#CO81-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The <code class="literal">--ignition-hash</code> option is required when the Ignition config file is obtained through an HTTP URL to validate the authenticity of the Ignition config file on the cluster node. <code class="literal">&lt;digest&gt;</code> is the Ignition config file SHA512 digest obtained in a preceding step.
								</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If you want to provide your Ignition config files through an HTTPS server that uses TLS, you can add the internal certificate authority (CA) to the system trust store before running <code class="literal">coreos-installer</code>.
							</p></div></div><p class="simpara">
							The following example initializes a bootstrap node installation to the <code class="literal">/dev/sda</code> device. The Ignition config file for the bootstrap node is obtained from an HTTP web server with the IP address 192.168.1.2:
						</p><pre class="programlisting language-terminal">$ sudo coreos-installer install --ignition-url=http://192.168.1.2:80/installation_directory/bootstrap.ign /dev/sda --ignition-hash=sha512-a5a2d43879223273c9b60af66b44202a1d1248fc01cf156c46d4a79f552b6bad47bc8cc78ddf0116e80c59d2ea9e32ba53bc807afbca581aa059311def2c3e3b</pre></li><li class="listitem"><p class="simpara">
							Monitor the progress of the RHCOS installation on the console of the machine.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Ensure that the installation is successful on each node before commencing with the OpenShift Container Platform installation. Observing the installation process can also help to determine the cause of RHCOS installation issues that might arise.
							</p></div></div></li><li class="listitem">
							Continue to create more compute machines for your cluster.
						</li></ol></div></section><section class="section" id="machine-user-infra-machines-pxe_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.2.3. Creating RHCOS machines by PXE or iPXE booting</h3></div></div></div><p>
					You can create more Red Hat Enterprise Linux CoreOS (RHCOS) compute machines for your bare metal cluster by using PXE or iPXE booting.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Obtain the URL of the Ignition config file for the compute machines for your cluster. You uploaded this file to your HTTP server during installation.
						</li><li class="listitem">
							Obtain the URLs of the RHCOS ISO image, compressed metal BIOS, <code class="literal">kernel</code>, and <code class="literal">initramfs</code> files that you uploaded to your HTTP server during cluster installation.
						</li><li class="listitem">
							You have access to the PXE booting infrastructure that you used to create the machines for your OpenShift Container Platform cluster during installation. The machines must boot from their local disks after RHCOS is installed on them.
						</li><li class="listitem">
							If you use UEFI, you have access to the <code class="literal">grub.conf</code> file that you modified during OpenShift Container Platform installation.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Confirm that your PXE or iPXE installation for the RHCOS images is correct.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									For PXE:
								</p><pre class="screen">DEFAULT pxeboot
TIMEOUT 20
PROMPT 0
LABEL pxeboot
    KERNEL http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-kernel-&lt;architecture&gt; <span id="CO82-1"><!--Empty--></span><span class="callout">1</span>
    APPEND initrd=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-initramfs.&lt;architecture&gt;.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://&lt;HTTP_server&gt;/worker.ign coreos.live.rootfs_url=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-rootfs.&lt;architecture&gt;.img <span id="CO82-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO82-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the location of the live <code class="literal">kernel</code> file that you uploaded to your HTTP server.
										</div></dd><dt><a href="#CO82-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Specify locations of the RHCOS files that you uploaded to your HTTP server. The <code class="literal">initrd</code> parameter value is the location of the live <code class="literal">initramfs</code> file, the <code class="literal">coreos.inst.ignition_url</code> parameter value is the location of the worker Ignition config file, and the <code class="literal">coreos.live.rootfs_url</code> parameter value is the location of the live <code class="literal">rootfs</code> file. The <code class="literal">coreos.inst.ignition_url</code> and <code class="literal">coreos.live.rootfs_url</code> parameters only support HTTP and HTTPS.
										</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										This configuration does not enable serial console access on machines with a graphical console. To configure a different console, add one or more <code class="literal">console=</code> arguments to the <code class="literal">APPEND</code> line. For example, add <code class="literal">console=tty0 console=ttyS0</code> to set the first PC serial port as the primary console and the graphical console as a secondary console. For more information, see <a class="link" href="https://access.redhat.com/articles/7212">How does one set up a serial terminal and/or console in Red Hat Enterprise Linux?</a>.
									</p></div></div></li><li class="listitem"><p class="simpara">
									For iPXE (<code class="literal">x86_64</code> + <code class="literal">aarch64</code>):
								</p><pre class="screen">kernel http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-kernel-&lt;architecture&gt; initrd=main coreos.live.rootfs_url=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-rootfs.&lt;architecture&gt;.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://&lt;HTTP_server&gt;/worker.ign <span id="CO83-1"><!--Empty--></span><span class="callout">1</span> <span id="CO83-2"><!--Empty--></span><span class="callout">2</span>
initrd --name main http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-initramfs.&lt;architecture&gt;.img <span id="CO83-3"><!--Empty--></span><span class="callout">3</span>
boot</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO83-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the locations of the RHCOS files that you uploaded to your HTTP server. The <code class="literal">kernel</code> parameter value is the location of the <code class="literal">kernel</code> file, the <code class="literal">initrd=main</code> argument is needed for booting on UEFI systems, the <code class="literal">coreos.live.rootfs_url</code> parameter value is the location of the <code class="literal">rootfs</code> file, and the <code class="literal">coreos.inst.ignition_url</code> parameter value is the location of the worker Ignition config file.
										</div></dd><dt><a href="#CO83-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											If you use multiple NICs, specify a single interface in the <code class="literal">ip</code> option. For example, to use DHCP on a NIC that is named <code class="literal">eno1</code>, set <code class="literal">ip=eno1:dhcp</code>.
										</div></dd><dt><a href="#CO83-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Specify the location of the <code class="literal">initramfs</code> file that you uploaded to your HTTP server.
										</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										This configuration does not enable serial console access on machines with a graphical console To configure a different console, add one or more <code class="literal">console=</code> arguments to the <code class="literal">kernel</code> line. For example, add <code class="literal">console=tty0 console=ttyS0</code> to set the first PC serial port as the primary console and the graphical console as a secondary console. For more information, see <a class="link" href="https://access.redhat.com/articles/7212">How does one set up a serial terminal and/or console in Red Hat Enterprise Linux?</a> and "Enabling the serial console for PXE and ISO installation" in the "Advanced RHCOS installation configuration" section.
									</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										To network boot the CoreOS <code class="literal">kernel</code> on <code class="literal">aarch64</code> architecture, you need to use a version of iPXE build with the <code class="literal">IMAGE_GZIP</code> option enabled. See <a class="link" href="https://ipxe.org/buildcfg/image_gzip"><code class="literal">IMAGE_GZIP</code> option in iPXE</a>.
									</p></div></div></li><li class="listitem"><p class="simpara">
									For PXE (with UEFI and GRUB as second stage) on <code class="literal">aarch64</code>:
								</p><pre class="screen">menuentry 'Install CoreOS' {
    linux rhcos-&lt;version&gt;-live-kernel-&lt;architecture&gt;  coreos.live.rootfs_url=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-rootfs.&lt;architecture&gt;.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://&lt;HTTP_server&gt;/worker.ign <span id="CO84-1"><!--Empty--></span><span class="callout">1</span> <span id="CO84-2"><!--Empty--></span><span class="callout">2</span>
    initrd rhcos-&lt;version&gt;-live-initramfs.&lt;architecture&gt;.img <span id="CO84-3"><!--Empty--></span><span class="callout">3</span>
}</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO84-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the locations of the RHCOS files that you uploaded to your HTTP/TFTP server. The <code class="literal">kernel</code> parameter value is the location of the <code class="literal">kernel</code> file on your TFTP server. The <code class="literal">coreos.live.rootfs_url</code> parameter value is the location of the <code class="literal">rootfs</code> file, and the <code class="literal">coreos.inst.ignition_url</code> parameter value is the location of the worker Ignition config file on your HTTP Server.
										</div></dd><dt><a href="#CO84-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											If you use multiple NICs, specify a single interface in the <code class="literal">ip</code> option. For example, to use DHCP on a NIC that is named <code class="literal">eno1</code>, set <code class="literal">ip=eno1:dhcp</code>.
										</div></dd><dt><a href="#CO84-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Specify the location of the <code class="literal">initramfs</code> file that you uploaded to your TFTP server.
										</div></dd></dl></div></li></ul></div></li><li class="listitem">
							Use the PXE or iPXE infrastructure to create the required compute machines for your cluster.
						</li></ol></div></section><section class="section" id="installation-approve-csrs_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.2.4. Approving the certificate signing requests for your machines</h3></div></div></div><p>
					When you add machines to a cluster, two pending certificate signing requests (CSRs) are generated for each machine that you added. You must confirm that these CSRs are approved or, if necessary, approve them yourself. The client requests must be approved first, followed by the server requests.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You added machines to your cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Confirm that the cluster recognizes the machines:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  63m  v1.26.0
master-1  Ready     master  63m  v1.26.0
master-2  Ready     master  64m  v1.26.0</pre>

							</p></div><p class="simpara">
							The output lists all of the machines that you created.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The preceding output might not include the compute nodes, also known as worker nodes, until some CSRs are approved.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Review the pending CSRs and ensure that you see the client requests with the <code class="literal">Pending</code> or <code class="literal">Approved</code> status for each machine that you added to the cluster:
						</p><pre class="programlisting language-terminal">$ oc get csr</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-8b2br   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-8vnps   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
...</pre>

							</p></div><p class="simpara">
							In this example, two machines are joining the cluster. You might see more approved CSRs in the list.
						</p></li><li class="listitem"><p class="simpara">
							If the CSRs were not approved, after all of the pending CSRs for the machines you added are in <code class="literal">Pending</code> status, approve the CSRs for your cluster machines:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Because the CSRs rotate automatically, approve your CSRs within an hour of adding the machines to the cluster. If you do not approve them within an hour, the certificates will rotate, and more than two certificates will be present for each node. You must approve all of these certificates. After the client CSR is approved, the Kubelet creates a secondary CSR for the serving certificate, which requires manual approval. Then, subsequent serving certificate renewal requests are automatically approved by the <code class="literal">machine-approver</code> if the Kubelet requests a new certificate with identical parameters.
							</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								For clusters running on platforms that are not machine API enabled, such as bare metal and other user-provisioned infrastructure, you must implement a method of automatically approving the kubelet serving certificate requests (CSRs). If a request is not approved, then the <code class="literal">oc exec</code>, <code class="literal">oc rsh</code>, and <code class="literal">oc logs</code> commands cannot succeed, because a serving certificate is required when the API server connects to the kubelet. Any operation that contacts the Kubelet endpoint requires this certificate approval to be in place. The method must watch for new CSRs, confirm that the CSR was submitted by the <code class="literal">node-bootstrapper</code> service account in the <code class="literal">system:node</code> or <code class="literal">system:admin</code> groups, and confirm the identity of the node.
							</p></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To approve them individually, run the following command for each valid CSR:
								</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt; <span id="CO85-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO85-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											<code class="literal">&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To approve all pending CSRs, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs --no-run-if-empty oc adm certificate approve</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Some Operators might not become available until some CSRs are approved.
									</p></div></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Now that your client requests are approved, you must review the server requests for each machine that you added to the cluster:
						</p><pre class="programlisting language-terminal">$ oc get csr</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-bfd72   5m26s   system:node:ip-10-0-50-126.us-east-2.compute.internal                       Pending
csr-c57lv   5m26s   system:node:ip-10-0-95-157.us-east-2.compute.internal                       Pending
...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							If the remaining CSRs are not approved, and are in the <code class="literal">Pending</code> status, approve the CSRs for your cluster machines:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To approve them individually, run the following command for each valid CSR:
								</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt; <span id="CO86-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO86-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											<code class="literal">&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To approve all pending CSRs, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs oc adm certificate approve</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							After all client and server CSRs have been approved, the machines have the <code class="literal">Ready</code> status. Verify this by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  73m  v1.26.0
master-1  Ready     master  73m  v1.26.0
master-2  Ready     master  74m  v1.26.0
worker-0  Ready     worker  11m  v1.26.0
worker-1  Ready     worker  11m  v1.26.0</pre>

							</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								It can take a few minutes after approval of the server CSRs for the machines to transition to the <code class="literal">Ready</code> status.
							</p></div></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Additional information</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							For more information on CSRs, see <a class="link" href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/">Certificate Signing Requests</a>.
						</li></ul></div></section><section class="section" id="machine-node-custom-partition_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.2.5. Adding a new RHCOS worker node with a custom <code class="literal">/var</code> partition in AWS</h3></div></div></div><p>
					OpenShift Container Platform supports partitioning devices during installation by using machine configs that are processed during the bootstrap. However, if you use <code class="literal">/var</code> partitioning, the device name must be determined at installation and cannot be changed. You cannot add different instance types as nodes if they have a different device naming schema. For example, if you configured the <code class="literal">/var</code> partition with the default AWS device name for <code class="literal">m4.large</code> instances, <code class="literal">dev/xvdb</code>, you cannot directly add an AWS <code class="literal">m5.large</code> instance, as <code class="literal">m5.large</code> instances use a <code class="literal">/dev/nvme1n1</code> device by default. The device might fail to partition due to the different naming schema.
				</p><p>
					The procedure in this section shows how to add a new Red Hat Enterprise Linux CoreOS (RHCOS) compute node with an instance that uses a different device name from what was configured at installation. You create a custom user data secret and configure a new compute machine set. These steps are specific to an AWS cluster. The principles apply to other cloud deployments also. However, the device naming schema is different for other deployments and should be determined on a per-case basis.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							On a command line, change to the <code class="literal">openshift-machine-api</code> namespace:
						</p><pre class="programlisting language-terminal">$ oc project openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
							Create a new secret from the <code class="literal">worker-user-data</code> secret:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Export the <code class="literal">userData</code> section of the secret to a text file:
								</p><pre class="programlisting language-terminal">$ oc get secret worker-user-data --template='{{index .data.userData | base64decode}}' | jq &gt; userData.txt</pre></li><li class="listitem"><p class="simpara">
									Edit the text file to add the <code class="literal">storage</code>, <code class="literal">filesystems</code>, and <code class="literal">systemd</code> stanzas for the partitions you want to use for the new node. You can specify any <a class="link" href="https://coreos.github.io/ignition/configuration-v3_2/">Ignition configuration parameters</a> as needed.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Do not change the values in the <code class="literal">ignition</code> stanza.
									</p></div></div><pre class="programlisting language-terminal">{
  "ignition": {
    "config": {
      "merge": [
        {
          "source": "https:...."
        }
      ]
    },
    "security": {
      "tls": {
        "certificateAuthorities": [
          {
            "source": "data:text/plain;charset=utf-8;base64,.....=="
          }
        ]
      }
    },
    "version": "3.2.0"
  },
  "storage": {
    "disks": [
      {
        "device": "/dev/nvme1n1", <span id="CO87-1"><!--Empty--></span><span class="callout">1</span>
        "partitions": [
          {
            "label": "var",
            "sizeMiB": 50000, <span id="CO87-2"><!--Empty--></span><span class="callout">2</span>
            "startMiB": 0 <span id="CO87-3"><!--Empty--></span><span class="callout">3</span>
          }
        ]
      }
    ],
    "filesystems": [
      {
        "device": "/dev/disk/by-partlabel/var", <span id="CO87-4"><!--Empty--></span><span class="callout">4</span>
        "format": "xfs", <span id="CO87-5"><!--Empty--></span><span class="callout">5</span>
        "path": "/var" <span id="CO87-6"><!--Empty--></span><span class="callout">6</span>
      }
    ]
  },
  "systemd": {
    "units": [ <span id="CO87-7"><!--Empty--></span><span class="callout">7</span>
      {
        "contents": "[Unit]\nBefore=local-fs.target\n[Mount]\nWhere=/var\nWhat=/dev/disk/by-partlabel/var\nOptions=defaults,pquota\n[Install]\nWantedBy=local-fs.target\n",
        "enabled": true,
        "name": "var.mount"
      }
    ]
  }
}</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO87-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specifies an absolute path to the AWS block device.
										</div></dd><dt><a href="#CO87-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Specifies the size of the data partition in Mebibytes.
										</div></dd><dt><a href="#CO87-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Specifies the start of the partition in Mebibytes. When adding a data partition to the boot disk, a minimum value of 25000 MB (Mebibytes) is recommended. The root file system is automatically resized to fill all available space up to the specified offset. If no value is specified, or if the specified value is smaller than the recommended minimum, the resulting root file system will be too small, and future reinstalls of RHCOS might overwrite the beginning of the data partition.
										</div></dd><dt><a href="#CO87-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Specifies an absolute path to the <code class="literal">/var</code> partition.
										</div></dd><dt><a href="#CO87-5"><span class="callout">5</span></a> </dt><dd><div class="para">
											Specifies the filesystem format.
										</div></dd><dt><a href="#CO87-6"><span class="callout">6</span></a> </dt><dd><div class="para">
											Specifies the mount-point of the filesystem while Ignition is running relative to where the root filesystem will be mounted. This is not necessarily the same as where it should be mounted in the real root, but it is encouraged to make it the same.
										</div></dd><dt><a href="#CO87-7"><span class="callout">7</span></a> </dt><dd><div class="para">
											Defines a systemd mount unit that mounts the <code class="literal">/dev/disk/by-partlabel/var</code> device to the <code class="literal">/var</code> partition.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Extract the <code class="literal">disableTemplating</code> section from the <code class="literal">work-user-data</code> secret to a text file:
								</p><pre class="programlisting language-terminal">$ oc get secret worker-user-data --template='{{index .data.disableTemplating | base64decode}}' | jq &gt; disableTemplating.txt</pre></li><li class="listitem"><p class="simpara">
									Create the new user data secret file from the two text files. This user data secret passes the additional node partition information in the <code class="literal">userData.txt</code> file to the newly created node.
								</p><pre class="programlisting language-terminal">$ oc create secret generic worker-user-data-x5 --from-file=userData=userData.txt --from-file=disableTemplating=disableTemplating.txt</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a new compute machine set for the new node:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a new compute machine set YAML file, similar to the following, which is configured for AWS. Add the required partitions and the newly-created user data secret:
								</p><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
									Use an existing compute machine set as a template and change the parameters as needed for the new node.
								</p></div></div><pre class="programlisting language-terminal">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: auto-52-92tf4
  name: worker-us-east-2-nvme1n1 <span id="CO88-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: auto-52-92tf4
      machine.openshift.io/cluster-api-machineset: auto-52-92tf4-worker-us-east-2b
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: auto-52-92tf4
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: auto-52-92tf4-worker-us-east-2b
    spec:
      metadata: {}
      providerSpec:
        value:
          ami:
            id: ami-0c2dbd95931a
          apiVersion: awsproviderconfig.openshift.io/v1beta1
          blockDevices:
          - DeviceName: /dev/nvme1n1 <span id="CO88-2"><!--Empty--></span><span class="callout">2</span>
            ebs:
              encrypted: true
              iops: 0
              volumeSize: 120
              volumeType: gp2
          - DeviceName: /dev/nvme1n2 <span id="CO88-3"><!--Empty--></span><span class="callout">3</span>
            ebs:
              encrypted: true
              iops: 0
              volumeSize: 50
              volumeType: gp2
          credentialsSecret:
            name: aws-cloud-credentials
          deviceIndex: 0
          iamInstanceProfile:
            id: auto-52-92tf4-worker-profile
          instanceType: m6i.large
          kind: AWSMachineProviderConfig
          metadata:
            creationTimestamp: null
          placement:
            availabilityZone: us-east-2b
            region: us-east-2
          securityGroups:
          - filters:
            - name: tag:Name
              values:
              - auto-52-92tf4-worker-sg
          subnet:
            id: subnet-07a90e5db1
          tags:
          - name: kubernetes.io/cluster/auto-52-92tf4
            value: owned
          userDataSecret:
            name: worker-user-data-x5 <span id="CO88-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO88-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specifies a name for the new node.
										</div></dd><dt><a href="#CO88-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Specifies an absolute path to the AWS block device, here an encrypted EBS volume.
										</div></dd><dt><a href="#CO88-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Optional. Specifies an additional EBS volume.
										</div></dd><dt><a href="#CO88-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Specifies the user data secret file.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Create the compute machine set:
								</p><pre class="programlisting language-yaml">$ oc create -f &lt;file-name&gt;.yaml</pre><p class="simpara">
									The machines might take a few moments to become available.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Verify that the new partition and nodes are created:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Verify that the compute machine set is created:
								</p><pre class="programlisting language-terminal">$ oc get machineset</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                               DESIRED   CURRENT   READY   AVAILABLE   AGE
ci-ln-2675bt2-76ef8-bdgsc-worker-us-east-1a        1         1         1       1           124m
ci-ln-2675bt2-76ef8-bdgsc-worker-us-east-1b        2         2         2       2           124m
worker-us-east-2-nvme1n1                           1         1         1       1           2m35s <span id="CO89-1"><!--Empty--></span><span class="callout">1</span></pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO89-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											This is the new compute machine set.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Verify that the new node is created:
								</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                           STATUS   ROLES    AGE     VERSION
ip-10-0-128-78.ec2.internal    Ready    worker   117m    v1.26.0
ip-10-0-146-113.ec2.internal   Ready    master   127m    v1.26.0
ip-10-0-153-35.ec2.internal    Ready    worker   118m    v1.26.0
ip-10-0-176-58.ec2.internal    Ready    master   126m    v1.26.0
ip-10-0-217-135.ec2.internal   Ready    worker   2m57s   v1.26.0 <span id="CO90-1"><!--Empty--></span><span class="callout">1</span>
ip-10-0-225-248.ec2.internal   Ready    master   127m    v1.26.0
ip-10-0-245-59.ec2.internal    Ready    worker   116m    v1.26.0</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO90-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											This is new new node.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Verify that the custom <code class="literal">/var</code> partition is created on the new node:
								</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node-name&gt; -- chroot /host lsblk</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc debug node/ip-10-0-217-135.ec2.internal -- chroot /host lsblk</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME        MAJ:MIN  RM  SIZE RO TYPE MOUNTPOINT
nvme0n1     202:0    0   120G  0 disk
|-nvme0n1p1 202:1    0     1M  0 part
|-nvme0n1p2 202:2    0   127M  0 part
|-nvme0n1p3 202:3    0   384M  0 part /boot
`-nvme0n1p4 202:4    0 119.5G  0 part /sysroot
nvme1n1     202:16   0    50G  0 disk
`-nvme1n1p1 202:17   0  48.8G  0 part /var <span id="CO91-1"><!--Empty--></span><span class="callout">1</span></pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO91-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The <code class="literal">nvme1n1</code> device is mounted to the <code class="literal">/var</code> partition.
										</div></dd></dl></div></li></ol></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For more information on how OpenShift Container Platform uses disk partitioning, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-user-infra-machines-advanced_disk_installing-bare-metal">Disk partitioning</a>.
						</li></ul></div></section></section><section class="section" id="post-installation-config-deploying-machine-health-checks"><div class="titlepage"><div><div><h2 class="title">8.3. Deploying machine health checks</h2></div></div></div><p>
				Understand and deploy machine health checks.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.
				</p><p>
					Clusters with the infrastructure platform type <code class="literal">none</code> cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.
				</p><p>
					To view the platform type for your cluster, run the following command:
				</p><pre class="programlisting language-terminal">$ oc get infrastructure cluster -o jsonpath='{.status.platform}'</pre></div></div><section class="section" id="machine-health-checks-about_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.3.1. About machine health checks</h3></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can only apply a machine health check to control plane machines on clusters that use control plane machine sets.
					</p></div></div><p>
					To monitor machine health, create a resource to define the configuration for a controller. Set a condition to check, such as staying in the <code class="literal">NotReady</code> status for five minutes or displaying a permanent condition in the node-problem-detector, and a label for the set of machines to monitor.
				</p><p>
					The controller that observes a <code class="literal">MachineHealthCheck</code> resource checks for the defined condition. If a machine fails the health check, the machine is automatically deleted and one is created to take its place. When a machine is deleted, you see a <code class="literal">machine deleted</code> event.
				</p><p>
					To limit disruptive impact of the machine deletion, the controller drains and deletes only one node at a time. If there are more unhealthy machines than the <code class="literal">maxUnhealthy</code> threshold allows for in the targeted pool of machines, remediation stops and therefore enables manual intervention.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Consider the timeouts carefully, accounting for workloads and requirements.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Long timeouts can result in long periods of downtime for the workload on the unhealthy machine.
							</li><li class="listitem">
								Too short timeouts can result in a remediation loop. For example, the timeout for checking the <code class="literal">NotReady</code> status must be long enough to allow the machine to complete the startup process.
							</li></ul></div></div></div><p>
					To stop the check, remove the resource.
				</p><section class="section" id="machine-health-checks-limitations_post-install-node-tasks"><div class="titlepage"><div><div><h4 class="title">8.3.1.1. Limitations when deploying machine health checks</h4></div></div></div><p>
						There are limitations to consider before deploying a machine health check:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Only machines owned by a machine set are remediated by a machine health check.
							</li><li class="listitem">
								If the node for a machine is removed from the cluster, a machine health check considers the machine to be unhealthy and remediates it immediately.
							</li><li class="listitem">
								If the corresponding node for a machine does not join the cluster after the <code class="literal">nodeStartupTimeout</code>, the machine is remediated.
							</li><li class="listitem">
								A machine is remediated immediately if the <code class="literal">Machine</code> resource phase is <code class="literal">Failed</code>.
							</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#cpmso-about">About control plane machine sets</a>
							</li></ul></div></section></section><section class="section" id="machine-health-checks-resource_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.3.2. Sample MachineHealthCheck resource</h3></div></div></div><p>
					The <code class="literal">MachineHealthCheck</code> resource for all cloud-based installation types, and other than bare metal, resembles the following YAML file:
				</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: example <span id="CO92-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-machine-api
spec:
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <span id="CO92-2"><!--Empty--></span><span class="callout">2</span>
      machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <span id="CO92-3"><!--Empty--></span><span class="callout">3</span>
      machine.openshift.io/cluster-api-machineset: &lt;cluster_name&gt;-&lt;label&gt;-&lt;zone&gt; <span id="CO92-4"><!--Empty--></span><span class="callout">4</span>
  unhealthyConditions:
  - type:    "Ready"
    timeout: "300s" <span id="CO92-5"><!--Empty--></span><span class="callout">5</span>
    status: "False"
  - type:    "Ready"
    timeout: "300s" <span id="CO92-6"><!--Empty--></span><span class="callout">6</span>
    status: "Unknown"
  maxUnhealthy: "40%" <span id="CO92-7"><!--Empty--></span><span class="callout">7</span>
  nodeStartupTimeout: "10m" <span id="CO92-8"><!--Empty--></span><span class="callout">8</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO92-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Specify the name of the machine health check to deploy.
						</div></dd><dt><a href="#CO92-2"><span class="callout">2</span></a> <a href="#CO92-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Specify a label for the machine pool that you want to check.
						</div></dd><dt><a href="#CO92-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Specify the machine set to track in <code class="literal">&lt;cluster_name&gt;-&lt;label&gt;-&lt;zone&gt;</code> format. For example, <code class="literal">prod-node-us-east-1a</code>.
						</div></dd><dt><a href="#CO92-5"><span class="callout">5</span></a> <a href="#CO92-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							Specify the timeout duration for a node condition. If a condition is met for the duration of the timeout, the machine will be remediated. Long timeouts can result in long periods of downtime for a workload on an unhealthy machine.
						</div></dd><dt><a href="#CO92-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							Specify the amount of machines allowed to be concurrently remediated in the targeted pool. This can be set as a percentage or an integer. If the number of unhealthy machines exceeds the limit set by <code class="literal">maxUnhealthy</code>, remediation is not performed.
						</div></dd><dt><a href="#CO92-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							Specify the timeout duration that a machine health check must wait for a node to join the cluster before a machine is determined to be unhealthy.
						</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The <code class="literal">matchLabels</code> are examples only; you must map your machine groups based on your specific needs.
					</p></div></div><section class="section" id="machine-health-checks-short-circuiting_post-install-node-tasks"><div class="titlepage"><div><div><h4 class="title">8.3.2.1. Short-circuiting machine health check remediation</h4></div></div></div><p>
						Short-circuiting ensures that machine health checks remediate machines only when the cluster is healthy. Short-circuiting is configured through the <code class="literal">maxUnhealthy</code> field in the <code class="literal">MachineHealthCheck</code> resource.
					</p><p>
						If the user defines a value for the <code class="literal">maxUnhealthy</code> field, before remediating any machines, the <code class="literal">MachineHealthCheck</code> compares the value of <code class="literal">maxUnhealthy</code> with the number of machines within its target pool that it has determined to be unhealthy. Remediation is not performed if the number of unhealthy machines exceeds the <code class="literal">maxUnhealthy</code> limit.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							If <code class="literal">maxUnhealthy</code> is not set, the value defaults to <code class="literal">100%</code> and the machines are remediated regardless of the state of the cluster.
						</p></div></div><p>
						The appropriate <code class="literal">maxUnhealthy</code> value depends on the scale of the cluster you deploy and how many machines the <code class="literal">MachineHealthCheck</code> covers. For example, you can use the <code class="literal">maxUnhealthy</code> value to cover multiple compute machine sets across multiple availability zones so that if you lose an entire zone, your <code class="literal">maxUnhealthy</code> setting prevents further remediation within the cluster. In global Azure regions that do not have multiple availability zones, you can use availability sets to ensure high availability.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							If you configure a <code class="literal">MachineHealthCheck</code> resource for the control plane, set the value of <code class="literal">maxUnhealthy</code> to <code class="literal">1</code>.
						</p><p>
							This configuration ensures that the machine health check takes no action when multiple control plane machines appear to be unhealthy. Multiple unhealthy control plane machines can indicate that the etcd cluster is degraded or that a scaling operation to replace a failed machine is in progress.
						</p><p>
							If the etcd cluster is degraded, manual intervention might be required. If a scaling operation is in progress, the machine health check should allow it to finish.
						</p></div></div><p>
						The <code class="literal">maxUnhealthy</code> field can be set as either an integer or percentage. There are different remediation implementations depending on the <code class="literal">maxUnhealthy</code> value.
					</p><section class="section" id="setting-maxunhealthy-by-using-an-absolute-value"><div class="titlepage"><div><div><h5 class="title">8.3.2.1.1. Setting maxUnhealthy by using an absolute value</h5></div></div></div><p>
							If <code class="literal">maxUnhealthy</code> is set to <code class="literal">2</code>:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Remediation will be performed if 2 or fewer nodes are unhealthy
								</li><li class="listitem">
									Remediation will not be performed if 3 or more nodes are unhealthy
								</li></ul></div><p>
							These values are independent of how many machines are being checked by the machine health check.
						</p></section><section class="section" id="setting-maxunhealthy-by-using-percentages"><div class="titlepage"><div><div><h5 class="title">8.3.2.1.2. Setting maxUnhealthy by using percentages</h5></div></div></div><p>
							If <code class="literal">maxUnhealthy</code> is set to <code class="literal">40%</code> and there are 25 machines being checked:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Remediation will be performed if 10 or fewer nodes are unhealthy
								</li><li class="listitem">
									Remediation will not be performed if 11 or more nodes are unhealthy
								</li></ul></div><p>
							If <code class="literal">maxUnhealthy</code> is set to <code class="literal">40%</code> and there are 6 machines being checked:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Remediation will be performed if 2 or fewer nodes are unhealthy
								</li><li class="listitem">
									Remediation will not be performed if 3 or more nodes are unhealthy
								</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The allowed number of machines is rounded down when the percentage of <code class="literal">maxUnhealthy</code> machines that are checked is not a whole number.
							</p></div></div></section></section></section><section class="section" id="machine-health-checks-creating_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.3.3. Creating a machine health check resource</h3></div></div></div><p>
					You can create a <code class="literal">MachineHealthCheck</code> resource for machine sets in your cluster.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can only apply a machine health check to control plane machines on clusters that use control plane machine sets.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the <code class="literal">oc</code> command line interface.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Create a <code class="literal">healthcheck.yml</code> file that contains the definition of your machine health check.
						</li><li class="listitem"><p class="simpara">
							Apply the <code class="literal">healthcheck.yml</code> file to your cluster:
						</p><pre class="programlisting language-terminal">$ oc apply -f healthcheck.yml</pre></li></ol></div></section><section class="section" id="machineset-manually-scaling_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.3.4. Scaling a compute machine set manually</h3></div></div></div><p>
					To add or remove an instance of a machine in a compute machine set, you can manually scale the compute machine set.
				</p><p>
					This guidance is relevant to fully automated, installer-provisioned infrastructure installations. Customized, user-provisioned infrastructure installations do not have compute machine sets.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install an OpenShift Container Platform cluster and the <code class="literal">oc</code> command line.
						</li><li class="listitem">
							Log in to <code class="literal">oc</code> as a user with <code class="literal">cluster-admin</code> permission.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the compute machine sets that are in the cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><p class="simpara">
							The compute machine sets are listed in the form of <code class="literal">&lt;clusterid&gt;-worker-&lt;aws-region-az&gt;</code>.
						</p></li><li class="listitem"><p class="simpara">
							View the compute machines that are in the cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machine -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
							Set the annotation on the compute machine that you want to delete by running the following command:
						</p><pre class="programlisting language-terminal">$ oc annotate machine/&lt;machine_name&gt; -n openshift-machine-api machine.openshift.io/delete-machine="true"</pre></li><li class="listitem"><p class="simpara">
							Scale the compute machine set by running one of the following commands:
						</p><pre class="programlisting language-terminal">$ oc scale --replicas=2 machineset &lt;machineset&gt; -n openshift-machine-api</pre><p class="simpara">
							Or:
						</p><pre class="programlisting language-terminal">$ oc edit machineset &lt;machineset&gt; -n openshift-machine-api</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can alternatively apply the following YAML to scale the compute machine set:
						</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: &lt;machineset&gt;
  namespace: openshift-machine-api
spec:
  replicas: 2</pre></div></div><p class="simpara">
							You can scale the compute machine set up or down. It takes several minutes for the new machines to be available.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								By default, the machine controller tries to drain the node that is backed by the machine until it succeeds. In some situations, such as with a misconfigured pod disruption budget, the drain operation might not be able to succeed. If the drain operation fails, the machine controller cannot proceed removing the machine.
							</p><p>
								You can skip draining the node by annotating <code class="literal">machine.openshift.io/exclude-node-draining</code> in a specific machine.
							</p></div></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Verify the deletion of the intended machine by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machines</pre></li></ul></div></section><section class="section" id="differences-between-machinesets-and-machineconfigpool_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.3.5. Understanding the difference between compute machine sets and the machine config pool</h3></div></div></div><p>
					<code class="literal">MachineSet</code> objects describe OpenShift Container Platform nodes with respect to the cloud or machine provider.
				</p><p>
					The <code class="literal">MachineConfigPool</code> object allows <code class="literal">MachineConfigController</code> components to define and provide the status of machines in the context of upgrades.
				</p><p>
					The <code class="literal">MachineConfigPool</code> object allows users to configure how upgrades are rolled out to the OpenShift Container Platform nodes in the machine config pool.
				</p><p>
					The <code class="literal">NodeSelector</code> object can be replaced with a reference to the <code class="literal">MachineSet</code> object.
				</p></section></section><section class="section" id="recommended-node-host-practices_post-install-node-tasks"><div class="titlepage"><div><div><h2 class="title">8.4. Recommended node host practices</h2></div></div></div><p>
				The OpenShift Container Platform node configuration file contains important options. For example, two parameters control the maximum number of pods that can be scheduled to a node: <code class="literal">podsPerCore</code> and <code class="literal">maxPods</code>.
			</p><p>
				When both options are in use, the lower of the two values limits the number of pods on a node. Exceeding these values can result in:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Increased CPU utilization.
					</li><li class="listitem">
						Slow pod scheduling.
					</li><li class="listitem">
						Potential out-of-memory scenarios, depending on the amount of memory in the node.
					</li><li class="listitem">
						Exhausting the pool of IP addresses.
					</li><li class="listitem">
						Resource overcommitting, leading to poor user application performance.
					</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					In Kubernetes, a pod that is holding a single container actually uses two containers. The second container is used to set up networking prior to the actual container starting. Therefore, a system running 10 pods will actually have 20 containers running.
				</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Disk IOPS throttling from the cloud provider might have an impact on CRI-O and kubelet. They might get overloaded when there are large number of I/O intensive pods running on the nodes. It is recommended that you monitor the disk I/O on the nodes and use volumes with sufficient throughput for the workload.
				</p></div></div><p>
				<code class="literal">podsPerCore</code> sets the number of pods the node can run based on the number of processor cores on the node. For example, if <code class="literal">podsPerCore</code> is set to <code class="literal">10</code> on a node with 4 processor cores, the maximum number of pods allowed on the node will be <code class="literal">40</code>.
			</p><pre class="programlisting language-yaml">kubeletConfig:
  podsPerCore: 10</pre><p>
				Setting <code class="literal">podsPerCore</code> to <code class="literal">0</code> disables this limit. The default is <code class="literal">0</code>. <code class="literal">podsPerCore</code> cannot exceed <code class="literal">maxPods</code>.
			</p><p>
				<code class="literal">maxPods</code> sets the number of pods the node can run to a fixed value, regardless of the properties of the node.
			</p><pre class="programlisting language-yaml"> kubeletConfig:
    maxPods: 250</pre><section class="section" id="create-a-kubeletconfig-crd-to-edit-kubelet-parameters_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.4.1. Creating a KubeletConfig CRD to edit kubelet parameters</h3></div></div></div><p>
					The kubelet configuration is currently serialized as an Ignition configuration, so it can be directly edited. However, there is also a new <code class="literal">kubelet-config-controller</code> added to the Machine Config Controller (MCC). This lets you use a <code class="literal">KubeletConfig</code> custom resource (CR) to edit the kubelet parameters.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						As the fields in the <code class="literal">kubeletConfig</code> object are passed directly to the kubelet from upstream Kubernetes, the kubelet validates those values directly. Invalid values in the <code class="literal">kubeletConfig</code> object might cause cluster nodes to become unavailable. For valid values, see the <a class="link" href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/">Kubernetes documentation</a>.
					</p></div></div><p>
					Consider the following guidance:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Create one <code class="literal">KubeletConfig</code> CR for each machine config pool with all the config changes you want for that pool. If you are applying the same content to all of the pools, you need only one <code class="literal">KubeletConfig</code> CR for all of the pools.
						</li><li class="listitem">
							Edit an existing <code class="literal">KubeletConfig</code> CR to modify existing settings or add new settings, instead of creating a CR for each change. It is recommended that you create a CR only to modify a different machine config pool, or for changes that are intended to be temporary, so that you can revert the changes.
						</li><li class="listitem">
							As needed, create multiple <code class="literal">KubeletConfig</code> CRs with a limit of 10 per cluster. For the first <code class="literal">KubeletConfig</code> CR, the Machine Config Operator (MCO) creates a machine config appended with <code class="literal">kubelet</code>. With each subsequent CR, the controller creates another <code class="literal">kubelet</code> machine config with a numeric suffix. For example, if you have a <code class="literal">kubelet</code> machine config with a <code class="literal">-2</code> suffix, the next <code class="literal">kubelet</code> machine config is appended with <code class="literal">-3</code>.
						</li></ul></div><p>
					If you want to delete the machine configs, delete them in reverse order to avoid exceeding the limit. For example, you delete the <code class="literal">kubelet-3</code> machine config before deleting the <code class="literal">kubelet-2</code> machine config.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you have a machine config with a <code class="literal">kubelet-9</code> suffix, and you create another <code class="literal">KubeletConfig</code> CR, a new machine config is not created, even if there are fewer than 10 <code class="literal">kubelet</code> machine configs.
					</p></div></div><div class="formalpara"><p class="title"><strong>Example <code class="literal">KubeletConfig</code> CR</strong></p><p>
						
<pre class="programlisting language-terminal">$ oc get kubeletconfig</pre>

					</p></div><pre class="programlisting language-terminal">NAME                AGE
set-max-pods        15m</pre><div class="formalpara"><p class="title"><strong>Example showing a <code class="literal">KubeletConfig</code> machine config</strong></p><p>
						
<pre class="programlisting language-terminal">$ oc get mc | grep kubelet</pre>

					</p></div><pre class="programlisting language-terminal">...
99-worker-generated-kubelet-1                  b5c5119de007945b6fe6fb215db3b8e2ceb12511   3.2.0             26m
...</pre><p>
					The following procedure is an example to show how to configure the maximum number of pods per node on the worker nodes.
				</p><div class="orderedlist"><p class="title"><strong>Prerequisites</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the label associated with the static <code class="literal">MachineConfigPool</code> CR for the type of node you want to configure. Perform one of the following steps:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									View the machine config pool:
								</p><pre class="programlisting language-terminal">$ oc describe machineconfigpool &lt;name&gt;</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc describe machineconfigpool worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: 2019-02-08T14:52:39Z
  generation: 1
  labels:
    custom-kubelet: set-max-pods <span id="CO93-1"><!--Empty--></span><span class="callout">1</span></pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO93-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											If a label has been added it appears under <code class="literal">labels</code>.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									If the label is not present, add a key/value pair:
								</p><pre class="programlisting language-terminal">$ oc label machineconfigpool worker custom-kubelet=set-max-pods</pre></li></ol></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the available machine configuration objects that you can select:
						</p><pre class="programlisting language-terminal">$ oc get machineconfig</pre><p class="simpara">
							By default, the two kubelet-related configs are <code class="literal">01-master-kubelet</code> and <code class="literal">01-worker-kubelet</code>.
						</p></li><li class="listitem"><p class="simpara">
							Check the current value for the maximum pods per node:
						</p><pre class="programlisting language-terminal">$ oc describe node &lt;node_name&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc describe node ci-ln-5grqprb-f76d1-ncnqq-worker-a-mdv94</pre><p class="simpara">
							Look for <code class="literal">value: pods: &lt;value&gt;</code> in the <code class="literal">Allocatable</code> stanza:
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Allocatable:
 attachable-volumes-aws-ebs:  25
 cpu:                         3500m
 hugepages-1Gi:               0
 hugepages-2Mi:               0
 memory:                      15341844Ki
 pods:                        250</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Set the maximum pods per node on the worker nodes by creating a custom resource file that contains the kubelet configuration:
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-max-pods
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: set-max-pods <span id="CO94-1"><!--Empty--></span><span class="callout">1</span>
  kubeletConfig:
    maxPods: 500 <span id="CO94-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO94-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Enter the label from the machine config pool.
								</div></dd><dt><a href="#CO94-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Add the kubelet configuration. In this example, use <code class="literal">maxPods</code> to set the maximum pods per node.
								</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The rate at which the kubelet talks to the API server depends on queries per second (QPS) and burst values. The default values, <code class="literal">50</code> for <code class="literal">kubeAPIQPS</code> and <code class="literal">100</code> for <code class="literal">kubeAPIBurst</code>, are sufficient if there are limited pods running on each node. It is recommended to update the kubelet QPS and burst rates if there are enough CPU and memory resources on the node.
							</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-max-pods
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: set-max-pods
  kubeletConfig:
    maxPods: &lt;pod_count&gt;
    kubeAPIBurst: &lt;burst_rate&gt;
    kubeAPIQPS: &lt;QPS&gt;</pre></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Update the machine config pool for workers with the label:
								</p><pre class="programlisting language-terminal">$ oc label machineconfigpool worker custom-kubelet=set-max-pods</pre></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">KubeletConfig</code> object:
								</p><pre class="programlisting language-terminal">$ oc create -f change-maxPods-cr.yaml</pre></li><li class="listitem"><p class="simpara">
									Verify that the <code class="literal">KubeletConfig</code> object is created:
								</p><pre class="programlisting language-terminal">$ oc get kubeletconfig</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                AGE
set-max-pods        15m</pre>

									</p></div><p class="simpara">
									Depending on the number of worker nodes in the cluster, wait for the worker nodes to be rebooted one by one. For a cluster with 3 worker nodes, this could take about 10 to 15 minutes.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Verify that the changes are applied to the node:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Check on a worker node that the <code class="literal">maxPods</code> value changed:
								</p><pre class="programlisting language-terminal">$ oc describe node &lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
									Locate the <code class="literal">Allocatable</code> stanza:
								</p><pre class="programlisting language-terminal"> ...
Allocatable:
  attachable-volumes-gce-pd:  127
  cpu:                        3500m
  ephemeral-storage:          123201474766
  hugepages-1Gi:              0
  hugepages-2Mi:              0
  memory:                     14225400Ki
  pods:                       500 <span id="CO95-1"><!--Empty--></span><span class="callout">1</span>
 ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO95-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											In this example, the <code class="literal">pods</code> parameter should report the value you set in the <code class="literal">KubeletConfig</code> object.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Verify the change in the <code class="literal">KubeletConfig</code> object:
						</p><pre class="programlisting language-terminal">$ oc get kubeletconfigs set-max-pods -o yaml</pre><p class="simpara">
							This should show a status of <code class="literal">True</code> and <code class="literal">type:Success</code>, as shown in the following example:
						</p><pre class="programlisting language-yaml">spec:
  kubeletConfig:
    maxPods: 500
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: set-max-pods
status:
  conditions:
  - lastTransitionTime: "2021-06-30T17:04:07Z"
    message: Success
    status: "True"
    type: Success</pre></li></ol></div></section><section class="section" id="modify-unavailable-workers_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.4.2. Modifying the number of unavailable worker nodes</h3></div></div></div><p>
					By default, only one machine is allowed to be unavailable when applying the kubelet-related configuration to the available worker nodes. For a large cluster, it can take a long time for the configuration change to be reflected. At any time, you can adjust the number of machines that are updating to speed up the process.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">worker</code> machine config pool:
						</p><pre class="programlisting language-terminal">$ oc edit machineconfigpool worker</pre></li><li class="listitem"><p class="simpara">
							Add the <code class="literal">maxUnavailable</code> field and set the value:
						</p><pre class="programlisting language-yaml">spec:
  maxUnavailable: &lt;node_count&gt;</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								When setting the value, consider the number of worker nodes that can be unavailable without affecting the applications running on the cluster.
							</p></div></div></li></ol></div></section><section class="section" id="master-node-sizing_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.4.3. Control plane node sizing</h3></div></div></div><p>
					The control plane node resource requirements depend on the number and type of nodes and objects in the cluster. The following control plane node size recommendations are based on the results of a control plane density focused testing, or <span class="emphasis"><em>Cluster-density</em></span>. This test creates the following objects across a given number of namespaces:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							1 image stream
						</li><li class="listitem">
							1 build
						</li><li class="listitem">
							5 deployments, with 2 pod replicas in a <code class="literal">sleep</code> state, mounting 4 secrets, 4 config maps, and 1 downward API volume each
						</li><li class="listitem">
							5 services, each one pointing to the TCP/8080 and TCP/8443 ports of one of the previous deployments
						</li><li class="listitem">
							1 route pointing to the first of the previous services
						</li><li class="listitem">
							10 secrets containing 2048 random string characters
						</li><li class="listitem">
							10 config maps containing 2048 random string characters
						</li></ul></div><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031635866608" scope="col">Number of worker nodes</th><th align="left" valign="top" id="idm140031635865520" scope="col">Cluster-density (namespaces)</th><th align="left" valign="top" id="idm140031635864416" scope="col">CPU cores</th><th align="left" valign="top" id="idm140031635863328" scope="col">Memory (GB)</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031635866608"> <p>
									24
								</p>
								 </td><td align="left" valign="top" headers="idm140031635865520"> <p>
									500
								</p>
								 </td><td align="left" valign="top" headers="idm140031635864416"> <p>
									4
								</p>
								 </td><td align="left" valign="top" headers="idm140031635863328"> <p>
									16
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635866608"> <p>
									120
								</p>
								 </td><td align="left" valign="top" headers="idm140031635865520"> <p>
									1000
								</p>
								 </td><td align="left" valign="top" headers="idm140031635864416"> <p>
									8
								</p>
								 </td><td align="left" valign="top" headers="idm140031635863328"> <p>
									32
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635866608"> <p>
									252
								</p>
								 </td><td align="left" valign="top" headers="idm140031635865520"> <p>
									4000
								</p>
								 </td><td align="left" valign="top" headers="idm140031635864416"> <p>
									16, but 24 if using the OVN-Kubernetes network plug-in
								</p>
								 </td><td align="left" valign="top" headers="idm140031635863328"> <p>
									64, but 128 if using the OVN-Kubernetes network plug-in
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635866608"> <p>
									501, but untested with the OVN-Kubernetes network plug-in
								</p>
								 </td><td align="left" valign="top" headers="idm140031635865520"> <p>
									4000
								</p>
								 </td><td align="left" valign="top" headers="idm140031635864416"> <p>
									16
								</p>
								 </td><td align="left" valign="top" headers="idm140031635863328"> <p>
									96
								</p>
								 </td></tr></tbody></table></div><p>
					The data from the table above is based on an OpenShift Container Platform running on top of AWS, using r5.4xlarge instances as control-plane nodes and m5.2xlarge instances as worker nodes.
				</p><p>
					On a large and dense cluster with three control plane nodes, the CPU and memory usage will spike up when one of the nodes is stopped, rebooted, or fails. The failures can be due to unexpected issues with power, network, underlying infrastructure, or intentional cases where the cluster is restarted after shutting it down to save costs. The remaining two control plane nodes must handle the load in order to be highly available, which leads to increase in the resource usage. This is also expected during upgrades because the control plane nodes are cordoned, drained, and rebooted serially to apply the operating system updates, as well as the control plane Operators update. To avoid cascading failures, keep the overall CPU and memory resource usage on the control plane nodes to at most 60% of all available capacity to handle the resource usage spikes. Increase the CPU and memory on the control plane nodes accordingly to avoid potential downtime due to lack of resources.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The node sizing varies depending on the number of nodes and object counts in the cluster. It also depends on whether the objects are actively being created on the cluster. During object creation, the control plane is more active in terms of resource usage compared to when the objects are in the <code class="literal">running</code> phase.
					</p></div></div><p>
					Operator Lifecycle Manager (OLM ) runs on the control plane nodes and its memory footprint depends on the number of namespaces and user installed operators that OLM needs to manage on the cluster. Control plane nodes need to be sized accordingly to avoid OOM kills. Following data points are based on the results from cluster maximums testing.
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031634451472" scope="col">Number of namespaces</th><th align="left" valign="top" id="idm140031634450384" scope="col">OLM memory at idle state (GB)</th><th align="left" valign="top" id="idm140031634449280" scope="col">OLM memory with 5 user operators installed (GB)</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031634451472"> <p>
									500
								</p>
								 </td><td align="left" valign="top" headers="idm140031634450384"> <p>
									0.823
								</p>
								 </td><td align="left" valign="top" headers="idm140031634449280"> <p>
									1.7
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031634451472"> <p>
									1000
								</p>
								 </td><td align="left" valign="top" headers="idm140031634450384"> <p>
									1.2
								</p>
								 </td><td align="left" valign="top" headers="idm140031634449280"> <p>
									2.5
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031634451472"> <p>
									1500
								</p>
								 </td><td align="left" valign="top" headers="idm140031634450384"> <p>
									1.7
								</p>
								 </td><td align="left" valign="top" headers="idm140031634449280"> <p>
									3.2
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031634451472"> <p>
									2000
								</p>
								 </td><td align="left" valign="top" headers="idm140031634450384"> <p>
									2
								</p>
								 </td><td align="left" valign="top" headers="idm140031634449280"> <p>
									4.4
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031634451472"> <p>
									3000
								</p>
								 </td><td align="left" valign="top" headers="idm140031634450384"> <p>
									2.7
								</p>
								 </td><td align="left" valign="top" headers="idm140031634449280"> <p>
									5.6
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031634451472"> <p>
									4000
								</p>
								 </td><td align="left" valign="top" headers="idm140031634450384"> <p>
									3.8
								</p>
								 </td><td align="left" valign="top" headers="idm140031634449280"> <p>
									7.6
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031634451472"> <p>
									5000
								</p>
								 </td><td align="left" valign="top" headers="idm140031634450384"> <p>
									4.2
								</p>
								 </td><td align="left" valign="top" headers="idm140031634449280"> <p>
									9.02
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031634451472"> <p>
									6000
								</p>
								 </td><td align="left" valign="top" headers="idm140031634450384"> <p>
									5.8
								</p>
								 </td><td align="left" valign="top" headers="idm140031634449280"> <p>
									11.3
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031634451472"> <p>
									7000
								</p>
								 </td><td align="left" valign="top" headers="idm140031634450384"> <p>
									6.6
								</p>
								 </td><td align="left" valign="top" headers="idm140031634449280"> <p>
									12.9
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031634451472"> <p>
									8000
								</p>
								 </td><td align="left" valign="top" headers="idm140031634450384"> <p>
									6.9
								</p>
								 </td><td align="left" valign="top" headers="idm140031634449280"> <p>
									14.8
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031634451472"> <p>
									9000
								</p>
								 </td><td align="left" valign="top" headers="idm140031634450384"> <p>
									8
								</p>
								 </td><td align="left" valign="top" headers="idm140031634449280"> <p>
									17.7
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031634451472"> <p>
									10,000
								</p>
								 </td><td align="left" valign="top" headers="idm140031634450384"> <p>
									9.9
								</p>
								 </td><td align="left" valign="top" headers="idm140031634449280"> <p>
									21.6
								</p>
								 </td></tr></tbody></table></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						You can modify the control plane node size in a running OpenShift Container Platform 4.13 cluster for the following configurations only:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Clusters installed with a user-provisioned installation method.
							</li><li class="listitem">
								AWS clusters installed with an installer-provisioned infrastructure installation method.
							</li><li class="listitem">
								Clusters that use a control plane machine set to manage control plane machines.
							</li></ul></div><p>
						For all other configurations, you must estimate your total node count and use the suggested control plane node size during installation.
					</p></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The recommendations are based on the data points captured on OpenShift Container Platform clusters with OpenShift SDN as the network plugin.
					</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						In OpenShift Container Platform 4.13, half of a CPU core (500 millicore) is now reserved by the system by default compared to OpenShift Container Platform 3.11 and previous versions. The sizes are determined taking that into consideration.
					</p></div></div></section><section class="section" id="seting_up_cpu_manager_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.4.4. Setting up CPU Manager</h3></div></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Optional: Label a node:
						</p><pre class="programlisting language-terminal"># oc label node perf-node.example.com cpumanager=true</pre></li><li class="listitem"><p class="simpara">
							Edit the <code class="literal">MachineConfigPool</code> of the nodes where CPU Manager should be enabled. In this example, all workers have CPU Manager enabled:
						</p><pre class="programlisting language-terminal"># oc edit machineconfigpool worker</pre></li><li class="listitem"><p class="simpara">
							Add a label to the worker machine config pool:
						</p><pre class="programlisting language-yaml">metadata:
  creationTimestamp: 2020-xx-xxx
  generation: 3
  labels:
    custom-kubelet: cpumanager-enabled</pre></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">KubeletConfig</code>, <code class="literal">cpumanager-kubeletconfig.yaml</code>, custom resource (CR). Refer to the label created in the previous step to have the correct nodes updated with the new kubelet config. See the <code class="literal">machineConfigPoolSelector</code> section:
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: cpumanager-enabled
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: cpumanager-enabled
  kubeletConfig:
     cpuManagerPolicy: static <span id="CO96-1"><!--Empty--></span><span class="callout">1</span>
     cpuManagerReconcilePeriod: 5s <span id="CO96-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO96-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify a policy:
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">none</code>. This policy explicitly enables the existing default CPU affinity scheme, providing no affinity beyond what the scheduler does automatically. This is the default policy.
										</li><li class="listitem">
											<code class="literal">static</code>. This policy allows containers in guaranteed pods with integer CPU requests. It also limits access to exclusive CPUs on the node. If <code class="literal">static</code>, you must use a lowercase <code class="literal">s</code>.
										</li></ul></div></dd><dt><a href="#CO96-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Optional. Specify the CPU Manager reconcile frequency. The default is <code class="literal">5s</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the dynamic kubelet config:
						</p><pre class="programlisting language-terminal"># oc create -f cpumanager-kubeletconfig.yaml</pre><p class="simpara">
							This adds the CPU Manager feature to the kubelet config and, if needed, the Machine Config Operator (MCO) reboots the node. To enable CPU Manager, a reboot is not needed.
						</p></li><li class="listitem"><p class="simpara">
							Check for the merged kubelet config:
						</p><pre class="programlisting language-terminal"># oc get machineconfig 99-worker-XXXXXX-XXXXX-XXXX-XXXXX-kubelet -o json | grep ownerReference -A7</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-json">       "ownerReferences": [
            {
                "apiVersion": "machineconfiguration.openshift.io/v1",
                "kind": "KubeletConfig",
                "name": "cpumanager-enabled",
                "uid": "7ed5616d-6b72-11e9-aae1-021e1ce18878"
            }
        ]</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check the worker for the updated <code class="literal">kubelet.conf</code>:
						</p><pre class="programlisting language-terminal"># oc debug node/perf-node.example.com
sh-4.2# cat /host/etc/kubernetes/kubelet.conf | grep cpuManager</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">cpuManagerPolicy: static        <span id="CO97-1"><!--Empty--></span><span class="callout">1</span>
cpuManagerReconcilePeriod: 5s   <span id="CO97-2"><!--Empty--></span><span class="callout">2</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO97-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal">cpuManagerPolicy</code> is defined when you create the <code class="literal">KubeletConfig</code> CR.
								</div></dd><dt><a href="#CO97-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									<code class="literal">cpuManagerReconcilePeriod</code> is defined when you create the <code class="literal">KubeletConfig</code> CR.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create a pod that requests a core or multiple cores. Both limits and requests must have their CPU value set to a whole integer. That is the number of cores that will be dedicated to this pod:
						</p><pre class="programlisting language-terminal"># cat cpumanager-pod.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  generateName: cpumanager-
spec:
  containers:
  - name: cpumanager
    image: gcr.io/google_containers/pause-amd64:3.0
    resources:
      requests:
        cpu: 1
        memory: "1G"
      limits:
        cpu: 1
        memory: "1G"
  nodeSelector:
    cpumanager: "true"</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create the pod:
						</p><pre class="programlisting language-terminal"># oc create -f cpumanager-pod.yaml</pre></li><li class="listitem"><p class="simpara">
							Verify that the pod is scheduled to the node that you labeled:
						</p><pre class="programlisting language-terminal"># oc describe pod cpumanager</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Name:               cpumanager-6cqz7
Namespace:          default
Priority:           0
PriorityClassName:  &lt;none&gt;
Node:  perf-node.example.com/xxx.xx.xx.xxx
...
 Limits:
      cpu:     1
      memory:  1G
    Requests:
      cpu:        1
      memory:     1G
...
QoS Class:       Guaranteed
Node-Selectors:  cpumanager=true</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify that the <code class="literal">cgroups</code> are set up correctly. Get the process ID (PID) of the <code class="literal">pause</code> process:
						</p><pre class="programlisting language-terminal"># ├─init.scope
│ └─1 /usr/lib/systemd/systemd --switched-root --system --deserialize 17
└─kubepods.slice
  ├─kubepods-pod69c01f8e_6b74_11e9_ac0f_0a2b62178a22.slice
  │ ├─crio-b5437308f1a574c542bdf08563b865c0345c8f8c0b0a655612c.scope
  │ └─32706 /pause</pre><p class="simpara">
							Pods of quality of service (QoS) tier <code class="literal">Guaranteed</code> are placed within the <code class="literal">kubepods.slice</code>. Pods of other QoS tiers end up in child <code class="literal">cgroups</code> of <code class="literal">kubepods</code>:
						</p><pre class="programlisting language-terminal"># cd /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-pod69c01f8e_6b74_11e9_ac0f_0a2b62178a22.slice/crio-b5437308f1ad1a7db0574c542bdf08563b865c0345c86e9585f8c0b0a655612c.scope
# for i in `ls cpuset.cpus tasks` ; do echo -n "$i "; cat $i ; done</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">cpuset.cpus 1
tasks 32706</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check the allowed CPU list for the task:
						</p><pre class="programlisting language-terminal"># grep ^Cpus_allowed_list /proc/32706/status</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal"> Cpus_allowed_list:    1</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify that another pod (in this case, the pod in the <code class="literal">burstable</code> QoS tier) on the system cannot run on the core allocated for the <code class="literal">Guaranteed</code> pod:
						</p><pre class="programlisting language-terminal"># cat /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-podc494a073_6b77_11e9_98c0_06bba5c387ea.slice/crio-c56982f57b75a2420947f0afc6cafe7534c5734efc34157525fa9abbf99e3849.scope/cpuset.cpus
0
# oc describe node perf-node.example.com</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">...
Capacity:
 attachable-volumes-aws-ebs:  39
 cpu:                         2
 ephemeral-storage:           124768236Ki
 hugepages-1Gi:               0
 hugepages-2Mi:               0
 memory:                      8162900Ki
 pods:                        250
Allocatable:
 attachable-volumes-aws-ebs:  39
 cpu:                         1500m
 ephemeral-storage:           124768236Ki
 hugepages-1Gi:               0
 hugepages-2Mi:               0
 memory:                      7548500Ki
 pods:                        250
-------                               ----                           ------------  ----------  ---------------  -------------  ---
  default                                 cpumanager-6cqz7               1 (66%)       1 (66%)     1G (12%)         1G (12%)       29m

Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource                    Requests          Limits
  --------                    --------          ------
  cpu                         1440m (96%)       1 (66%)</pre>

							</p></div><p class="simpara">
							This VM has two CPU cores. The <code class="literal">system-reserved</code> setting reserves 500 millicores, meaning that half of one core is subtracted from the total capacity of the node to arrive at the <code class="literal">Node Allocatable</code> amount. You can see that <code class="literal">Allocatable CPU</code> is 1500 millicores. This means you can run one of the CPU Manager pods since each will take one whole core. A whole core is equivalent to 1000 millicores. If you try to schedule a second pod, the system will accept the pod, but it will never be scheduled:
						</p><pre class="programlisting language-terminal">NAME                    READY   STATUS    RESTARTS   AGE
cpumanager-6cqz7        1/1     Running   0          33m
cpumanager-7qc2t        0/1     Pending   0          11s</pre></li></ol></div></section></section><section class="section" id="post-install-huge-pages"><div class="titlepage"><div><div><h2 class="title">8.5. Huge pages</h2></div></div></div><p>
				Understand and configure huge pages.
			</p><section class="section" id="what-huge-pages-do_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.5.1. What huge pages do</h3></div></div></div><p>
					Memory is managed in blocks known as pages. On most systems, a page is 4Ki. 1Mi of memory is equal to 256 pages; 1Gi of memory is 256,000 pages, and so on. CPUs have a built-in memory management unit that manages a list of these pages in hardware. The Translation Lookaside Buffer (TLB) is a small hardware cache of virtual-to-physical page mappings. If the virtual address passed in a hardware instruction can be found in the TLB, the mapping can be determined quickly. If not, a TLB miss occurs, and the system falls back to slower, software-based address translation, resulting in performance issues. Since the size of the TLB is fixed, the only way to reduce the chance of a TLB miss is to increase the page size.
				</p><p>
					A huge page is a memory page that is larger than 4Ki. On x86_64 architectures, there are two common huge page sizes: 2Mi and 1Gi. Sizes vary on other architectures. To use huge pages, code must be written so that applications are aware of them. Transparent Huge Pages (THP) attempt to automate the management of huge pages without application knowledge, but they have limitations. In particular, they are limited to 2Mi page sizes. THP can lead to performance degradation on nodes with high memory utilization or fragmentation due to defragmenting efforts of THP, which can lock memory pages. For this reason, some applications may be designed to (or recommend) usage of pre-allocated huge pages instead of THP.
				</p></section><section class="section" id="how-huge-pages-are-consumed-by-apps_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.5.2. How huge pages are consumed by apps</h3></div></div></div><p>
					Nodes must pre-allocate huge pages in order for the node to report its huge page capacity. A node can only pre-allocate huge pages for a single size.
				</p><p>
					Huge pages can be consumed through container-level resource requirements using the resource name <code class="literal">hugepages-&lt;size&gt;</code>, where size is the most compact binary notation using integer values supported on a particular node. For example, if a node supports 2048KiB page sizes, it exposes a schedulable resource <code class="literal">hugepages-2Mi</code>. Unlike CPU or memory, huge pages do not support over-commitment.
				</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  generateName: hugepages-volume-
spec:
  containers:
  - securityContext:
      privileged: true
    image: rhel7:latest
    command:
    - sleep
    - inf
    name: example
    volumeMounts:
    - mountPath: /dev/hugepages
      name: hugepage
    resources:
      limits:
        hugepages-2Mi: 100Mi <span id="CO98-1"><!--Empty--></span><span class="callout">1</span>
        memory: "1Gi"
        cpu: "1"
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO98-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Specify the amount of memory for <code class="literal">hugepages</code> as the exact amount to be allocated. Do not specify this value as the amount of memory for <code class="literal">hugepages</code> multiplied by the size of the page. For example, given a huge page size of 2MB, if you want to use 100MB of huge-page-backed RAM for your application, then you would allocate 50 huge pages. OpenShift Container Platform handles the math for you. As in the above example, you can specify <code class="literal">100MB</code> directly.
						</div></dd></dl></div><p>
					<span class="strong strong"><strong>Allocating huge pages of a specific size</strong></span>
				</p><p>
					Some platforms support multiple huge page sizes. To allocate huge pages of a specific size, precede the huge pages boot command parameters with a huge page size selection parameter <code class="literal">hugepagesz=&lt;size&gt;</code>. The <code class="literal">&lt;size&gt;</code> value must be specified in bytes with an optional scale suffix [<code class="literal">kKmMgG</code>]. The default huge page size can be defined with the <code class="literal">default_hugepagesz=&lt;size&gt;</code> boot parameter.
				</p><p>
					<span class="strong strong"><strong>Huge page requirements</strong></span>
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Huge page requests must equal the limits. This is the default if limits are specified, but requests are not.
						</li><li class="listitem">
							Huge pages are isolated at a pod scope. Container isolation is planned in a future iteration.
						</li><li class="listitem">
							<code class="literal">EmptyDir</code> volumes backed by huge pages must not consume more huge page memory than the pod request.
						</li><li class="listitem">
							Applications that consume huge pages via <code class="literal">shmget()</code> with <code class="literal">SHM_HUGETLB</code> must run with a supplemental group that matches <span class="strong strong"><strong><span class="emphasis"><em>proc/sys/vm/hugetlb_shm_group</em></span></strong></span>.
						</li></ul></div></section><section class="section" id="configuring-huge-pages_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.5.3. Configuring huge pages at boot time</h3></div></div></div><p>
					Nodes must pre-allocate huge pages used in an OpenShift Container Platform cluster. There are two ways of reserving huge pages: at boot time and at run time. Reserving at boot time increases the possibility of success because the memory has not yet been significantly fragmented. The Node Tuning Operator currently supports boot time allocation of huge pages on specific nodes.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To minimize node reboots, the order of the steps below needs to be followed:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Label all nodes that need the same huge pages setting by a label.
						</p><pre class="programlisting language-terminal">$ oc label node &lt;node_using_hugepages&gt; node-role.kubernetes.io/worker-hp=</pre></li><li class="listitem"><p class="simpara">
							Create a file with the following content and name it <code class="literal">hugepages-tuned-boottime.yaml</code>:
						</p><pre class="programlisting language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: hugepages <span id="CO99-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile: <span id="CO99-2"><!--Empty--></span><span class="callout">2</span>
  - data: |
      [main]
      summary=Boot time configuration for hugepages
      include=openshift-node
      [bootloader]
      cmdline_openshift_node_hugepages=hugepagesz=2M hugepages=50 <span id="CO99-3"><!--Empty--></span><span class="callout">3</span>
    name: openshift-node-hugepages

  recommend:
  - machineConfigLabels: <span id="CO99-4"><!--Empty--></span><span class="callout">4</span>
      machineconfiguration.openshift.io/role: "worker-hp"
    priority: 30
    profile: openshift-node-hugepages</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO99-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Set the <code class="literal">name</code> of the Tuned resource to <code class="literal">hugepages</code>.
								</div></dd><dt><a href="#CO99-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Set the <code class="literal">profile</code> section to allocate huge pages.
								</div></dd><dt><a href="#CO99-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Note the order of parameters is important as some platforms support huge pages of various sizes.
								</div></dd><dt><a href="#CO99-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Enable machine config pool based matching.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the Tuned <code class="literal">hugepages</code> object
						</p><pre class="programlisting language-terminal">$ oc create -f hugepages-tuned-boottime.yaml</pre></li><li class="listitem"><p class="simpara">
							Create a file with the following content and name it <code class="literal">hugepages-mcp.yaml</code>:
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-hp
  labels:
    worker-hp: ""
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-hp]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-hp: ""</pre></li><li class="listitem"><p class="simpara">
							Create the machine config pool:
						</p><pre class="programlisting language-terminal">$ oc create -f hugepages-mcp.yaml</pre></li></ol></div><p>
					Given enough non-fragmented memory, all the nodes in the <code class="literal">worker-hp</code> machine config pool should now have 50 2Mi huge pages allocated.
				</p><pre class="programlisting language-terminal">$ oc get node &lt;node_using_hugepages&gt; -o jsonpath="{.status.allocatable.hugepages-2Mi}"
100Mi</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The TuneD bootloader plugin only supports Red Hat Enterprise Linux CoreOS (RHCOS) worker nodes.
					</p></div></div></section></section><section class="section" id="nodes-pods-plugins-about_post-install-node-tasks"><div class="titlepage"><div><div><h2 class="title">8.6. Understanding device plugins</h2></div></div></div><p>
				The device plugin provides a consistent and portable solution to consume hardware devices across clusters. The device plugin provides support for these devices through an extension mechanism, which makes these devices available to Containers, provides health checks of these devices, and securely shares them.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					OpenShift Container Platform supports the device plugin API, but the device plugin Containers are supported by individual vendors.
				</p></div></div><p>
				A device plugin is a gRPC service running on the nodes (external to the <code class="literal">kubelet</code>) that is responsible for managing specific hardware resources. Any device plugin must support following remote procedure calls (RPCs):
			</p><pre class="programlisting language-golang">service DevicePlugin {
      // GetDevicePluginOptions returns options to be communicated with Device
      // Manager
      rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {}

      // ListAndWatch returns a stream of List of Devices
      // Whenever a Device state change or a Device disappears, ListAndWatch
      // returns the new list
      rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}

      // Allocate is called during container creation so that the Device
      // Plug-in can run device specific operations and instruct Kubelet
      // of the steps to make the Device available in the container
      rpc Allocate(AllocateRequest) returns (AllocateResponse) {}

      // PreStartcontainer is called, if indicated by Device Plug-in during
      // registration phase, before each container start. Device plug-in
      // can run device specific operations such as reseting the device
      // before making devices available to the container
      rpc PreStartcontainer(PreStartcontainerRequest) returns (PreStartcontainerResponse) {}
}</pre><h5 id="example-device-plugins">Example device plugins</h5><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://github.com/GoogleCloudPlatform/Container-engine-accelerators/tree/master/cmd/nvidia_gpu">Nvidia GPU device plugin for COS-based operating system</a>
					</li><li class="listitem">
						<a class="link" href="https://github.com/NVIDIA/k8s-device-plugin">Nvidia official GPU device plugin</a>
					</li><li class="listitem">
						<a class="link" href="https://github.com/vikaschoudhary16/sfc-device-plugin">Solarflare device plugin</a>
					</li><li class="listitem">
						<a class="link" href="https://github.com/kubevirt/kubernetes-device-plugins">KubeVirt device plugins: vfio and kvm</a>
					</li><li class="listitem">
						<a class="link" href="https://github.com/ibm-s390-cloud/k8s-cex-dev-plugin">Kubernetes device plugin for IBM Crypto Express (CEX) cards</a>
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					For easy device plugin reference implementation, there is a stub device plugin in the Device Manager code: <span class="strong strong"><strong><span class="emphasis"><em>vendor/k8s.io/kubernetes/pkg/kubelet/cm/deviceplugin/device_plugin_stub.go</em></span></strong></span>.
				</p></div></div><section class="section" id="methods-for-deploying-a-device-plugin_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.6.1. Methods for deploying a device plugin</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Daemon sets are the recommended approach for device plugin deployments.
						</li><li class="listitem">
							Upon start, the device plugin will try to create a UNIX domain socket at <span class="strong strong"><strong><span class="emphasis"><em>/var/lib/kubelet/device-plugin/</em></span></strong></span> on the node to serve RPCs from Device Manager.
						</li><li class="listitem">
							Since device plugins must manage hardware resources, access to the host file system, as well as socket creation, they must be run in a privileged security context.
						</li><li class="listitem">
							More specific details regarding deployment steps can be found with each device plugin implementation.
						</li></ul></div></section><section class="section" id="nodes-pods-plugins-device-mgr_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.6.2. Understanding the Device Manager</h3></div></div></div><p>
					Device Manager provides a mechanism for advertising specialized node hardware resources with the help of plugins known as device plugins.
				</p><p>
					You can advertise specialized hardware without requiring any upstream code changes.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						OpenShift Container Platform supports the device plugin API, but the device plugin Containers are supported by individual vendors.
					</p></div></div><p>
					Device Manager advertises devices as <span class="strong strong"><strong>Extended Resources</strong></span>. User pods can consume devices, advertised by Device Manager, using the same <span class="strong strong"><strong>Limit/Request</strong></span> mechanism, which is used for requesting any other <span class="strong strong"><strong>Extended Resource</strong></span>.
				</p><p>
					Upon start, the device plugin registers itself with Device Manager invoking <code class="literal">Register</code> on the <span class="strong strong"><strong><span class="emphasis"><em>/var/lib/kubelet/device-plugins/kubelet.sock</em></span></strong></span> and starts a gRPC service at <span class="strong strong"><strong><span class="emphasis"><em>/var/lib/kubelet/device-plugins/&lt;plugin&gt;.sock</em></span></strong></span> for serving Device Manager requests.
				</p><p>
					Device Manager, while processing a new registration request, invokes <code class="literal">ListAndWatch</code> remote procedure call (RPC) at the device plugin service. In response, Device Manager gets a list of <span class="strong strong"><strong>Device</strong></span> objects from the plugin over a gRPC stream. Device Manager will keep watching on the stream for new updates from the plugin. On the plugin side, the plugin will also keep the stream open and whenever there is a change in the state of any of the devices, a new device list is sent to the Device Manager over the same streaming connection.
				</p><p>
					While handling a new pod admission request, Kubelet passes requested <code class="literal">Extended Resources</code> to the Device Manager for device allocation. Device Manager checks in its database to verify if a corresponding plugin exists or not. If the plugin exists and there are free allocatable devices as well as per local cache, <code class="literal">Allocate</code> RPC is invoked at that particular device plugin.
				</p><p>
					Additionally, device plugins can also perform several other device-specific operations, such as driver installation, device initialization, and device resets. These functionalities vary from implementation to implementation.
				</p></section><section class="section" id="nodes-pods-plugins-install_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.6.3. Enabling Device Manager</h3></div></div></div><p>
					Enable Device Manager to implement a device plugin to advertise specialized hardware without any upstream code changes.
				</p><p>
					Device Manager provides a mechanism for advertising specialized node hardware resources with the help of plugins known as device plugins.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the label associated with the static <code class="literal">MachineConfigPool</code> CRD for the type of node you want to configure by entering the following command. Perform one of the following steps:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									View the machine config:
								</p><pre class="programlisting language-terminal"># oc describe machineconfig &lt;name&gt;</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal"># oc describe machineconfig 00-worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">Name:         00-worker
Namespace:
Labels:       machineconfiguration.openshift.io/role=worker <span id="CO100-1"><!--Empty--></span><span class="callout">1</span></pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO100-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Label required for the Device Manager.
										</div></dd></dl></div></li></ol></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a custom resource (CR) for your configuration change.
						</p><div class="formalpara"><p class="title"><strong>Sample configuration for a Device Manager CR</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: devicemgr <span id="CO101-1"><!--Empty--></span><span class="callout">1</span>
spec:
  machineConfigPoolSelector:
    matchLabels:
       machineconfiguration.openshift.io: devicemgr <span id="CO101-2"><!--Empty--></span><span class="callout">2</span>
  kubeletConfig:
    feature-gates:
      - DevicePlugins=true <span id="CO101-3"><!--Empty--></span><span class="callout">3</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO101-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Assign a name to CR.
								</div></dd><dt><a href="#CO101-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Enter the label from the Machine Config Pool.
								</div></dd><dt><a href="#CO101-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Set <code class="literal">DevicePlugins</code> to 'true`.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the Device Manager:
						</p><pre class="programlisting language-terminal">$ oc create -f devicemgr.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">kubeletconfig.machineconfiguration.openshift.io/devicemgr created</pre>

							</p></div></li><li class="listitem">
							Ensure that Device Manager was actually enabled by confirming that <span class="strong strong"><strong><span class="emphasis"><em>/var/lib/kubelet/device-plugins/kubelet.sock</em></span></strong></span> is created on the node. This is the UNIX domain socket on which the Device Manager gRPC server listens for new plugin registrations. This sock file is created when the Kubelet is started only if Device Manager is enabled.
						</li></ol></div></section></section><section class="section" id="post-install-taints-tolerations"><div class="titlepage"><div><div><h2 class="title">8.7. Taints and tolerations</h2></div></div></div><p>
				Understand and work with taints and tolerations.
			</p><section class="section" id="nodes-scheduler-taints-tolerations-about_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.7.1. Understanding taints and tolerations</h3></div></div></div><p>
					A <span class="emphasis"><em>taint</em></span> allows a node to refuse a pod to be scheduled unless that pod has a matching <span class="emphasis"><em>toleration</em></span>.
				</p><p>
					You apply taints to a node through the <code class="literal">Node</code> specification (<code class="literal">NodeSpec</code>) and apply tolerations to a pod through the <code class="literal">Pod</code> specification (<code class="literal">PodSpec</code>). When you apply a taint a node, the scheduler cannot place a pod on that node unless the pod can tolerate the taint.
				</p><div class="formalpara"><p class="title"><strong>Example taint in a node specification</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: Node
metadata:
  name: my-node
#...
spec:
  taints:
  - effect: NoExecute
    key: key1
    value: value1
#...</pre>

					</p></div><div class="formalpara"><p class="title"><strong>Example toleration in a <code class="literal">Pod</code> spec</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoExecute"
    tolerationSeconds: 3600
#...</pre>

					</p></div><p>
					Taints and tolerations consist of a key, value, and effect.
				</p><div class="table" id="taint-components-table_post-install-node-tasks"><p class="title"><strong>Table 8.1. Taint and toleration components</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 27%; " class="col_1"><!--Empty--></col><col style="width: 73%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031636280592" scope="col">Parameter</th><th align="left" valign="top" id="idm140031636279504" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031636280592"> <p>
									<code class="literal">key</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636279504"> <p>
									The <code class="literal">key</code> is any string, up to 253 characters. The key must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031636280592"> <p>
									<code class="literal">value</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636279504"> <p>
									The <code class="literal">value</code> is any string, up to 63 characters. The value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031636280592"> <p>
									<code class="literal">effect</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636279504"> <p>
									The effect is one of the following:
								</p>
								 <div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 40%; " class="col_1"><!--Empty--></col><col style="width: 60%; " class="col_2"><!--Empty--></col></colgroup><tbody><tr><td align="left" valign="top" headers="idm140031636280592"> <p>
													<code class="literal">NoSchedule</code> <sup>[1]</sup>
												</p>
												 </td><td align="left" valign="top" headers="idm140031636279504"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
															New pods that do not match the taint are not scheduled onto that node.
														</li><li class="listitem">
															Existing pods on the node remain.
														</li></ul></div>
												 </td></tr><tr><td align="left" valign="top" headers="idm140031636280592"> <p>
													<code class="literal">PreferNoSchedule</code>
												</p>
												 </td><td align="left" valign="top" headers="idm140031636279504"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
															New pods that do not match the taint might be scheduled onto that node, but the scheduler tries not to.
														</li><li class="listitem">
															Existing pods on the node remain.
														</li></ul></div>
												 </td></tr><tr><td align="left" valign="top" headers="idm140031636280592"> <p>
													<code class="literal">NoExecute</code>
												</p>
												 </td><td align="left" valign="top" headers="idm140031636279504"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
															New pods that do not match the taint cannot be scheduled onto that node.
														</li><li class="listitem">
															Existing pods on the node that do not have a matching toleration are removed.
														</li></ul></div>
												 </td></tr></tbody></table></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031636280592"> <p>
									<code class="literal">operator</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636279504"> <div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 40%; " class="col_1"><!--Empty--></col><col style="width: 60%; " class="col_2"><!--Empty--></col></colgroup><tbody><tr><td align="left" valign="top" headers="idm140031636280592"> <p>
													<code class="literal">Equal</code>
												</p>
												 </td><td align="left" valign="top" headers="idm140031636279504"> <p>
													The <code class="literal">key</code>/<code class="literal">value</code>/<code class="literal">effect</code> parameters must match. This is the default.
												</p>
												 </td></tr><tr><td align="left" valign="top" headers="idm140031636280592"> <p>
													<code class="literal">Exists</code>
												</p>
												 </td><td align="left" valign="top" headers="idm140031636279504"> <p>
													The <code class="literal">key</code>/<code class="literal">effect</code> parameters must match. You must leave a blank <code class="literal">value</code> parameter, which matches any.
												</p>
												 </td></tr></tbody></table></div>
								 </td></tr></tbody></table></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							If you add a <code class="literal">NoSchedule</code> taint to a control plane node, the node must have the <code class="literal">node-role.kubernetes.io/master=:NoSchedule</code> taint, which is added by default.
						</p><p class="simpara">
							For example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Node
metadata:
  annotations:
    machine.openshift.io/machine: openshift-machine-api/ci-ln-62s7gtb-f76d1-v8jxv-master-0
    machineconfiguration.openshift.io/currentConfig: rendered-master-cdc1ab7da414629332cc4c3926e6e59c
  name: my-node
#...
spec:
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
#...</pre></li></ol></div><p>
					A toleration matches a taint:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							If the <code class="literal">operator</code> parameter is set to <code class="literal">Equal</code>:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									the <code class="literal">key</code> parameters are the same;
								</li><li class="listitem">
									the <code class="literal">value</code> parameters are the same;
								</li><li class="listitem">
									the <code class="literal">effect</code> parameters are the same.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							If the <code class="literal">operator</code> parameter is set to <code class="literal">Exists</code>:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									the <code class="literal">key</code> parameters are the same;
								</li><li class="listitem">
									the <code class="literal">effect</code> parameters are the same.
								</li></ul></div></li></ul></div><p>
					The following taints are built into OpenShift Container Platform:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">node.kubernetes.io/not-ready</code>: The node is not ready. This corresponds to the node condition <code class="literal">Ready=False</code>.
						</li><li class="listitem">
							<code class="literal">node.kubernetes.io/unreachable</code>: The node is unreachable from the node controller. This corresponds to the node condition <code class="literal">Ready=Unknown</code>.
						</li><li class="listitem">
							<code class="literal">node.kubernetes.io/memory-pressure</code>: The node has memory pressure issues. This corresponds to the node condition <code class="literal">MemoryPressure=True</code>.
						</li><li class="listitem">
							<code class="literal">node.kubernetes.io/disk-pressure</code>: The node has disk pressure issues. This corresponds to the node condition <code class="literal">DiskPressure=True</code>.
						</li><li class="listitem">
							<code class="literal">node.kubernetes.io/network-unavailable</code>: The node network is unavailable.
						</li><li class="listitem">
							<code class="literal">node.kubernetes.io/unschedulable</code>: The node is unschedulable.
						</li><li class="listitem">
							<code class="literal">node.cloudprovider.kubernetes.io/uninitialized</code>: When the node controller is started with an external cloud provider, this taint is set on a node to mark it as unusable. After a controller from the cloud-controller-manager initializes this node, the kubelet removes this taint.
						</li><li class="listitem"><p class="simpara">
							<code class="literal">node.kubernetes.io/pid-pressure</code>: The node has pid pressure. This corresponds to the node condition <code class="literal">PIDPressure=True</code>.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								OpenShift Container Platform does not set a default pid.available <code class="literal">evictionHard</code>.
							</p></div></div></li></ul></div><section class="section" id="nodes-scheduler-taints-tolerations-about-seconds_post-install-node-tasks"><div class="titlepage"><div><div><h4 class="title">8.7.1.1. Understanding how to use toleration seconds to delay pod evictions</h4></div></div></div><p>
						You can specify how long a pod can remain bound to a node before being evicted by specifying the <code class="literal">tolerationSeconds</code> parameter in the <code class="literal">Pod</code> specification or <code class="literal">MachineSet</code> object. If a taint with the <code class="literal">NoExecute</code> effect is added to a node, a pod that does tolerate the taint, which has the <code class="literal">tolerationSeconds</code> parameter, the pod is not evicted until that time period expires.
					</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoExecute"
    tolerationSeconds: 3600
#...</pre>

						</p></div><p>
						Here, if this pod is running but does not have a matching toleration, the pod stays bound to the node for 3,600 seconds and then be evicted. If the taint is removed before that time, the pod is not evicted.
					</p></section><section class="section" id="nodes-scheduler-taints-tolerations-about-multiple_post-install-node-tasks"><div class="titlepage"><div><div><h4 class="title">8.7.1.2. Understanding how to use multiple taints</h4></div></div></div><p>
						You can put multiple taints on the same node and multiple tolerations on the same pod. OpenShift Container Platform processes multiple taints and tolerations as follows:
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								Process the taints for which the pod has a matching toleration.
							</li><li class="listitem"><p class="simpara">
								The remaining unmatched taints have the indicated effects on the pod:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										If there is at least one unmatched taint with effect <code class="literal">NoSchedule</code>, OpenShift Container Platform cannot schedule a pod onto that node.
									</li><li class="listitem">
										If there is no unmatched taint with effect <code class="literal">NoSchedule</code> but there is at least one unmatched taint with effect <code class="literal">PreferNoSchedule</code>, OpenShift Container Platform tries to not schedule the pod onto the node.
									</li><li class="listitem"><p class="simpara">
										If there is at least one unmatched taint with effect <code class="literal">NoExecute</code>, OpenShift Container Platform evicts the pod from the node if it is already running on the node, or the pod is not scheduled onto the node if it is not yet running on the node.
									</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
												Pods that do not tolerate the taint are evicted immediately.
											</li><li class="listitem">
												Pods that tolerate the taint without specifying <code class="literal">tolerationSeconds</code> in their <code class="literal">Pod</code> specification remain bound forever.
											</li><li class="listitem">
												Pods that tolerate the taint with a specified <code class="literal">tolerationSeconds</code> remain bound for the specified amount of time.
											</li></ul></div></li></ul></div></li></ol></div><p>
						For example:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Add the following taints to the node:
							</p><pre class="programlisting language-terminal">$ oc adm taint nodes node1 key1=value1:NoSchedule</pre><pre class="programlisting language-terminal">$ oc adm taint nodes node1 key1=value1:NoExecute</pre><pre class="programlisting language-terminal">$ oc adm taint nodes node1 key2=value2:NoSchedule</pre></li><li class="listitem"><p class="simpara">
								The pod has the following tolerations:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoSchedule"
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoExecute"
#...</pre></li></ul></div><p>
						In this case, the pod cannot be scheduled onto the node, because there is no toleration matching the third taint. The pod continues running if it is already running on the node when the taint is added, because the third taint is the only one of the three that is not tolerated by the pod.
					</p></section><section class="section" id="nodes-scheduler-taints-tolerations-about-taintNodesByCondition_post-install-node-tasks"><div class="titlepage"><div><div><h4 class="title">8.7.1.3. Understanding pod scheduling and node conditions (taint node by condition)</h4></div></div></div><p>
						The Taint Nodes By Condition feature, which is enabled by default, automatically taints nodes that report conditions such as memory pressure and disk pressure. If a node reports a condition, a taint is added until the condition clears. The taints have the <code class="literal">NoSchedule</code> effect, which means no pod can be scheduled on the node unless the pod has a matching toleration.
					</p><p>
						The scheduler checks for these taints on nodes before scheduling pods. If the taint is present, the pod is scheduled on a different node. Because the scheduler checks for taints and not the actual node conditions, you configure the scheduler to ignore some of these node conditions by adding appropriate pod tolerations.
					</p><p>
						To ensure backward compatibility, the daemon set controller automatically adds the following tolerations to all daemons:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								node.kubernetes.io/memory-pressure
							</li><li class="listitem">
								node.kubernetes.io/disk-pressure
							</li><li class="listitem">
								node.kubernetes.io/unschedulable (1.10 or later)
							</li><li class="listitem">
								node.kubernetes.io/network-unavailable (host network only)
							</li></ul></div><p>
						You can also add arbitrary tolerations to daemon sets.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The control plane also adds the <code class="literal">node.kubernetes.io/memory-pressure</code> toleration on pods that have a QoS class. This is because Kubernetes manages pods in the <code class="literal">Guaranteed</code> or <code class="literal">Burstable</code> QoS classes. The new <code class="literal">BestEffort</code> pods do not get scheduled onto the affected node.
						</p></div></div></section><section class="section" id="nodes-scheduler-taints-tolerations-about-taintBasedEvictions_post-install-node-tasks"><div class="titlepage"><div><div><h4 class="title">8.7.1.4. Understanding evicting pods by condition (taint-based evictions)</h4></div></div></div><p>
						The Taint-Based Evictions feature, which is enabled by default, evicts pods from a node that experiences specific conditions, such as <code class="literal">not-ready</code> and <code class="literal">unreachable</code>. When a node experiences one of these conditions, OpenShift Container Platform automatically adds taints to the node, and starts evicting and rescheduling the pods on different nodes.
					</p><p>
						Taint Based Evictions have a <code class="literal">NoExecute</code> effect, where any pod that does not tolerate the taint is evicted immediately and any pod that does tolerate the taint will never be evicted, unless the pod uses the <code class="literal">tolerationSeconds</code> parameter.
					</p><p>
						The <code class="literal">tolerationSeconds</code> parameter allows you to specify how long a pod stays bound to a node that has a node condition. If the condition still exists after the <code class="literal">tolerationSeconds</code> period, the taint remains on the node and the pods with a matching toleration are evicted. If the condition clears before the <code class="literal">tolerationSeconds</code> period, pods with matching tolerations are not removed.
					</p><p>
						If you use the <code class="literal">tolerationSeconds</code> parameter with no value, pods are never evicted because of the not ready and unreachable node conditions.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							OpenShift Container Platform evicts pods in a rate-limited way to prevent massive pod evictions in scenarios such as the master becoming partitioned from the nodes.
						</p><p>
							By default, if more than 55% of nodes in a given zone are unhealthy, the node lifecycle controller changes that zone’s state to <code class="literal">PartialDisruption</code> and the rate of pod evictions is reduced. For small clusters (by default, 50 nodes or less) in this state, nodes in this zone are not tainted and evictions are stopped.
						</p><p>
							For more information, see <a class="link" href="https://kubernetes.io/docs/concepts/architecture/nodes/#rate-limits-on-eviction">Rate limits on eviction</a> in the Kubernetes documentation.
						</p></div></div><p>
						OpenShift Container Platform automatically adds a toleration for <code class="literal">node.kubernetes.io/not-ready</code> and <code class="literal">node.kubernetes.io/unreachable</code> with <code class="literal">tolerationSeconds=300</code>, unless the <code class="literal">Pod</code> configuration specifies either toleration.
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: node.kubernetes.io/not-ready
    operator: Exists
    effect: NoExecute
    tolerationSeconds: 300 <span id="CO102-1"><!--Empty--></span><span class="callout">1</span>
  - key: node.kubernetes.io/unreachable
    operator: Exists
    effect: NoExecute
    tolerationSeconds: 300
#...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO102-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								These tolerations ensure that the default pod behavior is to remain bound for five minutes after one of these node conditions problems is detected.
							</div></dd></dl></div><p>
						You can configure these tolerations as needed. For example, if you have an application with a lot of local state, you might want to keep the pods bound to node for a longer time in the event of network partition, allowing for the partition to recover and avoiding pod eviction.
					</p><p>
						Pods spawned by a daemon set are created with <code class="literal">NoExecute</code> tolerations for the following taints with no <code class="literal">tolerationSeconds</code>:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<code class="literal">node.kubernetes.io/unreachable</code>
							</li><li class="listitem">
								<code class="literal">node.kubernetes.io/not-ready</code>
							</li></ul></div><p>
						As a result, daemon set pods are never evicted because of these node conditions.
					</p></section><section class="section" id="nodes-scheduler-taints-tolerations-all_post-install-node-tasks"><div class="titlepage"><div><div><h4 class="title">8.7.1.5. Tolerating all taints</h4></div></div></div><p>
						You can configure a pod to tolerate all taints by adding an <code class="literal">operator: "Exists"</code> toleration with no <code class="literal">key</code> and <code class="literal">values</code> parameters. Pods with this toleration are not removed from a node that has taints.
					</p><div class="formalpara"><p class="title"><strong><code class="literal">Pod</code> spec for tolerating all taints</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - operator: "Exists"
#...</pre>

						</p></div></section></section><section class="section" id="nodes-scheduler-taints-tolerations-adding_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.7.2. Adding taints and tolerations</h3></div></div></div><p>
					You add tolerations to pods and taints to nodes to allow the node to control which pods should or should not be scheduled on them. For existing pods and nodes, you should add the toleration to the pod first, then add the taint to the node to avoid pods being removed from the node before you can add the toleration.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add a toleration to a pod by editing the <code class="literal">Pod</code> spec to include a <code class="literal">tolerations</code> stanza:
						</p><div class="formalpara"><p class="title"><strong>Sample pod configuration file with an Equal operator</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key1" <span id="CO103-1"><!--Empty--></span><span class="callout">1</span>
    value: "value1"
    operator: "Equal"
    effect: "NoExecute"
    tolerationSeconds: 3600 <span id="CO103-2"><!--Empty--></span><span class="callout">2</span>
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO103-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The toleration parameters, as described in the <span class="strong strong"><strong>Taint and toleration components</strong></span> table.
								</div></dd><dt><a href="#CO103-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The <code class="literal">tolerationSeconds</code> parameter specifies how long a pod can remain bound to a node before being evicted.
								</div></dd></dl></div><p class="simpara">
							For example:
						</p><div class="formalpara"><p class="title"><strong>Sample pod configuration file with an Exists operator</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
   tolerations:
    - key: "key1"
      operator: "Exists" <span id="CO104-1"><!--Empty--></span><span class="callout">1</span>
      effect: "NoExecute"
      tolerationSeconds: 3600
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO104-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal">Exists</code> operator does not take a <code class="literal">value</code>.
								</div></dd></dl></div><p class="simpara">
							This example places a taint on <code class="literal">node1</code> that has key <code class="literal">key1</code>, value <code class="literal">value1</code>, and taint effect <code class="literal">NoExecute</code>.
						</p></li><li class="listitem"><p class="simpara">
							Add a taint to a node by using the following command with the parameters described in the <span class="strong strong"><strong>Taint and toleration components</strong></span> table:
						</p><pre class="programlisting language-terminal">$ oc adm taint nodes &lt;node_name&gt; &lt;key&gt;=&lt;value&gt;:&lt;effect&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc adm taint nodes node1 key1=value1:NoExecute</pre><p class="simpara">
							This command places a taint on <code class="literal">node1</code> that has key <code class="literal">key1</code>, value <code class="literal">value1</code>, and effect <code class="literal">NoExecute</code>.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If you add a <code class="literal">NoSchedule</code> taint to a control plane node, the node must have the <code class="literal">node-role.kubernetes.io/master=:NoSchedule</code> taint, which is added by default.
							</p><p>
								For example:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Node
metadata:
  annotations:
    machine.openshift.io/machine: openshift-machine-api/ci-ln-62s7gtb-f76d1-v8jxv-master-0
    machineconfiguration.openshift.io/currentConfig: rendered-master-cdc1ab7da414629332cc4c3926e6e59c
  name: my-node
#...
spec:
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
#...</pre></div></div><p class="simpara">
							The tolerations on the pod match the taint on the node. A pod with either toleration can be scheduled onto <code class="literal">node1</code>.
						</p></li></ol></div></section><section class="section" id="nodes-scheduler-taints-tolerations-adding-machineset_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.7.3. Adding taints and tolerations using a compute machine set</h3></div></div></div><p>
					You can add taints to nodes using a compute machine set. All nodes associated with the <code class="literal">MachineSet</code> object are updated with the taint. Tolerations respond to taints added by a compute machine set in the same manner as taints added directly to the nodes.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add a toleration to a pod by editing the <code class="literal">Pod</code> spec to include a <code class="literal">tolerations</code> stanza:
						</p><div class="formalpara"><p class="title"><strong>Sample pod configuration file with <code class="literal">Equal</code> operator</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key1" <span id="CO105-1"><!--Empty--></span><span class="callout">1</span>
    value: "value1"
    operator: "Equal"
    effect: "NoExecute"
    tolerationSeconds: 3600 <span id="CO105-2"><!--Empty--></span><span class="callout">2</span>
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO105-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The toleration parameters, as described in the <span class="strong strong"><strong>Taint and toleration components</strong></span> table.
								</div></dd><dt><a href="#CO105-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The <code class="literal">tolerationSeconds</code> parameter specifies how long a pod is bound to a node before being evicted.
								</div></dd></dl></div><p class="simpara">
							For example:
						</p><div class="formalpara"><p class="title"><strong>Sample pod configuration file with <code class="literal">Exists</code> operator</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key1"
    operator: "Exists"
    effect: "NoExecute"
    tolerationSeconds: 3600
#...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Add the taint to the <code class="literal">MachineSet</code> object:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Edit the <code class="literal">MachineSet</code> YAML for the nodes you want to taint or you can create a new <code class="literal">MachineSet</code> object:
								</p><pre class="programlisting language-terminal">$ oc edit machineset &lt;machineset&gt;</pre></li><li class="listitem"><p class="simpara">
									Add the taint to the <code class="literal">spec.template.spec</code> section:
								</p><div class="formalpara"><p class="title"><strong>Example taint in a compute machine set specification</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: my-machineset
#...
spec:
#...
  template:
#...
    spec:
      taints:
      - effect: NoExecute
        key: key1
        value: value1
#...</pre>

									</p></div><p class="simpara">
									This example places a taint that has the key <code class="literal">key1</code>, value <code class="literal">value1</code>, and taint effect <code class="literal">NoExecute</code> on the nodes.
								</p></li><li class="listitem"><p class="simpara">
									Scale down the compute machine set to 0:
								</p><pre class="programlisting language-terminal">$ oc scale --replicas=0 machineset &lt;machineset&gt; -n openshift-machine-api</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
									You can alternatively apply the following YAML to scale the compute machine set:
								</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: &lt;machineset&gt;
  namespace: openshift-machine-api
spec:
  replicas: 0</pre></div></div><p class="simpara">
									Wait for the machines to be removed.
								</p></li><li class="listitem"><p class="simpara">
									Scale up the compute machine set as needed:
								</p><pre class="programlisting language-terminal">$ oc scale --replicas=2 machineset &lt;machineset&gt; -n openshift-machine-api</pre><p class="simpara">
									Or:
								</p><pre class="programlisting language-terminal">$ oc edit machineset &lt;machineset&gt; -n openshift-machine-api</pre><p class="simpara">
									Wait for the machines to start. The taint is added to the nodes associated with the <code class="literal">MachineSet</code> object.
								</p></li></ol></div></li></ol></div></section><section class="section" id="nodes-scheduler-taints-tolerations-bindings_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.7.4. Binding a user to a node using taints and tolerations</h3></div></div></div><p>
					If you want to dedicate a set of nodes for exclusive use by a particular set of users, add a toleration to their pods. Then, add a corresponding taint to those nodes. The pods with the tolerations are allowed to use the tainted nodes or any other nodes in the cluster.
				</p><p>
					If you want ensure the pods are scheduled to only those tainted nodes, also add a label to the same set of nodes and add a node affinity to the pods so that the pods can only be scheduled onto nodes with that label.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To configure a node so that users can use only that node:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add a corresponding taint to those nodes:
						</p><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc adm taint nodes node1 dedicated=groupName:NoSchedule</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can alternatively apply the following YAML to add the taint:
						</p><pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  name: my-node
#...
spec:
  taints:
    - key: dedicated
      value: groupName
      effect: NoSchedule
#...</pre></div></div></li><li class="listitem">
							Add a toleration to the pods by writing a custom admission controller.
						</li></ol></div></section><section class="section" id="nodes-scheduler-taints-tolerations-special_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.7.5. Controlling nodes with special hardware using taints and tolerations</h3></div></div></div><p>
					In a cluster where a small subset of nodes have specialized hardware, you can use taints and tolerations to keep pods that do not need the specialized hardware off of those nodes, leaving the nodes for pods that do need the specialized hardware. You can also require pods that need specialized hardware to use specific nodes.
				</p><p>
					You can achieve this by adding a toleration to pods that need the special hardware and tainting the nodes that have the specialized hardware.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To ensure nodes with specialized hardware are reserved for specific pods:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add a toleration to pods that need the special hardware.
						</p><p class="simpara">
							For example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
    - key: "disktype"
      value: "ssd"
      operator: "Equal"
      effect: "NoSchedule"
      tolerationSeconds: 3600
#...</pre></li><li class="listitem"><p class="simpara">
							Taint the nodes that have the specialized hardware using one of the following commands:
						</p><pre class="programlisting language-terminal">$ oc adm taint nodes &lt;node-name&gt; disktype=ssd:NoSchedule</pre><p class="simpara">
							Or:
						</p><pre class="programlisting language-terminal">$ oc adm taint nodes &lt;node-name&gt; disktype=ssd:PreferNoSchedule</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can alternatively apply the following YAML to add the taint:
						</p><pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  name: my_node
#...
spec:
  taints:
    - key: disktype
      value: ssd
      effect: PreferNoSchedule
#...</pre></div></div></li></ol></div></section><section class="section" id="nodes-scheduler-taints-tolerations-removing_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.7.6. Removing taints and tolerations</h3></div></div></div><p>
					You can remove taints from nodes and tolerations from pods as needed. You should add the toleration to the pod first, then add the taint to the node to avoid pods being removed from the node before you can add the toleration.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To remove taints and tolerations:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To remove a taint from a node:
						</p><pre class="programlisting language-terminal">$ oc adm taint nodes &lt;node-name&gt; &lt;key&gt;-</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc adm taint nodes ip-10-0-132-248.ec2.internal key1-</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">node/ip-10-0-132-248.ec2.internal untainted</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To remove a toleration from a pod, edit the <code class="literal">Pod</code> spec to remove the toleration:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key2"
    operator: "Exists"
    effect: "NoExecute"
    tolerationSeconds: 3600
#...</pre></li></ol></div></section></section><section class="section" id="post-install-topology-manager"><div class="titlepage"><div><div><h2 class="title">8.8. Topology Manager</h2></div></div></div><p>
				Understand and work with Topology Manager.
			</p><section class="section" id="topology_manager_policies_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.8.1. Topology Manager policies</h3></div></div></div><p>
					Topology Manager aligns <code class="literal">Pod</code> resources of all Quality of Service (QoS) classes by collecting topology hints from Hint Providers, such as CPU Manager and Device Manager, and using the collected hints to align the <code class="literal">Pod</code> resources.
				</p><p>
					Topology Manager supports four allocation policies, which you assign in the <code class="literal">KubeletConfig</code> custom resource (CR) named <code class="literal">cpumanager-enabled</code>:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">none</code> policy</span></dt><dd>
								This is the default policy and does not perform any topology alignment.
							</dd><dt><span class="term"><code class="literal">best-effort</code> policy</span></dt><dd>
								For each container in a pod with the <code class="literal">best-effort</code> topology management policy, kubelet calls each Hint Provider to discover their resource availability. Using this information, the Topology Manager stores the preferred NUMA Node affinity for that container. If the affinity is not preferred, Topology Manager stores this and admits the pod to the node.
							</dd><dt><span class="term"><code class="literal">restricted</code> policy</span></dt><dd>
								For each container in a pod with the <code class="literal">restricted</code> topology management policy, kubelet calls each Hint Provider to discover their resource availability. Using this information, the Topology Manager stores the preferred NUMA Node affinity for that container. If the affinity is not preferred, Topology Manager rejects this pod from the node, resulting in a pod in a <code class="literal">Terminated</code> state with a pod admission failure.
							</dd><dt><span class="term"><code class="literal">single-numa-node</code> policy</span></dt><dd>
								For each container in a pod with the <code class="literal">single-numa-node</code> topology management policy, kubelet calls each Hint Provider to discover their resource availability. Using this information, the Topology Manager determines if a single NUMA Node affinity is possible. If it is, the pod is admitted to the node. If a single NUMA Node affinity is not possible, the Topology Manager rejects the pod from the node. This results in a pod in a Terminated state with a pod admission failure.
							</dd></dl></div></section><section class="section" id="seting_up_topology_manager_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.8.2. Setting up Topology Manager</h3></div></div></div><p>
					To use Topology Manager, you must configure an allocation policy in the <code class="literal">KubeletConfig</code> custom resource (CR) named <code class="literal">cpumanager-enabled</code>. This file might exist if you have set up CPU Manager. If the file does not exist, you can create the file.
				</p><div class="itemizedlist"><p class="title"><strong>Prequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Configure the CPU Manager policy to be <code class="literal">static</code>.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To activate Topololgy Manager:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Configure the Topology Manager allocation policy in the custom resource.
						</p><pre class="programlisting language-terminal">$ oc edit KubeletConfig cpumanager-enabled</pre><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: cpumanager-enabled
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: cpumanager-enabled
  kubeletConfig:
     cpuManagerPolicy: static <span id="CO106-1"><!--Empty--></span><span class="callout">1</span>
     cpuManagerReconcilePeriod: 5s
     topologyManagerPolicy: single-numa-node <span id="CO106-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO106-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									This parameter must be <code class="literal">static</code> with a lowercase <code class="literal">s</code>.
								</div></dd><dt><a href="#CO106-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify your selected Topology Manager allocation policy. Here, the policy is <code class="literal">single-numa-node</code>. Acceptable values are: <code class="literal">default</code>, <code class="literal">best-effort</code>, <code class="literal">restricted</code>, <code class="literal">single-numa-node</code>.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="pod-interactions-with-topology-manager_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.8.3. Pod interactions with Topology Manager policies</h3></div></div></div><p>
					The example <code class="literal">Pod</code> specs below help illustrate pod interactions with Topology Manager.
				</p><p>
					The following pod runs in the <code class="literal">BestEffort</code> QoS class because no resource requests or limits are specified.
				</p><pre class="programlisting language-yaml">spec:
  containers:
  - name: nginx
    image: nginx</pre><p>
					The next pod runs in the <code class="literal">Burstable</code> QoS class because requests are less than limits.
				</p><pre class="programlisting language-yaml">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"</pre><p>
					If the selected policy is anything other than <code class="literal">none</code>, Topology Manager would not consider either of these <code class="literal">Pod</code> specifications.
				</p><p>
					The last example pod below runs in the Guaranteed QoS class because requests are equal to limits.
				</p><pre class="programlisting language-yaml">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
        example.com/device: "1"
      requests:
        memory: "200Mi"
        cpu: "2"
        example.com/device: "1"</pre><p>
					Topology Manager would consider this pod. The Topology Manager would consult the hint providers, which are CPU Manager and Device Manager, to get topology hints for the pod.
				</p><p>
					Topology Manager will use this information to store the best topology for this container. In the case of this pod, CPU Manager and Device Manager will use this stored information at the resource allocation stage.
				</p></section></section><section class="section" id="nodes-cluster-overcommit-resource-requests_post-install-node-tasks"><div class="titlepage"><div><div><h2 class="title">8.9. Resource requests and overcommitment</h2></div></div></div><p>
				For each compute resource, a container may specify a resource request and limit. Scheduling decisions are made based on the request to ensure that a node has enough capacity available to meet the requested value. If a container specifies limits, but omits requests, the requests are defaulted to the limits. A container is not able to exceed the specified limit on the node.
			</p><p>
				The enforcement of limits is dependent upon the compute resource type. If a container makes no request or limit, the container is scheduled to a node with no resource guarantees. In practice, the container is able to consume as much of the specified resource as is available with the lowest local priority. In low resource situations, containers that specify no resource requests are given the lowest quality of service.
			</p><p>
				Scheduling is based on resources requested, while quota and hard limits refer to resource limits, which can be set higher than requested resources. The difference between request and limit determines the level of overcommit; for instance, if a container is given a memory request of 1Gi and a memory limit of 2Gi, it is scheduled based on the 1Gi request being available on the node, but could use up to 2Gi; so it is 200% overcommitted.
			</p></section><section class="section" id="nodes-cluster-resource-override_post-install-node-tasks"><div class="titlepage"><div><div><h2 class="title">8.10. Cluster-level overcommit using the Cluster Resource Override Operator</h2></div></div></div><p>
				The Cluster Resource Override Operator is an admission webhook that allows you to control the level of overcommit and manage container density across all the nodes in your cluster. The Operator controls how nodes in specific projects can exceed defined memory and CPU limits.
			</p><p>
				You must install the Cluster Resource Override Operator using the OpenShift Container Platform console or CLI as shown in the following sections. During the installation, you create a <code class="literal">ClusterResourceOverride</code> custom resource (CR), where you set the level of overcommit, as shown in the following example:
			</p><pre class="programlisting language-yaml">apiVersion: operator.autoscaling.openshift.io/v1
kind: ClusterResourceOverride
metadata:
    name: cluster <span id="CO107-1"><!--Empty--></span><span class="callout">1</span>
spec:
  podResourceOverride:
    spec:
      memoryRequestToLimitPercent: 50 <span id="CO107-2"><!--Empty--></span><span class="callout">2</span>
      cpuRequestToLimitPercent: 25 <span id="CO107-3"><!--Empty--></span><span class="callout">3</span>
      limitCPUToMemoryPercent: 200 <span id="CO107-4"><!--Empty--></span><span class="callout">4</span>
# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO107-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						The name must be <code class="literal">cluster</code>.
					</div></dd><dt><a href="#CO107-2"><span class="callout">2</span></a> </dt><dd><div class="para">
						Optional. If a container memory limit has been specified or defaulted, the memory request is overridden to this percentage of the limit, between 1-100. The default is 50.
					</div></dd><dt><a href="#CO107-3"><span class="callout">3</span></a> </dt><dd><div class="para">
						Optional. If a container CPU limit has been specified or defaulted, the CPU request is overridden to this percentage of the limit, between 1-100. The default is 25.
					</div></dd><dt><a href="#CO107-4"><span class="callout">4</span></a> </dt><dd><div class="para">
						Optional. If a container memory limit has been specified or defaulted, the CPU limit is overridden to a percentage of the memory limit, if specified. Scaling 1Gi of RAM at 100 percent is equal to 1 CPU core. This is processed prior to overriding the CPU request (if configured). The default is 200.
					</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The Cluster Resource Override Operator overrides have no effect if limits have not been set on containers. Create a <code class="literal">LimitRange</code> object with default limits per individual project or configure limits in <code class="literal">Pod</code> specs for the overrides to apply.
				</p></div></div><p>
				When configured, overrides can be enabled per-project by applying the following label to the Namespace object for each project:
			</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:

# ...

  labels:
    clusterresourceoverrides.admission.autoscaling.openshift.io/enabled: "true"

# ...</pre><p>
				The Operator watches for the <code class="literal">ClusterResourceOverride</code> CR and ensures that the <code class="literal">ClusterResourceOverride</code> admission webhook is installed into the same namespace as the operator.
			</p><section class="section" id="nodes-cluster-resource-override-deploy-console_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.10.1. Installing the Cluster Resource Override Operator using the web console</h3></div></div></div><p>
					You can use the OpenShift Container Platform web console to install the Cluster Resource Override Operator to help control overcommit in your cluster.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The Cluster Resource Override Operator has no effect if limits have not been set on containers. You must specify default limits for a project using a <code class="literal">LimitRange</code> object or configure limits in <code class="literal">Pod</code> specs for the overrides to apply.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To install the Cluster Resource Override Operator using the OpenShift Container Platform web console:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong>Home</strong></span> → <span class="strong strong"><strong>Projects</strong></span>
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Click <span class="strong strong"><strong>Create Project</strong></span>.
								</li><li class="listitem">
									Specify <code class="literal">clusterresourceoverride-operator</code> as the name of the project.
								</li><li class="listitem">
									Click <span class="strong strong"><strong>Create</strong></span>.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span>.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Choose <span class="strong strong"><strong>ClusterResourceOverride Operator</strong></span> from the list of available Operators and click <span class="strong strong"><strong>Install</strong></span>.
								</li><li class="listitem">
									On the <span class="strong strong"><strong>Install Operator</strong></span> page, make sure <span class="strong strong"><strong>A specific Namespace on the cluster</strong></span> is selected for <span class="strong strong"><strong>Installation Mode</strong></span>.
								</li><li class="listitem">
									Make sure <span class="strong strong"><strong>clusterresourceoverride-operator</strong></span> is selected for <span class="strong strong"><strong>Installed Namespace</strong></span>.
								</li><li class="listitem">
									Select an <span class="strong strong"><strong>Update Channel</strong></span> and <span class="strong strong"><strong>Approval Strategy</strong></span>.
								</li><li class="listitem">
									Click <span class="strong strong"><strong>Install</strong></span>.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							On the <span class="strong strong"><strong>Installed Operators</strong></span> page, click <span class="strong strong"><strong>ClusterResourceOverride</strong></span>.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									On the <span class="strong strong"><strong>ClusterResourceOverride Operator</strong></span> details page, click <span class="strong strong"><strong>Create ClusterResourceOverride</strong></span>.
								</li><li class="listitem"><p class="simpara">
									On the <span class="strong strong"><strong>Create ClusterResourceOverride</strong></span> page, click <span class="strong strong"><strong>YAML view</strong></span> and edit the YAML template to set the overcommit values as needed:
								</p><pre class="programlisting language-yaml">apiVersion: operator.autoscaling.openshift.io/v1
kind: ClusterResourceOverride
metadata:
  name: cluster <span id="CO108-1"><!--Empty--></span><span class="callout">1</span>
spec:
  podResourceOverride:
    spec:
      memoryRequestToLimitPercent: 50 <span id="CO108-2"><!--Empty--></span><span class="callout">2</span>
      cpuRequestToLimitPercent: 25 <span id="CO108-3"><!--Empty--></span><span class="callout">3</span>
      limitCPUToMemoryPercent: 200 <span id="CO108-4"><!--Empty--></span><span class="callout">4</span>
# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO108-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The name must be <code class="literal">cluster</code>.
										</div></dd><dt><a href="#CO108-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Optional. Specify the percentage to override the container memory limit, if used, between 1-100. The default is 50.
										</div></dd><dt><a href="#CO108-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Optional. Specify the percentage to override the container CPU limit, if used, between 1-100. The default is 25.
										</div></dd><dt><a href="#CO108-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Optional. Specify the percentage to override the container memory limit, if used. Scaling 1Gi of RAM at 100 percent is equal to 1 CPU core. This is processed prior to overriding the CPU request, if configured. The default is 200.
										</div></dd></dl></div></li><li class="listitem">
									Click <span class="strong strong"><strong>Create</strong></span>.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Check the current state of the admission webhook by checking the status of the cluster custom resource:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									On the <span class="strong strong"><strong>ClusterResourceOverride Operator</strong></span> page, click <span class="strong strong"><strong>cluster</strong></span>.
								</li><li class="listitem"><p class="simpara">
									On the <span class="strong strong"><strong>ClusterResourceOverride Details</strong></span> page, click <span class="strong strong"><strong>YAML</strong></span>. The <code class="literal">mutatingWebhookConfigurationRef</code> section appears when the webhook is called.
								</p><pre class="programlisting language-yaml">apiVersion: operator.autoscaling.openshift.io/v1
kind: ClusterResourceOverride
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"operator.autoscaling.openshift.io/v1","kind":"ClusterResourceOverride","metadata":{"annotations":{},"name":"cluster"},"spec":{"podResourceOverride":{"spec":{"cpuRequestToLimitPercent":25,"limitCPUToMemoryPercent":200,"memoryRequestToLimitPercent":50}}}}
  creationTimestamp: "2019-12-18T22:35:02Z"
  generation: 1
  name: cluster
  resourceVersion: "127622"
  selfLink: /apis/operator.autoscaling.openshift.io/v1/clusterresourceoverrides/cluster
  uid: 978fc959-1717-4bd1-97d0-ae00ee111e8d
spec:
  podResourceOverride:
    spec:
      cpuRequestToLimitPercent: 25
      limitCPUToMemoryPercent: 200
      memoryRequestToLimitPercent: 50
status:

# ...

    mutatingWebhookConfigurationRef: <span id="CO109-1"><!--Empty--></span><span class="callout">1</span>
      apiVersion: admissionregistration.k8s.io/v1
      kind: MutatingWebhookConfiguration
      name: clusterresourceoverrides.admission.autoscaling.openshift.io
      resourceVersion: "127621"
      uid: 98b3b8ae-d5ce-462b-8ab5-a729ea8f38f3

# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO109-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Reference to the <code class="literal">ClusterResourceOverride</code> admission webhook.
										</div></dd></dl></div></li></ol></div></li></ol></div></section><section class="section" id="nodes-cluster-resource-override-deploy-cli_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.10.2. Installing the Cluster Resource Override Operator using the CLI</h3></div></div></div><p>
					You can use the OpenShift Container Platform CLI to install the Cluster Resource Override Operator to help control overcommit in your cluster.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The Cluster Resource Override Operator has no effect if limits have not been set on containers. You must specify default limits for a project using a <code class="literal">LimitRange</code> object or configure limits in <code class="literal">Pod</code> specs for the overrides to apply.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To install the Cluster Resource Override Operator using the CLI:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a namespace for the Cluster Resource Override Operator:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a <code class="literal">Namespace</code> object YAML file (for example, <code class="literal">cro-namespace.yaml</code>) for the Cluster Resource Override Operator:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: clusterresourceoverride-operator</pre></li><li class="listitem"><p class="simpara">
									Create the namespace:
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc create -f cro-namespace.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create an Operator group:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create an <code class="literal">OperatorGroup</code> object YAML file (for example, cro-og.yaml) for the Cluster Resource Override Operator:
								</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: clusterresourceoverride-operator
  namespace: clusterresourceoverride-operator
spec:
  targetNamespaces:
    - clusterresourceoverride-operator</pre></li><li class="listitem"><p class="simpara">
									Create the Operator Group:
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc create -f cro-og.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a subscription:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a <code class="literal">Subscription</code> object YAML file (for example, cro-sub.yaml) for the Cluster Resource Override Operator:
								</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: clusterresourceoverride
  namespace: clusterresourceoverride-operator
spec:
  channel: "4.13"
  name: clusterresourceoverride
  source: redhat-operators
  sourceNamespace: openshift-marketplace</pre></li><li class="listitem"><p class="simpara">
									Create the subscription:
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc create -f cro-sub.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">ClusterResourceOverride</code> custom resource (CR) object in the <code class="literal">clusterresourceoverride-operator</code> namespace:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Change to the <code class="literal">clusterresourceoverride-operator</code> namespace.
								</p><pre class="programlisting language-terminal">$ oc project clusterresourceoverride-operator</pre></li><li class="listitem"><p class="simpara">
									Create a <code class="literal">ClusterResourceOverride</code> object YAML file (for example, cro-cr.yaml) for the Cluster Resource Override Operator:
								</p><pre class="programlisting language-yaml">apiVersion: operator.autoscaling.openshift.io/v1
kind: ClusterResourceOverride
metadata:
    name: cluster <span id="CO110-1"><!--Empty--></span><span class="callout">1</span>
spec:
  podResourceOverride:
    spec:
      memoryRequestToLimitPercent: 50 <span id="CO110-2"><!--Empty--></span><span class="callout">2</span>
      cpuRequestToLimitPercent: 25 <span id="CO110-3"><!--Empty--></span><span class="callout">3</span>
      limitCPUToMemoryPercent: 200 <span id="CO110-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO110-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The name must be <code class="literal">cluster</code>.
										</div></dd><dt><a href="#CO110-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Optional. Specify the percentage to override the container memory limit, if used, between 1-100. The default is 50.
										</div></dd><dt><a href="#CO110-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Optional. Specify the percentage to override the container CPU limit, if used, between 1-100. The default is 25.
										</div></dd><dt><a href="#CO110-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Optional. Specify the percentage to override the container memory limit, if used. Scaling 1Gi of RAM at 100 percent is equal to 1 CPU core. This is processed prior to overriding the CPU request, if configured. The default is 200.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">ClusterResourceOverride</code> object:
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc create -f cro-cr.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Verify the current state of the admission webhook by checking the status of the cluster custom resource.
						</p><pre class="programlisting language-terminal">$ oc get clusterresourceoverride cluster -n clusterresourceoverride-operator -o yaml</pre><p class="simpara">
							The <code class="literal">mutatingWebhookConfigurationRef</code> section appears when the webhook is called.
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: operator.autoscaling.openshift.io/v1
kind: ClusterResourceOverride
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"operator.autoscaling.openshift.io/v1","kind":"ClusterResourceOverride","metadata":{"annotations":{},"name":"cluster"},"spec":{"podResourceOverride":{"spec":{"cpuRequestToLimitPercent":25,"limitCPUToMemoryPercent":200,"memoryRequestToLimitPercent":50}}}}
  creationTimestamp: "2019-12-18T22:35:02Z"
  generation: 1
  name: cluster
  resourceVersion: "127622"
  selfLink: /apis/operator.autoscaling.openshift.io/v1/clusterresourceoverrides/cluster
  uid: 978fc959-1717-4bd1-97d0-ae00ee111e8d
spec:
  podResourceOverride:
    spec:
      cpuRequestToLimitPercent: 25
      limitCPUToMemoryPercent: 200
      memoryRequestToLimitPercent: 50
status:

# ...

    mutatingWebhookConfigurationRef: <span id="CO111-1"><!--Empty--></span><span class="callout">1</span>
      apiVersion: admissionregistration.k8s.io/v1
      kind: MutatingWebhookConfiguration
      name: clusterresourceoverrides.admission.autoscaling.openshift.io
      resourceVersion: "127621"
      uid: 98b3b8ae-d5ce-462b-8ab5-a729ea8f38f3

# ...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO111-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Reference to the <code class="literal">ClusterResourceOverride</code> admission webhook.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="nodes-cluster-resource-configure_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.10.3. Configuring cluster-level overcommit</h3></div></div></div><p>
					The Cluster Resource Override Operator requires a <code class="literal">ClusterResourceOverride</code> custom resource (CR) and a label for each project where you want the Operator to control overcommit.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The Cluster Resource Override Operator has no effect if limits have not been set on containers. You must specify default limits for a project using a <code class="literal">LimitRange</code> object or configure limits in <code class="literal">Pod</code> specs for the overrides to apply.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To modify cluster-level overcommit:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">ClusterResourceOverride</code> CR:
						</p><pre class="programlisting language-yaml">apiVersion: operator.autoscaling.openshift.io/v1
kind: ClusterResourceOverride
metadata:
    name: cluster
spec:
  podResourceOverride:
    spec:
      memoryRequestToLimitPercent: 50 <span id="CO112-1"><!--Empty--></span><span class="callout">1</span>
      cpuRequestToLimitPercent: 25 <span id="CO112-2"><!--Empty--></span><span class="callout">2</span>
      limitCPUToMemoryPercent: 200 <span id="CO112-3"><!--Empty--></span><span class="callout">3</span>
# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO112-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Optional. Specify the percentage to override the container memory limit, if used, between 1-100. The default is 50.
								</div></dd><dt><a href="#CO112-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Optional. Specify the percentage to override the container CPU limit, if used, between 1-100. The default is 25.
								</div></dd><dt><a href="#CO112-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Optional. Specify the percentage to override the container memory limit, if used. Scaling 1Gi of RAM at 100 percent is equal to 1 CPU core. This is processed prior to overriding the CPU request, if configured. The default is 200.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Ensure the following label has been added to the Namespace object for each project where you want the Cluster Resource Override Operator to control overcommit:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:

# ...

  labels:
    clusterresourceoverrides.admission.autoscaling.openshift.io/enabled: "true" <span id="CO113-1"><!--Empty--></span><span class="callout">1</span>

# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO113-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add this label to each project.
								</div></dd></dl></div></li></ol></div></section></section><section class="section" id="nodes-cluster-node-overcommit_post-install-node-tasks"><div class="titlepage"><div><div><h2 class="title">8.11. Node-level overcommit</h2></div></div></div><p>
				You can use various ways to control overcommit on specific nodes, such as quality of service (QOS) guarantees, CPU limits, or reserve resources. You can also disable overcommit for specific nodes and specific projects.
			</p><section class="section" id="nodes-cluster-overcommit-reserving-memory_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.11.1. Understanding compute resources and containers</h3></div></div></div><p>
					The node-enforced behavior for compute resources is specific to the resource type.
				</p><section class="section" id="understanding-container-CPU-requests_post-install-node-tasks"><div class="titlepage"><div><div><h4 class="title">8.11.1.1. Understanding container CPU requests</h4></div></div></div><p>
						A container is guaranteed the amount of CPU it requests and is additionally able to consume excess CPU available on the node, up to any limit specified by the container. If multiple containers are attempting to use excess CPU, CPU time is distributed based on the amount of CPU requested by each container.
					</p><p>
						For example, if one container requested 500m of CPU time and another container requested 250m of CPU time, then any extra CPU time available on the node is distributed among the containers in a 2:1 ratio. If a container specified a limit, it will be throttled not to use more CPU than the specified limit. CPU requests are enforced using the CFS shares support in the Linux kernel. By default, CPU limits are enforced using the CFS quota support in the Linux kernel over a 100ms measuring interval, though this can be disabled.
					</p></section><section class="section" id="understanding-memory-requests-container_post-install-node-tasks"><div class="titlepage"><div><div><h4 class="title">8.11.1.2. Understanding container memory requests</h4></div></div></div><p>
						A container is guaranteed the amount of memory it requests. A container can use more memory than requested, but once it exceeds its requested amount, it could be terminated in a low memory situation on the node. If a container uses less memory than requested, it will not be terminated unless system tasks or daemons need more memory than was accounted for in the node’s resource reservation. If a container specifies a limit on memory, it is immediately terminated if it exceeds the limit amount.
					</p></section></section><section class="section" id="nodes-cluster-overcommit-qos-about_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.11.2. Understanding overcomitment and quality of service classes</h3></div></div></div><p>
					A node is <span class="emphasis"><em>overcommitted</em></span> when it has a pod scheduled that makes no request, or when the sum of limits across all pods on that node exceeds available machine capacity.
				</p><p>
					In an overcommitted environment, it is possible that the pods on the node will attempt to use more compute resource than is available at any given point in time. When this occurs, the node must give priority to one pod over another. The facility used to make this decision is referred to as a Quality of Service (QoS) Class.
				</p><p>
					A pod is designated as one of three QoS classes with decreasing order of priority:
				</p><div class="table" id="idm140031632113744"><p class="title"><strong>Table 8.2. Quality of Service Classes</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 14%; " class="col_1"><!--Empty--></col><col style="width: 14%; " class="col_2"><!--Empty--></col><col style="width: 72%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031632107968" scope="col">Priority</th><th align="left" valign="top" id="idm140031632106880" scope="col">Class Name</th><th align="left" valign="top" id="idm140031632105792" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031632107968"> <p>
									1 (highest)
								</p>
								 </td><td align="left" valign="top" headers="idm140031632106880"> <p>
									<span class="strong strong"><strong>Guaranteed</strong></span>
								</p>
								 </td><td align="left" valign="top" headers="idm140031632105792"> <p>
									If limits and optionally requests are set (not equal to 0) for all resources and they are equal, then the pod is classified as <span class="strong strong"><strong>Guaranteed</strong></span>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031632107968"> <p>
									2
								</p>
								 </td><td align="left" valign="top" headers="idm140031632106880"> <p>
									<span class="strong strong"><strong>Burstable</strong></span>
								</p>
								 </td><td align="left" valign="top" headers="idm140031632105792"> <p>
									If requests and optionally limits are set (not equal to 0) for all resources, and they are not equal, then the pod is classified as <span class="strong strong"><strong>Burstable</strong></span>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031632107968"> <p>
									3 (lowest)
								</p>
								 </td><td align="left" valign="top" headers="idm140031632106880"> <p>
									<span class="strong strong"><strong>BestEffort</strong></span>
								</p>
								 </td><td align="left" valign="top" headers="idm140031632105792"> <p>
									If requests and limits are not set for any of the resources, then the pod is classified as <span class="strong strong"><strong>BestEffort</strong></span>.
								</p>
								 </td></tr></tbody></table></div></div><p>
					Memory is an incompressible resource, so in low memory situations, containers that have the lowest priority are terminated first:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<span class="strong strong"><strong>Guaranteed</strong></span> containers are considered top priority, and are guaranteed to only be terminated if they exceed their limits, or if the system is under memory pressure and there are no lower priority containers that can be evicted.
						</li><li class="listitem">
							<span class="strong strong"><strong>Burstable</strong></span> containers under system memory pressure are more likely to be terminated once they exceed their requests and no other <span class="strong strong"><strong>BestEffort</strong></span> containers exist.
						</li><li class="listitem">
							<span class="strong strong"><strong>BestEffort</strong></span> containers are treated with the lowest priority. Processes in these containers are first to be terminated if the system runs out of memory.
						</li></ul></div><section class="section" id="qos-about-reserve_post-install-node-tasks"><div class="titlepage"><div><div><h4 class="title">8.11.2.1. Understanding how to reserve memory across quality of service tiers</h4></div></div></div><p>
						You can use the <code class="literal">qos-reserved</code> parameter to specify a percentage of memory to be reserved by a pod in a particular QoS level. This feature attempts to reserve requested resources to exclude pods from lower OoS classes from using resources requested by pods in higher QoS classes.
					</p><p>
						OpenShift Container Platform uses the <code class="literal">qos-reserved</code> parameter as follows:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								A value of <code class="literal">qos-reserved=memory=100%</code> will prevent the <code class="literal">Burstable</code> and <code class="literal">BestEffort</code> QoS classes from consuming memory that was requested by a higher QoS class. This increases the risk of inducing OOM on <code class="literal">BestEffort</code> and <code class="literal">Burstable</code> workloads in favor of increasing memory resource guarantees for <code class="literal">Guaranteed</code> and <code class="literal">Burstable</code> workloads.
							</li><li class="listitem">
								A value of <code class="literal">qos-reserved=memory=50%</code> will allow the <code class="literal">Burstable</code> and <code class="literal">BestEffort</code> QoS classes to consume half of the memory requested by a higher QoS class.
							</li><li class="listitem">
								A value of <code class="literal">qos-reserved=memory=0%</code> will allow a <code class="literal">Burstable</code> and <code class="literal">BestEffort</code> QoS classes to consume up to the full node allocatable amount if available, but increases the risk that a <code class="literal">Guaranteed</code> workload will not have access to requested memory. This condition effectively disables this feature.
							</li></ul></div></section></section><section class="section" id="nodes-qos-about-swap_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.11.3. Understanding swap memory and QOS</h3></div></div></div><p>
					You can disable swap by default on your nodes to preserve quality of service (QOS) guarantees. Otherwise, physical resources on a node can oversubscribe, affecting the resource guarantees the Kubernetes scheduler makes during pod placement.
				</p><p>
					For example, if two guaranteed pods have reached their memory limit, each container could start using swap memory. Eventually, if there is not enough swap space, processes in the pods can be terminated due to the system being oversubscribed.
				</p><p>
					Failing to disable swap results in nodes not recognizing that they are experiencing <span class="strong strong"><strong>MemoryPressure</strong></span>, resulting in pods not receiving the memory they made in their scheduling request. As a result, additional pods are placed on the node to further increase memory pressure, ultimately increasing your risk of experiencing a system out of memory (OOM) event.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						If swap is enabled, any out-of-resource handling eviction thresholds for available memory will not work as expected. Take advantage of out-of-resource handling to allow pods to be evicted from a node when it is under memory pressure, and rescheduled on an alternative node that has no such pressure.
					</p></div></div></section><section class="section" id="nodes-cluster-overcommit-configure-nodes_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.11.4. Understanding nodes overcommitment</h3></div></div></div><p>
					In an overcommitted environment, it is important to properly configure your node to provide best system behavior.
				</p><p>
					When the node starts, it ensures that the kernel tunable flags for memory management are set properly. The kernel should never fail memory allocations unless it runs out of physical memory.
				</p><p>
					To ensure this behavior, OpenShift Container Platform configures the kernel to always overcommit memory by setting the <code class="literal">vm.overcommit_memory</code> parameter to <code class="literal">1</code>, overriding the default operating system setting.
				</p><p>
					OpenShift Container Platform also configures the kernel not to panic when it runs out of memory by setting the <code class="literal">vm.panic_on_oom</code> parameter to <code class="literal">0</code>. A setting of 0 instructs the kernel to call oom_killer in an Out of Memory (OOM) condition, which kills processes based on priority
				</p><p>
					You can view the current setting by running the following commands on your nodes:
				</p><pre class="programlisting language-terminal">$ sysctl -a |grep commit</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">#...
vm.overcommit_memory = 0
#...</pre>

					</p></div><pre class="programlisting language-terminal">$ sysctl -a |grep panic</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">#...
vm.panic_on_oom = 0
#...</pre>

					</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The above flags should already be set on nodes, and no further action is required.
					</p></div></div><p>
					You can also perform the following configurations for each node:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Disable or enforce CPU limits using CPU CFS quotas
						</li><li class="listitem">
							Reserve resources for system processes
						</li><li class="listitem">
							Reserve memory across quality of service tiers
						</li></ul></div></section><section class="section" id="nodes-cluster-overcommit-node-enforcing_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.11.5. Disabling or enforcing CPU limits using CPU CFS quotas</h3></div></div></div><p>
					Nodes by default enforce specified CPU limits using the Completely Fair Scheduler (CFS) quota support in the Linux kernel.
				</p><p>
					If you disable CPU limit enforcement, it is important to understand the impact on your node:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							If a container has a CPU request, the request continues to be enforced by CFS shares in the Linux kernel.
						</li><li class="listitem">
							If a container does not have a CPU request, but does have a CPU limit, the CPU request defaults to the specified CPU limit, and is enforced by CFS shares in the Linux kernel.
						</li><li class="listitem">
							If a container has both a CPU request and limit, the CPU request is enforced by CFS shares in the Linux kernel, and the CPU limit has no impact on the node.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Obtain the label associated with the static <code class="literal">MachineConfigPool</code> CRD for the type of node you want to configure by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc edit machineconfigpool &lt;name&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc edit machineconfigpool worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: "2022-11-16T15:34:25Z"
  generation: 4
  labels:
    pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO114-1"><!--Empty--></span><span class="callout">1</span>
  name: worker</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO114-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The label appears under Labels.
								</div></dd></dl></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							If the label is not present, add a key/value pair such as:
						</p><pre class="programlisting language-terminal">$ oc label machineconfigpool worker custom-kubelet=small-pods</pre></div></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a custom resource (CR) for your configuration change.
						</p><div class="formalpara"><p class="title"><strong>Sample configuration for a disabling CPU limits</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: disable-cpu-units <span id="CO115-1"><!--Empty--></span><span class="callout">1</span>
spec:
  machineConfigPoolSelector:
    matchLabels:
      pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO115-2"><!--Empty--></span><span class="callout">2</span>
  kubeletConfig:
    cpuCfsQuota: false <span id="CO115-3"><!--Empty--></span><span class="callout">3</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO115-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Assign a name to CR.
								</div></dd><dt><a href="#CO115-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the label from the machine config pool.
								</div></dd><dt><a href="#CO115-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Set the <code class="literal">cpuCfsQuota</code> parameter to <code class="literal">false</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Run the following command to create the CR:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div></section><section class="section" id="nodes-cluster-overcommit-node-resources_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.11.6. Reserving resources for system processes</h3></div></div></div><p>
					To provide more reliable scheduling and minimize node resource overcommitment, each node can reserve a portion of its resources for use by system daemons that are required to run on your node for your cluster to function. In particular, it is recommended that you reserve resources for incompressible resources such as memory.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To explicitly reserve resources for non-pod processes, allocate node resources by specifying resources available for scheduling. For more details, see Allocating Resources for Nodes.
					</p></div></section><section class="section" id="nodes-cluster-overcommit-node-disable_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.11.7. Disabling overcommitment for a node</h3></div></div></div><p>
					When enabled, overcommitment can be disabled on each node.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To disable overcommitment in a node run the following command on that node:
					</p></div><pre class="programlisting language-terminal">$ sysctl -w vm.overcommit_memory=0</pre></section></section><section class="section" id="nodes-cluster-project-overcommit_post-install-node-tasks"><div class="titlepage"><div><div><h2 class="title">8.12. Project-level limits</h2></div></div></div><p>
				To help control overcommit, you can set per-project resource limit ranges, specifying memory and CPU limits and defaults for a project that overcommit cannot exceed.
			</p><p>
				For information on project-level resource limits, see Additional resources.
			</p><p>
				Alternatively, you can disable overcommitment for specific projects.
			</p><section class="section" id="nodes-cluster-overcommit-project-disable_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.12.1. Disabling overcommitment for a project</h3></div></div></div><p>
					When enabled, overcommitment can be disabled per-project. For example, you can allow infrastructure components to be configured independently of overcommitment.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To disable overcommitment in a project:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the namespace object to add the following annotation:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
  annotations:
    quota.openshift.io/cluster-resource-override-enabled: "false" <span id="CO116-1"><!--Empty--></span><span class="callout">1</span>
# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO116-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Setting this annotation to <code class="literal">false</code> disables overcommit for this namespace.
								</div></dd></dl></div></li></ol></div></section></section><section class="section" id="post-install-garbage-collection"><div class="titlepage"><div><div><h2 class="title">8.13. Freeing node resources using garbage collection</h2></div></div></div><p>
				Understand and use garbage collection.
			</p><section class="section" id="nodes-nodes-garbage-collection-containers_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.13.1. Understanding how terminated containers are removed through garbage collection</h3></div></div></div><p>
					Container garbage collection removes terminated containers by using eviction thresholds.
				</p><p>
					When eviction thresholds are set for garbage collection, the node tries to keep any container for any pod accessible from the API. If the pod has been deleted, the containers will be as well. Containers are preserved as long the pod is not deleted and the eviction threshold is not reached. If the node is under disk pressure, it will remove containers and their logs will no longer be accessible using <code class="literal">oc logs</code>.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<span class="strong strong"><strong>eviction-soft</strong></span> - A soft eviction threshold pairs an eviction threshold with a required administrator-specified grace period.
						</li><li class="listitem">
							<span class="strong strong"><strong>eviction-hard</strong></span> - A hard eviction threshold has no grace period, and if observed, OpenShift Container Platform takes immediate action.
						</li></ul></div><p>
					The following table lists the eviction thresholds:
				</p><div class="table" id="idm140031636151760"><p class="title"><strong>Table 8.3. Variables for configuring container garbage collection</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031636145968" scope="col">Node condition</th><th align="left" valign="top" id="idm140031636144880" scope="col">Eviction signal</th><th align="left" valign="top" id="idm140031636143792" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031636145968"> <p>
									MemoryPressure
								</p>
								 </td><td align="left" valign="top" headers="idm140031636144880"> <p>
									<code class="literal">memory.available</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636143792"> <p>
									The available memory on the node.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031636145968"> <p>
									DiskPressure
								</p>
								 </td><td align="left" valign="top" headers="idm140031636144880"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">nodefs.available</code>
										</li><li class="listitem">
											<code class="literal">nodefs.inodesFree</code>
										</li><li class="listitem">
											<code class="literal">imagefs.available</code>
										</li><li class="listitem">
											<code class="literal">imagefs.inodesFree</code>
										</li></ul></div>
								 </td><td align="left" valign="top" headers="idm140031636143792"> <p>
									The available disk space or inodes on the node root file system, <code class="literal">nodefs</code>, or image file system, <code class="literal">imagefs</code>.
								</p>
								 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						For <code class="literal">evictionHard</code> you must specify all of these parameters. If you do not specify all parameters, only the specified parameters are applied and the garbage collection will not function properly.
					</p></div></div><p>
					If a node is oscillating above and below a soft eviction threshold, but not exceeding its associated grace period, the corresponding node would constantly oscillate between <code class="literal">true</code> and <code class="literal">false</code>. As a consequence, the scheduler could make poor scheduling decisions.
				</p><p>
					To protect against this oscillation, use the <code class="literal">eviction-pressure-transition-period</code> flag to control how long OpenShift Container Platform must wait before transitioning out of a pressure condition. OpenShift Container Platform will not set an eviction threshold as being met for the specified pressure condition for the period specified before toggling the condition back to false.
				</p></section><section class="section" id="nodes-nodes-garbage-collection-images_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.13.2. Understanding how images are removed through garbage collection</h3></div></div></div><p>
					Image garbage collection removes images that are not referenced by any running pods.
				</p><p>
					OpenShift Container Platform determines which images to remove from a node based on the disk usage that is reported by <span class="strong strong"><strong>cAdvisor</strong></span>.
				</p><p>
					The policy for image garbage collection is based on two conditions:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The percent of disk usage (expressed as an integer) which triggers image garbage collection. The default is <span class="strong strong"><strong>85</strong></span>.
						</li><li class="listitem">
							The percent of disk usage (expressed as an integer) to which image garbage collection attempts to free. Default is <span class="strong strong"><strong>80</strong></span>.
						</li></ul></div><p>
					For image garbage collection, you can modify any of the following variables using a custom resource.
				</p><div class="table" id="idm140031636042016"><p class="title"><strong>Table 8.4. Variables for configuring image garbage collection</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 75%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031636037152" scope="col">Setting</th><th align="left" valign="top" id="idm140031636036064" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031636037152"> <p>
									<code class="literal">imageMinimumGCAge</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636036064"> <p>
									The minimum age for an unused image before the image is removed by garbage collection. The default is <span class="strong strong"><strong>2m</strong></span>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031636037152"> <p>
									<code class="literal">imageGCHighThresholdPercent</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636036064"> <p>
									The percent of disk usage, expressed as an integer, which triggers image garbage collection. The default is <span class="strong strong"><strong>85</strong></span>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031636037152"> <p>
									<code class="literal">imageGCLowThresholdPercent</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636036064"> <p>
									The percent of disk usage, expressed as an integer, to which image garbage collection attempts to free. The default is <span class="strong strong"><strong>80</strong></span>.
								</p>
								 </td></tr></tbody></table></div></div><p>
					Two lists of images are retrieved in each garbage collector run:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							A list of images currently running in at least one pod.
						</li><li class="listitem">
							A list of images available on a host.
						</li></ol></div><p>
					As new containers are run, new images appear. All images are marked with a time stamp. If the image is running (the first list above) or is newly detected (the second list above), it is marked with the current time. The remaining images are already marked from the previous spins. All images are then sorted by the time stamp.
				</p><p>
					Once the collection starts, the oldest images get deleted first until the stopping criterion is met.
				</p></section><section class="section" id="nodes-nodes-garbage-collection-configuring_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.13.3. Configuring garbage collection for containers and images</h3></div></div></div><p>
					As an administrator, you can configure how OpenShift Container Platform performs garbage collection by creating a <code class="literal">kubeletConfig</code> object for each machine config pool.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						OpenShift Container Platform supports only one <code class="literal">kubeletConfig</code> object for each machine config pool.
					</p></div></div><p>
					You can configure any combination of the following:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Soft eviction for containers
						</li><li class="listitem">
							Hard eviction for containers
						</li><li class="listitem">
							Eviction for images
						</li></ul></div><p>
					Container garbage collection removes terminated containers. Image garbage collection removes images that are not referenced by any running pods.
				</p><div class="orderedlist"><p class="title"><strong>Prerequisites</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the label associated with the static <code class="literal">MachineConfigPool</code> CRD for the type of node you want to configure by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc edit machineconfigpool &lt;name&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc edit machineconfigpool worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: "2022-11-16T15:34:25Z"
  generation: 4
  labels:
    pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO117-1"><!--Empty--></span><span class="callout">1</span>
  name: worker
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO117-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The label appears under Labels.
								</div></dd></dl></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							If the label is not present, add a key/value pair such as:
						</p><pre class="screen">$ oc label machineconfigpool worker custom-kubelet=small-pods</pre></div></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a custom resource (CR) for your configuration change.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								If there is one file system, or if <code class="literal">/var/lib/kubelet</code> and <code class="literal">/var/lib/containers/</code> are in the same file system, the settings with the highest values trigger evictions, as those are met first. The file system triggers the eviction.
							</p></div></div><div class="formalpara"><p class="title"><strong>Sample configuration for a container garbage collection CR:</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: worker-kubeconfig <span id="CO118-1"><!--Empty--></span><span class="callout">1</span>
spec:
  machineConfigPoolSelector:
    matchLabels:
      pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO118-2"><!--Empty--></span><span class="callout">2</span>
  kubeletConfig:
    evictionSoft: <span id="CO118-3"><!--Empty--></span><span class="callout">3</span>
      memory.available: "500Mi" <span id="CO118-4"><!--Empty--></span><span class="callout">4</span>
      nodefs.available: "10%"
      nodefs.inodesFree: "5%"
      imagefs.available: "15%"
      imagefs.inodesFree: "10%"
    evictionSoftGracePeriod:  <span id="CO118-5"><!--Empty--></span><span class="callout">5</span>
      memory.available: "1m30s"
      nodefs.available: "1m30s"
      nodefs.inodesFree: "1m30s"
      imagefs.available: "1m30s"
      imagefs.inodesFree: "1m30s"
    evictionHard: <span id="CO118-6"><!--Empty--></span><span class="callout">6</span>
      memory.available: "200Mi"
      nodefs.available: "5%"
      nodefs.inodesFree: "4%"
      imagefs.available: "10%"
      imagefs.inodesFree: "5%"
    evictionPressureTransitionPeriod: 0s <span id="CO118-7"><!--Empty--></span><span class="callout">7</span>
    imageMinimumGCAge: 5m <span id="CO118-8"><!--Empty--></span><span class="callout">8</span>
    imageGCHighThresholdPercent: 80 <span id="CO118-9"><!--Empty--></span><span class="callout">9</span>
    imageGCLowThresholdPercent: 75 <span id="CO118-10"><!--Empty--></span><span class="callout">10</span>
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO118-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Name for the object.
								</div></dd><dt><a href="#CO118-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the label from the machine config pool.
								</div></dd><dt><a href="#CO118-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									For container garbage collection: Type of eviction: <code class="literal">evictionSoft</code> or <code class="literal">evictionHard</code>.
								</div></dd><dt><a href="#CO118-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									For container garbage collection: Eviction thresholds based on a specific eviction trigger signal.
								</div></dd><dt><a href="#CO118-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									For container garbage collection: Grace periods for the soft eviction. This parameter does not apply to <code class="literal">eviction-hard</code>.
								</div></dd><dt><a href="#CO118-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									For container garbage collection: Eviction thresholds based on a specific eviction trigger signal. For <code class="literal">evictionHard</code> you must specify all of these parameters. If you do not specify all parameters, only the specified parameters are applied and the garbage collection will not function properly.
								</div></dd><dt><a href="#CO118-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									For container garbage collection: The duration to wait before transitioning out of an eviction pressure condition.
								</div></dd><dt><a href="#CO118-8"><span class="callout">8</span></a> </dt><dd><div class="para">
									For image garbage collection: The minimum age for an unused image before the image is removed by garbage collection.
								</div></dd><dt><a href="#CO118-9"><span class="callout">9</span></a> </dt><dd><div class="para">
									For image garbage collection: The percent of disk usage (expressed as an integer) that triggers image garbage collection.
								</div></dd><dt><a href="#CO118-10"><span class="callout">10</span></a> </dt><dd><div class="para">
									For image garbage collection: The percent of disk usage (expressed as an integer) that image garbage collection attempts to free.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Run the following command to create the CR:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc create -f gc-container.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">kubeletconfig.machineconfiguration.openshift.io/gc-container created</pre>

							</p></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Verify that garbage collection is active by entering the following command. The Machine Config Pool you specified in the custom resource appears with <code class="literal">UPDATING</code> as 'true` until the change is fully implemented:
						</p><pre class="programlisting language-terminal">$ oc get machineconfigpool</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME     CONFIG                                   UPDATED   UPDATING
master   rendered-master-546383f80705bd5aeaba93   True      False
worker   rendered-worker-b4c51bb33ccaae6fc4a6a5   False     True</pre>

							</p></div></li></ol></div></section></section><section class="section" id="post-using-node-tuning-operator"><div class="titlepage"><div><div><h2 class="title">8.14. Using the Node Tuning Operator</h2></div></div></div><p>
				Understand and use the Node Tuning Operator.
			</p><h5 id="about-node-tuning-operator_post-install-node-tasks">Purpose</h5><p>
				The Node Tuning Operator helps you manage node-level tuning by orchestrating the TuneD daemon and achieves low latency performance by using the Performance Profile controller. The majority of high-performance applications require some level of kernel tuning. The Node Tuning Operator provides a unified management interface to users of node-level sysctls and more flexibility to add custom tuning specified by user needs.
			</p><p>
				The Operator manages the containerized TuneD daemon for OpenShift Container Platform as a Kubernetes daemon set. It ensures the custom tuning specification is passed to all containerized TuneD daemons running in the cluster in the format that the daemons understand. The daemons run on all nodes in the cluster, one per node.
			</p><p>
				Node-level settings applied by the containerized TuneD daemon are rolled back on an event that triggers a profile change or when the containerized TuneD daemon is terminated gracefully by receiving and handling a termination signal.
			</p><p>
				The Node Tuning Operator uses the Performance Profile controller to implement automatic tuning to achieve low latency performance for OpenShift Container Platform applications.
			</p><p>
				The cluster administrator configures a performance profile to define node-level settings such as the following:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Updating the kernel to kernel-rt.
					</li><li class="listitem">
						Choosing CPUs for housekeeping.
					</li><li class="listitem">
						Choosing CPUs for running workloads.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Currently, disabling CPU load balancing is not supported by cgroup v2. As a result, you might not get the desired behavior from performance profiles if you have cgroup v2 enabled. Enabling cgroup v2 is not recommended if you are using performance profiles.
				</p></div></div><p>
				The Node Tuning Operator is part of a standard OpenShift Container Platform installation in version 4.1 and later.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					In earlier versions of OpenShift Container Platform, the Performance Addon Operator was used to implement automatic tuning to achieve low latency performance for OpenShift applications. In OpenShift Container Platform 4.11 and later, this functionality is part of the Node Tuning Operator.
				</p></div></div><section class="section" id="accessing-an-example-node-tuning-operator-specification_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.14.1. Accessing an example Node Tuning Operator specification</h3></div></div></div><p>
					Use this process to access an example Node Tuning Operator specification.
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Run the following command to access an example Node Tuning Operator specification:
						</p><pre class="programlisting language-terminal">$ oc get Tuned/default -o yaml -n openshift-cluster-node-tuning-operator</pre></li></ul></div><p>
					The default CR is meant for delivering standard node-level tuning for the OpenShift Container Platform platform and it can only be modified to set the Operator Management state. Any other custom changes to the default CR will be overwritten by the Operator. For custom tuning, create your own Tuned CRs. Newly created CRs will be combined with the default CR and custom tuning applied to OpenShift Container Platform nodes based on node or pod labels and profile priorities.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						While in certain situations the support for pod labels can be a convenient way of automatically delivering required tuning, this practice is discouraged and strongly advised against, especially in large-scale clusters. The default Tuned CR ships without pod label matching. If a custom profile is created with pod label matching, then the functionality will be enabled at that time. The pod label functionality will be deprecated in future versions of the Node Tuning Operator.
					</p></div></div></section><section class="section" id="custom-tuning-specification_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.14.2. Custom tuning specification</h3></div></div></div><p>
					The custom resource (CR) for the Operator has two major sections. The first section, <code class="literal">profile:</code>, is a list of TuneD profiles and their names. The second, <code class="literal">recommend:</code>, defines the profile selection logic.
				</p><p>
					Multiple custom tuning specifications can co-exist as multiple CRs in the Operator’s namespace. The existence of new CRs or the deletion of old CRs is detected by the Operator. All existing custom tuning specifications are merged and appropriate objects for the containerized TuneD daemons are updated.
				</p><p>
					<span class="strong strong"><strong>Management state</strong></span>
				</p><p>
					The Operator Management state is set by adjusting the default Tuned CR. By default, the Operator is in the Managed state and the <code class="literal">spec.managementState</code> field is not present in the default Tuned CR. Valid values for the Operator Management state are as follows:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Managed: the Operator will update its operands as configuration resources are updated
						</li><li class="listitem">
							Unmanaged: the Operator will ignore changes to the configuration resources
						</li><li class="listitem">
							Removed: the Operator will remove its operands and resources the Operator provisioned
						</li></ul></div><p>
					<span class="strong strong"><strong>Profile data</strong></span>
				</p><p>
					The <code class="literal">profile:</code> section lists TuneD profiles and their names.
				</p><pre class="programlisting language-yaml">profile:
- name: tuned_profile_1
  data: |
    # TuneD profile specification
    [main]
    summary=Description of tuned_profile_1 profile

    [sysctl]
    net.ipv4.ip_forward=1
    # ... other sysctl's or other TuneD daemon plugins supported by the containerized TuneD

# ...

- name: tuned_profile_n
  data: |
    # TuneD profile specification
    [main]
    summary=Description of tuned_profile_n profile

    # tuned_profile_n profile settings</pre><p>
					<span class="strong strong"><strong>Recommended profiles</strong></span>
				</p><p>
					The <code class="literal">profile:</code> selection logic is defined by the <code class="literal">recommend:</code> section of the CR. The <code class="literal">recommend:</code> section is a list of items to recommend the profiles based on a selection criteria.
				</p><pre class="programlisting language-yaml">recommend:
&lt;recommend-item-1&gt;
# ...
&lt;recommend-item-n&gt;</pre><p>
					The individual items of the list:
				</p><pre class="programlisting language-yaml">- machineConfigLabels: <span id="CO119-1"><!--Empty--></span><span class="callout">1</span>
    &lt;mcLabels&gt; <span id="CO119-2"><!--Empty--></span><span class="callout">2</span>
  match: <span id="CO119-3"><!--Empty--></span><span class="callout">3</span>
    &lt;match&gt; <span id="CO119-4"><!--Empty--></span><span class="callout">4</span>
  priority: &lt;priority&gt; <span id="CO119-5"><!--Empty--></span><span class="callout">5</span>
  profile: &lt;tuned_profile_name&gt; <span id="CO119-6"><!--Empty--></span><span class="callout">6</span>
  operand: <span id="CO119-7"><!--Empty--></span><span class="callout">7</span>
    debug: &lt;bool&gt; <span id="CO119-8"><!--Empty--></span><span class="callout">8</span>
    tunedConfig:
      reapply_sysctl: &lt;bool&gt; <span id="CO119-9"><!--Empty--></span><span class="callout">9</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO119-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Optional.
						</div></dd><dt><a href="#CO119-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							A dictionary of key/value <code class="literal">MachineConfig</code> labels. The keys must be unique.
						</div></dd><dt><a href="#CO119-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							If omitted, profile match is assumed unless a profile with a higher priority matches first or <code class="literal">machineConfigLabels</code> is set.
						</div></dd><dt><a href="#CO119-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							An optional list.
						</div></dd><dt><a href="#CO119-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Profile ordering priority. Lower numbers mean higher priority (<code class="literal">0</code> is the highest priority).
						</div></dd><dt><a href="#CO119-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							A TuneD profile to apply on a match. For example <code class="literal">tuned_profile_1</code>.
						</div></dd><dt><a href="#CO119-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							Optional operand configuration.
						</div></dd><dt><a href="#CO119-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							Turn debugging on or off for the TuneD daemon. Options are <code class="literal">true</code> for on or <code class="literal">false</code> for off. The default is <code class="literal">false</code>.
						</div></dd><dt><a href="#CO119-9"><span class="callout">9</span></a> </dt><dd><div class="para">
							Turn <code class="literal">reapply_sysctl</code> functionality on or off for the TuneD daemon. Options are <code class="literal">true</code> for on and <code class="literal">false</code> for off.
						</div></dd></dl></div><p>
					<code class="literal">&lt;match&gt;</code> is an optional list recursively defined as follows:
				</p><pre class="programlisting language-yaml">- label: &lt;label_name&gt; <span id="CO120-1"><!--Empty--></span><span class="callout">1</span>
  value: &lt;label_value&gt; <span id="CO120-2"><!--Empty--></span><span class="callout">2</span>
  type: &lt;label_type&gt; <span id="CO120-3"><!--Empty--></span><span class="callout">3</span>
    &lt;match&gt; <span id="CO120-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO120-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Node or pod label name.
						</div></dd><dt><a href="#CO120-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Optional node or pod label value. If omitted, the presence of <code class="literal">&lt;label_name&gt;</code> is enough to match.
						</div></dd><dt><a href="#CO120-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Optional object type (<code class="literal">node</code> or <code class="literal">pod</code>). If omitted, <code class="literal">node</code> is assumed.
						</div></dd><dt><a href="#CO120-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							An optional <code class="literal">&lt;match&gt;</code> list.
						</div></dd></dl></div><p>
					If <code class="literal">&lt;match&gt;</code> is not omitted, all nested <code class="literal">&lt;match&gt;</code> sections must also evaluate to <code class="literal">true</code>. Otherwise, <code class="literal">false</code> is assumed and the profile with the respective <code class="literal">&lt;match&gt;</code> section will not be applied or recommended. Therefore, the nesting (child <code class="literal">&lt;match&gt;</code> sections) works as logical AND operator. Conversely, if any item of the <code class="literal">&lt;match&gt;</code> list matches, the entire <code class="literal">&lt;match&gt;</code> list evaluates to <code class="literal">true</code>. Therefore, the list acts as logical OR operator.
				</p><p>
					If <code class="literal">machineConfigLabels</code> is defined, machine config pool based matching is turned on for the given <code class="literal">recommend:</code> list item. <code class="literal">&lt;mcLabels&gt;</code> specifies the labels for a machine config. The machine config is created automatically to apply host settings, such as kernel boot parameters, for the profile <code class="literal">&lt;tuned_profile_name&gt;</code>. This involves finding all machine config pools with machine config selector matching <code class="literal">&lt;mcLabels&gt;</code> and setting the profile <code class="literal">&lt;tuned_profile_name&gt;</code> on all nodes that are assigned the found machine config pools. To target nodes that have both master and worker roles, you must use the master role.
				</p><p>
					The list items <code class="literal">match</code> and <code class="literal">machineConfigLabels</code> are connected by the logical OR operator. The <code class="literal">match</code> item is evaluated first in a short-circuit manner. Therefore, if it evaluates to <code class="literal">true</code>, the <code class="literal">machineConfigLabels</code> item is not considered.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						When using machine config pool based matching, it is advised to group nodes with the same hardware configuration into the same machine config pool. Not following this practice might result in TuneD operands calculating conflicting kernel parameters for two or more nodes sharing the same machine config pool.
					</p></div></div><div class="formalpara"><p class="title"><strong>Example: node or pod label based matching</strong></p><p>
						
<pre class="programlisting language-yaml">- match:
  - label: tuned.openshift.io/elasticsearch
    match:
    - label: node-role.kubernetes.io/master
    - label: node-role.kubernetes.io/infra
    type: pod
  priority: 10
  profile: openshift-control-plane-es
- match:
  - label: node-role.kubernetes.io/master
  - label: node-role.kubernetes.io/infra
  priority: 20
  profile: openshift-control-plane
- priority: 30
  profile: openshift-node</pre>

					</p></div><p>
					The CR above is translated for the containerized TuneD daemon into its <code class="literal">recommend.conf</code> file based on the profile priorities. The profile with the highest priority (<code class="literal">10</code>) is <code class="literal">openshift-control-plane-es</code> and, therefore, it is considered first. The containerized TuneD daemon running on a given node looks to see if there is a pod running on the same node with the <code class="literal">tuned.openshift.io/elasticsearch</code> label set. If not, the entire <code class="literal">&lt;match&gt;</code> section evaluates as <code class="literal">false</code>. If there is such a pod with the label, in order for the <code class="literal">&lt;match&gt;</code> section to evaluate to <code class="literal">true</code>, the node label also needs to be <code class="literal">node-role.kubernetes.io/master</code> or <code class="literal">node-role.kubernetes.io/infra</code>.
				</p><p>
					If the labels for the profile with priority <code class="literal">10</code> matched, <code class="literal">openshift-control-plane-es</code> profile is applied and no other profile is considered. If the node/pod label combination did not match, the second highest priority profile (<code class="literal">openshift-control-plane</code>) is considered. This profile is applied if the containerized TuneD pod runs on a node with labels <code class="literal">node-role.kubernetes.io/master</code> or <code class="literal">node-role.kubernetes.io/infra</code>.
				</p><p>
					Finally, the profile <code class="literal">openshift-node</code> has the lowest priority of <code class="literal">30</code>. It lacks the <code class="literal">&lt;match&gt;</code> section and, therefore, will always match. It acts as a profile catch-all to set <code class="literal">openshift-node</code> profile, if no other profile with higher priority matches on a given node.
				</p><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Post-installation_configuration-en-US/images/b350c395f7c7262cec5e5d9d7404ce73/node-tuning-operator-workflow-revised.png" alt="Decision workflow"/></div></div><div class="formalpara"><p class="title"><strong>Example: machine config pool based matching</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: openshift-node-custom
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=Custom OpenShift node profile with an additional kernel parameter
      include=openshift-node
      [bootloader]
      cmdline_openshift_node_custom=+skew_tick=1
    name: openshift-node-custom

  recommend:
  - machineConfigLabels:
      machineconfiguration.openshift.io/role: "worker-custom"
    priority: 20
    profile: openshift-node-custom</pre>

					</p></div><p>
					To minimize node reboots, label the target nodes with a label the machine config pool’s node selector will match, then create the Tuned CR above and finally create the custom machine config pool itself.
				</p><p>
					<span class="strong strong"><strong>Cloud provider-specific TuneD profiles</strong></span>
				</p><p>
					With this functionality, all Cloud provider-specific nodes can conveniently be assigned a TuneD profile specifically tailored to a given Cloud provider on a OpenShift Container Platform cluster. This can be accomplished without adding additional node labels or grouping nodes into machine config pools.
				</p><p>
					This functionality takes advantage of <code class="literal">spec.providerID</code> node object values in the form of <code class="literal">&lt;cloud-provider&gt;://&lt;cloud-provider-specific-id&gt;</code> and writes the file <code class="literal">/var/lib/tuned/provider</code> with the value <code class="literal">&lt;cloud-provider&gt;</code> in NTO operand containers. The content of this file is then used by TuneD to load <code class="literal">provider-&lt;cloud-provider&gt;</code> profile if such profile exists.
				</p><p>
					The <code class="literal">openshift</code> profile that both <code class="literal">openshift-control-plane</code> and <code class="literal">openshift-node</code> profiles inherit settings from is now updated to use this functionality through the use of conditional profile loading. Neither NTO nor TuneD currently include any Cloud provider-specific profiles. However, it is possible to create a custom profile <code class="literal">provider-&lt;cloud-provider&gt;</code> that will be applied to all Cloud provider-specific cluster nodes.
				</p><div class="formalpara"><p class="title"><strong>Example GCE Cloud provider profile</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: provider-gce
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=GCE Cloud provider-specific profile
      # Your tuning for GCE Cloud provider goes here.
    name: provider-gce</pre>

					</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Due to profile inheritance, any setting specified in the <code class="literal">provider-&lt;cloud-provider&gt;</code> profile will be overwritten by the <code class="literal">openshift</code> profile and its child profiles.
					</p></div></div></section><section class="section" id="custom-tuning-default-profiles-set_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.14.3. Default profiles set on a cluster</h3></div></div></div><p>
					The following are the default profiles set on a cluster.
				</p><pre class="programlisting language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: default
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=Optimize systems running OpenShift (provider specific parent profile)
      include=-provider-${f:exec:cat:/var/lib/tuned/provider},openshift
    name: openshift
  recommend:
  - profile: openshift-control-plane
    priority: 30
    match:
    - label: node-role.kubernetes.io/master
    - label: node-role.kubernetes.io/infra
  - profile: openshift-node
    priority: 40</pre><p>
					Starting with OpenShift Container Platform 4.9, all OpenShift TuneD profiles are shipped with the TuneD package. You can use the <code class="literal">oc exec</code> command to view the contents of these profiles:
				</p><pre class="programlisting language-terminal">$ oc exec $tuned_pod -n openshift-cluster-node-tuning-operator -- find /usr/lib/tuned/openshift{,-control-plane,-node} -name tuned.conf -exec grep -H ^ {} \;</pre></section><section class="section" id="supported-tuned-daemon-plug-ins_post-install-node-tasks"><div class="titlepage"><div><div><h3 class="title">8.14.4. Supported TuneD daemon plugins</h3></div></div></div><p>
					Excluding the <code class="literal">[main]</code> section, the following TuneD plugins are supported when using custom profiles defined in the <code class="literal">profile:</code> section of the Tuned CR:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							audio
						</li><li class="listitem">
							cpu
						</li><li class="listitem">
							disk
						</li><li class="listitem">
							eeepc_she
						</li><li class="listitem">
							modules
						</li><li class="listitem">
							mounts
						</li><li class="listitem">
							net
						</li><li class="listitem">
							scheduler
						</li><li class="listitem">
							scsi_host
						</li><li class="listitem">
							selinux
						</li><li class="listitem">
							sysctl
						</li><li class="listitem">
							sysfs
						</li><li class="listitem">
							usb
						</li><li class="listitem">
							video
						</li><li class="listitem">
							vm
						</li><li class="listitem">
							bootloader
						</li></ul></div><p>
					There is some dynamic tuning functionality provided by some of these plugins that is not supported. The following TuneD plugins are currently not supported:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							script
						</li><li class="listitem">
							systemd
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The TuneD bootloader plugin only supports Red Hat Enterprise Linux CoreOS (RHCOS) worker nodes.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/customizing-tuned-profiles_monitoring-and-managing-system-status-and-performance#available-tuned-plug-ins_customizing-tuned-profiles">Available TuneD Plugins</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/getting-started-with-tuned_monitoring-and-managing-system-status-and-performance">Getting Started with TuneD</a>
						</li></ul></div></section></section><section class="section" id="nodes-nodes-managing-max-pods-proc_post-install-node-tasks"><div class="titlepage"><div><div><h2 class="title">8.15. Configuring the maximum number of pods per node</h2></div></div></div><p>
				Two parameters control the maximum number of pods that can be scheduled to a node: <code class="literal">podsPerCore</code> and <code class="literal">maxPods</code>. If you use both options, the lower of the two limits the number of pods on a node.
			</p><p>
				For example, if <code class="literal">podsPerCore</code> is set to <code class="literal">10</code> on a node with 4 processor cores, the maximum number of pods allowed on the node will be 40.
			</p><div class="orderedlist"><p class="title"><strong>Prerequisites</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Obtain the label associated with the static <code class="literal">MachineConfigPool</code> CRD for the type of node you want to configure by entering the following command:
					</p><pre class="programlisting language-terminal">$ oc edit machineconfigpool &lt;name&gt;</pre><p class="simpara">
						For example:
					</p><pre class="programlisting language-terminal">$ oc edit machineconfigpool worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: "2022-11-16T15:34:25Z"
  generation: 4
  labels:
    pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO121-1"><!--Empty--></span><span class="callout">1</span>
  name: worker
#...</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO121-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The label appears under Labels.
							</div></dd></dl></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
						If the label is not present, add a key/value pair such as:
					</p><pre class="screen">$ oc label machineconfigpool worker custom-kubelet=small-pods</pre></div></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a custom resource (CR) for your configuration change.
					</p><div class="formalpara"><p class="title"><strong>Sample configuration for a <code class="literal">max-pods</code> CR</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-max-pods <span id="CO122-1"><!--Empty--></span><span class="callout">1</span>
spec:
  machineConfigPoolSelector:
    matchLabels:
      pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO122-2"><!--Empty--></span><span class="callout">2</span>
  kubeletConfig:
    podsPerCore: 10 <span id="CO122-3"><!--Empty--></span><span class="callout">3</span>
    maxPods: 250 <span id="CO122-4"><!--Empty--></span><span class="callout">4</span>
#...</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO122-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Assign a name to CR.
							</div></dd><dt><a href="#CO122-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specify the label from the machine config pool.
							</div></dd><dt><a href="#CO122-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specify the number of pods the node can run based on the number of processor cores on the node.
							</div></dd><dt><a href="#CO122-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Specify the number of pods the node can run to a fixed value, regardless of the properties of the node.
							</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Setting <code class="literal">podsPerCore</code> to <code class="literal">0</code> disables this limit.
						</p></div></div><p class="simpara">
						In the above example, the default value for <code class="literal">podsPerCore</code> is <code class="literal">10</code> and the default value for <code class="literal">maxPods</code> is <code class="literal">250</code>. This means that unless the node has 25 cores or more, by default, <code class="literal">podsPerCore</code> will be the limiting factor.
					</p></li><li class="listitem"><p class="simpara">
						Run the following command to create the CR:
					</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						List the <code class="literal">MachineConfigPool</code> CRDs to see if the change is applied. The <code class="literal">UPDATING</code> column reports <code class="literal">True</code> if the change is picked up by the Machine Config Controller:
					</p><pre class="programlisting language-terminal">$ oc get machineconfigpools</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME     CONFIG                        UPDATED   UPDATING   DEGRADED
master   master-9cc2c72f205e103bb534   False     False      False
worker   worker-8cecd1236b33ee3f8a5e   False     True       False</pre>

						</p></div><p class="simpara">
						Once the change is complete, the <code class="literal">UPDATED</code> column reports <code class="literal">True</code>.
					</p><pre class="programlisting language-terminal">$ oc get machineconfigpools</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME     CONFIG                        UPDATED   UPDATING   DEGRADED
master   master-9cc2c72f205e103bb534   False     True       False
worker   worker-8cecd1236b33ee3f8a5e   True      False      False</pre>

						</p></div></li></ol></div></section></section><section class="chapter" id="post-install-network-configuration"><div class="titlepage"><div><div><h1 class="title">Chapter 9. Post-installation network configuration</h1></div></div></div><p>
			After installing OpenShift Container Platform, you can further expand and customize your network to your requirements.
		</p><section class="section" id="nw-operator-cr_post-install-network-configuration"><div class="titlepage"><div><div><h2 class="title">9.1. Cluster Network Operator configuration</h2></div></div></div><p>
				The configuration for the cluster network is specified as part of the Cluster Network Operator (CNO) configuration and stored in a custom resource (CR) object that is named <code class="literal">cluster</code>. The CR specifies the fields for the <code class="literal">Network</code> API in the <code class="literal">operator.openshift.io</code> API group.
			</p><p>
				The CNO configuration inherits the following fields during cluster installation from the <code class="literal">Network</code> API in the <code class="literal">Network.config.openshift.io</code> API group and these fields cannot be changed:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">clusterNetwork</code></span></dt><dd>
							IP address pools from which pod IP addresses are allocated.
						</dd><dt><span class="term"><code class="literal">serviceNetwork</code></span></dt><dd>
							IP address pool for services.
						</dd><dt><span class="term"><code class="literal">defaultNetwork.type</code></span></dt><dd>
							Cluster network plugin, such as OpenShift SDN or OVN-Kubernetes.
						</dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					After cluster installation, you cannot modify the fields listed in the previous section.
				</p></div></div></section><section class="section" id="nw-proxy-configure-object_post-install-network-configuration"><div class="titlepage"><div><div><h2 class="title">9.2. Enabling the cluster-wide proxy</h2></div></div></div><p>
				The <code class="literal">Proxy</code> object is used to manage the cluster-wide egress proxy. When a cluster is installed or upgraded without the proxy configured, a <code class="literal">Proxy</code> object is still generated but it will have a nil <code class="literal">spec</code>. For example:
			</p><pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec:
  trustedCA:
    name: ""
status:</pre><p>
				A cluster administrator can configure the proxy for OpenShift Container Platform by modifying this <code class="literal">cluster</code> <code class="literal">Proxy</code> object.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Only the <code class="literal">Proxy</code> object named <code class="literal">cluster</code> is supported, and no additional proxies can be created.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Cluster administrator permissions
					</li><li class="listitem">
						OpenShift Container Platform <code class="literal">oc</code> CLI tool installed
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a config map that contains any additional CA certificates required for proxying HTTPS connections.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You can skip this step if the proxy’s identity certificate is signed by an authority from the RHCOS trust bundle.
						</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create a file called <code class="literal">user-ca-bundle.yaml</code> with the following contents, and provide the values of your PEM-encoded certificates:
							</p><pre class="programlisting language-yaml">apiVersion: v1
data:
  ca-bundle.crt: | <span id="CO123-1"><!--Empty--></span><span class="callout">1</span>
    &lt;MY_PEM_ENCODED_CERTS&gt; <span id="CO123-2"><!--Empty--></span><span class="callout">2</span>
kind: ConfigMap
metadata:
  name: user-ca-bundle <span id="CO123-3"><!--Empty--></span><span class="callout">3</span>
  namespace: openshift-config <span id="CO123-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO123-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										This data key must be named <code class="literal">ca-bundle.crt</code>.
									</div></dd><dt><a href="#CO123-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										One or more PEM-encoded X.509 certificates used to sign the proxy’s identity certificate.
									</div></dd><dt><a href="#CO123-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The config map name that will be referenced from the <code class="literal">Proxy</code> object.
									</div></dd><dt><a href="#CO123-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										The config map must be in the <code class="literal">openshift-config</code> namespace.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create the config map from this file:
							</p><pre class="programlisting language-terminal">$ oc create -f user-ca-bundle.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Use the <code class="literal">oc edit</code> command to modify the <code class="literal">Proxy</code> object:
					</p><pre class="programlisting language-terminal">$ oc edit proxy/cluster</pre></li><li class="listitem"><p class="simpara">
						Configure the necessary fields for the proxy:
					</p><pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec:
  httpProxy: http://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt; <span id="CO124-1"><!--Empty--></span><span class="callout">1</span>
  httpsProxy: https://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt; <span id="CO124-2"><!--Empty--></span><span class="callout">2</span>
  noProxy: example.com <span id="CO124-3"><!--Empty--></span><span class="callout">3</span>
  readinessEndpoints:
  - http://www.google.com <span id="CO124-4"><!--Empty--></span><span class="callout">4</span>
  - https://www.google.com
  trustedCA:
    name: user-ca-bundle <span id="CO124-5"><!--Empty--></span><span class="callout">5</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO124-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								A proxy URL to use for creating HTTP connections outside the cluster. The URL scheme must be <code class="literal">http</code>.
							</div></dd><dt><a href="#CO124-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								A proxy URL to use for creating HTTPS connections outside the cluster. The URL scheme must be either <code class="literal">http</code> or <code class="literal">https</code>. Specify a URL for the proxy that supports the URL scheme. For example, most proxies will report an error if they are configured to use <code class="literal">https</code> but they only support <code class="literal">http</code>. This failure message may not propagate to the logs and can appear to be a network connection failure instead. If using a proxy that listens for <code class="literal">https</code> connections from the cluster, you may need to configure the cluster to accept the CAs and certificates that the proxy uses.
							</div></dd><dt><a href="#CO124-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								A comma-separated list of destination domain names, domains, IP addresses or other network CIDRs to exclude proxying.
							</div><p>
								Preface a domain with <code class="literal">.</code> to match subdomains only. For example, <code class="literal">.y.com</code> matches <code class="literal">x.y.com</code>, but not <code class="literal">y.com</code>. Use <code class="literal">*</code> to bypass proxy for all destinations. If you scale up workers that are not included in the network defined by the <code class="literal">networking.machineNetwork[].cidr</code> field from the installation configuration, you must add them to this list to prevent connection issues.
							</p><p>
								This field is ignored if neither the <code class="literal">httpProxy</code> or <code class="literal">httpsProxy</code> fields are set.
							</p></dd><dt><a href="#CO124-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								One or more URLs external to the cluster to use to perform a readiness check before writing the <code class="literal">httpProxy</code> and <code class="literal">httpsProxy</code> values to status.
							</div></dd><dt><a href="#CO124-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								A reference to the config map in the <code class="literal">openshift-config</code> namespace that contains additional CA certificates required for proxying HTTPS connections. Note that the config map must already exist before referencing it here. This field is required unless the proxy’s identity certificate is signed by an authority from the RHCOS trust bundle.
							</div></dd></dl></div></li><li class="listitem">
						Save the file to apply the changes.
					</li></ol></div></section><section class="section" id="private-clusters-setting-dns-private_post-install-network-configuration"><div class="titlepage"><div><div><h2 class="title">9.3. Setting DNS to private</h2></div></div></div><p>
				After you deploy a cluster, you can modify its DNS to use only a private zone.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Review the <code class="literal">DNS</code> custom resource for your cluster:
					</p><pre class="programlisting language-terminal">$ oc get dnses.config.openshift.io/cluster -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: DNS
metadata:
  creationTimestamp: "2019-10-25T18:27:09Z"
  generation: 2
  name: cluster
  resourceVersion: "37966"
  selfLink: /apis/config.openshift.io/v1/dnses/cluster
  uid: 0e714746-f755-11f9-9cb1-02ff55d8f976
spec:
  baseDomain: &lt;base_domain&gt;
  privateZone:
    tags:
      Name: &lt;infrastructure_id&gt;-int
      kubernetes.io/cluster/&lt;infrastructure_id&gt;: owned
  publicZone:
    id: Z2XXXXXXXXXXA4
status: {}</pre>

						</p></div><p class="simpara">
						Note that the <code class="literal">spec</code> section contains both a private and a public zone.
					</p></li><li class="listitem"><p class="simpara">
						Patch the <code class="literal">DNS</code> custom resource to remove the public zone:
					</p><pre class="programlisting language-terminal">$ oc patch dnses.config.openshift.io/cluster --type=merge --patch='{"spec": {"publicZone": null}}'
dns.config.openshift.io/cluster patched</pre><p class="simpara">
						Because the Ingress Controller consults the <code class="literal">DNS</code> definition when it creates <code class="literal">Ingress</code> objects, when you create or modify <code class="literal">Ingress</code> objects, only private records are created.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							DNS records for the existing Ingress objects are not modified when you remove the public zone.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Optional: Review the <code class="literal">DNS</code> custom resource for your cluster and confirm that the public zone was removed:
					</p><pre class="programlisting language-terminal">$ oc get dnses.config.openshift.io/cluster -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: DNS
metadata:
  creationTimestamp: "2019-10-25T18:27:09Z"
  generation: 2
  name: cluster
  resourceVersion: "37966"
  selfLink: /apis/config.openshift.io/v1/dnses/cluster
  uid: 0e714746-f755-11f9-9cb1-02ff55d8f976
spec:
  baseDomain: &lt;base_domain&gt;
  privateZone:
    tags:
      Name: &lt;infrastructure_id&gt;-int
      kubernetes.io/cluster/&lt;infrastructure_id&gt;-wfpg4: owned
status: {}</pre>

						</p></div></li></ol></div></section><section class="section" id="post-install-configuring_ingress_cluster_traffic"><div class="titlepage"><div><div><h2 class="title">9.4. Configuring ingress cluster traffic</h2></div></div></div><p>
				OpenShift Container Platform provides the following methods for communicating from outside the cluster with services running in the cluster:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						If you have HTTP/HTTPS, use an Ingress Controller.
					</li><li class="listitem">
						If you have a TLS-encrypted protocol other than HTTPS, such as TLS with the SNI header, use an Ingress Controller.
					</li><li class="listitem">
						Otherwise, use a load balancer, an external IP, or a node port.
					</li></ul></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031633869296" scope="col">Method</th><th align="left" valign="top" id="idm140031633868208" scope="col">Purpose</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031633869296"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-ingress-cluster-traffic-ingress-controller">Use an Ingress Controller</a>
							</p>
							 </td><td align="left" valign="top" headers="idm140031633868208"> <p>
								Allows access to HTTP/HTTPS traffic and TLS-encrypted protocols other than HTTPS, such as TLS with the SNI header.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031633869296"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-ingress-cluster-traffic-load-balancer">Automatically assign an external IP by using a load balancer service</a>
							</p>
							 </td><td align="left" valign="top" headers="idm140031633868208"> <p>
								Allows traffic to non-standard ports through an IP address assigned from a pool.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031633869296"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-ingress-cluster-traffic-service-external-ip">Manually assign an external IP to a service</a>
							</p>
							 </td><td align="left" valign="top" headers="idm140031633868208"> <p>
								Allows traffic to non-standard ports through a specific IP address.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031633869296"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-ingress-cluster-traffic-nodeport">Configure a <code class="literal">NodePort</code></a>
							</p>
							 </td><td align="left" valign="top" headers="idm140031633868208"> <p>
								Expose a service on all nodes in the cluster.
							</p>
							 </td></tr></tbody></table></div></section><section class="section" id="post-install-configuring-node-port-service-range"><div class="titlepage"><div><div><h2 class="title">9.5. Configuring the node port service range</h2></div></div></div><p>
				As a cluster administrator, you can expand the available node port range. If your cluster uses of a large number of node ports, you might need to increase the number of available ports.
			</p><p>
				The default port range is <code class="literal">30000-32767</code>. You can never reduce the port range, even if you first expand it beyond the default range.
			</p><section class="section" id="post-install-configuring-node-port-service-range-prerequisites"><div class="titlepage"><div><div><h3 class="title">9.5.1. Prerequisites</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Your cluster infrastructure must allow access to the ports that you specify within the expanded range. For example, if you expand the node port range to <code class="literal">30000-32900</code>, the inclusive port range of <code class="literal">32768-32900</code> must be allowed by your firewall or packet filtering configuration.
						</li></ul></div><section class="section" id="nw-nodeport-service-range-edit_post-install-network-configuration"><div class="titlepage"><div><div><h4 class="title">9.5.1.1. Expanding the node port range</h4></div></div></div><p>
						You can expand the node port range for the cluster.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Install the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								Log in to the cluster with a user with <code class="literal">cluster-admin</code> privileges.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								To expand the node port range, enter the following command. Replace <code class="literal">&lt;port&gt;</code> with the largest port number in the new range.
							</p><pre class="programlisting language-terminal">$ oc patch network.config.openshift.io cluster --type=merge -p \
  '{
    "spec":
      { "serviceNodePortRange": "30000-&lt;port&gt;" }
  }'</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
								You can alternatively apply the following YAML to update the node port range:
							</p><pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  serviceNodePortRange: "30000-&lt;port&gt;"</pre></div></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">network.config.openshift.io/cluster patched</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								To confirm that the configuration is active, enter the following command. It can take several minutes for the update to apply.
							</p><pre class="programlisting language-terminal">$ oc get configmaps -n openshift-kube-apiserver config \
  -o jsonpath="{.data['config\.yaml']}" | \
  grep -Eo '"service-node-port-range":["[[:digit:]]+-[[:digit:]]+"]'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">"service-node-port-range":["30000-33000"]</pre>

								</p></div></li></ol></div></section></section></section><section class="section" id="post-install-configuring-ipsec-ovn"><div class="titlepage"><div><div><h2 class="title">9.6. Configuring IPsec encryption</h2></div></div></div><p>
				With IPsec enabled, all network traffic between nodes on the OVN-Kubernetes network plugin travels through an encrypted tunnel.
			</p><p>
				IPsec is disabled by default.
			</p><section class="section" id="post-install-configuring-ipsec-ovn-prerequisites"><div class="titlepage"><div><div><h3 class="title">9.6.1. Prerequisites</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Your cluster must use the OVN-Kubernetes network plugin.
						</li></ul></div><section class="section" id="nw-ovn-ipsec-enable_post-install-network-configuration"><div class="titlepage"><div><div><h4 class="title">9.6.1.1. Enabling IPsec encryption</h4></div></div></div><p>
						As a cluster administrator, you can enable IPsec encryption after cluster installation.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Install the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								Log in to the cluster with a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have reduced the size of your cluster MTU by <code class="literal">46</code> bytes to allow for the overhead of the IPsec ESP header.
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								To enable IPsec encryption, enter the following command:
							</p><pre class="programlisting language-terminal">$ oc patch networks.operator.openshift.io cluster --type=merge \
-p '{"spec":{"defaultNetwork":{"ovnKubernetesConfig":{"ipsecConfig":{ }}}}}'</pre></li></ul></div></section><section class="section" id="nw-ovn-ipsec-verification_post-install-network-configuration"><div class="titlepage"><div><div><h4 class="title">9.6.1.2. Verifying that IPsec is enabled</h4></div></div></div><p>
						As a cluster administrator, you can verify that IPsec is enabled.
					</p><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								To find the names of the OVN-Kubernetes control plane pods, enter the following command:
							</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-ovn-kubernetes | grep ovnkube-master</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">ovnkube-master-4496s        1/1     Running   0          6h39m
ovnkube-master-d6cht        1/1     Running   0          6h42m
ovnkube-master-skblc        1/1     Running   0          6h51m
ovnkube-master-vf8rf        1/1     Running   0          6h51m
ovnkube-master-w7hjr        1/1     Running   0          6h51m
ovnkube-master-zsk7x        1/1     Running   0          6h42m</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Verify that IPsec is enabled on your cluster:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-ovn-kubernetes -c nbdb rsh ovnkube-master-&lt;XXXXX&gt; \
  ovn-nbctl --no-leader-only get nb_global . ipsec</pre><p class="simpara">
								where:
							</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;XXXXX&gt;</code></span></dt><dd>
											Specifies the random sequence of letters for a pod from the previous step.
										</dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-text">true</pre>

								</p></div></li></ol></div></section></section></section><section class="section" id="post-install-configuring-network-policy"><div class="titlepage"><div><div><h2 class="title">9.7. Configuring network policy</h2></div></div></div><p>
				As a cluster administrator or project administrator, you can configure network policies for a project.
			</p><section class="section" id="nw-networkpolicy-about_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.7.1. About network policy</h3></div></div></div><p>
					In a cluster using a network plugin that supports Kubernetes network policy, network isolation is controlled entirely by <code class="literal">NetworkPolicy</code> objects. In OpenShift Container Platform 4.13, OpenShift SDN supports using network policy in its default network isolation mode.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Network policy does not apply to the host network namespace. Pods with host networking enabled are unaffected by network policy rules. However, pods connecting to the host-networked pods might be affected by the network policy rules.
					</p><p>
						Network policies cannot block traffic from localhost or from their resident nodes.
					</p></div></div><p>
					By default, all pods in a project are accessible from other pods and network endpoints. To isolate one or more pods in a project, you can create <code class="literal">NetworkPolicy</code> objects in that project to indicate the allowed incoming connections. Project administrators can create and delete <code class="literal">NetworkPolicy</code> objects within their own project.
				</p><p>
					If a pod is matched by selectors in one or more <code class="literal">NetworkPolicy</code> objects, then the pod will accept only connections that are allowed by at least one of those <code class="literal">NetworkPolicy</code> objects. A pod that is not selected by any <code class="literal">NetworkPolicy</code> objects is fully accessible.
				</p><p>
					A network policy applies to only the TCP, UDP, and SCTP protocols. Other protocols are not affected.
				</p><p>
					The following example <code class="literal">NetworkPolicy</code> objects demonstrate supporting different scenarios:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Deny all traffic:
						</p><p class="simpara">
							To make a project deny by default, add a <code class="literal">NetworkPolicy</code> object that matches all pods but accepts no traffic:
						</p><pre class="programlisting language-yaml">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-by-default
spec:
  podSelector: {}
  ingress: []</pre></li><li class="listitem"><p class="simpara">
							Only allow connections from the OpenShift Container Platform Ingress Controller:
						</p><p class="simpara">
							To make a project allow only connections from the OpenShift Container Platform Ingress Controller, add the following <code class="literal">NetworkPolicy</code> object.
						</p><pre class="programlisting language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-ingress
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: ingress
  podSelector: {}
  policyTypes:
  - Ingress</pre></li><li class="listitem"><p class="simpara">
							Only accept connections from pods within a project:
						</p><p class="simpara">
							To make pods accept connections from other pods in the same project, but reject all other connections from pods in other projects, add the following <code class="literal">NetworkPolicy</code> object:
						</p><pre class="programlisting language-yaml">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-same-namespace
spec:
  podSelector: {}
  ingress:
  - from:
    - podSelector: {}</pre></li><li class="listitem"><p class="simpara">
							Only allow HTTP and HTTPS traffic based on pod labels:
						</p><p class="simpara">
							To enable only HTTP and HTTPS access to the pods with a specific label (<code class="literal">role=frontend</code> in following example), add a <code class="literal">NetworkPolicy</code> object similar to the following:
						</p><pre class="programlisting language-yaml">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-http-and-https
spec:
  podSelector:
    matchLabels:
      role: frontend
  ingress:
  - ports:
    - protocol: TCP
      port: 80
    - protocol: TCP
      port: 443</pre></li><li class="listitem"><p class="simpara">
							Accept connections by using both namespace and pod selectors:
						</p><p class="simpara">
							To match network traffic by combining namespace and pod selectors, you can use a <code class="literal">NetworkPolicy</code> object similar to the following:
						</p><pre class="programlisting language-yaml">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-pod-and-namespace-both
spec:
  podSelector:
    matchLabels:
      name: test-pods
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            project: project_name
        podSelector:
          matchLabels:
            name: test-pods</pre></li></ul></div><p>
					<code class="literal">NetworkPolicy</code> objects are additive, which means you can combine multiple <code class="literal">NetworkPolicy</code> objects together to satisfy complex network requirements.
				</p><p>
					For example, for the <code class="literal">NetworkPolicy</code> objects defined in previous samples, you can define both <code class="literal">allow-same-namespace</code> and <code class="literal">allow-http-and-https</code> policies within the same project. Thus allowing the pods with the label <code class="literal">role=frontend</code>, to accept any connection allowed by each policy. That is, connections on any port from pods in the same namespace, and connections on ports <code class="literal">80</code> and <code class="literal">443</code> from pods in any namespace.
				</p><section class="section" id="nw-networkpolicy-allow-from-router_post-install-network-configuration"><div class="titlepage"><div><div><h4 class="title">9.7.1.1. Using the allow-from-router network policy</h4></div></div></div><p>
						Use the following <code class="literal">NetworkPolicy</code> to allow external traffic regardless of the router configuration:
					</p><pre class="programlisting language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-router
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          policy-group.network.openshift.io/ingress: ""<span id="CO125-1"><!--Empty--></span><span class="callout">1</span>
  podSelector: {}
  policyTypes:
  - Ingress</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO125-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								<code class="literal">policy-group.network.openshift.io/ingress:""</code> label supports both OpenShift-SDN and OVN-Kubernetes.
							</div></dd></dl></div></section><section class="section" id="nw-networkpolicy-allow-from-hostnetwork_post-install-network-configuration"><div class="titlepage"><div><div><h4 class="title">9.7.1.2. Using the allow-from-hostnetwork network policy</h4></div></div></div><p>
						Add the following <code class="literal">allow-from-hostnetwork</code> <code class="literal">NetworkPolicy</code> object to direct traffic from the host network pods:
					</p><pre class="programlisting language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-hostnetwork
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          policy-group.network.openshift.io/host-network: ""
  podSelector: {}
  policyTypes:
  - Ingress</pre></section></section><section class="section" id="nw-networkpolicy-object_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.7.2. Example NetworkPolicy object</h3></div></div></div><p>
					The following annotates an example NetworkPolicy object:
				</p><pre class="programlisting language-yaml">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-27107 <span id="CO126-1"><!--Empty--></span><span class="callout">1</span>
spec:
  podSelector: <span id="CO126-2"><!--Empty--></span><span class="callout">2</span>
    matchLabels:
      app: mongodb
  ingress:
  - from:
    - podSelector: <span id="CO126-3"><!--Empty--></span><span class="callout">3</span>
        matchLabels:
          app: app
    ports: <span id="CO126-4"><!--Empty--></span><span class="callout">4</span>
    - protocol: TCP
      port: 27017</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO126-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The name of the NetworkPolicy object.
						</div></dd><dt><a href="#CO126-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							A selector that describes the pods to which the policy applies. The policy object can only select pods in the project that defines the NetworkPolicy object.
						</div></dd><dt><a href="#CO126-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							A selector that matches the pods from which the policy object allows ingress traffic. The selector matches pods in the same namespace as the NetworkPolicy.
						</div></dd><dt><a href="#CO126-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							A list of one or more destination ports on which to accept traffic.
						</div></dd></dl></div></section><section class="section admin" id="nw-networkpolicy-create-cli_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.7.3. Creating a network policy using the CLI</h3></div></div></div><p class="admin admin">
					To define granular rules describing ingress or egress network traffic allowed for namespaces in your cluster, you can create a network policy.
				</p><div class="admonition note admin"><div class="admonition_header">Note</div><div><p class="admin admin">
						If you log in with a user with the <code class="literal admin">cluster-admin</code> role, then you can create a network policy in any namespace in the cluster.
					</p></div></div><div class="itemizedlist admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist admin" type="disc"><li class="listitem">
							Your cluster uses a network plugin that supports <code class="literal admin">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal admin">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
						</li><li class="listitem">
							You installed the OpenShift CLI (<code class="literal admin">oc</code>).
						</li><li class="listitem">
							You are logged in to the cluster with a user with <code class="literal admin">admin</code> privileges.
						</li><li class="listitem">
							You are working in the namespace that the network policy applies to.
						</li></ul></div><div class="orderedlist admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist admin" type="1"><li class="listitem"><p class="simpara">
							Create a policy rule:
						</p><div class="orderedlist admin"><ol class="orderedlist admin" type="a"><li class="listitem"><p class="simpara">
									Create a <code class="literal admin">&lt;policy_name&gt;.yaml</code> file:
								</p><pre class="programlisting language-terminal admin admin">$ touch &lt;policy_name&gt;.yaml</pre><p class="admin admin">
									where:
								</p><div class="variablelist admin"><dl class="variablelist admin"><dt><span class="term"><code class="literal admin">&lt;policy_name&gt;</code></span></dt><dd>
												Specifies the network policy file name.
											</dd></dl></div></li><li class="listitem"><p class="simpara">
									Define a network policy in the file that you just created, such as in the following examples:
								</p><div class="admin admin"><p class="title"><strong>Deny ingress from all pods in all namespaces</strong></p><p>
										This is a fundamental policy, blocking all cross-pod networking other than cross-pod traffic allowed by the configuration of other Network Policies.
									</p></div><pre class="programlisting language-yaml admin admin">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-by-default
spec:
  podSelector:
  ingress: []</pre><div class="admin admin"><p class="title"><strong>Allow ingress from all pods in the same namespace</strong></p><p>
										
<pre class="programlisting language-yaml">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-same-namespace
spec:
  podSelector:
  ingress:
  - from:
    - podSelector: {}</pre>

									</p></div><div class="admin admin"><p class="title"><strong>Allow ingress traffic to one pod from a particular namespace</strong></p><p>
										This policy allows traffic to pods labelled <code class="literal admin">pod-a</code> from pods running in <code class="literal admin">namespace-y</code>.
									</p></div><pre class="programlisting language-yaml admin admin">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-traffic-pod
spec:
  podSelector:
   matchLabels:
      pod: pod-a
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
           kubernetes.io/metadata.name: namespace-y</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							To create the network policy object, enter the following command:
						</p><pre class="programlisting language-terminal admin admin">$ oc apply -f &lt;policy_name&gt;.yaml -n &lt;namespace&gt;</pre><p class="admin admin">
							where:
						</p><div class="variablelist admin"><dl class="variablelist admin"><dt><span class="term"><code class="literal admin">&lt;policy_name&gt;</code></span></dt><dd>
										Specifies the network policy file name.
									</dd><dt><span class="term"><code class="literal admin">&lt;namespace&gt;</code></span></dt><dd>
										Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.
									</dd></dl></div><div class="admin admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">networkpolicy.networking.k8s.io/deny-by-default created</pre>

							</p></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you log in to the web console with <code class="literal">cluster-admin</code> privileges, you have a choice of creating a network policy in any namespace in the cluster directly in YAML or from a form in the web console.
					</p></div></div></section><section class="section" id="nw-networkpolicy-multitenant-isolation_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.7.4. Configuring multitenant isolation by using network policy</h3></div></div></div><p>
					You can configure your project to isolate it from pods and services in other project namespaces.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Your cluster uses a network plugin that supports <code class="literal">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
						</li><li class="listitem">
							You installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You are logged in to the cluster with a user with <code class="literal">admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the following <code class="literal">NetworkPolicy</code> objects:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									A policy named <code class="literal">allow-from-openshift-ingress</code>.
								</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-ingress
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          policy-group.network.openshift.io/ingress: ""
  podSelector: {}
  policyTypes:
  - Ingress
EOF</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										<code class="literal">policy-group.network.openshift.io/ingress: ""</code> is the preferred namespace selector label for OpenShift SDN. You can use the <code class="literal">network.openshift.io/policy-group: ingress</code> namespace selector label, but this is a legacy label.
									</p></div></div></li><li class="listitem"><p class="simpara">
									A policy named <code class="literal">allow-from-openshift-monitoring</code>:
								</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-monitoring
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: monitoring
  podSelector: {}
  policyTypes:
  - Ingress
EOF</pre></li><li class="listitem"><p class="simpara">
									A policy named <code class="literal">allow-same-namespace</code>:
								</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF| oc create -f -
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-same-namespace
spec:
  podSelector:
  ingress:
  - from:
    - podSelector: {}
EOF</pre></li><li class="listitem"><p class="simpara">
									A policy named <code class="literal">allow-from-kube-apiserver-operator</code>:
								</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-kube-apiserver-operator
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: openshift-kube-apiserver-operator
      podSelector:
        matchLabels:
          app: kube-apiserver-operator
  policyTypes:
  - Ingress
EOF</pre><p class="simpara">
									For more details, see <a class="link" href="https://access.redhat.com/solutions/6964520">New <code class="literal">kube-apiserver-operator</code> webhook controller validating health of webhook</a>.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Optional: To confirm that the network policies exist in your current project, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc describe networkpolicy</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">Name:         allow-from-openshift-ingress
Namespace:    example1
Created on:   2020-06-09 00:28:17 -0400 EDT
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Spec:
  PodSelector:     &lt;none&gt; (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    To Port: &lt;any&gt; (traffic allowed to all ports)
    From:
      NamespaceSelector: network.openshift.io/policy-group: ingress
  Not affecting egress traffic
  Policy Types: Ingress


Name:         allow-from-openshift-monitoring
Namespace:    example1
Created on:   2020-06-09 00:29:57 -0400 EDT
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Spec:
  PodSelector:     &lt;none&gt; (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    To Port: &lt;any&gt; (traffic allowed to all ports)
    From:
      NamespaceSelector: network.openshift.io/policy-group: monitoring
  Not affecting egress traffic
  Policy Types: Ingress</pre>

							</p></div></li></ol></div></section><section class="section" id="post-install-nw-networkpolicy-creating-default-networkpolicy-objects-for-a-new-project"><div class="titlepage"><div><div><h3 class="title">9.7.5. Creating default network policies for a new project</h3></div></div></div><p>
					As a cluster administrator, you can modify the new project template to automatically include <code class="literal">NetworkPolicy</code> objects when you create a new project.
				</p></section><section class="section" id="modifying-template-for-new-projects_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.7.6. Modifying the template for new projects</h3></div></div></div><p>
					As a cluster administrator, you can modify the default project template so that new projects are created using your custom requirements.
				</p><p>
					To create your own custom project template:
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem"><p class="simpara">
							Generate the default project template:
						</p><pre class="programlisting language-terminal">$ oc adm create-bootstrap-project-template -o yaml &gt; template.yaml</pre></li><li class="listitem">
							Use a text editor to modify the generated <code class="literal">template.yaml</code> file by adding objects or modifying existing objects.
						</li><li class="listitem"><p class="simpara">
							The project template must be created in the <code class="literal">openshift-config</code> namespace. Load your modified template:
						</p><pre class="programlisting language-terminal">$ oc create -f template.yaml -n openshift-config</pre></li><li class="listitem"><p class="simpara">
							Edit the project configuration resource using the web console or CLI.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Using the web console:
								</p><div class="orderedlist"><ol class="orderedlist" type="i"><li class="listitem">
											Navigate to the <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Cluster Settings</strong></span> page.
										</li><li class="listitem">
											Click <span class="strong strong"><strong>Configuration</strong></span> to view all configuration resources.
										</li><li class="listitem">
											Find the entry for <span class="strong strong"><strong>Project</strong></span> and click <span class="strong strong"><strong>Edit YAML</strong></span>.
										</li></ol></div></li><li class="listitem"><p class="simpara">
									Using the CLI:
								</p><div class="orderedlist"><ol class="orderedlist" type="i"><li class="listitem"><p class="simpara">
											Edit the <code class="literal">project.config.openshift.io/cluster</code> resource:
										</p><pre class="programlisting language-terminal">$ oc edit project.config.openshift.io/cluster</pre></li></ol></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Update the <code class="literal">spec</code> section to include the <code class="literal">projectRequestTemplate</code> and <code class="literal">name</code> parameters, and set the name of your uploaded project template. The default name is <code class="literal">project-request</code>.
						</p><div class="formalpara"><p class="title"><strong>Project configuration resource with custom project template</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Project
metadata:
  ...
spec:
  projectRequestTemplate:
    name: &lt;template_name&gt;</pre>

							</p></div></li><li class="listitem">
							After you save your changes, create a new project to verify that your changes were successfully applied.
						</li></ol></div><section class="section" id="nw-networkpolicy-project-defaults_post-install-network-configuration"><div class="titlepage"><div><div><h4 class="title">9.7.6.1. Adding network policies to the new project template</h4></div></div></div><p>
						As a cluster administrator, you can add network policies to the default template for new projects. OpenShift Container Platform will automatically create all the <code class="literal">NetworkPolicy</code> objects specified in the template in the project.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Your cluster uses a default CNI network provider that supports <code class="literal">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
							</li><li class="listitem">
								You installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You must log in to the cluster with a user with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You must have created a custom default project template for new projects.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the default template for a new project by running the following command:
							</p><pre class="programlisting language-terminal">$ oc edit template &lt;project_template&gt; -n openshift-config</pre><p class="simpara">
								Replace <code class="literal">&lt;project_template&gt;</code> with the name of the default template that you configured for your cluster. The default template name is <code class="literal">project-request</code>.
							</p></li><li class="listitem"><p class="simpara">
								In the template, add each <code class="literal">NetworkPolicy</code> object as an element to the <code class="literal">objects</code> parameter. The <code class="literal">objects</code> parameter accepts a collection of one or more objects.
							</p><p class="simpara">
								In the following example, the <code class="literal">objects</code> parameter collection includes several <code class="literal">NetworkPolicy</code> objects.
							</p><pre class="programlisting language-yaml">objects:
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allow-from-same-namespace
  spec:
    podSelector: {}
    ingress:
    - from:
      - podSelector: {}
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allow-from-openshift-ingress
  spec:
    ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            network.openshift.io/policy-group: ingress
    podSelector: {}
    policyTypes:
    - Ingress
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allow-from-kube-apiserver-operator
  spec:
    ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            kubernetes.io/metadata.name: openshift-kube-apiserver-operator
        podSelector:
          matchLabels:
            app: kube-apiserver-operator
    policyTypes:
    - Ingress
...</pre></li><li class="listitem"><p class="simpara">
								Optional: Create a new project to confirm that your network policy objects are created successfully by running the following commands:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Create a new project:
									</p><pre class="programlisting language-terminal">$ oc new-project &lt;project&gt; <span id="CO127-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO127-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Replace <code class="literal">&lt;project&gt;</code> with the name for the project you are creating.
											</div></dd></dl></div></li><li class="listitem"><p class="simpara">
										Confirm that the network policy objects in the new project template exist in the new project:
									</p><pre class="programlisting language-terminal">$ oc get networkpolicy
NAME                           POD-SELECTOR   AGE
allow-from-openshift-ingress   &lt;none&gt;         7s
allow-from-same-namespace      &lt;none&gt;         7s</pre></li></ol></div></li></ol></div></section></section></section><section class="section" id="ossm-supported-configurations_post-install-network-configuration"><div class="titlepage"><div><div><h2 class="title">9.8. Supported configurations</h2></div></div></div><p>
				The following configurations are supported for the current release of Red Hat OpenShift Service Mesh.
			</p><section class="section" id="ossm-supported-platforms_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.8.1. Supported platforms</h3></div></div></div><p>
					The Red Hat OpenShift Service Mesh Operator supports multiple versions of the <code class="literal">ServiceMeshControlPlane</code> resource. Version 2.4 Service Mesh control planes are supported on the following platform versions:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Red Hat OpenShift Container Platform version 4.10 or later.
						</li><li class="listitem">
							Red Hat OpenShift Dedicated version 4.
						</li><li class="listitem">
							Azure Red Hat OpenShift (ARO) version 4.
						</li><li class="listitem">
							Red Hat OpenShift Service on AWS (ROSA).
						</li></ul></div></section><section class="section" id="ossm-unsupported-configurations_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.8.2. Unsupported configurations</h3></div></div></div><p>
					Explicitly unsupported cases include:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							OpenShift Online is not supported for Red Hat OpenShift Service Mesh.
						</li><li class="listitem">
							Red Hat OpenShift Service Mesh does not support the management of microservices outside the cluster where Service Mesh is running.
						</li></ul></div></section><section class="section" id="ossm-supported-configurations-networks_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.8.3. Supported network configurations</h3></div></div></div><p>
					Red Hat OpenShift Service Mesh supports the following network configurations.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							OpenShift-SDN
						</li><li class="listitem">
							OVN-Kubernetes is available on all supported versions of OpenShift Container Platform.
						</li><li class="listitem">
							Third-Party Container Network Interface (CNI) plugins that have been certified on OpenShift Container Platform and passed Service Mesh conformance testing. See <a class="link" href="https://access.redhat.com/articles/5436171">Certified OpenShift CNI Plug-ins</a> for more information.
						</li></ul></div></section><section class="section" id="ossm-supported-configurations-sm_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.8.4. Supported configurations for Service Mesh</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							This release of Red Hat OpenShift Service Mesh is only available on OpenShift Container Platform x86_64, IBM Z, and IBM Power.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									IBM Z is only supported on OpenShift Container Platform 4.10 and later.
								</li><li class="listitem">
									IBM Power is only supported on OpenShift Container Platform 4.10 and later.
								</li></ul></div></li><li class="listitem">
							Configurations where all Service Mesh components are contained within a single OpenShift Container Platform cluster.
						</li><li class="listitem">
							Configurations that do not integrate external services such as virtual machines.
						</li><li class="listitem">
							Red Hat OpenShift Service Mesh does not support <code class="literal">EnvoyFilter</code> configuration except where explicitly documented.
						</li></ul></div></section><section class="section" id="ossm-supported-configurations-kiali_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.8.5. Supported configurations for Kiali</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The Kiali console is only supported on the two most recent releases of the Google Chrome, Microsoft Edge, Mozilla Firefox, or Apple Safari browsers.
						</li><li class="listitem">
							The <code class="literal">openshift</code> authentication strategy is the only supported authentication configuration when Kiali is deployed with Red Hat OpenShift Service Mesh (OSSM). The <code class="literal">openshift</code> strategy controls access based on the individual’s role-based access control (RBAC) roles of the OpenShift Container Platform.
						</li></ul></div></section><section class="section" id="ossm-supported-configurations-jaeger_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.8.6. Supported configurations for Distributed Tracing</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Jaeger agent as a sidecar is the only supported configuration for Jaeger. Jaeger as a daemonset is not supported for multitenant installations or OpenShift Dedicated.
						</li></ul></div></section><section class="section" id="ossm-supported-configurations-webassembly_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.8.7. Supported WebAssembly module</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							3scale WebAssembly is the only provided WebAssembly module. You can create custom WebAssembly modules.
						</li></ul></div></section><section class="section" id="ossm-installation-activities_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.8.8. Operator overview</h3></div></div></div><p>
					Red Hat OpenShift Service Mesh requires the following four Operators:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<span class="strong strong"><strong>OpenShift Elasticsearch</strong></span> - (Optional) Provides database storage for tracing and logging with the distributed tracing platform (Jaeger). It is based on the open source <a class="link" href="https://www.elastic.co/">Elasticsearch</a> project.
						</li><li class="listitem">
							<span class="strong strong"><strong>Red Hat OpenShift distributed tracing platform (Jaeger)</strong></span> - Provides distributed tracing to monitor and troubleshoot transactions in complex distributed systems. It is based on the open source <a class="link" href="https://www.jaegertracing.io/">Jaeger</a> project.
						</li><li class="listitem">
							<span class="strong strong"><strong>Kiali Operator provided by Red Hat</strong></span> - Provides observability for your service mesh. You can view configurations, monitor traffic, and analyze traces in a single console. It is based on the open source <a class="link" href="https://www.kiali.io/">Kiali</a> project.
						</li><li class="listitem">
							<span class="strong strong"><strong>Red Hat OpenShift Service Mesh</strong></span> - Allows you to connect, secure, control, and observe the microservices that comprise your applications. The Service Mesh Operator defines and monitors the <code class="literal">ServiceMeshControlPlane</code> resources that manage the deployment, updating, and deletion of the Service Mesh components. It is based on the open source <a class="link" href="https://istio.io/">Istio</a> project.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/service_mesh/#installing-ossm">Install Red Hat OpenShift Service Mesh</a> in your OpenShift Container Platform environment.
						</li></ul></div></section></section><section class="section" id="post-installationrouting-optimization"><div class="titlepage"><div><div><h2 class="title">9.9. Optimizing routing</h2></div></div></div><p>
				The OpenShift Container Platform HAProxy router can be scaled or configured to optimize performance.
			</p><section class="section" id="baseline-router-performance_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.9.1. Baseline Ingress Controller (router) performance</h3></div></div></div><p>
					The OpenShift Container Platform Ingress Controller, or router, is the ingress point for ingress traffic for applications and services that are configured using routes and ingresses.
				</p><p>
					When evaluating a single HAProxy router performance in terms of HTTP requests handled per second, the performance varies depending on many factors. In particular:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							HTTP keep-alive/close mode
						</li><li class="listitem">
							Route type
						</li><li class="listitem">
							TLS session resumption client support
						</li><li class="listitem">
							Number of concurrent connections per target route
						</li><li class="listitem">
							Number of target routes
						</li><li class="listitem">
							Back end server page size
						</li><li class="listitem">
							Underlying infrastructure (network/SDN solution, CPU, and so on)
						</li></ul></div><p>
					While performance in your specific environment will vary, Red Hat lab tests on a public cloud instance of size 4 vCPU/16GB RAM. A single HAProxy router handling 100 routes terminated by backends serving 1kB static pages is able to handle the following number of transactions per second.
				</p><p>
					In HTTP keep-alive mode scenarios:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031635812448" scope="col"><span class="strong strong"><strong>Encryption</strong></span></th><th align="left" valign="top" id="idm140031635810896" scope="col"><span class="strong strong"><strong>LoadBalancerService</strong></span></th><th align="left" valign="top" id="idm140031635809344" scope="col"><span class="strong strong"><strong>HostNetwork</strong></span></th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031635812448"> <p>
									none
								</p>
								 </td><td align="left" valign="top" headers="idm140031635810896"> <p>
									21515
								</p>
								 </td><td align="left" valign="top" headers="idm140031635809344"> <p>
									29622
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635812448"> <p>
									edge
								</p>
								 </td><td align="left" valign="top" headers="idm140031635810896"> <p>
									16743
								</p>
								 </td><td align="left" valign="top" headers="idm140031635809344"> <p>
									22913
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635812448"> <p>
									passthrough
								</p>
								 </td><td align="left" valign="top" headers="idm140031635810896"> <p>
									36786
								</p>
								 </td><td align="left" valign="top" headers="idm140031635809344"> <p>
									53295
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635812448"> <p>
									re-encrypt
								</p>
								 </td><td align="left" valign="top" headers="idm140031635810896"> <p>
									21583
								</p>
								 </td><td align="left" valign="top" headers="idm140031635809344"> <p>
									25198
								</p>
								 </td></tr></tbody></table></div><p>
					In HTTP close (no keep-alive) scenarios:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031635305136" scope="col"><span class="strong strong"><strong>Encryption</strong></span></th><th align="left" valign="top" id="idm140031635303584" scope="col"><span class="strong strong"><strong>LoadBalancerService</strong></span></th><th align="left" valign="top" id="idm140031635302032" scope="col"><span class="strong strong"><strong>HostNetwork</strong></span></th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031635305136"> <p>
									none
								</p>
								 </td><td align="left" valign="top" headers="idm140031635303584"> <p>
									5719
								</p>
								 </td><td align="left" valign="top" headers="idm140031635302032"> <p>
									8273
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635305136"> <p>
									edge
								</p>
								 </td><td align="left" valign="top" headers="idm140031635303584"> <p>
									2729
								</p>
								 </td><td align="left" valign="top" headers="idm140031635302032"> <p>
									4069
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635305136"> <p>
									passthrough
								</p>
								 </td><td align="left" valign="top" headers="idm140031635303584"> <p>
									4121
								</p>
								 </td><td align="left" valign="top" headers="idm140031635302032"> <p>
									5344
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635305136"> <p>
									re-encrypt
								</p>
								 </td><td align="left" valign="top" headers="idm140031635303584"> <p>
									2320
								</p>
								 </td><td align="left" valign="top" headers="idm140031635302032"> <p>
									2941
								</p>
								 </td></tr></tbody></table></div><p>
					The default Ingress Controller configuration was used with the <code class="literal">spec.tuningOptions.threadCount</code> field set to <code class="literal">4</code>. Two different endpoint publishing strategies were tested: Load Balancer Service and Host Network. TLS session resumption was used for encrypted routes. With HTTP keep-alive, a single HAProxy router is capable of saturating a 1 Gbit NIC at page sizes as small as 8 kB.
				</p><p>
					When running on bare metal with modern processors, you can expect roughly twice the performance of the public cloud instance above. This overhead is introduced by the virtualization layer in place on public clouds and holds mostly true for private cloud-based virtualization as well. The following table is a guide to how many applications to use behind the router:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 67%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031635271632" scope="col"><span class="strong strong"><strong>Number of applications</strong></span></th><th align="left" valign="top" id="idm140031632468592" scope="col"><span class="strong strong"><strong>Application type</strong></span></th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031635271632"> <p>
									5-10
								</p>
								 </td><td align="left" valign="top" headers="idm140031632468592"> <p>
									static file/web server or caching proxy
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635271632"> <p>
									100-1000
								</p>
								 </td><td align="left" valign="top" headers="idm140031632468592"> <p>
									applications generating dynamic content
								</p>
								 </td></tr></tbody></table></div><p>
					In general, HAProxy can support routes for up to 1000 applications, depending on the technology in use. Ingress Controller performance might be limited by the capabilities and performance of the applications behind it, such as language or static versus dynamic content.
				</p><p>
					Ingress, or router, sharding should be used to serve more routes towards applications and help horizontally scale the routing tier.
				</p></section><section class="section" id="ingress-liveness-readiness-startup-probes_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.9.2. Configuring Ingress Controller liveness, readiness, and startup probes</h3></div></div></div><p>
					Cluster administrators can configure the timeout values for the kubelet’s liveness, readiness, and startup probes for router deployments that are managed by the OpenShift Container Platform Ingress Controller (router). The liveness and readiness probes of the router use the default timeout value of 1 second, which is too brief when networking or runtime performance is severely degraded. Probe timeouts can cause unwanted router restarts that interrupt application connections. The ability to set larger timeout values can reduce the risk of unnecessary and unwanted restarts.
				</p><p>
					You can update the <code class="literal">timeoutSeconds</code> value on the <code class="literal">livenessProbe</code>, <code class="literal">readinessProbe</code>, and <code class="literal">startupProbe</code> parameters of the router container.
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 27%; " class="col_1"><!--Empty--></col><col style="width: 73%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031632447472" scope="col">Parameter</th><th align="left" valign="top" id="idm140031632446384" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031632447472"> <p>
									<code class="literal">livenessProbe</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031632446384"> <p>
									The <code class="literal">livenessProbe</code> reports to the kubelet whether a pod is dead and needs to be restarted.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031632447472"> <p>
									<code class="literal">readinessProbe</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031632446384"> <p>
									The <code class="literal">readinessProbe</code> reports whether a pod is healthy or unhealthy. When the readiness probe reports an unhealthy pod, then the kubelet marks the pod as not ready to accept traffic. Subsequently, the endpoints for that pod are marked as not ready, and this status propagates to the kube-proxy. On cloud platforms with a configured load balancer, the kube-proxy communicates to the cloud load-balancer not to send traffic to the node with that pod.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031632447472"> <p>
									<code class="literal">startupProbe</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031632446384"> <p>
									The <code class="literal">startupProbe</code> gives the router pod up to 2 minutes to initialize before the kubelet begins sending the router liveness and readiness probes. This initialization time can prevent routers with many routes or endpoints from prematurely restarting.
								</p>
								 </td></tr></tbody></table></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The timeout configuration option is an advanced tuning technique that can be used to work around issues. However, these issues should eventually be diagnosed and possibly a support case or <a class="link" href="https://issues.redhat.com/secure/CreateIssueDetails!init.jspa?pid=12332330&amp;summary=Summary&amp;issuetype=1&amp;priority=10200&amp;versions=12385624">Jira issue</a> opened for any issues that causes probes to time out.
					</p></div></div><p>
					The following example demonstrates how you can directly patch the default router deployment to set a 5-second timeout for the liveness and readiness probes:
				</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress patch deploy/router-default --type=strategic --patch='{"spec":{"template":{"spec":{"containers":[{"name":"router","livenessProbe":{"timeoutSeconds":5},"readinessProbe":{"timeoutSeconds":5}}]}}}}'</pre><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						
<pre class="programlisting language-terminal">$ oc -n openshift-ingress describe deploy/router-default | grep -e Liveness: -e Readiness:
    Liveness:   http-get http://:1936/healthz delay=0s timeout=5s period=10s #success=1 #failure=3
    Readiness:  http-get http://:1936/healthz/ready delay=0s timeout=5s period=10s #success=1 #failure=3</pre>

					</p></div></section><section class="section" id="configuring-haproxy-interval_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.9.3. Configuring HAProxy reload interval</h3></div></div></div><p>
					When you update a route or an endpoint associated with a route, OpenShift Container Platform router updates the configuration for HAProxy. Then, HAProxy reloads the updated configuration for those changes to take effect. When HAProxy reloads, it generates a new process that handles new connections using the updated configuration.
				</p><p>
					HAProxy keeps the old process running to handle existing connections until those connections are all closed. When old processes have long-lived connections, these processes can accumulate and consume resources.
				</p><p>
					The default minimum HAProxy reload interval is five seconds. You can configure an Ingress Controller using its <code class="literal">spec.tuningOptions.reloadInterval</code> field to set a longer minimum reload interval.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Setting a large value for the minimum HAProxy reload interval can cause latency in observing updates to routes and their endpoints. To lessen the risk, avoid setting a value larger than the tolerable latency for updates.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Change the minimum HAProxy reload interval of the default Ingress Controller to 15 seconds by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress-operator patch ingresscontrollers/default --type=merge --patch='{"spec":{"tuningOptions":{"reloadInterval":"15s"}}}'</pre></li></ul></div></section></section><section class="section" id="post-installation-osp-fips"><div class="titlepage"><div><div><h2 class="title">9.10. Post-installation RHOSP network configuration</h2></div></div></div><p>
				You can configure some aspects of an OpenShift Container Platform on Red Hat OpenStack Platform (RHOSP) cluster after installation.
			</p><section class="section" id="installation-osp-configuring-api-floating-ip_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.10.1. Configuring application access with floating IP addresses</h3></div></div></div><p>
					After you install OpenShift Container Platform, configure Red Hat OpenStack Platform (RHOSP) to allow application network traffic.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You do not need to perform this procedure if you provided values for <code class="literal">platform.openstack.apiFloatingIP</code> and <code class="literal">platform.openstack.ingressFloatingIP</code> in the <code class="literal">install-config.yaml</code> file, or <code class="literal">os_api_fip</code> and <code class="literal">os_ingress_fip</code> in the <code class="literal">inventory.yaml</code> playbook, during installation. The floating IP addresses are already set.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							OpenShift Container Platform cluster must be installed
						</li><li class="listitem">
							Floating IP addresses are enabled as described in the OpenShift Container Platform on RHOSP installation documentation.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						After you install the OpenShift Container Platform cluster, attach a floating IP address to the ingress port:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Show the port:
						</p><pre class="programlisting language-terminal">$ openstack port show &lt;cluster_name&gt;-&lt;cluster_ID&gt;-ingress-port</pre></li><li class="listitem"><p class="simpara">
							Attach the port to the IP address:
						</p><pre class="programlisting language-terminal">$ openstack floating ip set --port &lt;ingress_port_ID&gt; &lt;apps_FIP&gt;</pre></li><li class="listitem"><p class="simpara">
							Add a wildcard <code class="literal">A</code> record for <code class="literal">*apps.</code> to your DNS file:
						</p><pre class="programlisting language-dns">*.apps.&lt;cluster_name&gt;.&lt;base_domain&gt;  IN  A  &lt;apps_FIP&gt;</pre></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you do not control the DNS server but want to enable application access for non-production purposes, you can add these hostnames to <code class="literal">/etc/hosts</code>:
					</p><pre class="programlisting language-dns">&lt;apps_FIP&gt; console-openshift-console.apps.&lt;cluster name&gt;.&lt;base domain&gt;
&lt;apps_FIP&gt; integrated-oauth-server-openshift-authentication.apps.&lt;cluster name&gt;.&lt;base domain&gt;
&lt;apps_FIP&gt; oauth-openshift.apps.&lt;cluster name&gt;.&lt;base domain&gt;
&lt;apps_FIP&gt; prometheus-k8s-openshift-monitoring.apps.&lt;cluster name&gt;.&lt;base domain&gt;
&lt;apps_FIP&gt; &lt;app name&gt;.apps.&lt;cluster name&gt;.&lt;base domain&gt;</pre></div></div></section><section class="section" id="installation-osp-kuryr-port-pools_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.10.2. Kuryr ports pools</h3></div></div></div><p>
					A Kuryr ports pool maintains a number of ports on standby for pod creation.
				</p><p>
					Keeping ports on standby minimizes pod creation time. Without ports pools, Kuryr must explicitly request port creation or deletion whenever a pod is created or deleted.
				</p><p>
					The Neutron ports that Kuryr uses are created in subnets that are tied to namespaces. These pod ports are also added as subports to the primary port of OpenShift Container Platform cluster nodes.
				</p><p>
					Because Kuryr keeps each namespace in a separate subnet, a separate ports pool is maintained for each namespace-worker pair.
				</p><p>
					Prior to installing a cluster, you can set the following parameters in the <code class="literal">cluster-network-03-config.yml</code> manifest file to configure ports pool behavior:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The <code class="literal">enablePortPoolsPrepopulation</code> parameter controls pool prepopulation, which forces Kuryr to add Neutron ports to the pools when the first pod that is configured to use the dedicated network for pods is created in a namespace. The default value is <code class="literal">false</code>.
						</li><li class="listitem">
							The <code class="literal">poolMinPorts</code> parameter is the minimum number of free ports that are kept in the pool. The default value is <code class="literal">1</code>.
						</li><li class="listitem"><p class="simpara">
							The <code class="literal">poolMaxPorts</code> parameter is the maximum number of free ports that are kept in the pool. A value of <code class="literal">0</code> disables that upper bound. This is the default setting.
						</p><p class="simpara">
							If your OpenStack port quota is low, or you have a limited number of IP addresses on the pod network, consider setting this option to ensure that unneeded ports are deleted.
						</p></li><li class="listitem">
							The <code class="literal">poolBatchPorts</code> parameter defines the maximum number of Neutron ports that can be created at once. The default value is <code class="literal">3</code>.
						</li></ul></div></section><section class="section" id="installation-osp-kuryr-settings-active_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.10.3. Adjusting Kuryr ports pool settings in active deployments on RHOSP</h3></div></div></div><p>
					You can use a custom resource (CR) to configure how Kuryr manages Red Hat OpenStack Platform (RHOSP) Neutron ports to control the speed and efficiency of pod creation on a deployed cluster.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							From a command line, open the Cluster Network Operator (CNO) CR for editing:
						</p><pre class="programlisting language-terminal">$ oc edit networks.operator.openshift.io cluster</pre></li><li class="listitem"><p class="simpara">
							Edit the settings to meet your requirements. The following file is provided as an example:
						</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  serviceNetwork:
  - 172.30.0.0/16
  defaultNetwork:
    type: Kuryr
    kuryrConfig:
      enablePortPoolsPrepopulation: false <span id="CO128-1"><!--Empty--></span><span class="callout">1</span>
      poolMinPorts: 1 <span id="CO128-2"><!--Empty--></span><span class="callout">2</span>
      poolBatchPorts: 3 <span id="CO128-3"><!--Empty--></span><span class="callout">3</span>
      poolMaxPorts: 5 <span id="CO128-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO128-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Set <code class="literal">enablePortPoolsPrepopulation</code> to <code class="literal">true</code> to make Kuryr create Neutron ports when the first pod that is configured to use the dedicated network for pods is created in a namespace. This setting raises the Neutron ports quota but can reduce the time that is required to spawn pods. The default value is <code class="literal">false</code>.
								</div></dd><dt><a href="#CO128-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Kuryr creates new ports for a pool if the number of free ports in that pool is lower than the value of <code class="literal">poolMinPorts</code>. The default value is <code class="literal">1</code>.
								</div></dd><dt><a href="#CO128-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									<code class="literal">poolBatchPorts</code> controls the number of new ports that are created if the number of free ports is lower than the value of <code class="literal">poolMinPorts</code>. The default value is <code class="literal">3</code>.
								</div></dd><dt><a href="#CO128-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									If the number of free ports in a pool is higher than the value of <code class="literal">poolMaxPorts</code>, Kuryr deletes them until the number matches that value. Setting the value to <code class="literal">0</code> disables this upper bound, preventing pools from shrinking. The default value is <code class="literal">0</code>.
								</div></dd></dl></div></li><li class="listitem">
							Save your changes and quit the text editor to commit your changes.
						</li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Modifying these options on a running cluster forces the kuryr-controller and kuryr-cni pods to restart. As a result, the creation of new pods and services will be delayed.
					</p></div></div></section><section class="section" id="nw-osp-enabling-ovs-offload_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.10.4. Enabling OVS hardware offloading</h3></div></div></div><p>
					For clusters that run on Red Hat OpenStack Platform (RHOSP), you can enable <a class="link" href="https://www.openvswitch.org/">Open vSwitch (OVS)</a> hardware offloading.
				</p><p>
					OVS is a multi-layer virtual switch that enables large-scale, multi-server network virtualization.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You installed a cluster on RHOSP that is configured for single-root input/output virtualization (SR-IOV).
						</li><li class="listitem">
							You installed the SR-IOV Network Operator on your cluster.
						</li><li class="listitem">
							You created two <code class="literal">hw-offload</code> type virtual function (VF) interfaces on your cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an <code class="literal">SriovNetworkNodePolicy</code> policy for the two <code class="literal">hw-offload</code> type VF interfaces that are on your cluster:
						</p><div class="formalpara"><p class="title"><strong>The first virtual function interface</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy <span id="CO129-1"><!--Empty--></span><span class="callout">1</span>
metadata:
  name: "hwoffload9"
  namespace: openshift-sriov-network-operator
spec:
  deviceType: netdevice
  isRdma: true
  nicSelector:
    pfNames: <span id="CO129-2"><!--Empty--></span><span class="callout">2</span>
    - ens6
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: 'true'
  numVfs: 1
  priority: 99
  resourceName: "hwoffload9"</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO129-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Insert the <code class="literal">SriovNetworkNodePolicy</code> value here.
								</div></dd><dt><a href="#CO129-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Both interfaces must include physical function (PF) names.
								</div></dd></dl></div><div class="formalpara"><p class="title"><strong>The second virtual function interface</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy <span id="CO130-1"><!--Empty--></span><span class="callout">1</span>
metadata:
  name: "hwoffload10"
  namespace: openshift-sriov-network-operator
spec:
  deviceType: netdevice
  isRdma: true
  nicSelector:
    pfNames: <span id="CO130-2"><!--Empty--></span><span class="callout">2</span>
    - ens5
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: 'true'
  numVfs: 1
  priority: 99
  resourceName: "hwoffload10"</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO130-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Insert the <code class="literal">SriovNetworkNodePolicy</code> value here.
								</div></dd><dt><a href="#CO130-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Both interfaces must include physical function (PF) names.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create <code class="literal">NetworkAttachmentDefinition</code> resources for the two interfaces:
						</p><div class="formalpara"><p class="title"><strong>A <code class="literal">NetworkAttachmentDefinition</code> resource for the first interface</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    k8s.v1.cni.cncf.io/resourceName: openshift.io/hwoffload9
  name: hwoffload9
  namespace: default
spec:
    config: '{ "cniVersion":"0.3.1", "name":"hwoffload9","type":"host-device","device":"ens6"
    }'</pre>

							</p></div><div class="formalpara"><p class="title"><strong>A <code class="literal">NetworkAttachmentDefinition</code> resource for the second interface</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    k8s.v1.cni.cncf.io/resourceName: openshift.io/hwoffload10
  name: hwoffload10
  namespace: default
spec:
    config: '{ "cniVersion":"0.3.1", "name":"hwoffload10","type":"host-device","device":"ens5"
    }'</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Use the interfaces that you created with a pod. For example:
						</p><div class="formalpara"><p class="title"><strong>A pod that uses the two OVS offload interfaces</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: dpdk-testpmd
  namespace: default
  annotations:
    irq-load-balancing.crio.io: disable
    cpu-quota.crio.io: disable
    k8s.v1.cni.cncf.io/resourceName: openshift.io/hwoffload9
    k8s.v1.cni.cncf.io/resourceName: openshift.io/hwoffload10
spec:
  restartPolicy: Never
  containers:
  - name: dpdk-testpmd
    image: quay.io/krister/centos8_nfv-container-dpdk-testpmd:latest</pre>

							</p></div></li></ol></div></section><section class="section" id="nw-osp-hardware-offload-attaching-network_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.10.5. Attaching an OVS hardware offloading network</h3></div></div></div><p>
					You can attach an Open vSwitch (OVS) hardware offloading network to your cluster.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Your cluster is installed and running.
						</li><li class="listitem">
							You provisioned an OVS hardware offloading network on Red Hat OpenStack Platform (RHOSP) to use with your cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a file named <code class="literal">network.yaml</code> from the following template:
						</p><pre class="programlisting language-yaml">spec:
  additionalNetworks:
  - name: hwoffload1
    namespace: cnf
    rawCNIConfig: '{ "cniVersion": "0.3.1", "name": "hwoffload1", "type": "host-device","pciBusId": "0000:00:05.0", "ipam": {}}' <span id="CO131-1"><!--Empty--></span><span class="callout">1</span>
    type: Raw</pre><p class="simpara">
							where:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">pciBusId</code></span></dt><dd><p class="simpara">
										Specifies the device that is connected to the offloading network. If you do not have it, you can find this value by running the following command:
									</p><pre class="programlisting language-terminal">$ oc describe SriovNetworkNodeState -n openshift-sriov-network-operator</pre></dd></dl></div></li><li class="listitem"><p class="simpara">
							From a command line, enter the following command to patch your cluster with the file:
						</p><pre class="programlisting language-terminal">$ oc apply -f network.yaml</pre></li></ol></div></section><section class="section" id="nw-osp-pod-connections-ipv6_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.10.6. Enabling IPv6 connectivity to pods on RHOSP</h3></div></div></div><p>
					To enable IPv6 connectivity between pods that have additional networks that are on different nodes, disable port security for the IPv6 port of the server. Disabling port security obviates the need to create allowed address pairs for each IPv6 address that is assigned to pods and enables traffic on the security group.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Only the following IPv6 additional network configurations are supported:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								SLAAC and host-device
							</li><li class="listitem">
								SLAAC and MACVLAN
							</li><li class="listitem">
								DHCP stateless and host-device
							</li><li class="listitem">
								DHCP stateless and MACVLAN
							</li></ul></div></div></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							On a command line, enter the following command:
						</p><pre class="programlisting language-terminal">$ openstack port set --no-security-group --disable-port-security &lt;compute_ipv6_port&gt;</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								This command removes security groups from the port and disables port security. Traffic restrictions are removed entirely from the port.
							</p></div></div></li></ul></div><p>
					where:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;compute_ipv6_port&gt;</span></dt><dd>
								Specifies the IPv6 port of the compute server.
							</dd></dl></div></section><section class="section" id="nw-osp-pod-adding-connections-ipv6_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.10.7. Adding IPv6 connectivity to pods on RHOSP</h3></div></div></div><p>
					After you enable IPv6 connectivity in pods, add connectivity to them by using a Container Network Interface (CNI) configuration.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To edit the Cluster Network Operator (CNO), enter the following command:
						</p><pre class="programlisting language-terminal">$ oc edit networks.operator.openshift.io cluster</pre></li><li class="listitem"><p class="simpara">
							Specify your CNI configuration under the <code class="literal">spec</code> field. For example, the following configuration uses a SLAAC address mode with MACVLAN:
						</p><pre class="programlisting language-yaml">...
spec:
  additionalNetworks:
  - name: ipv6
    namespace: ipv6 <span id="CO131-2"><!--Empty--></span><span class="callout">1</span>
    rawCNIConfig: '{ "cniVersion": "0.3.1", "name": "ipv6", "type": "macvlan", "master": "ens4"}' <span id="CO131-3"><!--Empty--></span><span class="callout">2</span>
    type: Raw</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO131-1"><span class="callout">1</span></a> <a href="#CO131-2"><span class="callout">1</span></a> </dt><dd><div class="para">
									Be sure to create pods in the same namespace.
								</div></dd><dt><a href="#CO131-3"><span class="callout">2</span></a> </dt><dd><div class="para">
									The interface in the network attachment <code class="literal">"master"</code> field can differ from <code class="literal">"ens4"</code> when more networks are configured or when a different kernel driver is used.
								</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If you are using stateful address mode, include the IP Address Management (IPAM) in the CNI configuration.
							</p><p>
								DHCPv6 is not supported by Multus.
							</p></div></div></li><li class="listitem">
							Save your changes and quit the text editor to commit your changes.
						</li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							On a command line, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc get network-attachment-definitions -A</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAMESPACE       NAME            AGE
ipv6            ipv6            21h</pre>

							</p></div></li></ul></div><p>
					You can now create pods that have secondary IPv6 connections.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-additional-network_configuration-additional-network-attachment">Configuration for an additional network attachment</a>
						</li></ul></div></section><section class="section" id="nw-osp-pod-creating-ipv6_post-install-network-configuration"><div class="titlepage"><div><div><h3 class="title">9.10.8. Create pods that have IPv6 connectivity on RHOSP</h3></div></div></div><p>
					After you enable IPv6 connectivty for pods and add it to them, create pods that have secondary IPv6 connections.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Define pods that use your IPv6 namespace and the annotation <code class="literal">k8s.v1.cni.cncf.io/networks: &lt;additional_network_name&gt;</code>, where <code class="literal">&lt;additional_network_name</code> is the name of the additional network. For example, as part of a <code class="literal">Deployment</code> object:
						</p><pre class="programlisting language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-openshift
  namespace: ipv6
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
         - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - hello-openshift
  replicas: 2
  selector:
    matchLabels:
      app: hello-openshift
  template:
    metadata:
      labels:
        app: hello-openshift
      annotations:
        k8s.v1.cni.cncf.io/networks: ipv6
    spec:
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: hello-openshift
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
        image: quay.io/openshift/origin-hello-openshift
        ports:
        - containerPort: 8080</pre></li><li class="listitem"><p class="simpara">
							Create the pod. For example, on a command line, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;ipv6_enabled_resource&gt;</pre></li></ol></div><p>
					where:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;ipv6_enabled_resource&gt;</span></dt><dd>
								Specifies the file that contains your resource definition.
							</dd></dl></div></section></section></section><section class="chapter" id="post-install-storage-configuration"><div class="titlepage"><div><div><h1 class="title">Chapter 10. Post-installation storage configuration</h1></div></div></div><p>
			After installing OpenShift Container Platform, you can further expand and customize your cluster to your requirements, including storage configuration.
		</p><section class="section" id="post-install-dynamic-provisioning"><div class="titlepage"><div><div><h2 class="title">10.1. Dynamic provisioning</h2></div></div></div><section class="section" id="about_post-install-storage-configuration"><div class="titlepage"><div><div><h3 class="title">10.1.1. About dynamic provisioning</h3></div></div></div><p>
					The <code class="literal">StorageClass</code> resource object describes and classifies storage that can be requested, as well as provides a means for passing parameters for dynamically provisioned storage on demand. <code class="literal">StorageClass</code> objects can also serve as a management mechanism for controlling different levels of storage and access to the storage. Cluster Administrators (<code class="literal">cluster-admin</code>) or Storage Administrators (<code class="literal">storage-admin</code>) define and create the <code class="literal">StorageClass</code> objects that users can request without needing any detailed knowledge about the underlying storage volume sources.
				</p><p>
					The OpenShift Container Platform persistent volume framework enables this functionality and allows administrators to provision a cluster with persistent storage. The framework also gives users a way to request those resources without having any knowledge of the underlying infrastructure.
				</p><p>
					Many storage types are available for use as persistent volumes in OpenShift Container Platform. While all of them can be statically provisioned by an administrator, some types of storage are created dynamically using the built-in provider and plugin APIs.
				</p></section><section class="section" id="available-plug-ins_post-install-storage-configuration"><div class="titlepage"><div><div><h3 class="title">10.1.2. Available dynamic provisioning plugins</h3></div></div></div><p>
					OpenShift Container Platform provides the following provisioner plugins, which have generic implementations for dynamic provisioning that use the cluster’s configured provider’s API to create new storage resources:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031633033552" scope="col">Storage type</th><th align="left" valign="top" id="idm140031633032464" scope="col">Provisioner plugin name</th><th align="left" valign="top" id="idm140031633031376" scope="col">Notes</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031633033552"> <p>
									Red Hat OpenStack Platform (RHOSP) Cinder
								</p>
								 </td><td align="left" valign="top" headers="idm140031633032464"> <p>
									<code class="literal">kubernetes.io/cinder</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031633031376"> </td></tr><tr><td align="left" valign="top" headers="idm140031633033552"> <p>
									RHOSP Manila Container Storage Interface (CSI)
								</p>
								 </td><td align="left" valign="top" headers="idm140031633032464"> <p>
									<code class="literal">manila.csi.openstack.org</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031633031376"> <p>
									Once installed, the OpenStack Manila CSI Driver Operator and ManilaDriver automatically create the required storage classes for all available Manila share types needed for dynamic provisioning.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031633033552"> <p>
									Amazon Elastic Block Store (Amazon EBS)
								</p>
								 </td><td align="left" valign="top" headers="idm140031633032464"> <p>
									<code class="literal">kubernetes.io/aws-ebs</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031633031376"> <p>
									For dynamic provisioning when using multiple clusters in different zones, tag each node with <code class="literal">Key=kubernetes.io/cluster/&lt;cluster_name&gt;,Value=&lt;cluster_id&gt;</code> where <code class="literal">&lt;cluster_name&gt;</code> and <code class="literal">&lt;cluster_id&gt;</code> are unique per cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031633033552"> <p>
									Azure Disk
								</p>
								 </td><td align="left" valign="top" headers="idm140031633032464"> <p>
									<code class="literal">kubernetes.io/azure-disk</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031633031376"> </td></tr><tr><td align="left" valign="top" headers="idm140031633033552"> <p>
									Azure File
								</p>
								 </td><td align="left" valign="top" headers="idm140031633032464"> <p>
									<code class="literal">kubernetes.io/azure-file</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031633031376"> <p>
									The <code class="literal">persistent-volume-binder</code> service account requires permissions to create and get secrets to store the Azure storage account and keys.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031633033552"> <p>
									GCE Persistent Disk (gcePD)
								</p>
								 </td><td align="left" valign="top" headers="idm140031633032464"> <p>
									<code class="literal">kubernetes.io/gce-pd</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031633031376"> <p>
									In multi-zone configurations, it is advisable to run one OpenShift Container Platform cluster per GCE project to avoid PVs from being created in zones where no node in the current cluster exists.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031633033552"> <p>
									IBM Power Virtual Server Block
								</p>
								 </td><td align="left" valign="top" headers="idm140031633032464"> <p>
									<code class="literal">powervs.csi.ibm.com</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031633031376"> <p>
									After installation, the IBM Power Virtual Server Block CSI Driver Operator and IBM Power Virtual Server Block CSI Driver automatically create the required storage classes for dynamic provisioning.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031633033552"> <p>
									<a class="link" href="https://www.vmware.com/support/vsphere.html">VMware vSphere</a>
								</p>
								 </td><td align="left" valign="top" headers="idm140031633032464"> <p>
									<code class="literal">kubernetes.io/vsphere-volume</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031633031376"> </td></tr></tbody></table></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Any chosen provisioner plugin also requires configuration for the relevant cloud, host, or third-party provider as per the relevant documentation.
					</p></div></div></section></section><section class="section" id="defining-storage-classes_post-install-storage-configuration"><div class="titlepage"><div><div><h2 class="title">10.2. Defining a storage class</h2></div></div></div><p>
				<code class="literal">StorageClass</code> objects are currently a globally scoped object and must be created by <code class="literal">cluster-admin</code> or <code class="literal">storage-admin</code> users.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					The Cluster Storage Operator might install a default storage class depending on the platform in use. This storage class is owned and controlled by the Operator. It cannot be deleted or modified beyond defining annotations and labels. If different behavior is desired, you must define a custom storage class.
				</p></div></div><p>
				The following sections describe the basic definition for a <code class="literal">StorageClass</code> object and specific examples for each of the supported plugin types.
			</p><section class="section" id="basic-storage-class-definition_post-install-storage-configuration"><div class="titlepage"><div><div><h3 class="title">10.2.1. Basic StorageClass object definition</h3></div></div></div><p>
					The following resource shows the parameters and default values that you use to configure a storage class. This example uses the AWS ElasticBlockStore (EBS) object definition.
				</p><div class="formalpara"><p class="title"><strong>Sample <code class="literal">StorageClass</code> definition</strong></p><p>
						
<pre class="programlisting language-yaml">kind: StorageClass <span id="CO132-1"><!--Empty--></span><span class="callout">1</span>
apiVersion: storage.k8s.io/v1 <span id="CO132-2"><!--Empty--></span><span class="callout">2</span>
metadata:
  name: &lt;storage-class-name&gt; <span id="CO132-3"><!--Empty--></span><span class="callout">3</span>
  annotations: <span id="CO132-4"><!--Empty--></span><span class="callout">4</span>
    storageclass.kubernetes.io/is-default-class: 'true'
    ...
provisioner: kubernetes.io/aws-ebs <span id="CO132-5"><!--Empty--></span><span class="callout">5</span>
parameters: <span id="CO132-6"><!--Empty--></span><span class="callout">6</span>
  type: gp3
...</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO132-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							(required) The API object type.
						</div></dd><dt><a href="#CO132-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							(required) The current apiVersion.
						</div></dd><dt><a href="#CO132-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							(required) The name of the storage class.
						</div></dd><dt><a href="#CO132-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							(optional) Annotations for the storage class.
						</div></dd><dt><a href="#CO132-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							(required) The type of provisioner associated with this storage class.
						</div></dd><dt><a href="#CO132-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							(optional) The parameters required for the specific provisioner, this will change from plugin to plug-iin.
						</div></dd></dl></div></section><section class="section" id="storage-class-annotations_post-install-storage-configuration"><div class="titlepage"><div><div><h3 class="title">10.2.2. Storage class annotations</h3></div></div></div><p>
					To set a storage class as the cluster-wide default, add the following annotation to your storage class metadata:
				</p><pre class="programlisting language-yaml">storageclass.kubernetes.io/is-default-class: "true"</pre><p>
					For example:
				</p><pre class="programlisting language-yaml">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
...</pre><p>
					This enables any persistent volume claim (PVC) that does not specify a specific storage class to automatically be provisioned through the default storage class. However, your cluster can have more than one storage class, but only one of them can be the default storage class.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The beta annotation <code class="literal">storageclass.beta.kubernetes.io/is-default-class</code> is still working; however, it will be removed in a future release.
					</p></div></div><p>
					To set a storage class description, add the following annotation to your storage class metadata:
				</p><pre class="programlisting language-yaml">kubernetes.io/description: My Storage Class Description</pre><p>
					For example:
				</p><pre class="programlisting language-yaml">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    kubernetes.io/description: My Storage Class Description
...</pre></section><section class="section" id="openstack-cinder-storage-class_post-install-storage-configuration"><div class="titlepage"><div><div><h3 class="title">10.2.3. RHOSP Cinder object definition</h3></div></div></div><div class="formalpara"><p class="title"><strong>cinder-storageclass.yaml</strong></p><p>
						
<pre class="programlisting language-yaml">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: &lt;storage-class-name&gt; <span id="CO133-1"><!--Empty--></span><span class="callout">1</span>
provisioner: kubernetes.io/cinder
parameters:
  type: fast  <span id="CO133-2"><!--Empty--></span><span class="callout">2</span>
  availability: nova <span id="CO133-3"><!--Empty--></span><span class="callout">3</span>
  fsType: ext4 <span id="CO133-4"><!--Empty--></span><span class="callout">4</span></pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO133-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Name of the storage class. The persistent volume claim uses this storage class for provisioning the associated persistent volumes.
						</div></dd><dt><a href="#CO133-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Volume type created in Cinder. Default is empty.
						</div></dd><dt><a href="#CO133-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Availability Zone. If not specified, volumes are generally round-robined across all active zones where the OpenShift Container Platform cluster has a node.
						</div></dd><dt><a href="#CO133-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							File system that is created on dynamically provisioned volumes. This value is copied to the <code class="literal">fsType</code> field of dynamically provisioned persistent volumes and the file system is created when the volume is mounted for the first time. The default value is <code class="literal">ext4</code>.
						</div></dd></dl></div></section><section class="section" id="aws-definition_post-install-storage-configuration"><div class="titlepage"><div><div><h3 class="title">10.2.4. AWS Elastic Block Store (EBS) object definition</h3></div></div></div><div class="formalpara"><p class="title"><strong>aws-ebs-storageclass.yaml</strong></p><p>
						
<pre class="programlisting language-yaml">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: &lt;storage-class-name&gt; <span id="CO134-1"><!--Empty--></span><span class="callout">1</span>
provisioner: kubernetes.io/aws-ebs
parameters:
  type: io1 <span id="CO134-2"><!--Empty--></span><span class="callout">2</span>
  iopsPerGB: "10" <span id="CO134-3"><!--Empty--></span><span class="callout">3</span>
  encrypted: "true" <span id="CO134-4"><!--Empty--></span><span class="callout">4</span>
  kmsKeyId: keyvalue <span id="CO134-5"><!--Empty--></span><span class="callout">5</span>
  fsType: ext4 <span id="CO134-6"><!--Empty--></span><span class="callout">6</span></pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO134-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							(required) Name of the storage class. The persistent volume claim uses this storage class for provisioning the associated persistent volumes.
						</div></dd><dt><a href="#CO134-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							(required) Select from <code class="literal">io1</code>, <code class="literal">gp3</code>, <code class="literal">sc1</code>, <code class="literal">st1</code>. The default is <code class="literal">gp3</code>. See the <a class="link" href="http://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html">AWS documentation</a> for valid Amazon Resource Name (ARN) values.
						</div></dd><dt><a href="#CO134-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Optional: Only for <span class="strong strong"><strong>io1</strong></span> volumes. I/O operations per second per GiB. The AWS volume plugin multiplies this with the size of the requested volume to compute IOPS of the volume. The value cap is 20,000 IOPS, which is the maximum supported by AWS. See the <a class="link" href="http://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html">AWS documentation</a> for further details.
						</div></dd><dt><a href="#CO134-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Optional: Denotes whether to encrypt the EBS volume. Valid values are <code class="literal">true</code> or <code class="literal">false</code>.
						</div></dd><dt><a href="#CO134-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Optional: The full ARN of the key to use when encrypting the volume. If none is supplied, but <code class="literal">encypted</code> is set to <code class="literal">true</code>, then AWS generates a key. See the <a class="link" href="http://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html">AWS documentation</a> for a valid ARN value.
						</div></dd><dt><a href="#CO134-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							Optional: File system that is created on dynamically provisioned volumes. This value is copied to the <code class="literal">fsType</code> field of dynamically provisioned persistent volumes and the file system is created when the volume is mounted for the first time. The default value is <code class="literal">ext4</code>.
						</div></dd></dl></div></section><section class="section" id="azure-disk-definition_post-install-storage-configuration"><div class="titlepage"><div><div><h3 class="title">10.2.5. Azure Disk object definition</h3></div></div></div><div class="formalpara"><p class="title"><strong>azure-advanced-disk-storageclass.yaml</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: &lt;storage-class-name&gt; <span id="CO135-1"><!--Empty--></span><span class="callout">1</span>
provisioner: kubernetes.io/azure-disk
volumeBindingMode: WaitForFirstConsumer <span id="CO135-2"><!--Empty--></span><span class="callout">2</span>
allowVolumeExpansion: true
parameters:
  kind: Managed <span id="CO135-3"><!--Empty--></span><span class="callout">3</span>
  storageaccounttype: Premium_LRS <span id="CO135-4"><!--Empty--></span><span class="callout">4</span>
reclaimPolicy: Delete</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO135-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Name of the storage class. The persistent volume claim uses this storage class for provisioning the associated persistent volumes.
						</div></dd><dt><a href="#CO135-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Using <code class="literal">WaitForFirstConsumer</code> is strongly recommended. This provisions the volume while allowing enough storage to schedule the pod on a free worker node from an available zone.
						</div></dd><dt><a href="#CO135-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Possible values are <code class="literal">Shared</code> (default), <code class="literal">Managed</code>, and <code class="literal">Dedicated</code>.
						</div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Red Hat only supports the use of <code class="literal">kind: Managed</code> in the storage class.
							</p><p>
								With <code class="literal">Shared</code> and <code class="literal">Dedicated</code>, Azure creates unmanaged disks, while OpenShift Container Platform creates a managed disk for machine OS (root) disks. But because Azure Disk does not allow the use of both managed and unmanaged disks on a node, unmanaged disks created with <code class="literal">Shared</code> or <code class="literal">Dedicated</code> cannot be attached to OpenShift Container Platform nodes.
							</p></div></div></dd><dt><a href="#CO135-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Azure storage account SKU tier. Default is empty. Note that Premium VMs can attach both <code class="literal">Standard_LRS</code> and <code class="literal">Premium_LRS</code> disks, Standard VMs can only attach <code class="literal">Standard_LRS</code> disks, Managed VMs can only attach managed disks, and unmanaged VMs can only attach unmanaged disks.
						</div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									If <code class="literal">kind</code> is set to <code class="literal">Shared</code>, Azure creates all unmanaged disks in a few shared storage accounts in the same resource group as the cluster.
								</li><li class="listitem">
									If <code class="literal">kind</code> is set to <code class="literal">Managed</code>, Azure creates new managed disks.
								</li><li class="listitem"><p class="simpara">
									If <code class="literal">kind</code> is set to <code class="literal">Dedicated</code> and a <code class="literal">storageAccount</code> is specified, Azure uses the specified storage account for the new unmanaged disk in the same resource group as the cluster. For this to work:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											The specified storage account must be in the same region.
										</li><li class="listitem">
											Azure Cloud Provider must have write access to the storage account.
										</li></ul></div></li><li class="listitem">
									If <code class="literal">kind</code> is set to <code class="literal">Dedicated</code> and a <code class="literal">storageAccount</code> is not specified, Azure creates a new dedicated storage account for the new unmanaged disk in the same resource group as the cluster.
								</li></ol></div></dd></dl></div></section><section class="section" id="azure-file-definition_post-install-storage-configuration"><div class="titlepage"><div><div><h3 class="title">10.2.6. Azure File object definition</h3></div></div></div><p>
					The Azure File storage class uses secrets to store the Azure storage account name and the storage account key that are required to create an Azure Files share. These permissions are created as part of the following procedure.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Define a <code class="literal">ClusterRole</code> object that allows access to create and view secrets:
						</p><pre class="programlisting language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
#  name: system:azure-cloud-provider
  name: &lt;persistent-volume-binder-role&gt; <span id="CO136-1"><!--Empty--></span><span class="callout">1</span>
rules:
- apiGroups: ['']
  resources: ['secrets']
  verbs:     ['get','create']</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO136-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the cluster role to view and create secrets.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Add the cluster role to the service account:
						</p><pre class="programlisting language-terminal">$ oc adm policy add-cluster-role-to-user &lt;persistent-volume-binder-role&gt; system:serviceaccount:kube-system:persistent-volume-binder</pre></li><li class="listitem"><p class="simpara">
							Create the Azure File <code class="literal">StorageClass</code> object:
						</p><pre class="programlisting language-yaml">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: &lt;azure-file&gt; <span id="CO137-1"><!--Empty--></span><span class="callout">1</span>
provisioner: kubernetes.io/azure-file
parameters:
  location: eastus <span id="CO137-2"><!--Empty--></span><span class="callout">2</span>
  skuName: Standard_LRS <span id="CO137-3"><!--Empty--></span><span class="callout">3</span>
  storageAccount: &lt;storage-account&gt; <span id="CO137-4"><!--Empty--></span><span class="callout">4</span>
reclaimPolicy: Delete
volumeBindingMode: Immediate</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO137-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Name of the storage class. The persistent volume claim uses this storage class for provisioning the associated persistent volumes.
								</div></dd><dt><a href="#CO137-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Location of the Azure storage account, such as <code class="literal">eastus</code>. Default is empty, meaning that a new Azure storage account will be created in the OpenShift Container Platform cluster’s location.
								</div></dd><dt><a href="#CO137-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									SKU tier of the Azure storage account, such as <code class="literal">Standard_LRS</code>. Default is empty, meaning that a new Azure storage account will be created with the <code class="literal">Standard_LRS</code> SKU.
								</div></dd><dt><a href="#CO137-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Name of the Azure storage account. If a storage account is provided, then <code class="literal">skuName</code> and <code class="literal">location</code> are ignored. If no storage account is provided, then the storage class searches for any storage account that is associated with the resource group for any accounts that match the defined <code class="literal">skuName</code> and <code class="literal">location</code>.
								</div></dd></dl></div></li></ol></div><section class="section" id="azure-file-considerations_post-install-storage-configuration"><div class="titlepage"><div><div><h4 class="title">10.2.6.1. Considerations when using Azure File</h4></div></div></div><p>
						The following file system features are not supported by the default Azure File storage class:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Symlinks
							</li><li class="listitem">
								Hard links
							</li><li class="listitem">
								Extended attributes
							</li><li class="listitem">
								Sparse files
							</li><li class="listitem">
								Named pipes
							</li></ul></div><p>
						Additionally, the owner user identifier (UID) of the Azure File mounted directory is different from the process UID of the container. The <code class="literal">uid</code> mount option can be specified in the <code class="literal">StorageClass</code> object to define a specific user identifier to use for the mounted directory.
					</p><p>
						The following <code class="literal">StorageClass</code> object demonstrates modifying the user and group identifier, along with enabling symlinks for the mounted directory.
					</p><pre class="programlisting language-yaml">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: azure-file
mountOptions:
  - uid=1500 <span id="CO138-1"><!--Empty--></span><span class="callout">1</span>
  - gid=1500 <span id="CO138-2"><!--Empty--></span><span class="callout">2</span>
  - mfsymlinks <span id="CO138-3"><!--Empty--></span><span class="callout">3</span>
provisioner: kubernetes.io/azure-file
parameters:
  location: eastus
  skuName: Standard_LRS
reclaimPolicy: Delete
volumeBindingMode: Immediate</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO138-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specifies the user identifier to use for the mounted directory.
							</div></dd><dt><a href="#CO138-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specifies the group identifier to use for the mounted directory.
							</div></dd><dt><a href="#CO138-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Enables symlinks.
							</div></dd></dl></div></section></section><section class="section" id="gce-persistentdisk-storage-class_post-install-storage-configuration"><div class="titlepage"><div><div><h3 class="title">10.2.7. GCE PersistentDisk (gcePD) object definition</h3></div></div></div><div class="formalpara"><p class="title"><strong>gce-pd-storageclass.yaml</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: &lt;storage-class-name&gt; <span id="CO139-1"><!--Empty--></span><span class="callout">1</span>
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard <span id="CO139-2"><!--Empty--></span><span class="callout">2</span>
  replication-type: none
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Delete</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO139-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Name of the storage class. The persistent volume claim uses this storage class for provisioning the associated persistent volumes.
						</div></dd><dt><a href="#CO139-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Select either <code class="literal">pd-standard</code> or <code class="literal">pd-ssd</code>. The default is <code class="literal">pd-standard</code>.
						</div></dd></dl></div></section><section class="section" id="vsphere-definition_post-install-storage-configuration"><div class="titlepage"><div><div><h3 class="title">10.2.8. VMware vSphere object definition</h3></div></div></div><div class="formalpara"><p class="title"><strong>vsphere-storageclass.yaml</strong></p><p>
						
<pre class="programlisting language-yaml">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: &lt;storage-class-name&gt; <span id="CO140-1"><!--Empty--></span><span class="callout">1</span>
provisioner: kubernetes.io/vsphere-volume <span id="CO140-2"><!--Empty--></span><span class="callout">2</span>
parameters:
  diskformat: thin <span id="CO140-3"><!--Empty--></span><span class="callout">3</span></pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO140-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Name of the storage class. The persistent volume claim uses this storage class for provisioning the associated persistent volumes.
						</div></dd><dt><a href="#CO140-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							For more information about using VMware vSphere with OpenShift Container Platform, see the <a class="link" href="https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/index.html">VMware vSphere documentation</a>.
						</div></dd><dt><a href="#CO140-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							<code class="literal">diskformat</code>: <code class="literal">thin</code>, <code class="literal">zeroedthick</code> and <code class="literal">eagerzeroedthick</code> are all valid disk formats. See vSphere docs for additional details regarding the disk format types. The default value is <code class="literal">thin</code>.
						</div></dd></dl></div></section><section class="section" id="ovirt-csi-driver-storage-class_post-install-storage-configuration"><div class="titlepage"><div><div><h3 class="title">10.2.9. Red Hat Virtualization (RHV) object definition</h3></div></div></div><p>
					OpenShift Container Platform creates a default object of type <code class="literal">StorageClass</code> named <code class="literal">ovirt-csi-sc</code> which is used for creating dynamically provisioned persistent volumes.
				</p><p>
					To create additional storage classes for different configurations, create and save a file with the <code class="literal">StorageClass</code> object described by the following sample YAML:
				</p><div class="formalpara"><p class="title"><strong>ovirt-storageclass.yaml</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: &lt;storage_class_name&gt;  <span id="CO141-1"><!--Empty--></span><span class="callout">1</span>
  annotations:
    storageclass.kubernetes.io/is-default-class: "&lt;boolean&gt;"  <span id="CO141-2"><!--Empty--></span><span class="callout">2</span>
provisioner: csi.ovirt.org
allowVolumeExpansion: &lt;boolean&gt; <span id="CO141-3"><!--Empty--></span><span class="callout">3</span>
reclaimPolicy: Delete <span id="CO141-4"><!--Empty--></span><span class="callout">4</span>
volumeBindingMode: Immediate <span id="CO141-5"><!--Empty--></span><span class="callout">5</span>
parameters:
  storageDomainName: &lt;rhv-storage-domain-name&gt; <span id="CO141-6"><!--Empty--></span><span class="callout">6</span>
  thinProvisioning: "&lt;boolean&gt;"  <span id="CO141-7"><!--Empty--></span><span class="callout">7</span>
  csi.storage.k8s.io/fstype: &lt;file_system_type&gt; <span id="CO141-8"><!--Empty--></span><span class="callout">8</span></pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO141-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Name of the storage class.
						</div></dd><dt><a href="#CO141-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Set to <code class="literal">false</code> if the storage class is the default storage class in the cluster. If set to <code class="literal">true</code>, the existing default storage class must be edited and set to <code class="literal">false</code>.
						</div></dd><dt><a href="#CO141-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							<code class="literal">true</code> enables dynamic volume expansion, <code class="literal">false</code> prevents it. <code class="literal">true</code> is recommended.
						</div></dd><dt><a href="#CO141-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Dynamically provisioned persistent volumes of this storage class are created with this reclaim policy. This default policy is <code class="literal">Delete</code>.
						</div></dd><dt><a href="#CO141-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Indicates how to provision and bind <code class="literal">PersistentVolumeClaims</code>. When not set, <code class="literal">VolumeBindingImmediate</code> is used. This field is only applied by servers that enable the <code class="literal">VolumeScheduling</code> feature.
						</div></dd><dt><a href="#CO141-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							The RHV storage domain name to use.
						</div></dd><dt><a href="#CO141-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							If <code class="literal">true</code>, the disk is thin provisioned. If <code class="literal">false</code>, the disk is preallocated. Thin provisioning is recommended.
						</div></dd><dt><a href="#CO141-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							Optional: File system type to be created. Possible values: <code class="literal">ext4</code> (default) or <code class="literal">xfs</code>.
						</div></dd></dl></div></section></section><section class="section" id="change-default-storage-class_post-install-storage-configuration"><div class="titlepage"><div><div><h2 class="title">10.3. Changing the default storage class</h2></div></div></div><p>
				Use the following procedure to change the default storage class.
			</p><p>
				For example, if you have two defined storage classes, <code class="literal">gp3</code> and <code class="literal">standard</code>, and you want to change the default storage class from <code class="literal">gp3</code> to <code class="literal">standard</code>.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Access to the cluster with cluster-admin privileges.
					</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					To change the default storage class:
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						List the storage classes:
					</p><pre class="programlisting language-terminal">$ oc get storageclass</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                 TYPE
gp3 (default)        kubernetes.io/aws-ebs <span id="CO142-1"><!--Empty--></span><span class="callout">1</span>
standard             kubernetes.io/aws-ebs</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO142-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								<code class="literal">(default)</code> indicates the default storage class.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Make the desired storage class the default.
					</p><p class="simpara">
						For the desired storage class, set the <code class="literal">storageclass.kubernetes.io/is-default-class</code> annotation to <code class="literal">true</code> by running the following command:
					</p><pre class="programlisting language-terminal">$ oc patch storageclass standard -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "true"}}}'</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You can have multiple default storage classes for a short time. However, you should ensure that only one default storage class exists eventually.
						</p><p>
							With multiple default storage classes present, any persistent volume claim (PVC) requesting the default storage class (<code class="literal">pvc.spec.storageClassName</code>=nil) gets the most recently created default storage class, regardless of the default status of that storage class, and the administrator receives an alert in the alerts dashboard that there are multiple default storage classes, <code class="literal">MultipleDefaultStorageClasses</code>.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Remove the default storage class setting from the old default storage class.
					</p><p class="simpara">
						For the old default storage class, change the value of the <code class="literal">storageclass.kubernetes.io/is-default-class</code> annotation to <code class="literal">false</code> by running the following command:
					</p><pre class="programlisting language-terminal">$ oc patch storageclass gp3 -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "false"}}}'</pre></li><li class="listitem"><p class="simpara">
						Verify the changes:
					</p><pre class="programlisting language-terminal">$ oc get storageclass</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                 TYPE
gp3                  kubernetes.io/aws-ebs
standard (default)   kubernetes.io/aws-ebs</pre>

						</p></div></li></ol></div></section><section class="section" id="post-install-optimizing-storage"><div class="titlepage"><div><div><h2 class="title">10.4. Optimizing storage</h2></div></div></div><p>
				Optimizing storage helps to minimize storage use across all resources. By optimizing storage, administrators help ensure that existing storage resources are working in an efficient manner.
			</p></section><section class="section" id="available-persistent-storage-options_post-install-storage-configuration"><div class="titlepage"><div><div><h2 class="title">10.5. Available persistent storage options</h2></div></div></div><p>
				Understand your persistent storage options so that you can optimize your OpenShift Container Platform environment.
			</p><div class="table" id="idm140031633379696"><p class="title"><strong>Table 10.1. Available storage options</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 13%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col><col style="width: 38%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031633373920" scope="col">Storage type</th><th align="left" valign="top" id="idm140031633372832" scope="col">Description</th><th align="left" valign="top" id="idm140031633371744" scope="col">Examples</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031633373920"> <p>
								Block
							</p>
							 </td><td align="left" valign="top" headers="idm140031633372832"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Presented to the operating system (OS) as a block device
									</li><li class="listitem">
										Suitable for applications that need full control of storage and operate at a low level on files bypassing the file system
									</li><li class="listitem">
										Also referred to as a Storage Area Network (SAN)
									</li><li class="listitem">
										Non-shareable, which means that only one client at a time can mount an endpoint of this type
									</li></ul></div>
							 </td><td align="left" valign="top" headers="idm140031633371744"> <p>
								AWS EBS and VMware vSphere support dynamic persistent volume (PV) provisioning natively in OpenShift Container Platform.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031633373920"> <p>
								File
							</p>
							 </td><td align="left" valign="top" headers="idm140031633372832"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Presented to the OS as a file system export to be mounted
									</li><li class="listitem">
										Also referred to as Network Attached Storage (NAS)
									</li><li class="listitem">
										Concurrency, latency, file locking mechanisms, and other capabilities vary widely between protocols, implementations, vendors, and scales.
									</li></ul></div>
							 </td><td align="left" valign="top" headers="idm140031633371744"> <p>
								RHEL NFS, NetApp NFS <sup>[1]</sup>, and Vendor NFS
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031633373920"> <p>
								Object
							</p>
							 </td><td align="left" valign="top" headers="idm140031633372832"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Accessible through a REST API endpoint
									</li><li class="listitem">
										Configurable for use in the OpenShift image registry
									</li><li class="listitem">
										Applications must build their drivers into the application and/or container.
									</li></ul></div>
							 </td><td align="left" valign="top" headers="idm140031633371744"> <p>
								AWS S3
							</p>
							 </td></tr></tbody></table></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						NetApp NFS supports dynamic PV provisioning when using the Trident plugin.
					</li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Currently, CNS is not supported in OpenShift Container Platform 4.13.
				</p></div></div></section><section class="section" id="recommended-configurable-storage-technology_post-install-storage-configuration"><div class="titlepage"><div><div><h2 class="title">10.6. Recommended configurable storage technology</h2></div></div></div><p>
				The following table summarizes the recommended and configurable storage technologies for the given OpenShift Container Platform cluster application.
			</p><div class="table" id="idm140031633337808"><p class="title"><strong>Table 10.2. Recommended and configurable storage technology</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031633331088" scope="col">Storage type</th><th align="left" valign="top" id="idm140031632844496" scope="col">Block</th><th align="left" valign="top" id="idm140031632843408" scope="col">File</th><th align="left" valign="top" id="idm140031632842320" scope="col">Object</th></tr></thead><tfoot><tr><th colspan="4" align="left" valign="top" id="idm140031632840224" scope=""> <p>
							<sup>1</sup> <code class="literal">ReadOnlyMany</code>
						</p>
						 <p>
							<sup>2</sup> <code class="literal">ReadWriteMany</code>
						</p>
						 <p>
							<sup>3</sup> Prometheus is the underlying technology used for metrics.
						</p>
						 <p>
							<sup>4</sup> This does not apply to physical disk, VM physical disk, VMDK, loopback over NFS, AWS EBS, and Azure Disk.
						</p>
						 <p>
							<sup>5</sup> For metrics, using file storage with the <code class="literal">ReadWriteMany</code> (RWX) access mode is unreliable. If you use file storage, do not configure the RWX access mode on any persistent volume claims (PVCs) that are configured for use with metrics.
						</p>
						 <p>
							<sup>6</sup> For logging, review the recommended storage solution in Configuring persistent storage for the log store section. Using NFS storage as a persistent volume or through NAS, such as Gluster, can corrupt the data. Hence, NFS is not supported for Elasticsearch storage and LokiStack log store in OpenShift Container Platform Logging. You must use one persistent volume type per log store.
						</p>
						 <p>
							<sup>7</sup> Object storage is not consumed through OpenShift Container Platform’s PVs or PVCs. Apps must integrate with the object storage REST API.
						</p>
						 </th></tr></tfoot><tbody><tr><td align="left" valign="top" headers="idm140031633331088"> <p>
								ROX<sup>1</sup>
							</p>
							 </td><td align="left" valign="top" headers="idm140031632844496"> <p>
								Yes<sup>4</sup>
							</p>
							 </td><td align="left" valign="top" headers="idm140031632843408"> <p>
								Yes<sup>4</sup>
							</p>
							 </td><td align="left" valign="top" headers="idm140031632842320"> <p>
								Yes
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031633331088"> <p>
								RWX<sup>2</sup>
							</p>
							 </td><td align="left" valign="top" headers="idm140031632844496"> <p>
								No
							</p>
							 </td><td align="left" valign="top" headers="idm140031632843408"> <p>
								Yes
							</p>
							 </td><td align="left" valign="top" headers="idm140031632842320"> <p>
								Yes
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031633331088"> <p>
								Registry
							</p>
							 </td><td align="left" valign="top" headers="idm140031632844496"> <p>
								Configurable
							</p>
							 </td><td align="left" valign="top" headers="idm140031632843408"> <p>
								Configurable
							</p>
							 </td><td align="left" valign="top" headers="idm140031632842320"> <p>
								Recommended
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031633331088"> <p>
								Scaled registry
							</p>
							 </td><td align="left" valign="top" headers="idm140031632844496"> <p>
								Not configurable
							</p>
							 </td><td align="left" valign="top" headers="idm140031632843408"> <p>
								Configurable
							</p>
							 </td><td align="left" valign="top" headers="idm140031632842320"> <p>
								Recommended
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031633331088"> <p>
								Metrics<sup>3</sup>
							</p>
							 </td><td align="left" valign="top" headers="idm140031632844496"> <p>
								Recommended
							</p>
							 </td><td align="left" valign="top" headers="idm140031632843408"> <p>
								Configurable<sup>5</sup>
							</p>
							 </td><td align="left" valign="top" headers="idm140031632842320"> <p>
								Not configurable
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031633331088"> <p>
								Elasticsearch Logging
							</p>
							 </td><td align="left" valign="top" headers="idm140031632844496"> <p>
								Recommended
							</p>
							 </td><td align="left" valign="top" headers="idm140031632843408"> <p>
								Configurable<sup>6</sup>
							</p>
							 </td><td align="left" valign="top" headers="idm140031632842320"> <p>
								Not supported<sup>6</sup>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031633331088"> <p>
								Loki Logging
							</p>
							 </td><td align="left" valign="top" headers="idm140031632844496"> <p>
								Configurable
							</p>
							 </td><td align="left" valign="top" headers="idm140031632843408"> <p>
								Not configurable
							</p>
							 </td><td align="left" valign="top" headers="idm140031632842320"> <p>
								Recommended
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031633331088"> <p>
								Apps
							</p>
							 </td><td align="left" valign="top" headers="idm140031632844496"> <p>
								Recommended
							</p>
							 </td><td align="left" valign="top" headers="idm140031632843408"> <p>
								Recommended
							</p>
							 </td><td align="left" valign="top" headers="idm140031632842320"> <p>
								Not configurable<sup>7</sup>
							</p>
							 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					A scaled registry is an OpenShift image registry where two or more pod replicas are running.
				</p></div></div><section class="section" id="specific-application-storage-recommendations"><div class="titlepage"><div><div><h3 class="title">10.6.1. Specific application storage recommendations</h3></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Testing shows issues with using the NFS server on Red Hat Enterprise Linux (RHEL) as storage backend for core services. This includes the OpenShift Container Registry and Quay, Prometheus for monitoring storage, and Elasticsearch for logging storage. Therefore, using RHEL NFS to back PVs used by core services is not recommended.
					</p><p>
						Other NFS implementations on the marketplace might not have these issues. Contact the individual NFS implementation vendor for more information on any testing that was possibly completed against these OpenShift Container Platform core components.
					</p></div></div><section class="section" id="registry"><div class="titlepage"><div><div><h4 class="title">10.6.1.1. Registry</h4></div></div></div><p>
						In a non-scaled/high-availability (HA) OpenShift image registry cluster deployment:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								The storage technology does not have to support RWX access mode.
							</li><li class="listitem">
								The storage technology must ensure read-after-write consistency.
							</li><li class="listitem">
								The preferred storage technology is object storage followed by block storage.
							</li><li class="listitem">
								File storage is not recommended for OpenShift image registry cluster deployment with production workloads.
							</li></ul></div></section><section class="section" id="scaled-registry"><div class="titlepage"><div><div><h4 class="title">10.6.1.2. Scaled registry</h4></div></div></div><p>
						In a scaled/HA OpenShift image registry cluster deployment:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								The storage technology must support RWX access mode.
							</li><li class="listitem">
								The storage technology must ensure read-after-write consistency.
							</li><li class="listitem">
								The preferred storage technology is object storage.
							</li><li class="listitem">
								Red Hat OpenShift Data Foundation (ODF), Amazon Simple Storage Service (Amazon S3), Google Cloud Storage (GCS), Microsoft Azure Blob Storage, and OpenStack Swift are supported.
							</li><li class="listitem">
								Object storage should be S3 or Swift compliant.
							</li><li class="listitem">
								For non-cloud platforms, such as vSphere and bare metal installations, the only configurable technology is file storage.
							</li><li class="listitem">
								Block storage is not configurable.
							</li></ul></div></section><section class="section" id="metrics"><div class="titlepage"><div><div><h4 class="title">10.6.1.3. Metrics</h4></div></div></div><p>
						In an OpenShift Container Platform hosted metrics cluster deployment:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								The preferred storage technology is block storage.
							</li><li class="listitem">
								Object storage is not configurable.
							</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							It is not recommended to use file storage for a hosted metrics cluster deployment with production workloads.
						</p></div></div></section><section class="section" id="logging"><div class="titlepage"><div><div><h4 class="title">10.6.1.4. Logging</h4></div></div></div><p>
						In an OpenShift Container Platform hosted logging cluster deployment:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								The preferred storage technology is block storage.
							</li><li class="listitem">
								Object storage is not configurable.
							</li></ul></div></section><section class="section" id="applications"><div class="titlepage"><div><div><h4 class="title">10.6.1.5. Applications</h4></div></div></div><p>
						Application use cases vary from application to application, as described in the following examples:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Storage technologies that support dynamic PV provisioning have low mount time latencies, and are not tied to nodes to support a healthy cluster.
							</li><li class="listitem">
								Application developers are responsible for knowing and understanding the storage requirements for their application, and how it works with the provided storage to ensure that issues do not occur when an application scales or interacts with the storage layer.
							</li></ul></div></section></section><section class="section" id="other-specific-application-storage-recommendations"><div class="titlepage"><div><div><h3 class="title">10.6.2. Other specific application storage recommendations</h3></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						It is not recommended to use RAID configurations on <code class="literal">Write</code> intensive workloads, such as <code class="literal">etcd</code>. If you are running <code class="literal">etcd</code> with a RAID configuration, you might be at risk of encountering performance issues with your workloads.
					</p></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Red Hat OpenStack Platform (RHOSP) Cinder: RHOSP Cinder tends to be adept in ROX access mode use cases.
						</li><li class="listitem">
							Databases: Databases (RDBMSs, NoSQL DBs, etc.) tend to perform best with dedicated block storage.
						</li><li class="listitem">
							The etcd database must have enough storage and adequate performance capacity to enable a large cluster. Information about monitoring and benchmarking tools to establish ample storage and a high-performance environment is described in <span class="emphasis"><em>Recommended etcd practices</em></span>.
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#recommended-etcd-practices">Recommended etcd practices</a>
						</li></ul></div></section></section><section class="section" id="post-install-deploy-OCS"><div class="titlepage"><div><div><h2 class="title">10.7. Deploy Red Hat OpenShift Data Foundation</h2></div></div></div><p>
				Red Hat OpenShift Data Foundation is a provider of agnostic persistent storage for OpenShift Container Platform supporting file, block, and object storage, either in-house or in hybrid clouds. As a Red Hat storage solution, Red Hat OpenShift Data Foundation is completely integrated with OpenShift Container Platform for deployment, management, and monitoring.
			</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031634677984" scope="col">If you are looking for Red Hat OpenShift Data Foundation information about…​</th><th align="left" valign="top" id="idm140031634676752" scope="col">See the following Red Hat OpenShift Data Foundation documentation:</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031634677984"> <p>
								What’s new, known issues, notable bug fixes, and Technology Previews
							</p>
							 </td><td align="left" valign="top" headers="idm140031634676752"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/4.12_release_notes">OpenShift Data Foundation 4.12 Release Notes</a>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031634677984"> <p>
								Supported workloads, layouts, hardware and software requirements, sizing and scaling recommendations
							</p>
							 </td><td align="left" valign="top" headers="idm140031634676752"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/planning_your_deployment">Planning your OpenShift Data Foundation 4.12 deployment</a>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031634677984"> <p>
								Instructions on deploying OpenShift Data Foundation to use an external Red Hat Ceph Storage cluster
							</p>
							 </td><td align="left" valign="top" headers="idm140031634676752"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_in_external_mode">Deploying OpenShift Data Foundation 4.12 in external mode</a>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031634677984"> <p>
								Instructions on deploying OpenShift Data Foundation to local storage on bare metal infrastructure
							</p>
							 </td><td align="left" valign="top" headers="idm140031634676752"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_using_bare_metal_infrastructure">Deploying OpenShift Data Foundation 4.12 using bare metal infrastructure</a>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031634677984"> <p>
								Instructions on deploying OpenShift Data Foundation on Red Hat OpenShift Container Platform VMware vSphere clusters
							</p>
							 </td><td align="left" valign="top" headers="idm140031634676752"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_on_vmware_vsphere">Deploying OpenShift Data Foundation 4.12 on VMware vSphere</a>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031634677984"> <p>
								Instructions on deploying OpenShift Data Foundation using Amazon Web Services for local or cloud storage
							</p>
							 </td><td align="left" valign="top" headers="idm140031634676752"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_using_amazon_web_services">Deploying OpenShift Data Foundation 4.12 using Amazon Web Services</a>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031634677984"> <p>
								Instructions on deploying and managing OpenShift Data Foundation on existing Red Hat OpenShift Container Platform Google Cloud clusters
							</p>
							 </td><td align="left" valign="top" headers="idm140031634676752"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_and_managing_openshift_data_foundation_using_google_cloud">Deploying and managing OpenShift Data Foundation 4.12 using Google Cloud</a>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031634677984"> <p>
								Instructions on deploying and managing OpenShift Data Foundation on existing Red Hat OpenShift Container Platform Azure clusters
							</p>
							 </td><td align="left" valign="top" headers="idm140031634676752"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_using_microsoft_azure/index">Deploying and managing OpenShift Data Foundation 4.12 using Microsoft Azure</a>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031634677984"> <p>
								Instructions on deploying OpenShift Data Foundation to use local storage on IBM Power infrastructure
							</p>
							 </td><td align="left" valign="top" headers="idm140031634676752"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html-single/deploying_openshift_data_foundation_using_ibm_power/index">Deploying OpenShift Data Foundation on IBM Power</a>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031634677984"> <p>
								Instructions on deploying OpenShift Data Foundation to use local storage on IBM Z infrastructure
							</p>
							 </td><td align="left" valign="top" headers="idm140031634676752"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_using_ibm_z_infrastructure/index">Deploying OpenShift Data Foundation on IBM Z infrastructure</a>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031634677984"> <p>
								Allocating storage to core services and hosted applications in Red Hat OpenShift Data Foundation, including snapshot and clone
							</p>
							 </td><td align="left" valign="top" headers="idm140031634676752"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/managing_and_allocating_storage_resources">Managing and allocating resources</a>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031634677984"> <p>
								Managing storage resources across a hybrid cloud or multicloud environment using the Multicloud Object Gateway (NooBaa)
							</p>
							 </td><td align="left" valign="top" headers="idm140031634676752"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/managing_hybrid_and_multicloud_resources">Managing hybrid and multicloud resources</a>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031634677984"> <p>
								Safely replacing storage devices for Red Hat OpenShift Data Foundation
							</p>
							 </td><td align="left" valign="top" headers="idm140031634676752"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/replacing_devices">Replacing devices</a>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031634677984"> <p>
								Safely replacing a node in a Red Hat OpenShift Data Foundation cluster
							</p>
							 </td><td align="left" valign="top" headers="idm140031634676752"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/replacing_nodes">Replacing nodes</a>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031634677984"> <p>
								Scaling operations in Red Hat OpenShift Data Foundation
							</p>
							 </td><td align="left" valign="top" headers="idm140031634676752"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/scaling_storage">Scaling storage</a>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031634677984"> <p>
								Monitoring a Red Hat OpenShift Data Foundation 4.12 cluster
							</p>
							 </td><td align="left" valign="top" headers="idm140031634676752"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/monitoring_openshift_data_foundation">Monitoring Red Hat OpenShift Data Foundation 4.12</a>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031634677984"> <p>
								Resolve issues encountered during operations
							</p>
							 </td><td align="left" valign="top" headers="idm140031634676752"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/troubleshooting_openshift_data_foundation">Troubleshooting OpenShift Data Foundation 4.12</a>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031634677984"> <p>
								Migrating your OpenShift Container Platform cluster from version 3 to version 4
							</p>
							 </td><td align="left" valign="top" headers="idm140031634676752"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.12/html/migrating_from_version_3_to_4/index">Migration</a>
							</p>
							 </td></tr></tbody></table></div></section><section class="section _additional-resources" id="admission-plug-ins-additional-resources"><div class="titlepage"><div><div><h2 class="title">10.8. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/logging/#cluster-logging-elasticsearch-storage_cluster-logging-log-store">Configuring persistent storage for the log store</a>
					</li></ul></div></section></section><section class="chapter" id="post-install-preparing-for-users"><div class="titlepage"><div><div><h1 class="title">Chapter 11. Preparing for users</h1></div></div></div><p>
			After installing OpenShift Container Platform, you can further expand and customize your cluster to your requirements, including taking steps to prepare for users.
		</p><section class="section" id="post-install-understanding-identity-provider"><div class="titlepage"><div><div><h2 class="title">11.1. Understanding identity provider configuration</h2></div></div></div><p>
				The OpenShift Container Platform control plane includes a built-in OAuth server. Developers and administrators obtain OAuth access tokens to authenticate themselves to the API.
			</p><p>
				As an administrator, you can configure OAuth to specify an identity provider after you install your cluster.
			</p><section class="section" id="identity-provider-overview_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.1.1. About identity providers in OpenShift Container Platform</h3></div></div></div><p>
					By default, only a <code class="literal">kubeadmin</code> user exists on your cluster. To specify an identity provider, you must create a custom resource (CR) that describes that identity provider and add it to the cluster.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						OpenShift Container Platform user names containing <code class="literal">/</code>, <code class="literal">:</code>, and <code class="literal">%</code> are not supported.
					</p></div></div></section><section class="section" id="post-install-supported-identity-providers"><div class="titlepage"><div><div><h3 class="title">11.1.2. Supported identity providers</h3></div></div></div><p>
					You can configure the following types of identity providers:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031636867552" scope="col">Identity provider</th><th align="left" valign="top" id="idm140031636866464" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031636867552"> <p>
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#configuring-htpasswd-identity-provider">htpasswd</a>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636866464"> <p>
									Configure the <code class="literal">htpasswd</code> identity provider to validate user names and passwords against a flat file generated using <a class="link" href="http://httpd.apache.org/docs/2.4/programs/htpasswd.html"><code class="literal">htpasswd</code></a>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031636867552"> <p>
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#configuring-keystone-identity-provider">Keystone</a>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636866464"> <p>
									Configure the <code class="literal">keystone</code> identity provider to integrate your OpenShift Container Platform cluster with Keystone to enable shared authentication with an OpenStack Keystone v3 server configured to store users in an internal database.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031636867552"> <p>
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#configuring-ldap-identity-provider">LDAP</a>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636866464"> <p>
									Configure the <code class="literal">ldap</code> identity provider to validate user names and passwords against an LDAPv3 server, using simple bind authentication.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031636867552"> <p>
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#configuring-basic-authentication-identity-provider">Basic authentication</a>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636866464"> <p>
									Configure a <code class="literal">basic-authentication</code> identity provider for users to log in to OpenShift Container Platform with credentials validated against a remote identity provider. Basic authentication is a generic backend integration mechanism.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031636867552"> <p>
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#configuring-request-header-identity-provider">Request header</a>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636866464"> <p>
									Configure a <code class="literal">request-header</code> identity provider to identify users from request header values, such as <code class="literal">X-Remote-User</code>. It is typically used in combination with an authenticating proxy, which sets the request header value.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031636867552"> <p>
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#configuring-github-identity-provider">GitHub or GitHub Enterprise</a>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636866464"> <p>
									Configure a <code class="literal">github</code> identity provider to validate user names and passwords against GitHub or GitHub Enterprise’s OAuth authentication server.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031636867552"> <p>
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#configuring-gitlab-identity-provider">GitLab</a>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636866464"> <p>
									Configure a <code class="literal">gitlab</code> identity provider to use <a class="link" href="https://gitlab.com/">GitLab.com</a> or any other GitLab instance as an identity provider.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031636867552"> <p>
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#configuring-google-identity-provider">Google</a>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636866464"> <p>
									Configure a <code class="literal">google</code> identity provider using <a class="link" href="https://developers.google.com/identity/protocols/OpenIDConnect">Google’s OpenID Connect integration</a>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031636867552"> <p>
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#configuring-oidc-identity-provider">OpenID Connect</a>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636866464"> <p>
									Configure an <code class="literal">oidc</code> identity provider to integrate with an OpenID Connect identity provider using an <a class="link" href="http://openid.net/specs/openid-connect-core-1_0.html#CodeFlowAuth">Authorization Code Flow</a>.
								</p>
								 </td></tr></tbody></table></div><p>
					After you define an identity provider, you can <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#authorization-overview_using-rbac">use RBAC to define and apply permissions</a>.
				</p></section><section class="section" id="identity-provider-parameters_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.1.3. Identity provider parameters</h3></div></div></div><p>
					The following parameters are common to all identity providers:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031636805824" scope="col">Parameter</th><th align="left" valign="top" id="idm140031636804736" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031636805824"> <p>
									<code class="literal">name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636804736"> <p>
									The provider name is prefixed to provider user names to form an identity name.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031636805824"> <p>
									<code class="literal">mappingMethod</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031636804736"> <p>
									Defines how new identities are mapped to users when they log in. Enter one of the following values:
								</p>
								 <div class="variablelist"><dl class="variablelist"><dt><span class="term">claim</span></dt><dd>
												The default value. Provisions a user with the identity’s preferred user name. Fails if a user with that user name is already mapped to another identity.
											</dd><dt><span class="term">lookup</span></dt><dd>
												Looks up an existing identity, user identity mapping, and user, but does not automatically provision users or identities. This allows cluster administrators to set up identities and users manually, or using an external process. Using this method requires you to manually provision users.
											</dd><dt><span class="term">generate</span></dt><dd>
												Provisions a user with the identity’s preferred user name. If a user with the preferred user name is already mapped to an existing identity, a unique user name is generated. For example, <code class="literal">myuser2</code>. This method should not be used in combination with external processes that require exact matches between OpenShift Container Platform user names and identity provider user names, such as LDAP group sync.
											</dd><dt><span class="term">add</span></dt><dd>
												Provisions a user with the identity’s preferred user name. If a user with that user name already exists, the identity is mapped to the existing user, adding to any existing identity mappings for the user. Required when multiple identity providers are configured that identify the same set of users and map to the same user names.
											</dd></dl></div>
								 </td></tr></tbody></table></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When adding or changing identity providers, you can map identities from the new provider to existing users by setting the <code class="literal">mappingMethod</code> parameter to <code class="literal">add</code>.
					</p></div></div></section><section class="section" id="identity-provider-default-CR_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.1.4. Sample identity provider CR</h3></div></div></div><p>
					The following custom resource (CR) shows the parameters and default values that you use to configure an identity provider. This example uses the htpasswd identity provider.
				</p><div class="formalpara"><p class="title"><strong>Sample identity provider CR</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: my_identity_provider <span id="CO143-1"><!--Empty--></span><span class="callout">1</span>
    mappingMethod: claim <span id="CO143-2"><!--Empty--></span><span class="callout">2</span>
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpass-secret <span id="CO143-3"><!--Empty--></span><span class="callout">3</span></pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO143-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							This provider name is prefixed to provider user names to form an identity name.
						</div></dd><dt><a href="#CO143-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Controls how mappings are established between this provider’s identities and <code class="literal">User</code> objects.
						</div></dd><dt><a href="#CO143-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							An existing secret containing a file generated using <a class="link" href="http://httpd.apache.org/docs/2.4/programs/htpasswd.html"><code class="literal">htpasswd</code></a>.
						</div></dd></dl></div></section></section><section class="section" id="post-install-using-rbac-to-define-and-apply-permissions"><div class="titlepage"><div><div><h2 class="title">11.2. Using RBAC to define and apply permissions</h2></div></div></div><p>
				Understand and apply role-based access control.
			</p><section class="section" id="authorization-overview_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.2.1. RBAC overview</h3></div></div></div><p>
					Role-based access control (RBAC) objects determine whether a user is allowed to perform a given action within a project.
				</p><p>
					Cluster administrators can use the cluster roles and bindings to control who has various access levels to the OpenShift Container Platform platform itself and all projects.
				</p><p>
					Developers can use local roles and bindings to control who has access to their projects. Note that authorization is a separate step from authentication, which is more about determining the identity of who is taking the action.
				</p><p>
					Authorization is managed using:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031633295232" scope="col">Authorization object</th><th align="left" valign="top" id="idm140031633294144" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031633295232"> <p>
									Rules
								</p>
								 </td><td align="left" valign="top" headers="idm140031633294144"> <p>
									Sets of permitted verbs on a set of objects. For example, whether a user or service account can <code class="literal">create</code> pods.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031633295232"> <p>
									Roles
								</p>
								 </td><td align="left" valign="top" headers="idm140031633294144"> <p>
									Collections of rules. You can associate, or bind, users and groups to multiple roles.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031633295232"> <p>
									Bindings
								</p>
								 </td><td align="left" valign="top" headers="idm140031633294144"> <p>
									Associations between users and/or groups with a role.
								</p>
								 </td></tr></tbody></table></div><p>
					There are two levels of RBAC roles and bindings that control authorization:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031633275408" scope="col">RBAC level</th><th align="left" valign="top" id="idm140031633274320" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031633275408"> <p>
									Cluster RBAC
								</p>
								 </td><td align="left" valign="top" headers="idm140031633274320"> <p>
									Roles and bindings that are applicable across all projects. <span class="emphasis"><em>Cluster roles</em></span> exist cluster-wide, and <span class="emphasis"><em>cluster role bindings</em></span> can reference only cluster roles.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031633275408"> <p>
									Local RBAC
								</p>
								 </td><td align="left" valign="top" headers="idm140031633274320"> <p>
									Roles and bindings that are scoped to a given project. While <span class="emphasis"><em>local roles</em></span> exist only in a single project, local role bindings can reference <span class="emphasis"><em>both</em></span> cluster and local roles.
								</p>
								 </td></tr></tbody></table></div><p>
					A cluster role binding is a binding that exists at the cluster level. A role binding exists at the project level. The cluster role <span class="emphasis"><em>view</em></span> must be bound to a user using a local role binding for that user to view the project. Create local roles only if a cluster role does not provide the set of permissions needed for a particular situation.
				</p><p>
					This two-level hierarchy allows reuse across multiple projects through the cluster roles while allowing customization inside of individual projects through local roles.
				</p><p>
					During evaluation, both the cluster role bindings and the local role bindings are used. For example:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Cluster-wide "allow" rules are checked.
						</li><li class="listitem">
							Locally-bound "allow" rules are checked.
						</li><li class="listitem">
							Deny by default.
						</li></ol></div><section class="section" id="default-roles_post-install-preparing-for-users"><div class="titlepage"><div><div><h4 class="title">11.2.1.1. Default cluster roles</h4></div></div></div><p>
						OpenShift Container Platform includes a set of default cluster roles that you can bind to users and groups cluster-wide or locally.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							It is not recommended to manually modify the default cluster roles. Modifications to these system roles can prevent a cluster from functioning properly.
						</p></div></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031635209328" scope="col">Default cluster role</th><th align="left" valign="top" id="idm140031635208240" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031635209328"> <p>
										<code class="literal">admin</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140031635208240"> <p>
										A project manager. If used in a local binding, an <code class="literal">admin</code> has rights to view any resource in the project and modify any resource in the project except for quota.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140031635209328"> <p>
										<code class="literal">basic-user</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140031635208240"> <p>
										A user that can get basic information about projects and users.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140031635209328"> <p>
										<code class="literal">cluster-admin</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140031635208240"> <p>
										A super-user that can perform any action in any project. When bound to a user with a local binding, they have full control over quota and every action on every resource in the project.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140031635209328"> <p>
										<code class="literal">cluster-status</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140031635208240"> <p>
										A user that can get basic cluster status information.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140031635209328"> <p>
										<code class="literal">cluster-reader</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140031635208240"> <p>
										A user that can get or view most of the objects but cannot modify them.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140031635209328"> <p>
										<code class="literal">edit</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140031635208240"> <p>
										A user that can modify most objects in a project but does not have the power to view or modify roles or bindings.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140031635209328"> <p>
										<code class="literal">self-provisioner</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140031635208240"> <p>
										A user that can create their own projects.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140031635209328"> <p>
										<code class="literal">view</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140031635208240"> <p>
										A user who cannot make any modifications, but can see most objects in a project. They cannot view or modify roles or bindings.
									</p>
									 </td></tr></tbody></table></div><p>
						Be mindful of the difference between local and cluster bindings. For example, if you bind the <code class="literal">cluster-admin</code> role to a user by using a local role binding, it might appear that this user has the privileges of a cluster administrator. This is not the case. Binding the <code class="literal">cluster-admin</code> to a user in a project grants super administrator privileges for only that project to the user. That user has the permissions of the cluster role <code class="literal">admin</code>, plus a few additional permissions like the ability to edit rate limits, for that project. This binding can be confusing via the web console UI, which does not list cluster role bindings that are bound to true cluster administrators. However, it does list local role bindings that you can use to locally bind <code class="literal">cluster-admin</code>.
					</p><p>
						The relationships between cluster roles, local roles, cluster role bindings, local role bindings, users, groups and service accounts are illustrated below.
					</p><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Post-installation_configuration-en-US/images/052176e6fe02a52b8f683f060d917ad1/rbac.png" alt="OpenShift Container Platform RBAC"/></div></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							The <code class="literal">get pods/exec</code>, <code class="literal">get pods/*</code>, and <code class="literal">get *</code> rules grant execution privileges when they are applied to a role. Apply the principle of least privilege and assign only the minimal RBAC rights required for users and agents. For more information, see <a class="link" href="https://access.redhat.com/solutions/6989997">RBAC rules allow execution privileges</a>.
						</p></div></div></section><section class="section" id="evaluating-authorization_post-install-preparing-for-users"><div class="titlepage"><div><div><h4 class="title">11.2.1.2. Evaluating authorization</h4></div></div></div><p>
						OpenShift Container Platform evaluates authorization by using:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Identity</span></dt><dd>
									The user name and list of groups that the user belongs to.
								</dd><dt><span class="term">Action</span></dt><dd><p class="simpara">
									The action you perform. In most cases, this consists of:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<span class="strong strong"><strong>Project</strong></span>: The project you access. A project is a Kubernetes namespace with additional annotations that allows a community of users to organize and manage their content in isolation from other communities.
										</li><li class="listitem">
											<span class="strong strong"><strong>Verb</strong></span> : The action itself: <code class="literal">get</code>, <code class="literal">list</code>, <code class="literal">create</code>, <code class="literal">update</code>, <code class="literal">delete</code>, <code class="literal">deletecollection</code>, or <code class="literal">watch</code>.
										</li><li class="listitem">
											<span class="strong strong"><strong>Resource name</strong></span>: The API endpoint that you access.
										</li></ul></div></dd><dt><span class="term">Bindings</span></dt><dd>
									The full list of bindings, the associations between users or groups with a role.
								</dd></dl></div><p>
						OpenShift Container Platform evaluates authorization by using the following steps:
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								The identity and the project-scoped action is used to find all bindings that apply to the user or their groups.
							</li><li class="listitem">
								Bindings are used to locate all the roles that apply.
							</li><li class="listitem">
								Roles are used to find all the rules that apply.
							</li><li class="listitem">
								The action is checked against each rule to find a match.
							</li><li class="listitem">
								If no matching rule is found, the action is then denied by default.
							</li></ol></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
						Remember that users and groups can be associated with, or bound to, multiple roles at the same time.
					</p></div></div><p>
						Project administrators can use the CLI to view local roles and bindings, including a matrix of the verbs and resources each are associated with.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							The cluster role bound to the project administrator is limited in a project through a local binding. It is not bound cluster-wide like the cluster roles granted to the <span class="strong strong"><strong>cluster-admin</strong></span> or <span class="strong strong"><strong>system:admin</strong></span>.
						</p><p>
							Cluster roles are roles defined at the cluster level but can be bound either at the cluster level or at the project level.
						</p></div></div><section class="section" id="cluster-role-aggregations_post-install-preparing-for-users"><div class="titlepage"><div><div><h5 class="title">11.2.1.2.1. Cluster role aggregation</h5></div></div></div><p>
							The default admin, edit, view, and cluster-reader cluster roles support <a class="link" href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles">cluster role aggregation</a>, where the cluster rules for each role are dynamically updated as new rules are created. This feature is relevant only if you extend the Kubernetes API by creating custom resources.
						</p></section></section></section><section class="section" id="rbac-projects-namespaces_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.2.2. Projects and namespaces</h3></div></div></div><p>
					A Kubernetes <span class="emphasis"><em>namespace</em></span> provides a mechanism to scope resources in a cluster. The <a class="link" href="https://kubernetes.io/docs/tasks/administer-cluster/namespaces/">Kubernetes documentation</a> has more information on namespaces.
				</p><p>
					Namespaces provide a unique scope for:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Named resources to avoid basic naming collisions.
						</li><li class="listitem">
							Delegated management authority to trusted users.
						</li><li class="listitem">
							The ability to limit community resource consumption.
						</li></ul></div><p>
					Most objects in the system are scoped by namespace, but some are excepted and have no namespace, including nodes and users.
				</p><p>
					A <span class="emphasis"><em>project</em></span> is a Kubernetes namespace with additional annotations and is the central vehicle by which access to resources for regular users is managed. A project allows a community of users to organize and manage their content in isolation from other communities. Users must be given access to projects by administrators, or if allowed to create projects, automatically have access to their own projects.
				</p><p>
					Projects can have a separate <code class="literal">name</code>, <code class="literal">displayName</code>, and <code class="literal">description</code>.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The mandatory <code class="literal">name</code> is a unique identifier for the project and is most visible when using the CLI tools or API. The maximum name length is 63 characters.
						</li><li class="listitem">
							The optional <code class="literal">displayName</code> is how the project is displayed in the web console (defaults to <code class="literal">name</code>).
						</li><li class="listitem">
							The optional <code class="literal">description</code> can be a more detailed description of the project and is also visible in the web console.
						</li></ul></div><p>
					Each project scopes its own set of:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031632182592" scope="col">Object</th><th align="left" valign="top" id="idm140031632181504" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031632182592"> <p>
									<code class="literal">Objects</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031632181504"> <p>
									Pods, services, replication controllers, etc.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031632182592"> <p>
									<code class="literal">Policies</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031632181504"> <p>
									Rules for which users can or cannot perform actions on objects.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031632182592"> <p>
									<code class="literal">Constraints</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031632181504"> <p>
									Quotas for each kind of object that can be limited.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031632182592"> <p>
									<code class="literal">Service accounts</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031632181504"> <p>
									Service accounts act automatically with designated access to objects in the project.
								</p>
								 </td></tr></tbody></table></div><p>
					Cluster administrators can create projects and delegate administrative rights for the project to any member of the user community. Cluster administrators can also allow developers to create their own projects.
				</p><p>
					Developers and administrators can interact with projects by using the CLI or the web console.
				</p></section><section class="section" id="rbac-default-projects_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.2.3. Default projects</h3></div></div></div><p>
					OpenShift Container Platform comes with a number of default projects, and projects starting with <code class="literal">openshift-</code> are the most essential to users. These projects host master components that run as pods and other infrastructure components. The pods created in these namespaces that have a <a class="link" href="https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#rescheduler-guaranteed-scheduling-of-critical-add-ons">critical pod annotation</a> are considered critical, and the have guaranteed admission by kubelet. Pods created for master components in these namespaces are already marked as critical.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You cannot assign an SCC to pods created in one of the default namespaces: <code class="literal">default</code>, <code class="literal">kube-system</code>, <code class="literal">kube-public</code>, <code class="literal">openshift-node</code>, <code class="literal">openshift-infra</code>, and <code class="literal">openshift</code>. You cannot use these namespaces for running pods or services.
					</p></div></div></section><section class="section" id="viewing-cluster-roles_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.2.4. Viewing cluster roles and bindings</h3></div></div></div><p>
					You can use the <code class="literal">oc</code> CLI to view cluster roles and bindings by using the <code class="literal">oc describe</code> command.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the <code class="literal">oc</code> CLI.
						</li><li class="listitem">
							Obtain permission to view the cluster roles and bindings.
						</li></ul></div><p>
					Users with the <code class="literal">cluster-admin</code> default cluster role bound cluster-wide can perform any action on any resource, including viewing cluster roles and bindings.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To view the cluster roles and their associated rule sets:
						</p><pre class="programlisting language-terminal">$ oc describe clusterrole.rbac</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Name:         admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                                  Non-Resource URLs  Resource Names  Verbs
  ---------                                                  -----------------  --------------  -----
  .packages.apps.redhat.com                                  []                 []              [* create update patch delete get list watch]
  imagestreams                                               []                 []              [create delete deletecollection get list patch update watch create get list watch]
  imagestreams.image.openshift.io                            []                 []              [create delete deletecollection get list patch update watch create get list watch]
  secrets                                                    []                 []              [create delete deletecollection get list patch update watch get list watch create delete deletecollection patch update]
  buildconfigs/webhooks                                      []                 []              [create delete deletecollection get list patch update watch get list watch]
  buildconfigs                                               []                 []              [create delete deletecollection get list patch update watch get list watch]
  buildlogs                                                  []                 []              [create delete deletecollection get list patch update watch get list watch]
  deploymentconfigs/scale                                    []                 []              [create delete deletecollection get list patch update watch get list watch]
  deploymentconfigs                                          []                 []              [create delete deletecollection get list patch update watch get list watch]
  imagestreamimages                                          []                 []              [create delete deletecollection get list patch update watch get list watch]
  imagestreammappings                                        []                 []              [create delete deletecollection get list patch update watch get list watch]
  imagestreamtags                                            []                 []              [create delete deletecollection get list patch update watch get list watch]
  processedtemplates                                         []                 []              [create delete deletecollection get list patch update watch get list watch]
  routes                                                     []                 []              [create delete deletecollection get list patch update watch get list watch]
  templateconfigs                                            []                 []              [create delete deletecollection get list patch update watch get list watch]
  templateinstances                                          []                 []              [create delete deletecollection get list patch update watch get list watch]
  templates                                                  []                 []              [create delete deletecollection get list patch update watch get list watch]
  deploymentconfigs.apps.openshift.io/scale                  []                 []              [create delete deletecollection get list patch update watch get list watch]
  deploymentconfigs.apps.openshift.io                        []                 []              [create delete deletecollection get list patch update watch get list watch]
  buildconfigs.build.openshift.io/webhooks                   []                 []              [create delete deletecollection get list patch update watch get list watch]
  buildconfigs.build.openshift.io                            []                 []              [create delete deletecollection get list patch update watch get list watch]
  buildlogs.build.openshift.io                               []                 []              [create delete deletecollection get list patch update watch get list watch]
  imagestreamimages.image.openshift.io                       []                 []              [create delete deletecollection get list patch update watch get list watch]
  imagestreammappings.image.openshift.io                     []                 []              [create delete deletecollection get list patch update watch get list watch]
  imagestreamtags.image.openshift.io                         []                 []              [create delete deletecollection get list patch update watch get list watch]
  routes.route.openshift.io                                  []                 []              [create delete deletecollection get list patch update watch get list watch]
  processedtemplates.template.openshift.io                   []                 []              [create delete deletecollection get list patch update watch get list watch]
  templateconfigs.template.openshift.io                      []                 []              [create delete deletecollection get list patch update watch get list watch]
  templateinstances.template.openshift.io                    []                 []              [create delete deletecollection get list patch update watch get list watch]
  templates.template.openshift.io                            []                 []              [create delete deletecollection get list patch update watch get list watch]
  serviceaccounts                                            []                 []              [create delete deletecollection get list patch update watch impersonate create delete deletecollection patch update get list watch]
  imagestreams/secrets                                       []                 []              [create delete deletecollection get list patch update watch]
  rolebindings                                               []                 []              [create delete deletecollection get list patch update watch]
  roles                                                      []                 []              [create delete deletecollection get list patch update watch]
  rolebindings.authorization.openshift.io                    []                 []              [create delete deletecollection get list patch update watch]
  roles.authorization.openshift.io                           []                 []              [create delete deletecollection get list patch update watch]
  imagestreams.image.openshift.io/secrets                    []                 []              [create delete deletecollection get list patch update watch]
  rolebindings.rbac.authorization.k8s.io                     []                 []              [create delete deletecollection get list patch update watch]
  roles.rbac.authorization.k8s.io                            []                 []              [create delete deletecollection get list patch update watch]
  networkpolicies.extensions                                 []                 []              [create delete deletecollection patch update create delete deletecollection get list patch update watch get list watch]
  networkpolicies.networking.k8s.io                          []                 []              [create delete deletecollection patch update create delete deletecollection get list patch update watch get list watch]
  configmaps                                                 []                 []              [create delete deletecollection patch update get list watch]
  endpoints                                                  []                 []              [create delete deletecollection patch update get list watch]
  persistentvolumeclaims                                     []                 []              [create delete deletecollection patch update get list watch]
  pods                                                       []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers/scale                               []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers                                     []                 []              [create delete deletecollection patch update get list watch]
  services                                                   []                 []              [create delete deletecollection patch update get list watch]
  daemonsets.apps                                            []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps/scale                                     []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps                                           []                 []              [create delete deletecollection patch update get list watch]
  replicasets.apps/scale                                     []                 []              [create delete deletecollection patch update get list watch]
  replicasets.apps                                           []                 []              [create delete deletecollection patch update get list watch]
  statefulsets.apps/scale                                    []                 []              [create delete deletecollection patch update get list watch]
  statefulsets.apps                                          []                 []              [create delete deletecollection patch update get list watch]
  horizontalpodautoscalers.autoscaling                       []                 []              [create delete deletecollection patch update get list watch]
  cronjobs.batch                                             []                 []              [create delete deletecollection patch update get list watch]
  jobs.batch                                                 []                 []              [create delete deletecollection patch update get list watch]
  daemonsets.extensions                                      []                 []              [create delete deletecollection patch update get list watch]
  deployments.extensions/scale                               []                 []              [create delete deletecollection patch update get list watch]
  deployments.extensions                                     []                 []              [create delete deletecollection patch update get list watch]
  ingresses.extensions                                       []                 []              [create delete deletecollection patch update get list watch]
  replicasets.extensions/scale                               []                 []              [create delete deletecollection patch update get list watch]
  replicasets.extensions                                     []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers.extensions/scale                    []                 []              [create delete deletecollection patch update get list watch]
  poddisruptionbudgets.policy                                []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps/rollback                                  []                 []              [create delete deletecollection patch update]
  deployments.extensions/rollback                            []                 []              [create delete deletecollection patch update]
  catalogsources.operators.coreos.com                        []                 []              [create update patch delete get list watch]
  clusterserviceversions.operators.coreos.com                []                 []              [create update patch delete get list watch]
  installplans.operators.coreos.com                          []                 []              [create update patch delete get list watch]
  packagemanifests.operators.coreos.com                      []                 []              [create update patch delete get list watch]
  subscriptions.operators.coreos.com                         []                 []              [create update patch delete get list watch]
  buildconfigs/instantiate                                   []                 []              [create]
  buildconfigs/instantiatebinary                             []                 []              [create]
  builds/clone                                               []                 []              [create]
  deploymentconfigrollbacks                                  []                 []              [create]
  deploymentconfigs/instantiate                              []                 []              [create]
  deploymentconfigs/rollback                                 []                 []              [create]
  imagestreamimports                                         []                 []              [create]
  localresourceaccessreviews                                 []                 []              [create]
  localsubjectaccessreviews                                  []                 []              [create]
  podsecuritypolicyreviews                                   []                 []              [create]
  podsecuritypolicyselfsubjectreviews                        []                 []              [create]
  podsecuritypolicysubjectreviews                            []                 []              [create]
  resourceaccessreviews                                      []                 []              [create]
  routes/custom-host                                         []                 []              [create]
  subjectaccessreviews                                       []                 []              [create]
  subjectrulesreviews                                        []                 []              [create]
  deploymentconfigrollbacks.apps.openshift.io                []                 []              [create]
  deploymentconfigs.apps.openshift.io/instantiate            []                 []              [create]
  deploymentconfigs.apps.openshift.io/rollback               []                 []              [create]
  localsubjectaccessreviews.authorization.k8s.io             []                 []              [create]
  localresourceaccessreviews.authorization.openshift.io      []                 []              [create]
  localsubjectaccessreviews.authorization.openshift.io       []                 []              [create]
  resourceaccessreviews.authorization.openshift.io           []                 []              [create]
  subjectaccessreviews.authorization.openshift.io            []                 []              [create]
  subjectrulesreviews.authorization.openshift.io             []                 []              [create]
  buildconfigs.build.openshift.io/instantiate                []                 []              [create]
  buildconfigs.build.openshift.io/instantiatebinary          []                 []              [create]
  builds.build.openshift.io/clone                            []                 []              [create]
  imagestreamimports.image.openshift.io                      []                 []              [create]
  routes.route.openshift.io/custom-host                      []                 []              [create]
  podsecuritypolicyreviews.security.openshift.io             []                 []              [create]
  podsecuritypolicyselfsubjectreviews.security.openshift.io  []                 []              [create]
  podsecuritypolicysubjectreviews.security.openshift.io      []                 []              [create]
  jenkins.build.openshift.io                                 []                 []              [edit view view admin edit view]
  builds                                                     []                 []              [get create delete deletecollection get list patch update watch get list watch]
  builds.build.openshift.io                                  []                 []              [get create delete deletecollection get list patch update watch get list watch]
  projects                                                   []                 []              [get delete get delete get patch update]
  projects.project.openshift.io                              []                 []              [get delete get delete get patch update]
  namespaces                                                 []                 []              [get get list watch]
  pods/attach                                                []                 []              [get list watch create delete deletecollection patch update]
  pods/exec                                                  []                 []              [get list watch create delete deletecollection patch update]
  pods/portforward                                           []                 []              [get list watch create delete deletecollection patch update]
  pods/proxy                                                 []                 []              [get list watch create delete deletecollection patch update]
  services/proxy                                             []                 []              [get list watch create delete deletecollection patch update]
  routes/status                                              []                 []              [get list watch update]
  routes.route.openshift.io/status                           []                 []              [get list watch update]
  appliedclusterresourcequotas                               []                 []              [get list watch]
  bindings                                                   []                 []              [get list watch]
  builds/log                                                 []                 []              [get list watch]
  deploymentconfigs/log                                      []                 []              [get list watch]
  deploymentconfigs/status                                   []                 []              [get list watch]
  events                                                     []                 []              [get list watch]
  imagestreams/status                                        []                 []              [get list watch]
  limitranges                                                []                 []              [get list watch]
  namespaces/status                                          []                 []              [get list watch]
  pods/log                                                   []                 []              [get list watch]
  pods/status                                                []                 []              [get list watch]
  replicationcontrollers/status                              []                 []              [get list watch]
  resourcequotas/status                                      []                 []              [get list watch]
  resourcequotas                                             []                 []              [get list watch]
  resourcequotausages                                        []                 []              [get list watch]
  rolebindingrestrictions                                    []                 []              [get list watch]
  deploymentconfigs.apps.openshift.io/log                    []                 []              [get list watch]
  deploymentconfigs.apps.openshift.io/status                 []                 []              [get list watch]
  controllerrevisions.apps                                   []                 []              [get list watch]
  rolebindingrestrictions.authorization.openshift.io         []                 []              [get list watch]
  builds.build.openshift.io/log                              []                 []              [get list watch]
  imagestreams.image.openshift.io/status                     []                 []              [get list watch]
  appliedclusterresourcequotas.quota.openshift.io            []                 []              [get list watch]
  imagestreams/layers                                        []                 []              [get update get]
  imagestreams.image.openshift.io/layers                     []                 []              [get update get]
  builds/details                                             []                 []              [update]
  builds.build.openshift.io/details                          []                 []              [update]


Name:         basic-user
Labels:       &lt;none&gt;
Annotations:  openshift.io/description: A user that can get basic information about projects.
	              rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
	Resources                                           Non-Resource URLs  Resource Names  Verbs
	  ---------                                           -----------------  --------------  -----
	  selfsubjectrulesreviews                             []                 []              [create]
	  selfsubjectaccessreviews.authorization.k8s.io       []                 []              [create]
	  selfsubjectrulesreviews.authorization.openshift.io  []                 []              [create]
	  clusterroles.rbac.authorization.k8s.io              []                 []              [get list watch]
	  clusterroles                                        []                 []              [get list]
	  clusterroles.authorization.openshift.io             []                 []              [get list]
	  storageclasses.storage.k8s.io                       []                 []              [get list]
	  users                                               []                 [~]             [get]
	  users.user.openshift.io                             []                 [~]             [get]
	  projects                                            []                 []              [list watch]
	  projects.project.openshift.io                       []                 []              [list watch]
	  projectrequests                                     []                 []              [list]
	  projectrequests.project.openshift.io                []                 []              [list]

Name:         cluster-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
Resources  Non-Resource URLs  Resource Names  Verbs
---------  -----------------  --------------  -----
*.*        []                 []              [*]
           [*]                []              [*]

...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To view the current set of cluster role bindings, which shows the users and groups that are bound to various roles:
						</p><pre class="programlisting language-terminal">$ oc describe clusterrolebinding.rbac</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Name:         alertmanager-main
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Role:
  Kind:  ClusterRole
  Name:  alertmanager-main
Subjects:
  Kind            Name               Namespace
  ----            ----               ---------
  ServiceAccount  alertmanager-main  openshift-monitoring


Name:         basic-users
Labels:       &lt;none&gt;
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
Role:
  Kind:  ClusterRole
  Name:  basic-user
Subjects:
  Kind   Name                  Namespace
  ----   ----                  ---------
  Group  system:authenticated


Name:         cloud-credential-operator-rolebinding
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Role:
  Kind:  ClusterRole
  Name:  cloud-credential-operator-role
Subjects:
  Kind            Name     Namespace
  ----            ----     ---------
  ServiceAccount  default  openshift-cloud-credential-operator


Name:         cluster-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
Role:
  Kind:  ClusterRole
  Name:  cluster-admin
Subjects:
  Kind   Name            Namespace
  ----   ----            ---------
  Group  system:masters


Name:         cluster-admins
Labels:       &lt;none&gt;
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
Role:
  Kind:  ClusterRole
  Name:  cluster-admin
Subjects:
  Kind   Name                   Namespace
  ----   ----                   ---------
  Group  system:cluster-admins
  User   system:admin


Name:         cluster-api-manager-rolebinding
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Role:
  Kind:  ClusterRole
  Name:  cluster-api-manager-role
Subjects:
  Kind            Name     Namespace
  ----            ----     ---------
  ServiceAccount  default  openshift-machine-api

...</pre>

							</p></div></li></ol></div></section><section class="section" id="viewing-local-roles_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.2.5. Viewing local roles and bindings</h3></div></div></div><p>
					You can use the <code class="literal">oc</code> CLI to view local roles and bindings by using the <code class="literal">oc describe</code> command.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the <code class="literal">oc</code> CLI.
						</li><li class="listitem"><p class="simpara">
							Obtain permission to view the local roles and bindings:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									Users with the <code class="literal">cluster-admin</code> default cluster role bound cluster-wide can perform any action on any resource, including viewing local roles and bindings.
								</li><li class="listitem">
									Users with the <code class="literal">admin</code> default cluster role bound locally can view and manage roles and bindings in that project.
								</li></ul></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To view the current set of local role bindings, which show the users and groups that are bound to various roles for the current project:
						</p><pre class="programlisting language-terminal">$ oc describe rolebinding.rbac</pre></li><li class="listitem"><p class="simpara">
							To view the local role bindings for a different project, add the <code class="literal">-n</code> flag to the command:
						</p><pre class="programlisting language-terminal">$ oc describe rolebinding.rbac -n joe-project</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Name:         admin
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Role:
  Kind:  ClusterRole
  Name:  admin
Subjects:
  Kind  Name        Namespace
  ----  ----        ---------
  User  kube:admin


Name:         system:deployers
Labels:       &lt;none&gt;
Annotations:  openshift.io/description:
                Allows deploymentconfigs in this namespace to rollout pods in
                this namespace.  It is auto-managed by a controller; remove
                subjects to disa...
Role:
  Kind:  ClusterRole
  Name:  system:deployer
Subjects:
  Kind            Name      Namespace
  ----            ----      ---------
  ServiceAccount  deployer  joe-project


Name:         system:image-builders
Labels:       &lt;none&gt;
Annotations:  openshift.io/description:
                Allows builds in this namespace to push images to this
                namespace.  It is auto-managed by a controller; remove subjects
                to disable.
Role:
  Kind:  ClusterRole
  Name:  system:image-builder
Subjects:
  Kind            Name     Namespace
  ----            ----     ---------
  ServiceAccount  builder  joe-project


Name:         system:image-pullers
Labels:       &lt;none&gt;
Annotations:  openshift.io/description:
                Allows all pods in this namespace to pull images from this
                namespace.  It is auto-managed by a controller; remove subjects
                to disable.
Role:
  Kind:  ClusterRole
  Name:  system:image-puller
Subjects:
  Kind   Name                                Namespace
  ----   ----                                ---------
  Group  system:serviceaccounts:joe-project</pre>

							</p></div></li></ol></div></section><section class="section" id="adding-roles_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.2.6. Adding roles to users</h3></div></div></div><p>
					You can use the <code class="literal">oc adm</code> administrator CLI to manage the roles and bindings.
				</p><p>
					Binding, or adding, a role to users or groups gives the user or group the access that is granted by the role. You can add and remove roles to and from users and groups using <code class="literal">oc adm policy</code> commands.
				</p><p>
					You can bind any of the default cluster roles to local users or groups in your project.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add a role to a user in a specific project:
						</p><pre class="programlisting language-terminal">$ oc adm policy add-role-to-user &lt;role&gt; &lt;user&gt; -n &lt;project&gt;</pre><p class="simpara">
							For example, you can add the <code class="literal">admin</code> role to the <code class="literal">alice</code> user in <code class="literal">joe</code> project by running:
						</p><pre class="programlisting language-terminal">$ oc adm policy add-role-to-user admin alice -n joe</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can alternatively apply the following YAML to add the role to the user:
						</p><pre class="programlisting language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: admin-0
  namespace: joe
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: alice</pre></div></div></li><li class="listitem"><p class="simpara">
							View the local role bindings and verify the addition in the output:
						</p><pre class="programlisting language-terminal">$ oc describe rolebinding.rbac -n &lt;project&gt;</pre><p class="simpara">
							For example, to view the local role bindings for the <code class="literal">joe</code> project:
						</p><pre class="programlisting language-terminal">$ oc describe rolebinding.rbac -n joe</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Name:         admin
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Role:
  Kind:  ClusterRole
  Name:  admin
Subjects:
  Kind  Name        Namespace
  ----  ----        ---------
  User  kube:admin


Name:         admin-0
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Role:
  Kind:  ClusterRole
  Name:  admin
Subjects:
  Kind  Name   Namespace
  ----  ----   ---------
  User  alice <span id="CO144-1"><!--Empty--></span><span class="callout">1</span>


Name:         system:deployers
Labels:       &lt;none&gt;
Annotations:  openshift.io/description:
                Allows deploymentconfigs in this namespace to rollout pods in
                this namespace.  It is auto-managed by a controller; remove
                subjects to disa...
Role:
  Kind:  ClusterRole
  Name:  system:deployer
Subjects:
  Kind            Name      Namespace
  ----            ----      ---------
  ServiceAccount  deployer  joe


Name:         system:image-builders
Labels:       &lt;none&gt;
Annotations:  openshift.io/description:
                Allows builds in this namespace to push images to this
                namespace.  It is auto-managed by a controller; remove subjects
                to disable.
Role:
  Kind:  ClusterRole
  Name:  system:image-builder
Subjects:
  Kind            Name     Namespace
  ----            ----     ---------
  ServiceAccount  builder  joe


Name:         system:image-pullers
Labels:       &lt;none&gt;
Annotations:  openshift.io/description:
                Allows all pods in this namespace to pull images from this
                namespace.  It is auto-managed by a controller; remove subjects
                to disable.
Role:
  Kind:  ClusterRole
  Name:  system:image-puller
Subjects:
  Kind   Name                                Namespace
  ----   ----                                ---------
  Group  system:serviceaccounts:joe</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO144-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal">alice</code> user has been added to the <code class="literal">admins</code> <code class="literal">RoleBinding</code>.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="creating-local-role_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.2.7. Creating a local role</h3></div></div></div><p>
					You can create a local role for a project and then bind it to a user.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To create a local role for a project, run the following command:
						</p><pre class="programlisting language-terminal">$ oc create role &lt;name&gt; --verb=&lt;verb&gt; --resource=&lt;resource&gt; -n &lt;project&gt;</pre><p class="simpara">
							In this command, specify:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">&lt;name&gt;</code>, the local role’s name
								</li><li class="listitem">
									<code class="literal">&lt;verb&gt;</code>, a comma-separated list of the verbs to apply to the role
								</li><li class="listitem">
									<code class="literal">&lt;resource&gt;</code>, the resources that the role applies to
								</li><li class="listitem">
									<code class="literal">&lt;project&gt;</code>, the project name
								</li></ul></div><p class="simpara">
							For example, to create a local role that allows a user to view pods in the <code class="literal">blue</code> project, run the following command:
						</p><pre class="programlisting language-terminal">$ oc create role podview --verb=get --resource=pod -n blue</pre></li><li class="listitem"><p class="simpara">
							To bind the new role to a user, run the following command:
						</p><pre class="programlisting language-terminal">$ oc adm policy add-role-to-user podview user2 --role-namespace=blue -n blue</pre></li></ol></div></section><section class="section" id="creating-cluster-role_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.2.8. Creating a cluster role</h3></div></div></div><p>
					You can create a cluster role.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To create a cluster role, run the following command:
						</p><pre class="programlisting language-terminal">$ oc create clusterrole &lt;name&gt; --verb=&lt;verb&gt; --resource=&lt;resource&gt;</pre><p class="simpara">
							In this command, specify:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">&lt;name&gt;</code>, the local role’s name
								</li><li class="listitem">
									<code class="literal">&lt;verb&gt;</code>, a comma-separated list of the verbs to apply to the role
								</li><li class="listitem">
									<code class="literal">&lt;resource&gt;</code>, the resources that the role applies to
								</li></ul></div><p class="simpara">
							For example, to create a cluster role that allows a user to view pods, run the following command:
						</p><pre class="programlisting language-terminal">$ oc create clusterrole podviewonly --verb=get --resource=pod</pre></li></ol></div></section><section class="section" id="local-role-binding-commands_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.2.9. Local role binding commands</h3></div></div></div><p>
					When you manage a user or group’s associated roles for local role bindings using the following operations, a project may be specified with the <code class="literal">-n</code> flag. If it is not specified, then the current project is used.
				</p><p>
					You can use the following commands for local RBAC management.
				</p><div class="table" id="idm140031635425008"><p class="title"><strong>Table 11.1. Local role binding operations</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031635420160" scope="col">Command</th><th align="left" valign="top" id="idm140031635419072" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031635420160"> <p>
									<code class="literal">$ oc adm policy who-can <span class="emphasis"><em>&lt;verb&gt;</em></span> <span class="emphasis"><em>&lt;resource&gt;</em></span></code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031635419072"> <p>
									Indicates which users can perform an action on a resource.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635420160"> <p>
									<code class="literal">$ oc adm policy add-role-to-user <span class="emphasis"><em>&lt;role&gt;</em></span> <span class="emphasis"><em>&lt;username&gt;</em></span></code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031635419072"> <p>
									Binds a specified role to specified users in the current project.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635420160"> <p>
									<code class="literal">$ oc adm policy remove-role-from-user <span class="emphasis"><em>&lt;role&gt;</em></span> <span class="emphasis"><em>&lt;username&gt;</em></span></code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031635419072"> <p>
									Removes a given role from specified users in the current project.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635420160"> <p>
									<code class="literal">$ oc adm policy remove-user <span class="emphasis"><em>&lt;username&gt;</em></span></code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031635419072"> <p>
									Removes specified users and all of their roles in the current project.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635420160"> <p>
									<code class="literal">$ oc adm policy add-role-to-group <span class="emphasis"><em>&lt;role&gt;</em></span> <span class="emphasis"><em>&lt;groupname&gt;</em></span></code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031635419072"> <p>
									Binds a given role to specified groups in the current project.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635420160"> <p>
									<code class="literal">$ oc adm policy remove-role-from-group <span class="emphasis"><em>&lt;role&gt;</em></span> <span class="emphasis"><em>&lt;groupname&gt;</em></span></code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031635419072"> <p>
									Removes a given role from specified groups in the current project.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635420160"> <p>
									<code class="literal">$ oc adm policy remove-group <span class="emphasis"><em>&lt;groupname&gt;</em></span></code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031635419072"> <p>
									Removes specified groups and all of their roles in the current project.
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section" id="cluster-role-binding-commands_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.2.10. Cluster role binding commands</h3></div></div></div><p>
					You can also manage cluster role bindings using the following operations. The <code class="literal">-n</code> flag is not used for these operations because cluster role bindings use non-namespaced resources.
				</p><div class="table" id="idm140031635380160"><p class="title"><strong>Table 11.2. Cluster role binding operations</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031635375312" scope="col">Command</th><th align="left" valign="top" id="idm140031635374224" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031635375312"> <p>
									<code class="literal">$ oc adm policy add-cluster-role-to-user <span class="emphasis"><em>&lt;role&gt;</em></span> <span class="emphasis"><em>&lt;username&gt;</em></span></code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031635374224"> <p>
									Binds a given role to specified users for all projects in the cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635375312"> <p>
									<code class="literal">$ oc adm policy remove-cluster-role-from-user <span class="emphasis"><em>&lt;role&gt;</em></span> <span class="emphasis"><em>&lt;username&gt;</em></span></code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031635374224"> <p>
									Removes a given role from specified users for all projects in the cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635375312"> <p>
									<code class="literal">$ oc adm policy add-cluster-role-to-group <span class="emphasis"><em>&lt;role&gt;</em></span> <span class="emphasis"><em>&lt;groupname&gt;</em></span></code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031635374224"> <p>
									Binds a given role to specified groups for all projects in the cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031635375312"> <p>
									<code class="literal">$ oc adm policy remove-cluster-role-from-group <span class="emphasis"><em>&lt;role&gt;</em></span> <span class="emphasis"><em>&lt;groupname&gt;</em></span></code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031635374224"> <p>
									Removes a given role from specified groups for all projects in the cluster.
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section" id="creating-cluster-admin_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.2.11. Creating a cluster admin</h3></div></div></div><p>
					The <code class="literal">cluster-admin</code> role is required to perform administrator level tasks on the OpenShift Container Platform cluster, such as modifying cluster resources.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You must have created a user to define as the cluster admin.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Define the user as a cluster admin:
						</p><pre class="programlisting language-terminal">$ oc adm policy add-cluster-role-to-user cluster-admin &lt;user&gt;</pre></li></ul></div></section></section><section class="section" id="understanding-kubeadmin_post-install-preparing-for-users"><div class="titlepage"><div><div><h2 class="title">11.3. The kubeadmin user</h2></div></div></div><p>
				OpenShift Container Platform creates a cluster administrator, <code class="literal">kubeadmin</code>, after the installation process completes.
			</p><p>
				This user has the <code class="literal">cluster-admin</code> role automatically applied and is treated as the root user for the cluster. The password is dynamically generated and unique to your OpenShift Container Platform environment. After installation completes the password is provided in the installation program’s output. For example:
			</p><pre class="programlisting language-terminal">INFO Install complete!
INFO Run 'export KUBECONFIG=&lt;your working directory&gt;/auth/kubeconfig' to manage the cluster with 'oc', the OpenShift CLI.
INFO The cluster is ready when 'oc login -u kubeadmin -p &lt;provided&gt;' succeeds (wait a few minutes).
INFO Access the OpenShift web-console here: https://console-openshift-console.apps.demo1.openshift4-beta-abcorp.com
INFO Login to the console with user: kubeadmin, password: &lt;provided&gt;</pre><section class="section" id="removing-kubeadmin_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.3.1. Removing the kubeadmin user</h3></div></div></div><p>
					After you define an identity provider and create a new <code class="literal">cluster-admin</code> user, you can remove the <code class="literal">kubeadmin</code> to improve cluster security.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						If you follow this procedure before another user is a <code class="literal">cluster-admin</code>, then OpenShift Container Platform must be reinstalled. It is not possible to undo this command.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You must have configured at least one identity provider.
						</li><li class="listitem">
							You must have added the <code class="literal">cluster-admin</code> role to a user.
						</li><li class="listitem">
							You must be logged in as an administrator.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Remove the <code class="literal">kubeadmin</code> secrets:
						</p><pre class="programlisting language-terminal">$ oc delete secrets kubeadmin -n kube-system</pre></li></ul></div></section></section><section class="section" id="post-install-image-configuration-resources"><div class="titlepage"><div><div><h2 class="title">11.4. Image configuration</h2></div></div></div><p>
				Understand and configure image registry settings.
			</p><section class="section" id="images-configuration-parameters_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.4.1. Image controller configuration parameters</h3></div></div></div><p>
					The <code class="literal">image.config.openshift.io/cluster</code> resource holds cluster-wide information about how to handle images. The canonical, and only valid name is <code class="literal">cluster</code>. Its <code class="literal">spec</code> offers the following configuration parameters.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Parameters such as <code class="literal">DisableScheduledImport</code>, <code class="literal">MaxImagesBulkImportedPerRepository</code>, <code class="literal">MaxScheduledImportsPerMinute</code>, <code class="literal">ScheduledImageImportMinimumIntervalSeconds</code>, <code class="literal">InternalRegistryHostname</code> are not configurable.
					</p></div></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 27%; " class="col_1"><!--Empty--></col><col style="width: 73%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031634154272" scope="col">Parameter</th><th align="left" valign="top" id="idm140031634153184" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031634154272"> <p>
									<code class="literal">allowedRegistriesForImport</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031634153184"> <p>
									Limits the container image registries from which normal users can import images. Set this list to the registries that you trust to contain valid images, and that you want applications to be able to import from. Users with permission to create images or <code class="literal">ImageStreamMappings</code> from the API are not affected by this policy. Typically only cluster administrators have the appropriate permissions.
								</p>
								 <p>
									Every element of this list contains a location of the registry specified by the registry domain name.
								</p>
								 <p>
									<code class="literal">domainName</code>: Specifies a domain name for the registry. If the registry uses a non-standard <code class="literal">80</code> or <code class="literal">443</code> port, the port should be included in the domain name as well.
								</p>
								 <p>
									<code class="literal">insecure</code>: Insecure indicates whether the registry is secure or insecure. By default, if not otherwise specified, the registry is assumed to be secure.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031634154272"> <p>
									<code class="literal">additionalTrustedCA</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031634153184"> <p>
									A reference to a config map containing additional CAs that should be trusted during <code class="literal">image stream import</code>, <code class="literal">pod image pull</code>, <code class="literal">openshift-image-registry pullthrough</code>, and builds.
								</p>
								 <p>
									The namespace for this config map is <code class="literal">openshift-config</code>. The format of the config map is to use the registry hostname as the key, and the PEM-encoded certificate as the value, for each additional registry CA to trust.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031634154272"> <p>
									<code class="literal">externalRegistryHostnames</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031634153184"> <p>
									Provides the hostnames for the default external image registry. The external hostname should be set only when the image registry is exposed externally. The first value is used in <code class="literal">publicDockerImageRepository</code> field in image streams. The value must be in <code class="literal">hostname[:port]</code> format.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031634154272"> <p>
									<code class="literal">registrySources</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031634153184"> <p>
									Contains configuration that determines how the container runtime should treat individual registries when accessing images for builds and pods. For instance, whether or not to allow insecure access. It does not contain configuration for the internal cluster registry.
								</p>
								 <p>
									<code class="literal">insecureRegistries</code>: Registries which do not have a valid TLS certificate or only support HTTP connections. To specify all subdomains, add the asterisk (<code class="literal">*</code>) wildcard character as a prefix to the domain name. For example, <code class="literal">*.example.com</code>. You can specify an individual repository within a registry. For example: <code class="literal">reg1.io/myrepo/myapp:latest</code>.
								</p>
								 <p>
									<code class="literal">blockedRegistries</code>: Registries for which image pull and push actions are denied. To specify all subdomains, add the asterisk (<code class="literal">*</code>) wildcard character as a prefix to the domain name. For example, <code class="literal">*.example.com</code>. You can specify an individual repository within a registry. For example: <code class="literal">reg1.io/myrepo/myapp:latest</code>. All other registries are allowed.
								</p>
								 <p>
									<code class="literal">allowedRegistries</code>: Registries for which image pull and push actions are allowed. To specify all subdomains, add the asterisk (<code class="literal">*</code>) wildcard character as a prefix to the domain name. For example, <code class="literal">*.example.com</code>. You can specify an individual repository within a registry. For example: <code class="literal">reg1.io/myrepo/myapp:latest</code>. All other registries are blocked.
								</p>
								 <p>
									<code class="literal">containerRuntimeSearchRegistries</code>: Registries for which image pull and push actions are allowed using image short names. All other registries are blocked.
								</p>
								 <p>
									Either <code class="literal">blockedRegistries</code> or <code class="literal">allowedRegistries</code> can be set, but not both.
								</p>
								 </td></tr></tbody></table></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						When the <code class="literal">allowedRegistries</code> parameter is defined, all registries, including <code class="literal">registry.redhat.io</code> and <code class="literal">quay.io</code> registries and the default OpenShift image registry, are blocked unless explicitly listed. When using the parameter, to prevent pod failure, add all registries including the <code class="literal">registry.redhat.io</code> and <code class="literal">quay.io</code> registries and the <code class="literal">internalRegistryHostname</code> to the <code class="literal">allowedRegistries</code> list, as they are required by payload images within your environment. For disconnected clusters, mirror registries should also be added.
					</p></div></div><p>
					The <code class="literal">status</code> field of the <code class="literal">image.config.openshift.io/cluster</code> resource holds observed values from the cluster.
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 27%; " class="col_1"><!--Empty--></col><col style="width: 73%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031634104960" scope="col">Parameter</th><th align="left" valign="top" id="idm140031634103872" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031634104960"> <p>
									<code class="literal">internalRegistryHostname</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031634103872"> <p>
									Set by the Image Registry Operator, which controls the <code class="literal">internalRegistryHostname</code>. It sets the hostname for the default OpenShift image registry. The value must be in <code class="literal">hostname[:port]</code> format. For backward compatibility, you can still use the <code class="literal">OPENSHIFT_DEFAULT_REGISTRY</code> environment variable, but this setting overrides the environment variable.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140031634104960"> <p>
									<code class="literal">externalRegistryHostnames</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140031634103872"> <p>
									Set by the Image Registry Operator, provides the external hostnames for the image registry when it is exposed externally. The first value is used in <code class="literal">publicDockerImageRepository</code> field in image streams. The values must be in <code class="literal">hostname[:port]</code> format.
								</p>
								 </td></tr></tbody></table></div></section><section class="section" id="images-configuration-file_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.4.2. Configuring image registry settings</h3></div></div></div><p>
					You can configure image registry settings by editing the <code class="literal">image.config.openshift.io/cluster</code> custom resource (CR). When changes to the registry are applied to the <code class="literal">image.config.openshift.io/cluster</code> CR, the Machine Config Operator (MCO) performs the following sequential actions:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Cordons the node
						</li><li class="listitem">
							Applies changes by restarting CRI-O
						</li><li class="listitem"><p class="simpara">
							Uncordons the node
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The MCO does not restart nodes when it detects changes.
							</p></div></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">image.config.openshift.io/cluster</code> custom resource:
						</p><pre class="programlisting language-terminal">$ oc edit image.config.openshift.io/cluster</pre><p class="simpara">
							The following is an example <code class="literal">image.config.openshift.io/cluster</code> CR:
						</p><pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Image <span id="CO145-1"><!--Empty--></span><span class="callout">1</span>
metadata:
  annotations:
    release.openshift.io/create-only: "true"
  creationTimestamp: "2019-05-17T13:44:26Z"
  generation: 1
  name: cluster
  resourceVersion: "8302"
  selfLink: /apis/config.openshift.io/v1/images/cluster
  uid: e34555da-78a9-11e9-b92b-06d6c7da38dc
spec:
  allowedRegistriesForImport: <span id="CO145-2"><!--Empty--></span><span class="callout">2</span>
    - domainName: quay.io
      insecure: false
  additionalTrustedCA: <span id="CO145-3"><!--Empty--></span><span class="callout">3</span>
    name: myconfigmap
  registrySources: <span id="CO145-4"><!--Empty--></span><span class="callout">4</span>
    allowedRegistries:
    - example.com
    - quay.io
    - registry.redhat.io
    - image-registry.openshift-image-registry.svc:5000
    - reg1.io/myrepo/myapp:latest
    insecureRegistries:
    - insecure.com
status:
  internalRegistryHostname: image-registry.openshift-image-registry.svc:5000</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO145-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal">Image</code>: Holds cluster-wide information about how to handle images. The canonical, and only valid name is <code class="literal">cluster</code>.
								</div></dd><dt><a href="#CO145-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									<code class="literal">allowedRegistriesForImport</code>: Limits the container image registries from which normal users may import images. Set this list to the registries that you trust to contain valid images, and that you want applications to be able to import from. Users with permission to create images or <code class="literal">ImageStreamMappings</code> from the API are not affected by this policy. Typically only cluster administrators have the appropriate permissions.
								</div></dd><dt><a href="#CO145-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									<code class="literal">additionalTrustedCA</code>: A reference to a config map containing additional certificate authorities (CA) that are trusted during image stream import, pod image pull, <code class="literal">openshift-image-registry</code> pullthrough, and builds. The namespace for this config map is <code class="literal">openshift-config</code>. The format of the config map is to use the registry hostname as the key, and the PEM certificate as the value, for each additional registry CA to trust.
								</div></dd><dt><a href="#CO145-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									<code class="literal">registrySources</code>: Contains configuration that determines whether the container runtime allows or blocks individual registries when accessing images for builds and pods. Either the <code class="literal">allowedRegistries</code> parameter or the <code class="literal">blockedRegistries</code> parameter can be set, but not both. You can also define whether or not to allow access to insecure registries or registries that allow registries that use image short names. This example uses the <code class="literal">allowedRegistries</code> parameter, which defines the registries that are allowed to be used. The insecure registry <code class="literal">insecure.com</code> is also allowed. The <code class="literal">registrySources</code> parameter does not contain configuration for the internal cluster registry.
								</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								When the <code class="literal">allowedRegistries</code> parameter is defined, all registries, including the registry.redhat.io and quay.io registries and the default OpenShift image registry, are blocked unless explicitly listed. If you use the parameter, to prevent pod failure, you must add the <code class="literal">registry.redhat.io</code> and <code class="literal">quay.io</code> registries and the <code class="literal">internalRegistryHostname</code> to the <code class="literal">allowedRegistries</code> list, as they are required by payload images within your environment. Do not add the <code class="literal">registry.redhat.io</code> and <code class="literal">quay.io</code> registries to the <code class="literal">blockedRegistries</code> list.
							</p><p>
								When using the <code class="literal">allowedRegistries</code>, <code class="literal">blockedRegistries</code>, or <code class="literal">insecureRegistries</code> parameter, you can specify an individual repository within a registry. For example: <code class="literal">reg1.io/myrepo/myapp:latest</code>.
							</p><p>
								Insecure external registries should be avoided to reduce possible security risks.
							</p></div></div></li><li class="listitem"><p class="simpara">
							To check that the changes are applied, list your nodes:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                         STATUS                     ROLES                  AGE   VERSION
ip-10-0-137-182.us-east-2.compute.internal   Ready,SchedulingDisabled   worker                 65m   v1.26.0
ip-10-0-139-120.us-east-2.compute.internal   Ready,SchedulingDisabled   control-plane          74m   v1.26.0
ip-10-0-176-102.us-east-2.compute.internal   Ready                      control-plane          75m   v1.26.0
ip-10-0-188-96.us-east-2.compute.internal    Ready                      worker                 65m   v1.26.0
ip-10-0-200-59.us-east-2.compute.internal    Ready                      worker                 63m   v1.26.0
ip-10-0-223-123.us-east-2.compute.internal   Ready                      control-plane          73m   v1.26.0</pre>

							</p></div></li></ol></div><p>
					For more information on the allowed, blocked, and insecure registry parameters, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/images/#images-configuration-file_image-configuration">Configuring image registry settings</a>.
				</p></section><section class="section" id="images-configuration-cas_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.4.3. Configuring additional trust stores for image registry access</h3></div></div></div><p>
					The <code class="literal">image.config.openshift.io/cluster</code> custom resource can contain a reference to a config map that contains additional certificate authorities to be trusted during image registry access.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The certificate authorities (CA) must be PEM-encoded.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						You can create a config map in the <code class="literal">openshift-config</code> namespace and use its name in <code class="literal">AdditionalTrustedCA</code> in the <code class="literal">image.config.openshift.io</code> custom resource to provide additional CAs that should be trusted when contacting external registries.
					</p></div><p>
					The config map key is the hostname of a registry with the port for which this CA is to be trusted, and the PEM certificate content is the value, for each additional registry CA to trust.
				</p><div class="formalpara"><p class="title"><strong>Image registry CA config map example</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: my-registry-ca
data:
  registry.example.com: |
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----
  registry-with-port.example.com..5000: | <span id="CO146-1"><!--Empty--></span><span class="callout">1</span>
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO146-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							If the registry has the port, such as <code class="literal">registry-with-port.example.com:5000</code>, <code class="literal">:</code> should be replaced with <code class="literal">..</code>.
						</div></dd></dl></div><p>
					You can configure additional CAs with the following procedure.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To configure an additional CA:
						</p><pre class="programlisting language-terminal">$ oc create configmap registry-config --from-file=&lt;external_registry_address&gt;=ca.crt -n openshift-config</pre><pre class="programlisting language-terminal">$ oc edit image.config.openshift.io cluster</pre><pre class="programlisting language-yaml">spec:
  additionalTrustedCA:
    name: registry-config</pre></li></ol></div></section><section class="section" id="images-configuration-registry-mirror_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.4.4. Configuring image registry repository mirroring</h3></div></div></div><p>
					Setting up container registry repository mirroring enables you to perform the following tasks:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Configure your OpenShift Container Platform cluster to redirect requests to pull images from a repository on a source image registry and have it resolved by a repository on a mirrored image registry.
						</li><li class="listitem">
							Identify multiple mirrored repositories for each target repository, to make sure that if one mirror is down, another can be used.
						</li></ul></div><p>
					Repository mirroring in OpenShift Container Platform includes the following attributes:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Image pulls are resilient to registry downtimes.
						</li><li class="listitem">
							Clusters in disconnected environments can pull images from critical locations, such as quay.io, and have registries behind a company firewall provide the requested images.
						</li><li class="listitem">
							A particular order of registries is tried when an image pull request is made, with the permanent registry typically being the last one tried.
						</li><li class="listitem">
							The mirror information you enter is added to the <code class="literal">/etc/containers/registries.conf</code> file on every node in the OpenShift Container Platform cluster.
						</li><li class="listitem">
							When a node makes a request for an image from the source repository, it tries each mirrored repository in turn until it finds the requested content. If all mirrors fail, the cluster tries the source repository. If successful, the image is pulled to the node.
						</li></ul></div><p>
					Setting up repository mirroring can be done in the following ways:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							At OpenShift Container Platform installation:
						</p><p class="simpara">
							By pulling container images needed by OpenShift Container Platform and then bringing those images behind your company’s firewall, you can install OpenShift Container Platform into a datacenter that is in a disconnected environment.
						</p></li><li class="listitem"><p class="simpara">
							After OpenShift Container Platform installation:
						</p><p class="simpara">
							If you did not configure mirroring during OpenShift Container Platform installation, you can do so post-installation by using one of the following custom resource (CR) objects:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<code class="literal">ImageDigestMirrorSet</code>. This CR allows you to pull images from a mirrored registry by using digest specifications.
								</li><li class="listitem">
									<code class="literal">ImageTagMirrorSet</code>. This CR allows you to pull images from a mirrored registry by using image tags.
								</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Using an <code class="literal">ImageContentSourcePolicy</code> (ICSP) object to configure repository mirroring is a deprecated feature. Deprecated functionality is still included in OpenShift Container Platform and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. If you have existing YAML files that you used to create <code class="literal">ImageContentSourcePolicy</code> objects, you can use the <code class="literal">oc adm migrate icsp</code> command to convert those files to an <code class="literal">ImageDigestMirrorSet</code> YAML file. For more information, see "Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring" in the following section.
							</p></div></div></li></ul></div><p>
					Both of these custom resource objects identify the following information:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The source of the container image repository you want to mirror.
						</li><li class="listitem">
							A separate entry for each mirror repository you want to offer the content requested from the source repository.
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If your cluster uses an <code class="literal">ImageDigestMirrorSet</code> or <code class="literal">ImageTagMirrorSet</code> object to configure repository mirroring, you can use only global pull secrets for mirrored registries. You cannot add a pull secret to a project.
					</p></div></div><p>
					The following procedure creates a post-installation mirror configuration, where you create an <code class="literal">ImageDigestMirrorSet</code> object.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Ensure that you have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem"><p class="simpara">
							Ensure that there are no <code class="literal">ImageContentSourcePolicy</code> objects on your cluster. For example, you can use the following command:
						</p><pre class="programlisting language-terminal">$ oc get ImageContentSourcePolicy</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">No resources found</pre>

							</p></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Configure mirrored repositories, by either:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Setting up a mirrored repository with Red Hat Quay, as described in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/manage_red_hat_quay/repo-mirroring-in-red-hat-quay">Red Hat Quay Repository Mirroring</a>. Using Red Hat Quay allows you to copy images from one repository to another and also automatically sync those repositories repeatedly over time.
								</li><li class="listitem"><p class="simpara">
									Using a tool such as <code class="literal">skopeo</code> to copy images manually from the source directory to the mirrored repository.
								</p><p class="simpara">
									For example, after installing the skopeo RPM package on a Red Hat Enterprise Linux (RHEL) 7 or RHEL 8 system, use the <code class="literal">skopeo</code> command as shown in this example:
								</p><pre class="programlisting language-terminal">$ skopeo copy \
docker://registry.access.redhat.com/ubi9/ubi-minimal:latest@sha256:5cf... \
docker://example.io/example/ubi-minimal</pre><p class="simpara">
									In this example, you have a container image registry that is named <code class="literal">example.io</code> with an image repository named <code class="literal">example</code> to which you want to copy the <code class="literal">ubi9/ubi-minimal</code> image from <code class="literal">registry.access.redhat.com</code>. After you create the registry, you can configure your OpenShift Container Platform cluster to redirect requests made of the source repository to the mirrored repository.
								</p></li></ul></div></li><li class="listitem">
							Log in to your OpenShift Container Platform cluster.
						</li><li class="listitem"><p class="simpara">
							Create an <code class="literal">ImageDigestMirrorSet</code> or <code class="literal">ImageTagMirrorSet</code> CR, as needed, replacing the source and mirrors with your own registry and repository pairs and images:
						</p><pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1 <span id="CO147-1"><!--Empty--></span><span class="callout">1</span>
kind: ImageDigestMirrorSet <span id="CO147-2"><!--Empty--></span><span class="callout">2</span>
metadata:
  name: ubi9repo
spec:
  imageDigestMirrors: <span id="CO147-3"><!--Empty--></span><span class="callout">3</span>
  - mirrors:
    - example.io/example/ubi-minimal <span id="CO147-4"><!--Empty--></span><span class="callout">4</span>
    - example.com/example/ubi-minimal <span id="CO147-5"><!--Empty--></span><span class="callout">5</span>
    source: registry.access.redhat.com/ubi9/ubi-minimal <span id="CO147-6"><!--Empty--></span><span class="callout">6</span>
    mirrorSourcePolicy: AllowContactingSource <span id="CO147-7"><!--Empty--></span><span class="callout">7</span>
  - mirrors:
    - mirror.example.com/redhat
    source: registry.redhat.io/openshift4 <span id="CO147-8"><!--Empty--></span><span class="callout">8</span>
    mirrorSourcePolicy: AllowContactingSource
  - mirrors:
    - mirror.example.com
    source: registry.redhat.io <span id="CO147-9"><!--Empty--></span><span class="callout">9</span>
    mirrorSourcePolicy: AllowContactingSource
  - mirrors:
    - mirror.example.net/image
    source: registry.example.com/example/myimage <span id="CO147-10"><!--Empty--></span><span class="callout">10</span>
    mirrorSourcePolicy: AllowContactingSource
  - mirrors:
    - mirror.example.net
    source: registry.example.com/example <span id="CO147-11"><!--Empty--></span><span class="callout">11</span>
    mirrorSourcePolicy: AllowContactingSource
  - mirrors:
    - mirror.example.net/registry-example-com
    source: registry.example.com <span id="CO147-12"><!--Empty--></span><span class="callout">12</span>
    mirrorSourcePolicy: AllowContactingSource</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO147-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Indicates the API to use with this CR. This must be <code class="literal">config.openshift.io/v1</code>.
								</div></dd><dt><a href="#CO147-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Indicates the kind of object according to the pull type:
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">ImageDigestMirrorSet</code>: Pulls a digest reference image.
										</li><li class="listitem">
											<code class="literal">ImageTagMirrorSet</code>: Pulls a tag reference image.
										</li></ul></div></dd><dt><a href="#CO147-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Indicates the type of image pull method, either:
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">imageDigestMirrors</code>: Use for an <code class="literal">ImageDigestMirrorSet</code> CR.
										</li><li class="listitem">
											<code class="literal">imageTagMirrors</code>: Use for an <code class="literal">ImageTagMirrorSet</code> CR.
										</li></ul></div></dd><dt><a href="#CO147-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Indicates the name of the mirrored image registry and repository.
								</div></dd><dt><a href="#CO147-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Optional: Indicates a secondary mirror repository for each target repository. If one mirror is down, the target repository can use another mirror.
								</div></dd><dt><a href="#CO147-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Indicates the registry and repository source, which is the repository that is referred to in image pull specifications.
								</div></dd><dt><a href="#CO147-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									Optional: Indicates the fallback policy if the image pull fails:
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">AllowContactingSource</code>: Allows continued attempts to pull the image from the source repository. This is the default.
										</li><li class="listitem">
											<code class="literal">NeverContactSource</code>: Prevents continued attempts to pull the image from the source repository.
										</li></ul></div></dd><dt><a href="#CO147-8"><span class="callout">8</span></a> </dt><dd><div class="para">
									Optional: Indicates a namespace inside a registry, which allows you to use any image in that namespace. If you use a registry domain as a source, the object is applied to all repositories from the registry.
								</div></dd><dt><a href="#CO147-9"><span class="callout">9</span></a> </dt><dd><div class="para">
									Optional: Indicates a registry, which allows you to use any image in that registry. If you specify a registry name, the object is applied to all repositories from a source registry to a mirror registry.
								</div></dd><dt><a href="#CO147-10"><span class="callout">10</span></a> </dt><dd><div class="para">
									Pulls the image <code class="literal">registry.example.com/example/myimage@sha256:…​</code> from the mirror <code class="literal">mirror.example.net/image@sha256:..</code>.
								</div></dd><dt><a href="#CO147-11"><span class="callout">11</span></a> </dt><dd><div class="para">
									Pulls the image <code class="literal">registry.example.com/example/image@sha256:…​</code> in the source registry namespace from the mirror <code class="literal">mirror.example.net/image@sha256:…​</code>.
								</div></dd><dt><a href="#CO147-12"><span class="callout">12</span></a> </dt><dd><div class="para">
									Pulls the image <code class="literal">registry.example.com/myimage@sha256</code> from the mirror registry <code class="literal">example.net/registry-example-com/myimage@sha256:…​</code>. The <code class="literal">ImageContentSourcePolicy</code> resource is applied to all repositories from a source registry to a mirror registry <code class="literal">mirror.example.net/registry-example-com</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the new object:
						</p><pre class="programlisting language-terminal">$ oc create -f registryrepomirror.yaml</pre><p class="simpara">
							After the object is created, the Machine Config Operator (MCO) cordons the nodes as the new settings are deployed to each node. The MCO restarts the nodes for an <code class="literal">ImageTagMirrorSet</code> object only. The MCO does not restart the nodes for <code class="literal">ImageDigestMirrorSet</code> objects. When the nodes are uncordoned, the cluster starts using the mirrored repository for requests to the source repository.
						</p></li><li class="listitem"><p class="simpara">
							To check that the mirrored configuration settings are applied, do the following on one of the nodes.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									List your nodes:
								</p><pre class="programlisting language-terminal">$ oc get node</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                           STATUS                     ROLES    AGE  VERSION
ip-10-0-137-44.ec2.internal    Ready                      worker   7m   v1.26.0
ip-10-0-138-148.ec2.internal   Ready                      master   11m  v1.26.0
ip-10-0-139-122.ec2.internal   Ready                      master   11m  v1.26.0
ip-10-0-147-35.ec2.internal    Ready                      worker   7m   v1.26.0
ip-10-0-153-12.ec2.internal    Ready                      worker   7m   v1.26.0
ip-10-0-154-10.ec2.internal    Ready                      master   11m  v1.26.0</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Start the debugging process to access the node:
								</p><pre class="programlisting language-terminal">$ oc debug node/ip-10-0-147-35.ec2.internal</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">Starting pod/ip-10-0-147-35ec2internal-debug ...
To use host binaries, run `chroot /host`</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Change your root directory to <code class="literal">/host</code>:
								</p><pre class="programlisting language-terminal">sh-4.2# chroot /host</pre></li><li class="listitem"><p class="simpara">
									Check the <code class="literal">/etc/containers/registries.conf</code> file to make sure the changes were made:
								</p><pre class="programlisting language-terminal">sh-4.2# cat /etc/containers/registries.conf</pre><p class="simpara">
									The following output represents a <code class="literal">registries.conf</code> file where an <code class="literal">ImageDigestMirrorSet</code> object and an <code class="literal">ImageTagMirrorSet</code> object were applied. The final two entries are marked <code class="literal">digest-only</code> and <code class="literal">tag-only</code> respectively.
								</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">unqualified-search-registries = ["registry.access.redhat.com", "docker.io"]
short-name-mode = ""

[[registry]]
  prefix = ""
  location = "registry.access.redhat.com/ubi9/ubi-minimal" <span id="CO148-1"><!--Empty--></span><span class="callout">1</span>

  [[registry.mirror]]
    location = "example.io/example/ubi-minimal" <span id="CO148-2"><!--Empty--></span><span class="callout">2</span>
    pull-from-mirror = "digest-only" <span id="CO148-3"><!--Empty--></span><span class="callout">3</span>

  [[registry.mirror]]
    location = "example.com/example/ubi-minimal"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.example.com"

  [[registry.mirror]]
    location = "mirror.example.net/registry-example-com"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.example.com/example"

  [[registry.mirror]]
    location = "mirror.example.net"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.example.com/example/myimage"

  [[registry.mirror]]
    location = "mirror.example.net/image"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.redhat.io"

  [[registry.mirror]]
    location = "mirror.example.com"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.redhat.io/openshift4"

  [[registry.mirror]]
    location = "mirror.example.com/redhat"
    pull-from-mirror = "digest-only"
[[registry]]
  prefix = ""
  location = "registry.access.redhat.com/ubi9/ubi-minimal"
  blocked = true <span id="CO148-4"><!--Empty--></span><span class="callout">4</span>

  [[registry.mirror]]
    location = "example.io/example/ubi-minimal-tag"
    pull-from-mirror = "tag-only" <span id="CO148-5"><!--Empty--></span><span class="callout">5</span></pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO148-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Indicates the repository that is referred to in a pull spec.
										</div></dd><dt><a href="#CO148-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Indicates the mirror for that repository.
										</div></dd><dt><a href="#CO148-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Indicates that the image pull from the mirror is a digest reference image.
										</div></dd><dt><a href="#CO148-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Indicates that the <code class="literal">NeverContactSource</code> parameter is set for this repository.
										</div></dd><dt><a href="#CO148-5"><span class="callout">5</span></a> </dt><dd><div class="para">
											Indicates that the image pull from the mirror is a tag reference image.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Pull an image to the node from the source and check if it is resolved by the mirror.
								</p><pre class="programlisting language-terminal">sh-4.2# podman pull --log-level=debug registry.access.redhat.com/ubi9/ubi-minimal@sha256:5cf...</pre></li></ol></div></li></ol></div><div class="formalpara"><p class="title"><strong>Troubleshooting repository mirroring</strong></p><p>
						If the repository mirroring procedure does not work as described, use the following information about how repository mirroring works to help troubleshoot the problem.
					</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The first working mirror is used to supply the pulled image.
						</li><li class="listitem">
							The main registry is only used if no other mirror works.
						</li><li class="listitem">
							From the system context, the <code class="literal">Insecure</code> flags are used as fallback.
						</li><li class="listitem">
							The format of the <code class="literal">/etc/containers/registries.conf</code> file has changed recently. It is now version 2 and in TOML format.
						</li><li class="listitem">
							You cannot add the same repository to both an <code class="literal">ImageDigestMirrorSet</code> and an <code class="literal">ImageTagMirrorSet</code> object.
						</li></ul></div></section><section class="section" id="images-configuration-registry-mirror-convert_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.4.5. Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring</h3></div></div></div><p>
					Using an <code class="literal">ImageContentSourcePolicy</code> (ICSP) object to configure repository mirroring is a deprecated feature. This functionality is still included in OpenShift Container Platform and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments.
				</p><p>
					ICSP objects are being replaced by <code class="literal">ImageDigestMirrorSet</code> and <code class="literal">ImageTagMirrorSet</code> objects to configure repository mirroring. If you have existing YAML files that you used to create <code class="literal">ImageContentSourcePolicy</code> objects, you can use the <code class="literal">oc adm migrate icsp</code> command to convert those files to an <code class="literal">ImageDigestMirrorSet</code> YAML file. The command updates the API to the current version, changes the <code class="literal">kind</code> value to <code class="literal">ImageDigestMirrorSet</code>, and changes <code class="literal">spec.repositoryDigestMirrors</code> to <code class="literal">spec.imageDigestMirrors</code>. The rest of the file is not changed.
				</p><p>
					For more information about <code class="literal">ImageDigestMirrorSet</code> or <code class="literal">ImageTagMirrorSet</code> objects, see "Configuring image registry repository mirroring" in the previous section.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Ensure that you have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Ensure that you have <code class="literal">ImageContentSourcePolicy</code> objects on your cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Use the following command to convert one or more <code class="literal">ImageContentSourcePolicy</code> YAML files to an <code class="literal">ImageDigestMirrorSet</code> YAML file:
						</p><pre class="programlisting language-terminal">$ oc adm migrate icsp &lt;file_name&gt;.yaml &lt;file_name&gt;.yaml &lt;file_name&gt;.yaml --dest-dir &lt;path_to_the_directory&gt;</pre><p class="simpara">
							where:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;file_name&gt;</code></span></dt><dd>
										Specifies the name of the source <code class="literal">ImageContentSourcePolicy</code> YAML. You can list multiple file names.
									</dd><dt><span class="term"><code class="literal">--dest-dir</code></span></dt><dd>
										Optional: Specifies a directory for the output <code class="literal">ImageDigestMirrorSet</code> YAML. If unset, the file is written to the current directory.
									</dd></dl></div><p class="simpara">
							For example, the following command converts the <code class="literal">icsp.yaml</code> and <code class="literal">icsp-2.yaml</code> file and saves the new YAML files to the <code class="literal">idms-files</code> directory.
						</p><pre class="programlisting language-terminal">$ oc adm migrate icsp icsp.yaml icsp-2.yaml --dest-dir idms-files</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">wrote ImageDigestMirrorSet to idms-files/imagedigestmirrorset_ubi8repo.5911620242173376087.yaml
wrote ImageDigestMirrorSet to idms-files/imagedigestmirrorset_ubi9repo.6456931852378115011.yaml</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create the CR object by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;path_to_the_directory&gt;/&lt;file-name&gt;.yaml</pre><p class="simpara">
							where:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;path_to_the_directory&gt;</code></span></dt><dd>
										Specifies the path to the directory, if you used the <code class="literal">--dest-dir</code> flag.
									</dd><dt><span class="term"><code class="literal">&lt;file_name&gt;</code></span></dt><dd>
										Specifies the name of the <code class="literal">ImageDigestMirrorSet</code> YAML.
									</dd></dl></div></li></ol></div></section></section><section class="section" id="post-install-mirrored-catalogs"><div class="titlepage"><div><div><h2 class="title">11.5. Populating OperatorHub from mirrored Operator catalogs</h2></div></div></div><p>
				If you mirrored Operator catalogs for use with disconnected clusters, you can populate OperatorHub with the Operators from your mirrored catalogs. You can use the generated manifests from the mirroring process to create the required <code class="literal">ImageContentSourcePolicy</code> and <code class="literal">CatalogSource</code> objects.
			</p><section class="section" id="prerequisites_post-install-mirrored-catalogs"><div class="titlepage"><div><div><h3 class="title">11.5.1. Prerequisites</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#olm-mirror-catalog_installing-mirroring-installation-images">Mirroring Operator catalogs for use with disconnected clusters</a>
						</li></ul></div></section><section class="section" id="olm-mirror-catalog-icsp_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.5.2. Creating the ImageContentSourcePolicy object</h3></div></div></div><p>
					After mirroring Operator catalog content to your mirror registry, create the required <code class="literal">ImageContentSourcePolicy</code> (ICSP) object. The ICSP object configures nodes to translate between the image references stored in Operator manifests and the mirrored registry.
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							On a host with access to the disconnected cluster, create the ICSP by running the following command to specify the <code class="literal">imageContentSourcePolicy.yaml</code> file in your manifests directory:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;path/to/manifests/dir&gt;/imageContentSourcePolicy.yaml</pre><p class="simpara">
							where <code class="literal">&lt;path/to/manifests/dir&gt;</code> is the path to the manifests directory for your mirrored content.
						</p><p class="simpara">
							You can now create a <code class="literal">CatalogSource</code> object to reference your mirrored index image and Operator content.
						</p></li></ul></div></section><section class="section" id="olm-creating-catalog-from-index_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.5.3. Adding a catalog source to a cluster</h3></div></div></div><p>
					Adding a catalog source to an OpenShift Container Platform cluster enables the discovery and installation of Operators for users. Cluster administrators can create a <code class="literal">CatalogSource</code> object that references an index image. OperatorHub uses catalog sources to populate the user interface.
				</p><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
					Alternatively, you can use the web console to manage catalog sources. From the <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Cluster Settings</strong></span> → <span class="strong strong"><strong>Configuration</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span> page, click the <span class="strong strong"><strong>Sources</strong></span> tab, where you can create, update, delete, disable, and enable individual sources.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You built and pushed an index image to a registry.
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">CatalogSource</code> object that references your index image. If you used the <code class="literal">oc adm catalog mirror</code> command to mirror your catalog to a target registry, you can use the generated <code class="literal">catalogSource.yaml</code> file in your manifests directory as a starting point.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Modify the following to your specifications and save it as a <code class="literal">catalogSource.yaml</code> file:
								</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: my-operator-catalog <span id="CO149-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-marketplace <span id="CO149-2"><!--Empty--></span><span class="callout">2</span>
spec:
  sourceType: grpc
  grpcPodConfig:
    securityContextConfig: &lt;security_mode&gt; <span id="CO149-3"><!--Empty--></span><span class="callout">3</span>
  image: &lt;registry&gt;/&lt;namespace&gt;/redhat-operator-index:v4.13 <span id="CO149-4"><!--Empty--></span><span class="callout">4</span>
  displayName: My Operator Catalog
  publisher: &lt;publisher_name&gt; <span id="CO149-5"><!--Empty--></span><span class="callout">5</span>
  updateStrategy:
    registryPoll: <span id="CO149-6"><!--Empty--></span><span class="callout">6</span>
      interval: 30m</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO149-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											If you mirrored content to local files before uploading to a registry, remove any backslash (<code class="literal">/</code>) characters from the <code class="literal">metadata.name</code> field to avoid an "invalid resource name" error when you create the object.
										</div></dd><dt><a href="#CO149-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											If you want the catalog source to be available globally to users in all namespaces, specify the <code class="literal">openshift-marketplace</code> namespace. Otherwise, you can specify a different namespace for the catalog to be scoped and available only for that namespace.
										</div></dd><dt><a href="#CO149-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Specify the value of <code class="literal">legacy</code> or <code class="literal">restricted</code>. If the field is not set, the default value is <code class="literal">legacy</code>. In a future OpenShift Container Platform release, it is planned that the default value will be <code class="literal">restricted</code>. If your catalog cannot run with <code class="literal">restricted</code> permissions, it is recommended that you manually set this field to <code class="literal">legacy</code>.
										</div></dd><dt><a href="#CO149-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Specify your index image. If you specify a tag after the image name, for example <code class="literal">:v4.13</code>, the catalog source pod uses an image pull policy of <code class="literal">Always</code>, meaning the pod always pulls the image prior to starting the container. If you specify a digest, for example <code class="literal">@sha256:&lt;id&gt;</code>, the image pull policy is <code class="literal">IfNotPresent</code>, meaning the pod pulls the image only if it does not already exist on the node.
										</div></dd><dt><a href="#CO149-5"><span class="callout">5</span></a> </dt><dd><div class="para">
											Specify your name or an organization name publishing the catalog.
										</div></dd><dt><a href="#CO149-6"><span class="callout">6</span></a> </dt><dd><div class="para">
											Catalog sources can automatically check for new versions to keep up to date.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Use the file to create the <code class="literal">CatalogSource</code> object:
								</p><pre class="programlisting language-terminal">$ oc apply -f catalogSource.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Verify the following resources are created successfully.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Check the pods:
								</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-marketplace</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                    READY   STATUS    RESTARTS  AGE
my-operator-catalog-6njx6               1/1     Running   0         28s
marketplace-operator-d9f549946-96sgr    1/1     Running   0         26h</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Check the catalog source:
								</p><pre class="programlisting language-terminal">$ oc get catalogsource -n openshift-marketplace</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                  DISPLAY               TYPE PUBLISHER  AGE
my-operator-catalog   My Operator Catalog   grpc            5s</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Check the package manifest:
								</p><pre class="programlisting language-terminal">$ oc get packagemanifest -n openshift-marketplace</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                          CATALOG               AGE
jaeger-product                My Operator Catalog   93s</pre>

									</p></div></li></ol></div></li></ol></div><p>
					You can now install the Operators from the <span class="strong strong"><strong>OperatorHub</strong></span> page on your OpenShift Container Platform web console.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-accessing-images-private-registries_olm-managing-custom-catalogs">Accessing images for Operators from private registries</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-catalogsource-image-template_olm-understanding-olm">Image template for custom catalog sources</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/images/#image-pull-policy">Image pull policy</a>
						</li></ul></div></section></section><section class="section" id="olm-installing-operators-from-operatorhub_post-install-preparing-for-users"><div class="titlepage"><div><div><h2 class="title">11.6. About Operator installation with OperatorHub</h2></div></div></div><p>
				OperatorHub is a user interface for discovering Operators; it works in conjunction with Operator Lifecycle Manager (OLM), which installs and manages Operators on a cluster.
			</p><p>
				As a cluster administrator, you can install an Operator from OperatorHub by using the OpenShift Container Platform web console or CLI. Subscribing an Operator to one or more namespaces makes the Operator available to developers on your cluster.
			</p><p>
				During installation, you must determine the following initial settings for the Operator:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Installation Mode</span></dt><dd>
							Choose <span class="strong strong"><strong>All namespaces on the cluster (default)</strong></span> to have the Operator installed on all namespaces or choose individual namespaces, if available, to only install the Operator on selected namespaces. This example chooses <span class="strong strong"><strong>All namespaces…​</strong></span> to make the Operator available to all users and projects.
						</dd><dt><span class="term">Update Channel</span></dt><dd>
							If an Operator is available through multiple channels, you can choose which channel you want to subscribe to. For example, to deploy from the <span class="strong strong"><strong>stable</strong></span> channel, if available, select it from the list.
						</dd><dt><span class="term">Approval Strategy</span></dt><dd><p class="simpara">
							You can choose automatic or manual updates.
						</p><p class="simpara">
							If you choose automatic updates for an installed Operator, when a new version of that Operator is available in the selected channel, Operator Lifecycle Manager (OLM) automatically upgrades the running instance of your Operator without human intervention.
						</p><p class="simpara">
							If you select manual updates, when a newer version of an Operator is available, OLM creates an update request. As a cluster administrator, you must then manually approve that update request to have the Operator updated to the new version.
						</p></dd></dl></div><section class="section" id="olm-installing-from-operatorhub-using-web-console_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.6.1. Installing from OperatorHub using the web console</h3></div></div></div><p>
					You can install and subscribe to an Operator from OperatorHub by using the OpenShift Container Platform web console.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to an OpenShift Container Platform cluster using an account with <code class="literal">cluster-admin</code> permissions.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Navigate in the web console to the <span class="strong strong"><strong>Operators → OperatorHub</strong></span> page.
						</li><li class="listitem"><p class="simpara">
							Scroll or type a keyword into the <span class="strong strong"><strong>Filter by keyword</strong></span> box to find the Operator you want. For example, type <code class="literal">jaeger</code> to find the Jaeger Operator.
						</p><p class="simpara">
							You can also filter options by <span class="strong strong"><strong>Infrastructure Features</strong></span>. For example, select <span class="strong strong"><strong>Disconnected</strong></span> if you want to see Operators that work in disconnected environments, also known as restricted network environments.
						</p></li><li class="listitem"><p class="simpara">
							Select the Operator to display additional information.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Choosing a Community Operator warns that Red Hat does not certify Community Operators; you must acknowledge the warning before continuing.
							</p></div></div></li><li class="listitem">
							Read the information about the Operator and click <span class="strong strong"><strong>Install</strong></span>.
						</li><li class="listitem"><p class="simpara">
							On the <span class="strong strong"><strong>Install Operator</strong></span> page:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Select one of the following:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<span class="strong strong"><strong>All namespaces on the cluster (default)</strong></span> installs the Operator in the default <code class="literal">openshift-operators</code> namespace to watch and be made available to all namespaces in the cluster. This option is not always available.
										</li><li class="listitem">
											<span class="strong strong"><strong>A specific namespace on the cluster</strong></span> allows you to choose a specific, single namespace in which to install the Operator. The Operator will only watch and be made available for use in this single namespace.
										</li></ul></div></li><li class="listitem">
									Select an <span class="strong strong"><strong>Update channel</strong></span> (if more than one is available).
								</li><li class="listitem">
									Select <span class="strong strong"><strong>Automatic</strong></span> or <span class="strong strong"><strong>Manual</strong></span> approval strategy, as described earlier.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Install</strong></span> to make the Operator available to the selected namespaces on this OpenShift Container Platform cluster.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									If you selected a <span class="strong strong"><strong>Manual</strong></span> approval strategy, the upgrade status of the subscription remains <span class="strong strong"><strong>Upgrading</strong></span> until you review and approve the install plan.
								</p><p class="simpara">
									After approving on the <span class="strong strong"><strong>Install Plan</strong></span> page, the subscription upgrade status moves to <span class="strong strong"><strong>Up to date</strong></span>.
								</p></li><li class="listitem">
									If you selected an <span class="strong strong"><strong>Automatic</strong></span> approval strategy, the upgrade status should resolve to <span class="strong strong"><strong>Up to date</strong></span> without intervention.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							After the upgrade status of the subscription is <span class="strong strong"><strong>Up to date</strong></span>, select <span class="strong strong"><strong>Operators → Installed Operators</strong></span> to verify that the cluster service version (CSV) of the installed Operator eventually shows up. The <span class="strong strong"><strong>Status</strong></span> should ultimately resolve to <span class="strong strong"><strong>InstallSucceeded</strong></span> in the relevant namespace.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								For the <span class="strong strong"><strong>All namespaces…​</strong></span> installation mode, the status resolves to <span class="strong strong"><strong>InstallSucceeded</strong></span> in the <code class="literal">openshift-operators</code> namespace, but the status is <span class="strong strong"><strong>Copied</strong></span> if you check in other namespaces.
							</p></div></div><p class="simpara">
							If it does not:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Check the logs in any pods in the <code class="literal">openshift-operators</code> project (or other relevant namespace if <span class="strong strong"><strong>A specific namespace…​</strong></span> installation mode was selected) on the <span class="strong strong"><strong>Workloads → Pods</strong></span> page that are reporting issues to troubleshoot further.
								</li></ol></div></li></ol></div></section><section class="section" id="olm-installing-operator-from-operatorhub-using-cli_post-install-preparing-for-users"><div class="titlepage"><div><div><h3 class="title">11.6.2. Installing from OperatorHub using the CLI</h3></div></div></div><p>
					Instead of using the OpenShift Container Platform web console, you can install an Operator from OperatorHub by using the CLI. Use the <code class="literal">oc</code> command to create or update a <code class="literal">Subscription</code> object.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to an OpenShift Container Platform cluster using an account with <code class="literal">cluster-admin</code> permissions.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the list of Operators available to the cluster from OperatorHub:
						</p><pre class="programlisting language-terminal">$ oc get packagemanifests -n openshift-marketplace</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                               CATALOG               AGE
3scale-operator                    Red Hat Operators     91m
advanced-cluster-management        Red Hat Operators     91m
amq7-cert-manager                  Red Hat Operators     91m
...
couchbase-enterprise-certified     Certified Operators   91m
crunchy-postgres-operator          Certified Operators   91m
mongodb-enterprise                 Certified Operators   91m
...
etcd                               Community Operators   91m
jaeger                             Community Operators   91m
kubefed                            Community Operators   91m
...</pre>

							</p></div><p class="simpara">
							Note the catalog for your desired Operator.
						</p></li><li class="listitem"><p class="simpara">
							Inspect your desired Operator to verify its supported install modes and available channels:
						</p><pre class="programlisting language-terminal">$ oc describe packagemanifests &lt;operator_name&gt; -n openshift-marketplace</pre></li><li class="listitem"><p class="simpara">
							An Operator group, defined by an <code class="literal">OperatorGroup</code> object, selects target namespaces in which to generate required RBAC access for all Operators in the same namespace as the Operator group.
						</p><p class="simpara">
							The namespace to which you subscribe the Operator must have an Operator group that matches the install mode of the Operator, either the <code class="literal">AllNamespaces</code> or <code class="literal">SingleNamespace</code> mode. If the Operator you intend to install uses the <code class="literal">AllNamespaces</code>, then the <code class="literal">openshift-operators</code> namespace already has an appropriate Operator group in place.
						</p><p class="simpara">
							However, if the Operator uses the <code class="literal">SingleNamespace</code> mode and you do not already have an appropriate Operator group in place, you must create one.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The web console version of this procedure handles the creation of the <code class="literal">OperatorGroup</code> and <code class="literal">Subscription</code> objects automatically behind the scenes for you when choosing <code class="literal">SingleNamespace</code> mode.
							</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create an <code class="literal">OperatorGroup</code> object YAML file, for example <code class="literal">operatorgroup.yaml</code>:
								</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">OperatorGroup</code> object</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: &lt;operatorgroup_name&gt;
  namespace: &lt;namespace&gt;
spec:
  targetNamespaces:
  - &lt;namespace&gt;</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">OperatorGroup</code> object:
								</p><pre class="programlisting language-terminal">$ oc apply -f operatorgroup.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">Subscription</code> object YAML file to subscribe a namespace to an Operator, for example <code class="literal">sub.yaml</code>:
						</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">Subscription</code> object</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: &lt;subscription_name&gt;
  namespace: openshift-operators <span id="CO150-1"><!--Empty--></span><span class="callout">1</span>
spec:
  channel: &lt;channel_name&gt; <span id="CO150-2"><!--Empty--></span><span class="callout">2</span>
  name: &lt;operator_name&gt; <span id="CO150-3"><!--Empty--></span><span class="callout">3</span>
  source: redhat-operators <span id="CO150-4"><!--Empty--></span><span class="callout">4</span>
  sourceNamespace: openshift-marketplace <span id="CO150-5"><!--Empty--></span><span class="callout">5</span>
  config:
    env: <span id="CO150-6"><!--Empty--></span><span class="callout">6</span>
    - name: ARGS
      value: "-v=10"
    envFrom: <span id="CO150-7"><!--Empty--></span><span class="callout">7</span>
    - secretRef:
        name: license-secret
    volumes: <span id="CO150-8"><!--Empty--></span><span class="callout">8</span>
    - name: &lt;volume_name&gt;
      configMap:
        name: &lt;configmap_name&gt;
    volumeMounts: <span id="CO150-9"><!--Empty--></span><span class="callout">9</span>
    - mountPath: &lt;directory_name&gt;
      name: &lt;volume_name&gt;
    tolerations: <span id="CO150-10"><!--Empty--></span><span class="callout">10</span>
    - operator: "Exists"
    resources: <span id="CO150-11"><!--Empty--></span><span class="callout">11</span>
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
    nodeSelector: <span id="CO150-12"><!--Empty--></span><span class="callout">12</span>
      foo: bar</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO150-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									For default <code class="literal">AllNamespaces</code> install mode usage, specify the <code class="literal">openshift-operators</code> namespace. Alternatively, you can specify a custom global namespace, if you have created one. Otherwise, specify the relevant single namespace for <code class="literal">SingleNamespace</code> install mode usage.
								</div></dd><dt><a href="#CO150-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Name of the channel to subscribe to.
								</div></dd><dt><a href="#CO150-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Name of the Operator to subscribe to.
								</div></dd><dt><a href="#CO150-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Name of the catalog source that provides the Operator.
								</div></dd><dt><a href="#CO150-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Namespace of the catalog source. Use <code class="literal">openshift-marketplace</code> for the default OperatorHub catalog sources.
								</div></dd><dt><a href="#CO150-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									The <code class="literal">env</code> parameter defines a list of Environment Variables that must exist in all containers in the pod created by OLM.
								</div></dd><dt><a href="#CO150-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									The <code class="literal">envFrom</code> parameter defines a list of sources to populate Environment Variables in the container.
								</div></dd><dt><a href="#CO150-8"><span class="callout">8</span></a> </dt><dd><div class="para">
									The <code class="literal">volumes</code> parameter defines a list of Volumes that must exist on the pod created by OLM.
								</div></dd><dt><a href="#CO150-9"><span class="callout">9</span></a> </dt><dd><div class="para">
									The <code class="literal">volumeMounts</code> parameter defines a list of VolumeMounts that must exist in all containers in the pod created by OLM. If a <code class="literal">volumeMount</code> references a <code class="literal">volume</code> that does not exist, OLM fails to deploy the Operator.
								</div></dd><dt><a href="#CO150-10"><span class="callout">10</span></a> </dt><dd><div class="para">
									The <code class="literal">tolerations</code> parameter defines a list of Tolerations for the pod created by OLM.
								</div></dd><dt><a href="#CO150-11"><span class="callout">11</span></a> </dt><dd><div class="para">
									The <code class="literal">resources</code> parameter defines resource constraints for all the containers in the pod created by OLM.
								</div></dd><dt><a href="#CO150-12"><span class="callout">12</span></a> </dt><dd><div class="para">
									The <code class="literal">nodeSelector</code> parameter defines a <code class="literal">NodeSelector</code> for the pod created by OLM.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">Subscription</code> object:
						</p><pre class="programlisting language-terminal">$ oc apply -f sub.yaml</pre><p class="simpara">
							At this point, OLM is now aware of the selected Operator. A cluster service version (CSV) for the Operator should appear in the target namespace, and APIs provided by the Operator should be available for creation.
						</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-operatorgroups-about_olm-understanding-operatorgroups">About OperatorGroups</a>
						</li></ul></div></section></section></section><section class="chapter" id="configuring-alert-notifications"><div class="titlepage"><div><div><h1 class="title">Chapter 12. Configuring alert notifications</h1></div></div></div><p>
			In OpenShift Container Platform, an alert is fired when the conditions defined in an alerting rule are true. An alert provides a notification that a set of circumstances are apparent within a cluster. Firing alerts can be viewed in the Alerting UI in the OpenShift Container Platform web console by default. After an installation, you can configure OpenShift Container Platform to send alert notifications to external systems.
		</p><section class="section" id="sending-notifications-to-external-systems_configuring-alert-notifications"><div class="titlepage"><div><div><h2 class="title">12.1. Sending notifications to external systems</h2></div></div></div><p>
				In OpenShift Container Platform 4.13, firing alerts can be viewed in the Alerting UI. Alerts are not configured by default to be sent to any notification systems. You can configure OpenShift Container Platform to send alerts to the following receiver types:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						PagerDuty
					</li><li class="listitem">
						Webhook
					</li><li class="listitem">
						Email
					</li><li class="listitem">
						Slack
					</li></ul></div><p>
				Routing alerts to receivers enables you to send timely notifications to the appropriate teams when failures occur. For example, critical alerts require immediate attention and are typically paged to an individual or a critical response team. Alerts that provide non-critical warning notifications might instead be routed to a ticketing system for non-immediate review.
			</p><div class="formalpara"><p class="title"><strong>Checking that alerting is operational by using the watchdog alert</strong></p><p>
					OpenShift Container Platform monitoring includes a watchdog alert that fires continuously. Alertmanager repeatedly sends watchdog alert notifications to configured notification providers. The provider is usually configured to notify an administrator when it stops receiving the watchdog alert. This mechanism helps you quickly identify any communication issues between Alertmanager and the notification provider.
				</p></div><section class="section" id="configuring-alert-receivers_configuring-alert-notifications"><div class="titlepage"><div><div><h3 class="title">12.1.1. Configuring alert receivers</h3></div></div></div><p>
					You can configure alert receivers to ensure that you learn about important issues with your cluster.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							In the <span class="strong strong"><strong>Administrator</strong></span> perspective, navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Cluster Settings</strong></span> → <span class="strong strong"><strong>Configuration</strong></span> → <span class="strong strong"><strong>Alertmanager</strong></span>.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Alternatively, you can navigate to the same page through the notification drawer. Select the bell icon at the top right of the OpenShift Container Platform web console and choose <span class="strong strong"><strong>Configure</strong></span> in the <span class="strong strong"><strong>AlertmanagerReceiverNotConfigured</strong></span> alert.
							</p></div></div></li><li class="listitem">
							Select <span class="strong strong"><strong>Create Receiver</strong></span> in the <span class="strong strong"><strong>Receivers</strong></span> section of the page.
						</li><li class="listitem">
							In the <span class="strong strong"><strong>Create Receiver</strong></span> form, add a <span class="strong strong"><strong>Receiver Name</strong></span> and choose a <span class="strong strong"><strong>Receiver Type</strong></span> from the list.
						</li><li class="listitem"><p class="simpara">
							Edit the receiver configuration:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									For PagerDuty receivers:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
											Choose an integration type and add a PagerDuty integration key.
										</li><li class="listitem">
											Add the URL of your PagerDuty installation.
										</li><li class="listitem">
											Select <span class="strong strong"><strong>Show advanced configuration</strong></span> if you want to edit the client and incident details or the severity specification.
										</li></ol></div></li><li class="listitem"><p class="simpara">
									For webhook receivers:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
											Add the endpoint to send HTTP POST requests to.
										</li><li class="listitem">
											Select <span class="strong strong"><strong>Show advanced configuration</strong></span> if you want to edit the default option to send resolved alerts to the receiver.
										</li></ol></div></li><li class="listitem"><p class="simpara">
									For email receivers:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
											Add the email address to send notifications to.
										</li><li class="listitem">
											Add SMTP configuration details, including the address to send notifications from, the smarthost and port number used for sending emails, the hostname of the SMTP server, and authentication details.
										</li><li class="listitem">
											Choose whether TLS is required.
										</li><li class="listitem">
											Select <span class="strong strong"><strong>Show advanced configuration</strong></span> if you want to edit the default option not to send resolved alerts to the receiver or edit the body of email notifications configuration.
										</li></ol></div></li><li class="listitem"><p class="simpara">
									For Slack receivers:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
											Add the URL of the Slack webhook.
										</li><li class="listitem">
											Add the Slack channel or user name to send notifications to.
										</li><li class="listitem">
											Select <span class="strong strong"><strong>Show advanced configuration</strong></span> if you want to edit the default option not to send resolved alerts to the receiver or edit the icon and username configuration. You can also choose whether to find and link channel names and usernames.
										</li></ol></div></li></ul></div></li><li class="listitem"><p class="simpara">
							By default, firing alerts with labels that match all of the selectors will be sent to the receiver. If you want label values for firing alerts to be matched exactly before they are sent to the receiver:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Add routing label names and values in the <span class="strong strong"><strong>Routing Labels</strong></span> section of the form.
								</li><li class="listitem">
									Select <span class="strong strong"><strong>Regular Expression</strong></span> if want to use a regular expression.
								</li><li class="listitem">
									Select <span class="strong strong"><strong>Add Label</strong></span> to add further routing labels.
								</li></ol></div></li><li class="listitem">
							Select <span class="strong strong"><strong>Create</strong></span> to create the receiver.
						</li></ol></div></section></section><section class="section _additional-resources" id="configuring-alert-notifications-additional-resources"><div class="titlepage"><div><div><h2 class="title">12.2. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/monitoring/#monitoring-overview">Monitoring overview</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/monitoring/#managing-alerts">Managing alerts</a>
					</li></ul></div></section></section><section class="chapter" id="connected-to-disconnected"><div class="titlepage"><div><div><h1 class="title">Chapter 13. Converting a connected cluster to a disconnected cluster</h1></div></div></div><p>
			There might be some scenarios where you need to convert your OpenShift Container Platform cluster from a connected cluster to a disconnected cluster.
		</p><p>
			A disconnected cluster, also known as a restricted cluster, does not have an active connection to the internet. As such, you must mirror the contents of your registries and installation media. You can create this mirror registry on a host that can access both the internet and your closed network, or copy images to a device that you can move across network boundaries.
		</p><p>
			This topic describes the general process for converting an existing, connected cluster into a disconnected cluster.
		</p><section class="section" id="installation-about-mirror-registry_connected-to-disconnected"><div class="titlepage"><div><div><h2 class="title">13.1. About the mirror registry</h2></div></div></div><p>
				You can mirror the images that are required for OpenShift Container Platform installation and subsequent product updates to a container mirror registry such as Red Hat Quay, JFrog Artifactory, Sonatype Nexus Repository, or Harbor. If you do not have access to a large-scale container registry, you can use the <span class="emphasis"><em>mirror registry for Red Hat OpenShift</em></span>, a small-scale container registry included with OpenShift Container Platform subscriptions.
			</p><p>
				You can use any container registry that supports <a class="link" href="https://docs.docker.com/registry/spec/manifest-v2-2">Docker v2-2</a>, such as Red Hat Quay, the <span class="emphasis"><em>mirror registry for Red Hat OpenShift</em></span>, Artifactory, Sonatype Nexus Repository, or Harbor. Regardless of your chosen registry, the procedure to mirror content from Red Hat hosted sites on the internet to an isolated image registry is the same. After you mirror the content, you configure each cluster to retrieve this content from your mirror registry.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					The OpenShift image registry cannot be used as the target registry because it does not support pushing without a tag, which is required during the mirroring process.
				</p></div></div><p>
				If choosing a container registry that is not the <span class="emphasis"><em>mirror registry for Red Hat OpenShift</em></span>, it must be reachable by every machine in the clusters that you provision. If the registry is unreachable, installation, updating, or normal operations such as workload relocation might fail. For that reason, you must run mirror registries in a highly available way, and the mirror registries must at least match the production availability of your OpenShift Container Platform clusters.
			</p><p>
				When you populate your mirror registry with OpenShift Container Platform images, you can follow two scenarios. If you have a host that can access both the internet and your mirror registry, but not your cluster nodes, you can directly mirror the content from that machine. This process is referred to as <span class="emphasis"><em>connected mirroring</em></span>. If you have no such host, you must mirror the images to a file system and then bring that host or removable media into your restricted environment. This process is referred to as <span class="emphasis"><em>disconnected mirroring</em></span>.
			</p><p>
				For mirrored registries, to view the source of pulled images, you must review the <code class="literal">Trying to access</code> log entry in the CRI-O logs. Other methods to view the image pull source, such as using the <code class="literal">crictl images</code> command on a node, show the non-mirrored image name, even though the image is pulled from the mirrored location.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Red Hat does not test third party registries with OpenShift Container Platform.
				</p></div></div></section><section class="section" id="prerequisites_connected-to-disconnected"><div class="titlepage"><div><div><h2 class="title">13.2. Prerequisites</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The <code class="literal">oc</code> client is installed.
					</li><li class="listitem">
						A running cluster.
					</li><li class="listitem"><p class="simpara">
						An installed mirror registry, which is a container image registry that supports <a class="link" href="https://docs.docker.com/registry/spec/manifest-v2-2/">Docker v2-2</a> in the location that will host the OpenShift Container Platform cluster, such as one of the following registries:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								<a class="link" href="https://www.redhat.com/en/technologies/cloud-computing/quay">Red Hat Quay</a>
							</li><li class="listitem">
								<a class="link" href="https://jfrog.com/artifactory/">JFrog Artifactory</a>
							</li><li class="listitem">
								<a class="link" href="https://www.sonatype.com/products/repository-oss?topnav=true">Sonatype Nexus Repository</a>
							</li><li class="listitem">
								<a class="link" href="https://goharbor.io/">Harbor</a>
							</li></ul></div><p class="simpara">
						If you have an subscription to Red Hat Quay, see the documentation on deploying Red Hat Quay <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3.5/html/deploy_red_hat_quay_for_proof-of-concept_non-production_purposes/">for proof-of-concept purposes</a> or <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3.5/html/deploy_red_hat_quay_on_openshift_with_the_quay_operator/">by using the Quay Operator</a>.
					</p></li><li class="listitem">
						The mirror repository must be configured to share images. For example, a Red Hat Quay repository requires <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3.5/html/use_red_hat_quay/user-org-intro#org-create">Organizations</a> in order to share images.
					</li><li class="listitem">
						Access to the internet to obtain the necessary container images.
					</li></ul></div></section><section class="section" id="connected-to-disconnected-prepare-mirror_connected-to-disconnected"><div class="titlepage"><div><div><h2 class="title">13.3. Preparing the cluster for mirroring</h2></div></div></div><p>
				Before disconnecting your cluster, you must mirror, or copy, the images to a mirror registry that is reachable by every node in your disconnected cluster. In order to mirror the images, you must prepare your cluster by:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Adding the mirror registry certificates to the list of trusted CAs on your host.
					</li><li class="listitem">
						Creating a <code class="literal">.dockerconfigjson</code> file that contains your image pull secret, which is from the <code class="literal">cloud.openshift.com</code> token.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Configuring credentials that allow image mirroring:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Add the CA certificate for the mirror registry, in the simple PEM or DER file formats, to the list of trusted CAs. For example:
							</p><pre class="programlisting language-terminal">$ cp &lt;/path/to/cert.crt&gt; /usr/share/pki/ca-trust-source/anchors/</pre><div class="variablelist"><dl class="variablelist"><dt><span class="term">where, </span><span class="term"><code class="literal">&lt;/path/to/cert.crt&gt;</code></span></dt><dd>
											Specifies the path to the certificate on your local file system.
										</dd></dl></div></li><li class="listitem"><p class="simpara">
								Update the CA trust. For example, in Linux:
							</p><pre class="programlisting language-terminal">$ update-ca-trust</pre></li><li class="listitem"><p class="simpara">
								Extract the <code class="literal">.dockerconfigjson</code> file from the global pull secret:
							</p><pre class="programlisting language-terminal">$ oc extract secret/pull-secret -n openshift-config --confirm --to=.</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">.dockerconfigjson</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Edit the <code class="literal">.dockerconfigjson</code> file to add your mirror registry and authentication credentials and save it as a new file:
							</p><pre class="programlisting language-terminal">{"auths":{"&lt;local_registry&gt;": {"auth": "&lt;credentials&gt;","email": "you@example.com"}}},"&lt;registry&gt;:&lt;port&gt;/&lt;namespace&gt;/":{"auth":"&lt;token&gt;"}}}</pre><p class="simpara">
								where:
							</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;local_registry&gt;</code></span></dt><dd>
											Specifies the registry domain name, and optionally the port, that your mirror registry uses to serve content.
										</dd><dt><span class="term"><code class="literal">auth</code></span></dt><dd>
											Specifies the base64-encoded user name and password for your mirror registry.
										</dd><dt><span class="term"><code class="literal">&lt;registry&gt;:&lt;port&gt;/&lt;namespace&gt;</code></span></dt><dd>
											Specifies the mirror registry details.
										</dd><dt><span class="term"><code class="literal">&lt;token&gt;</code></span></dt><dd><p class="simpara">
											Specifies the base64-encoded <code class="literal">username:password</code> for your mirror registry.
										</p><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ {"auths":{"cloud.openshift.com":{"auth":"b3BlbnNoaWZ0Y3UjhGOVZPT0lOMEFaUjdPUzRGTA==","email":"user@example.com"},
"quay.io":{"auth":"b3BlbnNoaWZ0LXJlbGVhc2UtZGOVZPT0lOMEFaUGSTd4VGVGVUjdPUzRGTA==","email":"user@example.com"},
"registry.connect.redhat.com"{"auth":"NTE3MTMwNDB8dWhjLTFEZlN3VHkxOSTd4VGVGVU1MdTpleUpoYkdjaUailA==","email":"user@example.com"},
"registry.redhat.io":{"auth":"NTE3MTMwNDB8dWhjLTFEZlN3VH3BGSTd4VGVGVU1MdTpleUpoYkdjaU9fZw==","email":"user@example.com"},
"registry.svc.ci.openshift.org":{"auth":"dXNlcjpyWjAwWVFjSEJiT2RKVW1pSmg4dW92dGp1SXRxQ3RGN1pwajJhN1ZXeTRV"},"my-registry:5000/my-namespace/":{"auth":"dXNlcm5hbWU6cGFzc3dvcmQ="}}}</pre></dd></dl></div></li></ol></div></li></ol></div></section><section class="section" id="connected-to-disconnected-mirror-images_connected-to-disconnected"><div class="titlepage"><div><div><h2 class="title">13.4. Mirroring the images</h2></div></div></div><p>
				After the cluster is properly configured, you can mirror the images from your external repositories to the mirror repository.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Mirror the Operator Lifecycle Manager (OLM) images:
					</p><pre class="programlisting language-terminal">$ oc adm catalog mirror registry.redhat.io/redhat/redhat-operator-index:v{product-version} &lt;mirror_registry&gt;:&lt;port&gt;/olm -a &lt;reg_creds&gt;</pre><p class="simpara">
						where:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">product-version</code></span></dt><dd>
									Specifies the tag that corresponds to the version of OpenShift Container Platform to install, such as <code class="literal">4.8</code>.
								</dd><dt><span class="term"><code class="literal">mirror_registry</code></span></dt><dd>
									Specifies the fully qualified domain name (FQDN) for the target registry and namespace to mirror the Operator content to, where <code class="literal">&lt;namespace&gt;</code> is any existing namespace on the registry.
								</dd><dt><span class="term"><code class="literal">reg_creds</code></span></dt><dd>
									Specifies the location of your modified <code class="literal">.dockerconfigjson</code> file.
								</dd></dl></div><p class="simpara">
						For example:
					</p><pre class="programlisting language-terminal">$ oc adm catalog mirror registry.redhat.io/redhat/redhat-operator-index:v4.8 mirror.registry.com:443/olm -a ./.dockerconfigjson  --index-filter-by-os='.*'</pre></li><li class="listitem"><p class="simpara">
						Mirror the content for any other Red Hat-provided Operator:
					</p><pre class="programlisting language-terminal">$ oc adm catalog mirror &lt;index_image&gt; &lt;mirror_registry&gt;:&lt;port&gt;/&lt;namespace&gt; -a &lt;reg_creds&gt;</pre><p class="simpara">
						where:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">index_image</code></span></dt><dd>
									Specifies the index image for the catalog that you want to mirror.
								</dd><dt><span class="term"><code class="literal">mirror_registry</code></span></dt><dd>
									Specifies the FQDN for the target registry and namespace to mirror the Operator content to, where <code class="literal">&lt;namespace&gt;</code> is any existing namespace on the registry.
								</dd><dt><span class="term"><code class="literal">reg_creds</code></span></dt><dd>
									Optional: Specifies the location of your registry credentials file, if required.
								</dd></dl></div><p class="simpara">
						For example:
					</p><pre class="programlisting language-terminal">$ oc adm catalog mirror registry.redhat.io/redhat/community-operator-index:v4.8 mirror.registry.com:443/olm -a ./.dockerconfigjson  --index-filter-by-os='.*'</pre></li><li class="listitem"><p class="simpara">
						Mirror the OpenShift Container Platform image repository:
					</p><pre class="programlisting language-terminal">$ oc adm release mirror -a .dockerconfigjson --from=quay.io/openshift-release-dev/ocp-release:v&lt;product-version&gt;-&lt;architecture&gt; --to=&lt;local_registry&gt;/&lt;local_repository&gt; --to-release-image=&lt;local_registry&gt;/&lt;local_repository&gt;:v&lt;product-version&gt;-&lt;architecture&gt;</pre><p class="simpara">
						where:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">product-version</code></span></dt><dd>
									Specifies the tag that corresponds to the version of OpenShift Container Platform to install, such as <code class="literal">4.8.15-x86_64</code>.
								</dd><dt><span class="term"><code class="literal">architecture</code></span></dt><dd>
									Specifies the type of architecture for your server, such as <code class="literal">x86_64</code>.
								</dd><dt><span class="term"><code class="literal">local_registry</code></span></dt><dd>
									Specifies the registry domain name for your mirror repository.
								</dd><dt><span class="term"><code class="literal">local_repository</code></span></dt><dd>
									Specifies the name of the repository to create in your registry, such as <code class="literal">ocp4/openshift4</code>.
								</dd></dl></div><p class="simpara">
						For example:
					</p><pre class="programlisting language-terminal">$ oc adm release mirror -a .dockerconfigjson --from=quay.io/openshift-release-dev/ocp-release:4.8.15-x86_64 --to=mirror.registry.com:443/ocp/release --to-release-image=mirror.registry.com:443/ocp/release:4.8.15-x86_64</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">info: Mirroring 109 images to mirror.registry.com/ocp/release ...
mirror.registry.com:443/
  ocp/release
	manifests:
  	sha256:086224cadce475029065a0efc5244923f43fb9bb3bb47637e0aaf1f32b9cad47 -&gt; 4.8.15-x86_64-thanos
  	sha256:0a214f12737cb1cfbec473cc301aa2c289d4837224c9603e99d1e90fc00328db -&gt; 4.8.15-x86_64-kuryr-controller
  	sha256:0cf5fd36ac4b95f9de506623b902118a90ff17a07b663aad5d57c425ca44038c -&gt; 4.8.15-x86_64-pod
  	sha256:0d1c356c26d6e5945a488ab2b050b75a8b838fc948a75c0fa13a9084974680cb -&gt; 4.8.15-x86_64-kube-client-agent

…..
sha256:66e37d2532607e6c91eedf23b9600b4db904ce68e92b43c43d5b417ca6c8e63c mirror.registry.com:443/ocp/release:4.5.41-multus-admission-controller
sha256:d36efdbf8d5b2cbc4dcdbd64297107d88a31ef6b0ec4a39695915c10db4973f1 mirror.registry.com:443/ocp/release:4.5.41-cluster-kube-scheduler-operator
sha256:bd1baa5c8239b23ecdf76819ddb63cd1cd6091119fecdbf1a0db1fb3760321a2 mirror.registry.com:443/ocp/release:4.5.41-aws-machine-controllers
info: Mirroring completed in 2.02s (0B/s)

Success
Update image:  mirror.registry.com:443/ocp/release:4.5.41-x86_64
Mirror prefix: mirror.registry.com:443/ocp/release</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Mirror any other registries, as needed:
					</p><pre class="programlisting language-terminal">$ oc image mirror &lt;online_registry&gt;/my/image:latest &lt;mirror_registry&gt;</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Additional information</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						For more information about mirroring Operator catalogs, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-mirror-catalog_olm-restricted-networks">Mirroring an Operator catalog</a>.
					</li><li class="listitem">
						For more information about the <code class="literal">oc adm catalog mirror</code> command, see the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/cli_tools/#oc-adm-catalog-mirror">OpenShift CLI administrator command reference</a>.
					</li></ul></div></section><section class="section" id="connected-to-disconnected-config-registry_connected-to-disconnected"><div class="titlepage"><div><div><h2 class="title">13.5. Configuring the cluster for the mirror registry</h2></div></div></div><p>
				After creating and mirroring the images to the mirror registry, you must modify your cluster so that pods can pull images from the mirror registry.
			</p><p>
				You must:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Add the mirror registry credentials to the global pull secret.
					</li><li class="listitem">
						Add the mirror registry server certificate to the cluster.
					</li><li class="listitem"><p class="simpara">
						Create an <code class="literal">ImageContentSourcePolicy</code> custom resource (ICSP), which associates the mirror registry with the source registry.
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Add mirror registry credential to the cluster global pull-secret:
							</p><pre class="programlisting language-terminal">$ oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=&lt;pull_secret_location&gt; <span id="CO151-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO151-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Provide the path to the new pull secret file.
									</div></dd></dl></div><p class="simpara">
								For example:
							</p><pre class="programlisting language-terminal">$ oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=.mirrorsecretconfigjson</pre></li><li class="listitem"><p class="simpara">
								Add the CA-signed mirror registry server certificate to the nodes in the cluster:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Create a config map that includes the server certificate for the mirror registry
									</p><pre class="programlisting language-terminal">$ oc create configmap &lt;config_map_name&gt; --from-file=&lt;mirror_address_host&gt;..&lt;port&gt;=$path/ca.crt -n openshift-config</pre><p class="simpara">
										For example:
									</p><pre class="programlisting language-terminal">S oc create configmap registry-config --from-file=mirror.registry.com..443=/root/certs/ca-chain.cert.pem -n openshift-config</pre></li><li class="listitem"><p class="simpara">
										Use the config map to update the <code class="literal">image.config.openshift.io/cluster</code> custom resource (CR). OpenShift Container Platform applies the changes to this CR to all nodes in the cluster:
									</p><pre class="programlisting language-terminal">$ oc patch image.config.openshift.io/cluster --patch '{"spec":{"additionalTrustedCA":{"name":"&lt;config_map_name&gt;"}}}' --type=merge</pre><p class="simpara">
										For example:
									</p><pre class="programlisting language-terminal">$ oc patch image.config.openshift.io/cluster --patch '{"spec":{"additionalTrustedCA":{"name":"registry-config"}}}' --type=merge</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Create an ICSP to redirect container pull requests from the online registries to the mirror registry:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Create the <code class="literal">ImageContentSourcePolicy</code> custom resource:
									</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1alpha1
kind: ImageContentSourcePolicy
metadata:
  name: mirror-ocp
spec:
  repositoryDigestMirrors:
  - mirrors:
    - mirror.registry.com:443/ocp/release <span id="CO152-1"><!--Empty--></span><span class="callout">1</span>
    source: quay.io/openshift-release-dev/ocp-release <span id="CO152-2"><!--Empty--></span><span class="callout">2</span>
  - mirrors:
    - mirror.registry.com:443/ocp/release
    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO152-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Specifies the name of the mirror image registry and repository.
											</div></dd><dt><a href="#CO152-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Specifies the online registry and repository containing the content that is mirrored.
											</div></dd></dl></div></li><li class="listitem"><p class="simpara">
										Create the ICSP object:
									</p><pre class="programlisting language-terminal">$ oc create -f registryrepomirror.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">imagecontentsourcepolicy.operator.openshift.io/mirror-ocp created</pre>

										</p></div><p class="simpara">
										OpenShift Container Platform applies the changes to this CR to all nodes in the cluster.
									</p></li></ol></div></li><li class="listitem"><p class="simpara">
								Verify that the credentials, CA, and ICSP for mirror registry were added:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Log into a node:
									</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
										Set <code class="literal">/host</code> as the root directory within the debug shell:
									</p><pre class="programlisting language-terminal">sh-4.4# chroot /host</pre></li><li class="listitem"><p class="simpara">
										Check the <code class="literal">config.json</code> file for the credentials:
									</p><pre class="programlisting language-terminal">sh-4.4# cat /var/lib/kubelet/config.json</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">{"auths":{"brew.registry.redhat.io":{"xx=="},"brewregistry.stage.redhat.io":{"auth":"xxx=="},"mirror.registry.com:443":{"auth":"xx="}}} <span id="CO153-1"><!--Empty--></span><span class="callout">1</span></pre>

										</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO153-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Ensure that the mirror registry and credentials are present.
											</div></dd></dl></div></li><li class="listitem"><p class="simpara">
										Change to the <code class="literal">certs.d</code> directory
									</p><pre class="programlisting language-terminal">sh-4.4# cd /etc/docker/certs.d/</pre></li><li class="listitem"><p class="simpara">
										List the certificates in the <code class="literal">certs.d</code> directory:
									</p><pre class="programlisting language-terminal">sh-4.4# ls</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="screen">image-registry.openshift-image-registry.svc.cluster.local:5000
image-registry.openshift-image-registry.svc:5000
mirror.registry.com:443 <span id="CO154-1"><!--Empty--></span><span class="callout">1</span></pre>

										</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO154-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Ensure that the mirror registry is in the list.
											</div></dd></dl></div></li><li class="listitem"><p class="simpara">
										Check that the ICSP added the mirror registry to the <code class="literal">registries.conf</code> file:
									</p><pre class="programlisting language-terminal">sh-4.4# cat /etc/containers/registries.conf</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">unqualified-search-registries = ["registry.access.redhat.com", "docker.io"]

[[registry]]
  prefix = ""
  location = "quay.io/openshift-release-dev/ocp-release"
  mirror-by-digest-only = true

  [[registry.mirror]]
    location = "mirror.registry.com:443/ocp/release"

[[registry]]
  prefix = ""
  location = "quay.io/openshift-release-dev/ocp-v4.0-art-dev"
  mirror-by-digest-only = true

  [[registry.mirror]]
    location = "mirror.registry.com:443/ocp/release"</pre>

										</p></div><p class="simpara">
										The <code class="literal">registry.mirror</code> parameters indicate that the mirror registry is searched before the original registry.
									</p></li><li class="listitem"><p class="simpara">
										Exit the node.
									</p><pre class="programlisting language-terminal">sh-4.4# exit</pre></li></ol></div></li></ol></div></li></ul></div></section><section class="section" id="connected-to-disconnected-verify_connected-to-disconnected"><div class="titlepage"><div><div><h2 class="title">13.6. Ensure applications continue to work</h2></div></div></div><p>
				Before disconnecting the cluster from the network, ensure that your cluster is working as expected and all of your applications are working as expected.
			</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					Use the following commands to check the status of your cluster:
				</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Ensure your pods are running:
					</p><pre class="programlisting language-terminal">$ oc get pods --all-namespaces</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terinal">NAMESPACE                                          NAME                                                          READY   STATUS      RESTARTS   AGE
kube-system                                        apiserver-watcher-ci-ln-47ltxtb-f76d1-mrffg-master-0          1/1     Running     0          39m
kube-system                                        apiserver-watcher-ci-ln-47ltxtb-f76d1-mrffg-master-1          1/1     Running     0          39m
kube-system                                        apiserver-watcher-ci-ln-47ltxtb-f76d1-mrffg-master-2          1/1     Running     0          39m
openshift-apiserver-operator                       openshift-apiserver-operator-79c7c646fd-5rvr5                 1/1     Running     3          45m
openshift-apiserver                                apiserver-b944c4645-q694g                                     2/2     Running     0          29m
openshift-apiserver                                apiserver-b944c4645-shdxb                                     2/2     Running     0          31m
openshift-apiserver                                apiserver-b944c4645-x7rf2                                     2/2     Running     0          33m
 ...</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Ensure your nodes are in the READY status:
					</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                                       STATUS   ROLES    AGE   VERSION
ci-ln-47ltxtb-f76d1-mrffg-master-0         Ready    master   42m   v1.26.0
ci-ln-47ltxtb-f76d1-mrffg-master-1         Ready    master   42m   v1.26.0
ci-ln-47ltxtb-f76d1-mrffg-master-2         Ready    master   42m   v1.26.0
ci-ln-47ltxtb-f76d1-mrffg-worker-a-gsxbz   Ready    worker   35m   v1.26.0
ci-ln-47ltxtb-f76d1-mrffg-worker-b-5qqdx   Ready    worker   35m   v1.26.0
ci-ln-47ltxtb-f76d1-mrffg-worker-c-rjkpq   Ready    worker   34m   v1.26.0</pre>

						</p></div></li></ul></div></section><section class="section" id="connected-to-disconnected-disconnect_connected-to-disconnected"><div class="titlepage"><div><div><h2 class="title">13.7. Disconnect the cluster from the network</h2></div></div></div><p>
				After mirroring all the required repositories and configuring your cluster to work as a disconnected cluster, you can disconnect the cluster from the network.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The Insights Operator is degraded when the cluster loses its Internet connection. You can avoid this problem by temporarily <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#opting-out-of-remote-health-reporting">disabling the Insights Operator</a> until you can restore it.
				</p></div></div></section><section class="section" id="connected-to-disconnected-restore-insights_connected-to-disconnected"><div class="titlepage"><div><div><h2 class="title">13.8. Restoring a degraded Insights Operator</h2></div></div></div><p>
				Disconnecting the cluster from the network necessarily causes the cluster to lose the Internet connection. The Insights Operator becomes degraded because it requires access to <a class="link" href="https://console.redhat.com">Red Hat Insights</a>.
			</p><p>
				This topic describes how to recover from a degraded Insights Operator.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Edit your <code class="literal">.dockerconfigjson</code> file to remove the <code class="literal">cloud.openshift.com</code> entry, for example:
					</p><pre class="programlisting language-terminal">"cloud.openshift.com":{"auth":"&lt;hash&gt;","email":"user@example.com"}</pre></li><li class="listitem">
						Save the file.
					</li><li class="listitem"><p class="simpara">
						Update the cluster secret with the edited <code class="literal">.dockerconfigjson</code> file:
					</p><pre class="programlisting language-terminal">$ oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=./.dockerconfigjson</pre></li><li class="listitem"><p class="simpara">
						Verify that the Insights Operator is no longer degraded:
					</p><pre class="programlisting language-terminal">$ oc get co insights</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
insights   4.5.41    True        False         False      3d</pre>

						</p></div></li></ol></div></section><section class="section" id="connected-to-disconnected-restore_connected-to-disconnected"><div class="titlepage"><div><div><h2 class="title">13.9. Restoring the network</h2></div></div></div><p>
				If you want to reconnect a disconnected cluster and pull images from online registries, delete the cluster’s ImageContentSourcePolicy (ICSP) objects. Without the ICSP, pull requests to external registries are no longer redirected to the mirror registry.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						View the ICSP objects in your cluster:
					</p><pre class="programlisting language-terminal">$ oc get imagecontentsourcepolicy</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                 AGE
mirror-ocp           6d20h
ocp4-index-0         6d18h
qe45-index-0         6d15h</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Delete all the ICSP objects you created when disconnecting your cluster:
					</p><pre class="programlisting language-terminal">$ oc delete imagecontentsourcepolicy &lt;icsp_name&gt; &lt;icsp_name&gt; &lt;icsp_name&gt;</pre><p class="simpara">
						For example:
					</p><pre class="programlisting language-terminal">$ oc delete imagecontentsourcepolicy mirror-ocp ocp4-index-0 qe45-index-0</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">imagecontentsourcepolicy.operator.openshift.io "mirror-ocp" deleted
imagecontentsourcepolicy.operator.openshift.io "ocp4-index-0" deleted
imagecontentsourcepolicy.operator.openshift.io "qe45-index-0" deleted</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Wait for all the nodes to restart and return to the READY status and verify that the <code class="literal">registries.conf</code> file is pointing to the original registries and not the mirror registries:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Log into a node:
							</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
								Set <code class="literal">/host</code> as the root directory within the debug shell:
							</p><pre class="programlisting language-terminal">sh-4.4# chroot /host</pre></li><li class="listitem"><p class="simpara">
								Examine the <code class="literal">registries.conf</code> file:
							</p><pre class="programlisting language-terminal">sh-4.4# cat /etc/containers/registries.conf</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">unqualified-search-registries = ["registry.access.redhat.com", "docker.io"] <span id="CO155-1"><!--Empty--></span><span class="callout">1</span></pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO155-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The <code class="literal">registry</code> and <code class="literal">registry.mirror</code> entries created by the ICSPs you deleted are removed.
									</div></dd></dl></div></li></ol></div></li></ol></div></section></section><section class="chapter" id="enabling-cluster-capabilities"><div class="titlepage"><div><div><h1 class="title">Chapter 14. Enabling cluster capabilities</h1></div></div></div><p>
			Cluster administrators can enable cluster capabilities that were disabled prior to installation.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				Cluster administrators cannot disable a cluster capability after it is enabled.
			</p></div></div><section class="section" id="viewing_the_cluster_capabilities_enabling-cluster-capabilities"><div class="titlepage"><div><div><h2 class="title">14.1. Viewing the cluster capabilities</h2></div></div></div><p>
				As a cluster administrator, you can view the capabilities by using the <code class="literal">clusterversion</code> resource status.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						To view the status of the cluster capabilities, run the following command:
					</p><pre class="programlisting language-terminal">$ oc get clusterversion version -o jsonpath='{.spec.capabilities}{"\n"}{.status.capabilities}{"\n"}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">{"additionalEnabledCapabilities":["openshift-samples"],"baselineCapabilitySet":"None"}
{"enabledCapabilities":["openshift-samples"],"knownCapabilities":["CSISnapshot","Console","Insights","Storage","baremetal","marketplace","openshift-samples"]}</pre>

						</p></div></li></ul></div></section><section class="section" id="setting_baseline_capability_set_enabling-cluster-capabilities"><div class="titlepage"><div><div><h2 class="title">14.2. Enabling the cluster capabilities by setting baseline capability set</h2></div></div></div><p>
				As a cluster administrator, you can enable the capabilities by setting <code class="literal">baselineCapabilitySet</code>.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						To set the <code class="literal">baselineCapabilitySet</code>, run the following command:
					</p><pre class="programlisting language-terminal">$ oc patch clusterversion version --type merge -p '{"spec":{"capabilities":{"baselineCapabilitySet":"vCurrent"}}}' <span id="CO156-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO156-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								For <code class="literal">baselineCapabilitySet</code> you can specify <code class="literal">vCurrent</code>, <code class="literal">v4.12</code>, <code class="literal">v4.13</code>, or <code class="literal">None</code>.
							</div></dd></dl></div></li></ul></div><p>
				The following table describes the <code class="literal">baselineCapabilitySet</code> values.
			</p><div class="table" id="idm140031631687280"><p class="title"><strong>Table 14.1. Cluster capabilities <code class="literal">baselineCapabilitySet</code> values description</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 40%; " class="col_1"><!--Empty--></col><col style="width: 60%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140031631681968" scope="col">Value</th><th align="left" valign="middle" id="idm140031631680880" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140031631681968"> <p>
								<code class="literal">vCurrent</code>
							</p>
							 </td><td align="left" valign="middle" headers="idm140031631680880"> <p>
								Specify this option when you want to automatically add new, default capabilities that are introduced in new releases.
							</p>
							 </td></tr><tr><td align="left" valign="middle" headers="idm140031631681968"> <p>
								<code class="literal">v4.11</code>
							</p>
							 </td><td align="left" valign="middle" headers="idm140031631680880"> <p>
								Specify this option when you want to enable the default capabilities for OpenShift Container Platform 4.11. By specifying <code class="literal">v4.11</code>, capabilities that are introduced in newer versions of OpenShift Container Platform are not enabled. The default capabilities in OpenShift Container Platform 4.11 are <code class="literal">baremetal</code>, <code class="literal">marketplace</code>, and <code class="literal">openshift-samples</code>.
							</p>
							 </td></tr><tr><td align="left" valign="middle" headers="idm140031631681968"> <p>
								<code class="literal">v4.12</code>
							</p>
							 </td><td align="left" valign="middle" headers="idm140031631680880"> <p>
								Specify this option when you want to enable the default capabilities for OpenShift Container Platform 4.12. By specifying <code class="literal">v4.12</code>, capabilities that are introduced in newer versions of OpenShift Container Platform are not enabled. The default capabilities in OpenShift Container Platform 4.12 are <code class="literal">baremetal</code>, <code class="literal">marketplace</code>, <code class="literal">openshift-samples</code>, <code class="literal">Console</code>, <code class="literal">Insights</code>, <code class="literal">Storage</code> and <code class="literal">CSISnapshot</code>.
							</p>
							 </td></tr><tr><td align="left" valign="middle" headers="idm140031631681968"> <p>
								<code class="literal">v4.13</code>
							</p>
							 </td><td align="left" valign="middle" headers="idm140031631680880"> <p>
								Specify this option when you want to enable the default capabilities for OpenShift Container Platform 4.13. By specifying <code class="literal">v4.13</code>, capabilities that are introduced in newer versions of OpenShift Container Platform are not enabled. The default capabilities in OpenShift Container Platform 4.13 are <code class="literal">baremetal</code>, <code class="literal">marketplace</code>, <code class="literal">openshift-samples</code>, <code class="literal">Console</code>, <code class="literal">Insights</code>, <code class="literal">Storage</code>, <code class="literal">CSISnapshot</code> and <code class="literal">NodeTuning</code>.
							</p>
							 </td></tr><tr><td align="left" valign="middle" headers="idm140031631681968"> <p>
								<code class="literal">None</code>
							</p>
							 </td><td align="left" valign="middle" headers="idm140031631680880"> <p>
								Specify when the other sets are too large, and you do not need any capabilities or want to fine-tune via <code class="literal">additionalEnabledCapabilities</code>.
							</p>
							 </td></tr></tbody></table></div></div></section><section class="section" id="setting_additional_enabled_capabilities_enabling-cluster-capabilities"><div class="titlepage"><div><div><h2 class="title">14.3. Enabling the cluster capabilities by setting additional enabled capabilities</h2></div></div></div><p>
				As a cluster administrator, you can enable the cluster capabilities by setting <code class="literal">additionalEnabledCapabilities</code>.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						View the additional enabled capabilities by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get clusterversion version -o jsonpath='{.spec.capabilities.additionalEnabledCapabilities}{"\n"}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">["openshift-samples"]</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						To set the <code class="literal">additionalEnabledCapabilities</code>, run the following command:
					</p><pre class="programlisting language-terminal">$ oc patch clusterversion/version --type merge -p '{"spec":{"capabilities":{"additionalEnabledCapabilities":["openshift-samples", "marketplace"]}}}'</pre></li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					It is not possible to disable a capability which is already enabled in a cluster. The cluster version Operator (CVO) continues to reconcile the capability which is already enabled in the cluster.
				</p></div></div><p>
				If you try to disable a capability, then CVO shows the divergent spec:
			</p><pre class="programlisting language-terminal">$ oc get clusterversion version -o jsonpath='{.status.conditions[?(@.type=="ImplicitlyEnabledCapabilities")]}{"\n"}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
					
<pre class="programlisting language-terminal">{"lastTransitionTime":"2022-07-22T03:14:35Z","message":"The following capabilities could not be disabled: openshift-samples","reason":"CapabilitiesImplicitlyEnabled","status":"True","type":"ImplicitlyEnabledCapabilities"}</pre>

				</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					During the cluster upgrades, it is possible that a given capability could be implicitly enabled. If a resource was already running on the cluster before the upgrade, then any capabilities that is part of the resource will be enabled. For example, during a cluster upgrade, a resource that is already running on the cluster has been changed to be part of the <code class="literal">marketplace</code> capability by the system. Even if a cluster administrator does not explicitly enabled the <code class="literal">marketplace</code> capability, it is implicitly enabled by the system.
				</p></div></div></section><section class="section _additional-resources" id="additional-resources_enabling-cluster-capabilities"><div class="titlepage"><div><div><h2 class="title">14.4. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#cluster-capabilities">Cluster capabilities</a>
					</li></ul></div></section></section><section class="chapter" id="post-install-configure-additional-devices-ibmz"><div class="titlepage"><div><div><h1 class="title">Chapter 15. Configuring additional devices in an IBM Z or IBM(R) LinuxONE environment</h1></div></div></div><p>
			After installing OpenShift Container Platform, you can configure additional devices for your cluster in an IBM Z or IBM® LinuxONE environment, which is installed with z/VM. The following devices can be configured:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Fibre Channel Protocol (FCP) host
				</li><li class="listitem">
					FCP LUN
				</li><li class="listitem">
					DASD
				</li><li class="listitem">
					qeth
				</li></ul></div><p>
			You can configure devices by adding udev rules using the Machine Config Operator (MCO) or you can configure devices manually.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				The procedures described here apply only to z/VM installations. If you have installed your cluster with RHEL KVM on IBM Z or IBM® LinuxONE infrastructure, no additional configuration is needed inside the KVM guest after the devices were added to the KVM guests. However, both in z/VM and RHEL KVM environments the next steps to configure the Local Storage Operator and Kubernetes NMState Operator need to be applied.
			</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
					<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#post-install-machine-configuration-tasks">Post-installation machine configuration tasks</a>
				</li></ul></div><section class="section" id="configure-additional-devices-using-mco_post-install-configure-additional-devices-ibmz"><div class="titlepage"><div><div><h2 class="title">15.1. Configuring additional devices using the Machine Config Operator (MCO)</h2></div></div></div><p>
				Tasks in this section describe how to use features of the Machine Config Operator (MCO) to configure additional devices in an IBM Z or IBM® LinuxONE environment. Configuring devices with the MCO is persistent but only allows specific configurations for compute nodes. MCO does not allow control plane nodes to have different configurations.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You are logged in to the cluster as a user with administrative privileges.
					</li><li class="listitem">
						The device must be available to the z/VM guest.
					</li><li class="listitem">
						The device is already attached.
					</li><li class="listitem">
						The device is not included in the <code class="literal">cio_ignore</code> list, which can be set in the kernel parameters.
					</li><li class="listitem"><p class="simpara">
						You have created a <code class="literal">MachineConfig</code> object file with the following YAML:
					</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker0
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker0]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker0: ""</pre></li></ul></div><section class="section" id="configuring-fcp-host"><div class="titlepage"><div><div><h3 class="title">15.1.1. Configuring a Fibre Channel Protocol (FCP) host</h3></div></div></div><p>
					The following is an example of how to configure an FCP host adapter with N_Port Identifier Virtualization (NPIV) by adding a udev rule.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Take the following sample udev rule <code class="literal">441-zfcp-host-0.0.8000.rules</code>:
						</p><pre class="programlisting language-terminal">ACTION=="add", SUBSYSTEM=="ccw", KERNEL=="0.0.8000", DRIVER=="zfcp", GOTO="cfg_zfcp_host_0.0.8000"
ACTION=="add", SUBSYSTEM=="drivers", KERNEL=="zfcp", TEST=="[ccw/0.0.8000]", GOTO="cfg_zfcp_host_0.0.8000"
GOTO="end_zfcp_host_0.0.8000"

LABEL="cfg_zfcp_host_0.0.8000"
ATTR{[ccw/0.0.8000]online}="1"

LABEL="end_zfcp_host_0.0.8000"</pre></li><li class="listitem"><p class="simpara">
							Convert the rule to Base64 encoded by running the following command:
						</p><pre class="programlisting language-terminal">$ base64 /path/to/file/</pre></li><li class="listitem"><p class="simpara">
							Copy the following MCO sample profile into a YAML file:
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
   labels:
     machineconfiguration.openshift.io/role: worker0 <span id="CO157-1"><!--Empty--></span><span class="callout">1</span>
   name: 99-worker0-devices
spec:
   config:
     ignition:
       version: 3.2.0
     storage:
       files:
       - contents:
           source: data:text/plain;base64,&lt;encoded_base64_string&gt; <span id="CO157-2"><!--Empty--></span><span class="callout">2</span>
         filesystem: root
         mode: 420
         path: /etc/udev/rules.d/41-zfcp-host-0.0.8000.rules <span id="CO157-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO157-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The role you have defined in the machine config file.
								</div></dd><dt><a href="#CO157-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The Base64 encoded string that you have generated in the previous step.
								</div></dd><dt><a href="#CO157-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The path where the udev rule is located.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="configuring-fcp-lun"><div class="titlepage"><div><div><h3 class="title">15.1.2. Configuring an FCP LUN</h3></div></div></div><p>
					The following is an example of how to configure an FCP LUN by adding a udev rule. You can add new FCP LUNs or add additional paths to LUNs that are already configured with multipathing.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Take the following sample udev rule <code class="literal">41-zfcp-lun-0.0.8000:0x500507680d760026:0x00bc000000000000.rules</code>:
						</p><pre class="programlisting language-terminal">ACTION=="add", SUBSYSTEMS=="ccw", KERNELS=="0.0.8000", GOTO="start_zfcp_lun_0.0.8207"
GOTO="end_zfcp_lun_0.0.8000"

LABEL="start_zfcp_lun_0.0.8000"
SUBSYSTEM=="fc_remote_ports", ATTR{port_name}=="0x500507680d760026", GOTO="cfg_fc_0.0.8000_0x500507680d760026"
GOTO="end_zfcp_lun_0.0.8000"

LABEL="cfg_fc_0.0.8000_0x500507680d760026"
ATTR{[ccw/0.0.8000]0x500507680d760026/unit_add}="0x00bc000000000000"
GOTO="end_zfcp_lun_0.0.8000"

LABEL="end_zfcp_lun_0.0.8000"</pre></li><li class="listitem"><p class="simpara">
							Convert the rule to Base64 encoded by running the following command:
						</p><pre class="programlisting language-terminal">$ base64 /path/to/file/</pre></li><li class="listitem"><p class="simpara">
							Copy the following MCO sample profile into a YAML file:
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
   labels:
     machineconfiguration.openshift.io/role: worker0 <span id="CO158-1"><!--Empty--></span><span class="callout">1</span>
   name: 99-worker0-devices
spec:
   config:
     ignition:
       version: 3.2.0
     storage:
       files:
       - contents:
           source: data:text/plain;base64,&lt;encoded_base64_string&gt; <span id="CO158-2"><!--Empty--></span><span class="callout">2</span>
         filesystem: root
         mode: 420
         path: /etc/udev/rules.d/41-zfcp-lun-0.0.8000:0x500507680d760026:0x00bc000000000000.rules <span id="CO158-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO158-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The role you have defined in the machine config file.
								</div></dd><dt><a href="#CO158-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The Base64 encoded string that you have generated in the previous step.
								</div></dd><dt><a href="#CO158-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The path where the udev rule is located.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="configuring-dasd"><div class="titlepage"><div><div><h3 class="title">15.1.3. Configuring DASD</h3></div></div></div><p>
					The following is an example of how to configure a DASD device by adding a udev rule.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Take the following sample udev rule <code class="literal">41-dasd-eckd-0.0.4444.rules</code>:
						</p><pre class="programlisting language-terminal">ACTION=="add", SUBSYSTEM=="ccw", KERNEL=="0.0.4444", DRIVER=="dasd-eckd", GOTO="cfg_dasd_eckd_0.0.4444"
ACTION=="add", SUBSYSTEM=="drivers", KERNEL=="dasd-eckd", TEST=="[ccw/0.0.4444]", GOTO="cfg_dasd_eckd_0.0.4444"
GOTO="end_dasd_eckd_0.0.4444"

LABEL="cfg_dasd_eckd_0.0.4444"
ATTR{[ccw/0.0.4444]online}="1"

LABEL="end_dasd_eckd_0.0.4444"</pre></li><li class="listitem"><p class="simpara">
							Convert the rule to Base64 encoded by running the following command:
						</p><pre class="programlisting language-terminal">$ base64 /path/to/file/</pre></li><li class="listitem"><p class="simpara">
							Copy the following MCO sample profile into a YAML file:
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
   labels:
     machineconfiguration.openshift.io/role: worker0 <span id="CO159-1"><!--Empty--></span><span class="callout">1</span>
   name: 99-worker0-devices
spec:
   config:
     ignition:
       version: 3.2.0
     storage:
       files:
       - contents:
           source: data:text/plain;base64,&lt;encoded_base64_string&gt; <span id="CO159-2"><!--Empty--></span><span class="callout">2</span>
         filesystem: root
         mode: 420
         path: /etc/udev/rules.d/41-dasd-eckd-0.0.4444.rules <span id="CO159-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO159-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The role you have defined in the machine config file.
								</div></dd><dt><a href="#CO159-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The Base64 encoded string that you have generated in the previous step.
								</div></dd><dt><a href="#CO159-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The path where the udev rule is located.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="configuring-qeth"><div class="titlepage"><div><div><h3 class="title">15.1.4. Configuring qeth</h3></div></div></div><p>
					The following is an example of how to configure a qeth device by adding a udev rule.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Take the following sample udev rule <code class="literal">41-qeth-0.0.1000.rules</code>:
						</p><pre class="programlisting language-terminal">ACTION=="add", SUBSYSTEM=="drivers", KERNEL=="qeth", GOTO="group_qeth_0.0.1000"
ACTION=="add", SUBSYSTEM=="ccw", KERNEL=="0.0.1000", DRIVER=="qeth", GOTO="group_qeth_0.0.1000"
ACTION=="add", SUBSYSTEM=="ccw", KERNEL=="0.0.1001", DRIVER=="qeth", GOTO="group_qeth_0.0.1000"
ACTION=="add", SUBSYSTEM=="ccw", KERNEL=="0.0.1002", DRIVER=="qeth", GOTO="group_qeth_0.0.1000"
ACTION=="add", SUBSYSTEM=="ccwgroup", KERNEL=="0.0.1000", DRIVER=="qeth", GOTO="cfg_qeth_0.0.1000"
GOTO="end_qeth_0.0.1000"

LABEL="group_qeth_0.0.1000"
TEST=="[ccwgroup/0.0.1000]", GOTO="end_qeth_0.0.1000"
TEST!="[ccw/0.0.1000]", GOTO="end_qeth_0.0.1000"
TEST!="[ccw/0.0.1001]", GOTO="end_qeth_0.0.1000"
TEST!="[ccw/0.0.1002]", GOTO="end_qeth_0.0.1000"
ATTR{[drivers/ccwgroup:qeth]group}="0.0.1000,0.0.1001,0.0.1002"
GOTO="end_qeth_0.0.1000"

LABEL="cfg_qeth_0.0.1000"
ATTR{[ccwgroup/0.0.1000]online}="1"

LABEL="end_qeth_0.0.1000"</pre></li><li class="listitem"><p class="simpara">
							Convert the rule to Base64 encoded by running the following command:
						</p><pre class="programlisting language-terminal">$ base64 /path/to/file/</pre></li><li class="listitem"><p class="simpara">
							Copy the following MCO sample profile into a YAML file:
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
   labels:
     machineconfiguration.openshift.io/role: worker0 <span id="CO160-1"><!--Empty--></span><span class="callout">1</span>
   name: 99-worker0-devices
spec:
   config:
     ignition:
       version: 3.2.0
     storage:
       files:
       - contents:
           source: data:text/plain;base64,&lt;encoded_base64_string&gt; <span id="CO160-2"><!--Empty--></span><span class="callout">2</span>
         filesystem: root
         mode: 420
         path: /etc/udev/rules.d/41-dasd-eckd-0.0.4444.rules <span id="CO160-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO160-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The role you have defined in the machine config file.
								</div></dd><dt><a href="#CO160-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The Base64 encoded string that you have generated in the previous step.
								</div></dd><dt><a href="#CO160-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The path where the udev rule is located.
								</div></dd></dl></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/storage/#persistent-storage-using-local-volume">Install and configure the Local Storage Operator</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#k8s-nmstate-updating-node-network-config">Updating node network configuration</a>
						</li></ul></div></section></section><section class="section" id="configure-additional-devices-manually_post-install-configure-additional-devices-ibmz"><div class="titlepage"><div><div><h2 class="title">15.2. Configuring additional devices manually</h2></div></div></div><p>
				Tasks in this section describe how to manually configure additional devices in an IBM Z or IBM® LinuxONE environment. This configuration method is persistent over node restarts but not OpenShift Container Platform native and you need to redo the steps if you replace the node.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You are logged in to the cluster as a user with administrative privileges.
					</li><li class="listitem">
						The device must be available to the node.
					</li><li class="listitem">
						In a z/VM environment, the device must be attached to the z/VM guest.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Connect to the node via SSH by running the following command:
					</p><pre class="programlisting language-terminal">$ ssh &lt;user&gt;@&lt;node_ip_address&gt;</pre><p class="simpara">
						You can also start a debug session to the node by running the following command:
					</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
						To enable the devices with the <code class="literal">chzdev</code> command, enter the following command:
					</p><pre class="programlisting language-terminal">$ sudo chzdev -e 0.0.8000
  sudo chzdev -e 1000-1002
  sude chzdev -e 4444
  sudo chzdev -e 0.0.8000:0x500507680d760026:0x00bc000000000000</pre></li></ol></div><div class="_additional-resources _additional-resources"><p class="title"><strong>Additional resources</strong></p><p>
					See <a class="link" href="https://www.ibm.com/docs/en/linux-on-systems?topic=linuxonibm/com.ibm.linux.z.ludd/ludd_c_perscfg.html">Persistent device configuration</a> in IBM Documentation.
				</p></div></section><section class="section" id="roce-network-cards"><div class="titlepage"><div><div><h2 class="title">15.3. RoCE network Cards</h2></div></div></div><p>
				RoCE (RDMA over Converged Ethernet) network cards do not need to be enabled and their interfaces can be configured with the Kubernetes NMState Operator whenever they are available in the node. For example, RoCE network cards are available if they are attached in a z/VM environment or passed through in a RHEL KVM environment.
			</p></section><section class="section" id="enabling-multipathing-fcp-luns_post-install-configure-additional-devices-ibmz"><div class="titlepage"><div><div><h2 class="title">15.4. Enabling multipathing for FCP LUNs</h2></div></div></div><p>
				Tasks in this section describe how to manually configure additional devices in an IBM Z or IBM® LinuxONE environment. This configuration method is persistent over node restarts but not OpenShift Container Platform native and you need to redo the steps if you replace the node.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					On IBM Z and IBM® LinuxONE, you can enable multipathing only if you configured your cluster for it during installation. For more information, see "Installing RHCOS and starting the OpenShift Container Platform bootstrap process" in <span class="emphasis"><em>Installing a cluster with z/VM on IBM Z and IBM® LinuxONE</em></span>.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You are logged in to the cluster as a user with administrative privileges.
					</li><li class="listitem">
						You have configured multiple paths to a LUN with either method explained above.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Connect to the node via SSH by running the following command:
					</p><pre class="programlisting language-terminal">$ ssh &lt;user&gt;@&lt;node_ip_address&gt;</pre><p class="simpara">
						You can also start a debug session to the node by running the following command:
					</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
						To enable multipathing, run the following command:
					</p><pre class="programlisting language-terminal">$ sudo /sbin/mpathconf --enable</pre></li><li class="listitem"><p class="simpara">
						To start the <code class="literal">multipathd</code> daemon, run the following command:
					</p><pre class="programlisting language-terminal">$ sudo multipath</pre></li><li class="listitem"><p class="simpara">
						Optional: To format your multipath device with fdisk, run the following command:
					</p><pre class="programlisting language-terminal">$ sudo fdisk /dev/mapper/mpatha</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						To verify that the devices have been grouped, run the following command:
					</p><pre class="programlisting language-terminal">$ sudo multipath -II</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">mpatha (20017380030290197) dm-1 IBM,2810XIV
   size=512G features='1 queue_if_no_path' hwhandler='1 alua' wp=rw
	-+- policy='service-time 0' prio=50 status=enabled
 	|- 1:0:0:6  sde 68:16  active ready running
 	|- 1:0:1:6  sdf 69:24  active ready running
 	|- 0:0:0:6  sdg  8:80  active ready running
 	`- 0:0:1:6  sdh 66:48  active ready running</pre>

						</p></div></li></ul></div><div class="itemizedlist"><p class="title"><strong>Next steps</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/storage/#persistent-storage-using-local-volume">Install and configure the Local Storage Operator</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#k8s-nmstate-updating-node-network-config">Updating node network configuration</a>
					</li></ul></div></section></section><section class="chapter" id="post-install-vsphere-zones-regions-configuration"><div class="titlepage"><div><div><h1 class="title">Chapter 16. Multiple regions and zones configuration for a cluster on vSphere</h1></div></div></div><p>
			As an administrator, you can specify multiple regions and zones for your OpenShift Container Platform cluster that runs on a VMware vSphere instance. This configuration reduces the risk of a hardware failure or network outage causing your cluster to fail.
		</p><p>
			A failure domain configuration lists parameters that create a topology. The following list states some of these parameters:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<code class="literal">computeCluster</code>
				</li><li class="listitem">
					<code class="literal">datacenter</code>
				</li><li class="listitem">
					<code class="literal">datastore</code>
				</li><li class="listitem">
					<code class="literal">networks</code>
				</li><li class="listitem">
					<code class="literal">resourcePool</code>
				</li></ul></div><p>
			After you define multiple regions and zones for your OpenShift Container Platform cluster, you can create or migrate nodes to another failure domain.
		</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				If you want to migrate pre-existing OpenShift Container Platform cluster compute nodes to a failure domain, you must define a new compute machine set for the compute node. This new machine set can scale up a compute node according to the topology of the failure domain, and scale down the pre-existing compute node.
			</p><p>
				The cloud provider adds <code class="literal">topology.kubernetes.io/zone</code> and <code class="literal">topology.kubernetes.io/region</code> labels to any compute node provisioned by a machine set resource.
			</p><p>
				For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-a-compute-machine-set-on-vsphere">Creating a compute machine set</a>.
			</p></div></div><section class="section" id="specifying-regions-zones-infrastructure-vsphere_post-install-vsphere-zones-regions-configuration"><div class="titlepage"><div><div><h2 class="title">16.1. Specifying multiple regions and zones for your cluster on vSphere</h2></div></div></div><p>
				You can configure the <code class="literal">infrastructures.config.openshift.io</code> configuration resource to specify multiple regions and zones for your OpenShift Container Platform cluster that runs on a VMware vSphere instance.
			</p><p>
				Topology-aware features for the cloud controller manager and the vSphere Container Storage Interface (CSI) Operator Driver require information about the vSphere topology where you host your OpenShift Container Platform cluster. This topology information exists in the <code class="literal">infrastructures.config.openshift.io</code> configuration resource.
			</p><p>
				Before you specify regions and zones for your cluster, you must ensure that all datacenters and compute clusters contain tags, so that the cloud provider can add labels to your node. For example, if <code class="literal">datacenter-1</code> represents <code class="literal">region-a</code> and <code class="literal">compute-cluster-1</code> represents <code class="literal">zone-1</code>, the cloud provider adds an <code class="literal">openshift-region</code> category label with a value of <code class="literal">region-a</code> to <code class="literal">datacenter-1</code>. Additionally, the cloud provider adds an <code class="literal">openshift-zone</code> category tag with a value of <code class="literal">zone-1</code> to <code class="literal">compute-cluster-1</code>.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					You can migrate control plane nodes with vMotion capabilities to a failure domain. After you add these nodes to a failure domain, the cloud provider adds <code class="literal">topology.kubernetes.io/zone</code> and <code class="literal">topology.kubernetes.io/region</code> labels to these nodes.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You created the <code class="literal">openshift-region</code> and <code class="literal">openshift-zone</code> tag categories on the vCenter server.
					</li><li class="listitem">
						You ensured that each datacenter and compute cluster contains tags that represent the name of their associated region or zone, or both.
					</li><li class="listitem">
						Optional: If you defined <span class="strong strong"><strong>API</strong></span> and <span class="strong strong"><strong>Ingress</strong></span> static IP addresses to the installation program, you must ensure that all regions and zones share a common layer 2 network. This configuration ensures that API and Ingress Virtual IP (VIP) addresses can interact with your cluster.
					</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					If you do not supply tags to all datacenters and compute clusters before you create a node or migrate a node, the cloud provider cannot add the <code class="literal">topology.kubernetes.io/zone</code> and <code class="literal">topology.kubernetes.io/region</code> labels to the node. This means that services cannot route traffic to your node.
				</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Edit the <code class="literal">infrastructures.config.openshift.io</code> custom resource definition (CRD) of your cluster to specify multiple regions and zones in the <code class="literal">failureDomains</code> section of the resource by running the following command:
					</p><pre class="programlisting language-terminal">$ oc edit infrastructures.config.openshift.io cluster</pre><div class="formalpara"><p class="title"><strong>Example <code class="literal">infrastructures.config.openshift.io</code> CRD for a instance named <code class="literal">cluster</code> with multiple regions and zones defined in its configuration</strong></p><p>
							
<pre class="programlisting language-yaml">spec:
  cloudConfig:
    key: config
    name: cloud-provider-config
  platformSpec:
    type: vSphere
    vsphere:
      vcenters:
        - datacenters:
            - &lt;region_a_datacenter&gt;
            - &lt;region_b_datacenter&gt;
          port: 443
          server: &lt;your_vcenter_server&gt;
      failureDomains:
        - name: &lt;failure_domain_1&gt;
          region: &lt;region_a&gt;
          zone: &lt;zone_a&gt;
          server: &lt;your_vcenter_server&gt;
          topology:
            datacenter: &lt;region_a_dc&gt;
            computeCluster: "&lt;/region_a_dc/host/zone_a_cluster&gt;"
            resourcePool: "&lt;/region_a_dc/host/zone_a_cluster/Resources/resource_pool&gt;"
            datastore: "&lt;/region_a_dc/datastore/datastore_a&gt;"
            networks:
            - port-group
        - name: &lt;failure_domain_2&gt;
          region: &lt;region_a&gt;
          zone: &lt;zone_b&gt;
          server: &lt;your_vcenter_server&gt;
          topology:
            computeCluster: &lt;/region_a_dc/host/zone_b_cluster&gt;
            datacenter: &lt;region_a_dc&gt;
            datastore: &lt;/region_a_dc/datastore/datastore_a&gt;
            networks:
            - port-group
        - name: &lt;failure_domain_3&gt;
          region: &lt;region_b&gt;
          zone: &lt;zone_a&gt;
          server: &lt;your_vcenter_server&gt;
          topology:
            computeCluster: &lt;/region_b_dc/host/zone_a_cluster&gt;
            datacenter: &lt;region_b_dc&gt;
            datastore: &lt;/region_b_dc/datastore/datastore_b&gt;
            networks:
            - port-group
      nodeNetworking:
        external: {}
        internal: {}</pre>

						</p></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							After you create a failure domain and you define it in a CRD for a VMware vSphere cluster, you must not modify or delete the failure domain. Doing any of these actions with this configuration can impact the availability and fault tolerance of a control plane machine.
						</p></div></div></li><li class="listitem">
						Save the resource file to apply the changes.
					</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#references-regions-zones-infrastructure-vsphere_post-install-vsphere-zones-regions-configuration">Parameters for the cluster-wide infrastructure CRD</a>
					</li></ul></div></section><section class="section" id="vsphere-enabling-multiple-layer2-network_post-install-vsphere-zones-regions-configuration"><div class="titlepage"><div><div><h2 class="title">16.2. Enabling a multiple layer 2 network for your cluster</h2></div></div></div><p>
				You can configure your cluster to use a multiple layer 2 network configuration so that data transfer among nodes can span across multiple networks.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You configured network connectivity among machines so that cluster components can communicate with each other.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						If you installed your cluster with installer-provisioned infrastructure, you must ensure that all control plane nodes share a common layer 2 network. Additionally, ensure compute nodes that are configured for Ingress pod scheduling share a common layer 2 network.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								If you need compute nodes to span multiple layer 2 networks, you can create infrastructure nodes that can host Ingress pods.
							</li><li class="listitem">
								If you need to provision workloads across additional layer 2 networks, you can create compute machine sets on vSphere and then move these workloads to your target layer 2 networks.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						If you installed your cluster on infrastructure that you provided, which is defined as a user-provisioned infrastructure, complete the following actions to meet your needs:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Configure your API load balancer and network so that the load balancer can reach the API and Machine Config Server on the control plane nodes.
							</li><li class="listitem">
								Configure your Ingress load balancer and network so that the load balancer can reach the Ingress pods on the compute or infrastructure nodes.
							</li></ul></div></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-network-connectivity-user-infra_installing-vsphere-network-customizations">Network connectivity requirements</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-infrastructure-machinesets-production">Creating infrastructure machine sets for production environments</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machineset-creating_creating-machineset-vsphere">Creating a compute machine set</a>
					</li></ul></div></section><section class="section" id="references-regions-zones-infrastructure-vsphere_post-install-vsphere-zones-regions-configuration"><div class="titlepage"><div><div><h2 class="title">16.3. Parameters for the cluster-wide infrastructure CRD</h2></div></div></div><p>
				You must set values for specific parameters in the cluster-wide infrastructure, <code class="literal">infrastructures.config.openshift.io</code>, Custom Resource Definition (CRD) to define multiple regions and zones for your OpenShift Container Platform cluster that runs on a VMware vSphere instance.
			</p><p>
				The following table lists mandatory parameters for defining multiple regions and zones for your OpenShift Container Platform cluster:
			</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 67%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140031643925312" scope="col">Parameter</th><th align="left" valign="top" id="idm140031643924224" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140031643925312"> <p>
								<code class="literal">vcenters</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140031643924224"> <p>
								The vCenter server for your OpenShift Container Platform cluster. You can only specify one vCenter for your cluster.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031643925312"> <p>
								<code class="literal">datacenters</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140031643924224"> <p>
								vCenter datacenters where VMs associated with the OpenShift Container Platform cluster will be created or presently exist.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031643925312"> <p>
								<code class="literal">port</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140031643924224"> <p>
								The TCP port of the vCenter server.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031643925312"> <p>
								<code class="literal">server</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140031643924224"> <p>
								The fully qualified domain name (FQDN) of the vCenter server.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031643925312"> <p>
								<code class="literal">failureDomains</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140031643924224"> <p>
								The list of failure domains.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031643925312"> <p>
								<code class="literal">name</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140031643924224"> <p>
								The name of the failure domain.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031643925312"> <p>
								<code class="literal">region</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140031643924224"> <p>
								The value of the <code class="literal">openshift-region</code> tag assigned to the topology for the failure failure domain.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031643925312"> <p>
								<code class="literal">zone</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140031643924224"> <p>
								The value of the <code class="literal">openshift-zone</code> tag assigned to the topology for the failure failure domain.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031643925312"> <p>
								<code class="literal">topology</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140031643924224"> <p>
								The vCenter reources associated with the failure domain.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031643925312"> <p>
								<code class="literal">datacenter</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140031643924224"> <p>
								The datacenter associated with the failure domain.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031643925312"> <p>
								<code class="literal">computeCluster</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140031643924224"> <p>
								The full path of the compute cluster associated with the failure domain.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031643925312"> <p>
								<code class="literal">resourcePool</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140031643924224"> <p>
								The full path of the resource pool associated with the failure domain.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031643925312"> <p>
								<code class="literal">datastore</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140031643924224"> <p>
								The full path of the datastore associated with the failure domain.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140031643925312"> <p>
								<code class="literal">networks</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140031643924224"> <p>
								A list of port groups associated with the failure domain. Only one portgroup may be defined.
							</p>
							 </td></tr></tbody></table></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#specifying-regions-zones-infrastructure-vsphere_post-install-vsphere-zones-regions-configuration">Specifying multiple regions and zones for your cluster on vSphere</a>
					</li></ul></div></section></section><section class="chapter" id="coreos-layering"><div class="titlepage"><div><div><h1 class="title">Chapter 17. RHCOS image layering</h1></div></div></div><p>
			Red Hat Enterprise Linux CoreOS (RHCOS) image layering allows you to easily extend the functionality of your base RHCOS image by <span class="emphasis"><em>layering</em></span> additional images onto the base image. This layering does not modify the base RHCOS image. Instead, it creates a <span class="emphasis"><em>custom layered image</em></span> that includes all RHCOS functionality and adds additional functionality to specific nodes in the cluster.
		</p><p>
			You create a custom layered image by using a Containerfile and applying it to nodes by using a <code class="literal">MachineConfig</code> object. The Machine Config Operator overrides the base RHCOS image, as specified by the <code class="literal">osImageURL</code> value in the associated machine config, and boots the new image. You can remove the custom layered image by deleting the machine config, The MCO reboots the nodes back to the base RHCOS image.
		</p><p>
			With RHCOS image layering, you can install RPMs into your base image, and your custom content will be booted alongside RHCOS. The Machine Config Operator (MCO) can roll out these custom layered images and monitor these custom containers in the same way it does for the default RHCOS image. RHCOS image layering gives you greater flexibility in how you manage your RHCOS nodes.
		</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				Installing realtime kernel and extensions RPMs as custom layered content is not recommended. This is because these RPMs can conflict with RPMs installed by using a machine config. If there is a conflict, the MCO enters a <code class="literal">degraded</code> state when it tries to install the machine config RPM. You need to remove the conflicting extension from your machine config before proceeding.
			</p></div></div><p>
			As soon as you apply the custom layered image to your cluster, you effectively <span class="emphasis"><em>take ownership</em></span> of your custom layered images and those nodes. While Red Hat remains responsible for maintaining and updating the base RHCOS image on standard nodes, you are responsible for maintaining and updating images on nodes that use a custom layered image. You assume the responsibility for the package you applied with the custom layered image and any issues that might arise with the package.
		</p><p>
			To apply a custom layered image, you create a Containerfile that references an OpenShift Container Platform image and the RPM that you want to apply. You then push the resulting custom layered image to an image registry. In a non-production OpenShift Container Platform cluster, create a <code class="literal">MachineConfig</code> object for the targeted node pool that points to the new image.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				Use the same base RHCOS image installed on the rest of your cluster. Use the <code class="literal">oc adm release info --image-for rhel-coreos</code> command to obtain the base image used in your cluster.
			</p></div></div><p>
			RHCOS image layering allows you to use the following types of images to create custom layered images:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
					<span class="strong strong"><strong>OpenShift Container Platform Hotfixes</strong></span>. You can work with Customer Experience and Engagement (CEE) to obtain and apply <a class="link" href="https://access.redhat.com/solutions/2996001">Hotfix packages</a> on top of your RHCOS image. In some instances, you might want a bug fix or enhancement before it is included in an official OpenShift Container Platform release. RHCOS image layering allows you to easily add the Hotfix before it is officially released and remove the Hotfix when the underlying RHCOS image incorporates the fix.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Some Hotfixes require a Red Hat Support Exception and are outside of the normal scope of OpenShift Container Platform support coverage or life cycle policies.
					</p></div></div><p class="simpara">
					In the event you want a Hotfix, it will be provided to you based on <a class="link" href="https://access.redhat.com/solutions/2996001">Red Hat Hotfix policy</a>. Apply it on top of the base image and test that new custom layered image in a non-production environment. When you are satisfied that the custom layered image is safe to use in production, you can roll it out on your own schedule to specific node pools. For any reason, you can easily roll back the custom layered image and return to using the default RHCOS.
				</p><div class="formalpara"><p class="title"><strong>Example Containerfile to apply a Hotfix</strong></p><p>
						
<pre class="programlisting language-yaml"># Using a 4.12.0 image
FROM quay.io/openshift-release-dev/ocp-release@sha256...
#Install hotfix rpm
RUN rpm-ostree override replace https://example.com/myrepo/haproxy-1.0.16-5.el8.src.rpm &amp;&amp; \
    rpm-ostree cleanup -m &amp;&amp; \
    ostree container commit</pre>

					</p></div></li><li class="listitem"><p class="simpara">
					<span class="strong strong"><strong>RHEL packages</strong></span>. You can download Red Hat Enterprise Linux (RHEL) packages from the <a class="link" href="https://access.redhat.com/downloads/content/479/ver=/rhel---9/9.1/x86_64/packages">Red Hat Customer Portal</a>, such as chrony, firewalld, and iputils.
				</p><div class="formalpara"><p class="title"><strong>Example Containerfile to apply the firewalld utility</strong></p><p>
						
<pre class="programlisting language-yaml">FROM quay.io/openshift-release-dev/ocp-release@sha256...
ADD configure-firewall-playbook.yml .
RUN rpm-ostree install firewalld ansible &amp;&amp; \
    ansible-playbook configure-firewall-playbook.yml &amp;&amp; \
    rpm -e ansible &amp;&amp; \
    ostree container commit</pre>

					</p></div><div class="formalpara"><p class="title"><strong>Example Containerfile to apply the libreswan utility</strong></p><p>
						
<pre class="programlisting language-yaml"># Get RHCOS base image of target cluster `oc adm release info --image-for rhel-coreos`
# hadolint ignore=DL3006
FROM quay.io/openshift-release/ocp-release@sha256...

# Install our config file
COPY my-host-to-host.conf /etc/ipsec.d/

# RHEL entitled host is needed here to access RHEL packages
# Install libreswan as extra RHEL package
RUN rpm-ostree install libreswan &amp;&amp; \
    systemctl enable ipsec &amp;&amp; \
    ostree container commit</pre>

					</p></div><p class="simpara">
					Because libreswan requires additional RHEL packages, the image must be built on an entitled RHEL host.
				</p></li><li class="listitem"><p class="simpara">
					<span class="strong strong"><strong>Third-party packages</strong></span>. You can download and install RPMs from third-party organizations, such as the following types of packages:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
							Bleeding edge drivers and kernel enhancements to improve performance or add capabilities.
						</li><li class="listitem">
							Forensic client tools to investigate possible and actual break-ins.
						</li><li class="listitem">
							Security agents.
						</li><li class="listitem">
							Inventory agents that provide a coherent view of the entire cluster.
						</li><li class="listitem">
							SSH Key management packages.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Example Containerfile to apply a third-party package from EPEL</strong></p><p>
						
<pre class="programlisting language-yaml"># Get RHCOS base image of target cluster `oc adm release info --image-for rhel-coreos`
# hadolint ignore=DL3006
FROM quay.io/openshift-release/ocp-release@sha256...

# Install our config file
COPY my-host-to-host.conf /etc/ipsec.d/

# RHEL entitled host is needed here to access RHEL packages
# Install libreswan as extra RHEL package
RUN rpm-ostree install libreswan &amp;&amp; \
    systemctl enable ipsec &amp;&amp; \
    ostree container commit</pre>

					</p></div><div class="formalpara"><p class="title"><strong>Example Containerfile to apply a third-party package that has RHEL dependencies</strong></p><p>
						
<pre class="programlisting language-yaml"># Get RHCOS base image of target cluster `oc adm release info --image-for rhel-coreos`
# hadolint ignore=DL3006
FROM quay.io/openshift-release/ocp-release@sha256...

# Install our config file
COPY my-host-to-host.conf /etc/ipsec.d/

# RHEL entitled host is needed here to access RHEL packages
# Install libreswan as extra RHEL package
RUN rpm-ostree install libreswan &amp;&amp; \
    systemctl enable ipsec &amp;&amp; \
    ostree container commit</pre>

					</p></div><p class="simpara">
					This Containerfile installs the Linux fish program. Because fish requires additional RHEL packages, the image must be built on an entitled RHEL host.
				</p></li></ul></div><p>
			After you create the machine config, the Machine Config Operator (MCO) performs the following steps:
		</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
					Renders a new machine config for the specified pool or pools.
				</li><li class="listitem">
					Performs cordon and drain operations on the nodes in the pool or pools.
				</li><li class="listitem">
					Writes the rest of the machine config parameters onto the nodes.
				</li><li class="listitem">
					Applies the custom layered image to the node.
				</li><li class="listitem">
					Reboots the node using the new image.
				</li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				It is strongly recommended that you test your images outside of your production environment before rolling out to your cluster.
			</p></div></div><section class="section" id="coreos-layering-configuring_coreos-layering"><div class="titlepage"><div><div><h2 class="title">17.1. Applying a RHCOS custom layered image</h2></div></div></div><p>
				You can easily configure Red Hat Enterprise Linux CoreOS (RHCOS) image layering on the nodes in specific machine config pools. The Machine Config Operator (MCO) reboots those nodes with the new custom layered image, overriding the base Red Hat Enterprise Linux CoreOS (RHCOS) image.
			</p><p>
				To apply a custom layered image to your cluster, you must have the custom layered image in a repository that your cluster can access. Then, create a <code class="literal">MachineConfig</code> object that points to the custom layered image. You need a separate <code class="literal">MachineConfig</code> object for each machine config pool that you want to configure.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					When you configure a custom layered image, OpenShift Container Platform no longer automatically updates any node that uses the custom layered image. You become responsible for manually updating your nodes as appropriate. If you roll back the custom layer, OpenShift Container Platform will again automatically update the node. See the Additional resources section that follows for important information about updating nodes that use a custom layered image.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						You must create a custom layered image that is based on an OpenShift Container Platform image digest, not a tag.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You should use the same base RHCOS image that is installed on the rest of your cluster. Use the <code class="literal">oc adm release info --image-for rhel-coreos</code> command to obtain the base image being used in your cluster.
						</p></div></div><p class="simpara">
						For example, the following Containerfile creates a custom layered image from an OpenShift Container Platform 4.13 image and overrides the kernel package with one from CentOS 9 Stream:
					</p><div class="formalpara"><p class="title"><strong>Example Containerfile for a custom layer image</strong></p><p>
							
<pre class="programlisting language-yaml"># Using a 4.12.0 image
FROM quay.io/openshift-release/ocp-release@sha256... <span id="CO161-1"><!--Empty--></span><span class="callout">1</span>
#Install hotfix rpm
RUN rpm-ostree override cliwrap install-to-root / &amp;&amp; \ <span id="CO161-2"><!--Empty--></span><span class="callout">2</span>
    rpm-ostree override replace http://mirror.stream.centos.org/9-stream/BaseOS/x86_64/os/Packages/kernel-{,core-,modules-,modules-core-,modules-extra-}5.14.0-295.el9.x86_64.rpm &amp;&amp; \ <span id="CO161-3"><!--Empty--></span><span class="callout">3</span>
    rpm-ostree cleanup -m &amp;&amp; \
    ostree container commit</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO161-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specifies the RHCOS base image of your cluster.
							</div></dd><dt><a href="#CO161-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Enables <code class="literal">cliwrap</code>. This is currently required to intercept some command invocations made from kernel scripts.
							</div></dd><dt><a href="#CO161-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Replaces the kernel packages.
							</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Instructions on how to create a Containerfile are beyond the scope of this documentation.
						</p></div></div></li><li class="listitem">
						Because the process for building a custom layered image is performed outside of the cluster, you must use the <code class="literal">--authfile /path/to/pull-secret</code> option with Podman or Buildah. Alternatively, to have the pull secret read by these tools automatically, you can add it to one of the default file locations: <code class="literal">~/.docker/config.json</code>, <code class="literal">$XDG_RUNTIME_DIR/containers/auth.json</code>, <code class="literal">~/.docker/config.json</code>, or <code class="literal">~/.dockercfg</code>. Refer to the <code class="literal">containers-auth.json</code> man page for more information.
					</li><li class="listitem">
						You must push the custom layered image to a repository that your cluster can access.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a machine config file.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create a YAML file similar to the following:
							</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker <span id="CO162-1"><!--Empty--></span><span class="callout">1</span>
  name: os-layer-custom
spec:
  osImageURL: quay.io/my-registry/custom-image@sha256... <span id="CO162-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO162-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specifies the machine config pool to apply the custom layered image.
									</div></dd><dt><a href="#CO162-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specifies the path to the custom layered image in the repository.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">MachineConfig</code> object:
							</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									It is strongly recommended that you test your images outside of your production environment before rolling out to your cluster.
								</p></div></div></li></ol></div></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
					You can verify that the custom layered image is applied by performing any of the following checks:
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Check that the worker machine config pool has rolled out with the new machine config:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Check that the new machine config is created:
							</p><pre class="programlisting language-terminal">$ oc get mc</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
00-master                                          5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
00-worker                                          5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
01-master-container-runtime                        5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
01-master-kubelet                                  5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
01-worker-container-runtime                        5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
01-worker-kubelet                                  5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
99-master-generated-registries                     5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
99-master-ssh                                                                                 3.2.0             98m
99-worker-generated-registries                     5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
99-worker-ssh                                                                                 3.2.0             98m
os-layer-custom                                                                                                 10s <span id="CO163-1"><!--Empty--></span><span class="callout">1</span>
rendered-master-15961f1da260f7be141006404d17d39b   5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
rendered-worker-5aff604cb1381a4fe07feaf1595a797e   5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
rendered-worker-5de4837625b1cbc237de6b22bc0bc873   5bdb57489b720096ef912f738b46330a8f577803   3.2.0             4s  <span id="CO163-2"><!--Empty--></span><span class="callout">2</span></pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO163-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										New machine config
									</div></dd><dt><a href="#CO163-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										New rendered machine config
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Check that the <code class="literal">osImageURL</code> value in the new machine config points to the expected image:
							</p><pre class="programlisting language-terminal">$ oc describe mc rendered-master-4e8be63aef68b843b546827b6ebe0913</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Name:         rendered-master-4e8be63aef68b843b546827b6ebe0913
Namespace:
Labels:       &lt;none&gt;
Annotations:  machineconfiguration.openshift.io/generated-by-controller-version: 8276d9c1f574481043d3661a1ace1f36cd8c3b62
              machineconfiguration.openshift.io/release-image-version: 4.13.0-ec.3
API Version:  machineconfiguration.openshift.io/v1
Kind:         MachineConfig
...
  Os Image URL: quay.io/my-registry/custom-image@sha256...</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Check that the associated machine config pool is updating with the new machine config:
							</p><pre class="programlisting language-terminal">$ oc get mcp</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master   rendered-master-6faecdfa1b25c114a58cf178fbaa45e2   True      False      False      3              3                   3                     0                      39m
worker   rendered-worker-6b000dbc31aaee63c6a2d56d04cd4c1b   False     True       False      3              0                   0                     0                      39m <span id="CO164-1"><!--Empty--></span><span class="callout">1</span></pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO164-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										When the <code class="literal">UPDATING</code> field is <code class="literal">True</code>, the machine config pool is updating with the new machine config. When the field becomes <code class="literal">False</code>, the worker machine config pool has rolled out to the new machine config.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Check the nodes to see that scheduling on the nodes is disabled. This indicates that the change is being applied:
							</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                                         STATUS                     ROLES                  AGE   VERSION
ip-10-0-148-79.us-west-1.compute.internal    Ready                      worker                 32m   v1.26.0
ip-10-0-155-125.us-west-1.compute.internal   Ready,SchedulingDisabled   worker                 35m   v1.26.0
ip-10-0-170-47.us-west-1.compute.internal    Ready                      control-plane,master   42m   v1.26.0
ip-10-0-174-77.us-west-1.compute.internal    Ready                      control-plane,master   42m   v1.26.0
ip-10-0-211-49.us-west-1.compute.internal    Ready                      control-plane,master   42m   v1.26.0
ip-10-0-218-151.us-west-1.compute.internal   Ready                      worker                 31m   v1.26.0</pre>

								</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
						When the node is back in the <code class="literal">Ready</code> state, check that the node is using the custom layered image:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Open an <code class="literal">oc debug</code> session to the node. For example:
							</p><pre class="programlisting language-terminal">$ oc debug node/ip-10-0-155-125.us-west-1.compute.internal</pre></li><li class="listitem"><p class="simpara">
								Set <code class="literal">/host</code> as the root directory within the debug shell:
							</p><pre class="programlisting language-terminal">sh-4.4# chroot /host</pre></li><li class="listitem"><p class="simpara">
								Run the <code class="literal">rpm-ostree status</code> command to view that the custom layered image is in use:
							</p><pre class="programlisting language-terminal">sh-4.4# sudo rpm-ostree status</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="screen">State: idle
Deployments:
* ostree-unverified-registry:quay.io/my-registry/...
                   Digest: sha256:...</pre>

								</p></div></li></ol></div></li></ol></div><div class="formalpara"><p class="title"><strong>Additional resources</strong></p><p>
					<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#coreos-layering-updating_coreos-layering">Updating with a RHCOS custom layered image</a>
				</p></div></section><section class="section" id="coreos-layering-removing_coreos-layering"><div class="titlepage"><div><div><h2 class="title">17.2. Removing a RHCOS custom layered image</h2></div></div></div><p>
				You can easily revert Red Hat Enterprise Linux CoreOS (RHCOS) image layering from the nodes in specific machine config pools. The Machine Config Operator (MCO) reboots those nodes with the cluster base Red Hat Enterprise Linux CoreOS (RHCOS) image, overriding the custom layered image.
			</p><p>
				To remove a Red Hat Enterprise Linux CoreOS (RHCOS) custom layered image from your cluster, you need to delete the machine config that applied the image.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Delete the machine config that applied the custom layered image.
					</p><pre class="programlisting language-terminal">$ oc delete mc os-layer-custom</pre><p class="simpara">
						After deleting the machine config, the nodes reboot.
					</p></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
					You can verify that the custom layered image is removed by performing any of the following checks:
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Check that the worker machine config pool is updating with the previous machine config:
					</p><pre class="programlisting language-terminal">$ oc get mcp</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master   rendered-master-6faecdfa1b25c114a58cf178fbaa45e2   True      False      False      3              3                   3                     0                      39m
worker   rendered-worker-6b000dbc31aaee63c6a2d56d04cd4c1b   False     True       False      3              0                   0                     0                      39m <span id="CO165-1"><!--Empty--></span><span class="callout">1</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO165-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								When the <code class="literal">UPDATING</code> field is <code class="literal">True</code>, the machine config pool is updating with the previous machine config. When the field becomes <code class="literal">False</code>, the worker machine config pool has rolled out to the previous machine config.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Check the nodes to see that scheduling on the nodes is disabled. This indicates that the change is being applied:
					</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                                         STATUS                     ROLES                  AGE   VERSION
ip-10-0-148-79.us-west-1.compute.internal    Ready                      worker                 32m   v1.26.0
ip-10-0-155-125.us-west-1.compute.internal   Ready,SchedulingDisabled   worker                 35m   v1.26.0
ip-10-0-170-47.us-west-1.compute.internal    Ready                      control-plane,master   42m   v1.26.0
ip-10-0-174-77.us-west-1.compute.internal    Ready                      control-plane,master   42m   v1.26.0
ip-10-0-211-49.us-west-1.compute.internal    Ready                      control-plane,master   42m   v1.26.0
ip-10-0-218-151.us-west-1.compute.internal   Ready                      worker                 31m   v1.26.0</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						When the node is back in the <code class="literal">Ready</code> state, check that the node is using the base image:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Open an <code class="literal">oc debug</code> session to the node. For example:
							</p><pre class="programlisting language-terminal">$ oc debug node/ip-10-0-155-125.us-west-1.compute.internal</pre></li><li class="listitem"><p class="simpara">
								Set <code class="literal">/host</code> as the root directory within the debug shell:
							</p><pre class="programlisting language-terminal">sh-4.4# chroot /host</pre></li><li class="listitem"><p class="simpara">
								Run the <code class="literal">rpm-ostree status</code> command to view that the custom layered image is in use:
							</p><pre class="programlisting language-terminal">sh-4.4# sudo rpm-ostree status</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="screen">State: idle
Deployments:
* ostree-unverified-registry:podman pull quay.io/openshift-release-dev/ocp-release@sha256:e2044c3cfebe0ff3a99fc207ac5efe6e07878ad59fd4ad5e41f88cb016dacd73
                   Digest: sha256:e2044c3cfebe0ff3a99fc207ac5efe6e07878ad59fd4ad5e41f88cb016dacd73</pre>

								</p></div></li></ol></div></li></ol></div></section><section class="section" id="coreos-layering-updating_coreos-layering"><div class="titlepage"><div><div><h2 class="title">17.3. Updating with a RHCOS custom layered image</h2></div></div></div><p>
				When you configure Red Hat Enterprise Linux CoreOS (RHCOS) image layering, OpenShift Container Platform no longer automatically updates the node pool that uses the custom layered image. You become responsible to manually update your nodes as appropriate.
			</p><p>
				To update a node that uses a custom layered image, follow these general steps:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						The cluster automatically upgrades to version x.y.z+1, except for the nodes that use the custom layered image.
					</li><li class="listitem">
						You could then create a new Containerfile that references the updated OpenShift Container Platform image and the RPM that you had previously applied.
					</li><li class="listitem">
						Create a new machine config that points to the updated custom layered image.
					</li></ol></div><p>
				Updating a node with a custom layered image is not required. However, if that node gets too far behind the current OpenShift Container Platform version, you could experience unexpected results.
			</p></section></section><div><div xml:lang="en-US" class="legalnotice" id="idm140031663487472"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"><!--Empty--></span>© 2023 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div>


  <nav class="pvof-doc__book-nav">
  <ol class="book-nav__list">
              </ol>
</nav>


          </div>
              </div>
              <div id="comments-footer" class="book-comments">
          

  

        </div>
          </div>
  </article>
<meta itemscope="" itemref="md1">



    </div>
      <!-- CP_PRIMER_FOOTER -->            </div>
        </main>
    </div>
    <!--googleoff: all-->
    <div id="to-top"><a class="btn_slideto" href="#masthead" aria-label="Back to Top"><span class="web-icon-upload"></span></a></div>
    <footer class="footer-main">
        <div class="footer-top">
            <div class="container">

              <div class="brand">
                <a href="https://redhat.com">
                  <svg class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 613 145">
                    <defs>
                      <style>
                        .rh-logo-hat {
                          fill: #e00;
                        }
                        .rh-logo-type {
                          fill: #fff;
                        }
                      </style>
                    </defs>
                    <title>Red Hat</title>
                    <path
                      class="rh-logo-hat"
                      d="M127.47,83.49c12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42l-7.45-32.36c-1.72-7.12-3.23-10.35-15.73-16.6C124.89,8.69,103.76.5,97.51.5,91.69.5,90,8,83.06,8c-6.68,0-11.64-5.6-17.89-5.6-6,0-9.91,4.09-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33C22.27,52,.5,55,.5,74.22c0,31.48,74.59,70.28,133.65,70.28,45.28,0,56.7-20.48,56.7-36.65,0-12.72-11-27.16-30.83-35.78"/>
                      <path class="rh-logo-band"
                      d="M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45,12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/>
                      <path
                      class="rh-logo-type"
                      d="M579.74,92.8c0,11.89,7.15,17.67,20.19,17.67a52.11,52.11,0,0,0,11.89-1.68V95a24.84,24.84,0,0,1-7.68,1.16c-5.37,0-7.36-1.68-7.36-6.73V68.3h15.56V54.1H596.78v-18l-17,3.68V54.1H568.49V68.3h11.25Zm-53,.32c0-3.68,3.69-5.47,9.26-5.47a43.12,43.12,0,0,1,10.1,1.26v7.15a21.51,21.51,0,0,1-10.63,2.63c-5.46,0-8.73-2.1-8.73-5.57m5.2,17.56c6,0,10.84-1.26,15.36-4.31v3.37h16.82V74.08c0-13.56-9.14-21-24.39-21-8.52,0-16.94,2-26,6.1l6.1,12.52c6.52-2.74,12-4.42,16.83-4.42,7,0,10.62,2.73,10.62,8.31v2.73a49.53,49.53,0,0,0-12.62-1.58c-14.31,0-22.93,6-22.93,16.73,0,9.78,7.78,17.24,20.19,17.24m-92.44-.94h18.09V80.92h30.29v28.82H506V36.12H487.93V64.41H457.64V36.12H439.55ZM370.62,81.87c0-8,6.31-14.1,14.62-14.1A17.22,17.22,0,0,1,397,72.09V91.54A16.36,16.36,0,0,1,385.24,96c-8.2,0-14.62-6.1-14.62-14.09m26.61,27.87h16.83V32.44l-17,3.68V57.05a28.3,28.3,0,0,0-14.2-3.68c-16.19,0-28.92,12.51-28.92,28.5a28.25,28.25,0,0,0,28.4,28.6,25.12,25.12,0,0,0,14.93-4.83ZM320,67c5.36,0,9.88,3.47,11.67,8.83H308.47C310.15,70.3,314.36,67,320,67M291.33,82c0,16.2,13.25,28.82,30.28,28.82,9.36,0,16.2-2.53,23.25-8.42l-11.26-10c-2.63,2.74-6.52,4.21-11.14,4.21a14.39,14.39,0,0,1-13.68-8.83h39.65V83.55c0-17.67-11.88-30.39-28.08-30.39a28.57,28.57,0,0,0-29,28.81M262,51.58c6,0,9.36,3.78,9.36,8.31S268,68.2,262,68.2H244.11V51.58Zm-36,58.16h18.09V82.92h13.77l13.89,26.82H292l-16.2-29.45a22.27,22.27,0,0,0,13.88-20.72c0-13.25-10.41-23.45-26-23.45H226Z"/>
                  </svg>
                </a>
              </div>

              <div role="navigation" aria-label="quick">
                  <h3>Quick Links</h3>
                  <ul>
                      <li><a class="download-software" href="https://access.redhat.com/downloads/">Downloads</a></li>
                      <li><a class="manage-subscriptions" href="https://access.redhat.com/management">Subscriptions</a></li>
                      <li><a class="support-cases" href="https://access.redhat.com/support">Support Cases</a></li>
                      <li><a class="customer-service" href="https://access.redhat.com/support/customer-service">Customer Service</a></li>
                      <li><a class="quick-docs" href="https://access.redhat.com/documentation">Product Documentation</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="help">
                  <h3>Help</h3>
                  <ul>
                      <li><a class="contact-us" href="https://access.redhat.com/support/contact/">Contact Us</a></li>
                      <li><a class="cp-faqs" href="https://access.redhat.com/articles/33844">Customer Portal FAQ</a></li>
                      <li><a class="login-problems" href="https://access.redhat.com/help/login_assistance">Log-in Assistance</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="site">
                  <h3>Site Info</h3>
                  <ul>
                      <li><a class="trust-red-hat" href="https://www.redhat.com/en/trust">Trust Red Hat</a></li>
                      <li><a class="browser-support-policy" href="https://www.redhat.com/en/about/browser-support">Browser Support Policy</a></li>
                      <li><a class="accessibility" href="https://www.redhat.com/en/about/digital-accessibility">Accessibility</a></li>
                      <li><a class="recognition" href="https://access.redhat.com/recognition/">Awards and Recognition</a></li>
                      <li><a class="colophon" href="https://access.redhat.com/help/colophon/">Colophon</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="other">
                  <h3>Related Sites</h3>
                  <ul>
                      <li><a href="https://www.redhat.com/" class="red-hat-com">redhat.com</a></li>
                      <li><a href="http://developers.redhat.com/" class="red-hat-developers">developers.redhat.com</a></li>
                      <li><a href="https://connect.redhat.com/" class="partner-connect">connect.redhat.com</a></li>
                      <li><a href="https://cloud.redhat.com/" class="cloud-com">cloud.redhat.com</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="about">
                  <h3>About</h3>
                  <ul>
                      <li><a href="https://access.redhat.com/subscription-value" class="subscription-value">Red Hat Subscription Value</a></li>
                      <li><a href="https://www.redhat.com/about/" class="about-red-hat">About Red Hat</a></li>
                      <li><a href="http://jobs.redhat.com" class="about-jobs">Red Hat Jobs</a></li>
                  </ul>
              </div>

            </div>
        </div>

        <div class="anchor">
            <div class="container">
                <div class="status-legal">
                    <a hidden href="https://status.redhat.com" class="status-page-widget">
                        <span class="status-description"></span>
                        <span class="status-dot shape-circle"></span>
                    </a>
                    <div class="legal-copyright">
                        <div class="copyright">Copyright © 2023 Red Hat, Inc.</div>
                        <div role="navigation" aria-label="legal" class="legal">
                            <ul>
                                <li><a href="http://www.redhat.com/en/about/privacy-policy" class="privacy-policy">Privacy Statement</a></li>
                                <li><a href="https://www.redhat.com/en/about/terms-use" class="terms-of-use">Terms of Use</a></li>
                                <li><a href="http://www.redhat.com/en/about/all-policies-guidelines" class="all-policies">All Policies and Guidelines</a></li>
                                <li><a id="teconsent"></a></li>
                            </ul>
                            <div id="privacy_policy">We've updated our <a href='http://www.redhat.com/en/about/privacy-policy' class='privacy-policy'>Privacy Statement</a> effective September 15, 2023.
                            </div>
                          </div>
                        </div>
                </div>
                <div class="social">
                    <a href="http://www.redhat.com/summit/" class="summit">
                        <img src="https://access.redhat.com/chrome_themes/nimbus/img/rh-summit-red-a.svg" alt="Red Hat Summit" /> <span class="offscreen">Red Hat Summit</span>
                    </a>

                    <div class="social-media">
                        <a href="https://twitter.com/RedHat" class="sm-icon twitter"><span class="nicon-twitter"></span><span class="offscreen">Twitter</span></a>                        
                    </div>
                </div>
            </div>
        </div>
    </footer>
    <!-- TrustArc -->
    <div id="consent_blackbar"></div> 
    <!--googleon: all-->
</div>
<!-- /CP_PRIMER_FOOTER -->


  </div>

    
  </body>
</html>
