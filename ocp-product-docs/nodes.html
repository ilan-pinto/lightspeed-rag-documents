<!DOCTYPE html>
<html lang="en" dir="ltr" prefix="og: https://ogp.me/ns#">
  <head>
    <meta charset="utf-8" />
<link rel="canonical" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/index" />
<link rel="shortlink" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/index" />
<meta property="og:site_name" content="Red Hat Customer Portal" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/index" />
<meta property="og:title" content="Nodes OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<meta property="og:description" content="This document provides instructions for configuring and managing the nodes, Pods, and containers in your cluster. It also provides information on configuring Pod scheduling and placement, using jobs and DaemonSets to automate tasks, and other tasks to ensure an efficient cluster." />
<meta property="og:image" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:description" content="This document provides instructions for configuring and managing the nodes, Pods, and containers in your cluster. It also provides information on configuring Pod scheduling and placement, using jobs and DaemonSets to automate tasks, and other tasks to ensure an efficient cluster." />
<meta name="twitter:title" content="Nodes OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<meta name="twitter:url" content="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/index" />
<meta name="twitter:image" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
<meta name="title" content="Nodes OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<link rel="alternate" hreflang="en" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/index" />
<link rel="alternate" hreflang="ko" href="https://access.redhat.com/documentation/ko-kr/openshift_container_platform/4.13/html-single/nodes/index" />
<link rel="alternate" hreflang="zh-hans" href="https://access.redhat.com/documentation/zh-cn/openshift_container_platform/4.13/html-single/nodes/index" />
<link rel="alternate" hreflang="ja" href="https://access.redhat.com/documentation/ja-jp/openshift_container_platform/4.13/html-single/nodes/index" />
<link rel="alternate" hreflang="es" href="https://access.redhat.com/documentation/es-es/openshift_container_platform/4.13/html-single/nodes/index" />
<link rel="alternate" hreflang="ru" href="https://access.redhat.com/documentation/ru-ru/openshift_container_platform/4.13/html-single/nodes/index" />
<link rel="alternate" hreflang="pt-br" href="https://access.redhat.com/documentation/pt-br/openshift_container_platform/4.13/html-single/nodes/index" />
<link rel="alternate" hreflang="it" href="https://access.redhat.com/documentation/it-it/openshift_container_platform/4.13/html-single/nodes/index" />
<link rel="alternate" hreflang="de" href="https://access.redhat.com/documentation/de-de/openshift_container_platform/4.13/html-single/nodes/index" />
<link rel="alternate" hreflang="fr" href="https://access.redhat.com/documentation/fr-fr/openshift_container_platform/4.13/html-single/nodes/index" />
<link rel="alternate" hreflang="zh-hant" href="https://access.redhat.com/documentation/zh-tw/openshift_container_platform/4.13/html-single/nodes/index" />
<link rel="alternate" hreflang="id" href="https://access.redhat.com/documentation/id-id/openshift_container_platform/4.13/html-single/nodes/index" />
<link rel="alternate" hreflang="th" href="https://access.redhat.com/documentation/th-th/openshift_container_platform/4.13/html-single/nodes/index" />
<link rel="alternate" hreflang="vi" href="https://access.redhat.com/documentation/vi-vn/openshift_container_platform/4.13/html-single/nodes/index" />
<meta name="Generator" content="Drupal 9 (https://www.drupal.org)" />
<meta name="MobileOptimized" content="width" />
<meta name="HandheldFriendly" content="true" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="revision" product="b0738f19-59ac-47eb-9512-8a439cd6dfb0" title="5da91d9b-b659-4050-b742-adaf07d07f54" page="110b05e9-f606-4cc7-9ecc-44fe7a352ec8" revision="dccd48b1f252cc22dad43e9ae6252b9287d98ca1:en-us" body="be81b3f3b95852a34758ea0d2d0f4516.html" toc="42ca83a51c7bb5028fc31fbf05c4a133.json" />

    <title>Nodes OpenShift Container Platform 4.13 | Red Hat Customer Portal</title>
    <link rel="stylesheet" media="all" href="/sites/dxp-docs/files/css/css_87GMcmxT1ib8ziQiU2KUAnTDFtZQV6iP-KGslA9LigM.css" />
<link rel="stylesheet" media="all" href="/sites/dxp-docs/files/css/css__Xq4GfgPDJw9K_yYJFmlRZGJeCENu3R3r4s0K7Tr_9g.css" />

    
    <script type="application/json" data-drupal-selector="drupal-settings-json">{"path":{"baseUrl":"\/","scriptPath":null,"pathPrefix":"","currentPath":"documentation\/en-us\/openshift_container_platform\/4.13\/html-single\/nodes\/index","currentPathIsAdmin":false,"isFront":false,"currentLanguage":"en"},"pluralDelimiter":"\u0003","suppressDeprecationErrors":true,"red_hat_jwt":{"client_id":"customer-portal","cookie_name":"rh_jwt","leeway":"0","realm":"redhat-external","sso_host":"https:\/\/sso.redhat.com\/","user_integration":1,"user_plugin":"drupal_user_auth","use_external_js":0,"use_internal_js":0,"use_in_admin":0},"user":{"uid":0,"permissionsHash":"d8ea0bce2d740dacbdfe0257cf55baa0e33f7fb8468a26d055ce75daaaa2d315"}}</script>
<script src="/sites/dxp-docs/files/js/js_EQWKo9EokWkWS99x_e1oM-NEM0zlKyTkp_83mGdm5Ks.js"></script>

    <!-- CP_PRIMER_HEAD -->  <!-- TrustArc & DTM -->
  <script src="//static.redhat.com/libs/redhat/marketing/latest/trustarc/trustarc.js"></script>
  <script src="//www.redhat.com/dtm.js"></script><meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<!--[if IEMobile]><meta http-equiv="cleartype" content="on"><![endif]-->

<!-- metaInclude -->
<meta name="avalon-host-info" content="dxp-kbase-prod-139-77b4fb8768-25dr9" />
<meta name="avalon-version" content="27861f77" />
<meta name="cp-chrome-build-date" content="2023-10-06T19:17:59.039Z" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<!-- Chrome, Firefox OS and Opera -->
<meta name="theme-color" content="#000000">
<!-- Windows Phone -->
<meta name="msapplication-navbutton-color" content="#000000">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-status-bar-style" content="#000000">
<link rel="manifest" href="https://access.redhat.com/webassets/avalon/j/manifest.json">
<!-- Open Search - Tap to search -->
<link rel="search" type="application/opensearchdescription+xml" title="Red Hat Customer Portal" href="https://access.redhat.com/webassets/avalon/j/opensearch.xml" />
<!-- title -->
<title>Red Hat Customer Portal - Access to 24x7 support and knowledge</title>
<!-- /title -->
<script type="text/javascript">
    window.portal = {
        analytics : {},
        host      : "https://access.redhat.com",
        idp_url   : "https://sso.redhat.com",
        lang      : "en", 
        version   : "27861f77",
        builddate : "2023-10-06T19:17:59.039Z",        fetchdate : "2023-10-10T17:45:08-0400",        nrid      : "NOLONGERSUPPORTED",
        nrlk      : "NOLONGERSUPPORTED"
    };
</script>
<script type="text/javascript">
    if (!/\/logout.*/.test(location.pathname) && portal.host === location.origin && document.cookie.indexOf('rh_sso_session') >= 0 && !(document.cookie.indexOf('rh_jwt') >= 0)) window.location = '/login?redirectTo=' + encodeURIComponent(window.location.href);
</script>
<!-- cssInclude -->

<link rel="shortcut icon" href="https://access.redhat.com/webassets/avalon/g/favicon.ico" /><link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/bootstrap.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/bootstrap-grid.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/main.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/components.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/pages.css?v=27861f77" />

<link href="https://access.redhat.com/webassets/avalon/s/chosen.css?v=27861f77" rel="stylesheet" type="text/css" />

<!--[if lte IE 9]>
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/ie.css" />
<![endif]--><noscript>
    <style type="text/css" media="screen"> .primary-nav { display: block; } </style>
</noscript>
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/webassets/avalon/j/public_modules/node_modules/@cpelements/pfe-navigation/dist/pfe-navigation--lightdom.min.css" />
<!-- /cssInclude -->
<script src="https://access.redhat.com/webassets/avalon/j/public_modules/node_modules/@cpelements/pfe-navigation/dist/ie-polyfills.js?v=27861f77"></script>

<script async>
  if (!HTMLScriptElement.supports || !HTMLScriptElement.supports('importmap')) {
    import("https://www.redhatstatic.com/dx/v1-alpha/es-module-shims@1.7.3.js");
  }
</script>
<script type="importmap">
{
  "imports": {
    "@patternfly/elements/" : "https://www.redhatstatic.com/dx/v1-alpha/@patternfly/elements@2.2.2/",
    "@rhds/elements/":"https://www.redhatstatic.com/dx/v1-alpha/@rhds/elements@1.1.0/elements/",
    "@rhds/elements/lib/":"https://www.redhatstatic.com/dx/v1-alpha/@rhds/elements@1.1.0/lib/",
    "@cpelements/elements/":"https://www.redhatstatic.com/dx/v1-alpha/@cpelements/elements@2.0.0-alpha.7/elements/"
  }
}
</script><script type="text/javascript" src="https://access.redhat.com/webassets/avalon/j/lib/require.js?v=27861f77" data-main="/webassets/avalon/j/"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<script src="https://access.redhat.com/chrome_themes/nimbus/js/ie8.js"></script>
<![endif]-->
<script type="text/javascript" src="https://access.redhat.com/chrome_themes/nimbus/js/new-nav.js?v=27861f77" ></script>
<!-- /CP_PRIMER_HEAD -->

  </head>
  <body>
    
      <div class="dialog-off-canvas-main-canvas" data-off-canvas-main-canvas>
      <!-- CP_PRIMER_HEADER -->
<div id="page-wrap" class="page-wrap">
    <div id="pers-top-page-wrap" class="top-page-wrap pers-loader-bg">

      <div id="hero-bg-top-left" class="summit-bg-shapes"></div>
      <div id="hero-bg-top-right" class="summit-bg-shapes"></div>

        <!--googleoff: all-->
        <header class="masthead" id="masthead">

            <a href="#pfe-navigation" id="global-skip-to-nav" class="skip-link visually-hidden">Skip to navigation</a>
            <a href="#cp-main" class="skip-link visually-hidden">Skip to main content</a>            <nav id="portal-utility-nav" class="utility-navigation utility-navigation--bar hidden-at-mobile" data-analytics-region="utility" aria-labelledby="nav__utility-nav--desktop">
                <h3 id="nav__utility-nav--desktop" class="element-invisible">Utilities
                </h3>
                <ul aria-labelledby="nav__utility-nav--desktop">
                    <li id="nav-subscription" data-portal-tour-1="1">
                        <a class="top-nav-subscriptions" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Subscription" href="https://access.redhat.com/management/" >Subscriptions
                        </a>
                    </li>
                    <li id="nav-downloads" data-portal-tour-1="2">
                        <a class="top-nav-downloads" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Downloads" href="https://access.redhat.com/downloads/" >Downloads
                        </a>
                    </li>
                    <li id="nav-containers">
                        <a class="top-nav-containers" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Containers" href="https://catalog.redhat.com/software/containers/explore/" >Containers
                        </a>
                    </li>
                    <li id="nav-support" data-portal-tour-1="3">
                        <a class="top-nav-support-cases" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Support Cases" href="https://access.redhat.com/support/cases/" >Support Cases
                        </a>
                    </li>
                </ul>
            </nav>

            <pfe-navigation id="pfe-navigation" data-analytics-region="mega menu">
                <div class="pfe-navigation__logo-wrapper" id="pfe-navigation__logo-wrapper">
                    <a href="https://access.redhat.com/" class="pfe-navigation__logo-link" data-analytics-text="logo" data-analytics-category="MM|logo">
                        <img class="pfe-navigation__logo-image" alt="Red Hat Customer Portal" src="https://access.redhat.com/chrome_themes/nimbus/img/red-hat-customer-portal.svg" />
                    </a>
                </div>

                <nav class="pfe-navigation" aria-label="Main Navigation" data-analytics-region="main nav">
                    <ul class="pfe-navigation__menu" id="pfe-navigation__menu">                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-subscription--mobile" data-portal-tour-1="1">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Subscription" href="https://access.redhat.com/management/" >Subscriptions
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-downloads--mobile" data-portal-tour-1="2">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Downloads" href="https://access.redhat.com/downloads/" >Downloads
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-containers--mobile">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Containers" href="https://catalog.redhat.com/software/containers/explore/" >Containers
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-support--mobile" data-portal-tour-1="3">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Support Cases" href="https://access.redhat.com/support/cases/" >Support Cases
                            </a>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a href="https://access.redhat.com/products/" class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Products and Services">Products &amp; Services
                            </a>
                            <div class="pfe-navigation__dropdown has-primary-detail">                                <div class="desktop-col-span-2 tablet-col-span-all">
                                    <h3>
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Products" href="https://access.redhat.com/products/">Products
                                        </a>
                                    </h3>
                                    <slot name="main-menu__dropdown--product__product-listing"></slot>
                                </div>                                <div>
                                    <h3 id="nav__products__support">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Support" href="https://access.redhat.com/support">Support
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__support">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Production Support" href="https://access.redhat.com/support/offerings/production/">Production Support
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Development Support" href="https://access.redhat.com/support/offerings/developer/">Development Support
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Product Life Cycles" href="https://access.redhat.com/product-life-cycles/">Product Life Cycles
                                                    </a></li>
                                    </ul>

                                    <h3 id="nav__products__services">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Services" href="https://www.redhat.com/en/services">Services
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__services">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Consulting" href="https://www.redhat.com/en/services/consulting">Consulting
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Technical Account Management" href="https://access.redhat.com/support/offerings/tam/">Technical Account Management
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Training and Certifications" href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications
                                                    </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__products__documentation">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Documentation" href="https://access.redhat.com/documentation">Documentation
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__documentation">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat Enterprise Linux" href="https://access.redhat.com/documentation/en/red_hat_enterprise_linux">Red Hat Enterprise Linux
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat JBoss Enterprise Application Platform" href="https://access.redhat.com/documentation/en/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat OpenStack Platform" href="https://access.redhat.com/documentation/en/red_hat_openstack_platform">Red Hat OpenStack Platform
                                                    </a></li>
                                                    <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat OpenShift Container Platform" href="https://access.redhat.com/documentation/en/openshift_container_platform">Red Hat OpenShift Container Platform
                                                        </a></li>
                                    </ul>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="All Documentation" data-analytics-linkType="cta" href="https://access.redhat.com/documentation">All Documentation
                                        </a>
                                    </pfe-cta>

                                    <h3 id="nav__products__catalog"><a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Ecosystem Catalog" href="https://catalog.redhat.com/">Ecosystem Catalog
                                        </a></h3>
                                        <ul aria-labelledby="nav__products__catalog">
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Ecosystem Catalog" data-analytics-text="Red Hat Partner Ecosystem" href="https://access.redhat.com/ecosystem/">Red Hat Partner Ecosystem
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Ecosystem Catalog" data-analytics-text="Partner Resources" href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources
                                                    </a></li>
                                        </ul>
                                </div>
                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Tools" href="https://access.redhat.com/labs/">Tools
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="nav__tools__tools" data-analytics-level="2" data-analytics-text="Tools" data-analytics-category="Tools">Tools
                                    </h3>
                                    <ul aria-labelledby="nav__tools__tools">
                                        <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Solution Engine" href="https://access.redhat.com/support/cases/#/troubleshoot">Troubleshoot a product issue
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Packages" href="https://access.redhat.com/downloads/content/package-browser">Packages
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Errata" href="https://access.redhat.com/errata/">Errata
                                                    </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__tools__labs">
                                        <a data-analytics-level="2" data-analytics-category="Tools" data-analytics-text="Customer Portal Labs" href="https://access.redhat.com/labs/">Customer Portal Labs
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__tools__labs">
                                        <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Configuration" href="https://access.redhat.com/labs/#!?type=config">Configuration
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Deployment" href="https://access.redhat.com/labs/#!?type=deploy">Deployment
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Security" href="https://access.redhat.com/labs/#!?type=security">Security
                                                    </a></li>                                                    <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Troubleshooting" href="https://access.redhat.com/labs/#!?type=troubleshoot">Troubleshoot
                                                        </a></li>
                                    </ul>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="All Labs" data-analytics-linkType="cta" href="https://access.redhat.com/labs/">All labs
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h4 id="nav__tools__red-hat-insights">
                                        <a data-analytics-level="2" data-analytics-category="Tools" data-analytics-text="Red Hat Insights" href="//www.redhat.com/en/technologies/management/insights">Red Hat Insights
                                        </a>
                                    </h4>
                                    <p>Increase visibility into IT operations to detect and resolve technical issues before they impact your business.</p>
                                    <a data-analytics-level="3" data-analytics-category="Tools|Red Hat Insights" data-analytics-text="Learn more" href="https://www.redhat.com/en/technologies/management/insights">Learn More
                                    </a>
                                    <br>
                                    <a data-analytics-level="3" data-analytics-category="Tools|Red Hat Insights" data-analytics-text="Go to Insights" href="https://cloud.redhat.com/insights">Go to Insights
                                    </a>
                                </div>
                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Security" href="https://access.redhat.com/security/">Security
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="security__security-center">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Red Hat Product Security Center" href="https://access.redhat.com/security">Red Hat Product Security Center
                                        </a>
                                    </h3>
                                    <p>Engage with our Red Hat Product Security team, access security updates, and ensure your environments are not exposed to any known security vulnerabilities.
                                    </p>
                                    <pfe-cta pfe-priority="primary">
                                        <a data-analytics-level="3" data-analytics-category="Security|Red Hat Product Security Center" data-analytics-text="Product Security Center" data-analytics-linkType="cta" href="https://access.redhat.com/security/">Product Security Center
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__security__updates" data-analytics-level="2" data-analytics-text="Security Updates" data-analytics-category="Security">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Security Updates" href="/security">Security Updates
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__security__updates">
                                        <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Security Advisories" href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Red Hat CVE Database" href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Security Labs" href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs
                                                    </a></li>
                                    </ul>
                                    <p class="margin-top-xl">Keep your systems secure with Red Hat&#039;s specialized responses to security vulnerabilities.
                                    </p>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="View Responses" data-analytics-linkType="cta" href="https://access.redhat.com/security/vulnerability">View Responses
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__security__resources">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Resources" href="https://access.redhat.com/security/overview">Resources
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__security__resources">                                            <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Security Blog" href="//redhat.com/en/blog/channel/security">Security Blog
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Security Measurement" href="https://www.redhat.com/security/data/metrics/">Security Measurement
                                                    </a></li>
                                                    <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Severity Ratings" href="https://access.redhat.com/security/updates/classification/">Severity Ratings
                                                        </a></li>
                                                        <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Backporting Policies" href="https://access.redhat.com/security/updates/backporting/">Backporting Policies
                                                            </a></li>
                                                            <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Product Signing (GPG) Keys" href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys
                                                                </a></li>
                                    </ul>
                                </div>

                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a href="https://access.redhat.com/community/" class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Community">Community
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="nav__community__cp-community">
                                        <a href="https://access.redhat.com/community" data-analytics-level="2" data-analytics-text="Customer Portal Community" data-analytics-text="Customer Portal Community" data-analytics-category="Community">Customer Portal Community
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__community__cp-community">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Discussions" href="https://access.redhat.com/discussions">Discussions
                                            </a></li>                                                <li><a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Private Groups" href="https://access.redhat.com/groups/">Private Groups
                                                    </a></li>
                                    </ul>

                                    <pfe-cta pfe-priority="primary">
                                        <a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Community Activity" data-analytics-linkType="cta" href="https://access.redhat.com/community/">Community Activity
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__community__events" data-analytics-level="2" data-analytics-text="Customer Events" data-analytics-category="Community">Customer Events
                                    </h3>
                                    <ul aria-labelledby="nav__community__events">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Customer Events" data-analytics-text="Red Hat Convergence" href="https://access.redhat.com/convergence/">Red Hat Convergence
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Community|Customer Events" data-analytics-text="Red Hat Summit" href="http://www.redhat.com/summit/">Red Hat Summit
                                                </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__community__stories" data-analytics-level="2" data-analytics-text="Stories" data-analytics-category="Community">Stories
                                    </h3>
                                    <ul aria-labelledby="nav__community__stories">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Stories" data-analytics-text="Red Hat Subscription Value" href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-text="You Asked. We Acted." data-analytics-category="Community|Stories" href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Community|Stories" data-analytics-text="Open Source Communities" href="http://www.redhat.com/en/open-source">Open Source Communities
                                                    </a></li>
                                    </ul>
                                </div>
                            </div>
                        </li>
                    </ul>                </nav>                <div id="site-search" slot="search" class="utility-link site-search">
                    <div class="content">
                        <form class="ng-pristine ng-valid topSearchForm" id="topSearchForm" name="topSearchForm" action="/search/browse/search/" method="get" enctype="application/x-www-form-urlencoded">
                            <cp-search-autocomplete class="push-bottom" path="/webassets/avalon/j/data.json"></cp-search-autocomplete>                            <div>Or <a href="/support/cases/#/troubleshoot">troubleshoot an issue</a>.
                            </div>
                        </form>
                    </div>
                </div>


                <div slot="secondary-links" id="localesMenu">
                    <button class="pfe-navigation__secondary-link">
                        <pfe-icon icon="web-icon-globe" size="sm" aria-hidden="true"></pfe-icon>English
                    </button>

                    <pfe-navigation-dropdown dropdown-width="single">
                        <h2 class="utility-header">Select Your Language
                        </h2>
                        <ul class="reset">
                            <li><a href="https://access.redhat.com/changeLanguage?language=en" data-lang="en" id="en" data-analytics-text="English">English</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=ko" data-lang="ko" id="ko" data-analytics-text="Korean">한국어</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=ja"    data-lang="ja"    id="ja" data-analytics-text="Japanese">日本語</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" data-lang="zh_CN" id="zh_CN" data-analytics-text="Chinese">中文 (中国)</a></li>
                        </ul>

                    </pfe-navigation-dropdown>
                </div>                <rh-account-dropdown slot="account"></rh-account-dropdown>                <pfe-primary-detail breakpoint-width="600" class="main-menu__dropdown--product__product-listing" slot="main-menu__dropdown--product__product-listing" consistent-height>
                    <h3 slot="details-nav">Infrastructure and Management                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Enterprise Linux" href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Satellite" href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Subscription Management" href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Insights" href="https://access.redhat.com/products/red-hat-insights/">Red Hat Insights
                                </a>
                            </li>
                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Ansible Automation Platform" href="https://access.redhat.com/products/red-hat-ansible-automation-platform/">Red Hat Ansible Automation Platform
                                </a></li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Cloud Computing                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift" href="https://access.redhat.com/products/openshift">Red Hat OpenShift
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenStack Platform" href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat OpenShift Container Platform" href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat OpenShift Data Science" href="https://access.redhat.com/products/red-hat-openshift-data-science/">Red Hat OpenShift Data Science
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift Dedicated" href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat Advanced Cluster Security for Kubernetes" href="https://access.redhat.com/products/red-hat-advanced-cluster-security-for-kubernetes/">Red Hat Advanced Cluster Security for Kubernetes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat Advanced Cluster Management for Kubernetes" href="https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes/">Red Hat Advanced Cluster Management for Kubernetes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat Quay" href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat CodeReady Workspaces" href="https://access.redhat.com/products/red-hat-codeready-workspaces/">OpenShift Dev Spaces
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift Service on AWS" href="https://access.redhat.com/products/red-hat-openshift-service-aws">Red Hat OpenShift Service on AWS
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Storage                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Gluster Storage" href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Hyperconverged Infrastructure" href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Ceph Storage" href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Openshift Container Storage" href="https://access.redhat.com/products/red-hat-openshift-data-foundation">Red Hat OpenShift Data Foundation
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Runtimes                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Runtimes" href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat JBoss Enterprise Application Platform" href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Data Grid" href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat JBoss Web Server" href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Single Sign On" href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat support for Spring Boot" href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat build of Node.js" href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js
                                </a>
                            </li>                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat build of Quarkus" href="https://access.redhat.com/products/quarkus/">Red Hat build of Quarkus
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Integration and Automation                    </h3>
                    <div slot="details">
                        <ul class="border-bottom" id="portal-menu-border-bottom">
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat Application Foundations" href="https://access.redhat.com/products/red-hat-application-foundations/">Red Hat Application Foundations
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat Fuse" href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat AMQ" href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat 3scale API Management" href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management
                                </a>
                            </li>
                        </ul>
                    </div>
                    <div slot="details-nav--footer">
                        <pfe-cta pfe-priority="primary">
                            <a href="https://access.redhat.com/products/" class="pfe-navigation__menu-link" data-analytics-level="2" data-analytics-text="All Products" data-analytics-category="Products and Services|Products:" data-analytics-linkType="cta">All Products
                            </a>
                        </pfe-cta>
                    </div>
                </pfe-primary-detail>

            </pfe-navigation>

            <div id="scroll-anchor"></div>

            <!--[if IE 8]>
                <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                <span class="icon-warning alert-icon" aria-hidden="true"></span>
                You are using an unsupported web browser. Update to a supported browser for the best experience. <a href="/announcements/2120951">Read the announcement</a>.
                </div>
                </div>
            <![endif]-->
            <!--[if IE 9]>
                <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                <span class="icon-warning alert-icon" aria-hidden="true"></span>As of March 1, 2016, the Red Hat Customer Portal will no longer support Internet Explorer 9. See our new <a href="/help/browsers">browser support policy</a> for more information.
                </div>
                </div>
            <![endif]-->
            <div id="site-section"></div>
        </header>
        <!--googleon: all-->

        <main id="cp-main" class="portal-content-area">
            <div id="cp-content" class="main-content">                            <!-- /CP_PRIMER_HEADER -->

      <div class="container">
        

                                                                                                        <script>breadcrumbs = [["Products & Services","\/products\/"],["Product Documentation","\/documentation"],["OpenShift Container Platform","\/documentation\/en-us\/openshift_container_platform"],["4.13","\/documentation\/en-us\/openshift_container_platform\/4.13"],["Nodes","\/documentation\/en-us\/openshift_container_platform\/4.13\/html\/nodes"],["Nodes","\/documentation\/en-us\/openshift_container_platform\/4.13\/html\/nodes\/--single-page-document--"]]</script>

<div data-drupal-messages-fallback class="hidden"></div>


    </div>
        <div class="container">
        

  

  


  <article class="pvof-doc__content-wrapper__outer pvof-doc__content-wrapper__outer--css-not-removed">
    <script>
      'use strict';

            var $outerWrapper = document.querySelector('.pvof-doc__content-wrapper__outer');
      if ($outerWrapper && $outerWrapper.closest) {
        var $containerWrapper = $outerWrapper.closest('.container');
        if ($containerWrapper) {
          $containerWrapper.classList.remove('container');
          $containerWrapper.classList.add('j-chrome-content-container');
        }
      }

            var cssRemoved = false;
      try {
        var $crapCss = document.querySelectorAll(
          'link[href*="/chrome_themes/nimbus/css/pages.css"], link[href*="/chrome_themes/nimbus/css/components.css"]'
        );
        if ($crapCss.length) {
          for (let index = 0; index < $crapCss.length; index++) {
            const $stylesheet = $crapCss[index];
            $stylesheet.remove();
          }
        }
        cssRemoved = true;
      }
      catch (error) {
        console.error('Ran into an issue while trying to retheme page', error);
        cssRemoved = false;
      }

            if (cssRemoved) {
        var $pvofOuterWrapper = document.querySelector('.pvof-doc__content-wrapper__outer--css-not-removed');
        if ($pvofOuterWrapper) {
          $pvofOuterWrapper.classList.remove('pvof-doc__content-wrapper__outer--css-not-removed');
        }
      }
    </script>
    <div itemscope="" itemtype="https://schema.org/TechArticle" itemref="techArticle-md1 techArticle-md2 techArticle-md3"></div>
    <div itemscope="" itemtype="https://schema.org/SoftwareApplication" itemref="softwareApplication-md1 softwareApplication-md2 softwareApplication-md3 softwareApplication-md4"></div>
    <div class="pvof-doc__content-wrapper pvof-doc__content-wrapper--has-sidebar">
                                <div class="pvof-doc__content-wrapper__inner j-superdoc j-superdoc--has-nav">
                            <div class="pvof-sidebar__wrapper j-doc-nav j-superdoc__nav">
            <div class="j-sidebar__menu-container">
              <button class="j-sidebar__menu-trigger content-expander__trigger">
                <span class="j-sidebar__menu-trigger__open-text">Jump To</span>
                <span class="j-sidebar__menu-trigger__close-text">Close</span>
              </button>

              <div class="pvof-sidebar__inner-wrapper j-doc-nav__wrapper content-expander">
                <div class="j-sidebar__menu-details-container">
                  <button class="j-sidebar__menu-details-button j-sidebar__menu-details-button--expand">
                    Expand all
                  </button>
                  <button class="j-sidebar__menu-details-button j-sidebar__menu-details-button--collapse">
                    Collapse all
                  </button>
                </div>
                

  <nav id="pvof-doc__toc" class="pvof-doc__toc">
  <h2 class="j-doc-nav__title" id="j-doc-nav__title">
    Table of contents
  </h2>
  <div class="pvof-doc__toc-inner">
              <ol class="j-doc-nav__list" aria-labelledby="j-doc-nav__title">
                  <li class="j-doc-nav__list-item">
                                    

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes" class="j-doc-nav__link ">
    Nodes
  </a>
  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#overview-of-nodes" class="j-doc-nav__link j-doc-nav__link--has-children">
    1. Overview of nodes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "1. Overview of nodes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "1. Overview of nodes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-overview" class="j-doc-nav__link ">
    1.1. About nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#pods-overview" class="j-doc-nav__link ">
    1.2. About pods
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#containers-overview" class="j-doc-nav__link ">
    1.3. About containers
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-about-autoscaling-pod_overview-of-nodes" class="j-doc-nav__link ">
    1.4. About autoscaling pods on a node
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#commonterms-node" class="j-doc-nav__link ">
    1.5. Glossary of common terms for OpenShift Container Platform nodes
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#working-with-pods" class="j-doc-nav__link j-doc-nav__link--has-children">
    2. Working with pods
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2. Working with pods"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2. Working with pods"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-using-pp" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.1. Using pods
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.1. Using pods"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.1. Using pods"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-using-about_nodes-pods-using-ssy" class="j-doc-nav__link ">
    2.1.1. Understanding pods
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-using-example_nodes-pods-using-ssy" class="j-doc-nav__link ">
    2.1.2. Example pod configurations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#additional-resources" class="j-doc-nav__link ">
    2.1.3. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-viewing" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.2. Viewing pods
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.2. Viewing pods"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.2. Viewing pods"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-about_nodes-pods-viewing" class="j-doc-nav__link ">
    2.2.1. About pods
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-viewing-project_nodes-pods-viewing" class="j-doc-nav__link ">
    2.2.2. Viewing pods in a project
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-viewing-usage_nodes-pods-viewing" class="j-doc-nav__link ">
    2.2.3. Viewing pod usage statistics
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#viewing-resource-logs-cli-console_nodes-pods-viewing" class="j-doc-nav__link ">
    2.2.4. Viewing resource logs
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-configuring" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.3. Configuring an OpenShift Container Platform cluster for pods
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.3. Configuring an OpenShift Container Platform cluster for pods"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.3. Configuring an OpenShift Container Platform cluster for pods"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-configuring-restart_nodes-pods-configuring" class="j-doc-nav__link ">
    2.3.1. Configuring how pods behave after restart
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-configuring-bandwidth_nodes-pods-configuring" class="j-doc-nav__link ">
    2.3.2. Limiting the bandwidth available to pods
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-pod-distruption-about_nodes-pods-configuring" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.3.3. Understanding how to use pod disruption budgets to specify the number of pods that must be up
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.3.3. Understanding how to use pod disruption budgets to specify the number of pods that must be up"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.3.3. Understanding how to use pod disruption budgets to specify the number of pods that must be up"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-pod-disruption-configuring_nodes-pods-configuring" class="j-doc-nav__link ">
    2.3.3.1. Specifying the number of pods that must be up with pod disruption budgets
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#pod-disruption-eviction-policy_nodes-pods-configuring" class="j-doc-nav__link ">
    2.3.3.2. Specifying the eviction policy for unhealthy pods
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-configuring-critical_nodes-pods-configuring" class="j-doc-nav__link ">
    2.3.4. Preventing pod removal using critical pods
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-configuring-reducing_nodes-pods-configuring" class="j-doc-nav__link ">
    2.3.5. Reducing pod timeouts when using persistent volumes with high file counts
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.4. Automatically scaling pods with the horizontal pod autoscaler
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.4. Automatically scaling pods with the horizontal pod autoscaler"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.4. Automatically scaling pods with the horizontal pod autoscaler"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-about_nodes-pods-autoscaling" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.4.1. Understanding horizontal pod autoscalers
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.4.1. Understanding horizontal pod autoscalers"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.4.1. Understanding horizontal pod autoscalers"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#supported-metrics" class="j-doc-nav__link ">
    2.4.1.1. Supported metrics
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-workflow-hpa_nodes-pods-autoscaling" class="j-doc-nav__link ">
    2.4.2. How does the HPA work?
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-requests-and-limits-hpa_nodes-pods-autoscaling" class="j-doc-nav__link ">
    2.4.3. About requests and limits
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-best-practices-hpa_nodes-pods-autoscaling" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.4.4. Best practices
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.4.4. Best practices"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.4.4. Best practices"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-policies_nodes-pods-autoscaling" class="j-doc-nav__link ">
    2.4.4.1. Scaling policies
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-creating-web-console_nodes-pods-autoscaling" class="j-doc-nav__link ">
    2.4.5. Creating a horizontal pod autoscaler by using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-creating-cpu_nodes-pods-autoscaling" class="j-doc-nav__link ">
    2.4.6. Creating a horizontal pod autoscaler for CPU utilization by using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-creating-memory_nodes-pods-autoscaling" class="j-doc-nav__link ">
    2.4.7. Creating a horizontal pod autoscaler object for memory utilization by using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-status-about_nodes-pods-autoscaling" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.4.8. Understanding horizontal pod autoscaler status conditions by using the CLI
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.4.8. Understanding horizontal pod autoscaler status conditions by using the CLI"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.4.8. Understanding horizontal pod autoscaler status conditions by using the CLI"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-status-viewing_nodes-pods-autoscaling" class="j-doc-nav__link ">
    2.4.8.1. Viewing horizontal pod autoscaler status conditions by using the CLI
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#additional-resources-2" class="j-doc-nav__link ">
    2.4.9. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-vpa" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.5. Automatically adjust pod resource levels with the vertical pod autoscaler
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.5. Automatically adjust pod resource levels with the vertical pod autoscaler"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.5. Automatically adjust pod resource levels with the vertical pod autoscaler"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-vertical-autoscaler-about_nodes-pods-vertical-autoscaler" class="j-doc-nav__link ">
    2.5.1. About the Vertical Pod Autoscaler Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-vertical-autoscaler-install_nodes-pods-vertical-autoscaler" class="j-doc-nav__link ">
    2.5.2. Installing the Vertical Pod Autoscaler Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-vertical-autoscaler-using-about_nodes-pods-vertical-autoscaler" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.5.3. About Using the Vertical Pod Autoscaler Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.5.3. About Using the Vertical Pod Autoscaler Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.5.3. About Using the Vertical Pod Autoscaler Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-vertical-autoscaler-using-one-pod_nodes-pods-vertical-autoscaler" class="j-doc-nav__link ">
    2.5.3.1. Changing the VPA minimum value
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-vertical-autoscaler-using-auto_nodes-pods-vertical-autoscaler" class="j-doc-nav__link ">
    2.5.3.2. Automatically applying VPA recommendations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-vertical-autoscaler-using-pod_nodes-pods-vertical-autoscaler" class="j-doc-nav__link ">
    2.5.3.3. Automatically applying VPA recommendations on pod creation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-vertical-autoscaler-using-manual_nodes-pods-vertical-autoscaler" class="j-doc-nav__link ">
    2.5.3.4. Manually applying VPA recommendations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-vertical-autoscaler-using-exempt_nodes-pods-vertical-autoscaler" class="j-doc-nav__link ">
    2.5.3.5. Exempting containers from applying VPA recommendations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-vertical-autoscaler-custom_nodes-pods-vertical-autoscaler" class="j-doc-nav__link ">
    2.5.3.6. Using an alternative recommender
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-vertical-autoscaler-configuring_nodes-pods-vertical-autoscaler" class="j-doc-nav__link ">
    2.5.4. Using the Vertical Pod Autoscaler Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-vertical-autoscaler-uninstall_nodes-pods-vertical-autoscaler" class="j-doc-nav__link ">
    2.5.5. Uninstalling the Vertical Pod Autoscaler Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-secrets" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.6. Providing sensitive data to pods
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.6. Providing sensitive data to pods"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.6. Providing sensitive data to pods"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-secrets-about_nodes-pods-secrets" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.6.1. Understanding secrets
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.6.1. Understanding secrets"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.6.1. Understanding secrets"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-secrets-about-types_nodes-pods-secrets" class="j-doc-nav__link ">
    2.6.1.1. Types of secrets
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-secrets-about-keys_nodes-pods-secrets" class="j-doc-nav__link ">
    2.6.1.2. Secret data keys
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#auto-generated-sa-token-secrets_nodes-pods-secrets" class="j-doc-nav__link ">
    2.6.1.3. About automatically generated service account token secrets
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-secrets-creating_nodes-pods-secrets" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.6.2. Understanding how to create secrets
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.6.2. Understanding how to create secrets"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.6.2. Understanding how to create secrets"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#secret-creation-restrictions" class="j-doc-nav__link ">
    2.6.2.1. Secret creation restrictions
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-secrets-creating-opaque_nodes-pods-secrets" class="j-doc-nav__link ">
    2.6.2.2. Creating an opaque secret
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-secrets-creating-sa_nodes-pods-secrets" class="j-doc-nav__link ">
    2.6.2.3. Creating a service account token secret
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-secrets-creating-basic_nodes-pods-secrets" class="j-doc-nav__link ">
    2.6.2.4. Creating a basic authentication secret
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-secrets-creating-ssh_nodes-pods-secrets" class="j-doc-nav__link ">
    2.6.2.5. Creating an SSH authentication secret
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-secrets-creating-docker_nodes-pods-secrets" class="j-doc-nav__link ">
    2.6.2.6. Creating a Docker configuration secret
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-secrets-updating_nodes-pods-secrets" class="j-doc-nav__link ">
    2.6.3. Understanding how to update secrets
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-application-secrets-creating-using-sa_nodes-pods-secrets" class="j-doc-nav__link ">
    2.6.4. Creating and using secrets
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-secrets-certificates-about_nodes-pods-secrets" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.6.5. About using signed certificates with secrets
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.6.5. About using signed certificates with secrets"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.6.5. About using signed certificates with secrets"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-secrets-certificates-creating_nodes-pods-secrets" class="j-doc-nav__link ">
    2.6.5.1. Generating signed certificates for use with secrets
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-secrets-troubleshooting_nodes-pods-secrets" class="j-doc-nav__link ">
    2.6.6. Troubleshooting secrets
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#configmaps" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.7. Creating and using config maps
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.7. Creating and using config maps"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.7. Creating and using config maps"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-configmap-overview_configmaps" class="j-doc-nav__link ">
    2.7.1. Understanding config maps
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-configmap-create-from-console_configmaps" class="j-doc-nav__link ">
    2.7.2. Creating a config map in the OpenShift Container Platform web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-configmap-create_configmaps" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.7.3. Creating a config map by using the CLI
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.7.3. Creating a config map by using the CLI"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.7.3. Creating a config map by using the CLI"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-configmap-creating-from-directories_configmaps" class="j-doc-nav__link ">
    2.7.3.1. Creating a config map from a directory
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-configmap-creating-from-files_configmaps" class="j-doc-nav__link ">
    2.7.3.2. Creating a config map from a file
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-configmap-creating-from-literal-values_configmaps" class="j-doc-nav__link ">
    2.7.3.3. Creating a config map from literal values
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-configmaps-consuming-configmap-in-pods" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.7.4. Use cases: Consuming config maps in pods
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.7.4. Use cases: Consuming config maps in pods"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.7.4. Use cases: Consuming config maps in pods"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-configmaps-use-case-consuming-in-env-vars_configmaps" class="j-doc-nav__link ">
    2.7.4.1. Populating environment variables in containers by using config maps
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-configmaps-use-case-setting-command-line-arguments_configmaps" class="j-doc-nav__link ">
    2.7.4.2. Setting command-line arguments for container commands with config maps
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-configmaps-use-case-consuming-in-volumes_configmaps" class="j-doc-nav__link ">
    2.7.4.3. Injecting content into a volume by using config maps
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-device" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.8. Using device plugins to access external resources with pods
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.8. Using device plugins to access external resources with pods"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.8. Using device plugins to access external resources with pods"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-plugins-about_nodes-pods-device" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.8.1. Understanding device plugins
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.8.1. Understanding device plugins"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.8.1. Understanding device plugins"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#methods-for-deploying-a-device-plugin_nodes-pods-device" class="j-doc-nav__link ">
    2.8.1.1. Methods for deploying a device plugin
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-plugins-device-mgr_nodes-pods-device" class="j-doc-nav__link ">
    2.8.2. Understanding the Device Manager
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-plugins-install_nodes-pods-device" class="j-doc-nav__link ">
    2.8.3. Enabling Device Manager
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-priority" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.9. Including pod priority in pod scheduling decisions
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.9. Including pod priority in pod scheduling decisions"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.9. Including pod priority in pod scheduling decisions"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-priority-about_nodes-pods-priority" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.9.1. Understanding pod priority
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.9.1. Understanding pod priority"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.9.1. Understanding pod priority"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#admin-guide-priority-preemption-priority-class_nodes-pods-priority" class="j-doc-nav__link ">
    2.9.1.1. Pod priority classes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#admin-guide-priority-preemption-names_nodes-pods-priority" class="j-doc-nav__link ">
    2.9.1.2. Pod priority names
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-priority-preempt-about_nodes-pods-priority" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.9.2. Understanding pod preemption
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.9.2. Understanding pod preemption"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.9.2. Understanding pod preemption"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#non-preempting-priority-class_nodes-pods-priority" class="j-doc-nav__link ">
    2.9.2.1. Non-preempting priority classes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#priority-preemption-other_nodes-pods-priority" class="j-doc-nav__link ">
    2.9.2.2. Pod preemption and other scheduler settings
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#priority-preemption-graceful_nodes-pods-priority" class="j-doc-nav__link ">
    2.9.2.3. Graceful termination of preempted pods
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-priority-configuring_nodes-pods-priority" class="j-doc-nav__link ">
    2.9.3. Configuring priority and preemption
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-node-selectors" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.10. Placing pods on specific nodes using node selectors
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.10. Placing pods on specific nodes using node selectors"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.10. Placing pods on specific nodes using node selectors"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-node-selectors-pod_nodes-pods-node-selectors" class="j-doc-nav__link ">
    2.10.1. Using node selectors to control pod placement
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#run-once-duration-override-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.11. Run Once Duration Override Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.11. Run Once Duration Override Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.11. Run Once Duration Override Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#run-once-duration-override-about" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.11.1. Run Once Duration Override Operator overview
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.11.1. Run Once Duration Override Operator overview"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.11.1. Run Once Duration Override Operator overview"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#run-once-about_run-once-duration-override-about" class="j-doc-nav__link ">
    2.11.1.1. About the Run Once Duration Override Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#run-once-duration-override-release-notes" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.11.2. Run Once Duration Override Operator release notes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.11.2. Run Once Duration Override Operator release notes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.11.2. Run Once Duration Override Operator release notes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#run-once-duration-override-operator-release-notes-1-0-0" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.11.2.1. OpenShift Run Once Duration Override Operator 1.0.0
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.11.2.1. OpenShift Run Once Duration Override Operator 1.0.0"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.11.2.1. OpenShift Run Once Duration Override Operator 1.0.0"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#run-once-duration-override-operator-1-0-0-new-features-and-enhancements" class="j-doc-nav__link ">
    2.11.2.1.1. New features and enhancements
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#run-once-duration-override-install" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.11.3. Overriding the active deadline for run-once pods
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.11.3. Overriding the active deadline for run-once pods"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.11.3. Overriding the active deadline for run-once pods"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#rodoo-install-operator_run-once-duration-override-install" class="j-doc-nav__link ">
    2.11.3.1. Installing the Run Once Duration Override Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#rodoo-enable-override_run-once-duration-override-install" class="j-doc-nav__link ">
    2.11.3.2. Enabling the run-once duration override on a namespace
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#rodoo-update-active-deadline-seconds_run-once-duration-override-install" class="j-doc-nav__link ">
    2.11.3.3. Updating the run-once active deadline override value
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#run-once-duration-override-uninstall" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.11.4. Uninstalling the Run Once Duration Override Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.11.4. Uninstalling the Run Once Duration Override Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.11.4. Uninstalling the Run Once Duration Override Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#rodoo-uninstall-operator_run-once-duration-override-uninstall" class="j-doc-nav__link ">
    2.11.4.1. Uninstalling the Run Once Duration Override Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#rodoo-uninstall-resources_run-once-duration-override-uninstall" class="j-doc-nav__link ">
    2.11.4.2. Uninstalling Run Once Duration Override Operator resources
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#automatically-scaling-pods-with-the-custom-metrics-autoscaler-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    3. Automatically scaling pods with the Custom Metrics Autoscaler Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3. Automatically scaling pods with the Custom Metrics Autoscaler Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3. Automatically scaling pods with the Custom Metrics Autoscaler Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom" class="j-doc-nav__link ">
    3.1. Custom Metrics Autoscaler Operator overview
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-rn_nodes-cma-autoscaling-custom-removing" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.2. Custom Metrics Autoscaler Operator release notes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.2. Custom Metrics Autoscaler Operator release notes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.2. Custom Metrics Autoscaler Operator release notes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-custom-rn-versions_nodes-cma-autoscaling-custom-removing" class="j-doc-nav__link ">
    3.2.1. Supported versions
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-custom-rn-210-267_nodes-cma-autoscaling-custom-removing" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.2.2. Custom Metrics Autoscaler Operator 2.10.1-267 release notes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.2.2. Custom Metrics Autoscaler Operator 2.10.1-267 release notes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.2.2. Custom Metrics Autoscaler Operator 2.10.1-267 release notes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-custom-rn-210-267-bugs_nodes-cma-autoscaling-custom-removing" class="j-doc-nav__link ">
    3.2.2.1. Bug fixes
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-custom-rn-210_nodes-cma-autoscaling-custom-removing" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.2.3. Custom Metrics Autoscaler Operator 2.10.1 release notes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.2.3. Custom Metrics Autoscaler Operator 2.10.1 release notes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.2.3. Custom Metrics Autoscaler Operator 2.10.1 release notes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-custom-rn-210-new_nodes-cma-autoscaling-custom-removing" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.2.3.1. New features and enhancements
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.2.3.1. New features and enhancements"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.2.3.1. New features and enhancements"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-custom-rn-210-ga_nodes-cma-autoscaling-custom-removing" class="j-doc-nav__link ">
    3.2.3.1.1. Custom Metrics Autoscaler Operator general availability
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-custom-rn-210-metrics_nodes-cma-autoscaling-custom-removing" class="j-doc-nav__link ">
    3.2.3.1.2. Performance metrics
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-custom-rn-210-pause_nodes-cma-autoscaling-custom-removing" class="j-doc-nav__link ">
    3.2.3.1.3. Pausing the custom metrics autoscaling for scaled objects
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-custom-rn-210-fall-back_nodes-cma-autoscaling-custom-removing" class="j-doc-nav__link ">
    3.2.3.1.4. Replica fall back for scaled objects
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-custom-rn-210-hpa-name_nodes-cma-autoscaling-custom-removing" class="j-doc-nav__link ">
    3.2.3.1.5. Customizable HPA naming for scaled objects
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-autoscaling-custom-rn-210-activation_nodes-cma-autoscaling-custom-removing" class="j-doc-nav__link ">
    3.2.3.1.6. Activation and scaling thresholds
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-rn-282-174_nodes-cma-autoscaling-custom-removing" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.2.4. Custom Metrics Autoscaler Operator 2.8.2-174 release notes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.2.4. Custom Metrics Autoscaler Operator 2.8.2-174 release notes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.2.4. Custom Metrics Autoscaler Operator 2.8.2-174 release notes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-rn-282-174-new_nodes-cma-autoscaling-custom-removing" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.2.4.1. New features and enhancements
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.2.4.1. New features and enhancements"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.2.4.1. New features and enhancements"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#autoscaling-custom-2-8-2-upgrade-operator" class="j-doc-nav__link ">
    3.2.4.1.1. Operator upgrade support
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#autoscaling-custom-2-8-2-must-gather" class="j-doc-nav__link ">
    3.2.4.1.2. must-gather support
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-rn-282_nodes-cma-autoscaling-custom-removing" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.2.5. Custom Metrics Autoscaler Operator 2.8.2 release notes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.2.5. Custom Metrics Autoscaler Operator 2.8.2 release notes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.2.5. Custom Metrics Autoscaler Operator 2.8.2 release notes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-rn-282-new_nodes-cma-autoscaling-custom-removing" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.2.5.1. New features and enhancements
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.2.5.1. New features and enhancements"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.2.5.1. New features and enhancements"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#autoscaling-custom-2-8-2-audit-log" class="j-doc-nav__link ">
    3.2.5.1.1. Audit Logging
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#autoscaling-custom-2-8-2-kafka-metrics" class="j-doc-nav__link ">
    3.2.5.1.2. Scale applications based on Apache Kafka metrics
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#autoscaling-custom-2-8-2-cpu-metrics" class="j-doc-nav__link ">
    3.2.5.1.3. Scale applications based on CPU metrics
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#autoscaling-custom-2-8-2-memory-metrics" class="j-doc-nav__link ">
    3.2.5.1.4. Scale applications based on memory metrics
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-install" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.3. Installing the custom metrics autoscaler
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.3. Installing the custom metrics autoscaler"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.3. Installing the custom metrics autoscaler"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-install_nodes-cma-autoscaling-custom-install" class="j-doc-nav__link ">
    3.3.1. Installing the custom metrics autoscaler
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-overview-trigger" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.4. Understanding the custom metrics autoscaler triggers
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.4. Understanding the custom metrics autoscaler triggers"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.4. Understanding the custom metrics autoscaler triggers"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-trigger-prom_nodes-cma-autoscaling-custom-trigger" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.4.1. Understanding the Prometheus trigger
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.4.1. Understanding the Prometheus trigger"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.4.1. Understanding the Prometheus trigger"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-prometheus-config_nodes-cma-autoscaling-custom-trigger" class="j-doc-nav__link ">
    3.4.1.1. Configuring the custom metrics autoscaler to use OpenShift Container Platform monitoring
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-trigger-cpu_nodes-cma-autoscaling-custom-trigger" class="j-doc-nav__link ">
    3.4.2. Understanding the CPU trigger
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-trigger-memory_nodes-cma-autoscaling-custom-trigger" class="j-doc-nav__link ">
    3.4.3. Understanding the memory trigger
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-trigger-kafka_nodes-cma-autoscaling-custom-trigger" class="j-doc-nav__link ">
    3.4.4. Understanding the Kafka trigger
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-trigger-auth" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.5. Understanding custom metrics autoscaler trigger authentications
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.5. Understanding custom metrics autoscaler trigger authentications"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.5. Understanding custom metrics autoscaler trigger authentications"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-trigger-auth-using_nodes-cma-autoscaling-custom-trigger-auth" class="j-doc-nav__link ">
    3.5.1. Using trigger authentications
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-pausing" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.6. Pausing the custom metrics autoscaler for a scaled object
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.6. Pausing the custom metrics autoscaler for a scaled object"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.6. Pausing the custom metrics autoscaler for a scaled object"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-pausing-workload_nodes-cma-autoscaling-custom-pausing" class="j-doc-nav__link ">
    3.6.1. Pausing a custom metrics autoscaler
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-pausing-restart_nodes-cma-autoscaling-custom-pausing" class="j-doc-nav__link ">
    3.6.2. Restarting the custom metrics autoscaler for a scaled object
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-audit-log" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.7. Gathering audit logs
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.7. Gathering audit logs"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.7. Gathering audit logs"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-audit_nodes-cma-autoscaling-custom-audit-log" class="j-doc-nav__link ">
    3.7.1. Configuring audit logging
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-debugging" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.8. Gathering debugging data
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.8. Gathering debugging data"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.8. Gathering debugging data"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-debugging-gather_nodes-cma-autoscaling-custom-debugging" class="j-doc-nav__link ">
    3.8.1. Gathering debugging data
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-metrics" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.9. Viewing Operator metrics
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.9. Viewing Operator metrics"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.9. Viewing Operator metrics"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-metrics-access_nodes-cma-autoscaling-custom-metrics" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.9.1. Accessing performance metrics
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.9.1. Accessing performance metrics"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.9.1. Accessing performance metrics"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-metrics-provided_nodes-cma-autoscaling-custom-metrics" class="j-doc-nav__link ">
    3.9.1.1. Provided Operator metrics
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-adding" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.10. Understanding how to add custom metrics autoscalers
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.10. Understanding how to add custom metrics autoscalers"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.10. Understanding how to add custom metrics autoscalers"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-creating-workload_nodes-cma-autoscaling-custom-adding" class="j-doc-nav__link ">
    3.10.1. Adding a custom metrics autoscaler to a workload
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-creating-job_nodes-cma-autoscaling-custom-adding" class="j-doc-nav__link ">
    3.10.2. Adding a custom metrics autoscaler to a job
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-removing" class="j-doc-nav__link j-doc-nav__link--has-children">
    3.11. Removing the Custom Metrics Autoscaler Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3.11. Removing the Custom Metrics Autoscaler Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3.11. Removing the Custom Metrics Autoscaler Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cma-autoscaling-custom-uninstalling_nodes-cma-autoscaling-custom-removing" class="j-doc-nav__link ">
    3.11.1. Uninstalling the Custom Metrics Autoscaler Operator
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#controlling-pod-placement-onto-nodes-scheduling" class="j-doc-nav__link j-doc-nav__link--has-children">
    4. Controlling pod placement onto nodes (scheduling)
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4. Controlling pod placement onto nodes (scheduling)"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4. Controlling pod placement onto nodes (scheduling)"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-about" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.1. Controlling pod placement using the scheduler
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.1. Controlling pod placement using the scheduler"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.1. Controlling pod placement using the scheduler"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#about-default-scheduler" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.1.1. About the default scheduler
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.1.1. About the default scheduler"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.1.1. About the default scheduler"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-default-about_nodes-scheduler-about" class="j-doc-nav__link ">
    4.1.1.1. Understanding default scheduling
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-about-use-cases_nodes-scheduler-about" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.1.2. Scheduler use cases
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.1.2. Scheduler use cases"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.1.2. Scheduler use cases"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#infrastructure-topological-levels_nodes-scheduler-about" class="j-doc-nav__link ">
    4.1.2.1. Infrastructure topological levels
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#affinity_nodes-scheduler-about" class="j-doc-nav__link ">
    4.1.2.2. Affinity
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#anti-affinity_nodes-scheduler-about" class="j-doc-nav__link ">
    4.1.2.3. Anti-affinity
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-profiles" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.2. Scheduling pods using a scheduler profile
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.2. Scheduling pods using a scheduler profile"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.2. Scheduling pods using a scheduler profile"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-profiles-about_nodes-scheduler-profiles" class="j-doc-nav__link ">
    4.2.1. About scheduler profiles
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-profiles-configuring_nodes-scheduler-profiles" class="j-doc-nav__link ">
    4.2.2. Configuring a scheduler profile
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-pod-affinity" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.3. Placing pods relative to other pods using affinity and anti-affinity rules
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.3. Placing pods relative to other pods using affinity and anti-affinity rules"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.3. Placing pods relative to other pods using affinity and anti-affinity rules"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-pod-affinity-about_nodes-scheduler-pod-affinity" class="j-doc-nav__link ">
    4.3.1. Understanding pod affinity
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-pod-affinity-configuring_nodes-scheduler-pod-affinity" class="j-doc-nav__link ">
    4.3.2. Configuring a pod affinity rule
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-pod-anti-affinity-configuring_nodes-scheduler-pod-affinity" class="j-doc-nav__link ">
    4.3.3. Configuring a pod anti-affinity rule
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-pod-affinity-example_nodes-scheduler-pod-affinity" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.3.4. Sample pod affinity and anti-affinity rules
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.3.4. Sample pod affinity and anti-affinity rules"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.3.4. Sample pod affinity and anti-affinity rules"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-pod-affinity-example-affinity_nodes-scheduler-pod-affinity" class="j-doc-nav__link ">
    4.3.4.1. Pod Affinity
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-pod-affinity-example-antiaffinity_nodes-scheduler-pod-affinity" class="j-doc-nav__link ">
    4.3.4.2. Pod Anti-affinity
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-pod-affinity-example-no-labels_nodes-scheduler-pod-affinity" class="j-doc-nav__link ">
    4.3.4.3. Pod Affinity with no Matching Labels
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#olm-overriding-operator-pod-affinity_nodes-scheduler-pod-affinity" class="j-doc-nav__link ">
    4.3.5. Using pod affinity and anti-affinity to control where an Operator is installed
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-node-affinity" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.4. Controlling pod placement on nodes using node affinity rules
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.4. Controlling pod placement on nodes using node affinity rules"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.4. Controlling pod placement on nodes using node affinity rules"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-node-affinity-about_nodes-scheduler-node-affinity" class="j-doc-nav__link ">
    4.4.1. Understanding node affinity
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-node-affinity-configuring-required_nodes-scheduler-node-affinity" class="j-doc-nav__link ">
    4.4.2. Configuring a required node affinity rule
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-node-affinity-configuring-preferred_nodes-scheduler-node-affinity" class="j-doc-nav__link ">
    4.4.3. Configuring a preferred node affinity rule
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-node-affinity-example_nodes-scheduler-node-affinity" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.4.4. Sample node affinity rules
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.4.4. Sample node affinity rules"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.4.4. Sample node affinity rules"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#admin-guide-sched-affinity-examples1_nodes-scheduler-node-affinity" class="j-doc-nav__link ">
    4.4.4.1. Node affinity with matching labels
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#admin-guide-sched-affinity-examples2_nodes-scheduler-node-affinity" class="j-doc-nav__link ">
    4.4.4.2. Node affinity with no matching labels
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#olm-overriding-operator-pod-affinity_nodes-scheduler-node-affinity" class="j-doc-nav__link ">
    4.4.5. Using node affinity to control where an Operator is installed
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-node-affinity-addtl-resources_nodes-scheduler-node-affinity" class="j-doc-nav__link ">
    4.4.6. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-overcommit" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.5. Placing pods onto overcommited nodes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.5. Placing pods onto overcommited nodes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.5. Placing pods onto overcommited nodes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-overcommit-about_nodes-scheduler-overcommit" class="j-doc-nav__link ">
    4.5.1. Understanding overcommitment
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-overcommit-configure-nodes_nodes-scheduler-overcommit" class="j-doc-nav__link ">
    4.5.2. Understanding nodes overcommitment
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-taints-tolerations" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.6. Controlling pod placement using node taints
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.6. Controlling pod placement using node taints"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.6. Controlling pod placement using node taints"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-taints-tolerations-about_nodes-scheduler-taints-tolerations" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.6.1. Understanding taints and tolerations
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.6.1. Understanding taints and tolerations"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.6.1. Understanding taints and tolerations"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-taints-tolerations-about-seconds_nodes-scheduler-taints-tolerations" class="j-doc-nav__link ">
    4.6.1.1. Understanding how to use toleration seconds to delay pod evictions
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-taints-tolerations-about-multiple_nodes-scheduler-taints-tolerations" class="j-doc-nav__link ">
    4.6.1.2. Understanding how to use multiple taints
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-taints-tolerations-about-taintNodesByCondition_nodes-scheduler-taints-tolerations" class="j-doc-nav__link ">
    4.6.1.3. Understanding pod scheduling and node conditions (taint node by condition)
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-taints-tolerations-about-taintBasedEvictions_nodes-scheduler-taints-tolerations" class="j-doc-nav__link ">
    4.6.1.4. Understanding evicting pods by condition (taint-based evictions)
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-taints-tolerations-all_nodes-scheduler-taints-tolerations" class="j-doc-nav__link ">
    4.6.1.5. Tolerating all taints
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-taints-tolerations-adding_nodes-scheduler-taints-tolerations" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.6.2. Adding taints and tolerations
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.6.2. Adding taints and tolerations"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.6.2. Adding taints and tolerations"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-taints-tolerations-adding-machineset_nodes-scheduler-taints-tolerations" class="j-doc-nav__link ">
    4.6.2.1. Adding taints and tolerations using a compute machine set
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-taints-tolerations-bindings_nodes-scheduler-taints-tolerations" class="j-doc-nav__link ">
    4.6.2.2. Binding a user to a node using taints and tolerations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-taints-tolerations-projects_nodes-scheduler-taints-tolerations" class="j-doc-nav__link ">
    4.6.2.3. Creating a project with a node selector and toleration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-taints-tolerations-special_nodes-scheduler-taints-tolerations" class="j-doc-nav__link ">
    4.6.2.4. Controlling nodes with special hardware using taints and tolerations
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-taints-tolerations-removing_nodes-scheduler-taints-tolerations" class="j-doc-nav__link ">
    4.6.3. Removing taints and tolerations
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-node-selectors" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.7. Placing pods on specific nodes using node selectors
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.7. Placing pods on specific nodes using node selectors"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.7. Placing pods on specific nodes using node selectors"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-node-selectors-about_nodes-scheduler-node-selectors" class="j-doc-nav__link ">
    4.7.1. About node selectors
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-node-selectors-pod_nodes-scheduler-node-selectors" class="j-doc-nav__link ">
    4.7.2. Using node selectors to control pod placement
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-node-selectors-cluster_nodes-scheduler-node-selectors" class="j-doc-nav__link ">
    4.7.3. Creating default cluster-wide node selectors
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-node-selectors-project_nodes-scheduler-node-selectors" class="j-doc-nav__link ">
    4.7.4. Creating project-wide node selectors
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-pod-topology-spread-constraints" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.8. Controlling pod placement by using pod topology spread constraints
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.8. Controlling pod placement by using pod topology spread constraints"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.8. Controlling pod placement by using pod topology spread constraints"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-pod-topology-spread-constraints-about_nodes-scheduler-pod-topology-spread-constraints" class="j-doc-nav__link ">
    4.8.1. About pod topology spread constraints
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-pod-topology-spread-constraints-configuring_nodes-scheduler-pod-topology-spread-constraints" class="j-doc-nav__link ">
    4.8.2. Configuring pod topology spread constraints
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-pod-topology-spread-constraints-examples_nodes-scheduler-pod-topology-spread-constraints" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.8.3. Example pod topology spread constraints
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.8.3. Example pod topology spread constraints"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.8.3. Example pod topology spread constraints"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-pod-topology-spread-constraints-example-single_nodes-scheduler-pod-topology-spread-constraints" class="j-doc-nav__link ">
    4.8.3.1. Single pod topology spread constraint example
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-scheduler-pod-topology-spread-constraints-example-multiple_nodes-scheduler-pod-topology-spread-constraints" class="j-doc-nav__link ">
    4.8.3.2. Multiple pod topology spread constraints example
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#additional-resources-3" class="j-doc-nav__link ">
    4.8.4. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-descheduler" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.9. Evicting pods using the descheduler
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.9. Evicting pods using the descheduler"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.9. Evicting pods using the descheduler"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-descheduler-about_nodes-descheduler" class="j-doc-nav__link ">
    4.9.1. About the descheduler
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-descheduler-profiles_nodes-descheduler" class="j-doc-nav__link ">
    4.9.2. Descheduler profiles
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-descheduler-installing_nodes-descheduler" class="j-doc-nav__link ">
    4.9.3. Installing the descheduler
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-descheduler-configuring-profiles_nodes-descheduler" class="j-doc-nav__link ">
    4.9.4. Configuring descheduler profiles
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-descheduler-configuring-interval_nodes-descheduler" class="j-doc-nav__link ">
    4.9.5. Configuring the descheduler interval
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-descheduler-uninstalling_nodes-descheduler" class="j-doc-nav__link ">
    4.9.6. Uninstalling the descheduler
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#secondary-scheduler" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.10. Secondary scheduler
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.10. Secondary scheduler"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.10. Secondary scheduler"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-secondary-scheduler-about" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.10.1. Secondary scheduler overview
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.10.1. Secondary scheduler overview"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.10.1. Secondary scheduler overview"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-secondary-scheduler-about_nodes-secondary-scheduler-about" class="j-doc-nav__link ">
    4.10.1.1. About the Secondary Scheduler Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-secondary-scheduler-release-notes" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.10.2. Secondary Scheduler Operator for Red Hat OpenShift release notes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.10.2. Secondary Scheduler Operator for Red Hat OpenShift release notes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.10.2. Secondary Scheduler Operator for Red Hat OpenShift release notes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#secondary-scheduler-operator-release-notes-1.1.2" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.10.2.1. Release notes for Secondary Scheduler Operator for Red Hat OpenShift 1.1.2
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.10.2.1. Release notes for Secondary Scheduler Operator for Red Hat OpenShift 1.1.2"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.10.2.1. Release notes for Secondary Scheduler Operator for Red Hat OpenShift 1.1.2"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#secondary-scheduler-1.1.2-bug-fixes" class="j-doc-nav__link ">
    4.10.2.1.1. Bug fixes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#secondary-scheduler-operator-1.1.2-known-issues" class="j-doc-nav__link ">
    4.10.2.1.2. Known issues
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#secondary-scheduler-operator-release-notes-1.1.1" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.10.2.2. Release notes for Secondary Scheduler Operator for Red Hat OpenShift 1.1.1
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.10.2.2. Release notes for Secondary Scheduler Operator for Red Hat OpenShift 1.1.1"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.10.2.2. Release notes for Secondary Scheduler Operator for Red Hat OpenShift 1.1.1"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#secondary-scheduler-1.1.1-bug-fixes" class="j-doc-nav__link ">
    4.10.2.2.1. Bug fixes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#secondary-scheduler-operator-1.1.1-known-issues" class="j-doc-nav__link ">
    4.10.2.2.2. Known issues
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#secondary-scheduler-operator-release-notes-1.1.0" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.10.2.3. Release notes for Secondary Scheduler Operator for Red Hat OpenShift 1.1.0
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.10.2.3. Release notes for Secondary Scheduler Operator for Red Hat OpenShift 1.1.0"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.10.2.3. Release notes for Secondary Scheduler Operator for Red Hat OpenShift 1.1.0"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#secondary-scheduler-operator-1.1.0-new-features-and-enhancements" class="j-doc-nav__link ">
    4.10.2.3.1. New features and enhancements
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#secondary-scheduler-operator-1.1.0-known-issues" class="j-doc-nav__link ">
    4.10.2.3.2. Known issues
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#secondary-scheduler-configuring" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.10.3. Scheduling pods using a secondary scheduler
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.10.3. Scheduling pods using a secondary scheduler"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.10.3. Scheduling pods using a secondary scheduler"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-secondary-scheduler-install-console_secondary-scheduler-configuring" class="j-doc-nav__link ">
    4.10.3.1. Installing the Secondary Scheduler Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-secondary-scheduler-configuring-console_secondary-scheduler-configuring" class="j-doc-nav__link ">
    4.10.3.2. Deploying a secondary scheduler
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-secondary-scheduler-pod-console_secondary-scheduler-configuring" class="j-doc-nav__link ">
    4.10.3.3. Scheduling a pod using the secondary scheduler
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#secondary-scheduler-uninstalling" class="j-doc-nav__link j-doc-nav__link--has-children">
    4.10.4. Uninstalling the Secondary Scheduler Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4.10.4. Uninstalling the Secondary Scheduler Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4.10.4. Uninstalling the Secondary Scheduler Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-secondary-scheduler-uninstall-console_secondary-scheduler-uninstalling" class="j-doc-nav__link ">
    4.10.4.1. Uninstalling the Secondary Scheduler Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-secondary-scheduler-remove-resources-console_secondary-scheduler-uninstalling" class="j-doc-nav__link ">
    4.10.4.2. Removing Secondary Scheduler Operator resources
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#using-jobs-and-daemonsets" class="j-doc-nav__link j-doc-nav__link--has-children">
    5. Using Jobs and DaemonSets
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "5. Using Jobs and DaemonSets"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "5. Using Jobs and DaemonSets"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-daemonsets" class="j-doc-nav__link j-doc-nav__link--has-children">
    5.1. Running background tasks on nodes automatically with daemon sets
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "5.1. Running background tasks on nodes automatically with daemon sets"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "5.1. Running background tasks on nodes automatically with daemon sets"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#scheduled-by-default-scheduler" class="j-doc-nav__link ">
    5.1.1. Scheduled by default scheduler
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-daemonsets-creating_nodes-pods-daemonsets" class="j-doc-nav__link ">
    5.1.2. Creating daemonsets
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-jobs" class="j-doc-nav__link j-doc-nav__link--has-children">
    5.2. Running tasks in pods using jobs
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "5.2. Running tasks in pods using jobs"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "5.2. Running tasks in pods using jobs"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-jobs-about_nodes-nodes-jobs" class="j-doc-nav__link j-doc-nav__link--has-children">
    5.2.1. Understanding jobs and cron jobs
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "5.2.1. Understanding jobs and cron jobs"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "5.2.1. Understanding jobs and cron jobs"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#jobs-create_nodes-nodes-jobs" class="j-doc-nav__link ">
    5.2.1.1. Understanding how to create jobs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#jobs-set-max_nodes-nodes-jobs" class="j-doc-nav__link ">
    5.2.1.2. Understanding how to set a maximum duration for jobs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#jobs-set-backoff_nodes-nodes-jobs" class="j-doc-nav__link ">
    5.2.1.3. Understanding how to set a job back off policy for pod failure
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#jobs-artifacts_nodes-nodes-jobs" class="j-doc-nav__link ">
    5.2.1.4. Understanding how to configure a cron job to remove artifacts
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#jobs-limits_nodes-nodes-jobs" class="j-doc-nav__link ">
    5.2.1.5. Known limitations
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-jobs-creating_nodes-nodes-jobs" class="j-doc-nav__link ">
    5.2.2. Creating jobs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-jobs-creating-cron_nodes-nodes-jobs" class="j-doc-nav__link ">
    5.2.3. Creating cron jobs
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#working-with-nodes" class="j-doc-nav__link j-doc-nav__link--has-children">
    6. Working with nodes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6. Working with nodes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6. Working with nodes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-viewing" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.1. Viewing and listing the nodes in your OpenShift Container Platform cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.1. Viewing and listing the nodes in your OpenShift Container Platform cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.1. Viewing and listing the nodes in your OpenShift Container Platform cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-viewing-listing_nodes-nodes-viewing" class="j-doc-nav__link ">
    6.1.1. About listing all the nodes in a cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-viewing-listing-pods_nodes-nodes-viewing" class="j-doc-nav__link ">
    6.1.2. Listing pods on a node in your cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-viewing-memory_nodes-nodes-viewing" class="j-doc-nav__link ">
    6.1.3. Viewing memory and CPU usage statistics on your nodes
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-working" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.2. Working with nodes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.2. Working with nodes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.2. Working with nodes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-working-evacuating_nodes-nodes-working" class="j-doc-nav__link ">
    6.2.1. Understanding how to evacuate pods on nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-working-updating_nodes-nodes-working" class="j-doc-nav__link ">
    6.2.2. Understanding how to update labels on nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-working-marking_nodes-nodes-working" class="j-doc-nav__link ">
    6.2.3. Understanding how to mark nodes as unschedulable or schedulable
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#sno-clusters-reboot-without-drain_nodes-nodes-working" class="j-doc-nav__link ">
    6.2.4. Handling errors in single-node OpenShift clusters when the node reboots without draining application pods
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#deleting-nodes" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.2.5. Deleting nodes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.2.5. Deleting nodes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.2.5. Deleting nodes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-working-deleting_nodes-nodes-working" class="j-doc-nav__link ">
    6.2.5.1. Deleting nodes from a cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-working-deleting-bare-metal_nodes-nodes-working" class="j-doc-nav__link ">
    6.2.5.2. Deleting nodes from a bare metal cluster
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-managing" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.3. Managing nodes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.3. Managing nodes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.3. Managing nodes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-managing-about_nodes-nodes-managing" class="j-doc-nav__link ">
    6.3.1. Modifying nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-working-master-schedulable_nodes-nodes-managing" class="j-doc-nav__link ">
    6.3.2. Configuring control plane nodes as schedulable
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-working-setting-booleans_nodes-nodes-managing" class="j-doc-nav__link ">
    6.3.3. Setting SELinux booleans
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-kernel-arguments_nodes-nodes-managing" class="j-doc-nav__link ">
    6.3.4. Adding kernel arguments to nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-swap-memory_nodes-nodes-managing" class="j-doc-nav__link ">
    6.3.5. Enabling swap memory use on nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-control-plane-osp-migrating_nodes-nodes-managing" class="j-doc-nav__link ">
    6.3.6. Migrating control plane nodes from one RHOSP host to another
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-graceful-shutdown" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.4. Managing graceful node shutdown
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.4. Managing graceful node shutdown"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.4. Managing graceful node shutdown"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-cluster-timeout-graceful-shutdown_nodes-nodes-graceful-shutdown" class="j-doc-nav__link ">
    6.4.1. About graceful node shutdown
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-configuring-graceful-shutdown_nodes-nodes-graceful-shutdown" class="j-doc-nav__link ">
    6.4.2. Configuring graceful node shutdown
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-managing-max-pods" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.5. Managing the maximum number of pods per node
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.5. Managing the maximum number of pods per node"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.5. Managing the maximum number of pods per node"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-managing-max-pods-proc_nodes-nodes-managing-max-pods" class="j-doc-nav__link ">
    6.5.1. Configuring the maximum number of pods per node
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-node-tuning-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.6. Using the Node Tuning Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.6. Using the Node Tuning Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.6. Using the Node Tuning Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#accessing-an-example-node-tuning-operator-specification_nodes-node-tuning-operator" class="j-doc-nav__link ">
    6.6.1. Accessing an example Node Tuning Operator specification
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#custom-tuning-specification_nodes-node-tuning-operator" class="j-doc-nav__link ">
    6.6.2. Custom tuning specification
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#custom-tuning-default-profiles-set_nodes-node-tuning-operator" class="j-doc-nav__link ">
    6.6.3. Default profiles set on a cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#supported-tuned-daemon-plug-ins_nodes-node-tuning-operator" class="j-doc-nav__link ">
    6.6.4. Supported TuneD daemon plugins
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-remediating-fencing-maintaining-rhwa" class="j-doc-nav__link ">
    6.7. Remediating, fencing, and maintaining nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-rebooting" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.8. Understanding node rebooting
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.8. Understanding node rebooting"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.8. Understanding node rebooting"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-rebooting-infrastructure_nodes-nodes-rebooting" class="j-doc-nav__link ">
    6.8.1. About rebooting nodes running critical infrastructure
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-rebooting-affinity_nodes-nodes-rebooting" class="j-doc-nav__link ">
    6.8.2. Rebooting a node using pod anti-affinity
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-rebooting-router_nodes-nodes-rebooting" class="j-doc-nav__link ">
    6.8.3. Understanding how to reboot nodes running routers
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-rebooting-gracefully_nodes-nodes-rebooting" class="j-doc-nav__link ">
    6.8.4. Rebooting a node gracefully
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-garbage-collection" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.9. Freeing node resources using garbage collection
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.9. Freeing node resources using garbage collection"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.9. Freeing node resources using garbage collection"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-garbage-collection-containers_nodes-nodes-configuring" class="j-doc-nav__link ">
    6.9.1. Understanding how terminated containers are removed through garbage collection
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-garbage-collection-images_nodes-nodes-configuring" class="j-doc-nav__link ">
    6.9.2. Understanding how images are removed through garbage collection
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-garbage-collection-configuring_nodes-nodes-configuring" class="j-doc-nav__link ">
    6.9.3. Configuring garbage collection for containers and images
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-resources-configuring" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.10. Allocating resources for nodes in an OpenShift Container Platform cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.10. Allocating resources for nodes in an OpenShift Container Platform cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.10. Allocating resources for nodes in an OpenShift Container Platform cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-resources-configuring-about_nodes-nodes-resources-configuring" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.10.1. Understanding how to allocate resources for nodes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.10.1. Understanding how to allocate resources for nodes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.10.1. Understanding how to allocate resources for nodes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#computing-allocated-resources_nodes-nodes-resources-configuring" class="j-doc-nav__link ">
    6.10.1.1. How OpenShift Container Platform computes allocated resources
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#allocate-node-enforcement_nodes-nodes-resources-configuring" class="j-doc-nav__link ">
    6.10.1.2. How nodes enforce resource constraints
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#allocate-eviction-thresholds_nodes-nodes-resources-configuring" class="j-doc-nav__link ">
    6.10.1.3. Understanding Eviction Thresholds
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#allocate-scheduler-policy_nodes-nodes-resources-configuring" class="j-doc-nav__link ">
    6.10.1.4. How the scheduler determines resource availability
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-resources-configuring-auto_nodes-nodes-resources-configuring" class="j-doc-nav__link ">
    6.10.2. Automatically allocating resources for nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-resources-configuring-setting_nodes-nodes-resources-configuring" class="j-doc-nav__link ">
    6.10.3. Manually allocating resources for nodes
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-resources-cpus" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.11. Allocating specific CPUs for nodes in a cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.11. Allocating specific CPUs for nodes in a cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.11. Allocating specific CPUs for nodes in a cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-resources-cpus-reserve_nodes-nodes-resources-cpus" class="j-doc-nav__link ">
    6.11.1. Reserving CPUs for nodes
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-tls" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.12. Enabling TLS security profiles for the kubelet
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.12. Enabling TLS security profiles for the kubelet"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.12. Enabling TLS security profiles for the kubelet"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#tls-profiles-understanding_nodes-nodes-tls" class="j-doc-nav__link ">
    6.12.1. Understanding TLS security profiles
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#tls-profiles-kubelet-configuring_nodes-nodes-tls" class="j-doc-nav__link ">
    6.12.2. Configuring the TLS security profile for the kubelet
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#machine-config-daemon-metrics" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.13. Machine Config Daemon metrics
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.13. Machine Config Daemon metrics"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.13. Machine Config Daemon metrics"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#machine-config-daemon-metrics_machine-config-operator" class="j-doc-nav__link ">
    6.13.1. Machine Config Daemon metrics
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-nodes-creating-infrastructure-nodes" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.14. Creating infrastructure nodes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.14. Creating infrastructure nodes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.14. Creating infrastructure nodes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#infrastructure-components_creating-infrastructure-nodes" class="j-doc-nav__link j-doc-nav__link--has-children">
    6.14.1. OpenShift Container Platform infrastructure components
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6.14.1. OpenShift Container Platform infrastructure components"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6.14.1. OpenShift Container Platform infrastructure components"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#creating-an-infra-node_creating-infrastructure-nodes" class="j-doc-nav__link ">
    6.14.1.1. Creating an infrastructure node
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#working-with-containers" class="j-doc-nav__link j-doc-nav__link--has-children">
    7. Working with containers
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7. Working with containers"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7. Working with containers"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-using" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.1. Understanding Containers
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.1. Understanding Containers"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.1. Understanding Containers"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-memory" class="j-doc-nav__link ">
    7.1.1. About containers and RHEL kernel memory
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-runtimes" class="j-doc-nav__link ">
    7.1.2. About the container engine and container runtime
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-init" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.2. Using Init Containers to perform tasks before a pod is deployed
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.2. Using Init Containers to perform tasks before a pod is deployed"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.2. Using Init Containers to perform tasks before a pod is deployed"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-init-about_nodes-containers-init" class="j-doc-nav__link ">
    7.2.1. Understanding Init Containers
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-init-creating_nodes-containers-init" class="j-doc-nav__link ">
    7.2.2. Creating Init Containers
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-volumes" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.3. Using volumes to persist container data
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.3. Using volumes to persist container data"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.3. Using volumes to persist container data"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-volumes-about_nodes-containers-volumes" class="j-doc-nav__link ">
    7.3.1. Understanding volumes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-volumes-cli_nodes-containers-volumes" class="j-doc-nav__link ">
    7.3.2. Working with volumes using the OpenShift Container Platform CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-volumes-listing_nodes-containers-volumes" class="j-doc-nav__link ">
    7.3.3. Listing volumes and volume mounts in a pod
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-volumes-adding_nodes-containers-volumes" class="j-doc-nav__link ">
    7.3.4. Adding volumes to a pod
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-volumes-updating_nodes-containers-volumes" class="j-doc-nav__link ">
    7.3.5. Updating volumes and volume mounts in a pod
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-volumes-removing_nodes-containers-volumes" class="j-doc-nav__link ">
    7.3.6. Removing volumes and volume mounts from a pod
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-volumes-subpath_nodes-containers-volumes" class="j-doc-nav__link ">
    7.3.7. Configuring volumes for multiple uses in a pod
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-projected-volumes" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.4. Mapping volumes using projected volumes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.4. Mapping volumes using projected volumes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.4. Mapping volumes using projected volumes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-projected-volumes-about_nodes-containers-projected-volumes" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.4.1. Understanding projected volumes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.4.1. Understanding projected volumes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.4.1. Understanding projected volumes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#projected-volumes-examples_nodes-containers-projected-volumes" class="j-doc-nav__link ">
    7.4.1.1. Example Pod specs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#projected-volumes-pathing_nodes-containers-projected-volumes" class="j-doc-nav__link ">
    7.4.1.2. Pathing Considerations
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-projected-volumes-creating_nodes-containers-projected-volumes" class="j-doc-nav__link ">
    7.4.2. Configuring a Projected Volume for a Pod
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-downward-api" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.5. Allowing containers to consume API objects
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.5. Allowing containers to consume API objects"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.5. Allowing containers to consume API objects"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-projected-volumes-about_nodes-containers-downward-api" class="j-doc-nav__link ">
    7.5.1. Expose pod information to Containers using the Downward API
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-downward-api-container-values_nodes-containers-downward-api" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.5.2. Understanding how to consume container values using the downward API
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.5.2. Understanding how to consume container values using the downward API"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.5.2. Understanding how to consume container values using the downward API"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-downward-api-container-values-envars_nodes-containers-downward-api" class="j-doc-nav__link ">
    7.5.2.1. Consuming container values using environment variables
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-downward-api-container-values-plugin_nodes-containers-downward-api" class="j-doc-nav__link ">
    7.5.2.2. Consuming container values using a volume plugin
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-downward-api-container-resources-api_nodes-containers-downward-api" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.5.3. Understanding how to consume container resources using the Downward API
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.5.3. Understanding how to consume container resources using the Downward API"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.5.3. Understanding how to consume container resources using the Downward API"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-downward-api-container-resources-envars_nodes-containers-downward-api" class="j-doc-nav__link ">
    7.5.3.1. Consuming container resources using environment variables
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-downward-api-container-resources-plugin_nodes-containers-downward-api" class="j-doc-nav__link ">
    7.5.3.2. Consuming container resources using a volume plugin
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-downward-api-container-secrets_nodes-containers-downward-api" class="j-doc-nav__link ">
    7.5.4. Consuming secrets using the Downward API
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-downward-api-container-configmaps_nodes-containers-downward-api" class="j-doc-nav__link ">
    7.5.5. Consuming configuration maps using the Downward API
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-downward-api-container-envars_nodes-containers-downward-api" class="j-doc-nav__link ">
    7.5.6. Referencing environment variables
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-downward-api-container-escaping_nodes-containers-downward-api" class="j-doc-nav__link ">
    7.5.7. Escaping environment variable references
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-copying-files" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.6. Copying files to or from an OpenShift Container Platform container
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.6. Copying files to or from an OpenShift Container Platform container"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.6. Copying files to or from an OpenShift Container Platform container"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-copying-files-about_nodes-containers-copying-files" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.6.1. Understanding how to copy files
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.6.1. Understanding how to copy files"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.6.1. Understanding how to copy files"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#requirements" class="j-doc-nav__link ">
    7.6.1.1. Requirements
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-copying-files-procedure_nodes-containers-copying-files" class="j-doc-nav__link ">
    7.6.2. Copying files to and from containers
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-copying-files-rsync_nodes-containers-copying-files" class="j-doc-nav__link ">
    7.6.3. Using advanced Rsync features
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-remote-commands" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.7. Executing remote commands in an OpenShift Container Platform container
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.7. Executing remote commands in an OpenShift Container Platform container"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.7. Executing remote commands in an OpenShift Container Platform container"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-remote-commands-about_nodes-containers-remote-commands" class="j-doc-nav__link ">
    7.7.1. Executing remote commands in containers
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-remote-commands-protocol_nodes-containers-remote-commands" class="j-doc-nav__link ">
    7.7.2. Protocol for initiating a remote command from a client
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-port-forwarding" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.8. Using port forwarding to access applications in a container
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.8. Using port forwarding to access applications in a container"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.8. Using port forwarding to access applications in a container"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-port-forwarding-about_nodes-containers-port-forwarding" class="j-doc-nav__link ">
    7.8.1. Understanding port forwarding
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-port-forwarding-using_nodes-containers-port-forwarding" class="j-doc-nav__link ">
    7.8.2. Using port forwarding
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-port-forwarding-protocol_nodes-containers-port-forwarding" class="j-doc-nav__link ">
    7.8.3. Protocol for initiating port forwarding from a client
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-sysctls" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.9. Using sysctls in containers
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.9. Using sysctls in containers"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.9. Using sysctls in containers"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-sysctls-about_nodes-containers-using" class="j-doc-nav__link ">
    7.9.1. About sysctls
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#namespaced-and-node-level-sysctls" class="j-doc-nav__link ">
    7.9.2. Namespaced and node-level sysctls
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#safe_and_unsafe_sysctls_nodes-containers-using" class="j-doc-nav__link ">
    7.9.3. Safe and unsafe sysctls
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#updating-interface-specific-safe-sysctls-list_nodes-containers-using" class="j-doc-nav__link ">
    7.9.4. Updating the interface-specific safe sysctls list
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-starting-pod-safe-sysctls_nodes-containers-using" class="j-doc-nav__link ">
    7.9.5. Starting a pod with safe sysctls
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-starting-pod-with-unsafe-sysctls_nodes-containers-using" class="j-doc-nav__link ">
    7.9.6. Starting a pod with unsafe sysctls
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-sysctls-unsafe_nodes-containers-using" class="j-doc-nav__link ">
    7.9.7. Enabling unsafe sysctls
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#additional-resources_nodes-containers-sysctls" class="j-doc-nav__link ">
    7.9.8. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#working-with-clusters" class="j-doc-nav__link j-doc-nav__link--has-children">
    8. Working with clusters
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8. Working with clusters"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8. Working with clusters"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-events" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.1. Viewing system event information in an OpenShift Container Platform cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.1. Viewing system event information in an OpenShift Container Platform cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.1. Viewing system event information in an OpenShift Container Platform cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-events-about_nodes-containers-events" class="j-doc-nav__link ">
    8.1.1. Understanding events
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-events-viewing-cli_nodes-containers-events" class="j-doc-nav__link ">
    8.1.2. Viewing events using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-containers-events-list_nodes-containers-events" class="j-doc-nav__link ">
    8.1.3. List of events
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-resource-levels" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.2. Estimating the number of pods your OpenShift Container Platform nodes can hold
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.2. Estimating the number of pods your OpenShift Container Platform nodes can hold"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.2. Estimating the number of pods your OpenShift Container Platform nodes can hold"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-resource-levels-about_nodes-cluster-resource-levels" class="j-doc-nav__link ">
    8.2.1. Understanding the OpenShift Cluster Capacity Tool
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-resource-levels-command_nodes-cluster-resource-levels" class="j-doc-nav__link ">
    8.2.2. Running the OpenShift Cluster Capacity Tool on the command line
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-resource-levels-job_nodes-cluster-resource-levels" class="j-doc-nav__link ">
    8.2.3. Running the OpenShift Cluster Capacity Tool as a job inside a pod
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-pods-configuring" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.3. Configuring an OpenShift Container Platform cluster for pods
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.3. Configuring an OpenShift Container Platform cluster for pods"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.3. Configuring an OpenShift Container Platform cluster for pods"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-configuring-restart_nodes-cluster-pods" class="j-doc-nav__link ">
    8.3.1. Configuring how pods behave after restart
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-configuring-bandwidth_nodes-cluster-pods" class="j-doc-nav__link ">
    8.3.2. Limiting the bandwidth available to pods
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-pod-distruption-about_nodes-cluster-pods" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.3.3. Understanding how to use pod disruption budgets to specify the number of pods that must be up
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.3.3. Understanding how to use pod disruption budgets to specify the number of pods that must be up"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.3.3. Understanding how to use pod disruption budgets to specify the number of pods that must be up"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-pod-disruption-configuring_nodes-cluster-pods" class="j-doc-nav__link ">
    8.3.3.1. Specifying the number of pods that must be up with pod disruption budgets
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#pod-disruption-eviction-policy_nodes-cluster-pods" class="j-doc-nav__link ">
    8.3.3.2. Specifying the eviction policy for unhealthy pods
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-pods-configuring-critical_nodes-cluster-pods" class="j-doc-nav__link ">
    8.3.4. Preventing pod removal using critical pods
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-limit-ranges" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.4. Restrict resource consumption with limit ranges
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.4. Restrict resource consumption with limit ranges"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.4. Restrict resource consumption with limit ranges"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-limit-ranges-about_nodes-cluster-limit-ranges" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.4.1. About limit ranges
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.4.1. About limit ranges"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.4.1. About limit ranges"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-limit-ranges-limits_nodes-cluster-limit-ranges" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.4.1.1. About component limits
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.4.1.1. About component limits"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.4.1.1. About component limits"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-limit-container-limits" class="j-doc-nav__link ">
    8.4.1.1.1. Container limits
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-limit-pod-limits" class="j-doc-nav__link ">
    8.4.1.1.2. Pod limits
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-limit-image-limits" class="j-doc-nav__link ">
    8.4.1.1.3. Image limits
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-limit-stream-limits" class="j-doc-nav__link ">
    8.4.1.1.4. Image stream limits
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-limit-pvc-limits" class="j-doc-nav__link ">
    8.4.1.1.5. Persistent volume claim limits
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-limit-creating_nodes-cluster-limit-ranges" class="j-doc-nav__link ">
    8.4.2. Creating a Limit Range
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-limit-viewing_nodes-cluster-limit-ranges" class="j-doc-nav__link ">
    8.4.3. Viewing a limit
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-limit-ranges-deleting_nodes-cluster-limit-ranges" class="j-doc-nav__link ">
    8.4.4. Deleting a Limit Range
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-resource-configure" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.5. Configuring cluster memory to meet container memory and risk requirements
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.5. Configuring cluster memory to meet container memory and risk requirements"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.5. Configuring cluster memory to meet container memory and risk requirements"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-resource-configure-about_nodes-cluster-resource-configure" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.5.1. Understanding managing application memory
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.5.1. Understanding managing application memory"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.5.1. Understanding managing application memory"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-resource-configure-about-memory_nodes-cluster-resource-configure" class="j-doc-nav__link ">
    8.5.1.1. Managing application memory strategy
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-resource-configure-jdk_nodes-cluster-resource-configure" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.5.2. Understanding OpenJDK settings for OpenShift Container Platform
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.5.2. Understanding OpenJDK settings for OpenShift Container Platform"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.5.2. Understanding OpenJDK settings for OpenShift Container Platform"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-resource-configure-jdk-heap_nodes-cluster-resource-configure" class="j-doc-nav__link ">
    8.5.2.1. Understanding how to override the JVM maximum heap size
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-resource-configure-jdk-unused_nodes-cluster-resource-configure" class="j-doc-nav__link ">
    8.5.2.2. Understanding how to encourage the JVM to release unused memory to the operating system
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-resource-configure-jdk-proc_nodes-cluster-resource-configure" class="j-doc-nav__link ">
    8.5.2.3. Understanding how to ensure all JVM processes within a container are appropriately configured
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-resource-configure-request-limit_nodes-cluster-resource-configure" class="j-doc-nav__link ">
    8.5.3. Finding the memory request and limit from within a pod
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-resource-configure-oom_nodes-cluster-resource-configure" class="j-doc-nav__link ">
    8.5.4. Understanding OOM kill policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-resource-configure-evicted_nodes-cluster-resource-configure" class="j-doc-nav__link ">
    8.5.5. Understanding pod eviction
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-overcommit" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.6. Configuring your cluster to place pods on overcommitted nodes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.6. Configuring your cluster to place pods on overcommitted nodes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.6. Configuring your cluster to place pods on overcommitted nodes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-overcommit-resource-requests_nodes-cluster-overcommit" class="j-doc-nav__link ">
    8.6.1. Resource requests and overcommitment
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-resource-override_nodes-cluster-overcommit" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.6.2. Cluster-level overcommit using the Cluster Resource Override Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.6.2. Cluster-level overcommit using the Cluster Resource Override Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.6.2. Cluster-level overcommit using the Cluster Resource Override Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-resource-override-deploy-console_nodes-cluster-overcommit" class="j-doc-nav__link ">
    8.6.2.1. Installing the Cluster Resource Override Operator using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-resource-override-deploy-cli_nodes-cluster-overcommit" class="j-doc-nav__link ">
    8.6.2.2. Installing the Cluster Resource Override Operator using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-resource-configure_nodes-cluster-overcommit" class="j-doc-nav__link ">
    8.6.2.3. Configuring cluster-level overcommit
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-node-overcommit_nodes-cluster-overcommit" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.6.3. Node-level overcommit
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.6.3. Node-level overcommit"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.6.3. Node-level overcommit"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-overcommit-reserving-memory_nodes-cluster-overcommit" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.6.3.1. Understanding compute resources and containers
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.6.3.1. Understanding compute resources and containers"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.6.3.1. Understanding compute resources and containers"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#understanding-container-CPU-requests_nodes-cluster-overcommit" class="j-doc-nav__link ">
    8.6.3.1.1. Understanding container CPU requests
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#understanding-memory-requests-container_nodes-cluster-overcommit" class="j-doc-nav__link ">
    8.6.3.1.2. Understanding container memory requests
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-overcommit-qos-about_nodes-cluster-overcommit" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.6.3.2. Understanding overcomitment and quality of service classes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.6.3.2. Understanding overcomitment and quality of service classes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.6.3.2. Understanding overcomitment and quality of service classes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#qos-about-reserve_nodes-cluster-overcommit" class="j-doc-nav__link ">
    8.6.3.2.1. Understanding how to reserve memory across quality of service tiers
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-qos-about-swap_nodes-cluster-overcommit" class="j-doc-nav__link ">
    8.6.3.3. Understanding swap memory and QOS
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-overcommit-configure-nodes_nodes-cluster-overcommit" class="j-doc-nav__link ">
    8.6.3.4. Understanding nodes overcommitment
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-overcommit-node-enforcing_nodes-cluster-overcommit" class="j-doc-nav__link ">
    8.6.3.5. Disabling or enforcing CPU limits using CPU CFS quotas
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-overcommit-node-resources_nodes-cluster-overcommit" class="j-doc-nav__link ">
    8.6.3.6. Reserving resources for system processes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-overcommit-node-disable_nodes-cluster-overcommit" class="j-doc-nav__link ">
    8.6.3.7. Disabling overcommitment for a node
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-project-overcommit_nodes-cluster-overcommit" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.6.4. Project-level limits
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.6.4. Project-level limits"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.6.4. Project-level limits"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-overcommit-project-disable_nodes-cluster-overcommit" class="j-doc-nav__link ">
    8.6.4.1. Disabling overcommitment for a project
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-overcommit-addtl-resources" class="j-doc-nav__link ">
    8.6.5. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-cgroups-2" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.7. Configuring the Linux cgroup version on your nodes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.7. Configuring the Linux cgroup version on your nodes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.7. Configuring the Linux cgroup version on your nodes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-clusters-cgroups-2_nodes-cluster-cgroups-2" class="j-doc-nav__link ">
    8.7.1. Configuring Linux cgroup
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-enabling" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.8. Enabling features using feature gates
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.8. Enabling features using feature gates"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.8. Enabling features using feature gates"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-enabling-features-about_nodes-cluster-enabling" class="j-doc-nav__link ">
    8.8.1. Understanding feature gates
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-enabling-features-install_nodes-cluster-enabling" class="j-doc-nav__link ">
    8.8.2. Enabling feature sets at installation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-enabling-features-console_nodes-cluster-enabling" class="j-doc-nav__link ">
    8.8.3. Enabling feature sets using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-enabling-features-cli_nodes-cluster-enabling" class="j-doc-nav__link ">
    8.8.4. Enabling feature sets using the CLI
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-worker-latency-profiles" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.9. Improving cluster stability in high latency environments using worker latency profiles
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.9. Improving cluster stability in high latency environments using worker latency profiles"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.9. Improving cluster stability in high latency environments using worker latency profiles"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-worker-latency-profiles-about_nodes-cluster-worker-latency-profiles" class="j-doc-nav__link ">
    8.9.1. Understanding worker latency profiles
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-cluster-worker-latency-profiles-using_nodes-cluster-worker-latency-profiles" class="j-doc-nav__link ">
    8.9.2. Using worker latency profiles
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#remote-worker-nodes-on-the-network-edge" class="j-doc-nav__link j-doc-nav__link--has-children">
    9. Remote worker nodes on the network edge
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9. Remote worker nodes on the network edge"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9. Remote worker nodes on the network edge"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-edge-remote-workers" class="j-doc-nav__link j-doc-nav__link--has-children">
    9.1. Using remote worker nodes at the network edge
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9.1. Using remote worker nodes at the network edge"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9.1. Using remote worker nodes at the network edge"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-rwn_con_adding-remote-worker-nodes_nodes-edge-remote-workers" class="j-doc-nav__link ">
    9.1.1. Adding remote worker nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-edge-remote-workers-network_nodes-edge-remote-workers" class="j-doc-nav__link ">
    9.1.2. Network separation with remote worker nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-edge-remote-workers-power_nodes-edge-remote-workers" class="j-doc-nav__link ">
    9.1.3. Power loss on remote worker nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-edge-remote-workers-latency" class="j-doc-nav__link ">
    9.1.4. Latency spikes or temporary reduction in throughput to remote workers
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-edge-remote-workers-strategies_nodes-edge-remote-workers" class="j-doc-nav__link ">
    9.1.5. Remote worker node strategies
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#worker-nodes-for-single-node-openshift-clusters" class="j-doc-nav__link j-doc-nav__link--has-children">
    10. Worker nodes for single-node OpenShift clusters
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10. Worker nodes for single-node OpenShift clusters"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10. Worker nodes for single-node OpenShift clusters"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#nodes-sno-worker-nodes" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.1. Adding worker nodes to single-node OpenShift clusters
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.1. Adding worker nodes to single-node OpenShift clusters"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.1. Adding worker nodes to single-node OpenShift clusters"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#ai-sno-requirements-for-installing-worker-nodes_add-workers" class="j-doc-nav__link ">
    10.1.1. Requirements for installing single-node OpenShift worker nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#sno-adding-worker-nodes-to-sno-clusters_add-workers" class="j-doc-nav__link ">
    10.1.2. Adding worker nodes using the Assisted Installer and OpenShift Cluster Manager
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#adding-worker-nodes-using-the-assisted-installer-api" class="j-doc-nav__link j-doc-nav__link--has-children">
    10.1.3. Adding worker nodes using the Assisted Installer API
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10.1.3. Adding worker nodes using the Assisted Installer API"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10.1.3. Adding worker nodes using the Assisted Installer API"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#ai-authenticating-against-ai-rest-api_add-workers" class="j-doc-nav__link ">
    10.1.3.1. Authenticating against the Assisted Installer REST API
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#ai-adding-worker-nodes-to-cluster_add-workers" class="j-doc-nav__link ">
    10.1.3.2. Adding worker nodes using the Assisted Installer REST API
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#sno-adding-worker-nodes-to-single-node-clusters-manually_add-workers" class="j-doc-nav__link ">
    10.1.4. Adding worker nodes to single-node OpenShift clusters manually
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#installation-approve-csrs_add-workers" class="j-doc-nav__link ">
    10.1.5. Approving the certificate signing requests for your machines
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                    

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes#idm140232224692304" class="j-doc-nav__link ">
    Legal Notice
  </a>
  
          </li>
              </ol>
    
  </div>
</nav>


              </div>
            </div>
            <div class="j-options-container j-options-container--mobile">
              <button class="j-sidebar__menu-trigger j-sidebar__menu-trigger--options content-expander__trigger">
                <span class="j-sidebar__menu-trigger__open-headline">
                  Settings
                </span>
                <span class="j-sidebar__menu-trigger__close-text">Close</span>
              </button>
              

  <ul class="j-doc-options__list content-expander">
    <li class="j-doc-options__item">
          <label class="j-doc-option__label j-doc-option__label--language" for="j-doc-language">
        Language:
      </label>
      <select id="j-doc-language" class="j-doc-option__select">
                  <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes" selected=''>
            English
          </option>
                  <option value="/documentation/ja-jp/openshift_container_platform/4.13/html-single/nodes" >
            日本語
          </option>
                  <option value="/documentation/zh-cn/openshift_container_platform/4.13/html-single/nodes" >
            简体中文
          </option>
                  <option value="/documentation/ko-kr/openshift_container_platform/4.13/html-single/nodes" >
            한국어
          </option>
              </select>

            <noscript>
        <div class="j-doc-option__label j-doc-option__label--language" id="j-doc-option__label--language--nojs">
          Language:
        </div>
        <ul aria-labelledby="j-doc-option__label--language--nojs" class="j-doc-option__languages-list">
                      <li>
              <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes">English</a>
            </li>
                      <li>
              <a href="/documentation/ja-jp/openshift_container_platform/4.13/html-single/nodes">日本語</a>
            </li>
                      <li>
              <a href="/documentation/zh-cn/openshift_container_platform/4.13/html-single/nodes">简体中文</a>
            </li>
                      <li>
              <a href="/documentation/ko-kr/openshift_container_platform/4.13/html-single/nodes">한국어</a>
            </li>
                  </ul>
      </noscript>

      </li>

    <li class="j-doc-options__item">
    <label for="j-doc-mode" class="j-doc-option__label j-doc-option__label--format">
      Format:
    </label>
    <select id="j-doc-mode" class="j-doc-option__select">
              <option value="/documentation/en-us/openshift_container_platform/4.13/html/nodes"  class="j-doc-options__option j-doc-options__option--multi-page">
          Multi-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes" selected='' class="j-doc-options__option j-doc-options__option--single-page">
          Single-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/pdf/nodes/OpenShift_Container_Platform-4.13-Nodes-en-US.pdf"  class="j-doc-options__option j-doc-options__option--pdf">
          PDF
        </option>
          </select>

        <noscript>
      <div class="j-doc-option__label j-doc-option__label--format" id="j-doc-option__label--format--nojs">
        Format:
      </div>
      <ul class="j-doc-option__format-list" aria-labelledby="j-doc-option__label--format--nojs">
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--multi-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html/nodes">Multi-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--single-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes">Single-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--pdf"><a href="/documentation/en-us/openshift_container_platform/4.13/pdf/nodes/OpenShift_Container_Platform-4.13-Nodes-en-US.pdf">PDF</a></li>
              </ul>
    </noscript>
  </li>
</ul>


              </div>
          </div>
                <div class="pvof-doc__tertiary-sidebar j-doc__tertiary-sidebar">
          <div class="pvof-doc__tertiary-sidebar__inner j-doc__tertiary-sidebar__inner">
            <div class="j-doc__doc-options">
              <div class="j-options-container j-options-container--desktop">
                <button class="j-sidebar__menu-trigger j-sidebar__menu-trigger--tablet content-expander__trigger">
                  <span class="j-sidebar__menu-trigger-icon"></span>
                  <h2 class="visually-hidden">Language and Page Formatting Options</h2>
                </button>
                  

  <ul class="j-doc-options__list content-expander">
    <li class="j-doc-options__item">
          <label class="j-doc-option__label j-doc-option__label--language" for="j-doc-language--2">
        Language:
      </label>
      <select id="j-doc-language--2" class="j-doc-option__select">
                  <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes" selected=''>
            English
          </option>
                  <option value="/documentation/ja-jp/openshift_container_platform/4.13/html-single/nodes" >
            日本語
          </option>
                  <option value="/documentation/zh-cn/openshift_container_platform/4.13/html-single/nodes" >
            简体中文
          </option>
                  <option value="/documentation/ko-kr/openshift_container_platform/4.13/html-single/nodes" >
            한국어
          </option>
              </select>

            <noscript>
        <div class="j-doc-option__label j-doc-option__label--language" id="j-doc-option__label--language--nojs">
          Language:
        </div>
        <ul aria-labelledby="j-doc-option__label--language--nojs" class="j-doc-option__languages-list">
                      <li>
              <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes">English</a>
            </li>
                      <li>
              <a href="/documentation/ja-jp/openshift_container_platform/4.13/html-single/nodes">日本語</a>
            </li>
                      <li>
              <a href="/documentation/zh-cn/openshift_container_platform/4.13/html-single/nodes">简体中文</a>
            </li>
                      <li>
              <a href="/documentation/ko-kr/openshift_container_platform/4.13/html-single/nodes">한국어</a>
            </li>
                  </ul>
      </noscript>

      </li>

    <li class="j-doc-options__item">
    <label for="j-doc-mode--2" class="j-doc-option__label j-doc-option__label--format">
      Format:
    </label>
    <select id="j-doc-mode--2" class="j-doc-option__select">
              <option value="/documentation/en-us/openshift_container_platform/4.13/html/nodes"  class="j-doc-options__option j-doc-options__option--multi-page">
          Multi-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes" selected='' class="j-doc-options__option j-doc-options__option--single-page">
          Single-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/pdf/nodes/OpenShift_Container_Platform-4.13-Nodes-en-US.pdf"  class="j-doc-options__option j-doc-options__option--pdf">
          PDF
        </option>
          </select>

        <noscript>
      <div class="j-doc-option__label j-doc-option__label--format" id="j-doc-option__label--format--nojs">
        Format:
      </div>
      <ul class="j-doc-option__format-list" aria-labelledby="j-doc-option__label--format--nojs">
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--multi-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html/nodes">Multi-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--single-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html-single/nodes">Single-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--pdf"><a href="/documentation/en-us/openshift_container_platform/4.13/pdf/nodes/OpenShift_Container_Platform-4.13-Nodes-en-US.pdf">PDF</a></li>
              </ul>
    </noscript>
  </li>
</ul>


                </div>
              </div>
          </div>
        </div>

                  <div class="doc-wrapper pvof-doc__wrapper j-superdoc__content-wrapper" id="doc-wrapper">
            

  <div class="pane-page-title">
    <h1 class="title" itemprop="name">Nodes</h1>
  </div>


  <div xml:lang="en-US" class="book" id="idm140232242885776"><div class="titlepage"><div><div class="producttitle"><span class="productname">OpenShift Container Platform</span> <span class="productnumber">4.13</span></div><div><h2 class="subtitle">Configuring and managing nodes in OpenShift Container Platform </h2></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat OpenShift Documentation Team</span></div></div><div><a href="#idm140232224692304">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				This document provides instructions for configuring and managing the nodes, Pods, and containers in your cluster. It also provides information on configuring Pod scheduling and placement, using jobs and DaemonSets to automate tasks, and other tasks to ensure an efficient cluster.
			</div></div></div></div><hr/></div><section class="chapter" id="overview-of-nodes"><div class="titlepage"><div><div><h1 class="title">Chapter 1. Overview of nodes</h1></div></div></div><section class="section" id="nodes-overview"><div class="titlepage"><div><div><h2 class="title">1.1. About nodes</h2></div></div></div><p>
				A node is a virtual or bare-metal machine in a Kubernetes cluster. Worker nodes host your application containers, grouped as pods. The control plane nodes run services that are required to control the Kubernetes cluster. In OpenShift Container Platform, the control plane nodes contain more than just the Kubernetes services for managing the OpenShift Container Platform cluster.
			</p><p>
				Having stable and healthy nodes in a cluster is fundamental to the smooth functioning of your hosted application. In OpenShift Container Platform, you can access, manage, and monitor a node through the <code class="literal">Node</code> object representing the node. Using the OpenShift CLI (<code class="literal">oc</code>) or the web console, you can perform the following operations on a node.
			</p><p>
				The following components of a node are responsible for maintaining the running of pods and providing the Kubernetes runtime environment.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Container runtime:: The container runtime is responsible for running containers. Kubernetes offers several runtimes such as containerd, cri-o, rktlet, and Docker.
					</li><li class="listitem">
						Kubelet:: Kubelet runs on nodes and reads the container manifests. It ensures that the defined containers have started and are running. The kubelet process maintains the state of work and the node server. Kubelet manages network rules and port forwarding. The kubelet manages containers that are created by Kubernetes only.
					</li><li class="listitem">
						Kube-proxy:: Kube-proxy runs on every node in the cluster and maintains the network traffic between the Kubernetes resources. A Kube-proxy ensures that the networking environment is isolated and accessible.
					</li><li class="listitem">
						DNS:: Cluster DNS is a DNS server which serves DNS records for Kubernetes services. Containers started by Kubernetes automatically include this DNS server in their DNS searches.
					</li></ul></div><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Nodes-en-US/images/32829db347683a8f7181b419b4657e91/295_OpenShift_Nodes_Overview_1222.png" alt="Overview of control plane and worker node"/></div></div><h4 id="read-operations">Read operations</h4><p>
				The read operations allow an administrator or a developer to get information about nodes in an OpenShift Container Platform cluster.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-viewing-listing_nodes-nodes-viewing">List all the nodes in a cluster</a>.
					</li><li class="listitem">
						Get information about a node, such as memory and CPU usage, health, status, and age.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-viewing-listing-pods_nodes-nodes-viewing">List pods running on a node</a>.
					</li></ul></div><h4 id="management-operations">Management operations</h4><p>
				As an administrator, you can easily manage a node in an OpenShift Container Platform cluster through several tasks:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-working-updating_nodes-nodes-working">Add or update node labels</a>. A label is a key-value pair applied to a <code class="literal">Node</code> object. You can control the scheduling of pods using labels.
					</li><li class="listitem">
						Change node configuration using a custom resource definition (CRD), or the <code class="literal">kubeletConfig</code> object.
					</li><li class="listitem">
						Configure nodes to allow or disallow the scheduling of pods. Healthy worker nodes with a <code class="literal">Ready</code> status allow pod placement by default while the control plane nodes do not; you can change this default behavior by <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-working-marking_nodes-nodes-working">configuring the worker nodes to be unschedulable</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-working-marking_nodes-nodes-working">the control plane nodes to be schedulable</a>.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-resources-configuring">Allocate resources for nodes</a> using the <code class="literal">system-reserved</code> setting. You can allow OpenShift Container Platform to automatically determine the optimal <code class="literal">system-reserved</code> CPU and memory resources for your nodes, or you can manually determine and set the best resources for your nodes.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-managing-max-pods-proc_nodes-nodes-managing-max-pods">Configure the number of pods that can run on a node</a> based on the number of processor cores on the node, a hard limit, or both.
					</li><li class="listitem">
						Reboot a node gracefully using <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-rebooting-affinity_nodes-nodes-rebooting">pod anti-affinity</a>.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#deleting-nodes">Delete a node from a cluster</a> by scaling down the cluster using a compute machine set. To delete a node from a bare-metal cluster, you must first drain all pods on the node and then manually delete the node.
					</li></ul></div><h4 id="enhancement-operations">Enhancement operations</h4><p>
				OpenShift Container Platform allows you to do more than just access and manage nodes; as an administrator, you can perform the following tasks on nodes to make the cluster more efficient, application-friendly, and to provide a better environment for your developers.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Manage node-level tuning for high-performance applications that require some level of kernel tuning by <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-node-tuning-operator">using the Node Tuning Operator</a>.
					</li><li class="listitem">
						Enable TLS security profiles on the node to protect communication between the kubelet and the Kubernetes API server.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-daemonsets">Run background tasks on nodes automatically with daemon sets</a>. You can create and use daemon sets to create shared storage, run a logging pod on every node, or deploy a monitoring agent on all nodes.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-garbage-collection">Free node resources using garbage collection</a>. You can ensure that your nodes are running efficiently by removing terminated containers and the images not referenced by any running pods.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-kernel-arguments_nodes-nodes-managing">Add kernel arguments to a set of nodes</a>.
					</li><li class="listitem">
						Configure an OpenShift Container Platform cluster to have worker nodes at the network edge (remote worker nodes). For information on the challenges of having remote worker nodes in an OpenShift Container Platform cluster and some recommended approaches for managing pods on a remote worker node, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-edge-remote-workers">Using remote worker nodes at the network edge</a>.
					</li></ul></div></section><section class="section" id="pods-overview"><div class="titlepage"><div><div><h2 class="title">1.2. About pods</h2></div></div></div><p>
				A pod is one or more containers deployed together on a node. As a cluster administrator, you can define a pod, assign it to run on a healthy node that is ready for scheduling, and manage. A pod runs as long as the containers are running. You cannot change a pod once it is defined and is running. Some operations you can perform when working with pods are:
			</p><h4 id="read-operations-2">Read operations</h4><p>
				As an administrator, you can get information about pods in a project through the following tasks:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-viewing-project_nodes-pods-viewing">List pods associated with a project</a>, including information such as the number of replicas and restarts, current status, and age.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-viewing-usage_nodes-pods-viewing">View pod usage statistics</a> such as CPU, memory, and storage consumption.
					</li></ul></div><h4 id="management-operations-2">Management operations</h4><p>
				The following list of tasks provides an overview of how an administrator can manage pods in an OpenShift Container Platform cluster.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Control scheduling of pods using the advanced scheduling features available in OpenShift Container Platform:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Node-to-pod binding rules such as <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-pod-affinity-example-affinity_nodes-scheduler-pod-affinity">pod affinity</a>, <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-node-affinity">node affinity</a>, and <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-pod-anti-affinity-configuring_nodes-scheduler-pod-affinity">anti-affinity</a>.
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-node-selectors">Node labels and selectors</a>.
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-taints-tolerations">Taints and tolerations</a>.
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-pod-topology-spread-constraints">Pod topology spread constraints</a>.
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-secondary-scheduler-about">Secondary scheduling</a>.
							</li></ul></div></li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-descheduler">Configure the descheduler to evict pods</a> based on specific strategies so that the scheduler reschedules the pods to more appropriate nodes.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-configuring-restart_nodes-pods-configuring">Configure how pods behave after a restart using pod controllers and restart policies</a>.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-configuring-bandwidth_nodes-pods-configuring">Limit both egress and ingress traffic on a pod</a>.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-containers-volumes">Add and remove volumes to and from any object that has a pod template</a>. A volume is a mounted file system available to all the containers in a pod. Container storage is ephemeral; you can use volumes to persist container data.
					</li></ul></div><h4 id="enhancement-operations-2">Enhancement operations</h4><p>
				You can work with pods more easily and efficiently with the help of various tools and features available in OpenShift Container Platform. The following operations involve using those tools and features to better manage pods.
			</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 40%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 40%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232245152576" scope="col">Operation</th><th align="left" valign="top" id="idm140232245151600" scope="col">User</th><th align="left" valign="top" id="idm140232245150512" scope="col">More information</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232245152576"> <p>
								Create and use a horizontal pod autoscaler.
							</p>
							 </td><td align="left" valign="top" headers="idm140232245151600"> <p>
								Developer
							</p>
							 </td><td align="left" valign="top" headers="idm140232245150512"> <p>
								You can use a horizontal pod autoscaler to specify the minimum and the maximum number of pods you want to run, as well as the CPU utilization or memory utilization your pods should target. Using a horizontal pod autoscaler, you can <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-autoscaling">automatically scale pods</a>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140232245152576"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-vpa">Install and use a vertical pod autoscaler</a>.
							</p>
							 </td><td align="left" valign="top" headers="idm140232245151600"> <p>
								Administrator and developer
							</p>
							 </td><td align="left" valign="top" headers="idm140232245150512"> <p>
								As an administrator, use a vertical pod autoscaler to better use cluster resources by monitoring the resources and the resource requirements of workloads.
							</p>
							 <p>
								As a developer, use a vertical pod autoscaler to ensure your pods stay up during periods of high demand by scheduling pods to nodes that have enough resources for each pod.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140232245152576"> <p>
								Provide access to external resources using device plugins.
							</p>
							 </td><td align="left" valign="top" headers="idm140232245151600"> <p>
								Administrator
							</p>
							 </td><td align="left" valign="top" headers="idm140232245150512"> <p>
								A <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-device">device plugin</a> is a gRPC service running on nodes (external to the kubelet), which manages specific hardware resources. You can <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#methods-for-deploying-a-device-plugin_nodes-pods-device">deploy a device plugin</a> to provide a consistent and portable solution to consume hardware devices across clusters.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140232245152576"> <p>
								Provide sensitive data to pods <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-secrets">using the <code class="literal">Secret</code> object</a>.
							</p>
							 </td><td align="left" valign="top" headers="idm140232245151600"> <p>
								Administrator
							</p>
							 </td><td align="left" valign="top" headers="idm140232245150512"> <p>
								Some applications need sensitive information, such as passwords and usernames. You can use the <code class="literal">Secret</code> object to provide such information to an application pod.
							</p>
							 </td></tr></tbody></table></div></section><section class="section" id="containers-overview"><div class="titlepage"><div><div><h2 class="title">1.3. About containers</h2></div></div></div><p>
				A container is the basic unit of an OpenShift Container Platform application, which comprises the application code packaged along with its dependencies, libraries, and binaries. Containers provide consistency across environments and multiple deployment targets: physical servers, virtual machines (VMs), and private or public cloud.
			</p><p>
				Linux container technologies are lightweight mechanisms for isolating running processes and limiting access to only designated resources. As an administrator, You can perform various tasks on a Linux container, such as:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-containers-copying-files">Copy files to and from a container</a>.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-containers-downward-api">Allow containers to consume API objects</a>.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-containers-remote-commands">Execute remote commands in a container</a>.
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-containers-port-forwarding">Use port forwarding to access applications in a container</a>.
					</li></ul></div><p>
				OpenShift Container Platform provides specialized containers called <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-containers-init">Init containers</a>. Init containers run before application containers and can contain utilities or setup scripts not present in an application image. You can use an Init container to perform tasks before the rest of a pod is deployed.
			</p><p>
				Apart from performing specific tasks on nodes, pods, and containers, you can work with the overall OpenShift Container Platform cluster to keep the cluster efficient and the application pods highly available.
			</p></section><section class="section" id="nodes-about-autoscaling-pod_overview-of-nodes"><div class="titlepage"><div><div><h2 class="title">1.4. About autoscaling pods on a node</h2></div></div></div><p>
				OpenShift Container Platform offers three tools that you can use to automatically scale the number of pods on your nodes and the resources allocated to pods.
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Horizontal Pod Autoscaler</span></dt><dd><p class="simpara">
							The Horizontal Pod Autoscaler (HPA) can automatically increase or decrease the scale of a replication controller or deployment configuration, based on metrics collected from the pods that belong to that replication controller or deployment configuration.
						</p><p class="simpara">
							For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-autoscaling">Automatically scaling pods with the horizontal pod autoscaler</a>.
						</p></dd><dt><span class="term">Custom Metrics Autoscaler</span></dt><dd><p class="simpara">
							The Custom Metrics Autoscaler can automatically increase or decrease the number of pods for a deployment, stateful set, custom resource, or job based on custom metrics that are not based only on CPU or memory.
						</p><p class="simpara">
							For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-cma-autoscaling-custom">Custom Metrics Autoscaler Operator overview</a>.
						</p></dd><dt><span class="term">Vertical Pod Autoscaler</span></dt><dd><p class="simpara">
							The Vertical Pod Autoscaler (VPA) can automatically review the historic and current CPU and memory resources for containers in pods and can update the resource limits and requests based on the usage values it learns.
						</p><p class="simpara">
							For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-vpa">Automatically adjust pod resource levels with the vertical pod autoscaler</a>.
						</p></dd></dl></div></section><section class="section" id="commonterms-node"><div class="titlepage"><div><div><h2 class="title">1.5. Glossary of common terms for OpenShift Container Platform nodes</h2></div></div></div><p>
				This glossary defines common terms that are used in the <span class="emphasis"><em>node</em></span> content.
			</p><div class="variablelist" id="commonterms-node-container"><dl class="variablelist"><dt><span class="term">Container</span></dt><dd>
							It is a lightweight and executable image that comprises software and all its dependencies. Containers virtualize the operating system, as a result, you can run containers anywhere from a data center to a public or private cloud to even a developer’s laptop.
						</dd></dl></div><div class="variablelist" id="commonterms-node-daemonset"><dl class="variablelist"><dt><span class="term">Daemon set</span></dt><dd>
							Ensures that a replica of the pod runs on eligible nodes in an OpenShift Container Platform cluster.
						</dd></dl></div><div class="variablelist" id="commonterms-node-egress"><dl class="variablelist"><dt><span class="term">egress</span></dt><dd>
							The process of data sharing externally through a network’s outbound traffic from a pod.
						</dd></dl></div><div class="variablelist" id="commonterms-node-gc"><dl class="variablelist"><dt><span class="term">garbage collection</span></dt><dd>
							The process of cleaning up cluster resources, such as terminated containers and images that are not referenced by any running pods.
						</dd></dl></div><div class="variablelist" id="commonterms-node-hpa"><dl class="variablelist"><dt><span class="term">Horizontal Pod Autoscaler(HPA)</span></dt><dd>
							Implemented as a Kubernetes API resource and a controller. You can use the HPA to specify the minimum and maximum number of pods that you want to run. You can also specify the CPU or memory utilization that your pods should target. The HPA scales out and scales in pods when a given CPU or memory threshold is crossed.
						</dd></dl></div><div class="variablelist" id="commonterms-node-ingress"><dl class="variablelist"><dt><span class="term">Ingress</span></dt><dd>
							Incoming traffic to a pod.
						</dd></dl></div><div class="variablelist" id="commonterms-node-job"><dl class="variablelist"><dt><span class="term">Job</span></dt><dd>
							A process that runs to completion. A job creates one or more pod objects and ensures that the specified pods are successfully completed.
						</dd></dl></div><div class="variablelist" id="commonterms-node-label"><dl class="variablelist"><dt><span class="term">Labels</span></dt><dd>
							You can use labels, which are key-value pairs, to organise and select subsets of objects, such as a pod.
						</dd></dl></div><div class="variablelist" id="commonterms-node-nodenew"><dl class="variablelist"><dt><span class="term">Node</span></dt><dd>
							A worker machine in the OpenShift Container Platform cluster. A node can be either be a virtual machine (VM) or a physical machine.
						</dd></dl></div><div class="variablelist" id="commonterms-node-tuningop"><dl class="variablelist"><dt><span class="term">Node Tuning Operator</span></dt><dd>
							You can use the Node Tuning Operator to manage node-level tuning by using the TuneD daemon. It ensures custom tuning specifications are passed to all containerized TuneD daemons running in the cluster in the format that the daemons understand. The daemons run on all nodes in the cluster, one per node.
						</dd></dl></div><div class="variablelist" id="commonterms-node-self-remediationop"><dl class="variablelist"><dt><span class="term">Self Node Remediation Operator</span></dt><dd>
							The Operator runs on the cluster nodes and identifies and reboots nodes that are unhealthy.
						</dd></dl></div><div class="variablelist" id="commonterms-node-podnew"><dl class="variablelist"><dt><span class="term">Pod</span></dt><dd>
							One or more containers with shared resources, such as volume and IP addresses, running in your OpenShift Container Platform cluster. A pod is the smallest compute unit defined, deployed, and managed.
						</dd></dl></div><div class="variablelist" id="commonterms-node-toleration"><dl class="variablelist"><dt><span class="term">Toleration</span></dt><dd>
							Indicates that the pod is allowed (but not required) to be scheduled on nodes or node groups with matching taints. You can use tolerations to enable the scheduler to schedule pods with matching taints.
						</dd></dl></div><div class="variablelist" id="commonterms-node-taint"><dl class="variablelist"><dt><span class="term">Taint</span></dt><dd>
							A core object that comprises a key,value, and effect. Taints and tolerations work together to ensure that pods are not scheduled on irrelevant nodes.
						</dd></dl></div></section></section><section class="chapter" id="working-with-pods"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Working with pods</h1></div></div></div><section class="section" id="nodes-pods-using-pp"><div class="titlepage"><div><div><h2 class="title">2.1. Using pods</h2></div></div></div><p>
				A <span class="emphasis"><em>pod</em></span> is one or more containers deployed together on one host, and the smallest compute unit that can be defined, deployed, and managed.
			</p><section class="section" id="nodes-pods-using-about_nodes-pods-using-ssy"><div class="titlepage"><div><div><h3 class="title">2.1.1. Understanding pods</h3></div></div></div><p>
					Pods are the rough equivalent of a machine instance (physical or virtual) to a Container. Each pod is allocated its own internal IP address, therefore owning its entire port space, and containers within pods can share their local storage and networking.
				</p><p>
					Pods have a lifecycle; they are defined, then they are assigned to run on a node, then they run until their container(s) exit or they are removed for some other reason. Pods, depending on policy and exit code, might be removed after exiting, or can be retained to enable access to the logs of their containers.
				</p><p>
					OpenShift Container Platform treats pods as largely immutable; changes cannot be made to a pod definition while it is running. OpenShift Container Platform implements changes by terminating an existing pod and recreating it with modified configuration, base image(s), or both. Pods are also treated as expendable, and do not maintain state when recreated. Therefore pods should usually be managed by higher-level controllers, rather than directly by users.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						For the maximum number of pods per OpenShift Container Platform node host, see the Cluster Limits.
					</p></div></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Bare pods that are not managed by a replication controller will be not rescheduled upon node disruption.
					</p></div></div></section><section class="section" id="nodes-pods-using-example_nodes-pods-using-ssy"><div class="titlepage"><div><div><h3 class="title">2.1.2. Example pod configurations</h3></div></div></div><p>
					OpenShift Container Platform leverages the Kubernetes concept of a <span class="emphasis"><em>pod</em></span>, which is one or more containers deployed together on one host, and the smallest compute unit that can be defined, deployed, and managed.
				</p><p>
					The following is an example definition of a pod from a Rails application. It demonstrates many features of pods, most of which are discussed in other topics and thus only briefly mentioned here:
				</p><div id="example-pod-definition_nodes-pods-using-ssy" class="formalpara"><p class="title"><strong><code class="literal">Pod</code> object definition (YAML)</strong></p><p>
						
<pre class="programlisting language-yaml">kind: Pod
apiVersion: v1
metadata:
  name: example
  namespace: default
  selfLink: /api/v1/namespaces/default/pods/example
  uid: 5cc30063-0265780783bc
  resourceVersion: '165032'
  creationTimestamp: '2019-02-13T20:31:37Z'
  labels:
    app: hello-openshift <span id="CO1-1"><!--Empty--></span><span class="callout">1</span>
  annotations:
    openshift.io/scc: anyuid
spec:
  restartPolicy: Always <span id="CO1-2"><!--Empty--></span><span class="callout">2</span>
  serviceAccountName: default
  imagePullSecrets:
    - name: default-dockercfg-5zrhb
  priority: 0
  schedulerName: default-scheduler
  terminationGracePeriodSeconds: 30
  nodeName: ip-10-0-140-16.us-east-2.compute.internal
  securityContext: <span id="CO1-3"><!--Empty--></span><span class="callout">3</span>
    seLinuxOptions:
      level: 's0:c11,c10'
  containers: <span id="CO1-4"><!--Empty--></span><span class="callout">4</span>
    - resources: {}
      terminationMessagePath: /dev/termination-log
      name: hello-openshift
      securityContext:
        capabilities:
          drop:
            - MKNOD
        procMount: Default
      ports:
        - containerPort: 8080
          protocol: TCP
      imagePullPolicy: Always
      volumeMounts: <span id="CO1-5"><!--Empty--></span><span class="callout">5</span>
        - name: default-token-wbqsl
          readOnly: true
          mountPath: /var/run/secrets/kubernetes.io/serviceaccount <span id="CO1-6"><!--Empty--></span><span class="callout">6</span>
      terminationMessagePolicy: File
      image: registry.redhat.io/openshift4/ose-ogging-eventrouter:v4.3 <span id="CO1-7"><!--Empty--></span><span class="callout">7</span>
  serviceAccount: default <span id="CO1-8"><!--Empty--></span><span class="callout">8</span>
  volumes: <span id="CO1-9"><!--Empty--></span><span class="callout">9</span>
    - name: default-token-wbqsl
      secret:
        secretName: default-token-wbqsl
        defaultMode: 420
  dnsPolicy: ClusterFirst
status:
  phase: Pending
  conditions:
    - type: Initialized
      status: 'True'
      lastProbeTime: null
      lastTransitionTime: '2019-02-13T20:31:37Z'
    - type: Ready
      status: 'False'
      lastProbeTime: null
      lastTransitionTime: '2019-02-13T20:31:37Z'
      reason: ContainersNotReady
      message: 'containers with unready status: [hello-openshift]'
    - type: ContainersReady
      status: 'False'
      lastProbeTime: null
      lastTransitionTime: '2019-02-13T20:31:37Z'
      reason: ContainersNotReady
      message: 'containers with unready status: [hello-openshift]'
    - type: PodScheduled
      status: 'True'
      lastProbeTime: null
      lastTransitionTime: '2019-02-13T20:31:37Z'
  hostIP: 10.0.140.16
  startTime: '2019-02-13T20:31:37Z'
  containerStatuses:
    - name: hello-openshift
      state:
        waiting:
          reason: ContainerCreating
      lastState: {}
      ready: false
      restartCount: 0
      image: openshift/hello-openshift
      imageID: ''
  qosClass: BestEffort</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO1-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Pods can be "tagged" with one or more labels, which can then be used to select and manage groups of pods in a single operation. The labels are stored in key/value format in the <code class="literal">metadata</code> hash.
						</div></dd><dt><a href="#CO1-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							The pod restart policy with possible values <code class="literal">Always</code>, <code class="literal">OnFailure</code>, and <code class="literal">Never</code>. The default value is <code class="literal">Always</code>.
						</div></dd><dt><a href="#CO1-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							OpenShift Container Platform defines a security context for containers which specifies whether they are allowed to run as privileged containers, run as a user of their choice, and more. The default context is very restrictive but administrators can modify this as needed.
						</div></dd><dt><a href="#CO1-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							<code class="literal">containers</code> specifies an array of one or more container definitions.
						</div></dd><dt><a href="#CO1-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							The container specifies where external storage volumes are mounted within the container. In this case, there is a volume for storing access to credentials the registry needs for making requests against the OpenShift Container Platform API.
						</div></dd><dt><a href="#CO1-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							Specify the volumes to provide for the pod. Volumes mount at the specified path. Do not mount to the container root, <code class="literal">/</code>, or any path that is the same in the host and the container. This can corrupt your host system if the container is sufficiently privileged, such as the host <code class="literal">/dev/pts</code> files. It is safe to mount the host by using <code class="literal">/host</code>.
						</div></dd><dt><a href="#CO1-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							Each container in the pod is instantiated from its own container image.
						</div></dd><dt><a href="#CO1-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							Pods making requests against the OpenShift Container Platform API is a common enough pattern that there is a <code class="literal">serviceAccount</code> field for specifying which service account user the pod should authenticate as when making the requests. This enables fine-grained access control for custom infrastructure components.
						</div></dd><dt><a href="#CO1-9"><span class="callout">9</span></a> </dt><dd><div class="para">
							The pod defines storage volumes that are available to its container(s) to use. In this case, it provides an ephemeral volume for a <code class="literal">secret</code> volume containing the default service account tokens.
						</div><p>
							If you attach persistent volumes that have high file counts to pods, those pods can fail or can take a long time to start. For more information, see <a class="link" href="https://access.redhat.com/solutions/6221251">When using Persistent Volumes with high file counts in OpenShift, why do pods fail to start or take an excessive amount of time to achieve "Ready" state?</a>.
						</p></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						This pod definition does not include attributes that are filled by OpenShift Container Platform automatically after the pod is created and its lifecycle begins. The <a class="link" href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Kubernetes pod documentation</a> has details about the functionality and purpose of pods.
					</p></div></div></section><section class="section _additional-resources" id="additional-resources"><div class="titlepage"><div><div><h3 class="title">2.1.3. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							For more information on pods and storage see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/storage/#understanding-persistent-storage">Understanding persistent storage</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/storage/#understanding-ephemeral-storage">Understanding ephemeral storage</a>.
						</li></ul></div></section></section><section class="section" id="nodes-pods-viewing"><div class="titlepage"><div><div><h2 class="title">2.2. Viewing pods</h2></div></div></div><p>
				As an administrator, you can view the pods in your cluster and to determine the health of those pods and the cluster as a whole.
			</p><section class="section" id="nodes-pods-about_nodes-pods-viewing"><div class="titlepage"><div><div><h3 class="title">2.2.1. About pods</h3></div></div></div><p>
					OpenShift Container Platform leverages the Kubernetes concept of a <span class="emphasis"><em>pod</em></span>, which is one or more containers deployed together on one host, and the smallest compute unit that can be defined, deployed, and managed. Pods are the rough equivalent of a machine instance (physical or virtual) to a container.
				</p><p>
					You can view a list of pods associated with a specific project or view usage statistics about pods.
				</p></section><section class="section" id="nodes-pods-viewing-project_nodes-pods-viewing"><div class="titlepage"><div><div><h3 class="title">2.2.2. Viewing pods in a project</h3></div></div></div><p>
					You can view a list of pods associated with the current project, including the number of replica, the current status, number or restarts and the age of the pod.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To view the pods in a project:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Change to the project:
						</p><pre class="programlisting language-terminal">$ oc project &lt;project-name&gt;</pre></li><li class="listitem"><p class="simpara">
							Run the following command:
						</p><pre class="programlisting language-terminal">$ oc get pods</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc get pods</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                       READY   STATUS    RESTARTS   AGE
console-698d866b78-bnshf   1/1     Running   2          165m
console-698d866b78-m87pm   1/1     Running   2          165m</pre>

							</p></div><p class="simpara">
							Add the <code class="literal">-o wide</code> flags to view the pod IP address and the node where the pod is located.
						</p><pre class="programlisting language-terminal">$ oc get pods -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                       READY   STATUS    RESTARTS   AGE    IP            NODE                           NOMINATED NODE
console-698d866b78-bnshf   1/1     Running   2          166m   10.128.0.24   ip-10-0-152-71.ec2.internal    &lt;none&gt;
console-698d866b78-m87pm   1/1     Running   2          166m   10.129.0.23   ip-10-0-173-237.ec2.internal   &lt;none&gt;</pre>

							</p></div></li></ol></div></section><section class="section" id="nodes-pods-viewing-usage_nodes-pods-viewing"><div class="titlepage"><div><div><h3 class="title">2.2.3. Viewing pod usage statistics</h3></div></div></div><p>
					You can display usage statistics about pods, which provide the runtime environments for containers. These usage statistics include CPU, memory, and storage consumption.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You must have <code class="literal">cluster-reader</code> permission to view the usage statistics.
						</li><li class="listitem">
							Metrics must be installed to view the usage statistics.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To view the usage statistics:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Run the following command:
						</p><pre class="programlisting language-terminal">$ oc adm top pods</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc adm top pods -n openshift-console</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                         CPU(cores)   MEMORY(bytes)
console-7f58c69899-q8c8k     0m           22Mi
console-7f58c69899-xhbgg     0m           25Mi
downloads-594fcccf94-bcxk8   3m           18Mi
downloads-594fcccf94-kv4p6   2m           15Mi</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Run the following command to view the usage statistics for pods with labels:
						</p><pre class="programlisting language-terminal">$ oc adm top pod --selector=''</pre><p class="simpara">
							You must choose the selector (label query) to filter on. Supports <code class="literal">=</code>, <code class="literal">==</code>, and <code class="literal">!=</code>.
						</p><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc adm top pod --selector='name=my-pod'</pre></li></ol></div></section><section class="section" id="viewing-resource-logs-cli-console_nodes-pods-viewing"><div class="titlepage"><div><div><h3 class="title">2.2.4. Viewing resource logs</h3></div></div></div><p>
					You can view the log for various resources in the OpenShift CLI (oc) and web console. Logs read from the tail, or end, of the log.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to the OpenShift CLI (oc).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure (UI)</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							In the OpenShift Container Platform console, navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Pods</strong></span> or navigate to the pod through the resource you want to investigate.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Some resources, such as builds, do not have pods to query directly. In such instances, you can locate the <span class="strong strong"><strong>Logs</strong></span> link on the <span class="strong strong"><strong>Details</strong></span> page for the resource.
							</p></div></div></li><li class="listitem">
							Select a project from the drop-down menu.
						</li><li class="listitem">
							Click the name of the pod you want to investigate.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Logs</strong></span>.
						</li></ol></div><div class="itemizedlist"><p class="title"><strong>Procedure (CLI)</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							View the log for a specific pod:
						</p><pre class="programlisting language-terminal">$ oc logs -f &lt;pod_name&gt; -c &lt;container_name&gt;</pre><p class="simpara">
							where:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">-f</code></span></dt><dd>
										Optional: Specifies that the output follows what is being written into the logs.
									</dd><dt><span class="term"><code class="literal">&lt;pod_name&gt;</code></span></dt><dd>
										Specifies the name of the pod.
									</dd><dt><span class="term"><code class="literal">&lt;container_name&gt;</code></span></dt><dd>
										Optional: Specifies the name of a container. When a pod has more than one container, you must specify the container name.
									</dd></dl></div><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc logs ruby-58cd97df55-mww7r</pre><pre class="programlisting language-terminal">$ oc logs -f ruby-57f7f4855b-znl92 -c ruby</pre><p class="simpara">
							The contents of log files are printed out.
						</p></li><li class="listitem"><p class="simpara">
							View the log for a specific resource:
						</p><pre class="programlisting language-terminal">$ oc logs &lt;object_type&gt;/&lt;resource_name&gt; <span id="CO2-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO2-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specifies the resource type and name.
								</div></dd></dl></div><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc logs deployment/ruby</pre><p class="simpara">
							The contents of log files are printed out.
						</p></li></ul></div></section></section><section class="section" id="nodes-pods-configuring"><div class="titlepage"><div><div><h2 class="title">2.3. Configuring an OpenShift Container Platform cluster for pods</h2></div></div></div><p>
				As an administrator, you can create and maintain an efficient cluster for pods.
			</p><p>
				By keeping your cluster efficient, you can provide a better environment for your developers using such tools as what a pod does when it exits, ensuring that the required number of pods is always running, when to restart pods designed to run only once, limit the bandwidth available to pods, and how to keep pods running during disruptions.
			</p><section class="section" id="nodes-pods-configuring-restart_nodes-pods-configuring"><div class="titlepage"><div><div><h3 class="title">2.3.1. Configuring how pods behave after restart</h3></div></div></div><p>
					A pod restart policy determines how OpenShift Container Platform responds when Containers in that pod exit. The policy applies to all Containers in that pod.
				</p><p>
					The possible values are:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">Always</code> - Tries restarting a successfully exited Container on the pod continuously, with an exponential back-off delay (10s, 20s, 40s) capped at 5 minutes. The default is <code class="literal">Always</code>.
						</li><li class="listitem">
							<code class="literal">OnFailure</code> - Tries restarting a failed Container on the pod with an exponential back-off delay (10s, 20s, 40s) capped at 5 minutes.
						</li><li class="listitem">
							<code class="literal">Never</code> - Does not try to restart exited or failed Containers on the pod. Pods immediately fail and exit.
						</li></ul></div><p>
					After the pod is bound to a node, the pod will never be bound to another node. This means that a controller is necessary in order for a pod to survive node failure:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232246713728" scope="col">Condition</th><th align="left" valign="top" id="idm140232246712640" scope="col">Controller Type</th><th align="left" valign="top" id="idm140232246711552" scope="col">Restart Policy</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232246713728"> <p>
									Pods that are expected to terminate (such as batch computations)
								</p>
								 </td><td align="left" valign="top" headers="idm140232246712640"> <p>
									Job
								</p>
								 </td><td align="left" valign="top" headers="idm140232246711552"> <p>
									<code class="literal">OnFailure</code> or <code class="literal">Never</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232246713728"> <p>
									Pods that are expected to not terminate (such as web servers)
								</p>
								 </td><td align="left" valign="top" headers="idm140232246712640"> <p>
									Replication controller
								</p>
								 </td><td align="left" valign="top" headers="idm140232246711552"> <p>
									<code class="literal">Always</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232246713728"> <p>
									Pods that must run one-per-machine
								</p>
								 </td><td align="left" valign="top" headers="idm140232246712640"> <p>
									Daemon set
								</p>
								 </td><td align="left" valign="top" headers="idm140232246711552"> <p>
									Any
								</p>
								 </td></tr></tbody></table></div><p>
					If a Container on a pod fails and the restart policy is set to <code class="literal">OnFailure</code>, the pod stays on the node and the Container is restarted. If you do not want the Container to restart, use a restart policy of <code class="literal">Never</code>.
				</p><p>
					If an entire pod fails, OpenShift Container Platform starts a new pod. Developers must address the possibility that applications might be restarted in a new pod. In particular, applications must handle temporary files, locks, incomplete output, and so forth caused by previous runs.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Kubernetes architecture expects reliable endpoints from cloud providers. When a cloud provider is down, the kubelet prevents OpenShift Container Platform from restarting.
					</p><p>
						If the underlying cloud provider endpoints are not reliable, do not install a cluster using cloud provider integration. Install the cluster as if it was in a no-cloud environment. It is not recommended to toggle cloud provider integration on or off in an installed cluster.
					</p></div></div><p>
					For details on how OpenShift Container Platform uses restart policy with failed Containers, see the <a class="link" href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#example-states">Example States</a> in the Kubernetes documentation.
				</p></section><section class="section" id="nodes-pods-configuring-bandwidth_nodes-pods-configuring"><div class="titlepage"><div><div><h3 class="title">2.3.2. Limiting the bandwidth available to pods</h3></div></div></div><p>
					You can apply quality-of-service traffic shaping to a pod and effectively limit its available bandwidth. Egress traffic (from the pod) is handled by policing, which simply drops packets in excess of the configured rate. Ingress traffic (to the pod) is handled by shaping queued packets to effectively handle data. The limits you place on a pod do not affect the bandwidth of other pods.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To limit the bandwidth on a pod:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Write an object definition JSON file, and specify the data traffic speed using <code class="literal">kubernetes.io/ingress-bandwidth</code> and <code class="literal">kubernetes.io/egress-bandwidth</code> annotations. For example, to limit both pod egress and ingress bandwidth to 10M/s:
						</p><div class="formalpara"><p class="title"><strong>Limited <code class="literal">Pod</code> object definition</strong></p><p>
								
<pre class="programlisting language-json">{
    "kind": "Pod",
    "spec": {
        "containers": [
            {
                "image": "openshift/hello-openshift",
                "name": "hello-openshift"
            }
        ]
    },
    "apiVersion": "v1",
    "metadata": {
        "name": "iperf-slow",
        "annotations": {
            "kubernetes.io/ingress-bandwidth": "10M",
            "kubernetes.io/egress-bandwidth": "10M"
        }
    }
}</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create the pod using the object definition:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_or_dir_path&gt;</pre></li></ol></div></section><section class="section" id="nodes-pods-pod-distruption-about_nodes-pods-configuring"><div class="titlepage"><div><div><h3 class="title">2.3.3. Understanding how to use pod disruption budgets to specify the number of pods that must be up</h3></div></div></div><p>
					A <span class="emphasis"><em>pod disruption budget</em></span> allows the specification of safety constraints on pods during operations, such as draining a node for maintenance.
				</p><p>
					<code class="literal">PodDisruptionBudget</code> is an API object that specifies the minimum number or percentage of replicas that must be up at a time. Setting these in projects can be helpful during node maintenance (such as scaling a cluster down or a cluster upgrade) and is only honored on voluntary evictions (not on node failures).
				</p><p>
					A <code class="literal">PodDisruptionBudget</code> object’s configuration consists of the following key parts:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							A label selector, which is a label query over a set of pods.
						</li><li class="listitem"><p class="simpara">
							An availability level, which specifies the minimum number of pods that must be available simultaneously, either:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<code class="literal">minAvailable</code> is the number of pods must always be available, even during a disruption.
								</li><li class="listitem">
									<code class="literal">maxUnavailable</code> is the number of pods can be unavailable during a disruption.
								</li></ul></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						<code class="literal">Available</code> refers to the number of pods that has condition <code class="literal">Ready=True</code>. <code class="literal">Ready=True</code> refers to the pod that is able to serve requests and should be added to the load balancing pools of all matching services.
					</p><p>
						A <code class="literal">maxUnavailable</code> of <code class="literal">0%</code> or <code class="literal">0</code> or a <code class="literal">minAvailable</code> of <code class="literal">100%</code> or equal to the number of replicas is permitted but can block nodes from being drained.
					</p></div></div><p>
					You can check for pod disruption budgets across all projects with the following:
				</p><pre class="programlisting language-terminal">$ oc get poddisruptionbudget --all-namespaces</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">NAMESPACE                              NAME                                    MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
openshift-apiserver                    openshift-apiserver-pdb                 N/A             1                 1                     121m
openshift-cloud-controller-manager     aws-cloud-controller-manager            1               N/A               1                     125m
openshift-cloud-credential-operator    pod-identity-webhook                    1               N/A               1                     117m
openshift-cluster-csi-drivers          aws-ebs-csi-driver-controller-pdb       N/A             1                 1                     121m
openshift-cluster-storage-operator     csi-snapshot-controller-pdb             N/A             1                 1                     122m
openshift-cluster-storage-operator     csi-snapshot-webhook-pdb                N/A             1                 1                     122m
openshift-console                      console                                 N/A             1                 1                     116m
#...</pre>

					</p></div><p>
					The <code class="literal">PodDisruptionBudget</code> is considered healthy when there are at least <code class="literal">minAvailable</code> pods running in the system. Every pod above that limit can be evicted.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Depending on your pod priority and preemption settings, lower-priority pods might be removed despite their pod disruption budget requirements.
					</p></div></div><section class="section" id="nodes-pods-pod-disruption-configuring_nodes-pods-configuring"><div class="titlepage"><div><div><h4 class="title">2.3.3.1. Specifying the number of pods that must be up with pod disruption budgets</h4></div></div></div><p>
						You can use a <code class="literal">PodDisruptionBudget</code> object to specify the minimum number or percentage of replicas that must be up at a time.
					</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To configure a pod disruption budget:
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a YAML file with the an object definition similar to the following:
							</p><pre class="programlisting language-yaml">apiVersion: policy/v1 <span id="CO3-1"><!--Empty--></span><span class="callout">1</span>
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  minAvailable: 2  <span id="CO3-2"><!--Empty--></span><span class="callout">2</span>
  selector:  <span id="CO3-3"><!--Empty--></span><span class="callout">3</span>
    matchLabels:
      name: my-pod</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO3-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										<code class="literal">PodDisruptionBudget</code> is part of the <code class="literal">policy/v1</code> API group.
									</div></dd><dt><a href="#CO3-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The minimum number of pods that must be available simultaneously. This can be either an integer or a string specifying a percentage, for example, <code class="literal">20%</code>.
									</div></dd><dt><a href="#CO3-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										A label query over a set of resources. The result of <code class="literal">matchLabels</code> and <code class="literal">matchExpressions</code> are logically conjoined. Leave this parameter blank, for example <code class="literal">selector {}</code>, to select all pods in the project.
									</div></dd></dl></div><p class="simpara">
								Or:
							</p><pre class="programlisting language-yaml">apiVersion: policy/v1 <span id="CO4-1"><!--Empty--></span><span class="callout">1</span>
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  maxUnavailable: 25% <span id="CO4-2"><!--Empty--></span><span class="callout">2</span>
  selector: <span id="CO4-3"><!--Empty--></span><span class="callout">3</span>
    matchLabels:
      name: my-pod</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO4-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										<code class="literal">PodDisruptionBudget</code> is part of the <code class="literal">policy/v1</code> API group.
									</div></dd><dt><a href="#CO4-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The maximum number of pods that can be unavailable simultaneously. This can be either an integer or a string specifying a percentage, for example, <code class="literal">20%</code>.
									</div></dd><dt><a href="#CO4-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										A label query over a set of resources. The result of <code class="literal">matchLabels</code> and <code class="literal">matchExpressions</code> are logically conjoined. Leave this parameter blank, for example <code class="literal">selector {}</code>, to select all pods in the project.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Run the following command to add the object to project:
							</p><pre class="programlisting language-terminal">$ oc create -f &lt;/path/to/file&gt; -n &lt;project_name&gt;</pre></li></ol></div></section><section class="section" id="pod-disruption-eviction-policy_nodes-pods-configuring"><div class="titlepage"><div><div><h4 class="title">2.3.3.2. Specifying the eviction policy for unhealthy pods</h4></div></div></div><p>
						When you use pod disruption budgets (PDBs) to specify how many pods must be available simultaneously, you can also define the criteria for how unhealthy pods are considered for eviction.
					</p><p>
						You can choose one of the following policies:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">IfHealthyBudget</span></dt><dd>
									Running pods that are not yet healthy can be evicted only if the guarded application is not disrupted.
								</dd><dt><span class="term">AlwaysAllow</span></dt><dd>
									Running pods that are not yet healthy can be evicted regardless of whether the criteria in the pod disruption budget is met. This policy can help evict malfunctioning applications, such as ones with pods stuck in the <code class="literal">CrashLoopBackOff</code> state or failing to report the <code class="literal">Ready</code> status.
								</dd></dl></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Specifying the unhealthy pod eviction policy for pod disruption budgets is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
						</p><p>
							For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
						</p></div></div><p>
						To use this Technology Preview feature, you must have enabled the <code class="literal">TechPreviewNoUpgrade</code> feature set.
					</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							Enabling the <code class="literal">TechPreviewNoUpgrade</code> feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.
						</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a YAML file that defines a <code class="literal">PodDisruptionBudget</code> object and specify the unhealthy pod eviction policy:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">pod-disruption-budget.yaml</code> file</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      name: my-pod
  unhealthyPodEvictionPolicy: AlwaysAllow <span id="CO5-1"><!--Empty--></span><span class="callout">1</span></pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO5-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Choose either <code class="literal">IfHealthyBudget</code> or <code class="literal">AlwaysAllow</code> as the unhealthy pod eviction policy. The default is <code class="literal">IfHealthyBudget</code> when the <code class="literal">unhealthyPodEvictionPolicy</code> field is empty.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">PodDisruptionBudget</code> object by running the following command:
							</p><pre class="programlisting language-terminal">$ oc create -f pod-disruption-budget.yaml</pre></li></ol></div><p>
						With a PDB that has the <code class="literal">AlwaysAllow</code> unhealthy pod eviction policy set, you can now drain nodes and evict the pods for a malfunctioning application guarded by this PDB.
					</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-cluster-enabling">Enabling features using feature gates</a>
							</li><li class="listitem">
								<a class="link" href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy">Unhealthy Pod Eviction Policy</a> in the Kubernetes documentation
							</li></ul></div></section></section><section class="section" id="nodes-pods-configuring-critical_nodes-pods-configuring"><div class="titlepage"><div><div><h3 class="title">2.3.4. Preventing pod removal using critical pods</h3></div></div></div><p>
					There are a number of core components that are critical to a fully functional cluster, but, run on a regular cluster node rather than the master. A cluster might stop working properly if a critical add-on is evicted.
				</p><p>
					Pods marked as critical are not allowed to be evicted.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To make a pod critical:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">Pod</code> spec or edit existing pods to include the <code class="literal">system-cluster-critical</code> priority class:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pdb
spec:
  template:
    metadata:
      name: critical-pod
    priorityClassName: system-cluster-critical <span id="CO6-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO6-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Default priority class for pods that should never be evicted from a node.
								</div></dd></dl></div><p class="simpara">
							Alternatively, you can specify <code class="literal">system-node-critical</code> for pods that are important to the cluster but can be removed if necessary.
						</p></li><li class="listitem"><p class="simpara">
							Create the pod:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre></li></ol></div></section><section class="section" id="nodes-pods-configuring-reducing_nodes-pods-configuring"><div class="titlepage"><div><div><h3 class="title">2.3.5. Reducing pod timeouts when using persistent volumes with high file counts</h3></div></div></div><p>
					If a storage volume contains many files (~1,000,000 or greater), you might experience pod timeouts.
				</p><p>
					This can occur because, when volumes are mounted, OpenShift Container Platform recursively changes the ownership and permissions of the contents of each volume in order to match the <code class="literal">fsGroup</code> specified in a pod’s <code class="literal">securityContext</code>. For large volumes, checking and changing the ownership and permissions can be time consuming, resulting in a very slow pod startup.
				</p><p>
					You can reduce this delay by applying one of the following workarounds:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Use a security context constraint (SCC) to skip the SELinux relabeling for a volume.
						</li><li class="listitem">
							Use the <code class="literal">fsGroupChangePolicy</code> field inside an SCC to control the way that OpenShift Container Platform checks and manages ownership and permissions for a volume.
						</li><li class="listitem">
							Use the Cluster Resource Override Operator to automatically apply an SCC to skip the SELinux relabeling.
						</li><li class="listitem">
							Use a runtime class to skip the SELinux relabeling for a volume.
						</li></ul></div><p>
					For information, see <a class="link" href="https://access.redhat.com/solutions/6221251">When using Persistent Volumes with high file counts in OpenShift, why do pods fail to start or take an excessive amount of time to achieve "Ready" state?</a>.
				</p></section></section><section class="section" id="nodes-pods-autoscaling"><div class="titlepage"><div><div><h2 class="title">2.4. Automatically scaling pods with the horizontal pod autoscaler</h2></div></div></div><p>
				As a developer, you can use a horizontal pod autoscaler (HPA) to specify how OpenShift Container Platform should automatically increase or decrease the scale of a replication controller or deployment configuration, based on metrics collected from the pods that belong to that replication controller or deployment configuration. You can create an HPA for any any deployment, deployment config, replica set, replication controller, or stateful set.
			</p><p>
				For information on scaling pods based on custom metrics, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-cma-autoscaling-custom">Automatically scaling pods based on custom metrics</a>.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					It is recommended to use a <code class="literal">Deployment</code> object or <code class="literal">ReplicaSet</code> object unless you need a specific feature or behavior provided by other objects. For more information on these objects, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/building_applications/#what-deployments-are">Understanding Deployment and DeploymentConfig objects</a>.
				</p></div></div><section class="section" id="nodes-pods-autoscaling-about_nodes-pods-autoscaling"><div class="titlepage"><div><div><h3 class="title">2.4.1. Understanding horizontal pod autoscalers</h3></div></div></div><p>
					You can create a horizontal pod autoscaler to specify the minimum and maximum number of pods you want to run, as well as the CPU utilization or memory utilization your pods should target.
				</p><p>
					After you create a horizontal pod autoscaler, OpenShift Container Platform begins to query the CPU and/or memory resource metrics on the pods. When these metrics are available, the horizontal pod autoscaler computes the ratio of the current metric utilization with the desired metric utilization, and scales up or down accordingly. The query and scaling occurs at a regular interval, but can take one to two minutes before metrics become available.
				</p><p>
					For replication controllers, this scaling corresponds directly to the replicas of the replication controller. For deployment configurations, scaling corresponds directly to the replica count of the deployment configuration. Note that autoscaling applies only to the latest deployment in the <code class="literal">Complete</code> phase.
				</p><p>
					OpenShift Container Platform automatically accounts for resources and prevents unnecessary autoscaling during resource spikes, such as during start up. Pods in the <code class="literal">unready</code> state have <code class="literal">0 CPU</code> usage when scaling up and the autoscaler ignores the pods when scaling down. Pods without known metrics have <code class="literal">0% CPU</code> usage when scaling up and <code class="literal">100% CPU</code> when scaling down. This allows for more stability during the HPA decision. To use this feature, you must configure readiness checks to determine if a new pod is ready for use.
				</p><p>
					To use horizontal pod autoscalers, your cluster administrator must have properly configured cluster metrics.
				</p><section class="section" id="supported-metrics"><div class="titlepage"><div><div><h4 class="title">2.4.1.1. Supported metrics</h4></div></div></div><p>
						The following metrics are supported by horizontal pod autoscalers:
					</p><div class="table" id="idm140232247119072"><p class="title"><strong>Table 2.1. Metrics</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 23%; " class="col_1"><!--Empty--></col><col style="width: 38%; " class="col_2"><!--Empty--></col><col style="width: 38%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232243199616" scope="col">Metric</th><th align="left" valign="top" id="idm140232243198528" scope="col">Description</th><th align="left" valign="top" id="idm140232243197440" scope="col">API version</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232243199616"> <p>
										CPU utilization
									</p>
									 </td><td align="left" valign="top" headers="idm140232243198528"> <p>
										Number of CPU cores used. Can be used to calculate a percentage of the pod’s requested CPU.
									</p>
									 </td><td align="left" valign="top" headers="idm140232243197440"> <p>
										<code class="literal">autoscaling/v1</code>, <code class="literal">autoscaling/v2</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140232243199616"> <p>
										Memory utilization
									</p>
									 </td><td align="left" valign="top" headers="idm140232243198528"> <p>
										Amount of memory used. Can be used to calculate a percentage of the pod’s requested memory.
									</p>
									 </td><td align="left" valign="top" headers="idm140232243197440"> <p>
										<code class="literal">autoscaling/v2</code>
									</p>
									 </td></tr></tbody></table></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							For memory-based autoscaling, memory usage must increase and decrease proportionally to the replica count. On average:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									An increase in replica count must lead to an overall decrease in memory (working set) usage per-pod.
								</li><li class="listitem">
									A decrease in replica count must lead to an overall increase in per-pod memory usage.
								</li></ul></div><p>
							Use the OpenShift Container Platform web console to check the memory behavior of your application and ensure that your application meets these requirements before using memory-based autoscaling.
						</p></div></div><p>
						The following example shows autoscaling for the <code class="literal">image-registry</code> <code class="literal">Deployment</code> object. The initial deployment requires 3 pods. The HPA object increases the minimum to 5. If CPU usage on the pods reaches 75%, the pods increase to 7:
					</p><pre class="programlisting language-terminal">$ oc autoscale deployment/image-registry --min=5 --max=7 --cpu-percent=75</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">horizontalpodautoscaler.autoscaling/image-registry autoscaled</pre>

						</p></div><div class="formalpara"><p class="title"><strong>Sample HPA for the <code class="literal">image-registry</code> <code class="literal">Deployment</code> object with <code class="literal">minReplicas</code> set to 3</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: image-registry
  namespace: default
spec:
  maxReplicas: 7
  minReplicas: 3
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: image-registry
  targetCPUUtilizationPercentage: 75
status:
  currentReplicas: 5
  desiredReplicas: 0</pre>

						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								View the new state of the deployment:
							</p><pre class="programlisting language-terminal">$ oc get deployment image-registry</pre><p class="simpara">
								There are now 5 pods in the deployment:
							</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME             REVISION   DESIRED   CURRENT   TRIGGERED BY
image-registry   1          5         5         config</pre>

								</p></div></li></ol></div></section></section><section class="section" id="nodes-pods-autoscaling-workflow-hpa_nodes-pods-autoscaling"><div class="titlepage"><div><div><h3 class="title">2.4.2. How does the HPA work?</h3></div></div></div><p>
					The horizontal pod autoscaler (HPA) extends the concept of pod auto-scaling. The HPA lets you create and manage a group of load-balanced nodes. The HPA automatically increases or decreases the number of pods when a given CPU or memory threshold is crossed.
				</p><div class="figure" id="idm140232257850544"><p class="title"><strong>Figure 2.1. High level workflow of the HPA</strong></p><div class="figure-contents"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Nodes-en-US/images/e5f238cca06e9ee1647ad76efa36dcef/HPAflow.png" alt="workflow"/></div></div></div><p>
					The HPA is an API resource in the Kubernetes autoscaling API group. The autoscaler works as a control loop with a default of 15 seconds for the sync period. During this period, the controller manager queries the CPU, memory utilization, or both, against what is defined in the YAML file for the HPA. The controller manager obtains the utilization metrics from the resource metrics API for per-pod resource metrics like CPU or memory, for each pod that is targeted by the HPA.
				</p><p>
					If a utilization value target is set, the controller calculates the utilization value as a percentage of the equivalent resource request on the containers in each pod. The controller then takes the average of utilization across all targeted pods and produces a ratio that is used to scale the number of desired replicas. The HPA is configured to fetch metrics from <code class="literal">metrics.k8s.io</code>, which is provided by the metrics server. Because of the dynamic nature of metrics evaluation, the number of replicas can fluctuate during scaling for a group of replicas.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						To implement the HPA, all targeted pods must have a resource request set on their containers.
					</p></div></div></section><section class="section" id="nodes-pods-autoscaling-requests-and-limits-hpa_nodes-pods-autoscaling"><div class="titlepage"><div><div><h3 class="title">2.4.3. About requests and limits</h3></div></div></div><p>
					The scheduler uses the resource request that you specify for containers in a pod, to decide which node to place the pod on. The kubelet enforces the resource limit that you specify for a container to ensure that the container is not allowed to use more than the specified limit. The kubelet also reserves the request amount of that system resource specifically for that container to use.
				</p><div class="formalpara"><p class="title"><strong>How to use resource metrics?</strong></p><p>
						In the pod specifications, you must specify the resource requests, such as CPU and memory. The HPA uses this specification to determine the resource utilization and then scales the target up or down.
					</p></div><p>
					For example, the HPA object uses the following metric source:
				</p><pre class="programlisting language-yaml">type: Resource
resource:
  name: cpu
  target:
    type: Utilization
    averageUtilization: 60</pre><p>
					In this example, the HPA keeps the average utilization of the pods in the scaling target at 60%. Utilization is the ratio between the current resource usage to the requested resource of the pod.
				</p></section><section class="section" id="nodes-pods-autoscaling-best-practices-hpa_nodes-pods-autoscaling"><div class="titlepage"><div><div><h3 class="title">2.4.4. Best practices</h3></div></div></div><div class="formalpara"><p class="title"><strong>All pods must have resource requests configured</strong></p><p>
						The HPA makes a scaling decision based on the observed CPU or memory utilization values of pods in an OpenShift Container Platform cluster. Utilization values are calculated as a percentage of the resource requests of each pod. Missing resource request values can affect the optimal performance of the HPA.
					</p></div><div class="formalpara"><p class="title"><strong>Configure the cool down period</strong></p><p>
						During horizontal pod autoscaling, there might be a rapid scaling of events without a time gap. Configure the cool down period to prevent frequent replica fluctuations. You can specify a cool down period by configuring the <code class="literal">stabilizationWindowSeconds</code> field. The stabilization window is used to restrict the fluctuation of replicas count when the metrics used for scaling keep fluctuating. The autoscaling algorithm uses this window to infer a previous desired state and avoid unwanted changes to workload scale.
					</p></div><p>
					For example, a stabilization window is specified for the <code class="literal">scaleDown</code> field:
				</p><pre class="programlisting language-yaml">behavior:
  scaleDown:
    stabilizationWindowSeconds: 300</pre><p>
					In the above example, all desired states for the past 5 minutes are considered. This approximates a rolling maximum, and avoids having the scaling algorithm frequently remove pods only to trigger recreating an equivalent pod just moments later.
				</p><section class="section" id="nodes-pods-autoscaling-policies_nodes-pods-autoscaling"><div class="titlepage"><div><div><h4 class="title">2.4.4.1. Scaling policies</h4></div></div></div><p>
						The <code class="literal">autoscaling/v2</code> API allows you to add <span class="emphasis"><em>scaling policies</em></span> to a horizontal pod autoscaler. A scaling policy controls how the OpenShift Container Platform horizontal pod autoscaler (HPA) scales pods. Scaling policies allow you to restrict the rate that HPAs scale pods up or down by setting a specific number or specific percentage to scale in a specified period of time. You can also define a <span class="emphasis"><em>stabilization window</em></span>, which uses previously computed desired states to control scaling if the metrics are fluctuating. You can create multiple policies for the same scaling direction, and determine which policy is used, based on the amount of change. You can also restrict the scaling by timed iterations. The HPA scales pods during an iteration, then performs scaling, as needed, in further iterations.
					</p><div class="formalpara"><p class="title"><strong>Sample HPA object with a scaling policy</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: hpa-resource-metrics-memory
  namespace: default
spec:
  behavior:
    scaleDown: <span id="CO7-1"><!--Empty--></span><span class="callout">1</span>
      policies: <span id="CO7-2"><!--Empty--></span><span class="callout">2</span>
      - type: Pods <span id="CO7-3"><!--Empty--></span><span class="callout">3</span>
        value: 4 <span id="CO7-4"><!--Empty--></span><span class="callout">4</span>
        periodSeconds: 60 <span id="CO7-5"><!--Empty--></span><span class="callout">5</span>
      - type: Percent
        value: 10 <span id="CO7-6"><!--Empty--></span><span class="callout">6</span>
        periodSeconds: 60
      selectPolicy: Min <span id="CO7-7"><!--Empty--></span><span class="callout">7</span>
      stabilizationWindowSeconds: 300 <span id="CO7-8"><!--Empty--></span><span class="callout">8</span>
    scaleUp: <span id="CO7-9"><!--Empty--></span><span class="callout">9</span>
      policies:
      - type: Pods
        value: 5 <span id="CO7-10"><!--Empty--></span><span class="callout">10</span>
        periodSeconds: 70
      - type: Percent
        value: 12 <span id="CO7-11"><!--Empty--></span><span class="callout">11</span>
        periodSeconds: 80
      selectPolicy: Max
      stabilizationWindowSeconds: 0
...</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO7-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specifies the direction for the scaling policy, either <code class="literal">scaleDown</code> or <code class="literal">scaleUp</code>. This example creates a policy for scaling down.
							</div></dd><dt><a href="#CO7-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Defines the scaling policy.
							</div></dd><dt><a href="#CO7-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Determines if the policy scales by a specific number of pods or a percentage of pods during each iteration. The default value is <code class="literal">pods</code>.
							</div></dd><dt><a href="#CO7-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Limits the amount of scaling, either the number of pods or percentage of pods, during each iteration. There is no default value for scaling down by number of pods.
							</div></dd><dt><a href="#CO7-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Determines the length of a scaling iteration. The default value is <code class="literal">15</code> seconds.
							</div></dd><dt><a href="#CO7-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								The default value for scaling down by percentage is 100%.
							</div></dd><dt><a href="#CO7-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								Determines which policy to use first, if multiple policies are defined. Specify <code class="literal">Max</code> to use the policy that allows the highest amount of change, <code class="literal">Min</code> to use the policy that allows the lowest amount of change, or <code class="literal">Disabled</code> to prevent the HPA from scaling in that policy direction. The default value is <code class="literal">Max</code>.
							</div></dd><dt><a href="#CO7-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								Determines the time period the HPA should look back at desired states. The default value is <code class="literal">0</code>.
							</div></dd><dt><a href="#CO7-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								This example creates a policy for scaling up.
							</div></dd><dt><a href="#CO7-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								Limits the amount of scaling up by the number of pods. The default value for scaling up the number of pods is 4%.
							</div></dd><dt><a href="#CO7-11"><span class="callout">11</span></a> </dt><dd><div class="para">
								Limits the amount of scaling up by the percentage of pods. The default value for scaling up by percentage is 100%.
							</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example policy for scaling down</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: hpa-resource-metrics-memory
  namespace: default
spec:
...
  minReplicas: 20
...
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 4
        periodSeconds: 30
      - type: Percent
        value: 10
        periodSeconds: 60
      selectPolicy: Max
    scaleUp:
      selectPolicy: Disabled</pre>

						</p></div><p>
						In this example, when the number of pods is greater than 40, the percent-based policy is used for scaling down, as that policy results in a larger change, as required by the <code class="literal">selectPolicy</code>.
					</p><p>
						If there are 80 pod replicas, in the first iteration the HPA reduces the pods by 8, which is 10% of the 80 pods (based on the <code class="literal">type: Percent</code> and <code class="literal">value: 10</code> parameters), over one minute (<code class="literal">periodSeconds: 60</code>). For the next iteration, the number of pods is 72. The HPA calculates that 10% of the remaining pods is 7.2, which it rounds up to 8 and scales down 8 pods. On each subsequent iteration, the number of pods to be scaled is re-calculated based on the number of remaining pods. When the number of pods falls below 40, the pods-based policy is applied, because the pod-based number is greater than the percent-based number. The HPA reduces 4 pods at a time (<code class="literal">type: Pods</code> and <code class="literal">value: 4</code>), over 30 seconds (<code class="literal">periodSeconds: 30</code>), until there are 20 replicas remaining (<code class="literal">minReplicas</code>).
					</p><p>
						The <code class="literal">selectPolicy: Disabled</code> parameter prevents the HPA from scaling up the pods. You can manually scale up by adjusting the number of replicas in the replica set or deployment set, if needed.
					</p><p>
						If set, you can view the scaling policy by using the <code class="literal">oc edit</code> command:
					</p><pre class="programlisting language-terminal">$ oc edit hpa hpa-resource-metrics-memory</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  annotations:
    autoscaling.alpha.kubernetes.io/behavior:\
'{"ScaleUp":{"StabilizationWindowSeconds":0,"SelectPolicy":"Max","Policies":[{"Type":"Pods","Value":4,"PeriodSeconds":15},{"Type":"Percent","Value":100,"PeriodSeconds":15}]},\
"ScaleDown":{"StabilizationWindowSeconds":300,"SelectPolicy":"Min","Policies":[{"Type":"Pods","Value":4,"PeriodSeconds":60},{"Type":"Percent","Value":10,"PeriodSeconds":60}]}}'
...</pre>

						</p></div></section></section><section class="section" id="nodes-pods-autoscaling-creating-web-console_nodes-pods-autoscaling"><div class="titlepage"><div><div><h3 class="title">2.4.5. Creating a horizontal pod autoscaler by using the web console</h3></div></div></div><p>
					From the web console, you can create a horizontal pod autoscaler (HPA) that specifies the minimum and maximum number of pods you want to run on a <code class="literal">Deployment</code> or <code class="literal">DeploymentConfig</code> object. You can also define the amount of CPU or memory usage that your pods should target.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						An HPA cannot be added to deployments that are part of an Operator-backed service, Knative service, or Helm chart.
					</p></div></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To create an HPA in the web console:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							In the <span class="strong strong"><strong>Topology</strong></span> view, click the node to reveal the side pane.
						</li><li class="listitem"><p class="simpara">
							From the <span class="strong strong"><strong>Actions</strong></span> drop-down list, select <span class="strong strong"><strong>Add HorizontalPodAutoscaler</strong></span> to open the <span class="strong strong"><strong>Add HorizontalPodAutoscaler</strong></span> form.
						</p><div class="figure" id="idm140232246696448"><p class="title"><strong>Figure 2.2. Add HorizontalPodAutoscaler</strong></p><div class="figure-contents"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Nodes-en-US/images/0eaaed0e55e714f6e4f1094d5e62fa59/node-add-hpa-action.png" alt="Add HorizontalPodAutoscaler form"/></div></div></div></li><li class="listitem"><p class="simpara">
							From the <span class="strong strong"><strong>Add HorizontalPodAutoscaler</strong></span> form, define the name, minimum and maximum pod limits, the CPU and memory usage, and click <span class="strong strong"><strong>Save</strong></span>.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If any of the values for CPU and memory usage are missing, a warning is displayed.
							</p></div></div></li></ol></div><p>
					To edit an HPA in the web console:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							In the <span class="strong strong"><strong>Topology</strong></span> view, click the node to reveal the side pane.
						</li><li class="listitem">
							From the <span class="strong strong"><strong>Actions</strong></span> drop-down list, select <span class="strong strong"><strong>Edit HorizontalPodAutoscaler</strong></span> to open the <span class="strong strong"><strong>Edit Horizontal Pod Autoscaler</strong></span> form.
						</li><li class="listitem">
							From the <span class="strong strong"><strong>Edit Horizontal Pod Autoscaler</strong></span> form, edit the minimum and maximum pod limits and the CPU and memory usage, and click <span class="strong strong"><strong>Save</strong></span>.
						</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						While creating or editing the horizontal pod autoscaler in the web console, you can switch from <span class="strong strong"><strong>Form view</strong></span> to <span class="strong strong"><strong>YAML view</strong></span>.
					</p></div></div><p>
					To remove an HPA in the web console:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							In the <span class="strong strong"><strong>Topology</strong></span> view, click the node to reveal the side panel.
						</li><li class="listitem">
							From the <span class="strong strong"><strong>Actions</strong></span> drop-down list, select <span class="strong strong"><strong>Remove HorizontalPodAutoscaler</strong></span>.
						</li><li class="listitem">
							In the confirmation pop-up window, click <span class="strong strong"><strong>Remove</strong></span> to remove the HPA.
						</li></ol></div></section><section class="section" id="nodes-pods-autoscaling-creating-cpu_nodes-pods-autoscaling"><div class="titlepage"><div><div><h3 class="title">2.4.6. Creating a horizontal pod autoscaler for CPU utilization by using the CLI</h3></div></div></div><p>
					Using the OpenShift Container Platform CLI, you can create a horizontal pod autoscaler (HPA) to automatically scale an existing <code class="literal">Deployment</code>, <code class="literal">DeploymentConfig</code>, <code class="literal">ReplicaSet</code>, <code class="literal">ReplicationController</code>, or <code class="literal">StatefulSet</code> object. The HPA scales the pods associated with that object to maintain the CPU usage you specify.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						It is recommended to use a <code class="literal">Deployment</code> object or <code class="literal">ReplicaSet</code> object unless you need a specific feature or behavior provided by other objects.
					</p></div></div><p>
					The HPA increases and decreases the number of replicas between the minimum and maximum numbers to maintain the specified CPU utilization across all pods.
				</p><p>
					When autoscaling for CPU utilization, you can use the <code class="literal">oc autoscale</code> command and specify the minimum and maximum number of pods you want to run at any given time and the average CPU utilization your pods should target. If you do not specify a minimum, the pods are given default values from the OpenShift Container Platform server.
				</p><p>
					To autoscale for a specific CPU value, create a <code class="literal">HorizontalPodAutoscaler</code> object with the target CPU and pod limits.
				</p><div class="formalpara"><p class="title"><strong>Prerequisites</strong></p><p>
						To use horizontal pod autoscalers, your cluster administrator must have properly configured cluster metrics. You can use the <code class="literal">oc describe PodMetrics &lt;pod-name&gt;</code> command to determine if metrics are configured. If metrics are configured, the output appears similar to the following, with <code class="literal">Cpu</code> and <code class="literal">Memory</code> displayed under <code class="literal">Usage</code>.
					</p></div><pre class="programlisting language-terminal">$ oc describe PodMetrics openshift-kube-scheduler-ip-10-0-135-131.ec2.internal</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-text">Name:         openshift-kube-scheduler-ip-10-0-135-131.ec2.internal
Namespace:    openshift-kube-scheduler
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  metrics.k8s.io/v1beta1
Containers:
  Name:  wait-for-host-port
  Usage:
    Memory:  0
  Name:      scheduler
  Usage:
    Cpu:     8m
    Memory:  45440Ki
Kind:        PodMetrics
Metadata:
  Creation Timestamp:  2019-05-23T18:47:56Z
  Self Link:           /apis/metrics.k8s.io/v1beta1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-135-131.ec2.internal
Timestamp:             2019-05-23T18:47:56Z
Window:                1m0s
Events:                &lt;none&gt;</pre>

					</p></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To create a horizontal pod autoscaler for CPU utilization:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Perform one of the following:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To scale based on the percent of CPU utilization, create a <code class="literal">HorizontalPodAutoscaler</code> object for an existing object:
								</p><pre class="programlisting language-terminal">$ oc autoscale &lt;object_type&gt;/&lt;name&gt; \<span id="CO8-1"><!--Empty--></span><span class="callout">1</span>
  --min &lt;number&gt; \<span id="CO8-2"><!--Empty--></span><span class="callout">2</span>
  --max &lt;number&gt; \<span id="CO8-3"><!--Empty--></span><span class="callout">3</span>
  --cpu-percent=&lt;percent&gt; <span id="CO8-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO8-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the type and name of the object to autoscale. The object must exist and be a <code class="literal">Deployment</code>, <code class="literal">DeploymentConfig</code>/<code class="literal">dc</code>, <code class="literal">ReplicaSet</code>/<code class="literal">rs</code>, <code class="literal">ReplicationController</code>/<code class="literal">rc</code>, or <code class="literal">StatefulSet</code>.
										</div></dd><dt><a href="#CO8-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Optionally, specify the minimum number of replicas when scaling down.
										</div></dd><dt><a href="#CO8-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Specify the maximum number of replicas when scaling up.
										</div></dd><dt><a href="#CO8-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Specify the target average CPU utilization over all the pods, represented as a percent of requested CPU. If not specified or negative, a default autoscaling policy is used.
										</div></dd></dl></div><p class="simpara">
									For example, the following command shows autoscaling for the <code class="literal">image-registry</code> <code class="literal">Deployment</code> object. The initial deployment requires 3 pods. The HPA object increases the minimum to 5. If CPU usage on the pods reaches 75%, the pods will increase to 7:
								</p><pre class="programlisting language-terminal">$ oc autoscale deployment/image-registry --min=5 --max=7 --cpu-percent=75</pre></li><li class="listitem"><p class="simpara">
									To scale for a specific CPU value, create a YAML file similar to the following for an existing object:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
											Create a YAML file similar to the following:
										</p><pre class="programlisting language-yaml">apiVersion: autoscaling/v2 <span id="CO9-1"><!--Empty--></span><span class="callout">1</span>
kind: HorizontalPodAutoscaler
metadata:
  name: cpu-autoscale <span id="CO9-2"><!--Empty--></span><span class="callout">2</span>
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1 <span id="CO9-3"><!--Empty--></span><span class="callout">3</span>
    kind: Deployment <span id="CO9-4"><!--Empty--></span><span class="callout">4</span>
    name: example <span id="CO9-5"><!--Empty--></span><span class="callout">5</span>
  minReplicas: 1 <span id="CO9-6"><!--Empty--></span><span class="callout">6</span>
  maxReplicas: 10 <span id="CO9-7"><!--Empty--></span><span class="callout">7</span>
  metrics: <span id="CO9-8"><!--Empty--></span><span class="callout">8</span>
  - type: Resource
    resource:
      name: cpu <span id="CO9-9"><!--Empty--></span><span class="callout">9</span>
      target:
        type: AverageValue <span id="CO9-10"><!--Empty--></span><span class="callout">10</span>
        averageValue: 500m <span id="CO9-11"><!--Empty--></span><span class="callout">11</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO9-1"><span class="callout">1</span></a> </dt><dd><div class="para">
													Use the <code class="literal">autoscaling/v2</code> API.
												</div></dd><dt><a href="#CO9-2"><span class="callout">2</span></a> </dt><dd><div class="para">
													Specify a name for this horizontal pod autoscaler object.
												</div></dd><dt><a href="#CO9-3"><span class="callout">3</span></a> </dt><dd><div class="para">
													Specify the API version of the object to scale:
												</div><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
															For a <code class="literal">Deployment</code>, <code class="literal">ReplicaSet</code>, <code class="literal">Statefulset</code> object, use <code class="literal">apps/v1</code>.
														</li><li class="listitem">
															For a <code class="literal">ReplicationController</code>, use <code class="literal">v1</code>.
														</li><li class="listitem">
															For a <code class="literal">DeploymentConfig</code>, use <code class="literal">apps.openshift.io/v1</code>.
														</li></ul></div></dd><dt><a href="#CO9-4"><span class="callout">4</span></a> </dt><dd><div class="para">
													Specify the type of object. The object must be a <code class="literal">Deployment</code>, <code class="literal">DeploymentConfig</code>/<code class="literal">dc</code>, <code class="literal">ReplicaSet</code>/<code class="literal">rs</code>, <code class="literal">ReplicationController</code>/<code class="literal">rc</code>, or <code class="literal">StatefulSet</code>.
												</div></dd><dt><a href="#CO9-5"><span class="callout">5</span></a> </dt><dd><div class="para">
													Specify the name of the object to scale. The object must exist.
												</div></dd><dt><a href="#CO9-6"><span class="callout">6</span></a> </dt><dd><div class="para">
													Specify the minimum number of replicas when scaling down.
												</div></dd><dt><a href="#CO9-7"><span class="callout">7</span></a> </dt><dd><div class="para">
													Specify the maximum number of replicas when scaling up.
												</div></dd><dt><a href="#CO9-8"><span class="callout">8</span></a> </dt><dd><div class="para">
													Use the <code class="literal">metrics</code> parameter for memory utilization.
												</div></dd><dt><a href="#CO9-9"><span class="callout">9</span></a> </dt><dd><div class="para">
													Specify <code class="literal">cpu</code> for CPU utilization.
												</div></dd><dt><a href="#CO9-10"><span class="callout">10</span></a> </dt><dd><div class="para">
													Set to <code class="literal">AverageValue</code>.
												</div></dd><dt><a href="#CO9-11"><span class="callout">11</span></a> </dt><dd><div class="para">
													Set to <code class="literal">averageValue</code> with the targeted CPU value.
												</div></dd></dl></div></li><li class="listitem"><p class="simpara">
											Create the horizontal pod autoscaler:
										</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre></li></ol></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Verify that the horizontal pod autoscaler was created:
						</p><pre class="programlisting language-terminal">$ oc get hpa cpu-autoscale</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME            REFERENCE            TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
cpu-autoscale   Deployment/example   173m/500m       1         10        1          20m</pre>

							</p></div></li></ol></div></section><section class="section" id="nodes-pods-autoscaling-creating-memory_nodes-pods-autoscaling"><div class="titlepage"><div><div><h3 class="title">2.4.7. Creating a horizontal pod autoscaler object for memory utilization by using the CLI</h3></div></div></div><p>
					Using the OpenShift Container Platform CLI, you can create a horizontal pod autoscaler (HPA) to automatically scale an existing <code class="literal">Deployment</code>, <code class="literal">DeploymentConfig</code>, <code class="literal">ReplicaSet</code>, <code class="literal">ReplicationController</code>, or <code class="literal">StatefulSet</code> object. The HPA scales the pods associated with that object to maintain the average memory utilization you specify, either a direct value or a percentage of requested memory.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						It is recommended to use a <code class="literal">Deployment</code> object or <code class="literal">ReplicaSet</code> object unless you need a specific feature or behavior provided by other objects.
					</p></div></div><p>
					The HPA increases and decreases the number of replicas between the minimum and maximum numbers to maintain the specified memory utilization across all pods.
				</p><p>
					For memory utilization, you can specify the minimum and maximum number of pods and the average memory utilization your pods should target. If you do not specify a minimum, the pods are given default values from the OpenShift Container Platform server.
				</p><div class="formalpara"><p class="title"><strong>Prerequisites</strong></p><p>
						To use horizontal pod autoscalers, your cluster administrator must have properly configured cluster metrics. You can use the <code class="literal">oc describe PodMetrics &lt;pod-name&gt;</code> command to determine if metrics are configured. If metrics are configured, the output appears similar to the following, with <code class="literal">Cpu</code> and <code class="literal">Memory</code> displayed under <code class="literal">Usage</code>.
					</p></div><pre class="programlisting language-terminal">$ oc describe PodMetrics openshift-kube-scheduler-ip-10-0-129-223.compute.internal -n openshift-kube-scheduler</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-text">Name:         openshift-kube-scheduler-ip-10-0-129-223.compute.internal
Namespace:    openshift-kube-scheduler
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  metrics.k8s.io/v1beta1
Containers:
  Name:  wait-for-host-port
  Usage:
    Cpu:     0
    Memory:  0
  Name:      scheduler
  Usage:
    Cpu:     8m
    Memory:  45440Ki
Kind:        PodMetrics
Metadata:
  Creation Timestamp:  2020-02-14T22:21:14Z
  Self Link:           /apis/metrics.k8s.io/v1beta1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-129-223.compute.internal
Timestamp:             2020-02-14T22:21:14Z
Window:                5m0s
Events:                &lt;none&gt;</pre>

					</p></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To create a horizontal pod autoscaler for memory utilization:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a YAML file for one of the following:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To scale for a specific memory value, create a <code class="literal">HorizontalPodAutoscaler</code> object similar to the following for an existing object:
								</p><pre class="programlisting language-yaml">apiVersion: autoscaling/v2 <span id="CO10-1"><!--Empty--></span><span class="callout">1</span>
kind: HorizontalPodAutoscaler
metadata:
  name: hpa-resource-metrics-memory <span id="CO10-2"><!--Empty--></span><span class="callout">2</span>
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1 <span id="CO10-3"><!--Empty--></span><span class="callout">3</span>
    kind: Deployment <span id="CO10-4"><!--Empty--></span><span class="callout">4</span>
    name: example <span id="CO10-5"><!--Empty--></span><span class="callout">5</span>
  minReplicas: 1 <span id="CO10-6"><!--Empty--></span><span class="callout">6</span>
  maxReplicas: 10 <span id="CO10-7"><!--Empty--></span><span class="callout">7</span>
  metrics: <span id="CO10-8"><!--Empty--></span><span class="callout">8</span>
  - type: Resource
    resource:
      name: memory <span id="CO10-9"><!--Empty--></span><span class="callout">9</span>
      target:
        type: AverageValue <span id="CO10-10"><!--Empty--></span><span class="callout">10</span>
        averageValue: 500Mi <span id="CO10-11"><!--Empty--></span><span class="callout">11</span>
  behavior: <span id="CO10-12"><!--Empty--></span><span class="callout">12</span>
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 4
        periodSeconds: 60
      - type: Percent
        value: 10
        periodSeconds: 60
      selectPolicy: Max</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO10-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Use the <code class="literal">autoscaling/v2</code> API.
										</div></dd><dt><a href="#CO10-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Specify a name for this horizontal pod autoscaler object.
										</div></dd><dt><a href="#CO10-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Specify the API version of the object to scale:
										</div><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
													For a <code class="literal">Deployment</code>, <code class="literal">ReplicaSet</code>, or <code class="literal">Statefulset</code> object, use <code class="literal">apps/v1</code>.
												</li><li class="listitem">
													For a <code class="literal">ReplicationController</code>, use <code class="literal">v1</code>.
												</li><li class="listitem">
													For a <code class="literal">DeploymentConfig</code>, use <code class="literal">apps.openshift.io/v1</code>.
												</li></ul></div></dd><dt><a href="#CO10-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Specify the type of object. The object must be a <code class="literal">Deployment</code>, <code class="literal">DeploymentConfig</code>, <code class="literal">ReplicaSet</code>, <code class="literal">ReplicationController</code>, or <code class="literal">StatefulSet</code>.
										</div></dd><dt><a href="#CO10-5"><span class="callout">5</span></a> </dt><dd><div class="para">
											Specify the name of the object to scale. The object must exist.
										</div></dd><dt><a href="#CO10-6"><span class="callout">6</span></a> </dt><dd><div class="para">
											Specify the minimum number of replicas when scaling down.
										</div></dd><dt><a href="#CO10-7"><span class="callout">7</span></a> </dt><dd><div class="para">
											Specify the maximum number of replicas when scaling up.
										</div></dd><dt><a href="#CO10-8"><span class="callout">8</span></a> </dt><dd><div class="para">
											Use the <code class="literal">metrics</code> parameter for memory utilization.
										</div></dd><dt><a href="#CO10-9"><span class="callout">9</span></a> </dt><dd><div class="para">
											Specify <code class="literal">memory</code> for memory utilization.
										</div></dd><dt><a href="#CO10-10"><span class="callout">10</span></a> </dt><dd><div class="para">
											Set the type to <code class="literal">AverageValue</code>.
										</div></dd><dt><a href="#CO10-11"><span class="callout">11</span></a> </dt><dd><div class="para">
											Specify <code class="literal">averageValue</code> and a specific memory value.
										</div></dd><dt><a href="#CO10-12"><span class="callout">12</span></a> </dt><dd><div class="para">
											Optional: Specify a scaling policy to control the rate of scaling up or down.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To scale for a percentage, create a <code class="literal">HorizontalPodAutoscaler</code> object similar to the following for an existing object:
								</p><pre class="programlisting language-yaml">apiVersion: autoscaling/v2 <span id="CO11-1"><!--Empty--></span><span class="callout">1</span>
kind: HorizontalPodAutoscaler
metadata:
  name: memory-autoscale <span id="CO11-2"><!--Empty--></span><span class="callout">2</span>
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1 <span id="CO11-3"><!--Empty--></span><span class="callout">3</span>
    kind: Deployment <span id="CO11-4"><!--Empty--></span><span class="callout">4</span>
    name: example <span id="CO11-5"><!--Empty--></span><span class="callout">5</span>
  minReplicas: 1 <span id="CO11-6"><!--Empty--></span><span class="callout">6</span>
  maxReplicas: 10 <span id="CO11-7"><!--Empty--></span><span class="callout">7</span>
  metrics: <span id="CO11-8"><!--Empty--></span><span class="callout">8</span>
  - type: Resource
    resource:
      name: memory <span id="CO11-9"><!--Empty--></span><span class="callout">9</span>
      target:
        type: Utilization <span id="CO11-10"><!--Empty--></span><span class="callout">10</span>
        averageUtilization: 50 <span id="CO11-11"><!--Empty--></span><span class="callout">11</span>
  behavior: <span id="CO11-12"><!--Empty--></span><span class="callout">12</span>
    scaleUp:
      stabilizationWindowSeconds: 180
      policies:
      - type: Pods
        value: 6
        periodSeconds: 120
      - type: Percent
        value: 10
        periodSeconds: 120
      selectPolicy: Max</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO11-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Use the <code class="literal">autoscaling/v2</code> API.
										</div></dd><dt><a href="#CO11-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Specify a name for this horizontal pod autoscaler object.
										</div></dd><dt><a href="#CO11-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Specify the API version of the object to scale:
										</div><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
													For a ReplicationController, use <code class="literal">v1</code>.
												</li><li class="listitem">
													For a DeploymentConfig, use <code class="literal">apps.openshift.io/v1</code>.
												</li><li class="listitem">
													For a Deployment, ReplicaSet, Statefulset object, use <code class="literal">apps/v1</code>.
												</li></ul></div></dd><dt><a href="#CO11-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Specify the type of object. The object must be a <code class="literal">Deployment</code>, <code class="literal">DeploymentConfig</code>, <code class="literal">ReplicaSet</code>, <code class="literal">ReplicationController</code>, or <code class="literal">StatefulSet</code>.
										</div></dd><dt><a href="#CO11-5"><span class="callout">5</span></a> </dt><dd><div class="para">
											Specify the name of the object to scale. The object must exist.
										</div></dd><dt><a href="#CO11-6"><span class="callout">6</span></a> </dt><dd><div class="para">
											Specify the minimum number of replicas when scaling down.
										</div></dd><dt><a href="#CO11-7"><span class="callout">7</span></a> </dt><dd><div class="para">
											Specify the maximum number of replicas when scaling up.
										</div></dd><dt><a href="#CO11-8"><span class="callout">8</span></a> </dt><dd><div class="para">
											Use the <code class="literal">metrics</code> parameter for memory utilization.
										</div></dd><dt><a href="#CO11-9"><span class="callout">9</span></a> </dt><dd><div class="para">
											Specify <code class="literal">memory</code> for memory utilization.
										</div></dd><dt><a href="#CO11-10"><span class="callout">10</span></a> </dt><dd><div class="para">
											Set to <code class="literal">Utilization</code>.
										</div></dd><dt><a href="#CO11-11"><span class="callout">11</span></a> </dt><dd><div class="para">
											Specify <code class="literal">averageUtilization</code> and a target average memory utilization over all the pods, represented as a percent of requested memory. The target pods must have memory requests configured.
										</div></dd><dt><a href="#CO11-12"><span class="callout">12</span></a> </dt><dd><div class="para">
											Optional: Specify a scaling policy to control the rate of scaling up or down.
										</div></dd></dl></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Create the horizontal pod autoscaler:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc create -f hpa.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">horizontalpodautoscaler.autoscaling/hpa-resource-metrics-memory created</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify that the horizontal pod autoscaler was created:
						</p><pre class="programlisting language-terminal">$ oc get hpa hpa-resource-metrics-memory</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                          REFERENCE            TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
hpa-resource-metrics-memory   Deployment/example   2441216/500Mi   1         10        1          20m</pre>

							</p></div><pre class="programlisting language-terminal">$ oc describe hpa hpa-resource-metrics-memory</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">Name:                        hpa-resource-metrics-memory
Namespace:                   default
Labels:                      &lt;none&gt;
Annotations:                 &lt;none&gt;
CreationTimestamp:           Wed, 04 Mar 2020 16:31:37 +0530
Reference:                   Deployment/example
Metrics:                     ( current / target )
  resource memory on pods:   2441216 / 500Mi
Min replicas:                1
Max replicas:                10
ReplicationController pods:  1 current / 1 desired
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    ReadyForNewScale    recommended size matches current size
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from memory resource
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:
  Type     Reason                   Age                 From                       Message
  ----     ------                   ----                ----                       -------
  Normal   SuccessfulRescale        6m34s               horizontal-pod-autoscaler  New size: 1; reason: All metrics below target</pre>

							</p></div></li></ol></div></section><section class="section" id="nodes-pods-autoscaling-status-about_nodes-pods-autoscaling"><div class="titlepage"><div><div><h3 class="title">2.4.8. Understanding horizontal pod autoscaler status conditions by using the CLI</h3></div></div></div><p>
					You can use the status conditions set to determine whether or not the horizontal pod autoscaler (HPA) is able to scale and whether or not it is currently restricted in any way.
				</p><p>
					The HPA status conditions are available with the <code class="literal">v2</code> version of the autoscaling API.
				</p><p>
					The HPA responds with the following status conditions:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							The <code class="literal">AbleToScale</code> condition indicates whether HPA is able to fetch and update metrics, as well as whether any backoff-related conditions could prevent scaling.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									A <code class="literal">True</code> condition indicates scaling is allowed.
								</li><li class="listitem">
									A <code class="literal">False</code> condition indicates scaling is not allowed for the reason specified.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							The <code class="literal">ScalingActive</code> condition indicates whether the HPA is enabled (for example, the replica count of the target is not zero) and is able to calculate desired metrics.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									A <code class="literal">True</code> condition indicates metrics is working properly.
								</li><li class="listitem">
									A <code class="literal">False</code> condition generally indicates a problem with fetching metrics.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							The <code class="literal">ScalingLimited</code> condition indicates that the desired scale was capped by the maximum or minimum of the horizontal pod autoscaler.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									A <code class="literal">True</code> condition indicates that you need to raise or lower the minimum or maximum replica count in order to scale.
								</li><li class="listitem"><p class="simpara">
									A <code class="literal">False</code> condition indicates that the requested scaling is allowed.
								</p><pre class="programlisting language-terminal">$ oc describe hpa cm-test</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-text">Name:                           cm-test
Namespace:                      prom
Labels:                         &lt;none&gt;
Annotations:                    &lt;none&gt;
CreationTimestamp:              Fri, 16 Jun 2017 18:09:22 +0000
Reference:                      ReplicationController/cm-test
Metrics:                        ( current / target )
  "http_requests" on pods:      66m / 500m
Min replicas:                   1
Max replicas:                   4
ReplicationController pods:     1 current / 1 desired
Conditions: <span id="CO12-1"><!--Empty--></span><span class="callout">1</span>
  Type              Status    Reason              Message
  ----              ------    ------              -------
  AbleToScale       True      ReadyForNewScale    the last scale time was sufficiently old as to warrant a new scale
  ScalingActive     True      ValidMetricFound    the HPA was able to successfully calculate a replica count from pods metric http_request
  ScalingLimited    False     DesiredWithinRange  the desired replica count is within the acceptable range
Events:</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO12-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The horizontal pod autoscaler status messages.
										</div></dd></dl></div></li></ul></div></li></ul></div><p>
					The following is an example of a pod that is unable to scale:
				</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-text">Conditions:
  Type         Status  Reason          Message
  ----         ------  ------          -------
  AbleToScale  False   FailedGetScale  the HPA controller was unable to get the target's current scale: no matches for kind "ReplicationController" in group "apps"
Events:
  Type     Reason          Age               From                       Message
  ----     ------          ----              ----                       -------
  Warning  FailedGetScale  6s (x3 over 36s)  horizontal-pod-autoscaler  no matches for kind "ReplicationController" in group "apps"</pre>

					</p></div><p>
					The following is an example of a pod that could not obtain the needed metrics for scaling:
				</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-text">Conditions:
  Type                  Status    Reason                    Message
  ----                  ------    ------                    -------
  AbleToScale           True     SucceededGetScale          the HPA controller was able to get the target's current scale
  ScalingActive         False    FailedGetResourceMetric    the HPA was unable to compute the replica count: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API</pre>

					</p></div><p>
					The following is an example of a pod where the requested autoscaling was less than the required minimums:
				</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-text">Conditions:
  Type              Status    Reason              Message
  ----              ------    ------              -------
  AbleToScale       True      ReadyForNewScale    the last scale time was sufficiently old as to warrant a new scale
  ScalingActive     True      ValidMetricFound    the HPA was able to successfully calculate a replica count from pods metric http_request
  ScalingLimited    False     DesiredWithinRange  the desired replica count is within the acceptable range</pre>

					</p></div><section class="section" id="nodes-pods-autoscaling-status-viewing_nodes-pods-autoscaling"><div class="titlepage"><div><div><h4 class="title">2.4.8.1. Viewing horizontal pod autoscaler status conditions by using the CLI</h4></div></div></div><p>
						You can view the status conditions set on a pod by the horizontal pod autoscaler (HPA).
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The horizontal pod autoscaler status conditions are available with the <code class="literal">v2</code> version of the autoscaling API.
						</p></div></div><div class="formalpara"><p class="title"><strong>Prerequisites</strong></p><p>
							To use horizontal pod autoscalers, your cluster administrator must have properly configured cluster metrics. You can use the <code class="literal">oc describe PodMetrics &lt;pod-name&gt;</code> command to determine if metrics are configured. If metrics are configured, the output appears similar to the following, with <code class="literal">Cpu</code> and <code class="literal">Memory</code> displayed under <code class="literal">Usage</code>.
						</p></div><pre class="programlisting language-terminal">$ oc describe PodMetrics openshift-kube-scheduler-ip-10-0-135-131.ec2.internal</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">Name:         openshift-kube-scheduler-ip-10-0-135-131.ec2.internal
Namespace:    openshift-kube-scheduler
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  metrics.k8s.io/v1beta1
Containers:
  Name:  wait-for-host-port
  Usage:
    Memory:  0
  Name:      scheduler
  Usage:
    Cpu:     8m
    Memory:  45440Ki
Kind:        PodMetrics
Metadata:
  Creation Timestamp:  2019-05-23T18:47:56Z
  Self Link:           /apis/metrics.k8s.io/v1beta1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-135-131.ec2.internal
Timestamp:             2019-05-23T18:47:56Z
Window:                1m0s
Events:                &lt;none&gt;</pre>

						</p></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To view the status conditions on a pod, use the following command with the name of the pod:
						</p></div><pre class="programlisting language-terminal">$ oc describe hpa &lt;pod-name&gt;</pre><p>
						For example:
					</p><pre class="programlisting language-terminal">$ oc describe hpa cm-test</pre><p>
						The conditions appear in the <code class="literal">Conditions</code> field in the output.
					</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">Name:                           cm-test
Namespace:                      prom
Labels:                         &lt;none&gt;
Annotations:                    &lt;none&gt;
CreationTimestamp:              Fri, 16 Jun 2017 18:09:22 +0000
Reference:                      ReplicationController/cm-test
Metrics:                        ( current / target )
  "http_requests" on pods:      66m / 500m
Min replicas:                   1
Max replicas:                   4
ReplicationController pods:     1 current / 1 desired
Conditions: <span id="CO13-1"><!--Empty--></span><span class="callout">1</span>
  Type              Status    Reason              Message
  ----              ------    ------              -------
  AbleToScale       True      ReadyForNewScale    the last scale time was sufficiently old as to warrant a new scale
  ScalingActive     True      ValidMetricFound    the HPA was able to successfully calculate a replica count from pods metric http_request
  ScalingLimited    False     DesiredWithinRange  the desired replica count is within the acceptable range</pre>

						</p></div></section></section><section class="section _additional-resources" id="additional-resources-2"><div class="titlepage"><div><div><h3 class="title">2.4.9. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							For more information on replication controllers and deployment controllers, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/building_applications/#what-deployments-are">Understanding deployments and deployment configs</a>.
						</li><li class="listitem">
							For an example on the usage of HPA, see <a class="link" href="https://cloud.redhat.com/blog/horizontal-pod-autoscaling-of-quarkus-application-based-on-memory-utilization">Horizontal Pod Autoscaling of Quarkus Application Based on Memory Utilization</a>.
						</li></ul></div></section></section><section class="section" id="nodes-pods-vpa"><div class="titlepage"><div><div><h2 class="title">2.5. Automatically adjust pod resource levels with the vertical pod autoscaler</h2></div></div></div><p>
				The OpenShift Container Platform Vertical Pod Autoscaler Operator (VPA) automatically reviews the historic and current CPU and memory resources for containers in pods and can update the resource limits and requests based on the usage values it learns. The VPA uses individual custom resources (CR) to update all of the pods associated with a workload object, such as a <code class="literal">Deployment</code>, <code class="literal">DeploymentConfig</code>, <code class="literal">StatefulSet</code>, <code class="literal">Job</code>, <code class="literal">DaemonSet</code>, <code class="literal">ReplicaSet</code>, or <code class="literal">ReplicationController</code>, in a project.
			</p><p>
				The VPA helps you to understand the optimal CPU and memory usage for your pods and can automatically maintain pod resources through the pod lifecycle.
			</p><section class="section" id="nodes-pods-vertical-autoscaler-about_nodes-pods-vertical-autoscaler"><div class="titlepage"><div><div><h3 class="title">2.5.1. About the Vertical Pod Autoscaler Operator</h3></div></div></div><p>
					The Vertical Pod Autoscaler Operator (VPA) is implemented as an API resource and a custom resource (CR). The CR determines the actions the Vertical Pod Autoscaler Operator should take with the pods associated with a specific workload object, such as a daemon set, replication controller, and so forth, in a project.
				</p><p>
					You can use the default recommender or use your own alternative recommender to autoscale based on your own algorithms.
				</p><p>
					The default recommender automatically computes historic and current CPU and memory usage for the containers in those pods and uses this data to determine optimized resource limits and requests to ensure that these pods are operating efficiently at all times. For example, the default recommender suggests reduced resources for pods that are requesting more resources than they are using and increased resources for pods that are not requesting enough.
				</p><p>
					The VPA then automatically deletes any pods that are out of alignment with these recommendations one at a time, so that your applications can continue to serve requests with no downtime. The workload objects then re-deploy the pods with the original resource limits and requests. The VPA uses a mutating admission webhook to update the pods with optimized resource limits and requests before the pods are admitted to a node. If you do not want the VPA to delete pods, you can view the VPA resource limits and requests and manually update the pods as needed.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						By default, workload objects must specify a minimum of two replicas in order for the VPA to automatically delete their pods. Workload objects that specify fewer replicas than this minimum are not deleted. If you manually delete these pods, when the workload object redeploys the pods, the VPA does update the new pods with its recommendations. You can change this minimum by modifying the <code class="literal">VerticalPodAutoscalerController</code> object as shown shown in <span class="emphasis"><em>Changing the VPA minimum value</em></span>.
					</p></div></div><p>
					For example, if you have a pod that uses 50% of the CPU but only requests 10%, the VPA determines that the pod is consuming more CPU than requested and deletes the pod. The workload object, such as replica set, restarts the pods and the VPA updates the new pod with its recommended resources.
				</p><p>
					For developers, you can use the VPA to help ensure your pods stay up during periods of high demand by scheduling pods onto nodes that have appropriate resources for each pod.
				</p><p>
					Administrators can use the VPA to better utilize cluster resources, such as preventing pods from reserving more CPU resources than needed. The VPA monitors the resources that workloads are actually using and adjusts the resource requirements so capacity is available to other workloads. The VPA also maintains the ratios between limits and requests that are specified in initial container configuration.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you stop running the VPA or delete a specific VPA CR in your cluster, the resource requests for the pods already modified by the VPA do not change. Any new pods get the resources defined in the workload object, not the previous recommendations made by the VPA.
					</p></div></div></section><section class="section" id="nodes-pods-vertical-autoscaler-install_nodes-pods-vertical-autoscaler"><div class="titlepage"><div><div><h3 class="title">2.5.2. Installing the Vertical Pod Autoscaler Operator</h3></div></div></div><p>
					You can use the OpenShift Container Platform web console to install the Vertical Pod Autoscaler Operator (VPA).
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the OpenShift Container Platform web console, click <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span>.
						</li><li class="listitem">
							Choose <span class="strong strong"><strong>VerticalPodAutoscaler</strong></span> from the list of available Operators, and click <span class="strong strong"><strong>Install</strong></span>.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>Install Operator</strong></span> page, ensure that the <span class="strong strong"><strong>Operator recommended namespace</strong></span> option is selected. This installs the Operator in the mandatory <code class="literal">openshift-vertical-pod-autoscaler</code> namespace, which is automatically created if it does not exist.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Install</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Verify the installation by listing the VPA Operator components:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Pods</strong></span>.
								</li><li class="listitem">
									Select the <code class="literal">openshift-vertical-pod-autoscaler</code> project from the drop-down menu and verify that there are four pods running.
								</li><li class="listitem">
									Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Deployments</strong></span> to verify that there are four deployments running.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Optional. Verify the installation in the OpenShift Container Platform CLI using the following command:
						</p><pre class="programlisting language-terminal">$ oc get all -n openshift-vertical-pod-autoscaler</pre><p class="simpara">
							The output shows four pods and four deplyoments:
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                    READY   STATUS    RESTARTS   AGE
pod/vertical-pod-autoscaler-operator-85b4569c47-2gmhc   1/1     Running   0          3m13s
pod/vpa-admission-plugin-default-67644fc87f-xq7k9       1/1     Running   0          2m56s
pod/vpa-recommender-default-7c54764b59-8gckt            1/1     Running   0          2m56s
pod/vpa-updater-default-7f6cc87858-47vw9                1/1     Running   0          2m56s

NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/vpa-webhook   ClusterIP   172.30.53.206   &lt;none&gt;        443/TCP   2m56s

NAME                                               READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/vertical-pod-autoscaler-operator   1/1     1            1           3m13s
deployment.apps/vpa-admission-plugin-default       1/1     1            1           2m56s
deployment.apps/vpa-recommender-default            1/1     1            1           2m56s
deployment.apps/vpa-updater-default                1/1     1            1           2m56s

NAME                                                          DESIRED   CURRENT   READY   AGE
replicaset.apps/vertical-pod-autoscaler-operator-85b4569c47   1         1         1       3m13s
replicaset.apps/vpa-admission-plugin-default-67644fc87f       1         1         1       2m56s
replicaset.apps/vpa-recommender-default-7c54764b59            1         1         1       2m56s
replicaset.apps/vpa-updater-default-7f6cc87858                1         1         1       2m56s</pre>

							</p></div></li></ol></div></section><section class="section" id="nodes-pods-vertical-autoscaler-using-about_nodes-pods-vertical-autoscaler"><div class="titlepage"><div><div><h3 class="title">2.5.3. About Using the Vertical Pod Autoscaler Operator</h3></div></div></div><p>
					To use the Vertical Pod Autoscaler Operator (VPA), you create a VPA custom resource (CR) for a workload object in your cluster. The VPA learns and applies the optimal CPU and memory resources for the pods associated with that workload object. You can use a VPA with a deployment, stateful set, job, daemon set, replica set, or replication controller workload object. The VPA CR must be in the same project as the pods you want to monitor.
				</p><p>
					You use the VPA CR to associate a workload object and specify which mode the VPA operates in:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The <code class="literal">Auto</code> and <code class="literal">Recreate</code> modes automatically apply the VPA CPU and memory recommendations throughout the pod lifetime. The VPA deletes any pods in the project that are out of alignment with its recommendations. When redeployed by the workload object, the VPA updates the new pods with its recommendations.
						</li><li class="listitem">
							The <code class="literal">Initial</code> mode automatically applies VPA recommendations only at pod creation.
						</li><li class="listitem">
							The <code class="literal">Off</code> mode only provides recommended resource limits and requests, allowing you to manually apply the recommendations. The <code class="literal">off</code> mode does not update pods.
						</li></ul></div><p>
					You can also use the CR to opt-out certain containers from VPA evaluation and updates.
				</p><p>
					For example, a pod has the following limits and requests:
				</p><pre class="programlisting language-yaml">resources:
  limits:
    cpu: 1
    memory: 500Mi
  requests:
    cpu: 500m
    memory: 100Mi</pre><p>
					After creating a VPA that is set to <code class="literal">auto</code>, the VPA learns the resource usage and deletes the pod. When redeployed, the pod uses the new resource limits and requests:
				</p><pre class="programlisting language-yaml">resources:
  limits:
    cpu: 50m
    memory: 1250Mi
  requests:
    cpu: 25m
    memory: 262144k</pre><p>
					You can view the VPA recommendations using the following command:
				</p><pre class="programlisting language-terminal">$ oc get vpa &lt;vpa-name&gt; --output yaml</pre><p>
					After a few minutes, the output shows the recommendations for CPU and memory requests, similar to the following:
				</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-yaml">...
status:
...
  recommendation:
    containerRecommendations:
    - containerName: frontend
      lowerBound:
        cpu: 25m
        memory: 262144k
      target:
        cpu: 25m
        memory: 262144k
      uncappedTarget:
        cpu: 25m
        memory: 262144k
      upperBound:
        cpu: 262m
        memory: "274357142"
    - containerName: backend
      lowerBound:
        cpu: 12m
        memory: 131072k
      target:
        cpu: 12m
        memory: 131072k
      uncappedTarget:
        cpu: 12m
        memory: 131072k
      upperBound:
        cpu: 476m
        memory: "498558823"
...</pre>

					</p></div><p>
					The output shows the recommended resources, <code class="literal">target</code>, the minimum recommended resources, <code class="literal">lowerBound</code>, the highest recommended resources, <code class="literal">upperBound</code>, and the most recent resource recommendations, <code class="literal">uncappedTarget</code>.
				</p><p>
					The VPA uses the <code class="literal">lowerBound</code> and <code class="literal">upperBound</code> values to determine if a pod needs to be updated. If a pod has resource requests below the <code class="literal">lowerBound</code> values or above the <code class="literal">upperBound</code> values, the VPA terminates and recreates the pod with the <code class="literal">target</code> values.
				</p><section class="section" id="nodes-pods-vertical-autoscaler-using-one-pod_nodes-pods-vertical-autoscaler"><div class="titlepage"><div><div><h4 class="title">2.5.3.1. Changing the VPA minimum value</h4></div></div></div><p>
						By default, workload objects must specify a minimum of two replicas in order for the VPA to automatically delete and update their pods. As a result, workload objects that specify fewer than two replicas are not automatically acted upon by the VPA. The VPA does update new pods from these workload objects if the pods are restarted by some process external to the VPA. You can change this cluster-wide minimum value by modifying the <code class="literal">minReplicas</code> parameter in the <code class="literal">VerticalPodAutoscalerController</code> custom resource (CR).
					</p><p>
						For example, if you set <code class="literal">minReplicas</code> to <code class="literal">3</code>, the VPA does not delete and update pods for workload objects that specify fewer than three replicas.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you set <code class="literal">minReplicas</code> to <code class="literal">1</code>, the VPA can delete the only pod for a workload object that specifies only one replica. You should use this setting with one-replica objects only if your workload can tolerate downtime whenever the VPA deletes a pod to adjust its resources. To avoid unwanted downtime with one-replica objects, configure the VPA CRs with the <code class="literal">podUpdatePolicy</code> set to <code class="literal">Initial</code>, which automatically updates the pod only when it is restarted by some process external to the VPA, or <code class="literal">Off</code>, which allows you to update the pod manually at an appropriate time for your application.
						</p></div></div><div class="formalpara"><p class="title"><strong>Example <code class="literal">VerticalPodAutoscalerController</code> object</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: autoscaling.openshift.io/v1
kind: VerticalPodAutoscalerController
metadata:
  creationTimestamp: "2021-04-21T19:29:49Z"
  generation: 2
  name: default
  namespace: openshift-vertical-pod-autoscaler
  resourceVersion: "142172"
  uid: 180e17e9-03cc-427f-9955-3b4d7aeb2d59
spec:
  minReplicas: 3 <span id="CO13-2"><!--Empty--></span><span class="callout">1</span>
  podMinCPUMillicores: 25
  podMinMemoryMb: 250
  recommendationOnly: false
  safetyMarginFraction: 0.15</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO13-1"><span class="callout">1</span></a> <a href="#CO13-2"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specify the minimum number of replicas in a workload object for the VPA to act on. Any objects with replicas fewer than the minimum are not automatically deleted by the VPA.
							</div></dd></dl></div></section><section class="section" id="nodes-pods-vertical-autoscaler-using-auto_nodes-pods-vertical-autoscaler"><div class="titlepage"><div><div><h4 class="title">2.5.3.2. Automatically applying VPA recommendations</h4></div></div></div><p>
						To use the VPA to automatically update pods, create a VPA CR for a specific workload object with <code class="literal">updateMode</code> set to <code class="literal">Auto</code> or <code class="literal">Recreate</code>.
					</p><p>
						When the pods are created for the workload object, the VPA constantly monitors the containers to analyze their CPU and memory needs. The VPA deletes any pods that do not meet the VPA recommendations for CPU and memory. When redeployed, the pods use the new resource limits and requests based on the VPA recommendations, honoring any pod disruption budget set for your applications. The recommendations are added to the <code class="literal">status</code> field of the VPA CR for reference.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							By default, workload objects must specify a minimum of two replicas in order for the VPA to automatically delete their pods. Workload objects that specify fewer replicas than this minimum are not deleted. If you manually delete these pods, when the workload object redeploys the pods, the VPA does update the new pods with its recommendations. You can change this minimum by modifying the <code class="literal">VerticalPodAutoscalerController</code> object as shown shown in <span class="emphasis"><em>Changing the VPA minimum value</em></span>.
						</p></div></div><div class="formalpara"><p class="title"><strong>Example VPA CR for the <code class="literal">Auto</code> mode</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: vpa-recommender
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind:       Deployment <span id="CO14-1"><!--Empty--></span><span class="callout">1</span>
    name:       frontend <span id="CO14-2"><!--Empty--></span><span class="callout">2</span>
  updatePolicy:
    updateMode: "Auto" <span id="CO14-3"><!--Empty--></span><span class="callout">3</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO14-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The type of workload object you want this VPA CR to manage.
							</div></dd><dt><a href="#CO14-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The name of the workload object you want this VPA CR to manage.
							</div></dd><dt><a href="#CO14-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Set the mode to <code class="literal">Auto</code> or <code class="literal">Recreate</code>:
							</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">Auto</code>. The VPA assigns resource requests on pod creation and updates the existing pods by terminating them when the requested resources differ significantly from the new recommendation.
									</li><li class="listitem">
										<code class="literal">Recreate</code>. The VPA assigns resource requests on pod creation and updates the existing pods by terminating them when the requested resources differ significantly from the new recommendation. This mode should be used rarely, only if you need to ensure that the pods are restarted whenever the resource request changes.
									</li></ul></div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							There must be operating pods in the project before the VPA can determine recommended resources and apply the recommendations to new pods.
						</p></div></div></section><section class="section" id="nodes-pods-vertical-autoscaler-using-pod_nodes-pods-vertical-autoscaler"><div class="titlepage"><div><div><h4 class="title">2.5.3.3. Automatically applying VPA recommendations on pod creation</h4></div></div></div><p>
						To use the VPA to apply the recommended resources only when a pod is first deployed, create a VPA CR for a specific workload object with <code class="literal">updateMode</code> set to <code class="literal">Initial</code>.
					</p><p>
						Then, manually delete any pods associated with the workload object that you want to use the VPA recommendations. In the <code class="literal">Initial</code> mode, the VPA does not delete pods and does not update the pods as it learns new resource recommendations.
					</p><div class="formalpara"><p class="title"><strong>Example VPA CR for the <code class="literal">Initial</code> mode</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: vpa-recommender
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind:       Deployment <span id="CO15-1"><!--Empty--></span><span class="callout">1</span>
    name:       frontend <span id="CO15-2"><!--Empty--></span><span class="callout">2</span>
  updatePolicy:
    updateMode: "Initial" <span id="CO15-3"><!--Empty--></span><span class="callout">3</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO15-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The type of workload object you want this VPA CR to manage.
							</div></dd><dt><a href="#CO15-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The name of the workload object you want this VPA CR to manage.
							</div></dd><dt><a href="#CO15-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Set the mode to <code class="literal">Initial</code>. The VPA assigns resources when pods are created and does not change the resources during the lifetime of the pod.
							</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							There must be operating pods in the project before a VPA can determine recommended resources and apply the recommendations to new pods.
						</p></div></div></section><section class="section" id="nodes-pods-vertical-autoscaler-using-manual_nodes-pods-vertical-autoscaler"><div class="titlepage"><div><div><h4 class="title">2.5.3.4. Manually applying VPA recommendations</h4></div></div></div><p>
						To use the VPA to only determine the recommended CPU and memory values, create a VPA CR for a specific workload object with <code class="literal">updateMode</code> set to <code class="literal">off</code>.
					</p><p>
						When the pods are created for that workload object, the VPA analyzes the CPU and memory needs of the containers and records those recommendations in the <code class="literal">status</code> field of the VPA CR. The VPA does not update the pods as it determines new resource recommendations.
					</p><div class="formalpara"><p class="title"><strong>Example VPA CR for the <code class="literal">Off</code> mode</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: vpa-recommender
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind:       Deployment <span id="CO16-1"><!--Empty--></span><span class="callout">1</span>
    name:       frontend <span id="CO16-2"><!--Empty--></span><span class="callout">2</span>
  updatePolicy:
    updateMode: "Off" <span id="CO16-3"><!--Empty--></span><span class="callout">3</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO16-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The type of workload object you want this VPA CR to manage.
							</div></dd><dt><a href="#CO16-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The name of the workload object you want this VPA CR to manage.
							</div></dd><dt><a href="#CO16-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Set the mode to <code class="literal">Off</code>.
							</div></dd></dl></div><p>
						You can view the recommendations using the following command.
					</p><pre class="programlisting language-terminal">$ oc get vpa &lt;vpa-name&gt; --output yaml</pre><p>
						With the recommendations, you can edit the workload object to add CPU and memory requests, then delete and redeploy the pods using the recommended resources.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							There must be operating pods in the project before a VPA can determine recommended resources.
						</p></div></div></section><section class="section" id="nodes-pods-vertical-autoscaler-using-exempt_nodes-pods-vertical-autoscaler"><div class="titlepage"><div><div><h4 class="title">2.5.3.5. Exempting containers from applying VPA recommendations</h4></div></div></div><p>
						If your workload object has multiple containers and you do not want the VPA to evaluate and act on all of the containers, create a VPA CR for a specific workload object and add a <code class="literal">resourcePolicy</code> to opt-out specific containers.
					</p><p>
						When the VPA updates the pods with recommended resources, any containers with a <code class="literal">resourcePolicy</code> are not updated and the VPA does not present recommendations for those containers in the pod.
					</p><pre class="programlisting language-yaml">apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: vpa-recommender
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind:       Deployment <span id="CO17-1"><!--Empty--></span><span class="callout">1</span>
    name:       frontend <span id="CO17-2"><!--Empty--></span><span class="callout">2</span>
  updatePolicy:
    updateMode: "Auto" <span id="CO17-3"><!--Empty--></span><span class="callout">3</span>
  resourcePolicy: <span id="CO17-4"><!--Empty--></span><span class="callout">4</span>
    containerPolicies:
    - containerName: my-opt-sidecar
      mode: "Off"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO17-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The type of workload object you want this VPA CR to manage.
							</div></dd><dt><a href="#CO17-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The name of the workload object you want this VPA CR to manage.
							</div></dd><dt><a href="#CO17-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Set the mode to <code class="literal">Auto</code>, <code class="literal">Recreate</code>, or <code class="literal">Off</code>. The <code class="literal">Recreate</code> mode should be used rarely, only if you need to ensure that the pods are restarted whenever the resource request changes.
							</div></dd><dt><a href="#CO17-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Specify the containers you want to opt-out and set <code class="literal">mode</code> to <code class="literal">Off</code>.
							</div></dd></dl></div><p>
						For example, a pod has two containers, the same resource requests and limits:
					</p><pre class="programlisting language-yaml"># ...
spec:
  containers:
  - name: frontend
    resources:
      limits:
        cpu: 1
        memory: 500Mi
      requests:
        cpu: 500m
        memory: 100Mi
  - name: backend
    resources:
      limits:
        cpu: "1"
        memory: 500Mi
      requests:
        cpu: 500m
        memory: 100Mi
# ...</pre><p>
						After launching a VPA CR with the <code class="literal">backend</code> container set to opt-out, the VPA terminates and recreates the pod with the recommended resources applied only to the <code class="literal">frontend</code> container:
					</p><pre class="programlisting language-yaml">...
spec:
  containers:
    name: frontend
    resources:
      limits:
        cpu: 50m
        memory: 1250Mi
      requests:
        cpu: 25m
        memory: 262144k
...
    name: backend
    resources:
      limits:
        cpu: "1"
        memory: 500Mi
      requests:
        cpu: 500m
        memory: 100Mi
...</pre></section><section class="section" id="nodes-pods-vertical-autoscaler-custom_nodes-pods-vertical-autoscaler"><div class="titlepage"><div><div><h4 class="title">2.5.3.6. Using an alternative recommender</h4></div></div></div><p>
						You can use your own recommender to autoscale based on your own algorithms. If you do not specify an alternative recommender, OpenShift Container Platform uses the default recommender, which suggests CPU and memory requests based on historical usage. Because there is no universal recommendation policy that applies to all types of workloads, you might want to create and deploy different recommenders for specific workloads.
					</p><p>
						For example, the default recommender might not accurately predict future resource usage when containers exhibit certain resource behaviors, such as cyclical patterns that alternate between usage spikes and idling as used by monitoring applications, or recurring and repeating patterns used with deep learning applications. Using the default recommender with these usage behaviors might result in significant over-provisioning and Out of Memory (OOM) kills for your applications.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Instructions for how to create a recommender are beyond the scope of this documentation,
						</p></div></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To use an alternative recommender for your pods:
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a service account for the alternative recommender and bind that service account to the required cluster role:
							</p><pre class="programlisting language-yaml">apiVersion: v1 <span id="CO18-1"><!--Empty--></span><span class="callout">1</span>
kind: ServiceAccount
metadata:
  name: alt-vpa-recommender-sa
  namespace: &lt;namespace_name&gt;
---
apiVersion: rbac.authorization.k8s.io/v1 <span id="CO18-2"><!--Empty--></span><span class="callout">2</span>
kind: ClusterRoleBinding
metadata:
  name: system:example-metrics-reader
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:metrics-reader
subjects:
- kind: ServiceAccount
  name: alt-vpa-recommender-sa
  namespace: &lt;namespace_name&gt;
---
apiVersion: rbac.authorization.k8s.io/v1 <span id="CO18-3"><!--Empty--></span><span class="callout">3</span>
kind: ClusterRoleBinding
metadata:
  name: system:example-vpa-actor
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:vpa-actor
subjects:
- kind: ServiceAccount
  name: alt-vpa-recommender-sa
  namespace: &lt;namespace_name&gt;
---
apiVersion: rbac.authorization.k8s.io/v1 <span id="CO18-4"><!--Empty--></span><span class="callout">4</span>
kind: ClusterRoleBinding
metadata:
  name: system:example-vpa-target-reader-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:vpa-target-reader
subjects:
- kind: ServiceAccount
  name: alt-vpa-recommender-sa
  namespace: &lt;namespace_name&gt;</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO18-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Creates a service accocunt for the recommender in the namespace where the recommender is deployed.
									</div></dd><dt><a href="#CO18-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Binds the recommender service account to the <code class="literal">metrics-reader</code> role. Specify the namespace where the recommender is to be deployed.
									</div></dd><dt><a href="#CO18-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Binds the recommender service account to the <code class="literal">vpa-actor</code> role. Specify the namespace where the recommender is to be deployed.
									</div></dd><dt><a href="#CO18-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										Binds the recommender service account to the <code class="literal">vpa-target-reader</code> role. Specify the namespace where the recommender is to be deployed.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								To add the alternative recommender to the cluster, create a Deployment object similar to the following:
							</p><pre class="programlisting language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: alt-vpa-recommender
  namespace: &lt;namespace_name&gt;
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alt-vpa-recommender
  template:
    metadata:
      labels:
        app: alt-vpa-recommender
    spec:
      containers: <span id="CO19-1"><!--Empty--></span><span class="callout">1</span>
      - name: recommender
        image: quay.io/example/alt-recommender:latest <span id="CO19-2"><!--Empty--></span><span class="callout">2</span>
        imagePullPolicy: Always
        resources:
          limits:
            cpu: 200m
            memory: 1000Mi
          requests:
            cpu: 50m
            memory: 500Mi
        ports:
        - name: prometheus
          containerPort: 8942
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
          seccompProfile:
            type: RuntimeDefault
      serviceAccountName: alt-vpa-recommender-sa <span id="CO19-3"><!--Empty--></span><span class="callout">3</span>
      securityContext:
        runAsNonRoot: true</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO19-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Creates a container for your alternative recommender.
									</div></dd><dt><a href="#CO19-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specifies your recommender image.
									</div></dd><dt><a href="#CO19-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Associates the service account that you created for the recommender.
									</div></dd></dl></div><p class="simpara">
								A new pod is created for the alternative recommender in the same namespace.
							</p><pre class="programlisting language-terminal">$ oc get pods</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                                        READY   STATUS    RESTARTS   AGE
frontend-845d5478d-558zf                    1/1     Running   0          4m25s
frontend-845d5478d-7z9gx                    1/1     Running   0          4m25s
frontend-845d5478d-b7l4j                    1/1     Running   0          4m25s
vpa-alt-recommender-55878867f9-6tp5v        1/1     Running   0          9s</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Configure a VPA CR that includes the name of the alternative recommender <code class="literal">Deployment</code> object.
							</p><div class="formalpara"><p class="title"><strong>Example VPA CR to include the alternative recommender</strong></p><p>
									
<pre class="programlisting language-yml">apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: vpa-recommender
  namespace: &lt;namespace_name&gt;
spec:
  recommenders:
    - name: alt-vpa-recommender <span id="CO20-1"><!--Empty--></span><span class="callout">1</span>
  targetRef:
    apiVersion: "apps/v1"
    kind:       Deployment <span id="CO20-2"><!--Empty--></span><span class="callout">2</span>
    name:       frontend</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO20-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specifies the name of the alternative recommender deployment.
									</div></dd><dt><a href="#CO20-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specifies the name of an existing workload object you want this VPA to manage.
									</div></dd></dl></div></li></ol></div></section></section><section class="section" id="nodes-pods-vertical-autoscaler-configuring_nodes-pods-vertical-autoscaler"><div class="titlepage"><div><div><h3 class="title">2.5.4. Using the Vertical Pod Autoscaler Operator</h3></div></div></div><p>
					You can use the Vertical Pod Autoscaler Operator (VPA) by creating a VPA custom resource (CR). The CR indicates which pods it should analyze and determines the actions the VPA should take with those pods.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The workload object that you want to autoscale must exist.
						</li><li class="listitem">
							If you want to use an alternative recommender, a deployment including that recommender must exist.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To create a VPA CR for a specific workload object:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Change to the project where the workload object you want to scale is located.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a VPA CR YAML file:
								</p><pre class="programlisting language-yaml">apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: vpa-recommender
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind:       Deployment <span id="CO21-1"><!--Empty--></span><span class="callout">1</span>
    name:       frontend <span id="CO21-2"><!--Empty--></span><span class="callout">2</span>
  updatePolicy:
    updateMode: "Auto" <span id="CO21-3"><!--Empty--></span><span class="callout">3</span>
  resourcePolicy: <span id="CO21-4"><!--Empty--></span><span class="callout">4</span>
    containerPolicies:
    - containerName: my-opt-sidecar
      mode: "Off"
  recommenders: <span id="CO21-5"><!--Empty--></span><span class="callout">5</span>
    - name: my-recommender</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO21-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the type of workload object you want this VPA to manage: <code class="literal">Deployment</code>, <code class="literal">StatefulSet</code>, <code class="literal">Job</code>, <code class="literal">DaemonSet</code>, <code class="literal">ReplicaSet</code>, or <code class="literal">ReplicationController</code>.
										</div></dd><dt><a href="#CO21-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Specify the name of an existing workload object you want this VPA to manage.
										</div></dd><dt><a href="#CO21-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Specify the VPA mode:
										</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
													<code class="literal">auto</code> to automatically apply the recommended resources on pods associated with the controller. The VPA terminates existing pods and creates new pods with the recommended resource limits and requests.
												</li><li class="listitem">
													<code class="literal">recreate</code> to automatically apply the recommended resources on pods associated with the workload object. The VPA terminates existing pods and creates new pods with the recommended resource limits and requests. The <code class="literal">recreate</code> mode should be used rarely, only if you need to ensure that the pods are restarted whenever the resource request changes.
												</li><li class="listitem">
													<code class="literal">initial</code> to automatically apply the recommended resources when pods associated with the workload object are created. The VPA does not update the pods as it learns new resource recommendations.
												</li><li class="listitem">
													<code class="literal">off</code> to only generate resource recommendations for the pods associated with the workload object. The VPA does not update the pods as it learns new resource recommendations and does not apply the recommendations to new pods.
												</li></ul></div></dd><dt><a href="#CO21-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Optional. Specify the containers you want to opt-out and set the mode to <code class="literal">Off</code>.
										</div></dd><dt><a href="#CO21-5"><span class="callout">5</span></a> </dt><dd><div class="para">
											Optional. Specify an alternative recommender.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Create the VPA CR:
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre><p class="simpara">
									After a few moments, the VPA learns the resource usage of the containers in the pods associated with the workload object.
								</p><p class="simpara">
									You can view the VPA recommendations using the following command:
								</p><pre class="programlisting language-terminal">$ oc get vpa &lt;vpa-name&gt; --output yaml</pre><p class="simpara">
									The output shows the recommendations for CPU and memory requests, similar to the following:
								</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">...
status:

...

  recommendation:
    containerRecommendations:
    - containerName: frontend
      lowerBound: <span id="CO22-1"><!--Empty--></span><span class="callout">1</span>
        cpu: 25m
        memory: 262144k
      target: <span id="CO22-2"><!--Empty--></span><span class="callout">2</span>
        cpu: 25m
        memory: 262144k
      uncappedTarget: <span id="CO22-3"><!--Empty--></span><span class="callout">3</span>
        cpu: 25m
        memory: 262144k
      upperBound: <span id="CO22-4"><!--Empty--></span><span class="callout">4</span>
        cpu: 262m
        memory: "274357142"
    - containerName: backend
      lowerBound:
        cpu: 12m
        memory: 131072k
      target:
        cpu: 12m
        memory: 131072k
      uncappedTarget:
        cpu: 12m
        memory: 131072k
      upperBound:
        cpu: 476m
        memory: "498558823"

...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO22-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											<code class="literal">lowerBound</code> is the minimum recommended resource levels.
										</div></dd><dt><a href="#CO22-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											<code class="literal">target</code> is the recommended resource levels.
										</div></dd><dt><a href="#CO22-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											<code class="literal">upperBound</code> is the highest recommended resource levels.
										</div></dd><dt><a href="#CO22-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											<code class="literal">uncappedTarget</code> is the most recent resource recommendations.
										</div></dd></dl></div></li></ol></div></li></ol></div></section><section class="section" id="nodes-pods-vertical-autoscaler-uninstall_nodes-pods-vertical-autoscaler"><div class="titlepage"><div><div><h3 class="title">2.5.5. Uninstalling the Vertical Pod Autoscaler Operator</h3></div></div></div><p>
					You can remove the Vertical Pod Autoscaler Operator (VPA) from your OpenShift Container Platform cluster. After uninstalling, the resource requests for the pods already modified by an existing VPA CR do not change. Any new pods get the resources defined in the workload object, not the previous recommendations made by the Vertical Pod Autoscaler Operator.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can remove a specific VPA CR by using the <code class="literal">oc delete vpa &lt;vpa-name&gt;</code> command. The same actions apply for resource requests as uninstalling the vertical pod autoscaler.
					</p></div></div><p>
					After removing the VPA Operator, it is recommended that you remove the other components associated with the Operator to avoid potential issues.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The Vertical Pod Autoscaler Operator must be installed.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the OpenShift Container Platform web console, click <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
						</li><li class="listitem">
							Switch to the <span class="strong strong"><strong>openshift-vertical-pod-autoscaler</strong></span> project.
						</li><li class="listitem">
							For the <span class="strong strong"><strong>VerticalPodAutoscaler</strong></span> Operator, click the Options menu 
							<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Nodes-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
							 and select <span class="strong strong"><strong>Uninstall Operator</strong></span>.
						</li><li class="listitem">
							Optional: To remove all operands associated with the Operator, in the dialog box, select <span class="strong strong"><strong>Delete all operand instances for this operator</strong></span> checkbox.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Uninstall</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Optional: Use the OpenShift CLI to remove the VPA components:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Delete the VPA namespace:
								</p><pre class="programlisting language-terminal">$ oc delete namespace openshift-vertical-pod-autoscaler</pre></li><li class="listitem"><p class="simpara">
									Delete the VPA custom resource definition (CRD) objects:
								</p><pre class="programlisting language-terminal">$ oc delete crd verticalpodautoscalercheckpoints.autoscaling.k8s.io</pre><pre class="programlisting language-terminal">$ oc delete crd verticalpodautoscalercontrollers.autoscaling.openshift.io</pre><pre class="programlisting language-terminal">$ oc delete crd verticalpodautoscalers.autoscaling.k8s.io</pre><p class="simpara">
									Deleting the CRDs removes the associated roles, cluster roles, and role bindings.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										This action removes from the cluster all user-created VPA CRs. If you re-install the VPA, you must create these objects again.
									</p></div></div></li><li class="listitem"><p class="simpara">
									Delete the VPA Operator:
								</p><pre class="programlisting language-terminal">$ oc delete operator/vertical-pod-autoscaler.openshift-vertical-pod-autoscaler</pre></li></ol></div></li></ol></div></section></section><section class="section" id="nodes-pods-secrets"><div class="titlepage"><div><div><h2 class="title">2.6. Providing sensitive data to pods</h2></div></div></div><p>
				Some applications need sensitive information, such as passwords and user names, that you do not want developers to have.
			</p><p>
				As an administrator, you can use <code class="literal">Secret</code> objects to provide this information without exposing that information in clear text.
			</p><section class="section" id="nodes-pods-secrets-about_nodes-pods-secrets"><div class="titlepage"><div><div><h3 class="title">2.6.1. Understanding secrets</h3></div></div></div><p>
					The <code class="literal">Secret</code> object type provides a mechanism to hold sensitive information such as passwords, OpenShift Container Platform client configuration files, private source repository credentials, and so on. Secrets decouple sensitive content from the pods. You can mount secrets into containers using a volume plugin or the system can use secrets to perform actions on behalf of a pod.
				</p><p>
					Key properties include:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Secret data can be referenced independently from its definition.
						</li><li class="listitem">
							Secret data volumes are backed by temporary file-storage facilities (tmpfs) and never come to rest on a node.
						</li><li class="listitem">
							Secret data can be shared within a namespace.
						</li></ul></div><div class="formalpara"><p class="title"><strong>YAML <code class="literal">Secret</code> object definition</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: test-secret
  namespace: my-namespace
type: Opaque <span id="CO23-1"><!--Empty--></span><span class="callout">1</span>
data: <span id="CO23-2"><!--Empty--></span><span class="callout">2</span>
  username: &lt;username&gt; <span id="CO23-3"><!--Empty--></span><span class="callout">3</span>
  password: &lt;password&gt;
stringData: <span id="CO23-4"><!--Empty--></span><span class="callout">4</span>
  hostname: myapp.mydomain.com <span id="CO23-5"><!--Empty--></span><span class="callout">5</span></pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO23-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Indicates the structure of the secret’s key names and values.
						</div></dd><dt><a href="#CO23-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							The allowable format for the keys in the <code class="literal">data</code> field must meet the guidelines in the <span class="strong strong"><strong>DNS_SUBDOMAIN</strong></span> value in <a class="link" href="https://github.com/kubernetes/kubernetes/blob/v1.0.0/docs/design/identifiers.md">the Kubernetes identifiers glossary</a>.
						</div></dd><dt><a href="#CO23-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							The value associated with keys in the <code class="literal">data</code> map must be base64 encoded.
						</div></dd><dt><a href="#CO23-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Entries in the <code class="literal">stringData</code> map are converted to base64 and the entry will then be moved to the <code class="literal">data</code> map automatically. This field is write-only; the value will only be returned via the <code class="literal">data</code> field.
						</div></dd><dt><a href="#CO23-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							The value associated with keys in the <code class="literal">stringData</code> map is made up of plain text strings.
						</div></dd></dl></div><p>
					You must create a secret before creating the pods that depend on that secret.
				</p><p>
					When creating secrets:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Create a secret object with secret data.
						</li><li class="listitem">
							Update the pod’s service account to allow the reference to the secret.
						</li><li class="listitem">
							Create a pod, which consumes the secret as an environment variable or as a file (using a <code class="literal">secret</code> volume).
						</li></ul></div><section class="section" id="nodes-pods-secrets-about-types_nodes-pods-secrets"><div class="titlepage"><div><div><h4 class="title">2.6.1.1. Types of secrets</h4></div></div></div><p>
						The value in the <code class="literal">type</code> field indicates the structure of the secret’s key names and values. The type can be used to enforce the presence of user names and keys in the secret object. If you do not want validation, use the <code class="literal">opaque</code> type, which is the default.
					</p><p>
						Specify one of the following types to trigger minimal server-side validation to ensure the presence of specific key names in the secret data:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<code class="literal">kubernetes.io/service-account-token</code>. Uses a service account token.
							</li><li class="listitem">
								<code class="literal">kubernetes.io/basic-auth</code>. Use with Basic Authentication.
							</li><li class="listitem">
								<code class="literal">kubernetes.io/ssh-auth</code>. Use with SSH Key Authentication.
							</li><li class="listitem">
								<code class="literal">kubernetes.io/tls</code>. Use with TLS certificate authorities.
							</li></ul></div><p>
						Specify <code class="literal">type: Opaque</code> if you do not want validation, which means the secret does not claim to conform to any convention for key names or values. An <span class="emphasis"><em>opaque</em></span> secret, allows for unstructured <code class="literal">key:value</code> pairs that can contain arbitrary values.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You can specify other arbitrary types, such as <code class="literal">example.com/my-secret-type</code>. These types are not enforced server-side, but indicate that the creator of the secret intended to conform to the key/value requirements of that type.
						</p></div></div><p>
						For examples of different secret types, see the code samples in <span class="emphasis"><em>Using Secrets</em></span>.
					</p></section><section class="section" id="nodes-pods-secrets-about-keys_nodes-pods-secrets"><div class="titlepage"><div><div><h4 class="title">2.6.1.2. Secret data keys</h4></div></div></div><p>
						Secret keys must be in a DNS subdomain.
					</p></section><section class="section" id="auto-generated-sa-token-secrets_nodes-pods-secrets"><div class="titlepage"><div><div><h4 class="title">2.6.1.3. About automatically generated service account token secrets</h4></div></div></div><p>
						When a service account is created, a service account token secret is automatically generated for it. This service account token secret, along with an automatically generated docker configuration secret, is used to authenticate to the internal OpenShift Container Platform registry. Do not rely on these automatically generated secrets for your own use; they might be removed in a future OpenShift Container Platform release.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Prior to OpenShift Container Platform 4.11, a second service account token secret was generated when a service account was created. This service account token secret was used to access the Kubernetes API.
						</p><p>
							Starting with OpenShift Container Platform 4.11, this second service account token secret is no longer created. This is because the <code class="literal">LegacyServiceAccountTokenNoAutoGeneration</code> upstream Kubernetes feature gate was enabled, which stops the automatic generation of secret-based service account tokens to access the Kubernetes API.
						</p><p>
							After upgrading to 4.13, any existing service account token secrets are not deleted and continue to function.
						</p></div></div><p>
						Workloads are automatically injected with a projected volume to obtain a bound service account token. If your workload needs an additional service account token, add an additional projected volume in your workload manifest. Bound service account tokens are more secure than service account token secrets for the following reasons:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Bound service account tokens have a bounded lifetime.
							</li><li class="listitem">
								Bound service account tokens contain audiences.
							</li><li class="listitem">
								Bound service account tokens can be bound to pods or secrets and the bound tokens are invalidated when the bound object is removed.
							</li></ul></div><p>
						For more information, see <span class="emphasis"><em>Configuring bound service account tokens using volume projection</em></span>.
					</p><p>
						You can also manually create a service account token secret to obtain a token, if the security exposure of a non-expiring token in a readable API object is acceptable to you. For more information, see <span class="emphasis"><em>Creating a service account token secret</em></span>.
					</p><div class="itemizedlist"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								For information about requesting bound service account tokens, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#bound-sa-tokens-configuring_bound-service-account-tokens">Using bound service account tokens</a>
							</li><li class="listitem">
								For information about creating a service account token secret, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-secrets-creating-sa_nodes-pods-secrets">Creating a service account token secret</a>.
							</li></ul></div></section></section><section class="section" id="nodes-pods-secrets-creating_nodes-pods-secrets"><div class="titlepage"><div><div><h3 class="title">2.6.2. Understanding how to create secrets</h3></div></div></div><p>
					As an administrator you must create a secret before developers can create the pods that depend on that secret.
				</p><p>
					When creating secrets:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a secret object that contains the data you want to keep secret. The specific data required for each secret type is descibed in the following sections.
						</p><div class="formalpara"><p class="title"><strong>Example YAML object that creates an opaque secret</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: test-secret
type: Opaque <span id="CO24-1"><!--Empty--></span><span class="callout">1</span>
data: <span id="CO24-2"><!--Empty--></span><span class="callout">2</span>
  username: &lt;username&gt;
  password: &lt;password&gt;
stringData: <span id="CO24-3"><!--Empty--></span><span class="callout">3</span>
  hostname: myapp.mydomain.com
  secret.properties: |
    property1=valueA
    property2=valueB</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO24-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specifies the type of secret.
								</div></dd><dt><a href="#CO24-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specifies encoded string and data.
								</div></dd><dt><a href="#CO24-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specifies decoded string and data.
								</div></dd></dl></div><p class="simpara">
							Use either the <code class="literal">data</code> or <code class="literal">stringdata</code> fields, not both.
						</p></li><li class="listitem"><p class="simpara">
							Update the pod’s service account to reference the secret:
						</p><div class="formalpara"><p class="title"><strong>YAML of a service account that uses a secret</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: ServiceAccount
 ...
secrets:
- name: test-secret</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create a pod, which consumes the secret as an environment variable or as a file (using a <code class="literal">secret</code> volume):
						</p><div class="formalpara"><p class="title"><strong>YAML of a pod populating files in a volume with secret data</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: secret-example-pod
spec:
  containers:
    - name: secret-test-container
      image: busybox
      command: [ "/bin/sh", "-c", "cat /etc/secret-volume/*" ]
      volumeMounts: <span id="CO25-1"><!--Empty--></span><span class="callout">1</span>
          - name: secret-volume
            mountPath: /etc/secret-volume <span id="CO25-2"><!--Empty--></span><span class="callout">2</span>
            readOnly: true <span id="CO25-3"><!--Empty--></span><span class="callout">3</span>
  volumes:
    - name: secret-volume
      secret:
        secretName: test-secret <span id="CO25-4"><!--Empty--></span><span class="callout">4</span>
  restartPolicy: Never</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO25-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add a <code class="literal">volumeMounts</code> field to each container that needs the secret.
								</div></dd><dt><a href="#CO25-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specifies an unused directory name where you would like the secret to appear. Each key in the secret data map becomes the filename under <code class="literal">mountPath</code>.
								</div></dd><dt><a href="#CO25-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Set to <code class="literal">true</code>. If true, this instructs the driver to provide a read-only volume.
								</div></dd><dt><a href="#CO25-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specifies the name of the secret.
								</div></dd></dl></div><div class="formalpara"><p class="title"><strong>YAML of a pod populating environment variables with secret data</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: secret-example-pod
spec:
  containers:
    - name: secret-test-container
      image: busybox
      command: [ "/bin/sh", "-c", "export" ]
      env:
        - name: TEST_SECRET_USERNAME_ENV_VAR
          valueFrom:
            secretKeyRef: <span id="CO26-1"><!--Empty--></span><span class="callout">1</span>
              name: test-secret
              key: username
  restartPolicy: Never</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO26-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specifies the environment variable that consumes the secret key.
								</div></dd></dl></div><div class="formalpara"><p class="title"><strong>YAML of a build config populating environment variables with secret data</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: build.openshift.io/v1
kind: BuildConfig
metadata:
  name: secret-example-bc
spec:
  strategy:
    sourceStrategy:
      env:
      - name: TEST_SECRET_USERNAME_ENV_VAR
        valueFrom:
          secretKeyRef: <span id="CO27-1"><!--Empty--></span><span class="callout">1</span>
            name: test-secret
            key: username
      from:
        kind: ImageStreamTag
        namespace: openshift
        name: 'cli:latest'</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO27-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specifies the environment variable that consumes the secret key.
								</div></dd></dl></div></li></ol></div><section class="section" id="secret-creation-restrictions"><div class="titlepage"><div><div><h4 class="title">2.6.2.1. Secret creation restrictions</h4></div></div></div><p>
						To use a secret, a pod needs to reference the secret. A secret can be used with a pod in three ways:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								To populate environment variables for containers.
							</li><li class="listitem">
								As files in a volume mounted on one or more of its containers.
							</li><li class="listitem">
								By kubelet when pulling images for the pod.
							</li></ul></div><p>
						Volume type secrets write data into the container as a file using the volume mechanism. Image pull secrets use service accounts for the automatic injection of the secret into all pods in a namespace.
					</p><p>
						When a template contains a secret definition, the only way for the template to use the provided secret is to ensure that the secret volume sources are validated and that the specified object reference actually points to a <code class="literal">Secret</code> object. Therefore, a secret needs to be created before any pods that depend on it. The most effective way to ensure this is to have it get injected automatically through the use of a service account.
					</p><p>
						Secret API objects reside in a namespace. They can only be referenced by pods in that same namespace.
					</p><p>
						Individual secrets are limited to 1MB in size. This is to discourage the creation of large secrets that could exhaust apiserver and kubelet memory. However, creation of a number of smaller secrets could also exhaust memory.
					</p></section><section class="section" id="nodes-pods-secrets-creating-opaque_nodes-pods-secrets"><div class="titlepage"><div><div><h4 class="title">2.6.2.2. Creating an opaque secret</h4></div></div></div><p>
						As an administrator, you can create an opaque secret, which allows you to store unstructured <code class="literal">key:value</code> pairs that can contain arbitrary values.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a <code class="literal">Secret</code> object in a YAML file on a control plane node.
							</p><p class="simpara">
								For example:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque <span id="CO28-1"><!--Empty--></span><span class="callout">1</span>
data:
  username: &lt;username&gt;
  password: &lt;password&gt;</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO28-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specifies an opaque secret.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Use the following command to create a <code class="literal">Secret</code> object:
							</p><pre class="programlisting language-terminal">$ oc create -f &lt;filename&gt;.yaml</pre></li><li class="listitem"><p class="simpara">
								To use the secret in a pod:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Update the pod’s service account to reference the secret, as shown in the "Understanding how to create secrets" section.
									</li><li class="listitem">
										Create the pod, which consumes the secret as an environment variable or as a file (using a <code class="literal">secret</code> volume), as shown in the "Understanding how to create secrets" section.
									</li></ol></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								For more information on using secrets in pods, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-secrets-creating_nodes-pods-secrets">Understanding how to create secrets</a>.
							</li></ul></div></section><section class="section" id="nodes-pods-secrets-creating-sa_nodes-pods-secrets"><div class="titlepage"><div><div><h4 class="title">2.6.2.3. Creating a service account token secret</h4></div></div></div><p>
						As an administrator, you can create a service account token secret, which allows you to distribute a service account token to applications that must authenticate to the API.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							It is recommended to obtain bound service account tokens using the TokenRequest API instead of using service account token secrets. The tokens obtained from the TokenRequest API are more secure than the tokens stored in secrets, because they have a bounded lifetime and are not readable by other API clients.
						</p><p>
							You should create a service account token secret only if you cannot use the TokenRequest API and if the security exposure of a non-expiring token in a readable API object is acceptable to you.
						</p><p>
							See the Additional resources section that follows for information on creating bound service account tokens.
						</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a <code class="literal">Secret</code> object in a YAML file on a control plane node:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">secret</code> object:</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: secret-sa-sample
  annotations:
    kubernetes.io/service-account.name: "sa-name" <span id="CO29-1"><!--Empty--></span><span class="callout">1</span>
type: kubernetes.io/service-account-token <span id="CO29-2"><!--Empty--></span><span class="callout">2</span></pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO29-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specifies an existing service account name. If you are creating both the <code class="literal">ServiceAccount</code> and the <code class="literal">Secret</code> objects, create the <code class="literal">ServiceAccount</code> object first.
									</div></dd><dt><a href="#CO29-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specifies a service account token secret.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Use the following command to create the <code class="literal">Secret</code> object:
							</p><pre class="programlisting language-terminal">$ oc create -f &lt;filename&gt;.yaml</pre></li><li class="listitem"><p class="simpara">
								To use the secret in a pod:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Update the pod’s service account to reference the secret, as shown in the "Understanding how to create secrets" section.
									</li><li class="listitem">
										Create the pod, which consumes the secret as an environment variable or as a file (using a <code class="literal">secret</code> volume), as shown in the "Understanding how to create secrets" section.
									</li></ol></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								For more information on using secrets in pods, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-secrets-creating_nodes-pods-secrets">Understanding how to create secrets</a>.
							</li><li class="listitem">
								For information on requesting bound service account tokens, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#bound-sa-tokens-configuring_bound-service-account-tokens">Using bound service account tokens</a>
							</li><li class="listitem">
								For information on creating service accounts, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#understanding-and-creating-service-accounts">Understanding and creating service accounts</a>.
							</li></ul></div></section><section class="section" id="nodes-pods-secrets-creating-basic_nodes-pods-secrets"><div class="titlepage"><div><div><h4 class="title">2.6.2.4. Creating a basic authentication secret</h4></div></div></div><p>
						As an administrator, you can create a basic authentication secret, which allows you to store the credentials needed for basic authentication. When using this secret type, the <code class="literal">data</code> parameter of the <code class="literal">Secret</code> object must contain the following keys encoded in the base64 format:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<code class="literal">username</code>: the user name for authentication
							</li><li class="listitem">
								<code class="literal">password</code>: the password or token for authentication
							</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You can use the <code class="literal">stringData</code> parameter to use clear text content.
						</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a <code class="literal">Secret</code> object in a YAML file on a control plane node:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">secret</code> object</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: secret-basic-auth
type: kubernetes.io/basic-auth <span id="CO30-1"><!--Empty--></span><span class="callout">1</span>
data:
stringData: <span id="CO30-2"><!--Empty--></span><span class="callout">2</span>
  username: admin
  password: &lt;password&gt;</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO30-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specifies a basic authentication secret.
									</div></dd><dt><a href="#CO30-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specifies the basic authentication values to use.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Use the following command to create the <code class="literal">Secret</code> object:
							</p><pre class="programlisting language-terminal">$ oc create -f &lt;filename&gt;.yaml</pre></li><li class="listitem"><p class="simpara">
								To use the secret in a pod:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Update the pod’s service account to reference the secret, as shown in the "Understanding how to create secrets" section.
									</li><li class="listitem">
										Create the pod, which consumes the secret as an environment variable or as a file (using a <code class="literal">secret</code> volume), as shown in the "Understanding how to create secrets" section.
									</li></ol></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								For more information on using secrets in pods, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-secrets-creating_nodes-pods-secrets">Understanding how to create secrets</a>.
							</li></ul></div></section><section class="section" id="nodes-pods-secrets-creating-ssh_nodes-pods-secrets"><div class="titlepage"><div><div><h4 class="title">2.6.2.5. Creating an SSH authentication secret</h4></div></div></div><p>
						As an administrator, you can create an SSH authentication secret, which allows you to store data used for SSH authentication. When using this secret type, the <code class="literal">data</code> parameter of the <code class="literal">Secret</code> object must contain the SSH credential to use.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a <code class="literal">Secret</code> object in a YAML file on a control plane node:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">secret</code> object:</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: secret-ssh-auth
type: kubernetes.io/ssh-auth <span id="CO31-1"><!--Empty--></span><span class="callout">1</span>
data:
  ssh-privatekey: | <span id="CO31-2"><!--Empty--></span><span class="callout">2</span>
          MIIEpQIBAAKCAQEAulqb/Y ...</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO31-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specifies an SSH authentication secret.
									</div></dd><dt><a href="#CO31-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specifies the SSH key/value pair as the SSH credentials to use.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Use the following command to create the <code class="literal">Secret</code> object:
							</p><pre class="programlisting language-terminal">$ oc create -f &lt;filename&gt;.yaml</pre></li><li class="listitem"><p class="simpara">
								To use the secret in a pod:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Update the pod’s service account to reference the secret, as shown in the "Understanding how to create secrets" section.
									</li><li class="listitem">
										Create the pod, which consumes the secret as an environment variable or as a file (using a <code class="literal">secret</code> volume), as shown in the "Understanding how to create secrets" section.
									</li></ol></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-secrets-creating_nodes-pods-secrets">Understanding how to create secrets</a>.
							</li></ul></div></section><section class="section" id="nodes-pods-secrets-creating-docker_nodes-pods-secrets"><div class="titlepage"><div><div><h4 class="title">2.6.2.6. Creating a Docker configuration secret</h4></div></div></div><p>
						As an administrator, you can create a Docker configuration secret, which allows you to store the credentials for accessing a container image registry.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<code class="literal">kubernetes.io/dockercfg</code>. Use this secret type to store your local Docker configuration file. The <code class="literal">data</code> parameter of the <code class="literal">secret</code> object must contain the contents of a <code class="literal">.dockercfg</code> file encoded in the base64 format.
							</li><li class="listitem">
								<code class="literal">kubernetes.io/dockerconfigjson</code>. Use this secret type to store your local Docker configuration JSON file. The <code class="literal">data</code> parameter of the <code class="literal">secret</code> object must contain the contents of a <code class="literal">.docker/config.json</code> file encoded in the base64 format.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a <code class="literal">Secret</code> object in a YAML file on a control plane node.
							</p><div class="formalpara"><p class="title"><strong>Example Docker configuration <code class="literal">secret</code> object</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: secret-docker-cfg
  namespace: my-project
type: kubernetes.io/dockerconfig <span id="CO32-1"><!--Empty--></span><span class="callout">1</span>
data:
  .dockerconfig:bm5ubm5ubm5ubm5ubm5ubm5ubm5ubmdnZ2dnZ2dnZ2dnZ2dnZ2dnZ2cgYXV0aCBrZXlzCg== <span id="CO32-2"><!--Empty--></span><span class="callout">2</span></pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO32-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specifies that the secret is using a Docker configuration file.
									</div></dd><dt><a href="#CO32-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The output of a base64-encoded Docker configuration file
									</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example Docker configuration JSON <code class="literal">secret</code> object</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: secret-docker-json
  namespace: my-project
type: kubernetes.io/dockerconfig <span id="CO33-1"><!--Empty--></span><span class="callout">1</span>
data:
  .dockerconfigjson:bm5ubm5ubm5ubm5ubm5ubm5ubm5ubmdnZ2dnZ2dnZ2dnZ2dnZ2dnZ2cgYXV0aCBrZXlzCg== <span id="CO33-2"><!--Empty--></span><span class="callout">2</span></pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO33-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specifies that the secret is using a Docker configuration JSONfile.
									</div></dd><dt><a href="#CO33-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The output of a base64-encoded Docker configuration JSON file
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Use the following command to create the <code class="literal">Secret</code> object
							</p><pre class="programlisting language-terminal">$ oc create -f &lt;filename&gt;.yaml</pre></li><li class="listitem"><p class="simpara">
								To use the secret in a pod:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Update the pod’s service account to reference the secret, as shown in the "Understanding how to create secrets" section.
									</li><li class="listitem">
										Create the pod, which consumes the secret as an environment variable or as a file (using a <code class="literal">secret</code> volume), as shown in the "Understanding how to create secrets" section.
									</li></ol></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								For more information on using secrets in pods, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-secrets-creating_nodes-pods-secrets">Understanding how to create secrets</a>.
							</li></ul></div></section></section><section class="section" id="nodes-pods-secrets-updating_nodes-pods-secrets"><div class="titlepage"><div><div><h3 class="title">2.6.3. Understanding how to update secrets</h3></div></div></div><p>
					When you modify the value of a secret, the value (used by an already running pod) will not dynamically change. To change a secret, you must delete the original pod and create a new pod (perhaps with an identical PodSpec).
				</p><p>
					Updating a secret follows the same workflow as deploying a new Container image. You can use the <code class="literal">kubectl rolling-update</code> command.
				</p><p>
					The <code class="literal">resourceVersion</code> value in a secret is not specified when it is referenced. Therefore, if a secret is updated at the same time as pods are starting, the version of the secret that is used for the pod is not defined.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Currently, it is not possible to check the resource version of a secret object that was used when a pod was created. It is planned that pods will report this information, so that a controller could restart ones using an old <code class="literal">resourceVersion</code>. In the interim, do not update the data of existing secrets, but create new ones with distinct names.
					</p></div></div></section><section class="section" id="nodes-application-secrets-creating-using-sa_nodes-pods-secrets"><div class="titlepage"><div><div><h3 class="title">2.6.4. Creating and using secrets</h3></div></div></div><p>
					As an administrator, you can create a service account token secret. This allows you to distribute a service account token to applications that must authenticate to the API.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a service account in your namespace by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create sa &lt;service_account_name&gt; -n &lt;your_namespace&gt;</pre></li><li class="listitem"><p class="simpara">
							Save the following YAML example to a file named <code class="literal">service-account-token-secret.yaml</code>. The example includes a <code class="literal">Secret</code> object configuration that you can use to generate a service account token:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: &lt;secret_name&gt; <span id="CO34-1"><!--Empty--></span><span class="callout">1</span>
  annotations:
    kubernetes.io/service-account.name: "sa-name" <span id="CO34-2"><!--Empty--></span><span class="callout">2</span>
type: kubernetes.io/service-account-token <span id="CO34-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO34-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal">&lt;secret_name&gt;</code> with the name of your service token secret.
								</div></dd><dt><a href="#CO34-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specifies an existing service account name. If you are creating both the <code class="literal">ServiceAccount</code> and the <code class="literal">Secret</code> objects, create the <code class="literal">ServiceAccount</code> object first.
								</div></dd><dt><a href="#CO34-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specifies a service account token secret type.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Generate the service account token by applying the file:
						</p><pre class="programlisting language-terminal">$ oc apply -f service-account-token-secret.yaml</pre></li><li class="listitem"><p class="simpara">
							Get the service account token from the secret by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get secret &lt;sa_token_secret&gt; -o jsonpath='{.data.token}' | base64 --decode <span id="CO35-1"><!--Empty--></span><span class="callout">1</span></pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">ayJhbGciOiJSUzI1NiIsImtpZCI6IklOb2dtck1qZ3hCSWpoNnh5YnZhSE9QMkk3YnRZMVZoclFfQTZfRFp1YlUifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImJ1aWxkZXItdG9rZW4tdHZrbnIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiYnVpbGRlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjNmZGU2MGZmLTA1NGYtNDkyZi04YzhjLTNlZjE0NDk3MmFmNyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmJ1aWxkZXIifQ.OmqFTDuMHC_lYvvEUrjr1x453hlEEHYcxS9VKSzmRkP1SiVZWPNPkTWlfNRp6bIUZD3U6aN3N7dMSN0eI5hu36xPgpKTdvuckKLTCnelMx6cxOdAbrcw1mCmOClNscwjS1KO1kzMtYnnq8rXHiMJELsNlhnRyyIXRTtNBsy4t64T3283s3SLsancyx0gy0ujx-Ch3uKAKdZi5iT-I8jnnQ-ds5THDs2h65RJhgglQEmSxpHrLGZFmyHAQI-_SjvmHZPXEc482x3SkaQHNLqpmrpJorNqh1M8ZHKzlujhZgVooMvJmWPXTb2vnvi3DGn2XI-hZxl1yD2yGH1RBpYUHA</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO35-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace &lt;sa_token_secret&gt; with the name of your service token secret.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Use your service account token to authenticate with the API of your cluster:
						</p><pre class="programlisting language-terminal">$ curl -X GET &lt;openshift_cluster_api&gt; --header "Authorization: Bearer &lt;token&gt;" <span id="CO36-1"><!--Empty--></span><span class="callout">1</span> <span id="CO36-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO36-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal">&lt;openshift_cluster_api&gt;</code> with the OpenShift cluster API.
								</div></dd><dt><a href="#CO36-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Replace <code class="literal">&lt;token&gt;</code> with the service account token that is output in the preceding command.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="nodes-pods-secrets-certificates-about_nodes-pods-secrets"><div class="titlepage"><div><div><h3 class="title">2.6.5. About using signed certificates with secrets</h3></div></div></div><p>
					To secure communication to your service, you can configure OpenShift Container Platform to generate a signed serving certificate/key pair that you can add into a secret in a project.
				</p><p>
					A <span class="emphasis"><em>service serving certificate secret</em></span> is intended to support complex middleware applications that need out-of-the-box certificates. It has the same settings as the server certificates generated by the administrator tooling for nodes and masters.
				</p><div class="formalpara"><p class="title"><strong>Service <code class="literal">Pod</code> spec configured for a service serving certificates secret.</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: Service
metadata:
  name: registry
  annotations:
    service.beta.openshift.io/serving-cert-secret-name: registry-cert<span id="CO37-1"><!--Empty--></span><span class="callout">1</span>
# ...</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO37-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Specify the name for the certificate
						</div></dd></dl></div><p>
					Other pods can trust cluster-created certificates (which are only signed for internal DNS names), by using the CA bundle in the <span class="strong strong"><strong><span class="emphasis"><em>/var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt</em></span></strong></span> file that is automatically mounted in their pod.
				</p><p>
					The signature algorithm for this feature is <code class="literal">x509.SHA256WithRSA</code>. To manually rotate, delete the generated secret. A new certificate is created.
				</p><section class="section" id="nodes-pods-secrets-certificates-creating_nodes-pods-secrets"><div class="titlepage"><div><div><h4 class="title">2.6.5.1. Generating signed certificates for use with secrets</h4></div></div></div><p>
						To use a signed serving certificate/key pair with a pod, create or edit the service to add the <code class="literal">service.beta.openshift.io/serving-cert-secret-name</code> annotation, then add the secret to the pod.
					</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To create a <span class="emphasis"><em>service serving certificate secret</em></span>:
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								Edit the <code class="literal">Pod</code> spec for your service.
							</li><li class="listitem"><p class="simpara">
								Add the <code class="literal">service.beta.openshift.io/serving-cert-secret-name</code> annotation with the name you want to use for your secret.
							</p><pre class="programlisting language-yaml">kind: Service
apiVersion: v1
metadata:
  name: my-service
  annotations:
      service.beta.openshift.io/serving-cert-secret-name: my-cert <span id="CO38-1"><!--Empty--></span><span class="callout">1</span>
spec:
  selector:
    app: MyApp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376</pre><p class="simpara">
								The certificate and key are in PEM format, stored in <code class="literal">tls.crt</code> and <code class="literal">tls.key</code> respectively.
							</p></li><li class="listitem"><p class="simpara">
								Create the service:
							</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre></li><li class="listitem"><p class="simpara">
								View the secret to make sure it was created:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										View a list of all secrets:
									</p><pre class="programlisting language-terminal">$ oc get secrets</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">NAME                     TYPE                                  DATA      AGE
my-cert                  kubernetes.io/tls                     2         9m</pre>

										</p></div></li><li class="listitem"><p class="simpara">
										View details on your secret:
									</p><pre class="programlisting language-terminal">$ oc describe secret my-cert</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">Name:         my-cert
Namespace:    openshift-console
Labels:       &lt;none&gt;
Annotations:  service.beta.openshift.io/expiry: 2023-03-08T23:22:40Z
              service.beta.openshift.io/originating-service-name: my-service
              service.beta.openshift.io/originating-service-uid: 640f0ec3-afc2-4380-bf31-a8c784846a11
              service.beta.openshift.io/expiry: 2023-03-08T23:22:40Z

Type:  kubernetes.io/tls

Data
====
tls.key:  1679 bytes
tls.crt:  2595 bytes</pre>

										</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
								Edit your <code class="literal">Pod</code> spec with that secret.
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-service-pod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: my-container
      mountPath: "/etc/my-path"
  volumes:
  - name: my-volume
    secret:
      secretName: my-cert
      items:
      - key: username
        path: my-group/my-username
        mode: 511</pre><p class="simpara">
								When it is available, your pod will run. The certificate will be good for the internal service DNS name, <code class="literal">&lt;service.name&gt;.&lt;service.namespace&gt;.svc</code>.
							</p><p class="simpara">
								The certificate/key pair is automatically replaced when it gets close to expiration. View the expiration date in the <code class="literal">service.beta.openshift.io/expiry</code> annotation on the secret, which is in RFC3339 format.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									In most cases, the service DNS name <code class="literal">&lt;service.name&gt;.&lt;service.namespace&gt;.svc</code> is not externally routable. The primary use of <code class="literal">&lt;service.name&gt;.&lt;service.namespace&gt;.svc</code> is for intracluster or intraservice communication, and with re-encrypt routes.
								</p></div></div></li></ol></div></section></section><section class="section" id="nodes-pods-secrets-troubleshooting_nodes-pods-secrets"><div class="titlepage"><div><div><h3 class="title">2.6.6. Troubleshooting secrets</h3></div></div></div><p>
					If a service certificate generation fails with (service’s <code class="literal">service.beta.openshift.io/serving-cert-generation-error</code> annotation contains):
				</p><pre class="programlisting language-terminal">secret/ssl-key references serviceUID 62ad25ca-d703-11e6-9d6f-0e9c0057b608, which does not match 77b6dd80-d716-11e6-9d6f-0e9c0057b60</pre><p>
					The service that generated the certificate no longer exists, or has a different <code class="literal">serviceUID</code>. You must force certificates regeneration by removing the old secret, and clearing the following annotations on the service <code class="literal">service.beta.openshift.io/serving-cert-generation-error</code>, <code class="literal">service.beta.openshift.io/serving-cert-generation-error-num</code>:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Delete the secret:
						</p><pre class="programlisting language-terminal">$ oc delete secret &lt;secret_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Clear the annotations:
						</p><pre class="programlisting language-terminal">$ oc annotate service &lt;service_name&gt; service.beta.openshift.io/serving-cert-generation-error-</pre><pre class="programlisting language-terminal">$ oc annotate service &lt;service_name&gt; service.beta.openshift.io/serving-cert-generation-error-num-</pre></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The command removing annotation has a <code class="literal">-</code> after the annotation name to be removed.
					</p></div></div></section></section><section class="section" id="configmaps"><div class="titlepage"><div><div><h2 class="title">2.7. Creating and using config maps</h2></div></div></div><p>
				The following sections define config maps and how to create and use them.
			</p><section class="section" id="nodes-pods-configmap-overview_configmaps"><div class="titlepage"><div><div><h3 class="title">2.7.1. Understanding config maps</h3></div></div></div><p>
					Many applications require configuration by using some combination of configuration files, command line arguments, and environment variables. In OpenShift Container Platform, these configuration artifacts are decoupled from image content to keep containerized applications portable.
				</p><p>
					The <code class="literal">ConfigMap</code> object provides mechanisms to inject containers with configuration data while keeping containers agnostic of OpenShift Container Platform. A config map can be used to store fine-grained information like individual properties or coarse-grained information like entire configuration files or JSON blobs.
				</p><p>
					The <code class="literal">ConfigMap</code> object holds key-value pairs of configuration data that can be consumed in pods or used to store configuration data for system components such as controllers. For example:
				</p><div class="formalpara"><p class="title"><strong><code class="literal">ConfigMap</code> Object Definition</strong></p><p>
						
<pre class="programlisting language-yaml">kind: ConfigMap
apiVersion: v1
metadata:
  creationTimestamp: 2016-02-18T19:14:38Z
  name: example-config
  namespace: my-namespace
data: <span id="CO38-2"><!--Empty--></span><span class="callout">1</span>
  example.property.1: hello
  example.property.2: world
  example.property.file: |-
    property.1=value-1
    property.2=value-2
    property.3=value-3
binaryData:
  bar: L3Jvb3QvMTAw <span id="CO38-3"><!--Empty--></span><span class="callout">2</span></pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO38-1"><span class="callout">1</span></a> <a href="#CO38-2"><span class="callout">1</span></a> </dt><dd><div class="para">
							Contains the configuration data.
						</div></dd><dt><a href="#CO38-3"><span class="callout">2</span></a> </dt><dd><div class="para">
							Points to a file that contains non-UTF8 data, for example, a binary Java keystore file. Enter the file data in Base 64.
						</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can use the <code class="literal">binaryData</code> field when you create a config map from a binary file, such as an image.
					</p></div></div><p>
					Configuration data can be consumed in pods in a variety of ways. A config map can be used to:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Populate environment variable values in containers
						</li><li class="listitem">
							Set command-line arguments in a container
						</li><li class="listitem">
							Populate configuration files in a volume
						</li></ul></div><p>
					Users and system components can store configuration data in a config map.
				</p><p>
					A config map is similar to a secret, but designed to more conveniently support working with strings that do not contain sensitive information.
				</p><h5 id="config-map-restrictions">Config map restrictions</h5><p>
					<span class="strong strong"><strong>A config map must be created before its contents can be consumed in pods.</strong></span>
				</p><p>
					Controllers can be written to tolerate missing configuration data. Consult individual components configured by using config maps on a case-by-case basis.
				</p><p>
					<span class="strong strong"><strong><code class="literal">ConfigMap</code> objects reside in a project.</strong></span>
				</p><p>
					They can only be referenced by pods in the same project.
				</p><p>
					<span class="strong strong"><strong>The Kubelet only supports the use of a config map for pods it gets from the API server.</strong></span>
				</p><p>
					This includes any pods created by using the CLI, or indirectly from a replication controller. It does not include pods created by using the OpenShift Container Platform node’s <code class="literal">--manifest-url</code> flag, its <code class="literal">--config</code> flag, or its REST API because these are not common ways to create pods.
				</p></section><section class="section" id="nodes-pods-configmap-create-from-console_configmaps"><div class="titlepage"><div><div><h3 class="title">2.7.2. Creating a config map in the OpenShift Container Platform web console</h3></div></div></div><p>
					You can create a config map in the OpenShift Container Platform web console.
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To create a config map as a cluster administrator:
						</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
									In the Administrator perspective, select <code class="literal">Workloads</code> → <code class="literal">Config Maps</code>.
								</li><li class="listitem">
									At the top right side of the page, select <span class="strong strong"><strong>Create Config Map</strong></span>.
								</li><li class="listitem">
									Enter the contents of your config map.
								</li><li class="listitem">
									Select <span class="strong strong"><strong>Create</strong></span>.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							To create a config map as a developer:
						</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
									In the Developer perspective, select <code class="literal">Config Maps</code>.
								</li><li class="listitem">
									At the top right side of the page, select <span class="strong strong"><strong>Create Config Map</strong></span>.
								</li><li class="listitem">
									Enter the contents of your config map.
								</li><li class="listitem">
									Select <span class="strong strong"><strong>Create</strong></span>.
								</li></ol></div></li></ul></div></section><section class="section" id="nodes-pods-configmap-create_configmaps"><div class="titlepage"><div><div><h3 class="title">2.7.3. Creating a config map by using the CLI</h3></div></div></div><p>
					You can use the following command to create a config map from directories, specific files, or literal values.
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Create a config map:
						</p><pre class="programlisting language-terminal">$ oc create configmap &lt;configmap_name&gt; [options]</pre></li></ul></div><section class="section" id="nodes-pods-configmap-creating-from-directories_configmaps"><div class="titlepage"><div><div><h4 class="title">2.7.3.1. Creating a config map from a directory</h4></div></div></div><p>
						You can create a config map from a directory by using the <code class="literal">--from-file</code> flag. This method allows you to use multiple files within a directory to create a config map.
					</p><p>
						Each file in the directory is used to populate a key in the config map, where the name of the key is the file name, and the value of the key is the content of the file.
					</p><p>
						For example, the following command creates a config map with the contents of the <code class="literal">example-files</code> directory:
					</p><pre class="programlisting language-terminal">$ oc create configmap game-config --from-file=example-files/</pre><p>
						View the keys in the config map:
					</p><pre class="programlisting language-terminal">$ oc describe configmaps game-config</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">Name:           game-config
Namespace:      default
Labels:         &lt;none&gt;
Annotations:    &lt;none&gt;

Data

game.properties:        158 bytes
ui.properties:          83 bytes</pre>

						</p></div><p>
						You can see that the two keys in the map are created from the file names in the directory specified in the command. The content of those keys might be large, so the output of <code class="literal">oc describe</code> only shows the names of the keys and their sizes.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisite</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								You must have a directory with files that contain the data you want to populate a config map with.
							</p><p class="simpara">
								The following procedure uses these example files: <code class="literal">game.properties</code> and <code class="literal">ui.properties</code>:
							</p><pre class="programlisting language-terminal">$ cat example-files/game.properties</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">enemies=aliens
lives=3
enemies.cheat=true
enemies.cheat.level=noGoodRotten
secret.code.passphrase=UUDDLRLRBABAS
secret.code.allowed=true
secret.code.lives=30</pre>

								</p></div><pre class="programlisting language-terminal">$ cat example-files/ui.properties</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">color.good=purple
color.bad=yellow
allow.textmode=true
how.nice.to.look=fairlyNice</pre>

								</p></div></li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Create a config map holding the content of each file in this directory by entering the following command:
							</p><pre class="programlisting language-terminal">$ oc create configmap game-config \
    --from-file=example-files/</pre></li></ul></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Enter the <code class="literal">oc get</code> command for the object with the <code class="literal">-o</code> option to see the values of the keys:
							</p><pre class="programlisting language-terminal">$ oc get configmaps game-config -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
data:
  game.properties: |-
    enemies=aliens
    lives=3
    enemies.cheat=true
    enemies.cheat.level=noGoodRotten
    secret.code.passphrase=UUDDLRLRBABAS
    secret.code.allowed=true
    secret.code.lives=30
  ui.properties: |
    color.good=purple
    color.bad=yellow
    allow.textmode=true
    how.nice.to.look=fairlyNice
kind: ConfigMap
metadata:
  creationTimestamp: 2016-02-18T18:34:05Z
  name: game-config
  namespace: default
  resourceVersion: "407"
  selflink: /api/v1/namespaces/default/configmaps/game-config
  uid: 30944725-d66e-11e5-8cd0-68f728db1985</pre>

								</p></div></li></ul></div></section><section class="section" id="nodes-pods-configmap-creating-from-files_configmaps"><div class="titlepage"><div><div><h4 class="title">2.7.3.2. Creating a config map from a file</h4></div></div></div><p>
						You can create a config map from a file by using the <code class="literal">--from-file</code> flag. You can pass the <code class="literal">--from-file</code> option multiple times to the CLI.
					</p><p>
						You can also specify the key to set in a config map for content imported from a file by passing a <code class="literal">key=value</code> expression to the <code class="literal">--from-file</code> option. For example:
					</p><pre class="programlisting language-terminal">$ oc create configmap game-config-3 --from-file=game-special-key=example-files/game.properties</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you create a config map from a file, you can include files containing non-UTF8 data that are placed in this field without corrupting the non-UTF8 data. OpenShift Container Platform detects binary files and transparently encodes the file as <code class="literal">MIME</code>. On the server, the <code class="literal">MIME</code> payload is decoded and stored without corrupting the data.
						</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisite</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								You must have a directory with files that contain the data you want to populate a config map with.
							</p><p class="simpara">
								The following procedure uses these example files: <code class="literal">game.properties</code> and <code class="literal">ui.properties</code>:
							</p><pre class="programlisting language-terminal">$ cat example-files/game.properties</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">enemies=aliens
lives=3
enemies.cheat=true
enemies.cheat.level=noGoodRotten
secret.code.passphrase=UUDDLRLRBABAS
secret.code.allowed=true
secret.code.lives=30</pre>

								</p></div><pre class="programlisting language-terminal">$ cat example-files/ui.properties</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">color.good=purple
color.bad=yellow
allow.textmode=true
how.nice.to.look=fairlyNice</pre>

								</p></div></li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Create a config map by specifying a specific file:
							</p><pre class="programlisting language-terminal">$ oc create configmap game-config-2 \
    --from-file=example-files/game.properties \
    --from-file=example-files/ui.properties</pre></li><li class="listitem"><p class="simpara">
								Create a config map by specifying a key-value pair:
							</p><pre class="programlisting language-terminal">$ oc create configmap game-config-3 \
    --from-file=game-special-key=example-files/game.properties</pre></li></ul></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Enter the <code class="literal">oc get</code> command for the object with the <code class="literal">-o</code> option to see the values of the keys from the file:
							</p><pre class="programlisting language-terminal">$ oc get configmaps game-config-2 -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
data:
  game.properties: |-
    enemies=aliens
    lives=3
    enemies.cheat=true
    enemies.cheat.level=noGoodRotten
    secret.code.passphrase=UUDDLRLRBABAS
    secret.code.allowed=true
    secret.code.lives=30
  ui.properties: |
    color.good=purple
    color.bad=yellow
    allow.textmode=true
    how.nice.to.look=fairlyNice
kind: ConfigMap
metadata:
  creationTimestamp: 2016-02-18T18:52:05Z
  name: game-config-2
  namespace: default
  resourceVersion: "516"
  selflink: /api/v1/namespaces/default/configmaps/game-config-2
  uid: b4952dc3-d670-11e5-8cd0-68f728db1985</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Enter the <code class="literal">oc get</code> command for the object with the <code class="literal">-o</code> option to see the values of the keys from the key-value pair:
							</p><pre class="programlisting language-terminal">$ oc get configmaps game-config-3 -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
data:
  game-special-key: |- <span id="CO39-1"><!--Empty--></span><span class="callout">1</span>
    enemies=aliens
    lives=3
    enemies.cheat=true
    enemies.cheat.level=noGoodRotten
    secret.code.passphrase=UUDDLRLRBABAS
    secret.code.allowed=true
    secret.code.lives=30
kind: ConfigMap
metadata:
  creationTimestamp: 2016-02-18T18:54:22Z
  name: game-config-3
  namespace: default
  resourceVersion: "530"
  selflink: /api/v1/namespaces/default/configmaps/game-config-3
  uid: 05f8da22-d671-11e5-8cd0-68f728db1985</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO39-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										This is the key that you set in the preceding step.
									</div></dd></dl></div></li></ul></div></section><section class="section" id="nodes-pods-configmap-creating-from-literal-values_configmaps"><div class="titlepage"><div><div><h4 class="title">2.7.3.3. Creating a config map from literal values</h4></div></div></div><p>
						You can supply literal values for a config map.
					</p><p>
						The <code class="literal">--from-literal</code> option takes a <code class="literal">key=value</code> syntax, which allows literal values to be supplied directly on the command line.
					</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Create a config map by specifying a literal value:
							</p><pre class="programlisting language-terminal">$ oc create configmap special-config \
    --from-literal=special.how=very \
    --from-literal=special.type=charm</pre></li></ul></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Enter the <code class="literal">oc get</code> command for the object with the <code class="literal">-o</code> option to see the values of the keys:
							</p><pre class="programlisting language-terminal">$ oc get configmaps special-config -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
data:
  special.how: very
  special.type: charm
kind: ConfigMap
metadata:
  creationTimestamp: 2016-02-18T19:14:38Z
  name: special-config
  namespace: default
  resourceVersion: "651"
  selflink: /api/v1/namespaces/default/configmaps/special-config
  uid: dadce046-d673-11e5-8cd0-68f728db1985</pre>

								</p></div></li></ul></div></section></section><section class="section" id="nodes-pods-configmaps-consuming-configmap-in-pods"><div class="titlepage"><div><div><h3 class="title">2.7.4. Use cases: Consuming config maps in pods</h3></div></div></div><p>
					The following sections describe some uses cases when consuming <code class="literal">ConfigMap</code> objects in pods.
				</p><section class="section" id="nodes-pods-configmaps-use-case-consuming-in-env-vars_configmaps"><div class="titlepage"><div><div><h4 class="title">2.7.4.1. Populating environment variables in containers by using config maps</h4></div></div></div><p>
						You can use config maps to populate individual environment variables in containers or to populate environment variables in containers from all keys that form valid environment variable names.
					</p><p>
						As an example, consider the following config map:
					</p><div class="formalpara"><p class="title"><strong><code class="literal">ConfigMap</code> with two environment variables</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config <span id="CO40-1"><!--Empty--></span><span class="callout">1</span>
  namespace: default <span id="CO40-2"><!--Empty--></span><span class="callout">2</span>
data:
  special.how: very <span id="CO40-3"><!--Empty--></span><span class="callout">3</span>
  special.type: charm <span id="CO40-4"><!--Empty--></span><span class="callout">4</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO40-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Name of the config map.
							</div></dd><dt><a href="#CO40-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The project in which the config map resides. Config maps can only be referenced by pods in the same project.
							</div></dd><dt><a href="#CO40-3"><span class="callout">3</span></a> <a href="#CO40-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Environment variables to inject.
							</div></dd></dl></div><div class="formalpara"><p class="title"><strong><code class="literal">ConfigMap</code> with one environment variable</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: env-config <span id="CO41-1"><!--Empty--></span><span class="callout">1</span>
  namespace: default
data:
  log_level: INFO <span id="CO41-2"><!--Empty--></span><span class="callout">2</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO41-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Name of the config map.
							</div></dd><dt><a href="#CO41-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Environment variable to inject.
							</div></dd></dl></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								You can consume the keys of this <code class="literal">ConfigMap</code> in a pod using <code class="literal">configMapKeyRef</code> sections.
							</p><div class="formalpara"><p class="title"><strong>Sample <code class="literal">Pod</code> specification configured to inject specific environment variables</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ "/bin/sh", "-c", "env" ]
      env: <span id="CO42-1"><!--Empty--></span><span class="callout">1</span>
        - name: SPECIAL_LEVEL_KEY <span id="CO42-2"><!--Empty--></span><span class="callout">2</span>
          valueFrom:
            configMapKeyRef:
              name: special-config <span id="CO42-3"><!--Empty--></span><span class="callout">3</span>
              key: special.how <span id="CO42-4"><!--Empty--></span><span class="callout">4</span>
        - name: SPECIAL_TYPE_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config <span id="CO42-5"><!--Empty--></span><span class="callout">5</span>
              key: special.type <span id="CO42-6"><!--Empty--></span><span class="callout">6</span>
              optional: true <span id="CO42-7"><!--Empty--></span><span class="callout">7</span>
      envFrom: <span id="CO42-8"><!--Empty--></span><span class="callout">8</span>
        - configMapRef:
            name: env-config <span id="CO42-9"><!--Empty--></span><span class="callout">9</span>
  restartPolicy: Never</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO42-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Stanza to pull the specified environment variables from a <code class="literal">ConfigMap</code>.
									</div></dd><dt><a href="#CO42-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Name of a pod environment variable that you are injecting a key’s value into.
									</div></dd><dt><a href="#CO42-3"><span class="callout">3</span></a> <a href="#CO42-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										Name of the <code class="literal">ConfigMap</code> to pull specific environment variables from.
									</div></dd><dt><a href="#CO42-4"><span class="callout">4</span></a> <a href="#CO42-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										Environment variable to pull from the <code class="literal">ConfigMap</code>.
									</div></dd><dt><a href="#CO42-7"><span class="callout">7</span></a> </dt><dd><div class="para">
										Makes the environment variable optional. As optional, the pod will be started even if the specified <code class="literal">ConfigMap</code> and keys do not exist.
									</div></dd><dt><a href="#CO42-8"><span class="callout">8</span></a> </dt><dd><div class="para">
										Stanza to pull all environment variables from a <code class="literal">ConfigMap</code>.
									</div></dd><dt><a href="#CO42-9"><span class="callout">9</span></a> </dt><dd><div class="para">
										Name of the <code class="literal">ConfigMap</code> to pull all environment variables from.
									</div></dd></dl></div><p class="simpara">
								When this pod is run, the pod logs will include the following output:
							</p><pre class="screen">SPECIAL_LEVEL_KEY=very
log_level=INFO</pre></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							<code class="literal">SPECIAL_TYPE_KEY=charm</code> is not listed in the example output because <code class="literal">optional: true</code> is set.
						</p></div></div></section><section class="section" id="nodes-pods-configmaps-use-case-setting-command-line-arguments_configmaps"><div class="titlepage"><div><div><h4 class="title">2.7.4.2. Setting command-line arguments for container commands with config maps</h4></div></div></div><p>
						You can use a config map to set the value of the commands or arguments in a container by using the Kubernetes substitution syntax <code class="literal">$(VAR_NAME)</code>.
					</p><p>
						As an example, consider the following config map:
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  special.how: very
  special.type: charm</pre><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								To inject values into a command in a container, you must consume the keys you want to use as environment variables. Then you can refer to them in a container’s command using the <code class="literal">$(VAR_NAME)</code> syntax.
							</p><div class="formalpara"><p class="title"><strong>Sample pod specification configured to inject specific environment variables</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ "/bin/sh", "-c", "echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)" ] <span id="CO43-1"><!--Empty--></span><span class="callout">1</span>
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: SPECIAL_TYPE_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.type
  restartPolicy: Never</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO43-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Inject the values into a command in a container using the keys you want to use as environment variables.
									</div></dd></dl></div><p class="simpara">
								When this pod is run, the output from the echo command run in the test-container container is as follows:
							</p><pre class="screen">very charm</pre></li></ul></div></section><section class="section" id="nodes-pods-configmaps-use-case-consuming-in-volumes_configmaps"><div class="titlepage"><div><div><h4 class="title">2.7.4.3. Injecting content into a volume by using config maps</h4></div></div></div><p>
						You can inject content into a volume by using config maps.
					</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">ConfigMap</code> custom resource (CR)</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  special.how: very
  special.type: charm</pre>

						</p></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							You have a couple different options for injecting content into a volume by using config maps.
						</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								The most basic way to inject content into a volume by using a config map is to populate the volume with files where the key is the file name and the content of the file is the value of the key:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ "/bin/sh", "-c", "cat", "/etc/config/special.how" ]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        name: special-config <span id="CO44-1"><!--Empty--></span><span class="callout">1</span>
  restartPolicy: Never</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO44-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										File containing key.
									</div></dd></dl></div><p class="simpara">
								When this pod is run, the output of the cat command will be:
							</p><pre class="screen">very</pre></li><li class="listitem"><p class="simpara">
								You can also control the paths within the volume where config map keys are projected:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ "/bin/sh", "-c", "cat", "/etc/config/path/to/special-key" ]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        name: special-config
        items:
        - key: special.how
          path: path/to/special-key <span id="CO45-1"><!--Empty--></span><span class="callout">1</span>
  restartPolicy: Never</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO45-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Path to config map key.
									</div></dd></dl></div><p class="simpara">
								When this pod is run, the output of the cat command will be:
							</p><pre class="screen">very</pre></li></ul></div></section></section></section><section class="section" id="nodes-pods-device"><div class="titlepage"><div><div><h2 class="title">2.8. Using device plugins to access external resources with pods</h2></div></div></div><p>
				Device plugins allow you to use a particular device type (GPU, InfiniBand, or other similar computing resources that require vendor-specific initialization and setup) in your OpenShift Container Platform pod without needing to write custom code.
			</p><section class="section" id="nodes-pods-plugins-about_nodes-pods-device"><div class="titlepage"><div><div><h3 class="title">2.8.1. Understanding device plugins</h3></div></div></div><p>
					The device plugin provides a consistent and portable solution to consume hardware devices across clusters. The device plugin provides support for these devices through an extension mechanism, which makes these devices available to Containers, provides health checks of these devices, and securely shares them.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						OpenShift Container Platform supports the device plugin API, but the device plugin Containers are supported by individual vendors.
					</p></div></div><p>
					A device plugin is a gRPC service running on the nodes (external to the <code class="literal">kubelet</code>) that is responsible for managing specific hardware resources. Any device plugin must support following remote procedure calls (RPCs):
				</p><pre class="programlisting language-golang">service DevicePlugin {
      // GetDevicePluginOptions returns options to be communicated with Device
      // Manager
      rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {}

      // ListAndWatch returns a stream of List of Devices
      // Whenever a Device state change or a Device disappears, ListAndWatch
      // returns the new list
      rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}

      // Allocate is called during container creation so that the Device
      // Plug-in can run device specific operations and instruct Kubelet
      // of the steps to make the Device available in the container
      rpc Allocate(AllocateRequest) returns (AllocateResponse) {}

      // PreStartcontainer is called, if indicated by Device Plug-in during
      // registration phase, before each container start. Device plug-in
      // can run device specific operations such as reseting the device
      // before making devices available to the container
      rpc PreStartcontainer(PreStartcontainerRequest) returns (PreStartcontainerResponse) {}
}</pre><h6 id="example-device-plugins">Example device plugins</h6><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://github.com/GoogleCloudPlatform/Container-engine-accelerators/tree/master/cmd/nvidia_gpu">Nvidia GPU device plugin for COS-based operating system</a>
						</li><li class="listitem">
							<a class="link" href="https://github.com/NVIDIA/k8s-device-plugin">Nvidia official GPU device plugin</a>
						</li><li class="listitem">
							<a class="link" href="https://github.com/vikaschoudhary16/sfc-device-plugin">Solarflare device plugin</a>
						</li><li class="listitem">
							<a class="link" href="https://github.com/kubevirt/kubernetes-device-plugins">KubeVirt device plugins: vfio and kvm</a>
						</li><li class="listitem">
							<a class="link" href="https://github.com/ibm-s390-cloud/k8s-cex-dev-plugin">Kubernetes device plugin for IBM Crypto Express (CEX) cards</a>
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						For easy device plugin reference implementation, there is a stub device plugin in the Device Manager code: <span class="strong strong"><strong><span class="emphasis"><em>vendor/k8s.io/kubernetes/pkg/kubelet/cm/deviceplugin/device_plugin_stub.go</em></span></strong></span>.
					</p></div></div><section class="section" id="methods-for-deploying-a-device-plugin_nodes-pods-device"><div class="titlepage"><div><div><h4 class="title">2.8.1.1. Methods for deploying a device plugin</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Daemon sets are the recommended approach for device plugin deployments.
							</li><li class="listitem">
								Upon start, the device plugin will try to create a UNIX domain socket at <span class="strong strong"><strong><span class="emphasis"><em>/var/lib/kubelet/device-plugin/</em></span></strong></span> on the node to serve RPCs from Device Manager.
							</li><li class="listitem">
								Since device plugins must manage hardware resources, access to the host file system, as well as socket creation, they must be run in a privileged security context.
							</li><li class="listitem">
								More specific details regarding deployment steps can be found with each device plugin implementation.
							</li></ul></div></section></section><section class="section" id="nodes-pods-plugins-device-mgr_nodes-pods-device"><div class="titlepage"><div><div><h3 class="title">2.8.2. Understanding the Device Manager</h3></div></div></div><p>
					Device Manager provides a mechanism for advertising specialized node hardware resources with the help of plugins known as device plugins.
				</p><p>
					You can advertise specialized hardware without requiring any upstream code changes.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						OpenShift Container Platform supports the device plugin API, but the device plugin Containers are supported by individual vendors.
					</p></div></div><p>
					Device Manager advertises devices as <span class="strong strong"><strong>Extended Resources</strong></span>. User pods can consume devices, advertised by Device Manager, using the same <span class="strong strong"><strong>Limit/Request</strong></span> mechanism, which is used for requesting any other <span class="strong strong"><strong>Extended Resource</strong></span>.
				</p><p>
					Upon start, the device plugin registers itself with Device Manager invoking <code class="literal">Register</code> on the <span class="strong strong"><strong><span class="emphasis"><em>/var/lib/kubelet/device-plugins/kubelet.sock</em></span></strong></span> and starts a gRPC service at <span class="strong strong"><strong><span class="emphasis"><em>/var/lib/kubelet/device-plugins/&lt;plugin&gt;.sock</em></span></strong></span> for serving Device Manager requests.
				</p><p>
					Device Manager, while processing a new registration request, invokes <code class="literal">ListAndWatch</code> remote procedure call (RPC) at the device plugin service. In response, Device Manager gets a list of <span class="strong strong"><strong>Device</strong></span> objects from the plugin over a gRPC stream. Device Manager will keep watching on the stream for new updates from the plugin. On the plugin side, the plugin will also keep the stream open and whenever there is a change in the state of any of the devices, a new device list is sent to the Device Manager over the same streaming connection.
				</p><p>
					While handling a new pod admission request, Kubelet passes requested <code class="literal">Extended Resources</code> to the Device Manager for device allocation. Device Manager checks in its database to verify if a corresponding plugin exists or not. If the plugin exists and there are free allocatable devices as well as per local cache, <code class="literal">Allocate</code> RPC is invoked at that particular device plugin.
				</p><p>
					Additionally, device plugins can also perform several other device-specific operations, such as driver installation, device initialization, and device resets. These functionalities vary from implementation to implementation.
				</p></section><section class="section" id="nodes-pods-plugins-install_nodes-pods-device"><div class="titlepage"><div><div><h3 class="title">2.8.3. Enabling Device Manager</h3></div></div></div><p>
					Enable Device Manager to implement a device plugin to advertise specialized hardware without any upstream code changes.
				</p><p>
					Device Manager provides a mechanism for advertising specialized node hardware resources with the help of plugins known as device plugins.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the label associated with the static <code class="literal">MachineConfigPool</code> CRD for the type of node you want to configure by entering the following command. Perform one of the following steps:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									View the machine config:
								</p><pre class="programlisting language-terminal"># oc describe machineconfig &lt;name&gt;</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal"># oc describe machineconfig 00-worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">Name:         00-worker
Namespace:
Labels:       machineconfiguration.openshift.io/role=worker <span id="CO46-1"><!--Empty--></span><span class="callout">1</span></pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO46-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Label required for the Device Manager.
										</div></dd></dl></div></li></ol></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a custom resource (CR) for your configuration change.
						</p><div class="formalpara"><p class="title"><strong>Sample configuration for a Device Manager CR</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: devicemgr <span id="CO47-1"><!--Empty--></span><span class="callout">1</span>
spec:
  machineConfigPoolSelector:
    matchLabels:
       machineconfiguration.openshift.io: devicemgr <span id="CO47-2"><!--Empty--></span><span class="callout">2</span>
  kubeletConfig:
    feature-gates:
      - DevicePlugins=true <span id="CO47-3"><!--Empty--></span><span class="callout">3</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO47-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Assign a name to CR.
								</div></dd><dt><a href="#CO47-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Enter the label from the Machine Config Pool.
								</div></dd><dt><a href="#CO47-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Set <code class="literal">DevicePlugins</code> to 'true`.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the Device Manager:
						</p><pre class="programlisting language-terminal">$ oc create -f devicemgr.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">kubeletconfig.machineconfiguration.openshift.io/devicemgr created</pre>

							</p></div></li><li class="listitem">
							Ensure that Device Manager was actually enabled by confirming that <span class="strong strong"><strong><span class="emphasis"><em>/var/lib/kubelet/device-plugins/kubelet.sock</em></span></strong></span> is created on the node. This is the UNIX domain socket on which the Device Manager gRPC server listens for new plugin registrations. This sock file is created when the Kubelet is started only if Device Manager is enabled.
						</li></ol></div></section></section><section class="section" id="nodes-pods-priority"><div class="titlepage"><div><div><h2 class="title">2.9. Including pod priority in pod scheduling decisions</h2></div></div></div><p>
				You can enable pod priority and preemption in your cluster. Pod priority indicates the importance of a pod relative to other pods and queues the pods based on that priority. pod preemption allows the cluster to evict, or preempt, lower-priority pods so that higher-priority pods can be scheduled if there is no available space on a suitable node pod priority also affects the scheduling order of pods and out-of-resource eviction ordering on the node.
			</p><p>
				To use priority and preemption, you create priority classes that define the relative weight of your pods. Then, reference a priority class in the pod specification to apply that weight for scheduling.
			</p><section class="section" id="nodes-pods-priority-about_nodes-pods-priority"><div class="titlepage"><div><div><h3 class="title">2.9.1. Understanding pod priority</h3></div></div></div><p>
					When you use the Pod Priority and Preemption feature, the scheduler orders pending pods by their priority, and a pending pod is placed ahead of other pending pods with lower priority in the scheduling queue. As a result, the higher priority pod might be scheduled sooner than pods with lower priority if its scheduling requirements are met. If a pod cannot be scheduled, scheduler continues to schedule other lower priority pods.
				</p><section class="section" id="admin-guide-priority-preemption-priority-class_nodes-pods-priority"><div class="titlepage"><div><div><h4 class="title">2.9.1.1. Pod priority classes</h4></div></div></div><p>
						You can assign pods a priority class, which is a non-namespaced object that defines a mapping from a name to the integer value of the priority. The higher the value, the higher the priority.
					</p><p>
						A priority class object can take any 32-bit integer value smaller than or equal to 1000000000 (one billion). Reserve numbers larger than or equal to one billion for critical pods that must not be preempted or evicted. By default, OpenShift Container Platform has two reserved priority classes for critical system pods to have guaranteed scheduling.
					</p><pre class="programlisting language-terminal">$ oc get priorityclasses</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                      VALUE        GLOBAL-DEFAULT   AGE
system-node-critical      2000001000   false            72m
system-cluster-critical   2000000000   false            72m
openshift-user-critical   1000000000   false            3d13h
cluster-logging           1000000      false            29s</pre>

						</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								<span class="strong strong"><strong>system-node-critical</strong></span> - This priority class has a value of 2000001000 and is used for all pods that should never be evicted from a node. Examples of pods that have this priority class are <code class="literal">sdn-ovs</code>, <code class="literal">sdn</code>, and so forth. A number of critical components include the <code class="literal">system-node-critical</code> priority class by default, for example:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										master-api
									</li><li class="listitem">
										master-controller
									</li><li class="listitem">
										master-etcd
									</li><li class="listitem">
										sdn
									</li><li class="listitem">
										sdn-ovs
									</li><li class="listitem">
										sync
									</li></ul></div></li><li class="listitem"><p class="simpara">
								<span class="strong strong"><strong>system-cluster-critical</strong></span> - This priority class has a value of 2000000000 (two billion) and is used with pods that are important for the cluster. Pods with this priority class can be evicted from a node in certain circumstances. For example, pods configured with the <code class="literal">system-node-critical</code> priority class can take priority. However, this priority class does ensure guaranteed scheduling. Examples of pods that can have this priority class are fluentd, add-on components like descheduler, and so forth. A number of critical components include the <code class="literal">system-cluster-critical</code> priority class by default, for example:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										fluentd
									</li><li class="listitem">
										metrics-server
									</li><li class="listitem">
										descheduler
									</li></ul></div></li><li class="listitem">
								<span class="strong strong"><strong>openshift-user-critical</strong></span> - You can use the <code class="literal">priorityClassName</code> field with important pods that cannot bind their resource consumption and do not have predictable resource consumption behavior. Prometheus pods under the <code class="literal">openshift-monitoring</code> and <code class="literal">openshift-user-workload-monitoring</code> namespaces use the <code class="literal">openshift-user-critical</code> <code class="literal">priorityClassName</code>. Monitoring workloads use <code class="literal">system-critical</code> as their first <code class="literal">priorityClass</code>, but this causes problems when monitoring uses excessive memory and the nodes cannot evict them. As a result, monitoring drops priority to give the scheduler flexibility, moving heavy workloads around to keep critical nodes operating.
							</li><li class="listitem">
								<span class="strong strong"><strong>cluster-logging</strong></span> - This priority is used by Fluentd to make sure Fluentd pods are scheduled to nodes over other apps.
							</li></ul></div></section><section class="section" id="admin-guide-priority-preemption-names_nodes-pods-priority"><div class="titlepage"><div><div><h4 class="title">2.9.1.2. Pod priority names</h4></div></div></div><p>
						After you have one or more priority classes, you can create pods that specify a priority class name in a <code class="literal">Pod</code> spec. The priority admission controller uses the priority class name field to populate the integer value of the priority. If the named priority class is not found, the pod is rejected.
					</p></section></section><section class="section" id="nodes-pods-priority-preempt-about_nodes-pods-priority"><div class="titlepage"><div><div><h3 class="title">2.9.2. Understanding pod preemption</h3></div></div></div><p>
					When a developer creates a pod, the pod goes into a queue. If the developer configured the pod for pod priority or preemption, the scheduler picks a pod from the queue and tries to schedule the pod on a node. If the scheduler cannot find space on an appropriate node that satisfies all the specified requirements of the pod, preemption logic is triggered for the pending pod.
				</p><p>
					When the scheduler preempts one or more pods on a node, the <code class="literal">nominatedNodeName</code> field of higher-priority <code class="literal">Pod</code> spec is set to the name of the node, along with the <code class="literal">nodename</code> field. The scheduler uses the <code class="literal">nominatedNodeName</code> field to keep track of the resources reserved for pods and also provides information to the user about preemptions in the clusters.
				</p><p>
					After the scheduler preempts a lower-priority pod, the scheduler honors the graceful termination period of the pod. If another node becomes available while scheduler is waiting for the lower-priority pod to terminate, the scheduler can schedule the higher-priority pod on that node. As a result, the <code class="literal">nominatedNodeName</code> field and <code class="literal">nodeName</code> field of the <code class="literal">Pod</code> spec might be different.
				</p><p>
					Also, if the scheduler preempts pods on a node and is waiting for termination, and a pod with a higher-priority pod than the pending pod needs to be scheduled, the scheduler can schedule the higher-priority pod instead. In such a case, the scheduler clears the <code class="literal">nominatedNodeName</code> of the pending pod, making the pod eligible for another node.
				</p><p>
					Preemption does not necessarily remove all lower-priority pods from a node. The scheduler can schedule a pending pod by removing a portion of the lower-priority pods.
				</p><p>
					The scheduler considers a node for pod preemption only if the pending pod can be scheduled on the node.
				</p><section class="section" id="non-preempting-priority-class_nodes-pods-priority"><div class="titlepage"><div><div><h4 class="title">2.9.2.1. Non-preempting priority classes</h4></div></div></div><p>
						Pods with the preemption policy set to <code class="literal">Never</code> are placed in the scheduling queue ahead of lower-priority pods, but they cannot preempt other pods. A non-preempting pod waiting to be scheduled stays in the scheduling queue until sufficient resources are free and it can be scheduled. Non-preempting pods, like other pods, are subject to scheduler back-off. This means that if the scheduler tries unsuccessfully to schedule these pods, they are retried with lower frequency, allowing other pods with lower priority to be scheduled before them.
					</p><p>
						Non-preempting pods can still be preempted by other, high-priority pods.
					</p></section><section class="section" id="priority-preemption-other_nodes-pods-priority"><div class="titlepage"><div><div><h4 class="title">2.9.2.2. Pod preemption and other scheduler settings</h4></div></div></div><p>
						If you enable pod priority and preemption, consider your other scheduler settings:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Pod priority and pod disruption budget</span></dt><dd>
									A pod disruption budget specifies the minimum number or percentage of replicas that must be up at a time. If you specify pod disruption budgets, OpenShift Container Platform respects them when preempting pods at a best effort level. The scheduler attempts to preempt pods without violating the pod disruption budget. If no such pods are found, lower-priority pods might be preempted despite their pod disruption budget requirements.
								</dd><dt><span class="term">Pod priority and pod affinity</span></dt><dd>
									Pod affinity requires a new pod to be scheduled on the same node as other pods with the same label.
								</dd></dl></div><p>
						If a pending pod has inter-pod affinity with one or more of the lower-priority pods on a node, the scheduler cannot preempt the lower-priority pods without violating the affinity requirements. In this case, the scheduler looks for another node to schedule the pending pod. However, there is no guarantee that the scheduler can find an appropriate node and pending pod might not be scheduled.
					</p><p>
						To prevent this situation, carefully configure pod affinity with equal-priority pods.
					</p></section><section class="section" id="priority-preemption-graceful_nodes-pods-priority"><div class="titlepage"><div><div><h4 class="title">2.9.2.3. Graceful termination of preempted pods</h4></div></div></div><p>
						When preempting a pod, the scheduler waits for the pod graceful termination period to expire, allowing the pod to finish working and exit. If the pod does not exit after the period, the scheduler kills the pod. This graceful termination period creates a time gap between the point that the scheduler preempts the pod and the time when the pending pod can be scheduled on the node.
					</p><p>
						To minimize this gap, configure a small graceful termination period for lower-priority pods.
					</p></section></section><section class="section" id="nodes-pods-priority-configuring_nodes-pods-priority"><div class="titlepage"><div><div><h3 class="title">2.9.3. Configuring priority and preemption</h3></div></div></div><p>
					You apply pod priority and preemption by creating a priority class object and associating pods to the priority by using the <code class="literal">priorityClassName</code> in your pod specs.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You cannot add a priority class directly to an existing scheduled pod.
					</p></div></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To configure your cluster to use priority and preemption:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create one or more priority classes:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority <span id="CO48-1"><!--Empty--></span><span class="callout">1</span>
value: 1000000 <span id="CO48-2"><!--Empty--></span><span class="callout">2</span>
preemptionPolicy: PreemptLowerPriority <span id="CO48-3"><!--Empty--></span><span class="callout">3</span>
globalDefault: false <span id="CO48-4"><!--Empty--></span><span class="callout">4</span>
description: "This priority class should be used for XYZ service pods only." <span id="CO48-5"><!--Empty--></span><span class="callout">5</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO48-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The name of the priority class object.
										</div></dd><dt><a href="#CO48-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											The priority value of the object.
										</div></dd><dt><a href="#CO48-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Optional. Specifies whether this priority class is preempting or non-preempting. The preemption policy defaults to <code class="literal">PreemptLowerPriority</code>, which allows pods of that priority class to preempt lower-priority pods. If the preemption policy is set to <code class="literal">Never</code>, pods in that priority class are non-preempting.
										</div></dd><dt><a href="#CO48-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Optional. Specifies whether this priority class should be used for pods without a priority class name specified. This field is <code class="literal">false</code> by default. Only one priority class with <code class="literal">globalDefault</code> set to <code class="literal">true</code> can exist in the cluster. If there is no priority class with <code class="literal">globalDefault:true</code>, the priority of pods with no priority class name is zero. Adding a priority class with <code class="literal">globalDefault:true</code> affects only pods created after the priority class is added and does not change the priorities of existing pods.
										</div></dd><dt><a href="#CO48-5"><span class="callout">5</span></a> </dt><dd><div class="para">
											Optional. Describes which pods developers should use with this priority class. Enter an arbitrary text string.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Create the priority class:
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a pod spec to include the name of a priority class:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  priorityClassName: high-priority <span id="CO49-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO49-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the priority class to use with this pod.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Create the pod:
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre><p class="simpara">
									You can add the priority name directly to the pod configuration or to a pod template.
								</p></li></ol></div></li></ol></div></section></section><section class="section" id="nodes-pods-node-selectors"><div class="titlepage"><div><div><h2 class="title">2.10. Placing pods on specific nodes using node selectors</h2></div></div></div><p>
				A <span class="emphasis"><em>node selector</em></span> specifies a map of key-value pairs. The rules are defined using custom labels on nodes and selectors specified in pods.
			</p><p>
				For the pod to be eligible to run on a node, the pod must have the indicated key-value pairs as the label on the node.
			</p><p>
				If you are using node affinity and node selectors in the same pod configuration, see the important considerations below.
			</p><section class="section" id="nodes-scheduler-node-selectors-pod_nodes-pods-node-selectors"><div class="titlepage"><div><div><h3 class="title">2.10.1. Using node selectors to control pod placement</h3></div></div></div><p>
					You can use node selectors on pods and labels on nodes to control where the pod is scheduled. With node selectors, OpenShift Container Platform schedules the pods on nodes that contain matching labels.
				</p><p>
					You add labels to a node, a compute machine set, or a machine config. Adding the label to the compute machine set ensures that if the node or machine goes down, new nodes have the label. Labels added to a node or machine config do not persist if the node or machine goes down.
				</p><p>
					To add node selectors to an existing pod, add a node selector to the controlling object for that pod, such as a <code class="literal">ReplicaSet</code> object, <code class="literal">DaemonSet</code> object, <code class="literal">StatefulSet</code> object, <code class="literal">Deployment</code> object, or <code class="literal">DeploymentConfig</code> object. Any existing pods under that controlling object are recreated on a node with a matching label. If you are creating a new pod, you can add the node selector directly to the pod spec. If the pod does not have a controlling object, you must delete the pod, edit the pod spec, and recreate the pod.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You cannot add a node selector directly to an existing scheduled pod.
					</p></div></div><div class="formalpara"><p class="title"><strong>Prerequisites</strong></p><p>
						To add a node selector to existing pods, determine the controlling object for that pod. For example, the <code class="literal">router-default-66d5cf9464-m2g75</code> pod is controlled by the <code class="literal">router-default-66d5cf9464</code> replica set:
					</p></div><pre class="programlisting language-terminal">$ oc describe pod router-default-66d5cf9464-7pwkc</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">kind: Pod
apiVersion: v1
metadata:
#...
Name:               router-default-66d5cf9464-7pwkc
Namespace:          openshift-ingress
# ...
Controlled By:      ReplicaSet/router-default-66d5cf9464
# ...</pre>

					</p></div><p>
					The web console lists the controlling object under <code class="literal">ownerReferences</code> in the pod YAML:
				</p><pre class="programlisting language-terminal">apiVersion: v1
kind: Pod
metadata:
  name: router-default-66d5cf9464-7pwkc
# ...
  ownerReferences:
    - apiVersion: apps/v1
      kind: ReplicaSet
      name: router-default-66d5cf9464
      uid: d81dd094-da26-11e9-a48a-128e7edf0312
      controller: true
      blockOwnerDeletion: true
# ...</pre><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add labels to a node by using a compute machine set or editing the node directly:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Use a <code class="literal">MachineSet</code> object to add labels to nodes managed by the compute machine set when a node is created:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
											Run the following command to add labels to a <code class="literal">MachineSet</code> object:
										</p><pre class="programlisting language-terminal">$ oc patch MachineSet &lt;name&gt; --type='json' -p='[{"op":"add","path":"/spec/template/spec/metadata/labels", "value":{"&lt;key&gt;"="&lt;value&gt;","&lt;key&gt;"="&lt;value&gt;"}}]'  -n openshift-machine-api</pre><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc patch MachineSet abc612-msrtw-worker-us-east-1c  --type='json' -p='[{"op":"add","path":"/spec/template/spec/metadata/labels", "value":{"type":"user-node","region":"east"}}]'  -n openshift-machine-api</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
											You can alternatively apply the following YAML to add labels to a compute machine set:
										</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: xf2bd-infra-us-east-2a
  namespace: openshift-machine-api
spec:
  template:
    spec:
      metadata:
        labels:
          region: "east"
          type: "user-node"
#...</pre></div></div></li><li class="listitem"><p class="simpara">
											Verify that the labels are added to the <code class="literal">MachineSet</code> object by using the <code class="literal">oc edit</code> command:
										</p><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc edit MachineSet abc612-msrtw-worker-us-east-1c -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example <code class="literal">MachineSet</code> object</strong></p><p>
												
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet

# ...

spec:
# ...
  template:
    metadata:
# ...
    spec:
      metadata:
        labels:
          region: east
          type: user-node
# ...</pre>

											</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
									Add labels directly to a node:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
											Edit the <code class="literal">Node</code> object for the node:
										</p><pre class="programlisting language-terminal">$ oc label nodes &lt;name&gt; &lt;key&gt;=&lt;value&gt;</pre><p class="simpara">
											For example, to label a node:
										</p><pre class="programlisting language-terminal">$ oc label nodes ip-10-0-142-25.ec2.internal type=user-node region=east</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
											You can alternatively apply the following YAML to add labels to a node:
										</p><pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  name: hello-node-6fbccf8d9
  labels:
    type: "user-node"
    region: "east"
#...</pre></div></div></li><li class="listitem"><p class="simpara">
											Verify that the labels are added to the node:
										</p><pre class="programlisting language-terminal">$ oc get nodes -l type=user-node,region=east</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
												
<pre class="programlisting language-terminal">NAME                          STATUS   ROLES    AGE   VERSION
ip-10-0-142-25.ec2.internal   Ready    worker   17m   v1.26.0</pre>

											</p></div></li></ol></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Add the matching node selector to a pod:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To add a node selector to existing and future pods, add a node selector to the controlling object for the pods:
								</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">ReplicaSet</code> object with labels</strong></p><p>
										
<pre class="programlisting language-yaml">kind: ReplicaSet
apiVersion: apps/v1
metadata:
  name: hello-node-6fbccf8d9
# ...
spec:
# ...
  template:
    metadata:
      creationTimestamp: null
      labels:
        ingresscontroller.operator.openshift.io/deployment-ingresscontroller: default
        pod-template-hash: 66d5cf9464
    spec:
      nodeSelector:
        kubernetes.io/os: linux
        node-role.kubernetes.io/worker: ''
        type: user-node <span id="CO50-1"><!--Empty--></span><span class="callout">1</span>
#...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO50-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Add the node selector.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To add a node selector to a specific, new pod, add the selector to the <code class="literal">Pod</code> object directly:
								</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">Pod</code> object with a node selector</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: hello-node-6fbccf8d9
#...
spec:
  nodeSelector:
    region: east
    type: user-node
#...</pre>

									</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										You cannot add a node selector directly to an existing scheduled pod.
									</p></div></div></li></ul></div></li></ol></div></section></section><section class="section" id="run-once-duration-override-operator"><div class="titlepage"><div><div><h2 class="title">2.11. Run Once Duration Override Operator</h2></div></div></div><section class="section" id="run-once-duration-override-about"><div class="titlepage"><div><div><h3 class="title">2.11.1. Run Once Duration Override Operator overview</h3></div></div></div><p>
					You can use the Run Once Duration Override Operator to specify a maximum time limit that run-once pods can be active for.
				</p><section class="section" id="run-once-about_run-once-duration-override-about"><div class="titlepage"><div><div><h4 class="title">2.11.1.1. About the Run Once Duration Override Operator</h4></div></div></div><p>
						OpenShift Container Platform relies on run-once pods to perform tasks such as deploying a pod or performing a build. Run-once pods are pods that have a <code class="literal">RestartPolicy</code> of <code class="literal">Never</code> or <code class="literal">OnFailure</code>.
					</p><p>
						Cluster administrators can use the Run Once Duration Override Operator to force a limit on the time that those run-once pods can be active. After the time limit expires, the cluster will try to actively terminate those pods. The main reason to have such a limit is to prevent tasks such as builds to run for an excessive amount of time.
					</p><p>
						To apply the run-once duration override from the Run Once Duration Override Operator to run-once pods, you must enable it on each applicable namespace.
					</p><p>
						If both the run-once pod and the Run Once Duration Override Operator have their <code class="literal">activeDeadlineSeconds</code> value set, the lower of the two values is used.
					</p></section></section><section class="section" id="run-once-duration-override-release-notes"><div class="titlepage"><div><div><h3 class="title">2.11.2. Run Once Duration Override Operator release notes</h3></div></div></div><p>
					Cluster administrators can use the Run Once Duration Override Operator to force a limit on the time that run-once pods can be active. After the time limit expires, the cluster tries to terminate the run-once pods. The main reason to have such a limit is to prevent tasks such as builds to run for an excessive amount of time.
				</p><p>
					To apply the run-once duration override from the Run Once Duration Override Operator to run-once pods, you must enable it on each applicable namespace.
				</p><p>
					These release notes track the development of the Run Once Duration Override Operator for OpenShift Container Platform.
				</p><p>
					For an overview of the Run Once Duration Override Operator, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#run-once-about_run-once-duration-override-about">About the Run Once Duration Override Operator</a>.
				</p><section class="section" id="run-once-duration-override-operator-release-notes-1-0-0"><div class="titlepage"><div><div><h4 class="title">2.11.2.1. OpenShift Run Once Duration Override Operator 1.0.0</h4></div></div></div><p>
						Issued: 2023-05-18
					</p><p>
						The following advisory is available for the Run Once Duration Override Operator 1.0.0:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/errata/RHEA-2023:2035">RHEA-2023:2035</a>
							</li></ul></div><section class="section" id="run-once-duration-override-operator-1-0-0-new-features-and-enhancements"><div class="titlepage"><div><div><h5 class="title">2.11.2.1.1. New features and enhancements</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									This is the initial, generally available release of the Run Once Duration Override Operator. For installation information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#run-once-duration-override-install">Installing the Run Once Duration Override Operator</a>.
								</li></ul></div></section></section></section><section class="section" id="run-once-duration-override-install"><div class="titlepage"><div><div><h3 class="title">2.11.3. Overriding the active deadline for run-once pods</h3></div></div></div><p>
					You can use the Run Once Duration Override Operator to specify a maximum time limit that run-once pods can be active for. By enabling the run-once duration override on a namespace, all future run-once pods created or updated in that namespace have their <code class="literal">activeDeadlineSeconds</code> field set to the value specified by the Run Once Duration Override Operator.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If both the run-once pod and the Run Once Duration Override Operator have their <code class="literal">activeDeadlineSeconds</code> value set, the lower of the two values is used.
					</p></div></div><section class="section" id="rodoo-install-operator_run-once-duration-override-install"><div class="titlepage"><div><div><h4 class="title">2.11.3.1. Installing the Run Once Duration Override Operator</h4></div></div></div><p>
						You can use the web console to install the Run Once Duration Override Operator.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have access to the OpenShift Container Platform web console.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Log in to the OpenShift Container Platform web console.
							</li><li class="listitem"><p class="simpara">
								Create the required namespace for the Run Once Duration Override Operator.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Namespaces</strong></span> and click <span class="strong strong"><strong>Create Namespace</strong></span>.
									</li><li class="listitem">
										Enter <code class="literal">openshift-run-once-duration-override-operator</code> in the <span class="strong strong"><strong>Name</strong></span> field and click <span class="strong strong"><strong>Create</strong></span>.
									</li></ol></div></li><li class="listitem"><p class="simpara">
								Install the Run Once Duration Override Operator.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span>.
									</li><li class="listitem">
										Enter <span class="strong strong"><strong>Run Once Duration Override Operator</strong></span> into the filter box.
									</li><li class="listitem">
										Select the <span class="strong strong"><strong>Run Once Duration Override Operator</strong></span> and click <span class="strong strong"><strong>Install</strong></span>.
									</li><li class="listitem"><p class="simpara">
										On the <span class="strong strong"><strong>Install Operator</strong></span> page:
									</p><div class="orderedlist"><ol class="orderedlist" type="i"><li class="listitem">
												The <span class="strong strong"><strong>Update channel</strong></span> is set to <span class="strong strong"><strong>stable</strong></span>, which installs the latest stable release of the Run Once Duration Override Operator.
											</li><li class="listitem">
												Select <span class="strong strong"><strong>A specific namespace on the cluster</strong></span>.
											</li><li class="listitem">
												Choose <span class="strong strong"><strong>openshift-run-once-duration-override-operator</strong></span> from the dropdown menu under <span class="strong strong"><strong>Installed namespace</strong></span>.
											</li><li class="listitem"><p class="simpara">
												Select an <span class="strong strong"><strong>Update approval</strong></span> strategy.
											</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
														The <span class="strong strong"><strong>Automatic</strong></span> strategy allows Operator Lifecycle Manager (OLM) to automatically update the Operator when a new version is available.
													</li><li class="listitem">
														The <span class="strong strong"><strong>Manual</strong></span> strategy requires a user with appropriate credentials to approve the Operator update.
													</li></ul></div></li><li class="listitem">
												Click <span class="strong strong"><strong>Install</strong></span>.
											</li></ol></div></li></ol></div></li><li class="listitem"><p class="simpara">
								Create a <code class="literal">RunOnceDurationOverride</code> instance.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										From the <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span> page, click <span class="strong strong"><strong>Run Once Duration Override Operator</strong></span>.
									</li><li class="listitem">
										Select the <span class="strong strong"><strong>Run Once Duration Override</strong></span> tab and click <span class="strong strong"><strong>Create RunOnceDurationOverride</strong></span>.
									</li><li class="listitem"><p class="simpara">
										Edit the settings as necessary.
									</p><p class="simpara">
										Under the <code class="literal">runOnceDurationOverride</code> section, you can update the <code class="literal">spec.activeDeadlineSeconds</code> value, if required. The predefined value is <code class="literal">3600</code> seconds, or 1 hour.
									</p></li><li class="listitem">
										Click <span class="strong strong"><strong>Create</strong></span>.
									</li></ol></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Log in to the OpenShift CLI.
							</li><li class="listitem"><p class="simpara">
								Verify all pods are created and running properly.
							</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-run-once-duration-override-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                                                   READY   STATUS    RESTARTS   AGE
run-once-duration-override-operator-7b88c676f6-lcxgc   1/1     Running   0          7m46s
runoncedurationoverride-62blp                          1/1     Running   0          41s
runoncedurationoverride-h8h8b                          1/1     Running   0          41s
runoncedurationoverride-tdsqk                          1/1     Running   0          41s</pre>

								</p></div></li></ol></div></section><section class="section" id="rodoo-enable-override_run-once-duration-override-install"><div class="titlepage"><div><div><h4 class="title">2.11.3.2. Enabling the run-once duration override on a namespace</h4></div></div></div><p>
						To apply the run-once duration override from the Run Once Duration Override Operator to run-once pods, you must enable it on each applicable namespace.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								The Run Once Duration Override Operator is installed.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Log in to the OpenShift CLI.
							</li><li class="listitem"><p class="simpara">
								Add the label to enable the run-once duration override to your namespace:
							</p><pre class="programlisting language-terminal">$ oc label namespace &lt;namespace&gt; \ <span id="CO51-1"><!--Empty--></span><span class="callout">1</span>
    runoncedurationoverrides.admission.runoncedurationoverride.openshift.io/enabled=true</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO51-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify the namespace to enable the run-once duration override on.
									</div></dd></dl></div></li></ol></div><p>
						After you enable the run-once duration override on this namespace, future run-once pods that are created in this namespace will have their <code class="literal">activeDeadlineSeconds</code> field set to the override value from the Run Once Duration Override Operator. Existing pods in this namespace will also have their <code class="literal">activeDeadlineSeconds</code> value set when they are updated next.
					</p><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a test run-once pod in the namespace that you enabled the run-once duration override on:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: example
  namespace: &lt;namespace&gt;                 <span id="CO52-1"><!--Empty--></span><span class="callout">1</span>
spec:
  restartPolicy: Never                   <span id="CO52-2"><!--Empty--></span><span class="callout">2</span>
  containers:
    - name: busybox
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop: ["ALL"]
        runAsNonRoot:
          true
        seccompProfile:
          type: "RuntimeDefault"
      image: busybox:1.25
      command:
        - /bin/sh
        - -ec
        - |
          while sleep 5; do date; done</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO52-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;namespace&gt;</code> with the name of your namespace.
									</div></dd><dt><a href="#CO52-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The <code class="literal">restartPolicy</code> must be <code class="literal">Never</code> or <code class="literal">OnFailure</code> to be a run-once pod.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Verify that the pod has its <code class="literal">activeDeadlineSeconds</code> field set:
							</p><pre class="programlisting language-terminal">$ oc get pods -n &lt;namespace&gt; -o yaml | grep activeDeadlineSeconds</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">    activeDeadlineSeconds: 3600</pre>

								</p></div></li></ol></div></section><section class="section" id="rodoo-update-active-deadline-seconds_run-once-duration-override-install"><div class="titlepage"><div><div><h4 class="title">2.11.3.3. Updating the run-once active deadline override value</h4></div></div></div><p>
						You can customize the override value that the Run Once Duration Override Operator applies to run-once pods. The predefined value is <code class="literal">3600</code> seconds, or 1 hour.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have installed the Run Once Duration Override Operator.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Log in to the OpenShift CLI.
							</li><li class="listitem"><p class="simpara">
								Edit the <code class="literal">RunOnceDurationOverride</code> resource:
							</p><pre class="programlisting language-terminal">$ oc edit runoncedurationoverride cluster</pre></li><li class="listitem"><p class="simpara">
								Update the <code class="literal">activeDeadlineSeconds</code> field:
							</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: RunOnceDurationOverride
metadata:
# ...
spec:
  runOnceDurationOverride:
    spec:
      activeDeadlineSeconds: 1800 <span id="CO53-1"><!--Empty--></span><span class="callout">1</span>
# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO53-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Set the <code class="literal">activeDeadlineSeconds</code> field to the desired value, in seconds.
									</div></dd></dl></div></li><li class="listitem">
								Save the file to apply the changes.
							</li></ol></div><p>
						Any future run-once pods created in namespaces where the run-once duration override is enabled will have their <code class="literal">activeDeadlineSeconds</code> field set to this new value. Existing run-once pods in these namespaces will receive this new value when they are updated.
					</p></section></section><section class="section" id="run-once-duration-override-uninstall"><div class="titlepage"><div><div><h3 class="title">2.11.4. Uninstalling the Run Once Duration Override Operator</h3></div></div></div><p>
					You can remove the Run Once Duration Override Operator from OpenShift Container Platform by uninstalling the Operator and removing its related resources.
				</p><section class="section" id="rodoo-uninstall-operator_run-once-duration-override-uninstall"><div class="titlepage"><div><div><h4 class="title">2.11.4.1. Uninstalling the Run Once Duration Override Operator</h4></div></div></div><p>
						You can use the web console to uninstall the Run Once Duration Override Operator. Uninstalling the Run Once Duration Override Operator does not unset the <code class="literal">activeDeadlineSeconds</code> field for run-once pods, but it will no longer apply the override value to future run-once pods.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have access to the OpenShift Container Platform web console.
							</li><li class="listitem">
								You have installed the Run Once Duration Override Operator.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Log in to the OpenShift Container Platform web console.
							</li><li class="listitem">
								Navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
							</li><li class="listitem">
								Select <code class="literal">openshift-run-once-duration-override-operator</code> from the <span class="strong strong"><strong>Project</strong></span> dropdown list.
							</li><li class="listitem"><p class="simpara">
								Delete the <code class="literal">RunOnceDurationOverride</code> instance.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Click <span class="strong strong"><strong>Run Once Duration Override Operator</strong></span> and select the <span class="strong strong"><strong>Run Once Duration Override</strong></span> tab.
									</li><li class="listitem">
										Click the Options menu 
										<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Nodes-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
										 next to the <span class="strong strong"><strong>cluster</strong></span> entry and select <span class="strong strong"><strong>Delete RunOnceDurationOverride</strong></span>.
									</li><li class="listitem">
										In the confirmation dialog, click <span class="strong strong"><strong>Delete</strong></span>.
									</li></ol></div></li><li class="listitem"><p class="simpara">
								Uninstall the Run Once Duration Override Operator Operator.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
									</li><li class="listitem">
										Click the Options menu 
										<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Nodes-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
										 next to the <span class="strong strong"><strong>Run Once Duration Override Operator</strong></span> entry and click <span class="strong strong"><strong>Uninstall Operator</strong></span>.
									</li><li class="listitem">
										In the confirmation dialog, click <span class="strong strong"><strong>Uninstall</strong></span>.
									</li></ol></div></li></ol></div></section><section class="section" id="rodoo-uninstall-resources_run-once-duration-override-uninstall"><div class="titlepage"><div><div><h4 class="title">2.11.4.2. Uninstalling Run Once Duration Override Operator resources</h4></div></div></div><p>
						Optionally, after uninstalling the Run Once Duration Override Operator, you can remove its related resources from your cluster.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have access to the OpenShift Container Platform web console.
							</li><li class="listitem">
								You have uninstalled the Run Once Duration Override Operator.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Log in to the OpenShift Container Platform web console.
							</li><li class="listitem"><p class="simpara">
								Remove CRDs that were created when the Run Once Duration Override Operator was installed:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>CustomResourceDefinitions</strong></span>.
									</li><li class="listitem">
										Enter <code class="literal">RunOnceDurationOverride</code> in the <span class="strong strong"><strong>Name</strong></span> field to filter the CRDs.
									</li><li class="listitem">
										Click the Options menu 
										<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Nodes-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
										 next to the <span class="strong strong"><strong>RunOnceDurationOverride</strong></span> CRD and select <span class="strong strong"><strong>Delete CustomResourceDefinition</strong></span>.
									</li><li class="listitem">
										In the confirmation dialog, click <span class="strong strong"><strong>Delete</strong></span>.
									</li></ol></div></li><li class="listitem"><p class="simpara">
								Delete the <code class="literal">openshift-run-once-duration-override-operator</code> namespace.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Namespaces</strong></span>.
									</li><li class="listitem">
										Enter <code class="literal">openshift-run-once-duration-override-operator</code> into the filter box.
									</li><li class="listitem">
										Click the Options menu 
										<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Nodes-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
										 next to the <span class="strong strong"><strong>openshift-run-once-duration-override-operator</strong></span> entry and select <span class="strong strong"><strong>Delete Namespace</strong></span>.
									</li><li class="listitem">
										In the confirmation dialog, enter <code class="literal">openshift-run-once-duration-override-operator</code> and click <span class="strong strong"><strong>Delete</strong></span>.
									</li></ol></div></li><li class="listitem"><p class="simpara">
								Remove the run-once duration override label from the namespaces that it was enabled on.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Namespaces</strong></span>.
									</li><li class="listitem">
										Select your namespace.
									</li><li class="listitem">
										Click <span class="strong strong"><strong>Edit</strong></span> next to the <span class="strong strong"><strong>Labels</strong></span> field.
									</li><li class="listitem">
										Remove the <span class="strong strong"><strong>runoncedurationoverrides.admission.runoncedurationoverride.openshift.io/enabled=true</strong></span> label and click <span class="strong strong"><strong>Save</strong></span>.
									</li></ol></div></li></ol></div></section></section></section></section><section class="chapter" id="automatically-scaling-pods-with-the-custom-metrics-autoscaler-operator"><div class="titlepage"><div><div><h1 class="title">Chapter 3. Automatically scaling pods with the Custom Metrics Autoscaler Operator</h1></div></div></div><section class="section" id="nodes-cma-autoscaling-custom"><div class="titlepage"><div><div><h2 class="title">3.1. Custom Metrics Autoscaler Operator overview</h2></div></div></div><p>
				As a developer, you can use Custom Metrics Autoscaler Operator for Red Hat OpenShift to specify how OpenShift Container Platform should automatically increase or decrease the number of pods for a deployment, stateful set, custom resource, or job based on custom metrics that are not based only on CPU or memory.
			</p><p>
				The Custom Metrics Autoscaler Operator is an optional operator, based on the Kubernetes Event Driven Autoscaler (KEDA), that allows workloads to be scaled using additional metrics sources other than pod metrics.
			</p><p>
				The custom metrics autoscaler currently supports only the Prometheus, CPU, memory, and Apache Kafka metrics.
			</p><p>
				The Custom Metrics Autoscaler Operator scales your pods up and down based on custom, external metrics from specific applications. Your other applications continue to use other scaling methods. You configure <span class="emphasis"><em>triggers</em></span>, also known as scalers, which are the source of events and metrics that the custom metrics autoscaler uses to determine how to scale. The custom metrics autoscaler uses a metrics API to convert the external metrics to a form that OpenShift Container Platform can use. The custom metrics autoscaler creates a horizontal pod autoscaler (HPA) that performs the actual scaling.
			</p><p>
				To use the custom metrics autoscaler, you create a <code class="literal">ScaledObject</code> or <code class="literal">ScaledJob</code> object, which is a custom resource (CR) that defines the scaling metadata. You specify the deployment or job to scale, the source of the metrics to scale on (trigger), and other parameters such as the minimum and maximum replica counts allowed.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					You can create only one scaled object or scaled job for each workload that you want to scale. Also, you cannot use a scaled object or scaled job and the horizontal pod autoscaler (HPA) on the same workload.
				</p></div></div><p>
				The custom metrics autoscaler, unlike the HPA, can scale to zero. If you set the <code class="literal">minReplicaCount</code> value in the custom metrics autoscaler CR to <code class="literal">0</code>, the custom metrics autoscaler scales the workload down from 1 to 0 replicas to or up from 0 replicas to 1. This is known as the <span class="emphasis"><em>activation phase</em></span>. After scaling up to 1 replica, the HPA takes control of the scaling. This is known as the <span class="emphasis"><em>scaling phase</em></span>.
			</p><p>
				Some triggers allow you to change the number of replicas that are scaled by the cluster metrics autoscaler. In all cases, the parameter to configure the activation phase always uses the same phrase, prefixed with <span class="emphasis"><em>activation</em></span>. For example, if the <code class="literal">threshold</code> parameter configures scaling, <code class="literal">activationThreshold</code> would configure activation. Configuring the activation and scaling phases allows you more flexibility with your scaling policies. For example, you can configure a higher activation phase to prevent scaling up or down if the metric is particularly low.
			</p><p>
				The activation value has more priority than the scaling value in case of different decisions for each. For example, if the <code class="literal">threshold</code> is set to <code class="literal">10</code>, and the <code class="literal">activationThreshold</code> is <code class="literal">50</code>, if the metric reports <code class="literal">40</code>, the scaler is not active and the pods are scaled to zero even if the HPA requires 4 instances.
			</p><p>
				You can verify that the autoscaling has taken place by reviewing the number of pods in your custom resource or by reviewing the Custom Metrics Autoscaler Operator logs for messages similar to the following:
			</p><pre class="programlisting language-terminal">Successfully set ScaleTarget replica count</pre><pre class="programlisting language-terminal">Successfully updated ScaleTarget</pre><p>
				You can temporarily pause the autoscaling of a workload object, if needed. For example, you could pause autoscaling before performing cluster maintenance.
			</p></section><section class="section" id="nodes-cma-autoscaling-custom-rn_nodes-cma-autoscaling-custom-removing"><div class="titlepage"><div><div><h2 class="title">3.2. Custom Metrics Autoscaler Operator release notes</h2></div></div></div><p>
				The release notes for the Custom Metrics Autoscaler Operator for Red Hat OpenShift describe new features and enhancements, deprecated features, and known issues.
			</p><p>
				The Custom Metrics Autoscaler Operator uses the Kubernetes-based Event Driven Autoscaler (KEDA) and is built on top of the OpenShift Container Platform horizontal pod autoscaler (HPA).
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The Custom Metrics Autoscaler Operator for Red Hat OpenShift is provided as an installable component, with a distinct release cycle from the core OpenShift Container Platform. The <a class="link" href="https://access.redhat.com/support/policy/updates/openshift#cma">Red Hat OpenShift Container Platform Life Cycle Policy</a> outlines release compatibility.
				</p></div></div><section class="section" id="nodes-pods-autoscaling-custom-rn-versions_nodes-cma-autoscaling-custom-removing"><div class="titlepage"><div><div><h3 class="title">3.2.1. Supported versions</h3></div></div></div><p>
					The following table defines the Custom Metrics Autoscaler Operator versions for each OpenShift Container Platform version.
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 23%; " class="col_1"><!--Empty--></col><col style="width: 54%; " class="col_2"><!--Empty--></col><col style="width: 23%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232242881712" scope="col">Version</th><th align="left" valign="top" id="idm140232242880736" scope="col">OpenShift Container Platform version</th><th align="left" valign="top" id="idm140232242879632" scope="col">General availability</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232242881712"> <p>
									2.10.1-267
								</p>
								 </td><td align="left" valign="top" headers="idm140232242880736"> <p>
									4.13
								</p>
								 </td><td align="left" valign="top" headers="idm140232242879632"> <p>
									General availability
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232242881712"> <p>
									2.10.1-267
								</p>
								 </td><td align="left" valign="top" headers="idm140232242880736"> <p>
									4.12
								</p>
								 </td><td align="left" valign="top" headers="idm140232242879632"> <p>
									General availability
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232242881712"> <p>
									2.10.1-267
								</p>
								 </td><td align="left" valign="top" headers="idm140232242880736"> <p>
									4.11
								</p>
								 </td><td align="left" valign="top" headers="idm140232242879632"> <p>
									General availability
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232242881712"> <p>
									2.10.1-267
								</p>
								 </td><td align="left" valign="top" headers="idm140232242880736"> <p>
									4.10
								</p>
								 </td><td align="left" valign="top" headers="idm140232242879632"> <p>
									General availability
								</p>
								 </td></tr></tbody></table></div></section><section class="section" id="nodes-pods-autoscaling-custom-rn-210-267_nodes-cma-autoscaling-custom-removing"><div class="titlepage"><div><div><h3 class="title">3.2.2. Custom Metrics Autoscaler Operator 2.10.1-267 release notes</h3></div></div></div><p>
					This release of the Custom Metrics Autoscaler Operator 2.10.1-267 provides new features and bug fixes for running the Operator in an OpenShift Container Platform cluster. The components of the Custom Metrics Autoscaler Operator 2.10.1-267 were released in <a class="link" href="https://access.redhat.com/errata/RHBA-2023:4089">RHBA-2023:4089</a>.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Before installing this version of the Custom Metrics Autoscaler Operator, remove any previously installed Technology Preview versions or the community-supported version of KEDA.
					</p></div></div><section class="section" id="nodes-pods-autoscaling-custom-rn-210-267-bugs_nodes-cma-autoscaling-custom-removing"><div class="titlepage"><div><div><h4 class="title">3.2.2.1. Bug fixes</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Previously, the <code class="literal">custom-metrics-autoscaler</code> and <code class="literal">custom-metrics-autoscaler-adapter</code> images did not contain time zone information. Because of this, scaled objects with cron triggers failed to work because the controllers were unable to find time zone information. With this fix, the image builds now include time zone information. As a result, scaled objects containing cron triggers now function properly. (<a class="link" href="https://issues.redhat.com/browse/OCPBUGS-15264"><span class="strong strong"><strong>OCPBUGS-15264</strong></span></a>)
							</li><li class="listitem">
								Previously, the Custom Metrics Autoscaler Operator would attempt to take ownership of all managed objects, including objects in other namespaces and cluster-scoped objects. Because of this, the Custom Metrics Autoscaler Operator was unable to create the role binding for reading the credentials necessary to be an API server. This caused errors in the <code class="literal">kube-system</code> namespace. With this fix, the Custom Metrics Autoscaler Operator skips adding the <code class="literal">ownerReference</code> field to any object in another namespace or any cluster-scoped object. As a result, the role binding is now created without any errors. (<a class="link" href="https://issues.redhat.com/browse/OCPBUGS-15038"><span class="strong strong"><strong>OCPBUGS-15038</strong></span></a>)
							</li><li class="listitem">
								Previously, the Custom Metrics Autoscaler Operator added an <code class="literal">ownerReferences</code> field to the <code class="literal">openshift-keda</code> namespace. While this did not cause functionality problems, the presence of this field could have caused confusion for cluster administrators. With this fix, the Custom Metrics Autoscaler Operator does not add the <code class="literal">ownerReference</code> field to the <code class="literal">openshift-keda</code> namespace. As a result, the <code class="literal">openshift-keda</code> namespace no longer has a superfluous <code class="literal">ownerReference</code> field. (<a class="link" href="https://issues.redhat.com/browse/OCPBUGS-15293"><span class="strong strong"><strong>OCPBUGS-15293</strong></span></a>)
							</li><li class="listitem">
								Previously, if you used a Prometheus trigger configured with authentication method other than pod identity, and the <code class="literal">podIdentity</code> parameter was set to <code class="literal">none</code>, the trigger would fail to scale. With this fix, the Custom Metrics Autoscaler for OpenShift now properly handles the <code class="literal">none</code> pod identity provider type. As a result, a Prometheus trigger configured with authentication method other than pod identity, and the <code class="literal">podIdentity</code> parameter sset to <code class="literal">none</code> now properly scales. (<a class="link" href="https://issues.redhat.com/browse/OCPBUGS-15274"><span class="strong strong"><strong>OCPBUGS-15274</strong></span></a>)
							</li></ul></div></section></section><section class="section" id="nodes-pods-autoscaling-custom-rn-210_nodes-cma-autoscaling-custom-removing"><div class="titlepage"><div><div><h3 class="title">3.2.3. Custom Metrics Autoscaler Operator 2.10.1 release notes</h3></div></div></div><p>
					This release of the Custom Metrics Autoscaler Operator 2.10.1 provides new features and bug fixes for running the Operator in an OpenShift Container Platform cluster. The components of the Custom Metrics Autoscaler Operator 2.10.1 were released in <a class="link" href="https://access.redhat.com/errata/RHEA-2023:3199">RHEA-2023:3199</a>.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Before installing this version of the Custom Metrics Autoscaler Operator, remove any previously installed Technology Preview versions or the community-supported version of KEDA.
					</p></div></div><section class="section" id="nodes-pods-autoscaling-custom-rn-210-new_nodes-cma-autoscaling-custom-removing"><div class="titlepage"><div><div><h4 class="title">3.2.3.1. New features and enhancements</h4></div></div></div><section class="section" id="nodes-pods-autoscaling-custom-rn-210-ga_nodes-cma-autoscaling-custom-removing"><div class="titlepage"><div><div><h5 class="title">3.2.3.1.1. Custom Metrics Autoscaler Operator general availability</h5></div></div></div><p>
							The Custom Metrics Autoscaler Operator is now generally available as of Custom Metrics Autoscaler Operator version 2.10.1.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Scaling by using a scaled job is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
							</p><p>
								For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
							</p></div></div></section><section class="section" id="nodes-pods-autoscaling-custom-rn-210-metrics_nodes-cma-autoscaling-custom-removing"><div class="titlepage"><div><div><h5 class="title">3.2.3.1.2. Performance metrics</h5></div></div></div><p>
							You can now use the Prometheus Query Language (PromQL) to query metrics on the Custom Metrics Autoscaler Operator.
						</p></section><section class="section" id="nodes-pods-autoscaling-custom-rn-210-pause_nodes-cma-autoscaling-custom-removing"><div class="titlepage"><div><div><h5 class="title">3.2.3.1.3. Pausing the custom metrics autoscaling for scaled objects</h5></div></div></div><p>
							You can now pause the autoscaling of a scaled object, as needed, and resume autoscaling when ready.
						</p></section><section class="section" id="nodes-pods-autoscaling-custom-rn-210-fall-back_nodes-cma-autoscaling-custom-removing"><div class="titlepage"><div><div><h5 class="title">3.2.3.1.4. Replica fall back for scaled objects</h5></div></div></div><p>
							You can now specify the number of replicas to fall back to if a scaled object fails to get metrics from the source.
						</p></section><section class="section" id="nodes-pods-autoscaling-custom-rn-210-hpa-name_nodes-cma-autoscaling-custom-removing"><div class="titlepage"><div><div><h5 class="title">3.2.3.1.5. Customizable HPA naming for scaled objects</h5></div></div></div><p>
							You can now specify a custom name for the horizontal pod autoscaler in scaled objects.
						</p></section><section class="section" id="nodes-pods-autoscaling-custom-rn-210-activation_nodes-cma-autoscaling-custom-removing"><div class="titlepage"><div><div><h5 class="title">3.2.3.1.6. Activation and scaling thresholds</h5></div></div></div><p>
							Because the horizontal pod autoscaler (HPA) cannot scale to or from 0 replicas, the Custom Metrics Autoscaler Operator does that scaling, after which the HPA performs the scaling. You can now specify when the HPA takes over autoscaling, based on the number of replicas. This allows for more flexibility with your scaling policies.
						</p></section></section></section><section class="section" id="nodes-cma-autoscaling-custom-rn-282-174_nodes-cma-autoscaling-custom-removing"><div class="titlepage"><div><div><h3 class="title">3.2.4. Custom Metrics Autoscaler Operator 2.8.2-174 release notes</h3></div></div></div><p>
					This release of the Custom Metrics Autoscaler Operator 2.8.2-174 provides new features and bug fixes for running the Operator in an OpenShift Container Platform cluster. The components of the Custom Metrics Autoscaler Operator 2.8.2-174 were released in <a class="link" href="https://access.redhat.com/errata/RHEA-2023:1683">RHEA-2023:1683</a>.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The Custom Metrics Autoscaler Operator version 2.8.2-174 is a <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview</a> feature.
					</p></div></div><section class="section" id="nodes-cma-autoscaling-custom-rn-282-174-new_nodes-cma-autoscaling-custom-removing"><div class="titlepage"><div><div><h4 class="title">3.2.4.1. New features and enhancements</h4></div></div></div><section class="section" id="autoscaling-custom-2-8-2-upgrade-operator"><div class="titlepage"><div><div><h5 class="title">3.2.4.1.1. Operator upgrade support</h5></div></div></div><p>
							You can now upgrade from a prior version of the Custom Metrics Autoscaler Operator. See "Changing the update channel for an Operator" in the "Additional resources" for information on upgrading an Operator.
						</p></section><section class="section" id="autoscaling-custom-2-8-2-must-gather"><div class="titlepage"><div><div><h5 class="title">3.2.4.1.2. must-gather support</h5></div></div></div><p>
							You can now collect data about the Custom Metrics Autoscaler Operator and its components by using the OpenShift Container Platform <code class="literal">must-gather</code> tool. Currently, the process for using the <code class="literal">must-gather</code> tool with the Custom Metrics Autoscaler is different than for other operators. See "Gathering debugging data in the "Additional resources" for more information.
						</p></section></section></section><section class="section" id="nodes-cma-autoscaling-custom-rn-282_nodes-cma-autoscaling-custom-removing"><div class="titlepage"><div><div><h3 class="title">3.2.5. Custom Metrics Autoscaler Operator 2.8.2 release notes</h3></div></div></div><p>
					This release of the Custom Metrics Autoscaler Operator 2.8.2 provides new features and bug fixes for running the Operator in an OpenShift Container Platform cluster. The components of the Custom Metrics Autoscaler Operator 2.8.2 were released in <a class="link" href="https://access.redhat.com/errata/RHSA-2023:1042">RHSA-2023:1042</a>.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The Custom Metrics Autoscaler Operator version 2.8.2 is a <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview</a> feature.
					</p></div></div><section class="section" id="nodes-cma-autoscaling-custom-rn-282-new_nodes-cma-autoscaling-custom-removing"><div class="titlepage"><div><div><h4 class="title">3.2.5.1. New features and enhancements</h4></div></div></div><section class="section" id="autoscaling-custom-2-8-2-audit-log"><div class="titlepage"><div><div><h5 class="title">3.2.5.1.1. Audit Logging</h5></div></div></div><p>
							You can now gather and view audit logs for the Custom Metrics Autoscaler Operator and its associated components. Audit logs are security-relevant chronological sets of records that document the sequence of activities that have affected the system by individual users, administrators, or other components of the system.
						</p></section><section class="section" id="autoscaling-custom-2-8-2-kafka-metrics"><div class="titlepage"><div><div><h5 class="title">3.2.5.1.2. Scale applications based on Apache Kafka metrics</h5></div></div></div><p>
							You can now use the KEDA Apache kafka trigger/scaler to scale deployments based on an Apache Kafka topic.
						</p></section><section class="section" id="autoscaling-custom-2-8-2-cpu-metrics"><div class="titlepage"><div><div><h5 class="title">3.2.5.1.3. Scale applications based on CPU metrics</h5></div></div></div><p>
							You can now use the KEDA CPU trigger/scaler to scale deployments based on CPU metrics.
						</p></section><section class="section" id="autoscaling-custom-2-8-2-memory-metrics"><div class="titlepage"><div><div><h5 class="title">3.2.5.1.4. Scale applications based on memory metrics</h5></div></div></div><p>
							You can now use the KEDA memory trigger/scaler to scale deployments based on memory metrics.
						</p></section></section></section></section><section class="section" id="nodes-cma-autoscaling-custom-install"><div class="titlepage"><div><div><h2 class="title">3.3. Installing the custom metrics autoscaler</h2></div></div></div><p>
				You can use the OpenShift Container Platform web console to install the Custom Metrics Autoscaler Operator.
			</p><p>
				The installation creates the following five CRDs:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">ClusterTriggerAuthentication</code>
					</li><li class="listitem">
						<code class="literal">KedaController</code>
					</li><li class="listitem">
						<code class="literal">ScaledJob</code>
					</li><li class="listitem">
						<code class="literal">ScaledObject</code>
					</li><li class="listitem">
						<code class="literal">TriggerAuthentication</code>
					</li></ul></div><section class="section" id="nodes-cma-autoscaling-custom-install_nodes-cma-autoscaling-custom-install"><div class="titlepage"><div><div><h3 class="title">3.3.1. Installing the custom metrics autoscaler</h3></div></div></div><p>
					You can use the following procedure to install the Custom Metrics Autoscaler Operator.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Remove any previously-installed Technology Preview versions of the Cluster Metrics Autoscaler Operator.
						</li><li class="listitem"><p class="simpara">
							Remove any versions of the community-based KEDA.
						</p><p class="simpara">
							Also, remove the KEDA 1.x custom resource definitions by running the following commands:
						</p><pre class="programlisting language-terminal">$ oc delete crd scaledobjects.keda.k8s.io</pre><pre class="programlisting language-terminal">$ oc delete crd triggerauthentications.keda.k8s.io</pre></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the OpenShift Container Platform web console, click <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span>.
						</li><li class="listitem">
							Choose <span class="strong strong"><strong>Custom Metrics Autoscaler</strong></span> from the list of available Operators, and click <span class="strong strong"><strong>Install</strong></span>.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>Install Operator</strong></span> page, ensure that the <span class="strong strong"><strong>All namespaces on the cluster (default)</strong></span> option is selected for <span class="strong strong"><strong>Installation Mode</strong></span>. This installs the Operator in all namespaces.
						</li><li class="listitem">
							Ensure that the <span class="strong strong"><strong>openshift-keda</strong></span> namespace is selected for <span class="strong strong"><strong>Installed Namespace</strong></span>. OpenShift Container Platform creates the namespace, if not present in your cluster.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Install</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Verify the installation by listing the Custom Metrics Autoscaler Operator components:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Pods</strong></span>.
								</li><li class="listitem">
									Select the <code class="literal">openshift-keda</code> project from the drop-down menu and verify that the <code class="literal">custom-metrics-autoscaler-operator-*</code> pod is running.
								</li><li class="listitem">
									Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Deployments</strong></span> to verify that the <code class="literal">custom-metrics-autoscaler-operator</code> deployment is running.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Optional: Verify the installation in the OpenShift CLI using the following commands:
						</p><pre class="programlisting language-terminal">$ oc get all -n openshift-keda</pre><p class="simpara">
							The output appears similar to the following:
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                      READY   STATUS    RESTARTS   AGE
pod/custom-metrics-autoscaler-operator-5fd8d9ffd8-xt4xp   1/1     Running   0          18m

NAME                                                 READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/custom-metrics-autoscaler-operator   1/1     1            1           18m

NAME                                                            DESIRED   CURRENT   READY   AGE
replicaset.apps/custom-metrics-autoscaler-operator-5fd8d9ffd8   1         1         1       18m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Install the <code class="literal">KedaController</code> custom resource, which creates the required CRDs:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									In the OpenShift Container Platform web console, click <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
								</li><li class="listitem">
									Click <span class="strong strong"><strong>Custom Metrics Autoscaler</strong></span>.
								</li><li class="listitem">
									On the <span class="strong strong"><strong>Operator Details</strong></span> page, click the <span class="strong strong"><strong>KedaController</strong></span> tab.
								</li><li class="listitem"><p class="simpara">
									On the <span class="strong strong"><strong>KedaController</strong></span> tab, click <span class="strong strong"><strong>Create KedaController</strong></span> and edit the file.
								</p><pre class="programlisting language-yaml">kind: KedaController
apiVersion: keda.sh/v1alpha1
metadata:
  name: keda
  namespace: openshift-keda
spec:
  watchNamespace: '' <span id="CO54-1"><!--Empty--></span><span class="callout">1</span>
  operator:
    logLevel: info <span id="CO54-2"><!--Empty--></span><span class="callout">2</span>
    logEncoder: console <span id="CO54-3"><!--Empty--></span><span class="callout">3</span>
  metricsServer:
    logLevel: '0' <span id="CO54-4"><!--Empty--></span><span class="callout">4</span>
    auditConfig: <span id="CO54-5"><!--Empty--></span><span class="callout">5</span>
      logFormat: "json"
      logOutputVolumeClaim: "persistentVolumeClaimName"
      policy:
        rules:
        - level: Metadata
        omitStages: "RequestReceived"
        omitManagedFields: false
      lifetime:
        maxAge: "2"
        maxBackup: "1"
        maxSize: "50"
  serviceAccount: {}</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO54-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specifies the namespaces that the custom autoscaler should watch. Enter names in a comma-separated list. Omit or set empty to watch all namespaces. The default is empty.
										</div></dd><dt><a href="#CO54-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Specifies the level of verbosity for the Custom Metrics Autoscaler Operator log messages. The allowed values are <code class="literal">debug</code>, <code class="literal">info</code>, <code class="literal">error</code>. The default is <code class="literal">info</code>.
										</div></dd><dt><a href="#CO54-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Specifies the logging format for the Custom Metrics Autoscaler Operator log messages. The allowed values are <code class="literal">console</code> or <code class="literal">json</code>. The default is <code class="literal">console</code>.
										</div></dd><dt><a href="#CO54-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Specifies the logging level for the Custom Metrics Autoscaler Metrics Server. The allowed values are <code class="literal">0</code> for <code class="literal">info</code> and <code class="literal">4</code> or <code class="literal">debug</code>. The default is <code class="literal">0</code>.
										</div></dd><dt><a href="#CO54-5"><span class="callout">5</span></a> </dt><dd><div class="para">
											Activates audit logging for the Custom Metrics Autoscaler Operator and specifies the audit policy to use, as described in the "Configuring audit logging" section.
										</div></dd></dl></div></li><li class="listitem">
									Click <span class="strong strong"><strong>Create</strong></span> to create the KEDAController.
								</li></ol></div></li></ol></div></section></section><section class="section" id="nodes-cma-autoscaling-custom-overview-trigger"><div class="titlepage"><div><div><h2 class="title">3.4. Understanding the custom metrics autoscaler triggers</h2></div></div></div><p>
				Triggers, also known as scalers, provide the metrics that the Custom Metrics Autoscaler Operator uses to scale your pods.
			</p><p>
				The custom metrics autoscaler currently supports only the Prometheus, CPU, memory, and Apache Kafka triggers.
			</p><p>
				You use a <code class="literal">ScaledObject</code> or <code class="literal">ScaledJob</code> custom resource to configure triggers for specific objects, as described in the sections that follow.
			</p><section class="section" id="nodes-cma-autoscaling-custom-trigger-prom_nodes-cma-autoscaling-custom-trigger"><div class="titlepage"><div><div><h3 class="title">3.4.1. Understanding the Prometheus trigger</h3></div></div></div><p>
					You can scale pods based on Prometheus metrics, which can use the installed OpenShift Container Platform monitoring or an external Prometheus server as the metrics source. See "Additional resources" for information on the configurations required to use the OpenShift Container Platform monitoring as a source for metrics.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If Prometheus is collecting metrics from the application that the custom metrics autoscaler is scaling, do not set the minimum replicas to <code class="literal">0</code> in the custom resource. If there are no application pods, the custom metrics autoscaler does not have any metrics to scale on.
					</p></div></div><div class="formalpara"><p class="title"><strong>Example scaled object with a Prometheus target</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: prom-scaledobject
  namespace: my-namespace
spec:
# ...
  triggers:
  - type: prometheus <span id="CO55-1"><!--Empty--></span><span class="callout">1</span>
    metadata:
      serverAddress: https://thanos-querier.openshift-monitoring.svc.cluster.local:9092 <span id="CO55-2"><!--Empty--></span><span class="callout">2</span>
      namespace: kedatest <span id="CO55-3"><!--Empty--></span><span class="callout">3</span>
      metricName: http_requests_total <span id="CO55-4"><!--Empty--></span><span class="callout">4</span>
      threshold: '5' <span id="CO55-5"><!--Empty--></span><span class="callout">5</span>
      query: sum(rate(http_requests_total{job="test-app"}[1m])) <span id="CO55-6"><!--Empty--></span><span class="callout">6</span>
      authModes: basic <span id="CO55-7"><!--Empty--></span><span class="callout">7</span>
      cortexOrgID: my-org <span id="CO55-8"><!--Empty--></span><span class="callout">8</span>
      ignoreNullValues: false <span id="CO55-9"><!--Empty--></span><span class="callout">9</span>
      unsafeSsl: false <span id="CO55-10"><!--Empty--></span><span class="callout">10</span></pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO55-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Specifies Prometheus as the trigger type.
						</div></dd><dt><a href="#CO55-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Specifies the address of the Prometheus server. This example uses OpenShift Container Platform monitoring.
						</div></dd><dt><a href="#CO55-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Optional: Specifies the namespace of the object you want to scale. This parameter is mandatory if using OpenShift Container Platform monitoring as a source for the metrics.
						</div></dd><dt><a href="#CO55-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Specifies the name to identify the metric in the <code class="literal">external.metrics.k8s.io</code> API. If you are using more than one trigger, all metric names must be unique.
						</div></dd><dt><a href="#CO55-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Specifies the value that triggers scaling. Must be specified as a quoted string value.
						</div></dd><dt><a href="#CO55-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							Specifies the Prometheus query to use.
						</div></dd><dt><a href="#CO55-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							Specifies the authentication method to use. Prometheus scalers support bearer authentication (<code class="literal">bearer</code>), basic authentication (<code class="literal">basic</code>), or TLS authentication (<code class="literal">tls</code>). You configure the specific authentication parameters in a trigger authentication, as discussed in a following section. As needed, you can also use a secret.
						</div></dd><dt><a href="#CO55-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							Optional: Passes the <code class="literal">X-Scope-OrgID</code> header to multi-tenant <a class="link" href="https://cortexmetrics.io/">Cortex</a> or <a class="link" href="https://grafana.com/oss/mimir/">Mimir</a> storage for Prometheus. This parameter is required only with multi-tenant Prometheus storage, to indicate which data Prometheus should return.
						</div></dd><dt><a href="#CO55-9"><span class="callout">9</span></a> </dt><dd><div class="para">
							Optional: Specifies how the trigger should proceed if the Prometheus target is lost.
						</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									If <code class="literal">true</code>, the trigger continues to operate if the Prometheus target is lost. This is the default behavior.
								</li><li class="listitem">
									If <code class="literal">false</code>, the trigger returns an error if the Prometheus target is lost.
								</li></ul></div></dd><dt><a href="#CO55-10"><span class="callout">10</span></a> </dt><dd><div class="para">
							Optional: Specifies whether the certificate check should be skipped. For example, you might skip the check if you use self-signed certificates at the Prometheus endpoint.
						</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									If <code class="literal">true</code>, the certificate check is performed.
								</li><li class="listitem">
									If <code class="literal">false</code>, the certificate check is not performed. This is the default behavior.
								</li></ul></div></dd></dl></div><section class="section" id="nodes-cma-autoscaling-custom-prometheus-config_nodes-cma-autoscaling-custom-trigger"><div class="titlepage"><div><div><h4 class="title">3.4.1.1. Configuring the custom metrics autoscaler to use OpenShift Container Platform monitoring</h4></div></div></div><p>
						You can use the installed OpenShift Container Platform Prometheus monitoring as a source for the metrics used by the custom metrics autoscaler. However, there are some additional configurations you must perform.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							These steps are not required for an external Prometheus source.
						</p></div></div><p>
						You must perform the following tasks, as described in this section:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Create a service account to get a token.
							</li><li class="listitem">
								Create a role.
							</li><li class="listitem">
								Add that role to the service account.
							</li><li class="listitem">
								Reference the token in the trigger authentication object used by Prometheus.
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								OpenShift Container Platform monitoring must be installed.
							</li><li class="listitem">
								Monitoring of user-defined workloads must be enabled in OpenShift Container Platform monitoring, as described in the <span class="strong strong"><strong>Creating a user-defined workload monitoring config map</strong></span> section.
							</li><li class="listitem">
								The Custom Metrics Autoscaler Operator must be installed.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Change to the project with the object you want to scale:
							</p><pre class="programlisting language-terminal">$ oc project my-project</pre></li><li class="listitem"><p class="simpara">
								Use the following command to create a service account, if your cluster does not have one:
							</p><pre class="programlisting language-terminal">$ oc create serviceaccount &lt;service_account&gt;</pre><p class="simpara">
								where:
							</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;service_account&gt;</span></dt><dd>
											Specifies the name of the service account.
										</dd></dl></div></li><li class="listitem"><p class="simpara">
								Use the following command to locate the token assigned to the service account:
							</p><pre class="programlisting language-terminal">$ oc describe serviceaccount &lt;service_account&gt;</pre><p class="simpara">
								where:
							</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;service_account&gt;</span></dt><dd>
											Specifies the name of the service account.
										</dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Name:                thanos
Namespace:           my-project
Labels:              &lt;none&gt;
Annotations:         &lt;none&gt;
Image pull secrets:  thanos-dockercfg-nnwgj
Mountable secrets:   thanos-dockercfg-nnwgj
Tokens:              thanos-token-9g4n5 <span id="CO56-1"><!--Empty--></span><span class="callout">1</span>
Events:              &lt;none&gt;</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO56-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Use this token in the trigger authentication.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create a trigger authentication with the service account token:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Create a YAML file similar to the following:
									</p><pre class="programlisting language-yaml">apiVersion: keda.sh/v1alpha1
kind: TriggerAuthentication
metadata:
  name: keda-trigger-auth-prometheus
spec:
  secretTargetRef: <span id="CO57-1"><!--Empty--></span><span class="callout">1</span>
  - parameter: bearerToken <span id="CO57-2"><!--Empty--></span><span class="callout">2</span>
    name: thanos-token-9g4n5 <span id="CO57-3"><!--Empty--></span><span class="callout">3</span>
    key: token <span id="CO57-4"><!--Empty--></span><span class="callout">4</span>
  - parameter: ca
    name: thanos-token-9g4n5
    key: ca.crt</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO57-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Specifies that this object uses a secret for authorization.
											</div></dd><dt><a href="#CO57-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Specifies the authentication parameter to supply by using the token.
											</div></dd><dt><a href="#CO57-3"><span class="callout">3</span></a> </dt><dd><div class="para">
												Specifies the name of the token to use.
											</div></dd><dt><a href="#CO57-4"><span class="callout">4</span></a> </dt><dd><div class="para">
												Specifies the key in the token to use with the specified parameter.
											</div></dd></dl></div></li><li class="listitem"><p class="simpara">
										Create the CR object:
									</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Create a role for reading Thanos metrics:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Create a YAML file with the following parameters:
									</p><pre class="programlisting language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: thanos-metrics-reader
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
- apiGroups:
  - metrics.k8s.io
  resources:
  - pods
  - nodes
  verbs:
  - get
  - list
  - watch</pre></li><li class="listitem"><p class="simpara">
										Create the CR object:
									</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Create a role binding for reading Thanos metrics:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Create a YAML file similar to the following:
									</p><pre class="programlisting language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: thanos-metrics-reader <span id="CO58-1"><!--Empty--></span><span class="callout">1</span>
  namespace: my-project <span id="CO58-2"><!--Empty--></span><span class="callout">2</span>
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: thanos-metrics-reader
subjects:
- kind: ServiceAccount
  name: thanos <span id="CO58-3"><!--Empty--></span><span class="callout">3</span>
  namespace: my-project <span id="CO58-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO58-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Specifies the name of the role you created.
											</div></dd><dt><a href="#CO58-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Specifies the namespace of the object you want to scale.
											</div></dd><dt><a href="#CO58-3"><span class="callout">3</span></a> </dt><dd><div class="para">
												Specifies the name of the service account to bind to the role.
											</div></dd><dt><a href="#CO58-4"><span class="callout">4</span></a> </dt><dd><div class="para">
												Specifies the namespace of the object you want to scale.
											</div></dd></dl></div></li><li class="listitem"><p class="simpara">
										Create the CR object:
									</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre></li></ol></div></li></ol></div><p>
						You can now deploy a scaled object or scaled job to enable autoscaling for your application, as described in "Understanding how to add custom metrics autoscalers". To use OpenShift Container Platform monitoring as the source, in the trigger, or scaler, you must include the following parameters:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<code class="literal">triggers.type</code> must be <code class="literal">prometheus</code>
							</li><li class="listitem">
								<code class="literal">triggers.metadata.serverAddress</code> must be <code class="literal">https://thanos-querier.openshift-monitoring.svc.cluster.local:9092</code>
							</li><li class="listitem">
								<code class="literal">triggers.metadata.authModes</code> must be <code class="literal">bearer</code>
							</li><li class="listitem">
								<code class="literal">triggers.metadata.namespace</code> must be set to the namespace of the object to scale
							</li><li class="listitem">
								<code class="literal">triggers.authenticationRef</code> must point to the trigger authentication resource specified in the previous step
							</li></ul></div></section></section><section class="section" id="nodes-cma-autoscaling-custom-trigger-cpu_nodes-cma-autoscaling-custom-trigger"><div class="titlepage"><div><div><h3 class="title">3.4.2. Understanding the CPU trigger</h3></div></div></div><p>
					You can scale pods based on CPU metrics. This trigger uses cluster metrics as the source for metrics.
				</p><p>
					The custom metrics autoscaler scales the pods associated with an object to maintain the CPU usage that you specify. The autoscaler increases or decreases the number of replicas between the minimum and maximum numbers to maintain the specified CPU utilization across all pods. The memory trigger considers the memory utilization of the entire pod. If the pod has multiple containers, the memory trigger considers the total memory utilization of all containers in the pod.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								This trigger cannot be used with the <code class="literal">ScaledJob</code> custom resource.
							</li><li class="listitem">
								When using a memory trigger to scale an object, the object does not scale to <code class="literal">0</code>, even if you are using multiple triggers.
							</li></ul></div></div></div><div class="formalpara"><p class="title"><strong>Example scaled object with a CPU target</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: cpu-scaledobject
  namespace: my-namespace
spec:
# ...
  triggers:
  - type: cpu <span id="CO59-1"><!--Empty--></span><span class="callout">1</span>
    metricType: Utilization <span id="CO59-2"><!--Empty--></span><span class="callout">2</span>
    metadata:
      value: '60' <span id="CO59-3"><!--Empty--></span><span class="callout">3</span>
      containerName: api <span id="CO59-4"><!--Empty--></span><span class="callout">4</span></pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO59-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Specifies CPU as the trigger type.
						</div></dd><dt><a href="#CO59-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Specifies the type of metric to use, either <code class="literal">Utilization</code> or <code class="literal">AverageValue</code>.
						</div></dd><dt><a href="#CO59-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Specifies the value that triggers scaling. Must be specified as a quoted string value.
						</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									When using <code class="literal">Utilization</code>, the target value is the average of the resource metrics across all relevant pods, represented as a percentage of the requested value of the resource for the pods.
								</li><li class="listitem">
									When using <code class="literal">AverageValue</code>, the target value is the average of the metrics across all relevant pods.
								</li></ul></div></dd><dt><a href="#CO59-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Optional: Specifies an individual container to scale, based on the memory utilization of only that container, rather than the entire pod. In this example, only the container named <code class="literal">api</code> is to be scaled.
						</div></dd></dl></div></section><section class="section" id="nodes-cma-autoscaling-custom-trigger-memory_nodes-cma-autoscaling-custom-trigger"><div class="titlepage"><div><div><h3 class="title">3.4.3. Understanding the memory trigger</h3></div></div></div><p>
					You can scale pods based on memory metrics. This trigger uses cluster metrics as the source for metrics.
				</p><p>
					The custom metrics autoscaler scales the pods associated with an object to maintain the average memory usage that you specify. The autoscaler increases and decreases the number of replicas between the minimum and maximum numbers to maintain the specified memory utilization across all pods. The memory trigger considers the memory utilization of entire pod. If the pod has multiple containers, the memory utilization is the sum of all of the containers.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								This trigger cannot be used with the <code class="literal">ScaledJob</code> custom resource.
							</li><li class="listitem">
								When using a memory trigger to scale an object, the object does not scale to <code class="literal">0</code>, even if you are using multiple triggers.
							</li></ul></div></div></div><div class="formalpara"><p class="title"><strong>Example scaled object with a memory target</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: memory-scaledobject
  namespace: my-namespace
spec:
# ...
  triggers:
  - type: memory <span id="CO60-1"><!--Empty--></span><span class="callout">1</span>
    metricType: Utilization <span id="CO60-2"><!--Empty--></span><span class="callout">2</span>
    metadata:
      value: '60' <span id="CO60-3"><!--Empty--></span><span class="callout">3</span>
      containerName: api <span id="CO60-4"><!--Empty--></span><span class="callout">4</span></pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO60-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Specifies memory as the trigger type.
						</div></dd><dt><a href="#CO60-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Specifies the type of metric to use, either <code class="literal">Utilization</code> or <code class="literal">AverageValue</code>.
						</div></dd><dt><a href="#CO60-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Specifies the value that triggers scaling. Must be specified as a quoted string value.
						</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									When using <code class="literal">Utilization</code>, the target value is the average of the resource metrics across all relevant pods, represented as a percentage of the requested value of the resource for the pods.
								</li><li class="listitem">
									When using <code class="literal">AverageValue</code>, the target value is the average of the metrics across all relevant pods.
								</li></ul></div></dd><dt><a href="#CO60-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Optional: Specifies an individual container to scale, based on the memory utilization of only that container, rather than the entire pod. In this example, only the container named <code class="literal">api</code> is to be scaled.
						</div></dd></dl></div></section><section class="section" id="nodes-cma-autoscaling-custom-trigger-kafka_nodes-cma-autoscaling-custom-trigger"><div class="titlepage"><div><div><h3 class="title">3.4.4. Understanding the Kafka trigger</h3></div></div></div><p>
					You can scale pods based on an Apache Kafka topic or other services that support the Kafka protocol. The custom metrics autoscaler does not scale higher than the number of Kafka partitions, unless you set the <code class="literal">allowIdleConsumers</code> parameter to <code class="literal">true</code> in the scaled object or scaled job.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If the number of consumer groups exceeds the number of partitions in a topic, the extra consumer groups remain idle. To avoid this, by default the number of replicas does not exceed:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								The number of partitions on a topic, if a topic is specified
							</li><li class="listitem">
								The number of partitions of all topics in the consumer group, if no topic is specified
							</li><li class="listitem">
								The <code class="literal">maxReplicaCount</code> specified in scaled object or scaled job CR
							</li></ul></div><p>
						You can use the <code class="literal">allowIdleConsumers</code> parameter to disable these default behaviors.
					</p></div></div><div class="formalpara"><p class="title"><strong>Example scaled object with a Kafka target</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: kafka-scaledobject
  namespace: my-namespace
spec:
# ...
  triggers:
  - type: kafka <span id="CO61-1"><!--Empty--></span><span class="callout">1</span>
    metadata:
      topic: my-topic <span id="CO61-2"><!--Empty--></span><span class="callout">2</span>
      bootstrapServers: my-cluster-kafka-bootstrap.openshift-operators.svc:9092 <span id="CO61-3"><!--Empty--></span><span class="callout">3</span>
      consumerGroup: my-group <span id="CO61-4"><!--Empty--></span><span class="callout">4</span>
      lagThreshold: '10' <span id="CO61-5"><!--Empty--></span><span class="callout">5</span>
      activationLagThreshold: '5' <span id="CO61-6"><!--Empty--></span><span class="callout">6</span>
      offsetResetPolicy: latest <span id="CO61-7"><!--Empty--></span><span class="callout">7</span>
      allowIdleConsumers: true <span id="CO61-8"><!--Empty--></span><span class="callout">8</span>
      scaleToZeroOnInvalidOffset: false <span id="CO61-9"><!--Empty--></span><span class="callout">9</span>
      excludePersistentLag: false <span id="CO61-10"><!--Empty--></span><span class="callout">10</span>
      version: '1.0.0' <span id="CO61-11"><!--Empty--></span><span class="callout">11</span>
      partitionLimitation: '1,2,10-20,31' <span id="CO61-12"><!--Empty--></span><span class="callout">12</span></pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO61-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Specifies Kafka as the trigger type.
						</div></dd><dt><a href="#CO61-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Specifies the name of the Kafka topic on which Kafka is processing the offset lag.
						</div></dd><dt><a href="#CO61-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Specifies a comma-separated list of Kafka brokers to connect to.
						</div></dd><dt><a href="#CO61-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Specifies the name of the Kafka consumer group used for checking the offset on the topic and processing the related lag.
						</div></dd><dt><a href="#CO61-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Optional: Specifies the average target value that triggers scaling. Must be specified as a quoted string value. The default is <code class="literal">5</code>.
						</div></dd><dt><a href="#CO61-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							Optional: Specifies the target value for the activation phase. Must be specified as a quoted string value.
						</div></dd><dt><a href="#CO61-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							Optional: Specifies the Kafka offset reset policy for the Kafka consumer. The available values are: <code class="literal">latest</code> and <code class="literal">earliest</code>. The default is <code class="literal">latest</code>.
						</div></dd><dt><a href="#CO61-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							Optional: Specifies whether the number of Kafka replicas can exceed the number of partitions on a topic.
						</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									If <code class="literal">true</code>, the number of Kafka replicas can exceed the number of partitions on a topic. This allows for idle Kafka consumers.
								</li><li class="listitem">
									If <code class="literal">false</code>, the number of Kafka replicas cannot exceed the number of partitions on a topic. This is the default.
								</li></ul></div></dd><dt><a href="#CO61-9"><span class="callout">9</span></a> </dt><dd><div class="para">
							Specifies how the trigger behaves when a Kafka partition does not have a valid offset.
						</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									If <code class="literal">true</code>, the consumers are scaled to zero for that partition.
								</li><li class="listitem">
									If <code class="literal">false</code>, the scaler keeps a single consumer for that partition. This is the default.
								</li></ul></div></dd><dt><a href="#CO61-10"><span class="callout">10</span></a> </dt><dd><div class="para">
							Optional: Specifies whether the trigger includes or excludes partition lag for partitions whose current offset is the same as the current offset of the previous polling cycle.
						</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									If <code class="literal">true</code>, the scaler excludes partition lag in these partitions.
								</li><li class="listitem">
									If <code class="literal">false</code>, the trigger includes all consumer lag in all partitions. This is the default.
								</li></ul></div></dd><dt><a href="#CO61-11"><span class="callout">11</span></a> </dt><dd><div class="para">
							Optional: Specifies the version of your Kafka brokers. Must be specified as a quoted string value. The default is <code class="literal">1.0.0</code>.
						</div></dd><dt><a href="#CO61-12"><span class="callout">12</span></a> </dt><dd><div class="para">
							Optional: Specifies a comma-separated list of partition IDs to scope the scaling on. If set, only the listed IDs are considered when calculating lag. Must be specified as a quoted string value. The default is to consider all partitions.
						</div></dd></dl></div></section></section><section class="section" id="nodes-cma-autoscaling-custom-trigger-auth"><div class="titlepage"><div><div><h2 class="title">3.5. Understanding custom metrics autoscaler trigger authentications</h2></div></div></div><p>
				A trigger authentication allows you to include authentication information in a scaled object or a scaled job that can be used by the associated containers. You can use trigger authentications to pass OpenShift Container Platform secrets, platform-native pod authentication mechanisms, environment variables, and so on.
			</p><p>
				You define a <code class="literal">TriggerAuthentication</code> object in the same namespace as the object that you want to scale. That trigger authentication can be used only by objects in that namespace.
			</p><p>
				Alternatively, to share credentials between objects in multiple namespaces, you can create a <code class="literal">ClusterTriggerAuthentication</code> object that can be used across all namespaces.
			</p><p>
				Trigger authentications and cluster trigger authentication use the same configuration. However, a cluster trigger authentication requires an additional <code class="literal">kind</code> parameter in the authentication reference of the scaled object.
			</p><div class="formalpara"><p class="title"><strong>Example trigger authentication with a secret</strong></p><p>
					
<pre class="programlisting language-yaml">kind: TriggerAuthentication
apiVersion: keda.sh/v1alpha1
metadata:
  name: secret-triggerauthentication
  namespace: my-namespace <span id="CO62-1"><!--Empty--></span><span class="callout">1</span>
spec:
  secretTargetRef: <span id="CO62-2"><!--Empty--></span><span class="callout">2</span>
  - parameter: user-name <span id="CO62-3"><!--Empty--></span><span class="callout">3</span>
    name: my-secret <span id="CO62-4"><!--Empty--></span><span class="callout">4</span>
    key: USER_NAME <span id="CO62-5"><!--Empty--></span><span class="callout">5</span>
  - parameter: password
    name: my-secret
    key: USER_PASSWORD</pre>

				</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO62-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						Specifies the namespace of the object you want to scale.
					</div></dd><dt><a href="#CO62-2"><span class="callout">2</span></a> </dt><dd><div class="para">
						Specifies that this trigger authentication uses a secret for authorization.
					</div></dd><dt><a href="#CO62-3"><span class="callout">3</span></a> </dt><dd><div class="para">
						Specifies the authentication parameter to supply by using the secret.
					</div></dd><dt><a href="#CO62-4"><span class="callout">4</span></a> </dt><dd><div class="para">
						Specifies the name of the secret to use.
					</div></dd><dt><a href="#CO62-5"><span class="callout">5</span></a> </dt><dd><div class="para">
						Specifies the key in the secret to use with the specified parameter.
					</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example cluster trigger authentication with a secret</strong></p><p>
					
<pre class="programlisting language-yaml">kind: ClusterTriggerAuthentication
apiVersion: keda.sh/v1alpha1
metadata: <span id="CO63-1"><!--Empty--></span><span class="callout">1</span>
  name: secret-cluster-triggerauthentication
spec:
  secretTargetRef: <span id="CO63-2"><!--Empty--></span><span class="callout">2</span>
  - parameter: user-name <span id="CO63-3"><!--Empty--></span><span class="callout">3</span>
    name: secret-name <span id="CO63-4"><!--Empty--></span><span class="callout">4</span>
    key: USER_NAME <span id="CO63-5"><!--Empty--></span><span class="callout">5</span>
  - parameter: user-password
    name: secret-name
    key: USER_PASSWORD</pre>

				</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO63-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						Note that no namespace is used with a cluster trigger authentication.
					</div></dd><dt><a href="#CO63-2"><span class="callout">2</span></a> </dt><dd><div class="para">
						Specifies that this trigger authentication uses a secret for authorization.
					</div></dd><dt><a href="#CO63-3"><span class="callout">3</span></a> </dt><dd><div class="para">
						Specifies the authentication parameter to supply by using the secret.
					</div></dd><dt><a href="#CO63-4"><span class="callout">4</span></a> </dt><dd><div class="para">
						Specifies the name of the secret to use.
					</div></dd><dt><a href="#CO63-5"><span class="callout">5</span></a> </dt><dd><div class="para">
						Specifies the key in the secret to use with the specified parameter.
					</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example trigger authentication with a token</strong></p><p>
					
<pre class="programlisting language-yaml">kind: TriggerAuthentication
apiVersion: keda.sh/v1alpha1
metadata:
  name: token-triggerauthentication
  namespace: my-namespace <span id="CO64-1"><!--Empty--></span><span class="callout">1</span>
spec:
  secretTargetRef: <span id="CO64-2"><!--Empty--></span><span class="callout">2</span>
  - parameter: bearerToken <span id="CO64-3"><!--Empty--></span><span class="callout">3</span>
    name: my-token-2vzfq <span id="CO64-4"><!--Empty--></span><span class="callout">4</span>
    key: token <span id="CO64-5"><!--Empty--></span><span class="callout">5</span>
  - parameter: ca
    name: my-token-2vzfq
    key: ca.crt</pre>

				</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO64-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						Specifies the namespace of the object you want to scale.
					</div></dd><dt><a href="#CO64-2"><span class="callout">2</span></a> </dt><dd><div class="para">
						Specifies that this trigger authentication uses a secret for authorization.
					</div></dd><dt><a href="#CO64-3"><span class="callout">3</span></a> </dt><dd><div class="para">
						Specifies the authentication parameter to supply by using the token.
					</div></dd><dt><a href="#CO64-4"><span class="callout">4</span></a> </dt><dd><div class="para">
						Specifies the name of the token to use.
					</div></dd><dt><a href="#CO64-5"><span class="callout">5</span></a> </dt><dd><div class="para">
						Specifies the key in the token to use with the specified parameter.
					</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example trigger authentication with an environment variable</strong></p><p>
					
<pre class="programlisting language-yaml">kind: TriggerAuthentication
apiVersion: keda.sh/v1alpha1
metadata:
  name: env-var-triggerauthentication
  namespace: my-namespace <span id="CO65-1"><!--Empty--></span><span class="callout">1</span>
spec:
  env: <span id="CO65-2"><!--Empty--></span><span class="callout">2</span>
  - parameter: access_key <span id="CO65-3"><!--Empty--></span><span class="callout">3</span>
    name: ACCESS_KEY <span id="CO65-4"><!--Empty--></span><span class="callout">4</span>
    containerName: my-container <span id="CO65-5"><!--Empty--></span><span class="callout">5</span></pre>

				</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO65-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						Specifies the namespace of the object you want to scale.
					</div></dd><dt><a href="#CO65-2"><span class="callout">2</span></a> </dt><dd><div class="para">
						Specifies that this trigger authentication uses environment variables for authorization.
					</div></dd><dt><a href="#CO65-3"><span class="callout">3</span></a> </dt><dd><div class="para">
						Specify the parameter to set with this variable.
					</div></dd><dt><a href="#CO65-4"><span class="callout">4</span></a> </dt><dd><div class="para">
						Specify the name of the environment variable.
					</div></dd><dt><a href="#CO65-5"><span class="callout">5</span></a> </dt><dd><div class="para">
						Optional: Specify a container that requires authentication. The container must be in the same resource as referenced by <code class="literal">scaleTargetRef</code> in the scaled object.
					</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example trigger authentication with pod authentication providers</strong></p><p>
					
<pre class="programlisting language-yaml">kind: TriggerAuthentication
apiVersion: keda.sh/v1alpha1
metadata:
  name: pod-id-triggerauthentication
  namespace: my-namespace <span id="CO66-1"><!--Empty--></span><span class="callout">1</span>
spec:
  podIdentity: <span id="CO66-2"><!--Empty--></span><span class="callout">2</span>
    provider: aws-eks <span id="CO66-3"><!--Empty--></span><span class="callout">3</span></pre>

				</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO66-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						Specifies the namespace of the object you want to scale.
					</div></dd><dt><a href="#CO66-2"><span class="callout">2</span></a> </dt><dd><div class="para">
						Specifies that this trigger authentication uses a platform-native pod authentication method for authorization.
					</div></dd><dt><a href="#CO66-3"><span class="callout">3</span></a> </dt><dd><div class="para">
						Specifies a pod identity. Supported values are <code class="literal">none</code>, <code class="literal">azure</code>, <code class="literal">aws-eks</code>, or <code class="literal">aws-kiam</code>. The default is <code class="literal">none</code>.
					</div></dd></dl></div><div class="itemizedlist"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						For information about OpenShift Container Platform secrets, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-secrets">Providing sensitive data to pods</a>.
					</li></ul></div><section class="section" id="nodes-cma-autoscaling-custom-trigger-auth-using_nodes-cma-autoscaling-custom-trigger-auth"><div class="titlepage"><div><div><h3 class="title">3.5.1. Using trigger authentications</h3></div></div></div><p>
					You use trigger authentications and cluster trigger authentications by using a custom resource to create the authentication, then add a reference to a scaled object or scaled job.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The Custom Metrics Autoscaler Operator must be installed.
						</li><li class="listitem"><p class="simpara">
							If you are using a secret, the <code class="literal">Secret</code> object must exist, for example:
						</p><div class="formalpara"><p class="title"><strong>Example secret</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: my-secret
data:
  user-name: &lt;base64_USER_NAME&gt;
  password: &lt;base64_USER_PASSWORD&gt;</pre>

							</p></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the <code class="literal">TriggerAuthentication</code> or <code class="literal">ClusterTriggerAuthentication</code> object.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file that defines the object:
								</p><div class="formalpara"><p class="title"><strong>Example trigger authentication with a secret</strong></p><p>
										
<pre class="programlisting language-yaml">kind: TriggerAuthentication
apiVersion: keda.sh/v1alpha1
metadata:
  name: prom-triggerauthentication
  namespace: my-namespace
spec:
  secretTargetRef:
  - parameter: user-name
    name: my-secret
    key: USER_NAME
  - parameter: password
    name: my-secret
    key: USER_PASSWORD</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">TriggerAuthentication</code> object:
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create or edit a <code class="literal">ScaledObject</code> YAML file:
						</p><div class="formalpara"><p class="title"><strong>Example scaled object</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: scaledobject
  namespace: my-namespace
spec:
  scaleTargetRef:
    name: example-deployment
  maxReplicaCount: 100
  minReplicaCount: 0
  pollingInterval: 30
  triggers:
  - authenticationRef:
    type: prometheus
    metadata:
      serverAddress: https://thanos-querier.openshift-monitoring.svc.cluster.local:9092
      namespace: kedatest # replace &lt;NAMESPACE&gt;
      metricName: http_requests_total
      threshold: '5'
      query: sum(rate(http_requests_total{job="test-app"}[1m]))
      authModes: "basic"
    - authenticationRef: <span id="CO67-1"><!--Empty--></span><span class="callout">1</span>
        name: prom-triggerauthentication
      metadata:
        name: prom-triggerauthentication
      type: object
    - authenticationRef: <span id="CO67-2"><!--Empty--></span><span class="callout">2</span>
        name: prom-cluster-triggerauthentication
        kind: ClusterTriggerAuthentication
      metadata:
        name: prom-cluster-triggerauthentication
      type: object</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO67-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Optional: Specify a trigger authentication.
								</div></dd><dt><a href="#CO67-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Optional: Specify a cluster trigger authentication. You must include the <code class="literal">kind: ClusterTriggerAuthentication</code> parameter.
								</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								It is not necessary to specify both a namespace trigger authentication and a cluster trigger authentication.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Create the object. For example:
						</p><pre class="programlisting language-terminal">$ oc apply -f &lt;file-name&gt;</pre></li></ol></div></section></section><section class="section" id="nodes-cma-autoscaling-custom-pausing"><div class="titlepage"><div><div><h2 class="title">3.6. Pausing the custom metrics autoscaler for a scaled object</h2></div></div></div><p>
				You can pause and restart the autoscaling of a workload, as needed.
			</p><p>
				For example, you might want to pause autoscaling before performing cluster maintenance or to avoid resource starvation by removing non-mission-critical workloads.
			</p><section class="section" id="nodes-cma-autoscaling-custom-pausing-workload_nodes-cma-autoscaling-custom-pausing"><div class="titlepage"><div><div><h3 class="title">3.6.1. Pausing a custom metrics autoscaler</h3></div></div></div><p>
					You can pause the autoscaling of a scaled object by adding the <code class="literal">autoscaling.keda.sh/paused-replicas</code> annotation to the custom metrics autoscaler for that scaled object. The custom metrics autoscaler scales the replicas for that workload to the specified value and pauses autoscaling until the annotation is removed.
				</p><pre class="programlisting language-yaml">apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  annotations:
    autoscaling.keda.sh/paused-replicas: "4"
# ...</pre><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Use the following command to edit the <code class="literal">ScaledObject</code> CR for your workload:
						</p><pre class="programlisting language-terminal">$ oc edit ScaledObject scaledobject</pre></li><li class="listitem"><p class="simpara">
							Add the <code class="literal">autoscaling.keda.sh/paused-replicas</code> annotation with any value:
						</p><pre class="programlisting language-yaml">apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  annotations:
    autoscaling.keda.sh/paused-replicas: "4" <span id="CO68-1"><!--Empty--></span><span class="callout">1</span>
  creationTimestamp: "2023-02-08T14:41:01Z"
  generation: 1
  name: scaledobject
  namespace: my-project
  resourceVersion: '65729'
  uid: f5aec682-acdf-4232-a783-58b5b82f5dd0</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO68-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specifies that the Custom Metrics Autoscaler Operator is to scale the replicas to the specified value and stop autoscaling.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="nodes-cma-autoscaling-custom-pausing-restart_nodes-cma-autoscaling-custom-pausing"><div class="titlepage"><div><div><h3 class="title">3.6.2. Restarting the custom metrics autoscaler for a scaled object</h3></div></div></div><p>
					You can restart a paused custom metrics autoscaler by removing the <code class="literal">autoscaling.keda.sh/paused-replicas</code> annotation for that <code class="literal">ScaledObject</code>.
				</p><pre class="programlisting language-yaml">apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  annotations:
    autoscaling.keda.sh/paused-replicas: "4"
# ...</pre><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Use the following command to edit the <code class="literal">ScaledObject</code> CR for your workload:
						</p><pre class="programlisting language-terminal">$ oc edit ScaledObject scaledobject</pre></li><li class="listitem"><p class="simpara">
							Remove the <code class="literal">autoscaling.keda.sh/paused-replicas</code> annotation.
						</p><pre class="programlisting language-yaml">apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  annotations:
    autoscaling.keda.sh/paused-replicas: "4" <span id="CO69-1"><!--Empty--></span><span class="callout">1</span>
  creationTimestamp: "2023-02-08T14:41:01Z"
  generation: 1
  name: scaledobject
  namespace: my-project
  resourceVersion: '65729'
  uid: f5aec682-acdf-4232-a783-58b5b82f5dd0</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO69-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Remove this annotation to restart a paused custom metrics autoscaler.
								</div></dd></dl></div></li></ol></div></section></section><section class="section" id="nodes-cma-autoscaling-custom-audit-log"><div class="titlepage"><div><div><h2 class="title">3.7. Gathering audit logs</h2></div></div></div><p>
				You can gather audit logs, which are a security-relevant chronological set of records documenting the sequence of activities that have affected the system by individual users, administrators, or other components of the system.
			</p><p>
				For example, audit logs can help you understand where an autoscaling request is coming from. This is key information when backends are getting overloaded by autoscaling requests made by user applications and you need to determine which is the troublesome application.
			</p><section class="section" id="nodes-cma-autoscaling-custom-audit_nodes-cma-autoscaling-custom-audit-log"><div class="titlepage"><div><div><h3 class="title">3.7.1. Configuring audit logging</h3></div></div></div><p>
					You can configure auditing for the Custom Metrics Autoscaler Operator by editing the <code class="literal">KedaController</code> custom resource. The logs are sent to an audit log file on a volume that is secured by using a persistent volume claim in the <code class="literal">KedaController</code> CR.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The Custom Metrics Autoscaler Operator must be installed.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">KedaController</code> custom resource to add the <code class="literal">auditConfig</code> stanza:
						</p><pre class="programlisting language-yaml">kind: KedaController
apiVersion: keda.sh/v1alpha1
metadata:
  name: keda
  namespace: openshift-keda
spec:
# ...
  metricsServer:
# ...
    auditConfig:
      logFormat: "json" <span id="CO70-1"><!--Empty--></span><span class="callout">1</span>
      logOutputVolumeClaim: "pvc-audit-log" <span id="CO70-2"><!--Empty--></span><span class="callout">2</span>
      policy:
        rules: <span id="CO70-3"><!--Empty--></span><span class="callout">3</span>
        - level: Metadata
        omitStages: "RequestReceived" <span id="CO70-4"><!--Empty--></span><span class="callout">4</span>
        omitManagedFields: false <span id="CO70-5"><!--Empty--></span><span class="callout">5</span>
      lifetime: <span id="CO70-6"><!--Empty--></span><span class="callout">6</span>
        maxAge: "2"
        maxBackup: "1"
        maxSize: "50"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO70-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specifies the output format of the audit log, either <code class="literal">legacy</code> or <code class="literal">json</code>.
								</div></dd><dt><a href="#CO70-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specifies an existing persistent volume claim for storing the log data. All requests coming to the API server are logged to this persistent volume claim. If you leave this field empty, the log data is sent to stdout.
								</div></dd><dt><a href="#CO70-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specifies which events should be recorded and what data they should include:
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">None</code>: Do not log events.
										</li><li class="listitem">
											<code class="literal">Metadata</code>: Log only the metadata for the request, such as user, timestamp, and so forth. Do not log the request text and the response text. This is the default.
										</li><li class="listitem">
											<code class="literal">Request</code>: Log only the metadata and the request text but not the response text. This option does not apply for non-resource requests.
										</li><li class="listitem">
											<code class="literal">RequestResponse</code>: Log event metadata, request text, and response text. This option does not apply for non-resource requests.
										</li></ul></div></dd><dt><a href="#CO70-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specifies stages for which no event is created.
								</div></dd><dt><a href="#CO70-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Specifies whether to omit the managed fields of the request and response bodies from being written to the API audit log, either <code class="literal">true</code> to omit the fields or <code class="literal">false</code> to include the fields.
								</div></dd><dt><a href="#CO70-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Specifies the size and lifespan of the audit logs.
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">maxAge</code>: The maximum number of days to retain audit log files, based on the timestamp encoded in their filename.
										</li><li class="listitem">
											<code class="literal">maxBackup</code>: The maximum number of audit log files to retain. Set to <code class="literal">0</code> to retain all audit log files.
										</li><li class="listitem">
											<code class="literal">maxSize</code>: The maximum size in megabytes of an audit log file before it gets rotated.
										</li></ul></div></dd></dl></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the audit log file directly:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Obtain the name of the <code class="literal">keda-metrics-apiserver-*</code> pod:
								</p><pre class="programlisting language-terminal">oc get pod -n openshift-keda</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                                                  READY   STATUS    RESTARTS   AGE
custom-metrics-autoscaler-operator-5cb44cd75d-9v4lv   1/1     Running   0          8m20s
keda-metrics-apiserver-65c7cc44fd-rrl4r               1/1     Running   0          2m55s
keda-operator-776cbb6768-zpj5b                        1/1     Running   0          2m55s</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									View the log data by using a command similar to the following:
								</p><pre class="programlisting language-terminal">$ oc logs keda-metrics-apiserver-&lt;hash&gt;|grep -i metadata <span id="CO71-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO71-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Optional: You can use the <code class="literal">grep</code> command to specify the log level to display: <code class="literal">Metadata</code>, <code class="literal">Request</code>, <code class="literal">RequestResponse</code>.
										</div></dd></dl></div><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc logs keda-metrics-apiserver-65c7cc44fd-rrl4r|grep -i metadata</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal"> ...
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Metadata","auditID":"4c81d41b-3dab-4675-90ce-20b87ce24013","stage":"ResponseComplete","requestURI":"/healthz","verb":"get","user":{"username":"system:anonymous","groups":["system:unauthenticated"]},"sourceIPs":["10.131.0.1"],"userAgent":"kube-probe/1.26","responseStatus":{"metadata":{},"code":200},"requestReceivedTimestamp":"2023-02-16T13:00:03.554567Z","stageTimestamp":"2023-02-16T13:00:03.555032Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":""}}
 ...</pre>

									</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Alternatively, you can view a specific log:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Use a command similar to the following to log into the <code class="literal">keda-metrics-apiserver-*</code> pod:
								</p><pre class="programlisting language-terminal">$ oc rsh pod/keda-metrics-apiserver-&lt;hash&gt; -n openshift-keda</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc rsh pod/keda-metrics-apiserver-65c7cc44fd-rrl4r -n openshift-keda</pre></li><li class="listitem"><p class="simpara">
									Change to the <code class="literal">/var/audit-policy/</code> directory:
								</p><pre class="programlisting language-terminal">sh-4.4$ cd /var/audit-policy/</pre></li><li class="listitem"><p class="simpara">
									List the available logs:
								</p><pre class="programlisting language-terminal">sh-4.4$ ls</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">log-2023.02.17-14:50  policy.yaml</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									View the log, as needed:
								</p><pre class="programlisting language-terminal">sh-4.4$ cat &lt;log_name&gt;/&lt;pvc_name&gt;|grep -i &lt;log_level&gt; <span id="CO72-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO72-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Optional: You can use the <code class="literal">grep</code> command to specify the log level to display: <code class="literal">Metadata</code>, <code class="literal">Request</code>, <code class="literal">RequestResponse</code>.
										</div></dd></dl></div><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">sh-4.4$ cat log-2023.02.17-14:50/pvc-audit-log|grep -i Request</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="screen"> ...
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Request","auditID":"63e7f68c-04ec-4f4d-8749-bf1656572a41","stage":"ResponseComplete","requestURI":"/openapi/v2","verb":"get","user":{"username":"system:aggregator","groups":["system:authenticated"]},"sourceIPs":["10.128.0.1"],"responseStatus":{"metadata":{},"code":304},"requestReceivedTimestamp":"2023-02-17T13:12:55.035478Z","stageTimestamp":"2023-02-17T13:12:55.038346Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"RBAC: allowed by ClusterRoleBinding \"system:discovery\" of ClusterRole \"system:discovery\" to Group \"system:authenticated\""}}
 ...</pre>

									</p></div></li></ol></div></li></ol></div></section></section><section class="section" id="nodes-cma-autoscaling-custom-debugging"><div class="titlepage"><div><div><h2 class="title">3.8. Gathering debugging data</h2></div></div></div><p>
				When opening a support case, it is helpful to provide debugging information about your cluster to Red Hat Support.
			</p><p>
				To help troubleshoot your issue, provide the following information:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Data gathered using the <code class="literal">must-gather</code> tool.
					</li><li class="listitem">
						The unique cluster ID.
					</li></ul></div><p>
				You can use the <code class="literal">must-gather</code> tool to collect data about the Custom Metrics Autoscaler Operator and its components, including the following items:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The <code class="literal">openshift-keda</code> namespace and its child objects.
					</li><li class="listitem">
						The Custom Metric Autoscaler Operator installation objects.
					</li><li class="listitem">
						The Custom Metric Autoscaler Operator CRD objects.
					</li></ul></div><section class="section" id="nodes-cma-autoscaling-custom-debugging-gather_nodes-cma-autoscaling-custom-debugging"><div class="titlepage"><div><div><h3 class="title">3.8.1. Gathering debugging data</h3></div></div></div><p>
					The following command runs the <code class="literal">must-gather</code> tool for the Custom Metrics Autoscaler Operator:
				</p><pre class="programlisting language-terminal">$ oc adm must-gather --image="$(oc get packagemanifests openshift-custom-metrics-autoscaler-operator \
-n openshift-marketplace \
-o jsonpath='{.status.channels[?(@.name=="stable")].currentCSVDesc.annotations.containerImage}')"</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The standard OpenShift Container Platform <code class="literal">must-gather</code> command, <code class="literal">oc adm must-gather</code>, does not collect Custom Metrics Autoscaler Operator data.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							The OpenShift Container Platform CLI (<code class="literal">oc</code>) installed.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Navigate to the directory where you want to store the <code class="literal">must-gather</code> data.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If your cluster is using a restricted network, you must take additional steps. If your mirror registry has a trusted CA, you must first add the trusted CA to the cluster. For all clusters on restricted networks, you must import the default <code class="literal">must-gather</code> image as an image stream by running the following command.
							</p><pre class="programlisting language-terminal">$ oc import-image is/must-gather -n openshift</pre></div></div></li><li class="listitem"><p class="simpara">
							Perform one of the following:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To get only the Custom Metrics Autoscaler Operator <code class="literal">must-gather</code> data, use the following command:
								</p><pre class="programlisting language-terminal">$ oc adm must-gather --image="$(oc get packagemanifests openshift-custom-metrics-autoscaler-operator \
-n openshift-marketplace \
-o jsonpath='{.status.channels[?(@.name=="stable")].currentCSVDesc.annotations.containerImage}')"</pre><p class="simpara">
									The custom image for the <code class="literal">must-gather</code> command is pulled directly from the Operator package manifests, so that it works on any cluster where the Custom Metric Autoscaler Operator is available.
								</p></li><li class="listitem"><p class="simpara">
									To gather the default <code class="literal">must-gather</code> data in addition to the Custom Metric Autoscaler Operator information:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
											Use the following command to obtain the Custom Metrics Autoscaler Operator image and set it as an environment variable:
										</p><pre class="programlisting language-terminal">$ IMAGE="$(oc get packagemanifests openshift-custom-metrics-autoscaler-operator \
  -n openshift-marketplace \
  -o jsonpath='{.status.channels[?(@.name=="stable")].currentCSVDesc.annotations.containerImage}')"</pre></li><li class="listitem"><p class="simpara">
											Use the <code class="literal">oc adm must-gather</code> with the Custom Metrics Autoscaler Operator image:
										</p><pre class="programlisting language-terminal">$ oc adm must-gather --image-stream=openshift/must-gather --image=${IMAGE}</pre></li></ol></div></li></ul></div><div class="example" id="idm140232240177008"><p class="title"><strong>Example 3.1. Example must-gather output for the Custom Metric Autoscaler:</strong></p><div class="example-contents"><pre class="programlisting language-terminal">└── openshift-keda
    ├── apps
    │   ├── daemonsets.yaml
    │   ├── deployments.yaml
    │   ├── replicasets.yaml
    │   └── statefulsets.yaml
    ├── apps.openshift.io
    │   └── deploymentconfigs.yaml
    ├── autoscaling
    │   └── horizontalpodautoscalers.yaml
    ├── batch
    │   ├── cronjobs.yaml
    │   └── jobs.yaml
    ├── build.openshift.io
    │   ├── buildconfigs.yaml
    │   └── builds.yaml
    ├── core
    │   ├── configmaps.yaml
    │   ├── endpoints.yaml
    │   ├── events.yaml
    │   ├── persistentvolumeclaims.yaml
    │   ├── pods.yaml
    │   ├── replicationcontrollers.yaml
    │   ├── secrets.yaml
    │   └── services.yaml
    ├── discovery.k8s.io
    │   └── endpointslices.yaml
    ├── image.openshift.io
    │   └── imagestreams.yaml
    ├── k8s.ovn.org
    │   ├── egressfirewalls.yaml
    │   └── egressqoses.yaml
    ├── keda.sh
    │   ├── kedacontrollers
    │   │   └── keda.yaml
    │   ├── scaledobjects
    │   │   └── example-scaledobject.yaml
    │   └── triggerauthentications
    │       └── example-triggerauthentication.yaml
    ├── monitoring.coreos.com
    │   └── servicemonitors.yaml
    ├── networking.k8s.io
    │   └── networkpolicies.yaml
    ├── openshift-keda.yaml
    ├── pods
    │   ├── custom-metrics-autoscaler-operator-58bd9f458-ptgwx
    │   │   ├── custom-metrics-autoscaler-operator
    │   │   │   └── custom-metrics-autoscaler-operator
    │   │   │       └── logs
    │   │   │           ├── current.log
    │   │   │           ├── previous.insecure.log
    │   │   │           └── previous.log
    │   │   └── custom-metrics-autoscaler-operator-58bd9f458-ptgwx.yaml
    │   ├── custom-metrics-autoscaler-operator-58bd9f458-thbsh
    │   │   └── custom-metrics-autoscaler-operator
    │   │       └── custom-metrics-autoscaler-operator
    │   │           └── logs
    │   ├── keda-metrics-apiserver-65c7cc44fd-6wq4g
    │   │   ├── keda-metrics-apiserver
    │   │   │   └── keda-metrics-apiserver
    │   │   │       └── logs
    │   │   │           ├── current.log
    │   │   │           ├── previous.insecure.log
    │   │   │           └── previous.log
    │   │   └── keda-metrics-apiserver-65c7cc44fd-6wq4g.yaml
    │   └── keda-operator-776cbb6768-fb6m5
    │       ├── keda-operator
    │       │   └── keda-operator
    │       │       └── logs
    │       │           ├── current.log
    │       │           ├── previous.insecure.log
    │       │           └── previous.log
    │       └── keda-operator-776cbb6768-fb6m5.yaml
    ├── policy
    │   └── poddisruptionbudgets.yaml
    └── route.openshift.io
        └── routes.yaml</pre></div></div></li><li class="listitem"><p class="simpara">
							Create a compressed file from the <code class="literal">must-gather</code> directory that was created in your working directory. For example, on a computer that uses a Linux operating system, run the following command:
						</p><pre class="programlisting language-terminal">$ tar cvaf must-gather.tar.gz must-gather.local.5421342344627712289/ <span id="CO73-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO73-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal">must-gather-local.5421342344627712289/</code> with the actual directory name.
								</div></dd></dl></div></li><li class="listitem">
							Attach the compressed file to your support case on the <a class="link" href="https://access.redhat.com">Red Hat Customer Portal</a>.
						</li></ol></div></section></section><section class="section" id="nodes-cma-autoscaling-custom-metrics"><div class="titlepage"><div><div><h2 class="title">3.9. Viewing Operator metrics</h2></div></div></div><p>
				The Custom Metrics Autoscaler Operator exposes ready-to-use metrics that it pulls from the on-cluster monitoring component. You can query the metrics by using the Prometheus Query Language (PromQL) to analyze and diagnose issues. All metrics are reset when the controller pod restarts.
			</p><section class="section" id="nodes-cma-autoscaling-custom-metrics-access_nodes-cma-autoscaling-custom-metrics"><div class="titlepage"><div><div><h3 class="title">3.9.1. Accessing performance metrics</h3></div></div></div><p>
					You can access the metrics and run queries by using the OpenShift Container Platform web console.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Select the <span class="strong strong"><strong>Administrator</strong></span> perspective in the OpenShift Container Platform web console.
						</li><li class="listitem">
							Select <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Metrics</strong></span>.
						</li><li class="listitem">
							To create a custom query, add your PromQL query to the <span class="strong strong"><strong>Expression</strong></span> field.
						</li><li class="listitem">
							To add multiple queries, select <span class="strong strong"><strong>Add Query</strong></span>.
						</li></ol></div><section class="section" id="nodes-cma-autoscaling-custom-metrics-provided_nodes-cma-autoscaling-custom-metrics"><div class="titlepage"><div><div><h4 class="title">3.9.1.1. Provided Operator metrics</h4></div></div></div><p>
						The Custom Metrics Autoscaler Operator exposes the following metrics, which you can view by using the OpenShift Container Platform web console.
					</p><div class="table" id="idm140232257765664"><p class="title"><strong>Table 3.1. Custom Metric Autoscaler Operator metrics</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 30%; " class="col_1"><!--Empty--></col><col style="width: 70%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232240230656" scope="col">Metric name</th><th align="left" valign="top" id="idm140232240229568" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232240230656"> <p>
										<code class="literal">keda_scaler_activity</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140232240229568"> <p>
										Whether the particular scaler is active or inactive. A value of <code class="literal">1</code> indicates the scaler is active; a value of <code class="literal">0</code> indicates the scaler is inactive.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140232240230656"> <p>
										<code class="literal">keda_scaler_metrics_value</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140232240229568"> <p>
										The current value for each scaler’s metric, which is used by the Horizontal Pod Autoscaler (HPA) in computing the target average.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140232240230656"> <p>
										<code class="literal">keda_scaler_metrics_latency</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140232240229568"> <p>
										The latency of retrieving the current metric from each scaler.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140232240230656"> <p>
										<code class="literal">keda_scaler_errors</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140232240229568"> <p>
										The number of errors that have occurred for each scaler.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140232240230656"> <p>
										<code class="literal">keda_scaler_errors_total</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140232240229568"> <p>
										The total number of errors encountered for all scalers.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140232240230656"> <p>
										<code class="literal">keda_scaled_object_errors</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140232240229568"> <p>
										The number of errors that have occurred for each scaled obejct.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140232240230656"> <p>
										<code class="literal">keda_resource_totals</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140232240229568"> <p>
										The total number of Custom Metrics Autoscaler custom resources in each namespace for each custom resource type.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140232240230656"> <p>
										<code class="literal">keda_trigger_totals</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140232240229568"> <p>
										The total number of triggers by trigger type.
									</p>
									 </td></tr></tbody></table></div></div><div class="formalpara"><p class="title"><strong>Custom Metrics Autoscaler Admission webhook metrics</strong></p><p>
							The Custom Metrics Autoscaler Admission webhook also exposes the following Prometheus metrics.
						</p></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 30%; " class="col_1"><!--Empty--></col><col style="width: 70%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232244997344" scope="col">Metric name</th><th align="left" valign="top" id="idm140232244996256" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232244997344"> <p>
										<code class="literal">keda_scaled_object_validation_total</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140232244996256"> <p>
										The number of scaled object validations.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140232244997344"> <p>
										<code class="literal">keda_scaled_object_validation_errors</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140232244996256"> <p>
										The number of validation errors.
									</p>
									 </td></tr></tbody></table></div></section></section></section><section class="section" id="nodes-cma-autoscaling-custom-adding"><div class="titlepage"><div><div><h2 class="title">3.10. Understanding how to add custom metrics autoscalers</h2></div></div></div><p>
				To add a custom metrics autoscaler, create a <code class="literal">ScaledObject</code> custom resource for a deployment, stateful set, or custom resource. Create a <code class="literal">ScaledJob</code> custom resource for a job.
			</p><p>
				You can create only one scaled object for each workload that you want to scale. Also, you cannot use a scaled object and the horizontal pod autoscaler (HPA) on the same workload.
			</p><section class="section" id="nodes-cma-autoscaling-custom-creating-workload_nodes-cma-autoscaling-custom-adding"><div class="titlepage"><div><div><h3 class="title">3.10.1. Adding a custom metrics autoscaler to a workload</h3></div></div></div><p>
					You can create a custom metrics autoscaler for a workload that is created by a <code class="literal">Deployment</code>, <code class="literal">StatefulSet</code>, or <code class="literal">custom resource</code> object.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The Custom Metrics Autoscaler Operator must be installed.
						</li><li class="listitem"><p class="simpara">
							If you use a custom metrics autoscaler for scaling based on CPU or memory:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
									Your cluster administrator must have properly configured cluster metrics. You can use the <code class="literal">oc describe PodMetrics &lt;pod-name&gt;</code> command to determine if metrics are configured. If metrics are configured, the output appears similar to the following, with CPU and Memory displayed under Usage.
								</p><pre class="programlisting language-terminal">$ oc describe PodMetrics openshift-kube-scheduler-ip-10-0-135-131.ec2.internal</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">Name:         openshift-kube-scheduler-ip-10-0-135-131.ec2.internal
Namespace:    openshift-kube-scheduler
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  metrics.k8s.io/v1beta1
Containers:
  Name:  wait-for-host-port
  Usage:
    Memory:  0
  Name:      scheduler
  Usage:
    Cpu:     8m
    Memory:  45440Ki
Kind:        PodMetrics
Metadata:
  Creation Timestamp:  2019-05-23T18:47:56Z
  Self Link:           /apis/metrics.k8s.io/v1beta1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-135-131.ec2.internal
Timestamp:             2019-05-23T18:47:56Z
Window:                1m0s
Events:                &lt;none&gt;</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									The pods associated with the object you want to scale must include specified memory and CPU limits. For example:
								</p><div class="formalpara"><p class="title"><strong>Example pod spec</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
# ...
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      limits:
        memory: "128Mi"
        cpu: "500m"
# ...</pre>

									</p></div></li></ul></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a YAML file similar to the following. Only the name <code class="literal">&lt;2&gt;</code>, object name <code class="literal">&lt;4&gt;</code>, and object kind <code class="literal">&lt;5&gt;</code> are required:
						</p><div class="formalpara"><p class="title"><strong>Example scaled object</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  annotations:
    autoscaling.keda.sh/paused-replicas: "0" <span id="CO74-1"><!--Empty--></span><span class="callout">1</span>
  name: scaledobject <span id="CO74-2"><!--Empty--></span><span class="callout">2</span>
  namespace: my-namespace
spec:
  scaleTargetRef:
    apiVersion: apps/v1 <span id="CO74-3"><!--Empty--></span><span class="callout">3</span>
    name: example-deployment <span id="CO74-4"><!--Empty--></span><span class="callout">4</span>
    kind: Deployment <span id="CO74-5"><!--Empty--></span><span class="callout">5</span>
    envSourceContainerName: .spec.template.spec.containers[0] <span id="CO74-6"><!--Empty--></span><span class="callout">6</span>
  cooldownPeriod:  200 <span id="CO74-7"><!--Empty--></span><span class="callout">7</span>
  maxReplicaCount: 100 <span id="CO74-8"><!--Empty--></span><span class="callout">8</span>
  minReplicaCount: 0 <span id="CO74-9"><!--Empty--></span><span class="callout">9</span>
  metricsServer: <span id="CO74-10"><!--Empty--></span><span class="callout">10</span>
    auditConfig:
      logFormat: "json"
      logOutputVolumeClaim: "persistentVolumeClaimName"
      policy:
        rules:
        - level: Metadata
        omitStages: "RequestReceived"
        omitManagedFields: false
      lifetime:
        maxAge: "2"
        maxBackup: "1"
        maxSize: "50"
  fallback: <span id="CO74-11"><!--Empty--></span><span class="callout">11</span>
    failureThreshold: 3
    replicas: 6
  pollingInterval: 30 <span id="CO74-12"><!--Empty--></span><span class="callout">12</span>
  advanced:
    restoreToOriginalReplicaCount: false <span id="CO74-13"><!--Empty--></span><span class="callout">13</span>
    horizontalPodAutoscalerConfig:
      name: keda-hpa-scale-down <span id="CO74-14"><!--Empty--></span><span class="callout">14</span>
      behavior: <span id="CO74-15"><!--Empty--></span><span class="callout">15</span>
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
          - type: Percent
            value: 100
            periodSeconds: 15
  triggers:
  - type: prometheus <span id="CO74-16"><!--Empty--></span><span class="callout">16</span>
    metadata:
      serverAddress: https://thanos-querier.openshift-monitoring.svc.cluster.local:9092
      namespace: kedatest
      metricName: http_requests_total
      threshold: '5'
      query: sum(rate(http_requests_total{job="test-app"}[1m]))
      authModes: basic
  - authenticationRef: <span id="CO74-17"><!--Empty--></span><span class="callout">17</span>
      name: prom-triggerauthentication
    metadata:
      name: prom-triggerauthentication
    type: object
  - authenticationRef: <span id="CO74-18"><!--Empty--></span><span class="callout">18</span>
      name: prom-cluster-triggerauthentication
    metadata:
      name: prom-cluster-triggerauthentication
    type: object</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO74-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Optional: Specifies that the Custom Metrics Autoscaler Operator is to scale the replicas to the specified value and stop autoscaling, as described in the "Pausing the custom metrics autoscaler for a workload" section.
								</div></dd><dt><a href="#CO74-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specifies a name for this custom metrics autoscaler.
								</div></dd><dt><a href="#CO74-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Optional: Specifies the API version of the target resource. The default is <code class="literal">apps/v1</code>.
								</div></dd><dt><a href="#CO74-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specifies the name of the object that you want to scale.
								</div></dd><dt><a href="#CO74-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Specifies the <code class="literal">kind</code> as <code class="literal">Deployment</code>, <code class="literal">StatefulSet</code> or <code class="literal">CustomResource</code>.
								</div></dd><dt><a href="#CO74-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Optional: Specifies the name of the container in the target resource, from which the custom metrics autoscaler gets environment variables holding secrets and so forth. The default is <code class="literal">.spec.template.spec.containers[0]</code>.
								</div></dd><dt><a href="#CO74-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									Optional. Specifies the period in seconds to wait after the last trigger is reported before scaling the deployment back to <code class="literal">0</code> if the <code class="literal">minReplicaCount</code> is set to <code class="literal">0</code>. The default is <code class="literal">300</code>.
								</div></dd><dt><a href="#CO74-8"><span class="callout">8</span></a> </dt><dd><div class="para">
									Optional: Specifies the maximum number of replicas when scaling up. The default is <code class="literal">100</code>.
								</div></dd><dt><a href="#CO74-9"><span class="callout">9</span></a> </dt><dd><div class="para">
									Optional: Specifies the minimum number of replicas when scaling down.
								</div></dd><dt><a href="#CO74-10"><span class="callout">10</span></a> </dt><dd><div class="para">
									Optional: Specifies the parameters for audit logs. as described in the "Configuring audit logging" section.
								</div></dd><dt><a href="#CO74-11"><span class="callout">11</span></a> </dt><dd><div class="para">
									Optional: Specifies the number of replicas to fall back to if a scaler fails to get metrics from the source for the number of times defined by the <code class="literal">failureThreshold</code> parameter. For more information on fallback behavior, see the <a class="link" href="https://keda.sh/docs/2.7/concepts/scaling-deployments/#fallback">KEDA documentation</a>.
								</div></dd><dt><a href="#CO74-12"><span class="callout">12</span></a> </dt><dd><div class="para">
									Optional: Specifies the interval in seconds to check each trigger on. The default is <code class="literal">30</code>.
								</div></dd><dt><a href="#CO74-13"><span class="callout">13</span></a> </dt><dd><div class="para">
									Optional: Specifies whether to scale back the target resource to the original replica count after the scaled object is deleted. The default is <code class="literal">false</code>, which keeps the replica count as it is when the scaled object is deleted.
								</div></dd><dt><a href="#CO74-14"><span class="callout">14</span></a> </dt><dd><div class="para">
									Optional: Specifies a name for the horizontal pod autoscaler. The default is <code class="literal">keda-hpa-{scaled-object-name}</code>.
								</div></dd><dt><a href="#CO74-15"><span class="callout">15</span></a> </dt><dd><div class="para">
									Optional: Specifies a scaling policy to use to control the rate to scale pods up or down, as described in the "Scaling policies" section.
								</div></dd><dt><a href="#CO74-16"><span class="callout">16</span></a> </dt><dd><div class="para">
									Specifies the trigger to use as the basis for scaling, as described in the "Understanding the custom metrics autoscaler triggers" section. This example uses OpenShift Container Platform monitoring.
								</div></dd><dt><a href="#CO74-17"><span class="callout">17</span></a> </dt><dd><div class="para">
									Optional: Specifies a trigger authentication, as described in the "Creating a custom metrics autoscaler trigger authentication" section.
								</div></dd><dt><a href="#CO74-18"><span class="callout">18</span></a> </dt><dd><div class="para">
									Optional: Specifies a cluster trigger authentication, as described in the "Creating a custom metrics autoscaler trigger authentication" section.
								</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								It is not necessary to specify both a namespace trigger authentication and a cluster trigger authentication.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Create the custom metrics autoscaler:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							View the command output to verify that the custom metrics autoscaler was created:
						</p><pre class="programlisting language-terminal">$ oc get scaledobject &lt;scaled_object_name&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME            SCALETARGETKIND      SCALETARGETNAME        MIN   MAX   TRIGGERS     AUTHENTICATION               READY   ACTIVE   FALLBACK   AGE
scaledobject    apps/v1.Deployment   example-deployment     0     50    prometheus   prom-triggerauthentication   True    True     True       17s</pre>

							</p></div><p class="simpara">
							Note the following fields in the output:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<code class="literal">TRIGGERS</code>: Indicates the trigger, or scaler, that is being used.
								</li><li class="listitem">
									<code class="literal">AUTHENTICATION</code>: Indicates the name of any trigger authentication being used.
								</li><li class="listitem"><p class="simpara">
									<code class="literal">READY</code>: Indicates whether the scaled object is ready to start scaling:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="square"><li class="listitem">
											If <code class="literal">True</code>, the scaled object is ready.
										</li><li class="listitem">
											If <code class="literal">False</code>, the scaled object is not ready because of a problem in one or more of the objects you created.
										</li></ul></div></li><li class="listitem"><p class="simpara">
									<code class="literal">ACTIVE</code>: Indicates whether scaling is taking place:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="square"><li class="listitem">
											If <code class="literal">True</code>, scaling is taking place.
										</li><li class="listitem">
											If <code class="literal">False</code>, scaling is not taking place because there are no metrics or there is a problem in one or more of the objects you created.
										</li></ul></div></li><li class="listitem"><p class="simpara">
									<code class="literal">FALLBACK</code>: Indicates whether the custom metrics autoscaler is able to get metrics from the source
								</p><div class="itemizedlist"><ul class="itemizedlist" type="square"><li class="listitem">
											If <code class="literal">False</code>, the custom metrics autoscaler is getting metrics.
										</li><li class="listitem">
											If <code class="literal">True</code>, the custom metrics autoscaler is getting metrics because there are no metrics or there is a problem in one or more of the objects you created.
										</li></ul></div></li></ul></div></li></ul></div></section><section class="section" id="nodes-cma-autoscaling-custom-creating-job_nodes-cma-autoscaling-custom-adding"><div class="titlepage"><div><div><h3 class="title">3.10.2. Adding a custom metrics autoscaler to a job</h3></div></div></div><p>
					You can create a custom metrics autoscaler for any <code class="literal">Job</code> object.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Scaling by using a scaled job is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
					</p><p>
						For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The Custom Metrics Autoscaler Operator must be installed.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a YAML file similar to the following:
						</p><pre class="programlisting language-yaml">kind: ScaledJob
apiVersion: keda.sh/v1alpha1
metadata:
  name: scaledjob
  namespace: my-namespace
spec:
  failedJobsHistoryLimit: 5
  jobTargetRef:
    activeDeadlineSeconds: 600 <span id="CO75-1"><!--Empty--></span><span class="callout">1</span>
    backoffLimit: 6 <span id="CO75-2"><!--Empty--></span><span class="callout">2</span>
    parallelism: 1 <span id="CO75-3"><!--Empty--></span><span class="callout">3</span>
    completions: 1 <span id="CO75-4"><!--Empty--></span><span class="callout">4</span>
    template:  <span id="CO75-5"><!--Empty--></span><span class="callout">5</span>
      metadata:
        name: pi
      spec:
        containers:
        - name: pi
          image: perl
          command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
  maxReplicaCount: 100 <span id="CO75-6"><!--Empty--></span><span class="callout">6</span>
  pollingInterval: 30 <span id="CO75-7"><!--Empty--></span><span class="callout">7</span>
  successfulJobsHistoryLimit: 5 <span id="CO75-8"><!--Empty--></span><span class="callout">8</span>
  failedJobsHistoryLimit: 5 <span id="CO75-9"><!--Empty--></span><span class="callout">9</span>
  envSourceContainerName: <span id="CO75-10"><!--Empty--></span><span class="callout">10</span>
  rolloutStrategy: gradual <span id="CO75-11"><!--Empty--></span><span class="callout">11</span>
  scalingStrategy: <span id="CO75-12"><!--Empty--></span><span class="callout">12</span>
    strategy: "custom"
    customScalingQueueLengthDeduction: 1
    customScalingRunningJobPercentage: "0.5"
    pendingPodConditions:
      - "Ready"
      - "PodScheduled"
      - "AnyOtherCustomPodCondition"
    multipleScalersCalculation : "max"
  triggers:
  - type: prometheus <span id="CO75-13"><!--Empty--></span><span class="callout">13</span>
    metadata:
      serverAddress: https://thanos-querier.openshift-monitoring.svc.cluster.local:9092
      namespace: kedatest
      metricName: http_requests_total
      threshold: '5'
      query: sum(rate(http_requests_total{job="test-app"}[1m]))
      authModes: "bearer"
  - authenticationRef: <span id="CO75-14"><!--Empty--></span><span class="callout">14</span>
      name: prom-triggerauthentication
    metadata:
      name: prom-triggerauthentication
     type: object
  - authenticationRef: <span id="CO75-15"><!--Empty--></span><span class="callout">15</span>
      name: prom-cluster-triggerauthentication
    metadata:
      name: prom-cluster-triggerauthentication
    type: object</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO75-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specifies the maximum duration the job can run.
								</div></dd><dt><a href="#CO75-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specifies the number of retries for a job. The default is <code class="literal">6</code>.
								</div></dd><dt><a href="#CO75-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Optional: Specifies how many pod replicas a job should run in parallel; defaults to <code class="literal">1</code>.
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											For non-parallel jobs, leave unset. When unset, the default is <code class="literal">1</code>.
										</li></ul></div></dd><dt><a href="#CO75-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Optional: Specifies how many successful pod completions are needed to mark a job completed.
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											For non-parallel jobs, leave unset. When unset, the default is <code class="literal">1</code>.
										</li><li class="listitem">
											For parallel jobs with a fixed completion count, specify the number of completions.
										</li><li class="listitem">
											For parallel jobs with a work queue, leave unset. When unset the default is the value of the <code class="literal">parallelism</code> parameter.
										</li></ul></div></dd><dt><a href="#CO75-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Specifies the template for the pod the controller creates.
								</div></dd><dt><a href="#CO75-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Optional: Specifies the maximum number of replicas when scaling up. The default is <code class="literal">100</code>.
								</div></dd><dt><a href="#CO75-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									Optional: Specifies the interval in seconds to check each trigger on. The default is <code class="literal">30</code>.
								</div></dd><dt><a href="#CO75-8"><span class="callout">8</span></a> </dt><dd><div class="para">
									Optional: Specifies the number of successful finished jobs should be kept. The default is <code class="literal">100</code>.
								</div></dd><dt><a href="#CO75-9"><span class="callout">9</span></a> </dt><dd><div class="para">
									Optional: Specifies how many failed jobs should be kept. The default is <code class="literal">100</code>.
								</div></dd><dt><a href="#CO75-10"><span class="callout">10</span></a> </dt><dd><div class="para">
									Optional: Specifies the name of the container in the target resource, from which the custom autoscaler gets environment variables holding secrets and so forth. The default is <code class="literal">.spec.template.spec.containers[0]</code>.
								</div></dd><dt><a href="#CO75-11"><span class="callout">11</span></a> </dt><dd><div class="para">
									Optional: Specifies whether existing jobs are terminated whenever a scaled job is being updated:
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">default</code>: The autoscaler terminates an existing job if its associated scaled job is updated. The autoscaler recreates the job with the latest specs.
										</li><li class="listitem">
											<code class="literal">gradual</code>: The autoscaler does not terminate an existing job if its associated scaled job is updated. The autoscaler creates new jobs with the latest specs.
										</li></ul></div></dd><dt><a href="#CO75-12"><span class="callout">12</span></a> </dt><dd><div class="para">
									Optional: Specifies a scaling strategy: <code class="literal">default</code>, <code class="literal">custom</code>, or <code class="literal">accurate</code>. The default is <code class="literal">default</code>. For more information, see the link in the "Additional resources" section that follows.
								</div></dd><dt><a href="#CO75-13"><span class="callout">13</span></a> </dt><dd><div class="para">
									Specifies the trigger to use as the basis for scaling, as described in the "Understanding the custom metrics autoscaler triggers" section.
								</div></dd><dt><a href="#CO75-14"><span class="callout">14</span></a> </dt><dd><div class="para">
									Optional: Specifies a trigger authentication, as described in the "Creating a custom metrics autoscaler trigger authentication" section.
								</div></dd><dt><a href="#CO75-15"><span class="callout">15</span></a> </dt><dd><div class="para">
									Optional: Specifies a cluster trigger authentication, as described in the "Creating a custom metrics autoscaler trigger authentication" section.
								</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										It is not necessary to specify both a namespace trigger authentication and a cluster trigger authentication.
									</p></div></div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the custom metrics autoscaler:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							View the command output to verify that the custom metrics autoscaler was created:
						</p><pre class="programlisting language-terminal">$ oc get scaledjob &lt;scaled_job_name&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME        MAX   TRIGGERS     AUTHENTICATION              READY   ACTIVE    AGE
scaledjob   100   prometheus   prom-triggerauthentication  True    True      8s</pre>

							</p></div><p class="simpara">
							Note the following fields in the output:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<code class="literal">TRIGGERS</code>: Indicates the trigger, or scaler, that is being used.
								</li><li class="listitem">
									<code class="literal">AUTHENTICATION</code>: Indicates the name of any trigger authentication being used.
								</li><li class="listitem"><p class="simpara">
									<code class="literal">READY</code>: Indicates whether the scaled object is ready to start scaling:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="square"><li class="listitem">
											If <code class="literal">True</code>, the scaled object is ready.
										</li><li class="listitem">
											If <code class="literal">False</code>, the scaled object is not ready because of a problem in one or more of the objects you created.
										</li></ul></div></li><li class="listitem"><p class="simpara">
									<code class="literal">ACTIVE</code>: Indicates whether scaling is taking place:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="square"><li class="listitem">
											If <code class="literal">True</code>, scaling is taking place.
										</li><li class="listitem">
											If <code class="literal">False</code>, scaling is not taking place because there are no metrics or there is a problem in one or more of the objects you created.
										</li></ul></div></li></ul></div></li></ul></div></section></section><section class="section" id="nodes-cma-autoscaling-custom-removing"><div class="titlepage"><div><div><h2 class="title">3.11. Removing the Custom Metrics Autoscaler Operator</h2></div></div></div><p>
				You can remove the custom metrics autoscaler from your OpenShift Container Platform cluster. After removing the Custom Metrics Autoscaler Operator, remove other components associated with the Operator to avoid potential issues.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Delete the <code class="literal">KedaController</code> custom resource (CR) first. If you do not delete the <code class="literal">KedaController</code> CR, OpenShift Container Platform can hang when you delete the <code class="literal">openshift-keda</code> project. If you delete the Custom Metrics Autoscaler Operator before deleting the CR, you are not able to delete the CR.
				</p></div></div><section class="section" id="nodes-cma-autoscaling-custom-uninstalling_nodes-cma-autoscaling-custom-removing"><div class="titlepage"><div><div><h3 class="title">3.11.1. Uninstalling the Custom Metrics Autoscaler Operator</h3></div></div></div><p>
					Use the following procedure to remove the custom metrics autoscaler from your OpenShift Container Platform cluster.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The Custom Metrics Autoscaler Operator must be installed.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the OpenShift Container Platform web console, click <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
						</li><li class="listitem">
							Switch to the <span class="strong strong"><strong>openshift-keda</strong></span> project.
						</li><li class="listitem"><p class="simpara">
							Remove the <code class="literal">KedaController</code> custom resource.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Find the <span class="strong strong"><strong>CustomMetricsAutoscaler</strong></span> Operator and click the <span class="strong strong"><strong>KedaController</strong></span> tab.
								</li><li class="listitem">
									Find the custom resource, and then click <span class="strong strong"><strong>Delete KedaController</strong></span>.
								</li><li class="listitem">
									Click <span class="strong strong"><strong>Uninstall</strong></span>.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Remove the Custom Metrics Autoscaler Operator:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Click <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
								</li><li class="listitem">
									Find the <span class="strong strong"><strong>CustomMetricsAutoscaler</strong></span> Operator and click the <span class="strong strong"><strong>Options</strong></span> menu 
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Nodes-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
									 and select <span class="strong strong"><strong>Uninstall Operator</strong></span>.
								</li><li class="listitem">
									Click <span class="strong strong"><strong>Uninstall</strong></span>.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Optional: Use the OpenShift CLI to remove the custom metrics autoscaler components:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Delete the custom metrics autoscaler CRDs:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">clustertriggerauthentications.keda.sh</code>
										</li><li class="listitem">
											<code class="literal">kedacontrollers.keda.sh</code>
										</li><li class="listitem">
											<code class="literal">scaledjobs.keda.sh</code>
										</li><li class="listitem">
											<code class="literal">scaledobjects.keda.sh</code>
										</li><li class="listitem">
											<code class="literal">triggerauthentications.keda.sh</code>
										</li></ul></div><pre class="programlisting language-terminal">$ oc delete crd clustertriggerauthentications.keda.sh kedacontrollers.keda.sh scaledjobs.keda.sh scaledobjects.keda.sh triggerauthentications.keda.sh</pre><p class="simpara">
									Deleting the CRDs removes the associated roles, cluster roles, and role bindings. However, there might be a few cluster roles that must be manually deleted.
								</p></li><li class="listitem"><p class="simpara">
									List any custom metrics autoscaler cluster roles:
								</p><pre class="programlisting language-terminal">$ oc get clusterrole | grep keda.sh</pre></li><li class="listitem"><p class="simpara">
									Delete the listed custom metrics autoscaler cluster roles. For example:
								</p><pre class="programlisting language-terminal">$ oc delete clusterrole.keda.sh-v1alpha1-admin</pre></li><li class="listitem"><p class="simpara">
									List any custom metrics autoscaler cluster role bindings:
								</p><pre class="programlisting language-terminal">$ oc get clusterrolebinding | grep keda.sh</pre></li><li class="listitem"><p class="simpara">
									Delete the listed custom metrics autoscaler cluster role bindings. For example:
								</p><pre class="programlisting language-terminal">$ oc delete clusterrolebinding.keda.sh-v1alpha1-admin</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Delete the custom metrics autoscaler project:
						</p><pre class="programlisting language-terminal">$ oc delete project openshift-keda</pre></li><li class="listitem"><p class="simpara">
							Delete the Cluster Metric Autoscaler Operator:
						</p><pre class="programlisting language-terminal">$ oc delete operator/openshift-custom-metrics-autoscaler-operator.openshift-keda</pre></li></ol></div></section></section></section><section class="chapter" id="controlling-pod-placement-onto-nodes-scheduling"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Controlling pod placement onto nodes (scheduling)</h1></div></div></div><section class="section" id="nodes-scheduler-about"><div class="titlepage"><div><div><h2 class="title">4.1. Controlling pod placement using the scheduler</h2></div></div></div><p>
				Pod scheduling is an internal process that determines placement of new pods onto nodes within the cluster.
			</p><p>
				The scheduler code has a clean separation that watches new pods as they get created and identifies the most suitable node to host them. It then creates bindings (pod to node bindings) for the pods using the master API.
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Default pod scheduling</span></dt><dd>
							OpenShift Container Platform comes with a default scheduler that serves the needs of most users. The default scheduler uses both inherent and customization tools to determine the best fit for a pod.
						</dd><dt><span class="term">Advanced pod scheduling</span></dt><dd><p class="simpara">
							In situations where you might want more control over where new pods are placed, the OpenShift Container Platform advanced scheduling features allow you to configure a pod so that the pod is required or has a preference to run on a particular node or alongside a specific pod.
						</p><p class="simpara">
							You can control pod placement by using the following scheduling features:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-profiles">Scheduler profiles</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-pod-affinity">Pod affinity and anti-affinity rules</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-node-affinity-about_nodes-scheduler-node-affinity">Node affinity</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-node-selectors">Node selectors</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-taints-tolerations">Taints and tolerations</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-overcommit">Node overcommitment</a>
								</li></ul></div></dd></dl></div><section class="section" id="about-default-scheduler"><div class="titlepage"><div><div><h3 class="title">4.1.1. About the default scheduler</h3></div></div></div><p>
					The default OpenShift Container Platform pod scheduler is responsible for determining the placement of new pods onto nodes within the cluster. It reads data from the pod and finds a node that is a good fit based on configured profiles. It is completely independent and exists as a standalone solution. It does not modify the pod; it creates a binding for the pod that ties the pod to the particular node.
				</p><section class="section" id="nodes-scheduler-default-about_nodes-scheduler-about"><div class="titlepage"><div><div><h4 class="title">4.1.1.1. Understanding default scheduling</h4></div></div></div><p>
						The existing generic scheduler is the default platform-provided scheduler <span class="emphasis"><em>engine</em></span> that selects a node to host the pod in a three-step operation:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Filters the nodes</span></dt><dd>
									The available nodes are filtered based on the constraints or requirements specified. This is done by running each node through the list of filter functions called <span class="emphasis"><em>predicates</em></span>, or <span class="emphasis"><em>filters</em></span>.
								</dd><dt><span class="term">Prioritizes the filtered list of nodes</span></dt><dd>
									This is achieved by passing each node through a series of <span class="emphasis"><em>priority</em></span>, or <span class="emphasis"><em>scoring</em></span>, functions that assign it a score between 0 - 10, with 0 indicating a bad fit and 10 indicating a good fit to host the pod. The scheduler configuration can also take in a simple <span class="emphasis"><em>weight</em></span> (positive numeric value) for each scoring function. The node score provided by each scoring function is multiplied by the weight (default weight for most scores is 1) and then combined by adding the scores for each node provided by all the scores. This weight attribute can be used by administrators to give higher importance to some scores.
								</dd><dt><span class="term">Selects the best fit node</span></dt><dd>
									The nodes are sorted based on their scores and the node with the highest score is selected to host the pod. If multiple nodes have the same high score, then one of them is selected at random.
								</dd></dl></div></section></section><section class="section" id="nodes-scheduler-about-use-cases_nodes-scheduler-about"><div class="titlepage"><div><div><h3 class="title">4.1.2. Scheduler use cases</h3></div></div></div><p>
					One of the important use cases for scheduling within OpenShift Container Platform is to support flexible affinity and anti-affinity policies.
				</p><section class="section" id="infrastructure-topological-levels_nodes-scheduler-about"><div class="titlepage"><div><div><h4 class="title">4.1.2.1. Infrastructure topological levels</h4></div></div></div><p>
						Administrators can define multiple topological levels for their infrastructure (nodes) by specifying labels on nodes. For example: <code class="literal">region=r1</code>, <code class="literal">zone=z1</code>, <code class="literal">rack=s1</code>.
					</p><p>
						These label names have no particular meaning and administrators are free to name their infrastructure levels anything, such as city/building/room. Also, administrators can define any number of levels for their infrastructure topology, with three levels usually being adequate (such as: <code class="literal">regions</code> → <code class="literal">zones</code> → <code class="literal">racks</code>). Administrators can specify affinity and anti-affinity rules at each of these levels in any combination.
					</p></section><section class="section" id="affinity_nodes-scheduler-about"><div class="titlepage"><div><div><h4 class="title">4.1.2.2. Affinity</h4></div></div></div><p>
						Administrators should be able to configure the scheduler to specify affinity at any topological level, or even at multiple levels. Affinity at a particular level indicates that all pods that belong to the same service are scheduled onto nodes that belong to the same level. This handles any latency requirements of applications by allowing administrators to ensure that peer pods do not end up being too geographically separated. If no node is available within the same affinity group to host the pod, then the pod is not scheduled.
					</p><p>
						If you need greater control over where the pods are scheduled, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-node-affinity">Controlling pod placement on nodes using node affinity rules</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-pod-affinity">Placing pods relative to other pods using affinity and anti-affinity rules</a>.
					</p><p>
						These advanced scheduling features allow administrators to specify which node a pod can be scheduled on and to force or reject scheduling relative to other pods.
					</p></section><section class="section" id="anti-affinity_nodes-scheduler-about"><div class="titlepage"><div><div><h4 class="title">4.1.2.3. Anti-affinity</h4></div></div></div><p>
						Administrators should be able to configure the scheduler to specify anti-affinity at any topological level, or even at multiple levels. Anti-affinity (or 'spread') at a particular level indicates that all pods that belong to the same service are spread across nodes that belong to that level. This ensures that the application is well spread for high availability purposes. The scheduler tries to balance the service pods across all applicable nodes as evenly as possible.
					</p><p>
						If you need greater control over where the pods are scheduled, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-node-affinity">Controlling pod placement on nodes using node affinity rules</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-pod-affinity">Placing pods relative to other pods using affinity and anti-affinity rules</a>.
					</p><p>
						These advanced scheduling features allow administrators to specify which node a pod can be scheduled on and to force or reject scheduling relative to other pods.
					</p></section></section></section><section class="section" id="nodes-scheduler-profiles"><div class="titlepage"><div><div><h2 class="title">4.2. Scheduling pods using a scheduler profile</h2></div></div></div><p>
				You can configure OpenShift Container Platform to use a scheduling profile to schedule pods onto nodes within the cluster.
			</p><section class="section" id="nodes-scheduler-profiles-about_nodes-scheduler-profiles"><div class="titlepage"><div><div><h3 class="title">4.2.1. About scheduler profiles</h3></div></div></div><p>
					You can specify a scheduler profile to control how pods are scheduled onto nodes.
				</p><p>
					The following scheduler profiles are available:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">LowNodeUtilization</code></span></dt><dd>
								This profile attempts to spread pods evenly across nodes to get low resource usage per node. This profile provides the default scheduler behavior.
							</dd><dt><span class="term"><code class="literal">HighNodeUtilization</code></span></dt><dd>
								This profile attempts to place as many pods as possible on to as few nodes as possible. This minimizes node count and has high resource usage per node.
							</dd><dt><span class="term"><code class="literal">NoScoring</code></span></dt><dd>
								This is a low-latency profile that strives for the quickest scheduling cycle by disabling all score plugins. This might sacrifice better scheduling decisions for faster ones.
							</dd></dl></div></section><section class="section" id="nodes-scheduler-profiles-configuring_nodes-scheduler-profiles"><div class="titlepage"><div><div><h3 class="title">4.2.2. Configuring a scheduler profile</h3></div></div></div><p>
					You can configure the scheduler to use a scheduler profile.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">Scheduler</code> object:
						</p><pre class="programlisting language-terminal">$ oc edit scheduler cluster</pre></li><li class="listitem"><p class="simpara">
							Specify the profile to use in the <code class="literal">spec.profile</code> field:
						</p><pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  name: cluster
#...
spec:
  mastersSchedulable: false
  profile: HighNodeUtilization <span id="CO76-1"><!--Empty--></span><span class="callout">1</span>
#...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO76-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Set to <code class="literal">LowNodeUtilization</code>, <code class="literal">HighNodeUtilization</code>, or <code class="literal">NoScoring</code>.
								</div></dd></dl></div></li><li class="listitem">
							Save the file to apply the changes.
						</li></ol></div></section></section><section class="section" id="nodes-scheduler-pod-affinity"><div class="titlepage"><div><div><h2 class="title">4.3. Placing pods relative to other pods using affinity and anti-affinity rules</h2></div></div></div><p>
				Affinity is a property of pods that controls the nodes on which they prefer to be scheduled. Anti-affinity is a property of pods that prevents a pod from being scheduled on a node.
			</p><p>
				In OpenShift Container Platform, <span class="emphasis"><em>pod affinity</em></span> and <span class="emphasis"><em>pod anti-affinity</em></span> allow you to constrain which nodes your pod is eligible to be scheduled on based on the key-value labels on other pods.
			</p><section class="section" id="nodes-scheduler-pod-affinity-about_nodes-scheduler-pod-affinity"><div class="titlepage"><div><div><h3 class="title">4.3.1. Understanding pod affinity</h3></div></div></div><p>
					<span class="emphasis"><em>Pod affinity</em></span> and <span class="emphasis"><em>pod anti-affinity</em></span> allow you to constrain which nodes your pod is eligible to be scheduled on based on the key/value labels on other pods.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Pod affinity can tell the scheduler to locate a new pod on the same node as other pods if the label selector on the new pod matches the label on the current pod.
						</li><li class="listitem">
							Pod anti-affinity can prevent the scheduler from locating a new pod on the same node as pods with the same labels if the label selector on the new pod matches the label on the current pod.
						</li></ul></div><p>
					For example, using affinity rules, you could spread or pack pods within a service or relative to pods in other services. Anti-affinity rules allow you to prevent pods of a particular service from scheduling on the same nodes as pods of another service that are known to interfere with the performance of the pods of the first service. Or, you could spread the pods of a service across nodes, availability zones, or availability sets to reduce correlated failures.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						A label selector might match pods with multiple pod deployments. Use unique combinations of labels when configuring anti-affinity rules to avoid matching pods.
					</p></div></div><p>
					There are two types of pod affinity rules: <span class="emphasis"><em>required</em></span> and <span class="emphasis"><em>preferred</em></span>.
				</p><p>
					Required rules <span class="strong strong"><strong>must</strong></span> be met before a pod can be scheduled on a node. Preferred rules specify that, if the rule is met, the scheduler tries to enforce the rules, but does not guarantee enforcement.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Depending on your pod priority and preemption settings, the scheduler might not be able to find an appropriate node for a pod without violating affinity requirements. If so, a pod might not be scheduled.
					</p><p>
						To prevent this situation, carefully configure pod affinity with equal-priority pods.
					</p></div></div><p>
					You configure pod affinity/anti-affinity through the <code class="literal">Pod</code> spec files. You can specify a required rule, a preferred rule, or both. If you specify both, the node must first meet the required rule, then attempts to meet the preferred rule.
				</p><p>
					The following example shows a <code class="literal">Pod</code> spec configured for pod affinity and anti-affinity.
				</p><p>
					In this example, the pod affinity rule indicates that the pod can schedule onto a node only if that node has at least one already-running pod with a label that has the key <code class="literal">security</code> and value <code class="literal">S1</code>. The pod anti-affinity rule says that the pod prefers to not schedule onto a node if that node is already running a pod with label having key <code class="literal">security</code> and value <code class="literal">S2</code>.
				</p><div class="formalpara"><p class="title"><strong>Sample <code class="literal">Pod</code> config file with pod affinity</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAffinity: <span id="CO77-1"><!--Empty--></span><span class="callout">1</span>
      requiredDuringSchedulingIgnoredDuringExecution: <span id="CO77-2"><!--Empty--></span><span class="callout">2</span>
      - labelSelector:
          matchExpressions:
          - key: security <span id="CO77-3"><!--Empty--></span><span class="callout">3</span>
            operator: In <span id="CO77-4"><!--Empty--></span><span class="callout">4</span>
            values:
            - S1 <span id="CO77-5"><!--Empty--></span><span class="callout">5</span>
        topologyKey: topology.kubernetes.io/zone
  containers:
  - name: with-pod-affinity
    image: docker.io/ocpqe/hello-pod</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO77-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Stanza to configure pod affinity.
						</div></dd><dt><a href="#CO77-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Defines a required rule.
						</div></dd><dt><a href="#CO77-3"><span class="callout">3</span></a> <a href="#CO77-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							The key and value (label) that must be matched to apply the rule.
						</div></dd><dt><a href="#CO77-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							The operator represents the relationship between the label on the existing pod and the set of values in the <code class="literal">matchExpression</code> parameters in the specification for the new pod. Can be <code class="literal">In</code>, <code class="literal">NotIn</code>, <code class="literal">Exists</code>, or <code class="literal">DoesNotExist</code>.
						</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Sample <code class="literal">Pod</code> config file with pod anti-affinity</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: with-pod-antiaffinity
spec:
  affinity:
    podAntiAffinity: <span id="CO78-1"><!--Empty--></span><span class="callout">1</span>
      preferredDuringSchedulingIgnoredDuringExecution: <span id="CO78-2"><!--Empty--></span><span class="callout">2</span>
      - weight: 100  <span id="CO78-3"><!--Empty--></span><span class="callout">3</span>
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security <span id="CO78-4"><!--Empty--></span><span class="callout">4</span>
              operator: In <span id="CO78-5"><!--Empty--></span><span class="callout">5</span>
              values:
              - S2
          topologyKey: kubernetes.io/hostname
  containers:
  - name: with-pod-affinity
    image: docker.io/ocpqe/hello-pod</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO78-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Stanza to configure pod anti-affinity.
						</div></dd><dt><a href="#CO78-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Defines a preferred rule.
						</div></dd><dt><a href="#CO78-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Specifies a weight for a preferred rule. The node with the highest weight is preferred.
						</div></dd><dt><a href="#CO78-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Description of the pod label that determines when the anti-affinity rule applies. Specify a key and value for the label.
						</div></dd><dt><a href="#CO78-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							The operator represents the relationship between the label on the existing pod and the set of values in the <code class="literal">matchExpression</code> parameters in the specification for the new pod. Can be <code class="literal">In</code>, <code class="literal">NotIn</code>, <code class="literal">Exists</code>, or <code class="literal">DoesNotExist</code>.
						</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If labels on a node change at runtime such that the affinity rules on a pod are no longer met, the pod continues to run on the node.
					</p></div></div></section><section class="section" id="nodes-scheduler-pod-affinity-configuring_nodes-scheduler-pod-affinity"><div class="titlepage"><div><div><h3 class="title">4.3.2. Configuring a pod affinity rule</h3></div></div></div><p>
					The following steps demonstrate a simple two-pod configuration that creates pod with a label and a pod that uses affinity to allow scheduling with that pod.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You cannot add an affinity directly to a scheduled pod.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a pod with a specific label in the pod spec:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file with the following content:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: security-s1
  labels:
    security: S1
spec:
  containers:
  - name: security-s1
    image: docker.io/ocpqe/hello-pod</pre></li><li class="listitem"><p class="simpara">
									Create the pod.
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;pod-spec&gt;.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							When creating other pods, configure the following parameters to add the affinity:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file with the following content:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: security-s1-east
#...
spec
  affinity <span id="CO79-1"><!--Empty--></span><span class="callout">1</span>
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution: <span id="CO79-2"><!--Empty--></span><span class="callout">2</span>
      - labelSelector:
          matchExpressions:
          - key: security <span id="CO79-3"><!--Empty--></span><span class="callout">3</span>
            values:
            - S1
            operator: In <span id="CO79-4"><!--Empty--></span><span class="callout">4</span>
        topologyKey: topology.kubernetes.io/zone <span id="CO79-5"><!--Empty--></span><span class="callout">5</span>
#...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO79-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Adds a pod affinity.
										</div></dd><dt><a href="#CO79-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Configures the <code class="literal">requiredDuringSchedulingIgnoredDuringExecution</code> parameter or the <code class="literal">preferredDuringSchedulingIgnoredDuringExecution</code> parameter.
										</div></dd><dt><a href="#CO79-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Specifies the <code class="literal">key</code> and <code class="literal">values</code> that must be met. If you want the new pod to be scheduled with the other pod, use the same <code class="literal">key</code> and <code class="literal">values</code> parameters as the label on the first pod.
										</div></dd><dt><a href="#CO79-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Specifies an <code class="literal">operator</code>. The operator can be <code class="literal">In</code>, <code class="literal">NotIn</code>, <code class="literal">Exists</code>, or <code class="literal">DoesNotExist</code>. For example, use the operator <code class="literal">In</code> to require the label to be in the node.
										</div></dd><dt><a href="#CO79-5"><span class="callout">5</span></a> </dt><dd><div class="para">
											Specify a <code class="literal">topologyKey</code>, which is a prepopulated <a class="link" href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#interlude-built-in-node-labels">Kubernetes label</a> that the system uses to denote such a topology domain.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Create the pod.
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;pod-spec&gt;.yaml</pre></li></ol></div></li></ol></div></section><section class="section" id="nodes-scheduler-pod-anti-affinity-configuring_nodes-scheduler-pod-affinity"><div class="titlepage"><div><div><h3 class="title">4.3.3. Configuring a pod anti-affinity rule</h3></div></div></div><p>
					The following steps demonstrate a simple two-pod configuration that creates pod with a label and a pod that uses an anti-affinity preferred rule to attempt to prevent scheduling with that pod.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You cannot add an affinity directly to a scheduled pod.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a pod with a specific label in the pod spec:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file with the following content:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: security-s1
  labels:
    security: S1
spec:
  containers:
  - name: security-s1
    image: docker.io/ocpqe/hello-pod</pre></li><li class="listitem"><p class="simpara">
									Create the pod.
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;pod-spec&gt;.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							When creating other pods, configure the following parameters:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file with the following content:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: security-s2-east
#...
spec
  affinity <span id="CO80-1"><!--Empty--></span><span class="callout">1</span>
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution: <span id="CO80-2"><!--Empty--></span><span class="callout">2</span>
      - weight: 100 <span id="CO80-3"><!--Empty--></span><span class="callout">3</span>
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security <span id="CO80-4"><!--Empty--></span><span class="callout">4</span>
              values:
              - S1
              operator: In <span id="CO80-5"><!--Empty--></span><span class="callout">5</span>
          topologyKey: kubernetes.io/hostname <span id="CO80-6"><!--Empty--></span><span class="callout">6</span>
#...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO80-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Adds a pod anti-affinity.
										</div></dd><dt><a href="#CO80-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Configures the <code class="literal">requiredDuringSchedulingIgnoredDuringExecution</code> parameter or the <code class="literal">preferredDuringSchedulingIgnoredDuringExecution</code> parameter.
										</div></dd><dt><a href="#CO80-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											For a preferred rule, specifies a weight for the node, 1-100. The node that with highest weight is preferred.
										</div></dd><dt><a href="#CO80-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Specifies the <code class="literal">key</code> and <code class="literal">values</code> that must be met. If you want the new pod to not be scheduled with the other pod, use the same <code class="literal">key</code> and <code class="literal">values</code> parameters as the label on the first pod.
										</div></dd><dt><a href="#CO80-5"><span class="callout">5</span></a> </dt><dd><div class="para">
											Specifies an <code class="literal">operator</code>. The operator can be <code class="literal">In</code>, <code class="literal">NotIn</code>, <code class="literal">Exists</code>, or <code class="literal">DoesNotExist</code>. For example, use the operator <code class="literal">In</code> to require the label to be in the node.
										</div></dd><dt><a href="#CO80-6"><span class="callout">6</span></a> </dt><dd><div class="para">
											Specifies a <code class="literal">topologyKey</code>, which is a prepopulated <a class="link" href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#interlude-built-in-node-labels">Kubernetes label</a> that the system uses to denote such a topology domain.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Create the pod.
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;pod-spec&gt;.yaml</pre></li></ol></div></li></ol></div></section><section class="section" id="nodes-scheduler-pod-affinity-example_nodes-scheduler-pod-affinity"><div class="titlepage"><div><div><h3 class="title">4.3.4. Sample pod affinity and anti-affinity rules</h3></div></div></div><p>
					The following examples demonstrate pod affinity and pod anti-affinity.
				</p><section class="section" id="nodes-scheduler-pod-affinity-example-affinity_nodes-scheduler-pod-affinity"><div class="titlepage"><div><div><h4 class="title">4.3.4.1. Pod Affinity</h4></div></div></div><p>
						The following example demonstrates pod affinity for pods with matching labels and label selectors.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								The pod <span class="strong strong"><strong>team4</strong></span> has the label <code class="literal">team:4</code>.
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: team4
  labels:
     team: "4"
#...
spec:
  containers:
  - name: ocp
    image: docker.io/ocpqe/hello-pod
#...</pre></li><li class="listitem"><p class="simpara">
								The pod <span class="strong strong"><strong>team4a</strong></span> has the label selector <code class="literal">team:4</code> under <code class="literal">podAffinity</code>.
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: team4a
#...
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: team
            operator: In
            values:
            - "4"
        topologyKey: kubernetes.io/hostname
  containers:
  - name: pod-affinity
    image: docker.io/ocpqe/hello-pod
#...</pre></li><li class="listitem">
								The <span class="strong strong"><strong>team4a</strong></span> pod is scheduled on the same node as the <span class="strong strong"><strong>team4</strong></span> pod.
							</li></ul></div></section><section class="section" id="nodes-scheduler-pod-affinity-example-antiaffinity_nodes-scheduler-pod-affinity"><div class="titlepage"><div><div><h4 class="title">4.3.4.2. Pod Anti-affinity</h4></div></div></div><p>
						The following example demonstrates pod anti-affinity for pods with matching labels and label selectors.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								The pod <span class="strong strong"><strong>pod-s1</strong></span> has the label <code class="literal">security:s1</code>.
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod-s1
  labels:
    security: s1
#...
spec:
  containers:
  - name: ocp
    image: docker.io/ocpqe/hello-pod
#...</pre></li><li class="listitem"><p class="simpara">
								The pod <span class="strong strong"><strong>pod-s2</strong></span> has the label selector <code class="literal">security:s1</code> under <code class="literal">podAntiAffinity</code>.
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod-s2
#...
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - s1
        topologyKey: kubernetes.io/hostname
  containers:
  - name: pod-antiaffinity
    image: docker.io/ocpqe/hello-pod
#...</pre></li><li class="listitem">
								The pod <span class="strong strong"><strong>pod-s2</strong></span> cannot be scheduled on the same node as <code class="literal">pod-s1</code>.
							</li></ul></div></section><section class="section" id="nodes-scheduler-pod-affinity-example-no-labels_nodes-scheduler-pod-affinity"><div class="titlepage"><div><div><h4 class="title">4.3.4.3. Pod Affinity with no Matching Labels</h4></div></div></div><p>
						The following example demonstrates pod affinity for pods without matching labels and label selectors.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								The pod <span class="strong strong"><strong>pod-s1</strong></span> has the label <code class="literal">security:s1</code>.
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod-s1
  labels:
    security: s1
#...
spec:
  containers:
  - name: ocp
    image: docker.io/ocpqe/hello-pod
#...</pre></li><li class="listitem"><p class="simpara">
								The pod <span class="strong strong"><strong>pod-s2</strong></span> has the label selector <code class="literal">security:s2</code>.
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod-s2
#...
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - s2
        topologyKey: kubernetes.io/hostname
  containers:
  - name: pod-affinity
    image: docker.io/ocpqe/hello-pod
#...</pre></li><li class="listitem"><p class="simpara">
								The pod <span class="strong strong"><strong>pod-s2</strong></span> is not scheduled unless there is a node with a pod that has the <code class="literal">security:s2</code> label. If there is no other pod with that label, the new pod remains in a pending state:
							</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME      READY     STATUS    RESTARTS   AGE       IP        NODE
pod-s2    0/1       Pending   0          32s       &lt;none&gt;</pre>

								</p></div></li></ul></div></section></section><section class="section" id="olm-overriding-operator-pod-affinity_nodes-scheduler-pod-affinity"><div class="titlepage"><div><div><h3 class="title">4.3.5. Using pod affinity and anti-affinity to control where an Operator is installed</h3></div></div></div><p>
					By default, when you install an Operator, OpenShift Container Platform installs the Operator pod to one of your worker nodes randomly. However, there might be situations where you want that pod scheduled on a specific node or set of nodes.
				</p><p>
					The following examples describe situations where you might want to schedule an Operator pod to a specific node or set of nodes:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							If an Operator requires a particular platform, such as <code class="literal">amd64</code> or <code class="literal">arm64</code>
						</li><li class="listitem">
							If an Operator requires a particular operating system, such as Linux or Windows
						</li><li class="listitem">
							If you want Operators that work together scheduled on the same host or on hosts located on the same rack
						</li><li class="listitem">
							If you want Operators dispersed throughout the infrastructure to avoid downtime due to network or hardware issues
						</li></ul></div><p>
					You can control where an Operator pod is installed by adding a pod affinity or anti-affinity to the Operator’s <code class="literal">Subscription</code> object.
				</p><p>
					The following example shows how to use pod anti-affinity to prevent the installation the Custom Metrics Autoscaler Operator from any node that has pods with a specific label:
				</p><div class="formalpara"><p class="title"><strong>Pod affinity example that places the Operator pod on one or more specific nodes</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: openshift-custom-metrics-autoscaler-operator
  namespace: openshift-keda
spec:
  name: my-package
  source: my-operators
  sourceNamespace: operator-registries
  config:
    affinity:
      podAffinity: <span id="CO81-1"><!--Empty--></span><span class="callout">1</span>
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - test
          topologyKey: kubernetes.io/hostname
#...</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO81-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							A pod affinity that places the Operator’s pod on a node that has pods with the <code class="literal">app=test</code> label.
						</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Pod anti-affinity example that prevents the Operator pod from one or more specific nodes</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: openshift-custom-metrics-autoscaler-operator
  namespace: openshift-keda
spec:
  name: my-package
  source: my-operators
  sourceNamespace: operator-registries
  config:
    affinity:
      podAntiAffinity: <span id="CO82-1"><!--Empty--></span><span class="callout">1</span>
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: cpu
              operator: In
              values:
              - high
          topologyKey: kubernetes.io/hostname
#...</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO82-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							A pod anti-affinity that prevents the Operator’s pod from being scheduled on a node that has pods with the <code class="literal">cpu=high</code> label.
						</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To control the placement of an Operator pod, complete the following steps:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Install the Operator as usual.
						</li><li class="listitem">
							If needed, ensure that your nodes are labeled to properly respond to the affinity.
						</li><li class="listitem"><p class="simpara">
							Edit the Operator <code class="literal">Subscription</code> object to add an affinity:
						</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: openshift-custom-metrics-autoscaler-operator
  namespace: openshift-keda
spec:
  name: my-package
  source: my-operators
  sourceNamespace: operator-registries
  config:
    affinity:
      podAntiAffinity: <span id="CO83-1"><!--Empty--></span><span class="callout">1</span>
        requiredDuringSchedulingIgnoredDuringExecution:
          podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: kubernetes.io/hostname
                operator: In
                values:
                - ip-10-0-185-229.ec2.internal
            topologyKey: topology.kubernetes.io/zone
#...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO83-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add a <code class="literal">podAffinity</code> or <code class="literal">podAntiAffinity</code>.
								</div></dd></dl></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To ensure that the pod is deployed on the specific node, run the following command:
						</p><pre class="programlisting language-yaml">$ oc get pods -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                  READY   STATUS    RESTARTS   AGE   IP            NODE                           NOMINATED NODE   READINESS GATES
custom-metrics-autoscaler-operator-5dcc45d656-bhshg   1/1     Running   0          50s   10.131.0.20   ip-10-0-185-229.ec2.internal   &lt;none&gt;           &lt;none&gt;</pre>

							</p></div></li></ul></div></section></section><section class="section" id="nodes-scheduler-node-affinity"><div class="titlepage"><div><div><h2 class="title">4.4. Controlling pod placement on nodes using node affinity rules</h2></div></div></div><p>
				Affinity is a property of pods that controls the nodes on which they prefer to be scheduled.
			</p><p>
				In OpenShift Container Platform node affinity is a set of rules used by the scheduler to determine where a pod can be placed. The rules are defined using custom labels on the nodes and label selectors specified in pods.
			</p><section class="section" id="nodes-scheduler-node-affinity-about_nodes-scheduler-node-affinity"><div class="titlepage"><div><div><h3 class="title">4.4.1. Understanding node affinity</h3></div></div></div><p>
					Node affinity allows a pod to specify an affinity towards a group of nodes it can be placed on. The node does not have control over the placement.
				</p><p>
					For example, you could configure a pod to only run on a node with a specific CPU or in a specific availability zone.
				</p><p>
					There are two types of node affinity rules: <span class="emphasis"><em>required</em></span> and <span class="emphasis"><em>preferred</em></span>.
				</p><p>
					Required rules <span class="strong strong"><strong>must</strong></span> be met before a pod can be scheduled on a node. Preferred rules specify that, if the rule is met, the scheduler tries to enforce the rules, but does not guarantee enforcement.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If labels on a node change at runtime that results in an node affinity rule on a pod no longer being met, the pod continues to run on the node.
					</p></div></div><p>
					You configure node affinity through the <code class="literal">Pod</code> spec file. You can specify a required rule, a preferred rule, or both. If you specify both, the node must first meet the required rule, then attempts to meet the preferred rule.
				</p><p>
					The following example is a <code class="literal">Pod</code> spec with a rule that requires the pod be placed on a node with a label whose key is <code class="literal">e2e-az-NorthSouth</code> and whose value is either <code class="literal">e2e-az-North</code> or <code class="literal">e2e-az-South</code>:
				</p><div class="formalpara"><p class="title"><strong>Example pod configuration file with a node affinity required rule</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity: <span id="CO84-1"><!--Empty--></span><span class="callout">1</span>
      requiredDuringSchedulingIgnoredDuringExecution: <span id="CO84-2"><!--Empty--></span><span class="callout">2</span>
        nodeSelectorTerms:
        - matchExpressions:
          - key: e2e-az-NorthSouth <span id="CO84-3"><!--Empty--></span><span class="callout">3</span>
            operator: In <span id="CO84-4"><!--Empty--></span><span class="callout">4</span>
            values:
            - e2e-az-North <span id="CO84-5"><!--Empty--></span><span class="callout">5</span>
            - e2e-az-South <span id="CO84-6"><!--Empty--></span><span class="callout">6</span>
  containers:
  - name: with-node-affinity
    image: docker.io/ocpqe/hello-pod
#...</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO84-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The stanza to configure node affinity.
						</div></dd><dt><a href="#CO84-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Defines a required rule.
						</div></dd><dt><a href="#CO84-3"><span class="callout">3</span></a> <a href="#CO84-5"><span class="callout">5</span></a> <a href="#CO84-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							The key/value pair (label) that must be matched to apply the rule.
						</div></dd><dt><a href="#CO84-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							The operator represents the relationship between the label on the node and the set of values in the <code class="literal">matchExpression</code> parameters in the <code class="literal">Pod</code> spec. This value can be <code class="literal">In</code>, <code class="literal">NotIn</code>, <code class="literal">Exists</code>, or <code class="literal">DoesNotExist</code>, <code class="literal">Lt</code>, or <code class="literal">Gt</code>.
						</div></dd></dl></div><p>
					The following example is a node specification with a preferred rule that a node with a label whose key is <code class="literal">e2e-az-EastWest</code> and whose value is either <code class="literal">e2e-az-East</code> or <code class="literal">e2e-az-West</code> is preferred for the pod:
				</p><div class="formalpara"><p class="title"><strong>Example pod configuration file with a node affinity preferred rule</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity: <span id="CO85-1"><!--Empty--></span><span class="callout">1</span>
      preferredDuringSchedulingIgnoredDuringExecution: <span id="CO85-2"><!--Empty--></span><span class="callout">2</span>
      - weight: 1 <span id="CO85-3"><!--Empty--></span><span class="callout">3</span>
        preference:
          matchExpressions:
          - key: e2e-az-EastWest <span id="CO85-4"><!--Empty--></span><span class="callout">4</span>
            operator: In <span id="CO85-5"><!--Empty--></span><span class="callout">5</span>
            values:
            - e2e-az-East <span id="CO85-6"><!--Empty--></span><span class="callout">6</span>
            - e2e-az-West <span id="CO85-7"><!--Empty--></span><span class="callout">7</span>
  containers:
  - name: with-node-affinity
    image: docker.io/ocpqe/hello-pod
#...</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO85-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The stanza to configure node affinity.
						</div></dd><dt><a href="#CO85-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Defines a preferred rule.
						</div></dd><dt><a href="#CO85-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Specifies a weight for a preferred rule. The node with highest weight is preferred.
						</div></dd><dt><a href="#CO85-4"><span class="callout">4</span></a> <a href="#CO85-6"><span class="callout">6</span></a> <a href="#CO85-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							The key/value pair (label) that must be matched to apply the rule.
						</div></dd><dt><a href="#CO85-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							The operator represents the relationship between the label on the node and the set of values in the <code class="literal">matchExpression</code> parameters in the <code class="literal">Pod</code> spec. This value can be <code class="literal">In</code>, <code class="literal">NotIn</code>, <code class="literal">Exists</code>, or <code class="literal">DoesNotExist</code>, <code class="literal">Lt</code>, or <code class="literal">Gt</code>.
						</div></dd></dl></div><p>
					There is no explicit <span class="emphasis"><em>node anti-affinity</em></span> concept, but using the <code class="literal">NotIn</code> or <code class="literal">DoesNotExist</code> operator replicates that behavior.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you are using node affinity and node selectors in the same pod configuration, note the following:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								If you configure both <code class="literal">nodeSelector</code> and <code class="literal">nodeAffinity</code>, both conditions must be satisfied for the pod to be scheduled onto a candidate node.
							</li><li class="listitem">
								If you specify multiple <code class="literal">nodeSelectorTerms</code> associated with <code class="literal">nodeAffinity</code> types, then the pod can be scheduled onto a node if one of the <code class="literal">nodeSelectorTerms</code> is satisfied.
							</li><li class="listitem">
								If you specify multiple <code class="literal">matchExpressions</code> associated with <code class="literal">nodeSelectorTerms</code>, then the pod can be scheduled onto a node only if all <code class="literal">matchExpressions</code> are satisfied.
							</li></ul></div></div></div></section><section class="section" id="nodes-scheduler-node-affinity-configuring-required_nodes-scheduler-node-affinity"><div class="titlepage"><div><div><h3 class="title">4.4.2. Configuring a required node affinity rule</h3></div></div></div><p>
					Required rules <span class="strong strong"><strong>must</strong></span> be met before a pod can be scheduled on a node.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						The following steps demonstrate a simple configuration that creates a node and a pod that the scheduler is required to place on the node.
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add a label to a node using the <code class="literal">oc label node</code> command:
						</p><pre class="programlisting language-terminal">$ oc label node node1 e2e-az-name=e2e-az1</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can alternatively apply the following YAML to add the label:
						</p><pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  name: &lt;node_name&gt;
  labels:
    e2e-az-name: e2e-az1
#...</pre></div></div></li><li class="listitem"><p class="simpara">
							Create a pod with a specific label in the pod spec:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file with the following content:
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										You cannot add an affinity directly to a scheduled pod.
									</p></div></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: s1
spec:
  affinity: <span id="CO86-1"><!--Empty--></span><span class="callout">1</span>
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution: <span id="CO86-2"><!--Empty--></span><span class="callout">2</span>
        nodeSelectorTerms:
        - matchExpressions:
          - key: e2e-az-name <span id="CO86-3"><!--Empty--></span><span class="callout">3</span>
            values:
            - e2e-az1
            - e2e-az2
            operator: In <span id="CO86-4"><!--Empty--></span><span class="callout">4</span>
#...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO86-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Adds a pod affinity.
										</div></dd><dt><a href="#CO86-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Configures the <code class="literal">requiredDuringSchedulingIgnoredDuringExecution</code> parameter.
										</div></dd><dt><a href="#CO86-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Specifies the <code class="literal">key</code> and <code class="literal">values</code> that must be met. If you want the new pod to be scheduled on the node you edited, use the same <code class="literal">key</code> and <code class="literal">values</code> parameters as the label in the node.
										</div></dd><dt><a href="#CO86-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Specifies an <code class="literal">operator</code>. The operator can be <code class="literal">In</code>, <code class="literal">NotIn</code>, <code class="literal">Exists</code>, or <code class="literal">DoesNotExist</code>. For example, use the operator <code class="literal">In</code> to require the label to be in the node.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Create the pod:
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre></li></ol></div></li></ol></div></section><section class="section" id="nodes-scheduler-node-affinity-configuring-preferred_nodes-scheduler-node-affinity"><div class="titlepage"><div><div><h3 class="title">4.4.3. Configuring a preferred node affinity rule</h3></div></div></div><p>
					Preferred rules specify that, if the rule is met, the scheduler tries to enforce the rules, but does not guarantee enforcement.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						The following steps demonstrate a simple configuration that creates a node and a pod that the scheduler tries to place on the node.
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add a label to a node using the <code class="literal">oc label node</code> command:
						</p><pre class="programlisting language-terminal">$ oc label node node1 e2e-az-name=e2e-az3</pre></li><li class="listitem"><p class="simpara">
							Create a pod with a specific label:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file with the following content:
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										You cannot add an affinity directly to a scheduled pod.
									</p></div></div><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: s1
spec:
  affinity: <span id="CO87-1"><!--Empty--></span><span class="callout">1</span>
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution: <span id="CO87-2"><!--Empty--></span><span class="callout">2</span>
      - weight: <span id="CO87-3"><!--Empty--></span><span class="callout">3</span>
        preference:
          matchExpressions:
          - key: e2e-az-name <span id="CO87-4"><!--Empty--></span><span class="callout">4</span>
            values:
            - e2e-az3
            operator: In <span id="CO87-5"><!--Empty--></span><span class="callout">5</span>
#...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO87-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Adds a pod affinity.
										</div></dd><dt><a href="#CO87-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Configures the <code class="literal">preferredDuringSchedulingIgnoredDuringExecution</code> parameter.
										</div></dd><dt><a href="#CO87-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Specifies a weight for the node, as a number 1-100. The node with highest weight is preferred.
										</div></dd><dt><a href="#CO87-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Specifies the <code class="literal">key</code> and <code class="literal">values</code> that must be met. If you want the new pod to be scheduled on the node you edited, use the same <code class="literal">key</code> and <code class="literal">values</code> parameters as the label in the node.
										</div></dd><dt><a href="#CO87-5"><span class="callout">5</span></a> </dt><dd><div class="para">
											Specifies an <code class="literal">operator</code>. The operator can be <code class="literal">In</code>, <code class="literal">NotIn</code>, <code class="literal">Exists</code>, or <code class="literal">DoesNotExist</code>. For example, use the operator <code class="literal">In</code> to require the label to be in the node.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Create the pod.
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre></li></ol></div></li></ol></div></section><section class="section" id="nodes-scheduler-node-affinity-example_nodes-scheduler-node-affinity"><div class="titlepage"><div><div><h3 class="title">4.4.4. Sample node affinity rules</h3></div></div></div><p>
					The following examples demonstrate node affinity.
				</p><section class="section" id="admin-guide-sched-affinity-examples1_nodes-scheduler-node-affinity"><div class="titlepage"><div><div><h4 class="title">4.4.4.1. Node affinity with matching labels</h4></div></div></div><p>
						The following example demonstrates node affinity for a node and pod with matching labels:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								The Node1 node has the label <code class="literal">zone:us</code>:
							</p><pre class="programlisting language-terminal">$ oc label node node1 zone=us</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
								You can alternatively apply the following YAML to add the label:
							</p><pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  name: &lt;node_name&gt;
  labels:
    zone: us
#...</pre></div></div></li><li class="listitem"><p class="simpara">
								The pod-s1 pod has the <code class="literal">zone</code> and <code class="literal">us</code> key/value pair under a required node affinity rule:
							</p><pre class="programlisting language-terminal">$ cat pod-s1.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod-s1
spec:
  containers:
    - image: "docker.io/ocpqe/hello-pod"
      name: hello-pod
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
            - key: "zone"
              operator: In
              values:
              - us
#...</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								The pod-s1 pod can be scheduled on Node1:
							</p><pre class="programlisting language-terminal">$ oc get pod -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME     READY     STATUS       RESTARTS   AGE      IP      NODE
pod-s1   1/1       Running      0          4m       IP1     node1</pre>

								</p></div></li></ul></div></section><section class="section" id="admin-guide-sched-affinity-examples2_nodes-scheduler-node-affinity"><div class="titlepage"><div><div><h4 class="title">4.4.4.2. Node affinity with no matching labels</h4></div></div></div><p>
						The following example demonstrates node affinity for a node and pod without matching labels:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								The Node1 node has the label <code class="literal">zone:emea</code>:
							</p><pre class="programlisting language-terminal">$ oc label node node1 zone=emea</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
								You can alternatively apply the following YAML to add the label:
							</p><pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  name: &lt;node_name&gt;
  labels:
    zone: emea
#...</pre></div></div></li><li class="listitem"><p class="simpara">
								The pod-s1 pod has the <code class="literal">zone</code> and <code class="literal">us</code> key/value pair under a required node affinity rule:
							</p><pre class="programlisting language-terminal">$ cat pod-s1.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod-s1
spec:
  containers:
    - image: "docker.io/ocpqe/hello-pod"
      name: hello-pod
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
            - key: "zone"
              operator: In
              values:
              - us
#...</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								The pod-s1 pod cannot be scheduled on Node1:
							</p><pre class="programlisting language-terminal">$ oc describe pod pod-s1</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">...

Events:
 FirstSeen LastSeen Count From              SubObjectPath  Type                Reason
 --------- -------- ----- ----              -------------  --------            ------
 1m        33s      8     default-scheduler Warning        FailedScheduling    No nodes are available that match all of the following predicates:: MatchNodeSelector (1).</pre>

								</p></div></li></ul></div></section></section><section class="section" id="olm-overriding-operator-pod-affinity_nodes-scheduler-node-affinity"><div class="titlepage"><div><div><h3 class="title">4.4.5. Using node affinity to control where an Operator is installed</h3></div></div></div><p>
					By default, when you install an Operator, OpenShift Container Platform installs the Operator pod to one of your worker nodes randomly. However, there might be situations where you want that pod scheduled on a specific node or set of nodes.
				</p><p>
					The following examples describe situations where you might want to schedule an Operator pod to a specific node or set of nodes:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							If an Operator requires a particular platform, such as <code class="literal">amd64</code> or <code class="literal">arm64</code>
						</li><li class="listitem">
							If an Operator requires a particular operating system, such as Linux or Windows
						</li><li class="listitem">
							If you want Operators that work together scheduled on the same host or on hosts located on the same rack
						</li><li class="listitem">
							If you want Operators dispersed throughout the infrastructure to avoid downtime due to network or hardware issues
						</li></ul></div><p>
					You can control where an Operator pod is installed by adding a node affinity constraints to the Operator’s <code class="literal">Subscription</code> object.
				</p><p>
					The following examples show how to use node affinity to install an instance of the Custom Metrics Autoscaler Operator to a specific node in the cluster:
				</p><div class="formalpara"><p class="title"><strong>Node affinity example that places the Operator pod on a specific node</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: openshift-custom-metrics-autoscaler-operator
  namespace: openshift-keda
spec:
  name: my-package
  source: my-operators
  sourceNamespace: operator-registries
  config:
    affinity:
      nodeAffinity: <span id="CO88-1"><!--Empty--></span><span class="callout">1</span>
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - ip-10-0-163-94.us-west-2.compute.internal
#...</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO88-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							A node affinity that requires the Operator’s pod to be scheduled on a node named <code class="literal">ip-10-0-163-94.us-west-2.compute.internal</code>.
						</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Node affinity example that places the Operator pod on a node with a specific platform</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: openshift-custom-metrics-autoscaler-operator
  namespace: openshift-keda
spec:
  name: my-package
  source: my-operators
  sourceNamespace: operator-registries
  config:
    affinity:
      nodeAffinity: <span id="CO89-1"><!--Empty--></span><span class="callout">1</span>
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/arch
              operator: In
              values:
              - arm64
            - key: kubernetes.io/os
              operator: In
              values:
              - linux
#...</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO89-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							A node affinity that requires the Operator’s pod to be scheduled on a node with the <code class="literal">kubernetes.io/arch=arm64</code> and <code class="literal">kubernetes.io/os=linux</code> labels.
						</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To control the placement of an Operator pod, complete the following steps:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Install the Operator as usual.
						</li><li class="listitem">
							If needed, ensure that your nodes are labeled to properly respond to the affinity.
						</li><li class="listitem"><p class="simpara">
							Edit the Operator <code class="literal">Subscription</code> object to add an affinity:
						</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: openshift-custom-metrics-autoscaler-operator
  namespace: openshift-keda
spec:
  name: my-package
  source: my-operators
  sourceNamespace: operator-registries
  config:
    affinity: <span id="CO90-1"><!--Empty--></span><span class="callout">1</span>
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - ip-10-0-185-229.ec2.internal
#...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO90-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add a <code class="literal">nodeAffinity</code>.
								</div></dd></dl></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To ensure that the pod is deployed on the specific node, run the following command:
						</p><pre class="programlisting language-yaml">$ oc get pods -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                                  READY   STATUS    RESTARTS   AGE   IP            NODE                           NOMINATED NODE   READINESS GATES
custom-metrics-autoscaler-operator-5dcc45d656-bhshg   1/1     Running   0          50s   10.131.0.20   ip-10-0-185-229.ec2.internal   &lt;none&gt;           &lt;none&gt;</pre>

							</p></div></li></ul></div></section><section class="section _additional-resources" id="nodes-scheduler-node-affinity-addtl-resources_nodes-scheduler-node-affinity"><div class="titlepage"><div><div><h3 class="title">4.4.6. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-working-updating_nodes-nodes-working">Understanding how to update labels on nodes</a>
						</li></ul></div></section></section><section class="section" id="nodes-scheduler-overcommit"><div class="titlepage"><div><div><h2 class="title">4.5. Placing pods onto overcommited nodes</h2></div></div></div><p>
				In an <span class="emphasis"><em>overcommited</em></span> state, the sum of the container compute resource requests and limits exceeds the resources available on the system. Overcommitment might be desirable in development environments where a trade-off of guaranteed performance for capacity is acceptable.
			</p><p>
				Requests and limits enable administrators to allow and manage the overcommitment of resources on a node. The scheduler uses requests for scheduling your container and providing a minimum service guarantee. Limits constrain the amount of compute resource that may be consumed on your node.
			</p><section class="section" id="nodes-cluster-overcommit-about_nodes-scheduler-overcommit"><div class="titlepage"><div><div><h3 class="title">4.5.1. Understanding overcommitment</h3></div></div></div><p>
					Requests and limits enable administrators to allow and manage the overcommitment of resources on a node. The scheduler uses requests for scheduling your container and providing a minimum service guarantee. Limits constrain the amount of compute resource that may be consumed on your node.
				</p><p>
					OpenShift Container Platform administrators can control the level of overcommit and manage container density on nodes by configuring masters to override the ratio between request and limit set on developer containers. In conjunction with a per-project <code class="literal">LimitRange</code> object specifying limits and defaults, this adjusts the container limit and request to achieve the desired level of overcommit.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						That these overrides have no effect if no limits have been set on containers. Create a <code class="literal">LimitRange</code> object with default limits, per individual project, or in the project template, to ensure that the overrides apply.
					</p></div></div><p>
					After these overrides, the container limits and requests must still be validated by any <code class="literal">LimitRange</code> object in the project. It is possible, for example, for developers to specify a limit close to the minimum limit, and have the request then be overridden below the minimum limit, causing the pod to be forbidden. This unfortunate user experience should be addressed with future work, but for now, configure this capability and <code class="literal">LimitRange</code> objects with caution.
				</p></section><section class="section" id="nodes-cluster-overcommit-configure-nodes_nodes-scheduler-overcommit"><div class="titlepage"><div><div><h3 class="title">4.5.2. Understanding nodes overcommitment</h3></div></div></div><p>
					In an overcommitted environment, it is important to properly configure your node to provide best system behavior.
				</p><p>
					When the node starts, it ensures that the kernel tunable flags for memory management are set properly. The kernel should never fail memory allocations unless it runs out of physical memory.
				</p><p>
					To ensure this behavior, OpenShift Container Platform configures the kernel to always overcommit memory by setting the <code class="literal">vm.overcommit_memory</code> parameter to <code class="literal">1</code>, overriding the default operating system setting.
				</p><p>
					OpenShift Container Platform also configures the kernel not to panic when it runs out of memory by setting the <code class="literal">vm.panic_on_oom</code> parameter to <code class="literal">0</code>. A setting of 0 instructs the kernel to call oom_killer in an Out of Memory (OOM) condition, which kills processes based on priority
				</p><p>
					You can view the current setting by running the following commands on your nodes:
				</p><pre class="programlisting language-terminal">$ sysctl -a |grep commit</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">#...
vm.overcommit_memory = 0
#...</pre>

					</p></div><pre class="programlisting language-terminal">$ sysctl -a |grep panic</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">#...
vm.panic_on_oom = 0
#...</pre>

					</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The above flags should already be set on nodes, and no further action is required.
					</p></div></div><p>
					You can also perform the following configurations for each node:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Disable or enforce CPU limits using CPU CFS quotas
						</li><li class="listitem">
							Reserve resources for system processes
						</li><li class="listitem">
							Reserve memory across quality of service tiers
						</li></ul></div></section></section><section class="section" id="nodes-scheduler-taints-tolerations"><div class="titlepage"><div><div><h2 class="title">4.6. Controlling pod placement using node taints</h2></div></div></div><p>
				Taints and tolerations allow the node to control which pods should (or should not) be scheduled on them.
			</p><section class="section" id="nodes-scheduler-taints-tolerations-about_nodes-scheduler-taints-tolerations"><div class="titlepage"><div><div><h3 class="title">4.6.1. Understanding taints and tolerations</h3></div></div></div><p>
					A <span class="emphasis"><em>taint</em></span> allows a node to refuse a pod to be scheduled unless that pod has a matching <span class="emphasis"><em>toleration</em></span>.
				</p><p>
					You apply taints to a node through the <code class="literal">Node</code> specification (<code class="literal">NodeSpec</code>) and apply tolerations to a pod through the <code class="literal">Pod</code> specification (<code class="literal">PodSpec</code>). When you apply a taint a node, the scheduler cannot place a pod on that node unless the pod can tolerate the taint.
				</p><div class="formalpara"><p class="title"><strong>Example taint in a node specification</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: Node
metadata:
  name: my-node
#...
spec:
  taints:
  - effect: NoExecute
    key: key1
    value: value1
#...</pre>

					</p></div><div class="formalpara"><p class="title"><strong>Example toleration in a <code class="literal">Pod</code> spec</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoExecute"
    tolerationSeconds: 3600
#...</pre>

					</p></div><p>
					Taints and tolerations consist of a key, value, and effect.
				</p><div class="table" id="taint-components-table_nodes-scheduler-taints-tolerations"><p class="title"><strong>Table 4.1. Taint and toleration components</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 27%; " class="col_1"><!--Empty--></col><col style="width: 73%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232251991408" scope="col">Parameter</th><th align="left" valign="top" id="idm140232251990320" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232251991408"> <p>
									<code class="literal">key</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232251990320"> <p>
									The <code class="literal">key</code> is any string, up to 253 characters. The key must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232251991408"> <p>
									<code class="literal">value</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232251990320"> <p>
									The <code class="literal">value</code> is any string, up to 63 characters. The value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232251991408"> <p>
									<code class="literal">effect</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232251990320"> <p>
									The effect is one of the following:
								</p>
								 <div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 40%; " class="col_1"><!--Empty--></col><col style="width: 60%; " class="col_2"><!--Empty--></col></colgroup><tbody><tr><td align="left" valign="top" headers="idm140232251991408"> <p>
													<code class="literal">NoSchedule</code> <sup>[1]</sup>
												</p>
												 </td><td align="left" valign="top" headers="idm140232251990320"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
															New pods that do not match the taint are not scheduled onto that node.
														</li><li class="listitem">
															Existing pods on the node remain.
														</li></ul></div>
												 </td></tr><tr><td align="left" valign="top" headers="idm140232251991408"> <p>
													<code class="literal">PreferNoSchedule</code>
												</p>
												 </td><td align="left" valign="top" headers="idm140232251990320"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
															New pods that do not match the taint might be scheduled onto that node, but the scheduler tries not to.
														</li><li class="listitem">
															Existing pods on the node remain.
														</li></ul></div>
												 </td></tr><tr><td align="left" valign="top" headers="idm140232251991408"> <p>
													<code class="literal">NoExecute</code>
												</p>
												 </td><td align="left" valign="top" headers="idm140232251990320"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
															New pods that do not match the taint cannot be scheduled onto that node.
														</li><li class="listitem">
															Existing pods on the node that do not have a matching toleration are removed.
														</li></ul></div>
												 </td></tr></tbody></table></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232251991408"> <p>
									<code class="literal">operator</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232251990320"> <div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 40%; " class="col_1"><!--Empty--></col><col style="width: 60%; " class="col_2"><!--Empty--></col></colgroup><tbody><tr><td align="left" valign="top" headers="idm140232251991408"> <p>
													<code class="literal">Equal</code>
												</p>
												 </td><td align="left" valign="top" headers="idm140232251990320"> <p>
													The <code class="literal">key</code>/<code class="literal">value</code>/<code class="literal">effect</code> parameters must match. This is the default.
												</p>
												 </td></tr><tr><td align="left" valign="top" headers="idm140232251991408"> <p>
													<code class="literal">Exists</code>
												</p>
												 </td><td align="left" valign="top" headers="idm140232251990320"> <p>
													The <code class="literal">key</code>/<code class="literal">effect</code> parameters must match. You must leave a blank <code class="literal">value</code> parameter, which matches any.
												</p>
												 </td></tr></tbody></table></div>
								 </td></tr></tbody></table></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							If you add a <code class="literal">NoSchedule</code> taint to a control plane node, the node must have the <code class="literal">node-role.kubernetes.io/master=:NoSchedule</code> taint, which is added by default.
						</p><p class="simpara">
							For example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Node
metadata:
  annotations:
    machine.openshift.io/machine: openshift-machine-api/ci-ln-62s7gtb-f76d1-v8jxv-master-0
    machineconfiguration.openshift.io/currentConfig: rendered-master-cdc1ab7da414629332cc4c3926e6e59c
  name: my-node
#...
spec:
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
#...</pre></li></ol></div><p>
					A toleration matches a taint:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							If the <code class="literal">operator</code> parameter is set to <code class="literal">Equal</code>:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									the <code class="literal">key</code> parameters are the same;
								</li><li class="listitem">
									the <code class="literal">value</code> parameters are the same;
								</li><li class="listitem">
									the <code class="literal">effect</code> parameters are the same.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							If the <code class="literal">operator</code> parameter is set to <code class="literal">Exists</code>:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									the <code class="literal">key</code> parameters are the same;
								</li><li class="listitem">
									the <code class="literal">effect</code> parameters are the same.
								</li></ul></div></li></ul></div><p>
					The following taints are built into OpenShift Container Platform:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">node.kubernetes.io/not-ready</code>: The node is not ready. This corresponds to the node condition <code class="literal">Ready=False</code>.
						</li><li class="listitem">
							<code class="literal">node.kubernetes.io/unreachable</code>: The node is unreachable from the node controller. This corresponds to the node condition <code class="literal">Ready=Unknown</code>.
						</li><li class="listitem">
							<code class="literal">node.kubernetes.io/memory-pressure</code>: The node has memory pressure issues. This corresponds to the node condition <code class="literal">MemoryPressure=True</code>.
						</li><li class="listitem">
							<code class="literal">node.kubernetes.io/disk-pressure</code>: The node has disk pressure issues. This corresponds to the node condition <code class="literal">DiskPressure=True</code>.
						</li><li class="listitem">
							<code class="literal">node.kubernetes.io/network-unavailable</code>: The node network is unavailable.
						</li><li class="listitem">
							<code class="literal">node.kubernetes.io/unschedulable</code>: The node is unschedulable.
						</li><li class="listitem">
							<code class="literal">node.cloudprovider.kubernetes.io/uninitialized</code>: When the node controller is started with an external cloud provider, this taint is set on a node to mark it as unusable. After a controller from the cloud-controller-manager initializes this node, the kubelet removes this taint.
						</li><li class="listitem"><p class="simpara">
							<code class="literal">node.kubernetes.io/pid-pressure</code>: The node has pid pressure. This corresponds to the node condition <code class="literal">PIDPressure=True</code>.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								OpenShift Container Platform does not set a default pid.available <code class="literal">evictionHard</code>.
							</p></div></div></li></ul></div><section class="section" id="nodes-scheduler-taints-tolerations-about-seconds_nodes-scheduler-taints-tolerations"><div class="titlepage"><div><div><h4 class="title">4.6.1.1. Understanding how to use toleration seconds to delay pod evictions</h4></div></div></div><p>
						You can specify how long a pod can remain bound to a node before being evicted by specifying the <code class="literal">tolerationSeconds</code> parameter in the <code class="literal">Pod</code> specification or <code class="literal">MachineSet</code> object. If a taint with the <code class="literal">NoExecute</code> effect is added to a node, a pod that does tolerate the taint, which has the <code class="literal">tolerationSeconds</code> parameter, the pod is not evicted until that time period expires.
					</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoExecute"
    tolerationSeconds: 3600
#...</pre>

						</p></div><p>
						Here, if this pod is running but does not have a matching toleration, the pod stays bound to the node for 3,600 seconds and then be evicted. If the taint is removed before that time, the pod is not evicted.
					</p></section><section class="section" id="nodes-scheduler-taints-tolerations-about-multiple_nodes-scheduler-taints-tolerations"><div class="titlepage"><div><div><h4 class="title">4.6.1.2. Understanding how to use multiple taints</h4></div></div></div><p>
						You can put multiple taints on the same node and multiple tolerations on the same pod. OpenShift Container Platform processes multiple taints and tolerations as follows:
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								Process the taints for which the pod has a matching toleration.
							</li><li class="listitem"><p class="simpara">
								The remaining unmatched taints have the indicated effects on the pod:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										If there is at least one unmatched taint with effect <code class="literal">NoSchedule</code>, OpenShift Container Platform cannot schedule a pod onto that node.
									</li><li class="listitem">
										If there is no unmatched taint with effect <code class="literal">NoSchedule</code> but there is at least one unmatched taint with effect <code class="literal">PreferNoSchedule</code>, OpenShift Container Platform tries to not schedule the pod onto the node.
									</li><li class="listitem"><p class="simpara">
										If there is at least one unmatched taint with effect <code class="literal">NoExecute</code>, OpenShift Container Platform evicts the pod from the node if it is already running on the node, or the pod is not scheduled onto the node if it is not yet running on the node.
									</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
												Pods that do not tolerate the taint are evicted immediately.
											</li><li class="listitem">
												Pods that tolerate the taint without specifying <code class="literal">tolerationSeconds</code> in their <code class="literal">Pod</code> specification remain bound forever.
											</li><li class="listitem">
												Pods that tolerate the taint with a specified <code class="literal">tolerationSeconds</code> remain bound for the specified amount of time.
											</li></ul></div></li></ul></div></li></ol></div><p>
						For example:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Add the following taints to the node:
							</p><pre class="programlisting language-terminal">$ oc adm taint nodes node1 key1=value1:NoSchedule</pre><pre class="programlisting language-terminal">$ oc adm taint nodes node1 key1=value1:NoExecute</pre><pre class="programlisting language-terminal">$ oc adm taint nodes node1 key2=value2:NoSchedule</pre></li><li class="listitem"><p class="simpara">
								The pod has the following tolerations:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoSchedule"
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoExecute"
#...</pre></li></ul></div><p>
						In this case, the pod cannot be scheduled onto the node, because there is no toleration matching the third taint. The pod continues running if it is already running on the node when the taint is added, because the third taint is the only one of the three that is not tolerated by the pod.
					</p></section><section class="section" id="nodes-scheduler-taints-tolerations-about-taintNodesByCondition_nodes-scheduler-taints-tolerations"><div class="titlepage"><div><div><h4 class="title">4.6.1.3. Understanding pod scheduling and node conditions (taint node by condition)</h4></div></div></div><p>
						The Taint Nodes By Condition feature, which is enabled by default, automatically taints nodes that report conditions such as memory pressure and disk pressure. If a node reports a condition, a taint is added until the condition clears. The taints have the <code class="literal">NoSchedule</code> effect, which means no pod can be scheduled on the node unless the pod has a matching toleration.
					</p><p>
						The scheduler checks for these taints on nodes before scheduling pods. If the taint is present, the pod is scheduled on a different node. Because the scheduler checks for taints and not the actual node conditions, you configure the scheduler to ignore some of these node conditions by adding appropriate pod tolerations.
					</p><p>
						To ensure backward compatibility, the daemon set controller automatically adds the following tolerations to all daemons:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								node.kubernetes.io/memory-pressure
							</li><li class="listitem">
								node.kubernetes.io/disk-pressure
							</li><li class="listitem">
								node.kubernetes.io/unschedulable (1.10 or later)
							</li><li class="listitem">
								node.kubernetes.io/network-unavailable (host network only)
							</li></ul></div><p>
						You can also add arbitrary tolerations to daemon sets.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The control plane also adds the <code class="literal">node.kubernetes.io/memory-pressure</code> toleration on pods that have a QoS class. This is because Kubernetes manages pods in the <code class="literal">Guaranteed</code> or <code class="literal">Burstable</code> QoS classes. The new <code class="literal">BestEffort</code> pods do not get scheduled onto the affected node.
						</p></div></div></section><section class="section" id="nodes-scheduler-taints-tolerations-about-taintBasedEvictions_nodes-scheduler-taints-tolerations"><div class="titlepage"><div><div><h4 class="title">4.6.1.4. Understanding evicting pods by condition (taint-based evictions)</h4></div></div></div><p>
						The Taint-Based Evictions feature, which is enabled by default, evicts pods from a node that experiences specific conditions, such as <code class="literal">not-ready</code> and <code class="literal">unreachable</code>. When a node experiences one of these conditions, OpenShift Container Platform automatically adds taints to the node, and starts evicting and rescheduling the pods on different nodes.
					</p><p>
						Taint Based Evictions have a <code class="literal">NoExecute</code> effect, where any pod that does not tolerate the taint is evicted immediately and any pod that does tolerate the taint will never be evicted, unless the pod uses the <code class="literal">tolerationSeconds</code> parameter.
					</p><p>
						The <code class="literal">tolerationSeconds</code> parameter allows you to specify how long a pod stays bound to a node that has a node condition. If the condition still exists after the <code class="literal">tolerationSeconds</code> period, the taint remains on the node and the pods with a matching toleration are evicted. If the condition clears before the <code class="literal">tolerationSeconds</code> period, pods with matching tolerations are not removed.
					</p><p>
						If you use the <code class="literal">tolerationSeconds</code> parameter with no value, pods are never evicted because of the not ready and unreachable node conditions.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							OpenShift Container Platform evicts pods in a rate-limited way to prevent massive pod evictions in scenarios such as the master becoming partitioned from the nodes.
						</p><p>
							By default, if more than 55% of nodes in a given zone are unhealthy, the node lifecycle controller changes that zone’s state to <code class="literal">PartialDisruption</code> and the rate of pod evictions is reduced. For small clusters (by default, 50 nodes or less) in this state, nodes in this zone are not tainted and evictions are stopped.
						</p><p>
							For more information, see <a class="link" href="https://kubernetes.io/docs/concepts/architecture/nodes/#rate-limits-on-eviction">Rate limits on eviction</a> in the Kubernetes documentation.
						</p></div></div><p>
						OpenShift Container Platform automatically adds a toleration for <code class="literal">node.kubernetes.io/not-ready</code> and <code class="literal">node.kubernetes.io/unreachable</code> with <code class="literal">tolerationSeconds=300</code>, unless the <code class="literal">Pod</code> configuration specifies either toleration.
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: node.kubernetes.io/not-ready
    operator: Exists
    effect: NoExecute
    tolerationSeconds: 300 <span id="CO91-1"><!--Empty--></span><span class="callout">1</span>
  - key: node.kubernetes.io/unreachable
    operator: Exists
    effect: NoExecute
    tolerationSeconds: 300
#...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO91-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								These tolerations ensure that the default pod behavior is to remain bound for five minutes after one of these node conditions problems is detected.
							</div></dd></dl></div><p>
						You can configure these tolerations as needed. For example, if you have an application with a lot of local state, you might want to keep the pods bound to node for a longer time in the event of network partition, allowing for the partition to recover and avoiding pod eviction.
					</p><p>
						Pods spawned by a daemon set are created with <code class="literal">NoExecute</code> tolerations for the following taints with no <code class="literal">tolerationSeconds</code>:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<code class="literal">node.kubernetes.io/unreachable</code>
							</li><li class="listitem">
								<code class="literal">node.kubernetes.io/not-ready</code>
							</li></ul></div><p>
						As a result, daemon set pods are never evicted because of these node conditions.
					</p></section><section class="section" id="nodes-scheduler-taints-tolerations-all_nodes-scheduler-taints-tolerations"><div class="titlepage"><div><div><h4 class="title">4.6.1.5. Tolerating all taints</h4></div></div></div><p>
						You can configure a pod to tolerate all taints by adding an <code class="literal">operator: "Exists"</code> toleration with no <code class="literal">key</code> and <code class="literal">values</code> parameters. Pods with this toleration are not removed from a node that has taints.
					</p><div class="formalpara"><p class="title"><strong><code class="literal">Pod</code> spec for tolerating all taints</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - operator: "Exists"
#...</pre>

						</p></div></section></section><section class="section" id="nodes-scheduler-taints-tolerations-adding_nodes-scheduler-taints-tolerations"><div class="titlepage"><div><div><h3 class="title">4.6.2. Adding taints and tolerations</h3></div></div></div><p>
					You add tolerations to pods and taints to nodes to allow the node to control which pods should or should not be scheduled on them. For existing pods and nodes, you should add the toleration to the pod first, then add the taint to the node to avoid pods being removed from the node before you can add the toleration.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add a toleration to a pod by editing the <code class="literal">Pod</code> spec to include a <code class="literal">tolerations</code> stanza:
						</p><div class="formalpara"><p class="title"><strong>Sample pod configuration file with an Equal operator</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key1" <span id="CO92-1"><!--Empty--></span><span class="callout">1</span>
    value: "value1"
    operator: "Equal"
    effect: "NoExecute"
    tolerationSeconds: 3600 <span id="CO92-2"><!--Empty--></span><span class="callout">2</span>
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO92-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The toleration parameters, as described in the <span class="strong strong"><strong>Taint and toleration components</strong></span> table.
								</div></dd><dt><a href="#CO92-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The <code class="literal">tolerationSeconds</code> parameter specifies how long a pod can remain bound to a node before being evicted.
								</div></dd></dl></div><p class="simpara">
							For example:
						</p><div class="formalpara"><p class="title"><strong>Sample pod configuration file with an Exists operator</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
   tolerations:
    - key: "key1"
      operator: "Exists" <span id="CO93-1"><!--Empty--></span><span class="callout">1</span>
      effect: "NoExecute"
      tolerationSeconds: 3600
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO93-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal">Exists</code> operator does not take a <code class="literal">value</code>.
								</div></dd></dl></div><p class="simpara">
							This example places a taint on <code class="literal">node1</code> that has key <code class="literal">key1</code>, value <code class="literal">value1</code>, and taint effect <code class="literal">NoExecute</code>.
						</p></li><li class="listitem"><p class="simpara">
							Add a taint to a node by using the following command with the parameters described in the <span class="strong strong"><strong>Taint and toleration components</strong></span> table:
						</p><pre class="programlisting language-terminal">$ oc adm taint nodes &lt;node_name&gt; &lt;key&gt;=&lt;value&gt;:&lt;effect&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc adm taint nodes node1 key1=value1:NoExecute</pre><p class="simpara">
							This command places a taint on <code class="literal">node1</code> that has key <code class="literal">key1</code>, value <code class="literal">value1</code>, and effect <code class="literal">NoExecute</code>.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If you add a <code class="literal">NoSchedule</code> taint to a control plane node, the node must have the <code class="literal">node-role.kubernetes.io/master=:NoSchedule</code> taint, which is added by default.
							</p><p>
								For example:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Node
metadata:
  annotations:
    machine.openshift.io/machine: openshift-machine-api/ci-ln-62s7gtb-f76d1-v8jxv-master-0
    machineconfiguration.openshift.io/currentConfig: rendered-master-cdc1ab7da414629332cc4c3926e6e59c
  name: my-node
#...
spec:
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
#...</pre></div></div><p class="simpara">
							The tolerations on the pod match the taint on the node. A pod with either toleration can be scheduled onto <code class="literal">node1</code>.
						</p></li></ol></div><section class="section" id="nodes-scheduler-taints-tolerations-adding-machineset_nodes-scheduler-taints-tolerations"><div class="titlepage"><div><div><h4 class="title">4.6.2.1. Adding taints and tolerations using a compute machine set</h4></div></div></div><p>
						You can add taints to nodes using a compute machine set. All nodes associated with the <code class="literal">MachineSet</code> object are updated with the taint. Tolerations respond to taints added by a compute machine set in the same manner as taints added directly to the nodes.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Add a toleration to a pod by editing the <code class="literal">Pod</code> spec to include a <code class="literal">tolerations</code> stanza:
							</p><div class="formalpara"><p class="title"><strong>Sample pod configuration file with <code class="literal">Equal</code> operator</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key1" <span id="CO94-1"><!--Empty--></span><span class="callout">1</span>
    value: "value1"
    operator: "Equal"
    effect: "NoExecute"
    tolerationSeconds: 3600 <span id="CO94-2"><!--Empty--></span><span class="callout">2</span>
#...</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO94-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The toleration parameters, as described in the <span class="strong strong"><strong>Taint and toleration components</strong></span> table.
									</div></dd><dt><a href="#CO94-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The <code class="literal">tolerationSeconds</code> parameter specifies how long a pod is bound to a node before being evicted.
									</div></dd></dl></div><p class="simpara">
								For example:
							</p><div class="formalpara"><p class="title"><strong>Sample pod configuration file with <code class="literal">Exists</code> operator</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key1"
    operator: "Exists"
    effect: "NoExecute"
    tolerationSeconds: 3600
#...</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Add the taint to the <code class="literal">MachineSet</code> object:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Edit the <code class="literal">MachineSet</code> YAML for the nodes you want to taint or you can create a new <code class="literal">MachineSet</code> object:
									</p><pre class="programlisting language-terminal">$ oc edit machineset &lt;machineset&gt;</pre></li><li class="listitem"><p class="simpara">
										Add the taint to the <code class="literal">spec.template.spec</code> section:
									</p><div class="formalpara"><p class="title"><strong>Example taint in a compute machine set specification</strong></p><p>
											
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: my-machineset
#...
spec:
#...
  template:
#...
    spec:
      taints:
      - effect: NoExecute
        key: key1
        value: value1
#...</pre>

										</p></div><p class="simpara">
										This example places a taint that has the key <code class="literal">key1</code>, value <code class="literal">value1</code>, and taint effect <code class="literal">NoExecute</code> on the nodes.
									</p></li><li class="listitem"><p class="simpara">
										Scale down the compute machine set to 0:
									</p><pre class="programlisting language-terminal">$ oc scale --replicas=0 machineset &lt;machineset&gt; -n openshift-machine-api</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
										You can alternatively apply the following YAML to scale the compute machine set:
									</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: &lt;machineset&gt;
  namespace: openshift-machine-api
spec:
  replicas: 0</pre></div></div><p class="simpara">
										Wait for the machines to be removed.
									</p></li><li class="listitem"><p class="simpara">
										Scale up the compute machine set as needed:
									</p><pre class="programlisting language-terminal">$ oc scale --replicas=2 machineset &lt;machineset&gt; -n openshift-machine-api</pre><p class="simpara">
										Or:
									</p><pre class="programlisting language-terminal">$ oc edit machineset &lt;machineset&gt; -n openshift-machine-api</pre><p class="simpara">
										Wait for the machines to start. The taint is added to the nodes associated with the <code class="literal">MachineSet</code> object.
									</p></li></ol></div></li></ol></div></section><section class="section" id="nodes-scheduler-taints-tolerations-bindings_nodes-scheduler-taints-tolerations"><div class="titlepage"><div><div><h4 class="title">4.6.2.2. Binding a user to a node using taints and tolerations</h4></div></div></div><p>
						If you want to dedicate a set of nodes for exclusive use by a particular set of users, add a toleration to their pods. Then, add a corresponding taint to those nodes. The pods with the tolerations are allowed to use the tainted nodes or any other nodes in the cluster.
					</p><p>
						If you want ensure the pods are scheduled to only those tainted nodes, also add a label to the same set of nodes and add a node affinity to the pods so that the pods can only be scheduled onto nodes with that label.
					</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To configure a node so that users can use only that node:
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Add a corresponding taint to those nodes:
							</p><p class="simpara">
								For example:
							</p><pre class="programlisting language-terminal">$ oc adm taint nodes node1 dedicated=groupName:NoSchedule</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
								You can alternatively apply the following YAML to add the taint:
							</p><pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  name: my-node
#...
spec:
  taints:
    - key: dedicated
      value: groupName
      effect: NoSchedule
#...</pre></div></div></li><li class="listitem">
								Add a toleration to the pods by writing a custom admission controller.
							</li></ol></div></section><section class="section" id="nodes-scheduler-taints-tolerations-projects_nodes-scheduler-taints-tolerations"><div class="titlepage"><div><div><h4 class="title">4.6.2.3. Creating a project with a node selector and toleration</h4></div></div></div><p>
						You can create a project that uses a node selector and toleration, which are set as annotations, to control the placement of pods onto specific nodes. Any subsequent resources created in the project are then scheduled on nodes that have a taint matching the toleration.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								A label for node selection has been added to one or more nodes by using a compute machine set or editing the node directly.
							</li><li class="listitem">
								A taint has been added to one or more nodes by using a compute machine set or editing the node directly.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a <code class="literal">Project</code> resource definition, specifying a node selector and toleration in the <code class="literal">metadata.annotations</code> section:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">project.yaml</code> file</strong></p><p>
									
<pre class="programlisting language-yaml">kind: Project
apiVersion: project.openshift.io/v1
metadata:
  name: &lt;project_name&gt; <span id="CO95-1"><!--Empty--></span><span class="callout">1</span>
  annotations:
    openshift.io/node-selector: '&lt;label&gt;' <span id="CO95-2"><!--Empty--></span><span class="callout">2</span>
    scheduler.alpha.kubernetes.io/defaultTolerations: &gt;-
      [{"operator": "Exists", "effect": "NoSchedule", "key":
      "&lt;key_name&gt;"} <span id="CO95-3"><!--Empty--></span><span class="callout">3</span>
      ]</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO95-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The project name.
									</div></dd><dt><a href="#CO95-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The default node selector label.
									</div></dd><dt><a href="#CO95-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The toleration parameters, as described in the <span class="strong strong"><strong>Taint and toleration components</strong></span> table. This example uses the <code class="literal">NoSchedule</code> effect, which allows existing pods on the node to remain, and the <code class="literal">Exists</code> operator, which does not take a value.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Use the <code class="literal">oc apply</code> command to create the project:
							</p><pre class="programlisting language-terminal">$ oc apply -f project.yaml</pre></li></ol></div><p>
						Any subsequent resources created in the <code class="literal">&lt;project_name&gt;</code> namespace should now be scheduled on the specified nodes.
					</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								Adding taints and tolerations <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-taints-tolerations-adding_nodes-scheduler-taints-tolerations">manually to nodes</a> or <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-taints-tolerations-adding-machineset_nodes-scheduler-taints-tolerations">with compute machine sets</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-node-selectors-project_nodes-scheduler-node-selectors">Creating project-wide node selectors</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-pod-placement_olm-adding-operators-to-a-cluster">Pod placement of Operator workloads</a>
							</li></ul></div></section><section class="section" id="nodes-scheduler-taints-tolerations-special_nodes-scheduler-taints-tolerations"><div class="titlepage"><div><div><h4 class="title">4.6.2.4. Controlling nodes with special hardware using taints and tolerations</h4></div></div></div><p>
						In a cluster where a small subset of nodes have specialized hardware, you can use taints and tolerations to keep pods that do not need the specialized hardware off of those nodes, leaving the nodes for pods that do need the specialized hardware. You can also require pods that need specialized hardware to use specific nodes.
					</p><p>
						You can achieve this by adding a toleration to pods that need the special hardware and tainting the nodes that have the specialized hardware.
					</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To ensure nodes with specialized hardware are reserved for specific pods:
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Add a toleration to pods that need the special hardware.
							</p><p class="simpara">
								For example:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
    - key: "disktype"
      value: "ssd"
      operator: "Equal"
      effect: "NoSchedule"
      tolerationSeconds: 3600
#...</pre></li><li class="listitem"><p class="simpara">
								Taint the nodes that have the specialized hardware using one of the following commands:
							</p><pre class="programlisting language-terminal">$ oc adm taint nodes &lt;node-name&gt; disktype=ssd:NoSchedule</pre><p class="simpara">
								Or:
							</p><pre class="programlisting language-terminal">$ oc adm taint nodes &lt;node-name&gt; disktype=ssd:PreferNoSchedule</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
								You can alternatively apply the following YAML to add the taint:
							</p><pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  name: my_node
#...
spec:
  taints:
    - key: disktype
      value: ssd
      effect: PreferNoSchedule
#...</pre></div></div></li></ol></div></section></section><section class="section" id="nodes-scheduler-taints-tolerations-removing_nodes-scheduler-taints-tolerations"><div class="titlepage"><div><div><h3 class="title">4.6.3. Removing taints and tolerations</h3></div></div></div><p>
					You can remove taints from nodes and tolerations from pods as needed. You should add the toleration to the pod first, then add the taint to the node to avoid pods being removed from the node before you can add the toleration.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To remove taints and tolerations:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To remove a taint from a node:
						</p><pre class="programlisting language-terminal">$ oc adm taint nodes &lt;node-name&gt; &lt;key&gt;-</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc adm taint nodes ip-10-0-132-248.ec2.internal key1-</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">node/ip-10-0-132-248.ec2.internal untainted</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To remove a toleration from a pod, edit the <code class="literal">Pod</code> spec to remove the toleration:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key2"
    operator: "Exists"
    effect: "NoExecute"
    tolerationSeconds: 3600
#...</pre></li></ol></div></section></section><section class="section" id="nodes-scheduler-node-selectors"><div class="titlepage"><div><div><h2 class="title">4.7. Placing pods on specific nodes using node selectors</h2></div></div></div><p>
				A <span class="emphasis"><em>node selector</em></span> specifies a map of key/value pairs that are defined using custom labels on nodes and selectors specified in pods.
			</p><p>
				For the pod to be eligible to run on a node, the pod must have the same key/value node selector as the label on the node.
			</p><section class="section" id="nodes-scheduler-node-selectors-about_nodes-scheduler-node-selectors"><div class="titlepage"><div><div><h3 class="title">4.7.1. About node selectors</h3></div></div></div><p>
					You can use node selectors on pods and labels on nodes to control where the pod is scheduled. With node selectors, OpenShift Container Platform schedules the pods on nodes that contain matching labels.
				</p><p>
					You can use a node selector to place specific pods on specific nodes, cluster-wide node selectors to place new pods on specific nodes anywhere in the cluster, and project node selectors to place new pods in a project on specific nodes.
				</p><p>
					For example, as a cluster administrator, you can create an infrastructure where application developers can deploy pods only onto the nodes closest to their geographical location by including a node selector in every pod they create. In this example, the cluster consists of five data centers spread across two regions. In the U.S., label the nodes as <code class="literal">us-east</code>, <code class="literal">us-central</code>, or <code class="literal">us-west</code>. In the Asia-Pacific region (APAC), label the nodes as <code class="literal">apac-east</code> or <code class="literal">apac-west</code>. The developers can add a node selector to the pods they create to ensure the pods get scheduled on those nodes.
				</p><p>
					A pod is not scheduled if the <code class="literal">Pod</code> object contains a node selector, but no node has a matching label.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						If you are using node selectors and node affinity in the same pod configuration, the following rules control pod placement onto nodes:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								If you configure both <code class="literal">nodeSelector</code> and <code class="literal">nodeAffinity</code>, both conditions must be satisfied for the pod to be scheduled onto a candidate node.
							</li><li class="listitem">
								If you specify multiple <code class="literal">nodeSelectorTerms</code> associated with <code class="literal">nodeAffinity</code> types, then the pod can be scheduled onto a node if one of the <code class="literal">nodeSelectorTerms</code> is satisfied.
							</li><li class="listitem">
								If you specify multiple <code class="literal">matchExpressions</code> associated with <code class="literal">nodeSelectorTerms</code>, then the pod can be scheduled onto a node only if all <code class="literal">matchExpressions</code> are satisfied.
							</li></ul></div></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Node selectors on specific pods and nodes</span></dt><dd><p class="simpara">
								You can control which node a specific pod is scheduled on by using node selectors and labels.
							</p><p class="simpara">
								To use node selectors and labels, first label the node to avoid pods being descheduled, then add the node selector to the pod.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									You cannot add a node selector directly to an existing scheduled pod. You must label the object that controls the pod, such as deployment config.
								</p></div></div><p class="simpara">
								For example, the following <code class="literal">Node</code> object has the <code class="literal">region: east</code> label:
							</p><div class="formalpara"><p class="title"><strong>Sample <code class="literal">Node</code> object with a label</strong></p><p>
									
<pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  name: ip-10-0-131-14.ec2.internal
  selfLink: /api/v1/nodes/ip-10-0-131-14.ec2.internal
  uid: 7bc2580a-8b8e-11e9-8e01-021ab4174c74
  resourceVersion: '478704'
  creationTimestamp: '2019-06-10T14:46:08Z'
  labels:
    kubernetes.io/os: linux
    topology.kubernetes.io/zone: us-east-1a
    node.openshift.io/os_version: '4.5'
    node-role.kubernetes.io/worker: ''
    topology.kubernetes.io/region: us-east-1
    node.openshift.io/os_id: rhcos
    node.kubernetes.io/instance-type: m4.large
    kubernetes.io/hostname: ip-10-0-131-14
    kubernetes.io/arch: amd64
    region: east <span id="CO96-1"><!--Empty--></span><span class="callout">1</span>
    type: user-node
#...</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO96-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Labels to match the pod node selector.
									</div></dd></dl></div><p class="simpara">
								A pod has the <code class="literal">type: user-node,region: east</code> node selector:
							</p><div class="formalpara"><p class="title"><strong>Sample <code class="literal">Pod</code> object with node selectors</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: s1
#...
spec:
  nodeSelector: <span id="CO97-1"><!--Empty--></span><span class="callout">1</span>
    region: east
    type: user-node
#...</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO97-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Node selectors to match the node label. The node must have a label for each node selector.
									</div></dd></dl></div><p class="simpara">
								When you create the pod using the example pod spec, it can be scheduled on the example node.
							</p></dd><dt><span class="term">Default cluster-wide node selectors</span></dt><dd><p class="simpara">
								With default cluster-wide node selectors, when you create a pod in that cluster, OpenShift Container Platform adds the default node selectors to the pod and schedules the pod on nodes with matching labels.
							</p><p class="simpara">
								For example, the following <code class="literal">Scheduler</code> object has the default cluster-wide <code class="literal">region=east</code> and <code class="literal">type=user-node</code> node selectors:
							</p><div class="formalpara"><p class="title"><strong>Example Scheduler Operator Custom Resource</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  name: cluster
#...
spec:
  defaultNodeSelector: type=user-node,region=east
#...</pre>

								</p></div><p class="simpara">
								A node in that cluster has the <code class="literal">type=user-node,region=east</code> labels:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">Node</code> object</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
kind: Node
metadata:
  name: ci-ln-qg1il3k-f76d1-hlmhl-worker-b-df2s4
#...
  labels:
    region: east
    type: user-node
#...</pre>

								</p></div><div class="formalpara"><p class="title"><strong>Example <code class="literal">Pod</code> object with a node selector</strong></p><p>
									
<pre class="programlisting language-terminal">apiVersion: v1
kind: Pod
metadata:
  name: s1
#...
spec:
  nodeSelector:
    region: east
#...</pre>

								</p></div><p class="simpara">
								When you create the pod using the example pod spec in the example cluster, the pod is created with the cluster-wide node selector and is scheduled on the labeled node:
							</p><div class="formalpara"><p class="title"><strong>Example pod list with the pod on the labeled node</strong></p><p>
									
<pre class="programlisting language-terminal">NAME     READY   STATUS    RESTARTS   AGE   IP           NODE                                       NOMINATED NODE   READINESS GATES
pod-s1   1/1     Running   0          20s   10.131.2.6   ci-ln-qg1il3k-f76d1-hlmhl-worker-b-df2s4   &lt;none&gt;           &lt;none&gt;</pre>

								</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									If the project where you create the pod has a project node selector, that selector takes preference over a cluster-wide node selector. Your pod is not created or scheduled if the pod does not have the project node selector.
								</p></div></div></dd></dl></div><div class="variablelist" id="project-node-selectors_nodes-scheduler-node-selectors"><dl class="variablelist"><dt><span class="term">Project node selectors</span></dt><dd><p class="simpara">
								With project node selectors, when you create a pod in this project, OpenShift Container Platform adds the node selectors to the pod and schedules the pods on a node with matching labels. If there is a cluster-wide default node selector, a project node selector takes preference.
							</p><p class="simpara">
								For example, the following project has the <code class="literal">region=east</code> node selector:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">Namespace</code> object</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: east-region
  annotations:
    openshift.io/node-selector: "region=east"
#...</pre>

								</p></div><p class="simpara">
								The following node has the <code class="literal">type=user-node,region=east</code> labels:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">Node</code> object</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
kind: Node
metadata:
  name: ci-ln-qg1il3k-f76d1-hlmhl-worker-b-df2s4
#...
  labels:
    region: east
    type: user-node
#...</pre>

								</p></div><p class="simpara">
								When you create the pod using the example pod spec in this example project, the pod is created with the project node selectors and is scheduled on the labeled node:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">Pod</code> object</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  namespace: east-region
#...
spec:
  nodeSelector:
    region: east
    type: user-node
#...</pre>

								</p></div><div class="formalpara"><p class="title"><strong>Example pod list with the pod on the labeled node</strong></p><p>
									
<pre class="programlisting language-terminal">NAME     READY   STATUS    RESTARTS   AGE   IP           NODE                                       NOMINATED NODE   READINESS GATES
pod-s1   1/1     Running   0          20s   10.131.2.6   ci-ln-qg1il3k-f76d1-hlmhl-worker-b-df2s4   &lt;none&gt;           &lt;none&gt;</pre>

								</p></div><p class="simpara">
								A pod in the project is not created or scheduled if the pod contains different node selectors. For example, if you deploy the following pod into the example project, it is not be created:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">Pod</code> object with an invalid node selector</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: west-region
#...
spec:
  nodeSelector:
    region: west
#...</pre>

								</p></div></dd></dl></div></section><section class="section" id="nodes-scheduler-node-selectors-pod_nodes-scheduler-node-selectors"><div class="titlepage"><div><div><h3 class="title">4.7.2. Using node selectors to control pod placement</h3></div></div></div><p>
					You can use node selectors on pods and labels on nodes to control where the pod is scheduled. With node selectors, OpenShift Container Platform schedules the pods on nodes that contain matching labels.
				</p><p>
					You add labels to a node, a compute machine set, or a machine config. Adding the label to the compute machine set ensures that if the node or machine goes down, new nodes have the label. Labels added to a node or machine config do not persist if the node or machine goes down.
				</p><p>
					To add node selectors to an existing pod, add a node selector to the controlling object for that pod, such as a <code class="literal">ReplicaSet</code> object, <code class="literal">DaemonSet</code> object, <code class="literal">StatefulSet</code> object, <code class="literal">Deployment</code> object, or <code class="literal">DeploymentConfig</code> object. Any existing pods under that controlling object are recreated on a node with a matching label. If you are creating a new pod, you can add the node selector directly to the pod spec. If the pod does not have a controlling object, you must delete the pod, edit the pod spec, and recreate the pod.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You cannot add a node selector directly to an existing scheduled pod.
					</p></div></div><div class="formalpara"><p class="title"><strong>Prerequisites</strong></p><p>
						To add a node selector to existing pods, determine the controlling object for that pod. For example, the <code class="literal">router-default-66d5cf9464-m2g75</code> pod is controlled by the <code class="literal">router-default-66d5cf9464</code> replica set:
					</p></div><pre class="programlisting language-terminal">$ oc describe pod router-default-66d5cf9464-7pwkc</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">kind: Pod
apiVersion: v1
metadata:
#...
Name:               router-default-66d5cf9464-7pwkc
Namespace:          openshift-ingress
# ...
Controlled By:      ReplicaSet/router-default-66d5cf9464
# ...</pre>

					</p></div><p>
					The web console lists the controlling object under <code class="literal">ownerReferences</code> in the pod YAML:
				</p><pre class="programlisting language-terminal">apiVersion: v1
kind: Pod
metadata:
  name: router-default-66d5cf9464-7pwkc
# ...
  ownerReferences:
    - apiVersion: apps/v1
      kind: ReplicaSet
      name: router-default-66d5cf9464
      uid: d81dd094-da26-11e9-a48a-128e7edf0312
      controller: true
      blockOwnerDeletion: true
# ...</pre><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add labels to a node by using a compute machine set or editing the node directly:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Use a <code class="literal">MachineSet</code> object to add labels to nodes managed by the compute machine set when a node is created:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
											Run the following command to add labels to a <code class="literal">MachineSet</code> object:
										</p><pre class="programlisting language-terminal">$ oc patch MachineSet &lt;name&gt; --type='json' -p='[{"op":"add","path":"/spec/template/spec/metadata/labels", "value":{"&lt;key&gt;"="&lt;value&gt;","&lt;key&gt;"="&lt;value&gt;"}}]'  -n openshift-machine-api</pre><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc patch MachineSet abc612-msrtw-worker-us-east-1c  --type='json' -p='[{"op":"add","path":"/spec/template/spec/metadata/labels", "value":{"type":"user-node","region":"east"}}]'  -n openshift-machine-api</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
											You can alternatively apply the following YAML to add labels to a compute machine set:
										</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: xf2bd-infra-us-east-2a
  namespace: openshift-machine-api
spec:
  template:
    spec:
      metadata:
        labels:
          region: "east"
          type: "user-node"
#...</pre></div></div></li><li class="listitem"><p class="simpara">
											Verify that the labels are added to the <code class="literal">MachineSet</code> object by using the <code class="literal">oc edit</code> command:
										</p><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc edit MachineSet abc612-msrtw-worker-us-east-1c -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example <code class="literal">MachineSet</code> object</strong></p><p>
												
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet

# ...

spec:
# ...
  template:
    metadata:
# ...
    spec:
      metadata:
        labels:
          region: east
          type: user-node
# ...</pre>

											</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
									Add labels directly to a node:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
											Edit the <code class="literal">Node</code> object for the node:
										</p><pre class="programlisting language-terminal">$ oc label nodes &lt;name&gt; &lt;key&gt;=&lt;value&gt;</pre><p class="simpara">
											For example, to label a node:
										</p><pre class="programlisting language-terminal">$ oc label nodes ip-10-0-142-25.ec2.internal type=user-node region=east</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
											You can alternatively apply the following YAML to add labels to a node:
										</p><pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  name: hello-node-6fbccf8d9
  labels:
    type: "user-node"
    region: "east"
#...</pre></div></div></li><li class="listitem"><p class="simpara">
											Verify that the labels are added to the node:
										</p><pre class="programlisting language-terminal">$ oc get nodes -l type=user-node,region=east</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
												
<pre class="programlisting language-terminal">NAME                          STATUS   ROLES    AGE   VERSION
ip-10-0-142-25.ec2.internal   Ready    worker   17m   v1.26.0</pre>

											</p></div></li></ol></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Add the matching node selector to a pod:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To add a node selector to existing and future pods, add a node selector to the controlling object for the pods:
								</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">ReplicaSet</code> object with labels</strong></p><p>
										
<pre class="programlisting language-yaml">kind: ReplicaSet
apiVersion: apps/v1
metadata:
  name: hello-node-6fbccf8d9
# ...
spec:
# ...
  template:
    metadata:
      creationTimestamp: null
      labels:
        ingresscontroller.operator.openshift.io/deployment-ingresscontroller: default
        pod-template-hash: 66d5cf9464
    spec:
      nodeSelector:
        kubernetes.io/os: linux
        node-role.kubernetes.io/worker: ''
        type: user-node <span id="CO98-1"><!--Empty--></span><span class="callout">1</span>
#...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO98-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Add the node selector.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To add a node selector to a specific, new pod, add the selector to the <code class="literal">Pod</code> object directly:
								</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">Pod</code> object with a node selector</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: hello-node-6fbccf8d9
#...
spec:
  nodeSelector:
    region: east
    type: user-node
#...</pre>

									</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										You cannot add a node selector directly to an existing scheduled pod.
									</p></div></div></li></ul></div></li></ol></div></section><section class="section" id="nodes-scheduler-node-selectors-cluster_nodes-scheduler-node-selectors"><div class="titlepage"><div><div><h3 class="title">4.7.3. Creating default cluster-wide node selectors</h3></div></div></div><p>
					You can use default cluster-wide node selectors on pods together with labels on nodes to constrain all pods created in a cluster to specific nodes.
				</p><p>
					With cluster-wide node selectors, when you create a pod in that cluster, OpenShift Container Platform adds the default node selectors to the pod and schedules the pod on nodes with matching labels.
				</p><p>
					You configure cluster-wide node selectors by editing the Scheduler Operator custom resource (CR). You add labels to a node, a compute machine set, or a machine config. Adding the label to the compute machine set ensures that if the node or machine goes down, new nodes have the label. Labels added to a node or machine config do not persist if the node or machine goes down.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can add additional key/value pairs to a pod. But you cannot add a different value for a default key.
					</p></div></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To add a default cluster-wide node selector:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the Scheduler Operator CR to add the default cluster-wide node selectors:
						</p><pre class="programlisting language-terminal">$ oc edit scheduler cluster</pre><div class="formalpara"><p class="title"><strong>Example Scheduler Operator CR with a node selector</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  name: cluster
...
spec:
  defaultNodeSelector: type=user-node,region=east <span id="CO99-1"><!--Empty--></span><span class="callout">1</span>
  mastersSchedulable: false</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO99-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add a node selector with the appropriate <code class="literal">&lt;key&gt;:&lt;value&gt;</code> pairs.
								</div></dd></dl></div><p class="simpara">
							After making this change, wait for the pods in the <code class="literal">openshift-kube-apiserver</code> project to redeploy. This can take several minutes. The default cluster-wide node selector does not take effect until the pods redeploy.
						</p></li><li class="listitem"><p class="simpara">
							Add labels to a node by using a compute machine set or editing the node directly:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Use a compute machine set to add labels to nodes managed by the compute machine set when a node is created:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
											Run the following command to add labels to a <code class="literal">MachineSet</code> object:
										</p><pre class="programlisting language-terminal">$ oc patch MachineSet &lt;name&gt; --type='json' -p='[{"op":"add","path":"/spec/template/spec/metadata/labels", "value":{"&lt;key&gt;"="&lt;value&gt;","&lt;key&gt;"="&lt;value&gt;"}}]'  -n openshift-machine-api <span id="CO100-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO100-1"><span class="callout">1</span></a> </dt><dd><div class="para">
													Add a <code class="literal">&lt;key&gt;/&lt;value&gt;</code> pair for each label.
												</div></dd></dl></div><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc patch MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c --type='json' -p='[{"op":"add","path":"/spec/template/spec/metadata/labels", "value":{"type":"user-node","region":"east"}}]'  -n openshift-machine-api</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
											You can alternatively apply the following YAML to add labels to a compute machine set:
										</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: &lt;machineset&gt;
  namespace: openshift-machine-api
spec:
  template:
    spec:
      metadata:
        labels:
          region: "east"
          type: "user-node"</pre></div></div></li><li class="listitem"><p class="simpara">
											Verify that the labels are added to the <code class="literal">MachineSet</code> object by using the <code class="literal">oc edit</code> command:
										</p><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc edit MachineSet abc612-msrtw-worker-us-east-1c -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example <code class="literal">MachineSet</code> object</strong></p><p>
												
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
  ...
spec:
  ...
  template:
    metadata:
  ...
    spec:
      metadata:
        labels:
          region: east
          type: user-node
  ...</pre>

											</p></div></li><li class="listitem"><p class="simpara">
											Redeploy the nodes associated with that compute machine set by scaling down to <code class="literal">0</code> and scaling up the nodes:
										</p><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc scale --replicas=0 MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c -n openshift-machine-api</pre><pre class="programlisting language-terminal">$ oc scale --replicas=1 MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
											When the nodes are ready and available, verify that the label is added to the nodes by using the <code class="literal">oc get</code> command:
										</p><pre class="programlisting language-terminal">$ oc get nodes -l &lt;key&gt;=&lt;value&gt;</pre><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc get nodes -l type=user-node</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
												
<pre class="programlisting language-terminal">NAME                                       STATUS   ROLES    AGE   VERSION
ci-ln-l8nry52-f76d1-hl7m7-worker-c-vmqzp   Ready    worker   61s   v1.26.0</pre>

											</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
									Add labels directly to a node:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
											Edit the <code class="literal">Node</code> object for the node:
										</p><pre class="programlisting language-terminal">$ oc label nodes &lt;name&gt; &lt;key&gt;=&lt;value&gt;</pre><p class="simpara">
											For example, to label a node:
										</p><pre class="programlisting language-terminal">$ oc label nodes ci-ln-l8nry52-f76d1-hl7m7-worker-b-tgq49 type=user-node region=east</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
											You can alternatively apply the following YAML to add labels to a node:
										</p><pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  name: &lt;node_name&gt;
  labels:
    type: "user-node"
    region: "east"</pre></div></div></li><li class="listitem"><p class="simpara">
											Verify that the labels are added to the node using the <code class="literal">oc get</code> command:
										</p><pre class="programlisting language-terminal">$ oc get nodes -l &lt;key&gt;=&lt;value&gt;,&lt;key&gt;=&lt;value&gt;</pre><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc get nodes -l type=user-node,region=east</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
												
<pre class="programlisting language-terminal">NAME                                       STATUS   ROLES    AGE   VERSION
ci-ln-l8nry52-f76d1-hl7m7-worker-b-tgq49   Ready    worker   17m   v1.26.0</pre>

											</p></div></li></ol></div></li></ul></div></li></ol></div></section><section class="section" id="nodes-scheduler-node-selectors-project_nodes-scheduler-node-selectors"><div class="titlepage"><div><div><h3 class="title">4.7.4. Creating project-wide node selectors</h3></div></div></div><p>
					You can use node selectors in a project together with labels on nodes to constrain all pods created in that project to the labeled nodes.
				</p><p>
					When you create a pod in this project, OpenShift Container Platform adds the node selectors to the pods in the project and schedules the pods on a node with matching labels in the project. If there is a cluster-wide default node selector, a project node selector takes preference.
				</p><p>
					You add node selectors to a project by editing the <code class="literal">Namespace</code> object to add the <code class="literal">openshift.io/node-selector</code> parameter. You add labels to a node, a compute machine set, or a machine config. Adding the label to the compute machine set ensures that if the node or machine goes down, new nodes have the label. Labels added to a node or machine config do not persist if the node or machine goes down.
				</p><p>
					A pod is not scheduled if the <code class="literal">Pod</code> object contains a node selector, but no project has a matching node selector. When you create a pod from that spec, you receive an error similar to the following message:
				</p><div class="formalpara"><p class="title"><strong>Example error message</strong></p><p>
						
<pre class="programlisting language-terminal">Error from server (Forbidden): error when creating "pod.yaml": pods "pod-4" is forbidden: pod node label selector conflicts with its project node label selector</pre>

					</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can add additional key/value pairs to a pod. But you cannot add a different value for a project key.
					</p></div></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To add a default project node selector:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a namespace or edit an existing namespace to add the <code class="literal">openshift.io/node-selector</code> parameter:
						</p><pre class="programlisting language-terminal">$ oc edit namespace &lt;name&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
  annotations:
    openshift.io/node-selector: "type=user-node,region=east" <span id="CO101-1"><!--Empty--></span><span class="callout">1</span>
    openshift.io/description: ""
    openshift.io/display-name: ""
    openshift.io/requester: kube:admin
    openshift.io/sa.scc.mcs: s0:c30,c5
    openshift.io/sa.scc.supplemental-groups: 1000880000/10000
    openshift.io/sa.scc.uid-range: 1000880000/10000
  creationTimestamp: "2021-05-10T12:35:04Z"
  labels:
    kubernetes.io/metadata.name: demo
  name: demo
  resourceVersion: "145537"
  uid: 3f8786e3-1fcb-42e3-a0e3-e2ac54d15001
spec:
  finalizers:
  - kubernetes</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO101-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add the <code class="literal">openshift.io/node-selector</code> with the appropriate <code class="literal">&lt;key&gt;:&lt;value&gt;</code> pairs.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Add labels to a node by using a compute machine set or editing the node directly:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Use a <code class="literal">MachineSet</code> object to add labels to nodes managed by the compute machine set when a node is created:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
											Run the following command to add labels to a <code class="literal">MachineSet</code> object:
										</p><pre class="programlisting language-terminal">$ oc patch MachineSet &lt;name&gt; --type='json' -p='[{"op":"add","path":"/spec/template/spec/metadata/labels", "value":{"&lt;key&gt;"="&lt;value&gt;","&lt;key&gt;"="&lt;value&gt;"}}]'  -n openshift-machine-api</pre><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc patch MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c --type='json' -p='[{"op":"add","path":"/spec/template/spec/metadata/labels", "value":{"type":"user-node","region":"east"}}]'  -n openshift-machine-api</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
											You can alternatively apply the following YAML to add labels to a compute machine set:
										</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: &lt;machineset&gt;
  namespace: openshift-machine-api
spec:
  template:
    spec:
      metadata:
        labels:
          region: "east"
          type: "user-node"</pre></div></div></li><li class="listitem"><p class="simpara">
											Verify that the labels are added to the <code class="literal">MachineSet</code> object by using the <code class="literal">oc edit</code> command:
										</p><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc edit MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c -n openshift-machine-api</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
												
<pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
...
spec:
...
  template:
    metadata:
...
    spec:
      metadata:
        labels:
          region: east
          type: user-node</pre>

											</p></div></li><li class="listitem"><p class="simpara">
											Redeploy the nodes associated with that compute machine set:
										</p><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc scale --replicas=0 MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c -n openshift-machine-api</pre><pre class="programlisting language-terminal">$ oc scale --replicas=1 MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c -n openshift-machine-api</pre></li><li class="listitem"><p class="simpara">
											When the nodes are ready and available, verify that the label is added to the nodes by using the <code class="literal">oc get</code> command:
										</p><pre class="programlisting language-terminal">$ oc get nodes -l &lt;key&gt;=&lt;value&gt;</pre><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc get nodes -l type=user-node,region=east</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
												
<pre class="programlisting language-terminal">NAME                                       STATUS   ROLES    AGE   VERSION
ci-ln-l8nry52-f76d1-hl7m7-worker-c-vmqzp   Ready    worker   61s   v1.26.0</pre>

											</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
									Add labels directly to a node:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
											Edit the <code class="literal">Node</code> object to add labels:
										</p><pre class="programlisting language-terminal">$ oc label &lt;resource&gt; &lt;name&gt; &lt;key&gt;=&lt;value&gt;</pre><p class="simpara">
											For example, to label a node:
										</p><pre class="programlisting language-terminal">$ oc label nodes ci-ln-l8nry52-f76d1-hl7m7-worker-c-tgq49 type=user-node region=east</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
											You can alternatively apply the following YAML to add labels to a node:
										</p><pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  name: &lt;node_name&gt;
  labels:
    type: "user-node"
    region: "east"</pre></div></div></li><li class="listitem"><p class="simpara">
											Verify that the labels are added to the <code class="literal">Node</code> object using the <code class="literal">oc get</code> command:
										</p><pre class="programlisting language-terminal">$ oc get nodes -l &lt;key&gt;=&lt;value&gt;</pre><p class="simpara">
											For example:
										</p><pre class="programlisting language-terminal">$ oc get nodes -l type=user-node,region=east</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
												
<pre class="programlisting language-terminal">NAME                                       STATUS   ROLES    AGE   VERSION
ci-ln-l8nry52-f76d1-hl7m7-worker-b-tgq49   Ready    worker   17m   v1.26.0</pre>

											</p></div></li></ol></div></li></ul></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-taints-tolerations-projects_nodes-scheduler-taints-tolerations">Creating a project with a node selector and toleration</a>
						</li></ul></div></section></section><section class="section" id="nodes-scheduler-pod-topology-spread-constraints"><div class="titlepage"><div><div><h2 class="title">4.8. Controlling pod placement by using pod topology spread constraints</h2></div></div></div><p>
				You can use pod topology spread constraints to control the placement of your pods across nodes, zones, regions, or other user-defined topology domains.
			</p><section class="section" id="nodes-scheduler-pod-topology-spread-constraints-about_nodes-scheduler-pod-topology-spread-constraints"><div class="titlepage"><div><div><h3 class="title">4.8.1. About pod topology spread constraints</h3></div></div></div><p>
					By using a <span class="emphasis"><em>pod topology spread constraint</em></span>, you provide fine-grained control over the distribution of pods across failure domains to help achieve high availability and more efficient resource utilization.
				</p><p>
					OpenShift Container Platform administrators can label nodes to provide topology information, such as regions, zones, nodes, or other user-defined domains. After these labels are set on nodes, users can then define pod topology spread constraints to control the placement of pods across these topology domains.
				</p><p>
					You specify which pods to group together, which topology domains they are spread among, and the acceptable skew. Only pods within the same namespace are matched and grouped together when spreading due to a constraint.
				</p></section><section class="section" id="nodes-scheduler-pod-topology-spread-constraints-configuring_nodes-scheduler-pod-topology-spread-constraints"><div class="titlepage"><div><div><h3 class="title">4.8.2. Configuring pod topology spread constraints</h3></div></div></div><p>
					The following steps demonstrate how to configure pod topology spread constraints to distribute pods that match the specified labels based on their zone.
				</p><p>
					You can specify multiple pod topology spread constraints, but you must ensure that they do not conflict with each other. All pod topology spread constraints must be satisfied for a pod to be placed.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							A cluster administrator has added the required labels to nodes.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">Pod</code> spec and specify a pod topology spread constraint:
						</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">pod-spec.yaml</code> file</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  labels:
    region: us-east
spec:
  topologySpreadConstraints:
  - maxSkew: 1 <span id="CO102-1"><!--Empty--></span><span class="callout">1</span>
    topologyKey: topology.kubernetes.io/zone <span id="CO102-2"><!--Empty--></span><span class="callout">2</span>
    whenUnsatisfiable: DoNotSchedule <span id="CO102-3"><!--Empty--></span><span class="callout">3</span>
    labelSelector: <span id="CO102-4"><!--Empty--></span><span class="callout">4</span>
      matchLabels:
        region: us-east <span id="CO102-5"><!--Empty--></span><span class="callout">5</span>
  containers:
  - image: "docker.io/ocpqe/hello-pod"
    name: hello-pod</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO102-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The maximum difference in number of pods between any two topology domains. The default is <code class="literal">1</code>, and you cannot specify a value of <code class="literal">0</code>.
								</div></dd><dt><a href="#CO102-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The key of a node label. Nodes with this key and identical value are considered to be in the same topology.
								</div></dd><dt><a href="#CO102-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									How to handle a pod if it does not satisfy the spread constraint. The default is <code class="literal">DoNotSchedule</code>, which tells the scheduler not to schedule the pod. Set to <code class="literal">ScheduleAnyway</code> to still schedule the pod, but the scheduler prioritizes honoring the skew to not make the cluster more imbalanced.
								</div></dd><dt><a href="#CO102-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Pods that match this label selector are counted and recognized as a group when spreading to satisfy the constraint. Be sure to specify a label selector, otherwise no pods can be matched.
								</div></dd><dt><a href="#CO102-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Be sure that this <code class="literal">Pod</code> spec also sets its labels to match this label selector if you want it to be counted properly in the future.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the pod:
						</p><pre class="programlisting language-terminal">$ oc create -f pod-spec.yaml</pre></li></ol></div></section><section class="section" id="nodes-scheduler-pod-topology-spread-constraints-examples_nodes-scheduler-pod-topology-spread-constraints"><div class="titlepage"><div><div><h3 class="title">4.8.3. Example pod topology spread constraints</h3></div></div></div><p>
					The following examples demonstrate pod topology spread constraint configurations.
				</p><section class="section" id="nodes-scheduler-pod-topology-spread-constraints-example-single_nodes-scheduler-pod-topology-spread-constraints"><div class="titlepage"><div><div><h4 class="title">4.8.3.1. Single pod topology spread constraint example</h4></div></div></div><p>
						This example <code class="literal">Pod</code> spec defines one pod topology spread constraint. It matches on pods labeled <code class="literal">region: us-east</code>, distributes among zones, specifies a skew of <code class="literal">1</code>, and does not schedule the pod if it does not meet these requirements.
					</p><pre class="programlisting language-yaml">kind: Pod
apiVersion: v1
metadata:
  name: my-pod
  labels:
    region: us-east
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        region: us-east
  containers:
  - image: "docker.io/ocpqe/hello-pod"
    name: hello-pod</pre></section><section class="section" id="nodes-scheduler-pod-topology-spread-constraints-example-multiple_nodes-scheduler-pod-topology-spread-constraints"><div class="titlepage"><div><div><h4 class="title">4.8.3.2. Multiple pod topology spread constraints example</h4></div></div></div><p>
						This example <code class="literal">Pod</code> spec defines two pod topology spread constraints. Both match on pods labeled <code class="literal">region: us-east</code>, specify a skew of <code class="literal">1</code>, and do not schedule the pod if it does not meet these requirements.
					</p><p>
						The first constraint distributes pods based on a user-defined label <code class="literal">node</code>, and the second constraint distributes pods based on a user-defined label <code class="literal">rack</code>. Both constraints must be met for the pod to be scheduled.
					</p><pre class="programlisting language-yaml">kind: Pod
apiVersion: v1
metadata:
  name: my-pod-2
  labels:
    region: us-east
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: node
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        region: us-east
  - maxSkew: 1
    topologyKey: rack
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        region: us-east
  containers:
  - image: "docker.io/ocpqe/hello-pod"
    name: hello-pod</pre></section></section><section class="section _additional-resources" id="additional-resources-3"><div class="titlepage"><div><div><h3 class="title">4.8.4. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-working-updating_nodes-nodes-working">Understanding how to update labels on nodes</a>
						</li></ul></div></section></section><section class="section" id="nodes-descheduler"><div class="titlepage"><div><div><h2 class="title">4.9. Evicting pods using the descheduler</h2></div></div></div><p>
				While the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-about">scheduler</a> is used to determine the most suitable node to host a new pod, the descheduler can be used to evict a running pod so that the pod can be rescheduled onto a more suitable node.
			</p><section class="section" id="nodes-descheduler-about_nodes-descheduler"><div class="titlepage"><div><div><h3 class="title">4.9.1. About the descheduler</h3></div></div></div><p>
					You can use the descheduler to evict pods based on specific strategies so that the pods can be rescheduled onto more appropriate nodes.
				</p><p>
					You can benefit from descheduling running pods in situations such as the following:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Nodes are underutilized or overutilized.
						</li><li class="listitem">
							Pod and node affinity requirements, such as taints or labels, have changed and the original scheduling decisions are no longer appropriate for certain nodes.
						</li><li class="listitem">
							Node failure requires pods to be moved.
						</li><li class="listitem">
							New nodes are added to clusters.
						</li><li class="listitem">
							Pods have been restarted too many times.
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The descheduler does not schedule replacement of evicted pods. The scheduler automatically performs this task for the evicted pods.
					</p></div></div><p>
					When the descheduler decides to evict pods from a node, it employs the following general mechanism:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Pods in the <code class="literal">openshift-*</code> and <code class="literal">kube-system</code> namespaces are never evicted.
						</li><li class="listitem">
							Critical pods with <code class="literal">priorityClassName</code> set to <code class="literal">system-cluster-critical</code> or <code class="literal">system-node-critical</code> are never evicted.
						</li><li class="listitem">
							Static, mirrored, or stand-alone pods that are not part of a replication controller, replica set, deployment, or job are never evicted because these pods will not be recreated.
						</li><li class="listitem">
							Pods associated with daemon sets are never evicted.
						</li><li class="listitem">
							Pods with local storage are never evicted.
						</li><li class="listitem">
							Best effort pods are evicted before burstable and guaranteed pods.
						</li><li class="listitem">
							All types of pods with the <code class="literal">descheduler.alpha.kubernetes.io/evict</code> annotation are eligible for eviction. This annotation is used to override checks that prevent eviction, and the user can select which pod is evicted. Users should know how and if the pod will be recreated.
						</li><li class="listitem">
							Pods subject to pod disruption budget (PDB) are not evicted if descheduling violates its pod disruption budget (PDB). The pods are evicted by using eviction subresource to handle PDB.
						</li></ul></div></section><section class="section" id="nodes-descheduler-profiles_nodes-descheduler"><div class="titlepage"><div><div><h3 class="title">4.9.2. Descheduler profiles</h3></div></div></div><p>
					The following descheduler profiles are available:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">AffinityAndTaints</code></span></dt><dd><p class="simpara">
								This profile evicts pods that violate inter-pod anti-affinity, node affinity, and node taints.
							</p><p class="simpara">
								It enables the following strategies:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">RemovePodsViolatingInterPodAntiAffinity</code>: removes pods that are violating inter-pod anti-affinity.
									</li><li class="listitem">
										<code class="literal">RemovePodsViolatingNodeAffinity</code>: removes pods that are violating node affinity.
									</li><li class="listitem"><p class="simpara">
										<code class="literal">RemovePodsViolatingNodeTaints</code>: removes pods that are violating <code class="literal">NoSchedule</code> taints on nodes.
									</p><p class="simpara">
										Pods with a node affinity type of <code class="literal">requiredDuringSchedulingIgnoredDuringExecution</code> are removed.
									</p></li></ul></div></dd><dt><span class="term"><code class="literal">TopologyAndDuplicates</code></span></dt><dd><p class="simpara">
								This profile evicts pods in an effort to evenly spread similar pods, or pods of the same topology domain, among nodes.
							</p><p class="simpara">
								It enables the following strategies:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">RemovePodsViolatingTopologySpreadConstraint</code>: finds unbalanced topology domains and tries to evict pods from larger ones when <code class="literal">DoNotSchedule</code> constraints are violated.
									</li><li class="listitem">
										<code class="literal">RemoveDuplicates</code>: ensures that there is only one pod associated with a replica set, replication controller, deployment, or job running on same node. If there are more, those duplicate pods are evicted for better pod distribution in a cluster.
									</li></ul></div></dd><dt><span class="term"><code class="literal">LifecycleAndUtilization</code></span></dt><dd><p class="simpara">
								This profile evicts long-running pods and balances resource usage between nodes.
							</p><p class="simpara">
								It enables the following strategies:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
										<code class="literal">RemovePodsHavingTooManyRestarts</code>: removes pods whose containers have been restarted too many times.
									</p><p class="simpara">
										Pods where the sum of restarts over all containers (including Init Containers) is more than 100.
									</p></li><li class="listitem"><p class="simpara">
										<code class="literal">LowNodeUtilization</code>: finds nodes that are underutilized and evicts pods, if possible, from overutilized nodes in the hope that recreation of evicted pods will be scheduled on these underutilized nodes.
									</p><p class="simpara">
										A node is considered underutilized if its usage is below 20% for all thresholds (CPU, memory, and number of pods).
									</p><p class="simpara">
										A node is considered overutilized if its usage is above 50% for any of the thresholds (CPU, memory, and number of pods).
									</p></li><li class="listitem"><p class="simpara">
										<code class="literal">PodLifeTime</code>: evicts pods that are too old.
									</p><p class="simpara">
										By default, pods that are older than 24 hours are removed. You can customize the pod lifetime value.
									</p></li></ul></div></dd><dt><span class="term"><code class="literal">SoftTopologyAndDuplicates</code></span></dt><dd><p class="simpara">
								This profile is the same as <code class="literal">TopologyAndDuplicates</code>, except that pods with soft topology constraints, such as <code class="literal">whenUnsatisfiable: ScheduleAnyway</code>, are also considered for eviction.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Do not enable both <code class="literal">SoftTopologyAndDuplicates</code> and <code class="literal">TopologyAndDuplicates</code>. Enabling both results in a conflict.
								</p></div></div></dd><dt><span class="term"><code class="literal">EvictPodsWithLocalStorage</code></span></dt><dd>
								This profile allows pods with local storage to be eligible for eviction.
							</dd><dt><span class="term"><code class="literal">EvictPodsWithPVC</code></span></dt><dd>
								This profile allows pods with persistent volume claims to be eligible for eviction. If you are using <code class="literal">Kubernetes NFS Subdir External Provisioner</code>, you must add an excluded namespace for the namespace where the provisioner is installed.
							</dd></dl></div></section><section class="section" id="nodes-descheduler-installing_nodes-descheduler"><div class="titlepage"><div><div><h3 class="title">4.9.3. Installing the descheduler</h3></div></div></div><p>
					The descheduler is not available by default. To enable the descheduler, you must install the Kube Descheduler Operator from OperatorHub and enable one or more descheduler profiles.
				</p><p>
					By default, the descheduler runs in predictive mode, which means that it only simulates pod evictions. You must change the mode to automatic for the descheduler to perform the pod evictions.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						If you have enabled hosted control planes in your cluster, set a custom priority threshold to lower the chance that pods in the hosted control plane namespaces are evicted. Set the priority threshold class name to <code class="literal">hypershift-control-plane</code>, because it has the lowest priority value (<code class="literal">100000000</code>) of the hosted control plane priority classes.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Cluster administrator privileges.
						</li><li class="listitem">
							Access to the OpenShift Container Platform web console.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the OpenShift Container Platform web console.
						</li><li class="listitem"><p class="simpara">
							Create the required namespace for the Kube Descheduler Operator.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Namespaces</strong></span> and click <span class="strong strong"><strong>Create Namespace</strong></span>.
								</li><li class="listitem">
									Enter <code class="literal">openshift-kube-descheduler-operator</code> in the <span class="strong strong"><strong>Name</strong></span> field, enter <code class="literal">openshift.io/cluster-monitoring=true</code> in the <span class="strong strong"><strong>Labels</strong></span> field to enable descheduler metrics, and click <span class="strong strong"><strong>Create</strong></span>.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Install the Kube Descheduler Operator.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span>.
								</li><li class="listitem">
									Type <span class="strong strong"><strong>Kube Descheduler Operator</strong></span> into the filter box.
								</li><li class="listitem">
									Select the <span class="strong strong"><strong>Kube Descheduler Operator</strong></span> and click <span class="strong strong"><strong>Install</strong></span>.
								</li><li class="listitem">
									On the <span class="strong strong"><strong>Install Operator</strong></span> page, select <span class="strong strong"><strong>A specific namespace on the cluster</strong></span>. Select <span class="strong strong"><strong>openshift-kube-descheduler-operator</strong></span> from the drop-down menu.
								</li><li class="listitem">
									Adjust the values for the <span class="strong strong"><strong>Update Channel</strong></span> and <span class="strong strong"><strong>Approval Strategy</strong></span> to the desired values.
								</li><li class="listitem">
									Click <span class="strong strong"><strong>Install</strong></span>.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Create a descheduler instance.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									From the <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span> page, click the <span class="strong strong"><strong>Kube Descheduler Operator</strong></span>.
								</li><li class="listitem">
									Select the <span class="strong strong"><strong>Kube Descheduler</strong></span> tab and click <span class="strong strong"><strong>Create KubeDescheduler</strong></span>.
								</li><li class="listitem"><p class="simpara">
									Edit the settings as necessary.
								</p><div class="orderedlist"><ol class="orderedlist" type="i"><li class="listitem">
											To evict pods instead of simulating the evictions, change the <span class="strong strong"><strong>Mode</strong></span> field to <span class="strong strong"><strong>Automatic</strong></span>.
										</li><li class="listitem"><p class="simpara">
											Expand the <span class="strong strong"><strong>Profiles</strong></span> section to select one or more profiles to enable. The <code class="literal">AffinityAndTaints</code> profile is enabled by default. Click <span class="strong strong"><strong>Add Profile</strong></span> to select additional profiles.
										</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												Do not enable both <code class="literal">TopologyAndDuplicates</code> and <code class="literal">SoftTopologyAndDuplicates</code>. Enabling both results in a conflict.
											</p></div></div></li><li class="listitem"><p class="simpara">
											Optional: Expand the <span class="strong strong"><strong>Profile Customizations</strong></span> section to set optional configurations for the descheduler.
										</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
													Set a custom pod lifetime value for the <code class="literal">LifecycleAndUtilization</code> profile. Use the <span class="strong strong"><strong>podLifetime</strong></span> field to set a numerical value and a valid unit (<code class="literal">s</code>, <code class="literal">m</code>, or <code class="literal">h</code>). The default pod lifetime is 24 hours (<code class="literal">24h</code>).
												</li><li class="listitem"><p class="simpara">
													Set a custom priority threshold to consider pods for eviction only if their priority is lower than a specified priority level. Use the <span class="strong strong"><strong>thresholdPriority</strong></span> field to set a numerical priority threshold or use the <span class="strong strong"><strong>thresholdPriorityClassName</strong></span> field to specify a certain priority class name.
												</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
														Do not specify both <span class="strong strong"><strong>thresholdPriority</strong></span> and <span class="strong strong"><strong>thresholdPriorityClassName</strong></span> for the descheduler.
													</p></div></div></li><li class="listitem">
													Set specific namespaces to exclude or include from descheduler operations. Expand the <span class="strong strong"><strong>namespaces</strong></span> field and add namespaces to the <span class="strong strong"><strong>excluded</strong></span> or <span class="strong strong"><strong>included</strong></span> list. You can only either set a list of namespaces to exclude or a list of namespaces to include. Note that protected namespaces (<code class="literal">openshift-*</code>, <code class="literal">kube-system</code>, <code class="literal">hypershift</code>) are excluded by default.
												</li><li class="listitem"><p class="simpara">
													Experimental: Set thresholds for underutilization and overutilization for the <code class="literal">LowNodeUtilization</code> strategy. Use the <span class="strong strong"><strong>devLowNodeUtilizationThresholds</strong></span> field to set one of the following values:
												</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
															<code class="literal">Low</code>: 10% underutilized and 30% overutilized
														</li><li class="listitem">
															<code class="literal">Medium</code>: 20% underutilized and 50% overutilized (Default)
														</li><li class="listitem">
															<code class="literal">High</code>: 40% underutilized and 70% overutilized
														</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
														This setting is experimental and should not be used in a production environment.
													</p></div></div></li></ul></div></li><li class="listitem">
											Optional: Use the <span class="strong strong"><strong>Descheduling Interval Seconds</strong></span> field to change the number of seconds between descheduler runs. The default is <code class="literal">3600</code> seconds.
										</li></ol></div></li><li class="listitem">
									Click <span class="strong strong"><strong>Create</strong></span>.
								</li></ol></div></li></ol></div><p>
					You can also configure the profiles and settings for the descheduler later using the OpenShift CLI (<code class="literal">oc</code>). If you did not adjust the profiles when creating the descheduler instance from the web console, the <code class="literal">AffinityAndTaints</code> profile is enabled by default.
				</p></section><section class="section" id="nodes-descheduler-configuring-profiles_nodes-descheduler"><div class="titlepage"><div><div><h3 class="title">4.9.4. Configuring descheduler profiles</h3></div></div></div><p>
					You can configure which profiles the descheduler uses to evict pods.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Cluster administrator privileges
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">KubeDescheduler</code> object:
						</p><pre class="programlisting language-terminal">$ oc edit kubedeschedulers.operator.openshift.io cluster -n openshift-kube-descheduler-operator</pre></li><li class="listitem"><p class="simpara">
							Specify one or more profiles in the <code class="literal">spec.profiles</code> section.
						</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: KubeDescheduler
metadata:
  name: cluster
  namespace: openshift-kube-descheduler-operator
spec:
  deschedulingIntervalSeconds: 3600
  logLevel: Normal
  managementState: Managed
  operatorLogLevel: Normal
  mode: Predictive                                     <span id="CO103-1"><!--Empty--></span><span class="callout">1</span>
  profileCustomizations:
    namespaces:                                        <span id="CO103-2"><!--Empty--></span><span class="callout">2</span>
      excluded:
      - my-namespace
    podLifetime: 48h                                   <span id="CO103-3"><!--Empty--></span><span class="callout">3</span>
    thresholdPriorityClassName: my-priority-class-name <span id="CO103-4"><!--Empty--></span><span class="callout">4</span>
  profiles:                                            <span id="CO103-5"><!--Empty--></span><span class="callout">5</span>
  - AffinityAndTaints
  - TopologyAndDuplicates                              <span id="CO103-6"><!--Empty--></span><span class="callout">6</span>
  - LifecycleAndUtilization
  - EvictPodsWithLocalStorage
  - EvictPodsWithPVC</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO103-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Optional: By default, the descheduler does not evict pods. To evict pods, set <code class="literal">mode</code> to <code class="literal">Automatic</code>.
								</div></dd><dt><a href="#CO103-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Optional: Set a list of user-created namespaces to include or exclude from descheduler operations. Use <code class="literal">excluded</code> to set a list of namespaces to exclude or use <code class="literal">included</code> to set a list of namespaces to include. Note that protected namespaces (<code class="literal">openshift-*</code>, <code class="literal">kube-system</code>, <code class="literal">hypershift</code>) are excluded by default.
								</div></dd><dt><a href="#CO103-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Optional: Enable a custom pod lifetime value for the <code class="literal">LifecycleAndUtilization</code> profile. Valid units are <code class="literal">s</code>, <code class="literal">m</code>, or <code class="literal">h</code>. The default pod lifetime is 24 hours.
								</div></dd><dt><a href="#CO103-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Optional: Specify a priority threshold to consider pods for eviction only if their priority is lower than the specified level. Use the <code class="literal">thresholdPriority</code> field to set a numerical priority threshold (for example, <code class="literal">10000</code>) or use the <code class="literal">thresholdPriorityClassName</code> field to specify a certain priority class name (for example, <code class="literal">my-priority-class-name</code>). If you specify a priority class name, it must already exist or the descheduler will throw an error. Do not set both <code class="literal">thresholdPriority</code> and <code class="literal">thresholdPriorityClassName</code>.
								</div></dd><dt><a href="#CO103-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Add one or more profiles to enable. Available profiles: <code class="literal">AffinityAndTaints</code>, <code class="literal">TopologyAndDuplicates</code>, <code class="literal">LifecycleAndUtilization</code>, <code class="literal">SoftTopologyAndDuplicates</code>, <code class="literal">EvictPodsWithLocalStorage</code>, and <code class="literal">EvictPodsWithPVC</code>.
								</div></dd><dt><a href="#CO103-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Do not enable both <code class="literal">TopologyAndDuplicates</code> and <code class="literal">SoftTopologyAndDuplicates</code>. Enabling both results in a conflict.
								</div></dd></dl></div><p class="simpara">
							You can enable multiple profiles; the order that the profiles are specified in is not important.
						</p></li><li class="listitem">
							Save the file to apply the changes.
						</li></ol></div></section><section class="section" id="nodes-descheduler-configuring-interval_nodes-descheduler"><div class="titlepage"><div><div><h3 class="title">4.9.5. Configuring the descheduler interval</h3></div></div></div><p>
					You can configure the amount of time between descheduler runs. The default is 3600 seconds (one hour).
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Cluster administrator privileges
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">KubeDescheduler</code> object:
						</p><pre class="programlisting language-terminal">$ oc edit kubedeschedulers.operator.openshift.io cluster -n openshift-kube-descheduler-operator</pre></li><li class="listitem"><p class="simpara">
							Update the <code class="literal">deschedulingIntervalSeconds</code> field to the desired value:
						</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: KubeDescheduler
metadata:
  name: cluster
  namespace: openshift-kube-descheduler-operator
spec:
  deschedulingIntervalSeconds: 3600 <span id="CO104-1"><!--Empty--></span><span class="callout">1</span>
...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO104-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Set the number of seconds between descheduler runs. A value of <code class="literal">0</code> in this field runs the descheduler once and exits.
								</div></dd></dl></div></li><li class="listitem">
							Save the file to apply the changes.
						</li></ol></div></section><section class="section" id="nodes-descheduler-uninstalling_nodes-descheduler"><div class="titlepage"><div><div><h3 class="title">4.9.6. Uninstalling the descheduler</h3></div></div></div><p>
					You can remove the descheduler from your cluster by removing the descheduler instance and uninstalling the Kube Descheduler Operator. This procedure also cleans up the <code class="literal">KubeDescheduler</code> CRD and <code class="literal">openshift-kube-descheduler-operator</code> namespace.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Cluster administrator privileges.
						</li><li class="listitem">
							Access to the OpenShift Container Platform web console.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the OpenShift Container Platform web console.
						</li><li class="listitem"><p class="simpara">
							Delete the descheduler instance.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									From the <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span> page, click <span class="strong strong"><strong>Kube Descheduler Operator</strong></span>.
								</li><li class="listitem">
									Select the <span class="strong strong"><strong>Kube Descheduler</strong></span> tab.
								</li><li class="listitem">
									Click the Options menu 
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Nodes-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
									 next to the <span class="strong strong"><strong>cluster</strong></span> entry and select <span class="strong strong"><strong>Delete KubeDescheduler</strong></span>.
								</li><li class="listitem">
									In the confirmation dialog, click <span class="strong strong"><strong>Delete</strong></span>.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Uninstall the Kube Descheduler Operator.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
								</li><li class="listitem">
									Click the Options menu 
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Nodes-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
									 next to the <span class="strong strong"><strong>Kube Descheduler Operator</strong></span> entry and select <span class="strong strong"><strong>Uninstall Operator</strong></span>.
								</li><li class="listitem">
									In the confirmation dialog, click <span class="strong strong"><strong>Uninstall</strong></span>.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Delete the <code class="literal">openshift-kube-descheduler-operator</code> namespace.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Namespaces</strong></span>.
								</li><li class="listitem">
									Enter <code class="literal">openshift-kube-descheduler-operator</code> into the filter box.
								</li><li class="listitem">
									Click the Options menu 
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Nodes-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
									 next to the <span class="strong strong"><strong>openshift-kube-descheduler-operator</strong></span> entry and select <span class="strong strong"><strong>Delete Namespace</strong></span>.
								</li><li class="listitem">
									In the confirmation dialog, enter <code class="literal">openshift-kube-descheduler-operator</code> and click <span class="strong strong"><strong>Delete</strong></span>.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Delete the <code class="literal">KubeDescheduler</code> CRD.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Custom Resource Definitions</strong></span>.
								</li><li class="listitem">
									Enter <code class="literal">KubeDescheduler</code> into the filter box.
								</li><li class="listitem">
									Click the Options menu 
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Nodes-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
									 next to the <span class="strong strong"><strong>KubeDescheduler</strong></span> entry and select <span class="strong strong"><strong>Delete CustomResourceDefinition</strong></span>.
								</li><li class="listitem">
									In the confirmation dialog, click <span class="strong strong"><strong>Delete</strong></span>.
								</li></ol></div></li></ol></div></section></section><section class="section" id="secondary-scheduler"><div class="titlepage"><div><div><h2 class="title">4.10. Secondary scheduler</h2></div></div></div><section class="section" id="nodes-secondary-scheduler-about"><div class="titlepage"><div><div><h3 class="title">4.10.1. Secondary scheduler overview</h3></div></div></div><p>
					You can install the Secondary Scheduler Operator to run a custom secondary scheduler alongside the default scheduler to schedule pods.
				</p><section class="section" id="nodes-secondary-scheduler-about_nodes-secondary-scheduler-about"><div class="titlepage"><div><div><h4 class="title">4.10.1.1. About the Secondary Scheduler Operator</h4></div></div></div><p>
						The Secondary Scheduler Operator for Red Hat OpenShift provides a way to deploy a custom secondary scheduler in OpenShift Container Platform. The secondary scheduler runs alongside the default scheduler to schedule pods. Pod configurations can specify which scheduler to use.
					</p><p>
						The custom scheduler must have the <code class="literal">/bin/kube-scheduler</code> binary and be based on the <a class="link" href="https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/">Kubernetes scheduling framework</a>.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							You can use the Secondary Scheduler Operator to deploy a custom secondary scheduler in OpenShift Container Platform, but Red Hat does not directly support the functionality of the custom secondary scheduler.
						</p></div></div><p>
						The Secondary Scheduler Operator creates the default roles and role bindings required by the secondary scheduler. You can specify which scheduling plugins to enable or disable by configuring the <code class="literal">KubeSchedulerConfiguration</code> resource for the secondary scheduler.
					</p></section></section><section class="section" id="nodes-secondary-scheduler-release-notes"><div class="titlepage"><div><div><h3 class="title">4.10.2. Secondary Scheduler Operator for Red Hat OpenShift release notes</h3></div></div></div><p>
					The Secondary Scheduler Operator for Red Hat OpenShift allows you to deploy a custom secondary scheduler in your OpenShift Container Platform cluster.
				</p><p>
					These release notes track the development of the Secondary Scheduler Operator for Red Hat OpenShift.
				</p><p>
					For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-secondary-scheduler-about_nodes-secondary-scheduler-about">About the Secondary Scheduler Operator</a>.
				</p><section class="section" id="secondary-scheduler-operator-release-notes-1.1.2"><div class="titlepage"><div><div><h4 class="title">4.10.2.1. Release notes for Secondary Scheduler Operator for Red Hat OpenShift 1.1.2</h4></div></div></div><p>
						Issued: 2023-8-23
					</p><p>
						The following advisory is available for the Secondary Scheduler Operator for Red Hat OpenShift 1.1.2:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/errata/RHSA-2023:4657">RHSA-2023:4657</a>
							</li></ul></div><section class="section" id="secondary-scheduler-1.1.2-bug-fixes"><div class="titlepage"><div><div><h5 class="title">4.10.2.1.1. Bug fixes</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									This release of the Secondary Scheduler Operator addresses several Common Vulnerabilities and Exposures (CVEs).
								</li></ul></div></section><section class="section" id="secondary-scheduler-operator-1.1.2-known-issues"><div class="titlepage"><div><div><h5 class="title">4.10.2.1.2. Known issues</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Currently, you cannot deploy additional resources, such as config maps, CRDs, or RBAC policies through the Secondary Scheduler Operator. Any resources other than roles and role bindings that are required by your custom secondary scheduler must be applied externally. (<a class="link" href="https://issues.redhat.com/browse/WRKLDS-645"><span class="strong strong"><strong>WRKLDS-645</strong></span></a>)
								</li></ul></div></section></section><section class="section" id="secondary-scheduler-operator-release-notes-1.1.1"><div class="titlepage"><div><div><h4 class="title">4.10.2.2. Release notes for Secondary Scheduler Operator for Red Hat OpenShift 1.1.1</h4></div></div></div><p>
						Issued: 2023-5-18
					</p><p>
						The following advisory is available for the Secondary Scheduler Operator for Red Hat OpenShift 1.1.1:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/errata/RHSA-2023:0584">RHSA-2023:0584</a>
							</li></ul></div><section class="section" id="secondary-scheduler-1.1.1-bug-fixes"><div class="titlepage"><div><div><h5 class="title">4.10.2.2.1. Bug fixes</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									This release of the Secondary Scheduler Operator addresses several Common Vulnerabilities and Exposures (CVEs).
								</li></ul></div></section><section class="section" id="secondary-scheduler-operator-1.1.1-known-issues"><div class="titlepage"><div><div><h5 class="title">4.10.2.2.2. Known issues</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Currently, you cannot deploy additional resources, such as config maps, CRDs, or RBAC policies through the Secondary Scheduler Operator. Any resources other than roles and role bindings that are required by your custom secondary scheduler must be applied externally. (<a class="link" href="https://issues.redhat.com/browse/WRKLDS-645"><span class="strong strong"><strong>WRKLDS-645</strong></span></a>)
								</li></ul></div></section></section><section class="section" id="secondary-scheduler-operator-release-notes-1.1.0"><div class="titlepage"><div><div><h4 class="title">4.10.2.3. Release notes for Secondary Scheduler Operator for Red Hat OpenShift 1.1.0</h4></div></div></div><p>
						Issued: 2022-9-1
					</p><p>
						The following advisory is available for the Secondary Scheduler Operator for Red Hat OpenShift 1.1.0:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/errata/RHSA-2022:6152">RHSA-2022:6152</a>
							</li></ul></div><section class="section" id="secondary-scheduler-operator-1.1.0-new-features-and-enhancements"><div class="titlepage"><div><div><h5 class="title">4.10.2.3.1. New features and enhancements</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The Secondary Scheduler Operator security context configuration has been updated to comply with <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#understanding-and-managing-pod-security-admission">pod security admission enforcement</a>.
								</li></ul></div></section><section class="section" id="secondary-scheduler-operator-1.1.0-known-issues"><div class="titlepage"><div><div><h5 class="title">4.10.2.3.2. Known issues</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Currently, you cannot deploy additional resources, such as config maps, CRDs, or RBAC policies through the Secondary Scheduler Operator. Any resources other than roles and role bindings that are required by your custom secondary scheduler must be applied externally. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2071684"><span class="strong strong"><strong>BZ#2071684</strong></span></a>)
								</li></ul></div></section></section></section><section class="section" id="secondary-scheduler-configuring"><div class="titlepage"><div><div><h3 class="title">4.10.3. Scheduling pods using a secondary scheduler</h3></div></div></div><p>
					You can run a custom secondary scheduler in OpenShift Container Platform by installing the Secondary Scheduler Operator, deploying the secondary scheduler, and setting the secondary scheduler in the pod definition.
				</p><section class="section" id="nodes-secondary-scheduler-install-console_secondary-scheduler-configuring"><div class="titlepage"><div><div><h4 class="title">4.10.3.1. Installing the Secondary Scheduler Operator</h4></div></div></div><p>
						You can use the web console to install the Secondary Scheduler Operator for Red Hat OpenShift.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have access to the OpenShift Container Platform web console.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Log in to the OpenShift Container Platform web console.
							</li><li class="listitem"><p class="simpara">
								Create the required namespace for the Secondary Scheduler Operator for Red Hat OpenShift.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Namespaces</strong></span> and click <span class="strong strong"><strong>Create Namespace</strong></span>.
									</li><li class="listitem">
										Enter <code class="literal">openshift-secondary-scheduler-operator</code> in the <span class="strong strong"><strong>Name</strong></span> field and click <span class="strong strong"><strong>Create</strong></span>.
									</li></ol></div></li><li class="listitem"><p class="simpara">
								Install the Secondary Scheduler Operator for Red Hat OpenShift.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span>.
									</li><li class="listitem">
										Enter <span class="strong strong"><strong>Secondary Scheduler Operator for Red Hat OpenShift</strong></span> into the filter box.
									</li><li class="listitem">
										Select the <span class="strong strong"><strong>Secondary Scheduler Operator for Red Hat OpenShift</strong></span> and click <span class="strong strong"><strong>Install</strong></span>.
									</li><li class="listitem"><p class="simpara">
										On the <span class="strong strong"><strong>Install Operator</strong></span> page:
									</p><div class="orderedlist"><ol class="orderedlist" type="i"><li class="listitem">
												The <span class="strong strong"><strong>Update channel</strong></span> is set to <span class="strong strong"><strong>stable</strong></span>, which installs the latest stable release of the Secondary Scheduler Operator for Red Hat OpenShift.
											</li><li class="listitem">
												Select <span class="strong strong"><strong>A specific namespace on the cluster</strong></span> and select <span class="strong strong"><strong>openshift-secondary-scheduler-operator</strong></span> from the drop-down menu.
											</li><li class="listitem"><p class="simpara">
												Select an <span class="strong strong"><strong>Update approval</strong></span> strategy.
											</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
														The <span class="strong strong"><strong>Automatic</strong></span> strategy allows Operator Lifecycle Manager (OLM) to automatically update the Operator when a new version is available.
													</li><li class="listitem">
														The <span class="strong strong"><strong>Manual</strong></span> strategy requires a user with appropriate credentials to approve the Operator update.
													</li></ul></div></li><li class="listitem">
												Click <span class="strong strong"><strong>Install</strong></span>.
											</li></ol></div></li></ol></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
							</li><li class="listitem">
								Verify that <span class="strong strong"><strong>Secondary Scheduler Operator for Red Hat OpenShift</strong></span> is listed with a <span class="strong strong"><strong>Status</strong></span> of <span class="strong strong"><strong>Succeeded</strong></span>.
							</li></ol></div></section><section class="section" id="nodes-secondary-scheduler-configuring-console_secondary-scheduler-configuring"><div class="titlepage"><div><div><h4 class="title">4.10.3.2. Deploying a secondary scheduler</h4></div></div></div><p>
						After you have installed the Secondary Scheduler Operator, you can deploy a secondary scheduler.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisities</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have access to the OpenShift Container Platform web console.
							</li><li class="listitem">
								The Secondary Scheduler Operator for Red Hat OpenShift is installed.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Log in to the OpenShift Container Platform web console.
							</li><li class="listitem"><p class="simpara">
								Create config map to hold the configuration for the secondary scheduler.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>ConfigMaps</strong></span>.
									</li><li class="listitem">
										Click <span class="strong strong"><strong>Create ConfigMap</strong></span>.
									</li><li class="listitem"><p class="simpara">
										In the YAML editor, enter the config map definition that contains the necessary <code class="literal">KubeSchedulerConfiguration</code> configuration. For example:
									</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: "secondary-scheduler-config"                  <span id="CO105-1"><!--Empty--></span><span class="callout">1</span>
  namespace: "openshift-secondary-scheduler-operator" <span id="CO105-2"><!--Empty--></span><span class="callout">2</span>
data:
  "config.yaml": |
    apiVersion: kubescheduler.config.k8s.io/v1beta3
    kind: KubeSchedulerConfiguration                  <span id="CO105-3"><!--Empty--></span><span class="callout">3</span>
    leaderElection:
      leaderElect: false
    profiles:
      - schedulerName: secondary-scheduler            <span id="CO105-4"><!--Empty--></span><span class="callout">4</span>
        plugins:                                      <span id="CO105-5"><!--Empty--></span><span class="callout">5</span>
          score:
            disabled:
              - name: NodeResourcesBalancedAllocation
              - name: NodeResourcesLeastAllocated</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO105-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												The name of the config map. This is used in the <span class="strong strong"><strong>Scheduler Config</strong></span> field when creating the <code class="literal">SecondaryScheduler</code> CR.
											</div></dd><dt><a href="#CO105-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												The config map must be created in the <code class="literal">openshift-secondary-scheduler-operator</code> namespace.
											</div></dd><dt><a href="#CO105-3"><span class="callout">3</span></a> </dt><dd><div class="para">
												The <code class="literal">KubeSchedulerConfiguration</code> resource for the secondary scheduler. For more information, see <a class="link" href="https://kubernetes.io/docs/reference/config-api/kube-scheduler-config.v1beta3/#kubescheduler-config-k8s-io-v1beta3-KubeSchedulerConfiguration"><code class="literal">KubeSchedulerConfiguration</code></a> in the Kubernetes API documentation.
											</div></dd><dt><a href="#CO105-4"><span class="callout">4</span></a> </dt><dd><div class="para">
												The name of the secondary scheduler. Pods that set their <code class="literal">spec.schedulerName</code> field to this value are scheduled with this secondary scheduler.
											</div></dd><dt><a href="#CO105-5"><span class="callout">5</span></a> </dt><dd><div class="para">
												The plugins to enable or disable for the secondary scheduler. For a list default scheduling plugins, see <a class="link" href="https://kubernetes.io/docs/reference/scheduling/config/#scheduling-plugins">Scheduling plugins</a> in the Kubernetes documentation.
											</div></dd></dl></div></li><li class="listitem">
										Click <span class="strong strong"><strong>Create</strong></span>.
									</li></ol></div></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">SecondaryScheduler</code> CR:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
									</li><li class="listitem">
										Select <span class="strong strong"><strong>Secondary Scheduler Operator for Red Hat OpenShift</strong></span>.
									</li><li class="listitem">
										Select the <span class="strong strong"><strong>Secondary Scheduler</strong></span> tab and click <span class="strong strong"><strong>Create SecondaryScheduler</strong></span>.
									</li><li class="listitem">
										The <span class="strong strong"><strong>Name</strong></span> field defaults to <code class="literal">cluster</code>; do not change this name.
									</li><li class="listitem">
										The <span class="strong strong"><strong>Scheduler Config</strong></span> field defaults to <code class="literal">secondary-scheduler-config</code>. Ensure that this value matches the name of the config map created earlier in this procedure.
									</li><li class="listitem"><p class="simpara">
										In the <span class="strong strong"><strong>Scheduler Image</strong></span> field, enter the image name for your custom scheduler.
									</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
											Red Hat does not directly support the functionality of your custom secondary scheduler.
										</p></div></div></li><li class="listitem">
										Click <span class="strong strong"><strong>Create</strong></span>.
									</li></ol></div></li></ol></div></section><section class="section" id="nodes-secondary-scheduler-pod-console_secondary-scheduler-configuring"><div class="titlepage"><div><div><h4 class="title">4.10.3.3. Scheduling a pod using the secondary scheduler</h4></div></div></div><p>
						To schedule a pod using the secondary scheduler, set the <code class="literal">schedulerName</code> field in the pod definition.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisities</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have access to the OpenShift Container Platform web console.
							</li><li class="listitem">
								The Secondary Scheduler Operator for Red Hat OpenShift is installed.
							</li><li class="listitem">
								A secondary scheduler is configured.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Log in to the OpenShift Container Platform web console.
							</li><li class="listitem">
								Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Pods</strong></span>.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Create Pod</strong></span>.
							</li><li class="listitem"><p class="simpara">
								In the YAML editor, enter the desired pod configuration and add the <code class="literal">schedulerName</code> field:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: nginx
  namespace: default
spec:
  containers:
    - name: nginx
      image: nginx:1.14.2
      ports:
        - containerPort: 80
  schedulerName: secondary-scheduler <span id="CO106-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO106-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The <code class="literal">schedulerName</code> field must match the name that is defined in the config map when you configured the secondary scheduler.
									</div></dd></dl></div></li><li class="listitem">
								Click <span class="strong strong"><strong>Create</strong></span>.
							</li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Log in to the OpenShift CLI.
							</li><li class="listitem"><p class="simpara">
								Describe the pod using the following command:
							</p><pre class="programlisting language-terminal">$ oc describe pod nginx -n default</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-text">Name:         nginx
Namespace:    default
Priority:     0
Node:         ci-ln-t0w4r1k-72292-xkqs4-worker-b-xqkxp/10.0.128.3
...
Events:
  Type    Reason          Age   From                 Message
  ----    ------          ----  ----                 -------
  Normal  Scheduled       12s   secondary-scheduler  Successfully assigned default/nginx to ci-ln-t0w4r1k-72292-xkqs4-worker-b-xqkxp
...</pre>

								</p></div></li><li class="listitem">
								In the events table, find the event with a message similar to <code class="literal">Successfully assigned &lt;namespace&gt;/&lt;pod_name&gt; to &lt;node_name&gt;</code>.
							</li><li class="listitem"><p class="simpara">
								In the "From" column, verify that the event was generated from the secondary scheduler and not the default scheduler.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									You can also check the <code class="literal">secondary-scheduler-*</code> pod logs in the <code class="literal">openshift-secondary-scheduler-namespace</code> to verify that the pod was scheduled by the secondary scheduler.
								</p></div></div></li></ol></div></section></section><section class="section" id="secondary-scheduler-uninstalling"><div class="titlepage"><div><div><h3 class="title">4.10.4. Uninstalling the Secondary Scheduler Operator</h3></div></div></div><p>
					You can remove the Secondary Scheduler Operator for Red Hat OpenShift from OpenShift Container Platform by uninstalling the Operator and removing its related resources.
				</p><section class="section" id="nodes-secondary-scheduler-uninstall-console_secondary-scheduler-uninstalling"><div class="titlepage"><div><div><h4 class="title">4.10.4.1. Uninstalling the Secondary Scheduler Operator</h4></div></div></div><p>
						You can uninstall the Secondary Scheduler Operator for Red Hat OpenShift by using the web console.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have access to the OpenShift Container Platform web console.
							</li><li class="listitem">
								The Secondary Scheduler Operator for Red Hat OpenShift is installed.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Log in to the OpenShift Container Platform web console.
							</li><li class="listitem"><p class="simpara">
								Uninstall the Secondary Scheduler Operator for Red Hat OpenShift Operator.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
									</li><li class="listitem">
										Click the Options menu 
										<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Nodes-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
										 next to the <span class="strong strong"><strong>Secondary Scheduler Operator</strong></span> entry and click <span class="strong strong"><strong>Uninstall Operator</strong></span>.
									</li><li class="listitem">
										In the confirmation dialog, click <span class="strong strong"><strong>Uninstall</strong></span>.
									</li></ol></div></li></ol></div></section><section class="section" id="nodes-secondary-scheduler-remove-resources-console_secondary-scheduler-uninstalling"><div class="titlepage"><div><div><h4 class="title">4.10.4.2. Removing Secondary Scheduler Operator resources</h4></div></div></div><p>
						Optionally, after uninstalling the Secondary Scheduler Operator for Red Hat OpenShift, you can remove its related resources from your cluster.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster with <code class="literal">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have access to the OpenShift Container Platform web console.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Log in to the OpenShift Container Platform web console.
							</li><li class="listitem"><p class="simpara">
								Remove CRDs that were installed by the Secondary Scheduler Operator:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>CustomResourceDefinitions</strong></span>.
									</li><li class="listitem">
										Enter <code class="literal">SecondaryScheduler</code> in the <span class="strong strong"><strong>Name</strong></span> field to filter the CRDs.
									</li><li class="listitem">
										Click the Options menu 
										<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Nodes-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
										 next to the <span class="strong strong"><strong>SecondaryScheduler</strong></span> CRD and select <span class="strong strong"><strong>Delete Custom Resource Definition</strong></span>:
									</li></ol></div></li><li class="listitem"><p class="simpara">
								Remove the <code class="literal">openshift-secondary-scheduler-operator</code> namespace.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Namespaces</strong></span>.
									</li><li class="listitem">
										Click the Options menu 
										<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Nodes-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
										 next to the <span class="strong strong"><strong>openshift-secondary-scheduler-operator</strong></span> and select <span class="strong strong"><strong>Delete Namespace</strong></span>.
									</li><li class="listitem">
										In the confirmation dialog, enter <code class="literal">openshift-secondary-scheduler-operator</code> in the field and click <span class="strong strong"><strong>Delete</strong></span>.
									</li></ol></div></li></ol></div></section></section></section></section><section class="chapter" id="using-jobs-and-daemonsets"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Using Jobs and DaemonSets</h1></div></div></div><section class="section" id="nodes-pods-daemonsets"><div class="titlepage"><div><div><h2 class="title">5.1. Running background tasks on nodes automatically with daemon sets</h2></div></div></div><p>
				As an administrator, you can create and use daemon sets to run replicas of a pod on specific or all nodes in an OpenShift Container Platform cluster.
			</p><p>
				A daemon set ensures that all (or some) nodes run a copy of a pod. As nodes are added to the cluster, pods are added to the cluster. As nodes are removed from the cluster, those pods are removed through garbage collection. Deleting a daemon set will clean up the pods it created.
			</p><p>
				You can use daemon sets to create shared storage, run a logging pod on every node in your cluster, or deploy a monitoring agent on every node.
			</p><p>
				For security reasons, the cluster administrators and the project administrators can create daemon sets.
			</p><p>
				For more information on daemon sets, see the <a class="link" href="http://kubernetes.io/docs/admin/daemons/">Kubernetes documentation</a>.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Daemon set scheduling is incompatible with project’s default node selector. If you fail to disable it, the daemon set gets restricted by merging with the default node selector. This results in frequent pod recreates on the nodes that got unselected by the merged node selector, which in turn puts unwanted load on the cluster.
				</p></div></div><section class="section" id="scheduled-by-default-scheduler"><div class="titlepage"><div><div><h3 class="title">5.1.1. Scheduled by default scheduler</h3></div></div></div><p>
					A daemon set ensures that all eligible nodes run a copy of a pod. Normally, the node that a pod runs on is selected by the Kubernetes scheduler. However, daemon set pods are created and scheduled by the daemon set controller. That introduces the following issues:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Inconsistent pod behavior: Normal pods waiting to be scheduled are created and in Pending state, but daemon set pods are not created in <code class="literal">Pending</code> state. This is confusing to the user.
						</li><li class="listitem">
							Pod preemption is handled by default scheduler. When preemption is enabled, the daemon set controller will make scheduling decisions without considering pod priority and preemption.
						</li></ul></div><p>
					The <span class="strong strong"><strong>ScheduleDaemonSetPods</strong></span> feature, enabled by default in OpenShift Container Platform, lets you schedule daemon sets using the default scheduler instead of the daemon set controller, by adding the <code class="literal">NodeAffinity</code> term to the daemon set pods, instead of the <code class="literal">spec.nodeName</code> term. The default scheduler is then used to bind the pod to the target host. If node affinity of the daemon set pod already exists, it is replaced. The daemon set controller only performs these operations when creating or modifying daemon set pods, and no changes are made to the <code class="literal">spec.template</code> of the daemon set.
				</p><pre class="programlisting language-yaml">kind: Pod
apiVersion: v1
metadata:
  name: hello-node-6fbccf8d9-9tmzr
#...
spec:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchFields:
        - key: metadata.name
          operator: In
          values:
          - target-host-name
#...</pre><p>
					In addition, a <code class="literal">node.kubernetes.io/unschedulable:NoSchedule</code> toleration is added automatically to daemon set pods. The default scheduler ignores unschedulable Nodes when scheduling daemon set pods.
				</p></section><section class="section" id="nodes-pods-daemonsets-creating_nodes-pods-daemonsets"><div class="titlepage"><div><div><h3 class="title">5.1.2. Creating daemonsets</h3></div></div></div><p>
					When creating daemon sets, the <code class="literal">nodeSelector</code> field is used to indicate the nodes on which the daemon set should deploy replicas.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Before you start using daemon sets, disable the default project-wide node selector in your namespace, by setting the namespace annotation <code class="literal">openshift.io/node-selector</code> to an empty string:
						</p><pre class="programlisting language-terminal">$ oc patch namespace myproject -p \
    '{"metadata": {"annotations": {"openshift.io/node-selector": ""}}}'</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can alternatively apply the following YAML to disable the default project-wide node selector for a namespace:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: &lt;namespace&gt;
  annotations:
    openshift.io/node-selector: ''
#...</pre></div></div></li><li class="listitem"><p class="simpara">
							If you are creating a new project, overwrite the default node selector:
						</p><pre class="programlisting language-terminal">$ oc adm new-project &lt;name&gt; --node-selector=""</pre></li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To create a daemon set:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Define the daemon set yaml file:
						</p><pre class="programlisting language-yaml">apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: hello-daemonset
spec:
  selector:
      matchLabels:
        name: hello-daemonset <span id="CO107-1"><!--Empty--></span><span class="callout">1</span>
  template:
    metadata:
      labels:
        name: hello-daemonset <span id="CO107-2"><!--Empty--></span><span class="callout">2</span>
    spec:
      nodeSelector: <span id="CO107-3"><!--Empty--></span><span class="callout">3</span>
        role: worker
      containers:
      - image: openshift/hello-openshift
        imagePullPolicy: Always
        name: registry
        ports:
        - containerPort: 80
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
      serviceAccount: default
      terminationGracePeriodSeconds: 10
#...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO107-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The label selector that determines which pods belong to the daemon set.
								</div></dd><dt><a href="#CO107-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The pod template’s label selector. Must match the label selector above.
								</div></dd><dt><a href="#CO107-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The node selector that determines on which nodes pod replicas should be deployed. A matching label must be present on the node.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the daemon set object:
						</p><pre class="programlisting language-terminal">$ oc create -f daemonset.yaml</pre></li><li class="listitem"><p class="simpara">
							To verify that the pods were created, and that each node has a pod replica:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Find the daemonset pods:
								</p><pre class="programlisting language-terminal">$ oc get pods</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">hello-daemonset-cx6md   1/1       Running   0          2m
hello-daemonset-e3md9   1/1       Running   0          2m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									View the pods to verify the pod has been placed onto the node:
								</p><pre class="programlisting language-terminal">$ oc describe pod/hello-daemonset-cx6md|grep Node</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">Node:        openshift-node01.hostname.com/10.14.20.134</pre>

									</p></div><pre class="programlisting language-terminal">$ oc describe pod/hello-daemonset-e3md9|grep Node</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">Node:        openshift-node02.hostname.com/10.14.20.137</pre>

									</p></div></li></ol></div></li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								If you update a daemon set pod template, the existing pod replicas are not affected.
							</li><li class="listitem">
								If you delete a daemon set and then create a new daemon set with a different template but the same label selector, it recognizes any existing pod replicas as having matching labels and thus does not update them or create new replicas despite a mismatch in the pod template.
							</li><li class="listitem">
								If you change node labels, the daemon set adds pods to nodes that match the new labels and deletes pods from nodes that do not match the new labels.
							</li></ul></div><p>
						To update a daemon set, force new pod replicas to be created by deleting the old replicas or nodes.
					</p></div></div></section></section><section class="section" id="nodes-nodes-jobs"><div class="titlepage"><div><div><h2 class="title">5.2. Running tasks in pods using jobs</h2></div></div></div><p>
				A <span class="emphasis"><em>job</em></span> executes a task in your OpenShift Container Platform cluster.
			</p><p>
				A job tracks the overall progress of a task and updates its status with information about active, succeeded, and failed pods. Deleting a job will clean up any pod replicas it created. Jobs are part of the Kubernetes API, which can be managed with <code class="literal">oc</code> commands like other object types.
			</p><div class="formalpara"><p class="title"><strong>Sample Job specification</strong></p><p>
					
<pre class="programlisting language-yaml">apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  parallelism: 1    <span id="CO108-1"><!--Empty--></span><span class="callout">1</span>
  completions: 1    <span id="CO108-2"><!--Empty--></span><span class="callout">2</span>
  activeDeadlineSeconds: 1800 <span id="CO108-3"><!--Empty--></span><span class="callout">3</span>
  backoffLimit: 6   <span id="CO108-4"><!--Empty--></span><span class="callout">4</span>
  template:         <span id="CO108-5"><!--Empty--></span><span class="callout">5</span>
    metadata:
      name: pi
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: OnFailure    <span id="CO108-6"><!--Empty--></span><span class="callout">6</span>
#...</pre>

				</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO108-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						The pod replicas a job should run in parallel.
					</div></dd><dt><a href="#CO108-2"><span class="callout">2</span></a> </dt><dd><div class="para">
						Successful pod completions are needed to mark a job completed.
					</div></dd><dt><a href="#CO108-3"><span class="callout">3</span></a> </dt><dd><div class="para">
						The maximum duration the job can run.
					</div></dd><dt><a href="#CO108-4"><span class="callout">4</span></a> </dt><dd><div class="para">
						The number of retries for a job.
					</div></dd><dt><a href="#CO108-5"><span class="callout">5</span></a> </dt><dd><div class="para">
						The template for the pod the controller creates.
					</div></dd><dt><a href="#CO108-6"><span class="callout">6</span></a> </dt><dd><div class="para">
						The restart policy of the pod.
					</div></dd></dl></div><div class="itemizedlist"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">Jobs</a> in the Kubernetes documentation
					</li></ul></div><section class="section" id="nodes-nodes-jobs-about_nodes-nodes-jobs"><div class="titlepage"><div><div><h3 class="title">5.2.1. Understanding jobs and cron jobs</h3></div></div></div><p>
					A job tracks the overall progress of a task and updates its status with information about active, succeeded, and failed pods. Deleting a job cleans up any pods it created. Jobs are part of the Kubernetes API, which can be managed with <code class="literal">oc</code> commands like other object types.
				</p><p>
					There are two possible resource types that allow creating run-once objects in OpenShift Container Platform:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Job</span></dt><dd><p class="simpara">
								A regular job is a run-once object that creates a task and ensures the job finishes.
							</p><p class="simpara">
								There are three main types of task suitable to run as a job:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
										Non-parallel jobs:
									</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
												A job that starts only one pod, unless the pod fails.
											</li><li class="listitem">
												The job is complete as soon as its pod terminates successfully.
											</li></ul></div></li><li class="listitem"><p class="simpara">
										Parallel jobs with a fixed completion count:
									</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
												a job that starts multiple pods.
											</li><li class="listitem">
												The job represents the overall task and is complete when there is one successful pod for each value in the range <code class="literal">1</code> to the <code class="literal">completions</code> value.
											</li></ul></div></li><li class="listitem"><p class="simpara">
										Parallel jobs with a work queue:
									</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
												A job with multiple parallel worker processes in a given pod.
											</li><li class="listitem">
												OpenShift Container Platform coordinates pods to determine what each should work on or use an external queue service.
											</li><li class="listitem">
												Each pod is independently capable of determining whether or not all peer pods are complete and that the entire job is done.
											</li><li class="listitem">
												When any pod from the job terminates with success, no new pods are created.
											</li><li class="listitem">
												When at least one pod has terminated with success and all pods are terminated, the job is successfully completed.
											</li><li class="listitem"><p class="simpara">
												When any pod has exited with success, no other pod should be doing any work for this task or writing any output. Pods should all be in the process of exiting.
											</p><p class="simpara">
												For more information about how to make use of the different types of job, see <a class="link" href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#job-patterns">Job Patterns</a> in the Kubernetes documentation.
											</p></li></ul></div></li></ul></div></dd><dt><span class="term">Cron job</span></dt><dd><p class="simpara">
								A job can be scheduled to run multiple times, using a cron job.
							</p><p class="simpara">
								A <span class="emphasis"><em>cron job</em></span> builds on a regular job by allowing you to specify how the job should be run. Cron jobs are part of the <a class="link" href="http://kubernetes.io/docs/user-guide/cron-jobs">Kubernetes</a> API, which can be managed with <code class="literal">oc</code> commands like other object types.
							</p><p class="simpara">
								Cron jobs are useful for creating periodic and recurring tasks, like running backups or sending emails. Cron jobs can also schedule individual tasks for a specific time, such as if you want to schedule a job for a low activity period. A cron job creates a <code class="literal">Job</code> object based on the timezone configured on the control plane node that runs the cronjob controller.
							</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
									A cron job creates a <code class="literal">Job</code> object approximately once per execution time of its schedule, but there are circumstances in which it fails to create a job or two jobs might be created. Therefore, jobs must be idempotent and you must configure history limits.
								</p></div></div></dd></dl></div><section class="section" id="jobs-create_nodes-nodes-jobs"><div class="titlepage"><div><div><h4 class="title">5.2.1.1. Understanding how to create jobs</h4></div></div></div><p>
						Both resource types require a job configuration that consists of the following key parts:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								A pod template, which describes the pod that OpenShift Container Platform creates.
							</li><li class="listitem"><p class="simpara">
								The <code class="literal">parallelism</code> parameter, which specifies how many pods running in parallel at any point in time should execute a job.
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										For non-parallel jobs, leave unset. When unset, defaults to <code class="literal">1</code>.
									</li></ul></div></li><li class="listitem"><p class="simpara">
								The <code class="literal">completions</code> parameter, specifying how many successful pod completions are needed to finish a job.
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										For non-parallel jobs, leave unset. When unset, defaults to <code class="literal">1</code>.
									</li><li class="listitem">
										For parallel jobs with a fixed completion count, specify a value.
									</li><li class="listitem">
										For parallel jobs with a work queue, leave unset. When unset defaults to the <code class="literal">parallelism</code> value.
									</li></ul></div></li></ul></div></section><section class="section" id="jobs-set-max_nodes-nodes-jobs"><div class="titlepage"><div><div><h4 class="title">5.2.1.2. Understanding how to set a maximum duration for jobs</h4></div></div></div><p>
						When defining a job, you can define its maximum duration by setting the <code class="literal">activeDeadlineSeconds</code> field. It is specified in seconds and is not set by default. When not set, there is no maximum duration enforced.
					</p><p>
						The maximum duration is counted from the time when a first pod gets scheduled in the system, and defines how long a job can be active. It tracks overall time of an execution. After reaching the specified timeout, the job is terminated by OpenShift Container Platform.
					</p></section><section class="section" id="jobs-set-backoff_nodes-nodes-jobs"><div class="titlepage"><div><div><h4 class="title">5.2.1.3. Understanding how to set a job back off policy for pod failure</h4></div></div></div><p>
						A job can be considered failed, after a set amount of retries due to a logical error in configuration or other similar reasons. Failed pods associated with the job are recreated by the controller with an exponential back off delay (<code class="literal">10s</code>, <code class="literal">20s</code>, <code class="literal">40s</code> …) capped at six minutes. The limit is reset if no new failed pods appear between controller checks.
					</p><p>
						Use the <code class="literal">spec.backoffLimit</code> parameter to set the number of retries for a job.
					</p></section><section class="section" id="jobs-artifacts_nodes-nodes-jobs"><div class="titlepage"><div><div><h4 class="title">5.2.1.4. Understanding how to configure a cron job to remove artifacts</h4></div></div></div><p>
						Cron jobs can leave behind artifact resources such as jobs or pods. As a user it is important to configure history limits so that old jobs and their pods are properly cleaned. There are two fields within cron job’s spec responsible for that:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<code class="literal">.spec.successfulJobsHistoryLimit</code>. The number of successful finished jobs to retain (defaults to 3).
							</li><li class="listitem">
								<code class="literal">.spec.failedJobsHistoryLimit</code>. The number of failed finished jobs to retain (defaults to 1).
							</li></ul></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Delete cron jobs that you no longer need:
							</p><pre class="programlisting language-terminal">$ oc delete cronjob/&lt;cron_job_name&gt;</pre><p class="simpara">
								Doing this prevents them from generating unnecessary artifacts.
							</p></li><li class="listitem">
								You can suspend further executions by setting the <code class="literal">spec.suspend</code> to true. All subsequent executions are suspended until you reset to <code class="literal">false</code>.
							</li></ul></div></div></div></section><section class="section" id="jobs-limits_nodes-nodes-jobs"><div class="titlepage"><div><div><h4 class="title">5.2.1.5. Known limitations</h4></div></div></div><p>
						The job specification restart policy only applies to the <span class="emphasis"><em>pods</em></span>, and not the <span class="emphasis"><em>job controller</em></span>. However, the job controller is hard-coded to keep retrying jobs to completion.
					</p><p>
						As such, <code class="literal">restartPolicy: Never</code> or <code class="literal">--restart=Never</code> results in the same behavior as <code class="literal">restartPolicy: OnFailure</code> or <code class="literal">--restart=OnFailure</code>. That is, when a job fails it is restarted automatically until it succeeds (or is manually discarded). The policy only sets which subsystem performs the restart.
					</p><p>
						With the <code class="literal">Never</code> policy, the <span class="emphasis"><em>job controller</em></span> performs the restart. With each attempt, the job controller increments the number of failures in the job status and create new pods. This means that with each failed attempt, the number of pods increases.
					</p><p>
						With the <code class="literal">OnFailure</code> policy, <span class="emphasis"><em>kubelet</em></span> performs the restart. Each attempt does not increment the number of failures in the job status. In addition, kubelet will retry failed jobs starting pods on the same nodes.
					</p></section></section><section class="section" id="nodes-nodes-jobs-creating_nodes-nodes-jobs"><div class="titlepage"><div><div><h3 class="title">5.2.2. Creating jobs</h3></div></div></div><p>
					You create a job in OpenShift Container Platform by creating a job object.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To create a job:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a YAML file similar to the following:
						</p><pre class="programlisting language-yaml">apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  parallelism: 1    <span id="CO109-1"><!--Empty--></span><span class="callout">1</span>
  completions: 1    <span id="CO109-2"><!--Empty--></span><span class="callout">2</span>
  activeDeadlineSeconds: 1800 <span id="CO109-3"><!--Empty--></span><span class="callout">3</span>
  backoffLimit: 6   <span id="CO109-4"><!--Empty--></span><span class="callout">4</span>
  template:         <span id="CO109-5"><!--Empty--></span><span class="callout">5</span>
    metadata:
      name: pi
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: OnFailure    <span id="CO109-6"><!--Empty--></span><span class="callout">6</span>
#...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO109-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Optional: Specify how many pod replicas a job should run in parallel; defaults to <code class="literal">1</code>.
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											For non-parallel jobs, leave unset. When unset, defaults to <code class="literal">1</code>.
										</li></ul></div></dd><dt><a href="#CO109-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Optional: Specify how many successful pod completions are needed to mark a job completed.
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											For non-parallel jobs, leave unset. When unset, defaults to <code class="literal">1</code>.
										</li><li class="listitem">
											For parallel jobs with a fixed completion count, specify the number of completions.
										</li><li class="listitem">
											For parallel jobs with a work queue, leave unset. When unset defaults to the <code class="literal">parallelism</code> value.
										</li></ul></div></dd><dt><a href="#CO109-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Optional: Specify the maximum duration the job can run.
								</div></dd><dt><a href="#CO109-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Optional: Specify the number of retries for a job. This field defaults to six.
								</div></dd><dt><a href="#CO109-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Specify the template for the pod the controller creates.
								</div></dd><dt><a href="#CO109-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Specify the restart policy of the pod:
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">Never</code>. Do not restart the job.
										</li><li class="listitem">
											<code class="literal">OnFailure</code>. Restart the job only if it fails.
										</li><li class="listitem"><p class="simpara">
											<code class="literal">Always</code>. Always restart the job.
										</p><p class="simpara">
											For details on how OpenShift Container Platform uses restart policy with failed containers, see the <a class="link" href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#example-states">Example States</a> in the Kubernetes documentation.
										</p></li></ul></div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the job:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can also create and launch a job from a single command using <code class="literal">oc create job</code>. The following command creates and launches a job similar to the one specified in the previous example:
					</p><pre class="programlisting language-terminal">$ oc create job pi --image=perl -- perl -Mbignum=bpi -wle 'print bpi(2000)'</pre></div></div></section><section class="section" id="nodes-nodes-jobs-creating-cron_nodes-nodes-jobs"><div class="titlepage"><div><div><h3 class="title">5.2.3. Creating cron jobs</h3></div></div></div><p>
					You create a cron job in OpenShift Container Platform by creating a job object.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To create a cron job:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a YAML file similar to the following:
						</p><pre class="programlisting language-yaml">apiVersion: batch/v1
kind: CronJob
metadata:
  name: pi
spec:
  schedule: "*/1 * * * *"          <span id="CO110-1"><!--Empty--></span><span class="callout">1</span>
  timeZone: Etc/UTC                <span id="CO110-2"><!--Empty--></span><span class="callout">2</span>
  concurrencyPolicy: "Replace"     <span id="CO110-3"><!--Empty--></span><span class="callout">3</span>
  startingDeadlineSeconds: 200     <span id="CO110-4"><!--Empty--></span><span class="callout">4</span>
  suspend: true                    <span id="CO110-5"><!--Empty--></span><span class="callout">5</span>
  successfulJobsHistoryLimit: 3    <span id="CO110-6"><!--Empty--></span><span class="callout">6</span>
  failedJobsHistoryLimit: 1        <span id="CO110-7"><!--Empty--></span><span class="callout">7</span>
  jobTemplate:                     <span id="CO110-8"><!--Empty--></span><span class="callout">8</span>
    spec:
      template:
        metadata:
          labels:                  <span id="CO110-9"><!--Empty--></span><span class="callout">9</span>
            parent: "cronjobpi"
        spec:
          containers:
          - name: pi
            image: perl
            command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
          restartPolicy: OnFailure <span id="CO110-10"><!--Empty--></span><span class="callout">10</span>
#...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO110-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Schedule for the job specified in <a class="link" href="https://en.wikipedia.org/wiki/Cron">cron format</a>. In this example, the job will run every minute.
								</div></dd><dt><a href="#CO110-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									An optional time zone for the schedule. See <a class="link" href="https://en.wikipedia.org/wiki/List_of_tz_database_time_zones">List of tz database time zones</a> for valid options. If not specified, the Kubernetes controller manager interprets the schedule relative to its local time zone. This setting is offered as a <a class="link" href="https://access.redhat.com/support/offerings/techpreview">Technology Preview</a>.
								</div></dd><dt><a href="#CO110-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									An optional concurrency policy, specifying how to treat concurrent jobs within a cron job. Only one of the following concurrent policies may be specified. If not specified, this defaults to allowing concurrent executions.
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">Allow</code> allows cron jobs to run concurrently.
										</li><li class="listitem">
											<code class="literal">Forbid</code> forbids concurrent runs, skipping the next run if the previous has not finished yet.
										</li><li class="listitem">
											<code class="literal">Replace</code> cancels the currently running job and replaces it with a new one.
										</li></ul></div></dd><dt><a href="#CO110-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									An optional deadline (in seconds) for starting the job if it misses its scheduled time for any reason. Missed jobs executions will be counted as failed ones. If not specified, there is no deadline.
								</div></dd><dt><a href="#CO110-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									An optional flag allowing the suspension of a cron job. If set to <code class="literal">true</code>, all subsequent executions will be suspended.
								</div></dd><dt><a href="#CO110-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									The number of successful finished jobs to retain (defaults to 3).
								</div></dd><dt><a href="#CO110-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									The number of failed finished jobs to retain (defaults to 1).
								</div></dd><dt><a href="#CO110-8"><span class="callout">8</span></a> </dt><dd><div class="para">
									Job template. This is similar to the job example.
								</div></dd><dt><a href="#CO110-9"><span class="callout">9</span></a> </dt><dd><div class="para">
									Sets a label for jobs spawned by this cron job.
								</div></dd><dt><a href="#CO110-10"><span class="callout">10</span></a> </dt><dd><div class="para">
									The restart policy of the pod. This does not apply to the job controller.
								</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										The <code class="literal">.spec.successfulJobsHistoryLimit</code> and <code class="literal">.spec.failedJobsHistoryLimit</code> fields are optional. These fields specify how many completed and failed jobs should be kept. By default, they are set to <code class="literal">3</code> and <code class="literal">1</code> respectively. Setting a limit to <code class="literal">0</code> corresponds to keeping none of the corresponding kind of jobs after they finish.
									</p></div></div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the cron job:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can also create and launch a cron job from a single command using <code class="literal">oc create cronjob</code>. The following command creates and launches a cron job similar to the one specified in the previous example:
					</p><pre class="programlisting language-terminal">$ oc create cronjob pi --image=perl --schedule='*/1 * * * *' -- perl -Mbignum=bpi -wle 'print bpi(2000)'</pre><p>
						With <code class="literal">oc create cronjob</code>, the <code class="literal">--schedule</code> option accepts schedules in <a class="link" href="https://en.wikipedia.org/wiki/Cron">cron format</a>.
					</p></div></div></section></section></section><section class="chapter" id="working-with-nodes"><div class="titlepage"><div><div><h1 class="title">Chapter 6. Working with nodes</h1></div></div></div><section class="section" id="nodes-nodes-viewing"><div class="titlepage"><div><div><h2 class="title">6.1. Viewing and listing the nodes in your OpenShift Container Platform cluster</h2></div></div></div><p>
				You can list all the nodes in your cluster to obtain information such as status, age, memory usage, and details about the nodes.
			</p><p>
				When you perform node management operations, the CLI interacts with node objects that are representations of actual node hosts. The master uses the information from node objects to validate nodes with health checks.
			</p><section class="section" id="nodes-nodes-viewing-listing_nodes-nodes-viewing"><div class="titlepage"><div><div><h3 class="title">6.1.1. About listing all the nodes in a cluster</h3></div></div></div><p>
					You can get detailed information on the nodes in the cluster.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							The following command lists all nodes:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><p class="simpara">
							The following example is a cluster with healthy nodes:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                   STATUS    ROLES     AGE       VERSION
master.example.com     Ready     master    7h        v1.26.0
node1.example.com      Ready     worker    7h        v1.26.0
node2.example.com      Ready     worker    7h        v1.26.0</pre>

							</p></div><p class="simpara">
							The following example is a cluster with one unhealthy node:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                   STATUS                      ROLES     AGE       VERSION
master.example.com     Ready                       master    7h        v1.26.0
node1.example.com      NotReady,SchedulingDisabled worker    7h        v1.26.0
node2.example.com      Ready                       worker    7h        v1.26.0</pre>

							</p></div><p class="simpara">
							The conditions that trigger a <code class="literal">NotReady</code> status are shown later in this section.
						</p></li><li class="listitem"><p class="simpara">
							The <code class="literal">-o wide</code> option provides additional information on nodes.
						</p><pre class="programlisting language-terminal">$ oc get nodes -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                STATUS   ROLES    AGE    VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                                                       KERNEL-VERSION                 CONTAINER-RUNTIME
master.example.com  Ready    master   171m   v1.26.0   10.0.129.108   &lt;none&gt;        Red Hat Enterprise Linux CoreOS 48.83.202103210901-0 (Ootpa)   4.18.0-240.15.1.el8_3.x86_64   cri-o://1.26.0-30.rhaos4.10.gitf2f339d.el8-dev
node1.example.com   Ready    worker   72m    v1.26.0   10.0.129.222   &lt;none&gt;        Red Hat Enterprise Linux CoreOS 48.83.202103210901-0 (Ootpa)   4.18.0-240.15.1.el8_3.x86_64   cri-o://1.26.0-30.rhaos4.10.gitf2f339d.el8-dev
node2.example.com   Ready    worker   164m   v1.26.0   10.0.142.150   &lt;none&gt;        Red Hat Enterprise Linux CoreOS 48.83.202103210901-0 (Ootpa)   4.18.0-240.15.1.el8_3.x86_64   cri-o://1.26.0-30.rhaos4.10.gitf2f339d.el8-dev</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							The following command lists information about a single node:
						</p><pre class="programlisting language-terminal">$ oc get node &lt;node&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc get node node1.example.com</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                   STATUS    ROLES     AGE       VERSION
node1.example.com      Ready     worker    7h        v1.26.0</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							The following command provides more detailed information about a specific node, including the reason for the current condition:
						</p><pre class="programlisting language-terminal">$ oc describe node &lt;node&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc describe node node1.example.com</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">Name:               node1.example.com <span id="CO111-1"><!--Empty--></span><span class="callout">1</span>
Roles:              worker <span id="CO111-2"><!--Empty--></span><span class="callout">2</span>
Labels:             kubernetes.io/os=linux
                    kubernetes.io/hostname=ip-10-0-131-14
                    kubernetes.io/arch=amd64 <span id="CO111-3"><!--Empty--></span><span class="callout">3</span>
                    node-role.kubernetes.io/worker=
                    node.kubernetes.io/instance-type=m4.large
                    node.openshift.io/os_id=rhcos
                    node.openshift.io/os_version=4.5
                    region=east
                    topology.kubernetes.io/region=us-east-1
                    topology.kubernetes.io/zone=us-east-1a
Annotations:        cluster.k8s.io/machine: openshift-machine-api/ahardin-worker-us-east-2a-q5dzc  <span id="CO111-4"><!--Empty--></span><span class="callout">4</span>
                    machineconfiguration.openshift.io/currentConfig: worker-309c228e8b3a92e2235edd544c62fea8
                    machineconfiguration.openshift.io/desiredConfig: worker-309c228e8b3a92e2235edd544c62fea8
                    machineconfiguration.openshift.io/state: Done
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 13 Feb 2019 11:05:57 -0500
Taints:             &lt;none&gt;  <span id="CO111-5"><!--Empty--></span><span class="callout">5</span>
Unschedulable:      false
Conditions:                 <span id="CO111-6"><!--Empty--></span><span class="callout">6</span>
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  OutOfDisk        False   Wed, 13 Feb 2019 15:09:42 -0500   Wed, 13 Feb 2019 11:05:57 -0500   KubeletHasSufficientDisk     kubelet has sufficient disk space available
  MemoryPressure   False   Wed, 13 Feb 2019 15:09:42 -0500   Wed, 13 Feb 2019 11:05:57 -0500   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 13 Feb 2019 15:09:42 -0500   Wed, 13 Feb 2019 11:05:57 -0500   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 13 Feb 2019 15:09:42 -0500   Wed, 13 Feb 2019 11:05:57 -0500   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 13 Feb 2019 15:09:42 -0500   Wed, 13 Feb 2019 11:07:09 -0500   KubeletReady                 kubelet is posting ready status
Addresses:   <span id="CO111-7"><!--Empty--></span><span class="callout">7</span>
  InternalIP:   10.0.140.16
  InternalDNS:  ip-10-0-140-16.us-east-2.compute.internal
  Hostname:     ip-10-0-140-16.us-east-2.compute.internal
Capacity:    <span id="CO111-8"><!--Empty--></span><span class="callout">8</span>
 attachable-volumes-aws-ebs:  39
 cpu:                         2
 hugepages-1Gi:               0
 hugepages-2Mi:               0
 memory:                      8172516Ki
 pods:                        250
Allocatable:
 attachable-volumes-aws-ebs:  39
 cpu:                         1500m
 hugepages-1Gi:               0
 hugepages-2Mi:               0
 memory:                      7558116Ki
 pods:                        250
System Info:    <span id="CO111-9"><!--Empty--></span><span class="callout">9</span>
 Machine ID:                              63787c9534c24fde9a0cde35c13f1f66
 System UUID:                             EC22BF97-A006-4A58-6AF8-0A38DEEA122A
 Boot ID:                                 f24ad37d-2594-46b4-8830-7f7555918325
 Kernel Version:                          3.10.0-957.5.1.el7.x86_64
 OS Image:                                Red Hat Enterprise Linux CoreOS 410.8.20190520.0 (Ootpa)
 Operating System:                        linux
 Architecture:                            amd64
 Container Runtime Version:               cri-o://1.26.0-0.6.dev.rhaos4.3.git9ad059b.el8-rc2
 Kubelet Version:                         v1.26.0
 Kube-Proxy Version:                      v1.26.0
PodCIDR:                                  10.128.4.0/24
ProviderID:                               aws:///us-east-2a/i-04e87b31dc6b3e171
Non-terminated Pods:                      (12 in total)  <span id="CO111-10"><!--Empty--></span><span class="callout">10</span>
  Namespace                               Name                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------                               ----                                   ------------  ----------  ---------------  -------------
  openshift-cluster-node-tuning-operator  tuned-hdl5q                            0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-dns                           dns-default-l69zr                      0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-image-registry                node-ca-9hmcg                          0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-ingress                       router-default-76455c45c-c5ptv         0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-machine-config-operator       machine-config-daemon-cvqw9            20m (1%)      0 (0%)      50Mi (0%)        0 (0%)
  openshift-marketplace                   community-operators-f67fh              0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-monitoring                    alertmanager-main-0                    50m (3%)      50m (3%)    210Mi (2%)       10Mi (0%)
  openshift-monitoring                    node-exporter-l7q8d                    10m (0%)      20m (1%)    20Mi (0%)        40Mi (0%)
  openshift-monitoring                    prometheus-adapter-75d769c874-hvb85    0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-multus                        multus-kw8w5                           0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-sdn                           ovs-t4dsn                              100m (6%)     0 (0%)      300Mi (4%)       0 (0%)
  openshift-sdn                           sdn-g79hg                              100m (6%)     0 (0%)      200Mi (2%)       0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource                    Requests     Limits
  --------                    --------     ------
  cpu                         380m (25%)   270m (18%)
  memory                      880Mi (11%)  250Mi (3%)
  attachable-volumes-aws-ebs  0            0
Events:     <span id="CO111-11"><!--Empty--></span><span class="callout">11</span>
  Type     Reason                   Age                From                      Message
  ----     ------                   ----               ----                      -------
  Normal   NodeHasSufficientPID     6d (x5 over 6d)    kubelet, m01.example.com  Node m01.example.com status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  6d                 kubelet, m01.example.com  Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  6d (x6 over 6d)    kubelet, m01.example.com  Node m01.example.com status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    6d (x6 over 6d)    kubelet, m01.example.com  Node m01.example.com status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientDisk    6d (x6 over 6d)    kubelet, m01.example.com  Node m01.example.com status is now: NodeHasSufficientDisk
  Normal   NodeHasSufficientPID     6d                 kubelet, m01.example.com  Node m01.example.com status is now: NodeHasSufficientPID
  Normal   Starting                 6d                 kubelet, m01.example.com  Starting kubelet.
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO111-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the node.
								</div></dd><dt><a href="#CO111-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The role of the node, either <code class="literal">master</code> or <code class="literal">worker</code>.
								</div></dd><dt><a href="#CO111-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The labels applied to the node.
								</div></dd><dt><a href="#CO111-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The annotations applied to the node.
								</div></dd><dt><a href="#CO111-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									The taints applied to the node.
								</div></dd><dt><a href="#CO111-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									The node conditions and status. The <code class="literal">conditions</code> stanza lists the <code class="literal">Ready</code>, <code class="literal">PIDPressure</code>, <code class="literal">PIDPressure</code>, <code class="literal">MemoryPressure</code>, <code class="literal">DiskPressure</code> and <code class="literal">OutOfDisk</code> status. These condition are described later in this section.
								</div></dd><dt><a href="#CO111-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									The IP address and hostname of the node.
								</div></dd><dt><a href="#CO111-8"><span class="callout">8</span></a> </dt><dd><div class="para">
									The pod resources and allocatable resources.
								</div></dd><dt><a href="#CO111-9"><span class="callout">9</span></a> </dt><dd><div class="para">
									Information about the node host.
								</div></dd><dt><a href="#CO111-10"><span class="callout">10</span></a> </dt><dd><div class="para">
									The pods on the node.
								</div></dd><dt><a href="#CO111-11"><span class="callout">11</span></a> </dt><dd><div class="para">
									The events reported by the node.
								</div></dd></dl></div></li></ul></div><p>
					Among the information shown for nodes, the following node conditions appear in the output of the commands shown in this section:
				</p><div class="table" id="machine-health-checks-resource-conditions"><p class="title"><strong>Table 6.1. Node Conditions</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 27%; " class="col_1"><!--Empty--></col><col style="width: 73%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232283171312" scope="col">Condition</th><th align="left" valign="top" id="idm140232283170224" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232283171312"> <p>
									<code class="literal">Ready</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232283170224"> <p>
									If <code class="literal">true</code>, the node is healthy and ready to accept pods. If <code class="literal">false</code>, the node is not healthy and is not accepting pods. If <code class="literal">unknown</code>, the node controller has not received a heartbeat from the node for the <code class="literal">node-monitor-grace-period</code> (the default is 40 seconds).
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232283171312"> <p>
									<code class="literal">DiskPressure</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232283170224"> <p>
									If <code class="literal">true</code>, the disk capacity is low.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232283171312"> <p>
									<code class="literal">MemoryPressure</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232283170224"> <p>
									If <code class="literal">true</code>, the node memory is low.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232283171312"> <p>
									<code class="literal">PIDPressure</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232283170224"> <p>
									If <code class="literal">true</code>, there are too many processes on the node.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232283171312"> <p>
									<code class="literal">OutOfDisk</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232283170224"> <p>
									If <code class="literal">true</code>, the node has insufficient free space on the node for adding new pods.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232283171312"> <p>
									<code class="literal">NetworkUnavailable</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232283170224"> <p>
									If <code class="literal">true</code>, the network for the node is not correctly configured.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232283171312"> <p>
									<code class="literal">NotReady</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232283170224"> <p>
									If <code class="literal">true</code>, one of the underlying components, such as the container runtime or network, is experiencing issues or is not yet configured.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232283171312"> <p>
									<code class="literal">SchedulingDisabled</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232283170224"> <p>
									Pods cannot be scheduled for placement on the node.
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section" id="nodes-nodes-viewing-listing-pods_nodes-nodes-viewing"><div class="titlepage"><div><div><h3 class="title">6.1.2. Listing pods on a node in your cluster</h3></div></div></div><p>
					You can list all the pods on a specific node.
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To list all or selected pods on one or more nodes:
						</p><pre class="programlisting language-terminal">$ oc describe node &lt;node1&gt; &lt;node2&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc describe node ip-10-0-128-218.ec2.internal</pre></li><li class="listitem"><p class="simpara">
							To list all or selected pods on selected nodes:
						</p><pre class="programlisting language-terminal">$ oc describe --selector=&lt;node_selector&gt;</pre><pre class="programlisting language-terminal">$ oc describe node  --selector=kubernetes.io/os</pre><p class="simpara">
							Or:
						</p><pre class="programlisting language-terminal">$ oc describe -l=&lt;pod_selector&gt;</pre><pre class="programlisting language-terminal">$ oc describe node -l node-role.kubernetes.io/worker</pre></li><li class="listitem"><p class="simpara">
							To list all pods on a specific node, including terminated pods:
						</p><pre class="programlisting language-terminal">$ oc get pod --all-namespaces --field-selector=spec.nodeName=&lt;nodename&gt;</pre></li></ul></div></section><section class="section" id="nodes-nodes-viewing-memory_nodes-nodes-viewing"><div class="titlepage"><div><div><h3 class="title">6.1.3. Viewing memory and CPU usage statistics on your nodes</h3></div></div></div><p>
					You can display usage statistics about nodes, which provide the runtime environments for containers. These usage statistics include CPU, memory, and storage consumption.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You must have <code class="literal">cluster-reader</code> permission to view the usage statistics.
						</li><li class="listitem">
							Metrics must be installed to view the usage statistics.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To view the usage statistics:
						</p><pre class="programlisting language-terminal">$ oc adm top nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                   CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%
ip-10-0-12-143.ec2.compute.internal    1503m        100%      4533Mi          61%
ip-10-0-132-16.ec2.compute.internal    76m          5%        1391Mi          18%
ip-10-0-140-137.ec2.compute.internal   398m         26%       2473Mi          33%
ip-10-0-142-44.ec2.compute.internal    656m         43%       6119Mi          82%
ip-10-0-146-165.ec2.compute.internal   188m         12%       3367Mi          45%
ip-10-0-19-62.ec2.compute.internal     896m         59%       5754Mi          77%
ip-10-0-44-193.ec2.compute.internal    632m         42%       5349Mi          72%</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To view the usage statistics for nodes with labels:
						</p><pre class="programlisting language-terminal">$ oc adm top node --selector=''</pre><p class="simpara">
							You must choose the selector (label query) to filter on. Supports <code class="literal">=</code>, <code class="literal">==</code>, and <code class="literal">!=</code>.
						</p></li></ul></div></section></section><section class="section" id="nodes-nodes-working"><div class="titlepage"><div><div><h2 class="title">6.2. Working with nodes</h2></div></div></div><p>
				As an administrator, you can perform several tasks to make your clusters more efficient.
			</p><section class="section" id="nodes-nodes-working-evacuating_nodes-nodes-working"><div class="titlepage"><div><div><h3 class="title">6.2.1. Understanding how to evacuate pods on nodes</h3></div></div></div><p>
					Evacuating pods allows you to migrate all or selected pods from a given node or nodes.
				</p><p>
					You can only evacuate pods backed by a replication controller. The replication controller creates new pods on other nodes and removes the existing pods from the specified node(s).
				</p><p>
					Bare pods, meaning those not backed by a replication controller, are unaffected by default. You can evacuate a subset of pods by specifying a pod-selector. Pod selectors are based on labels, so all the pods with the specified label will be evacuated.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Mark the nodes unschedulable before performing the pod evacuation.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Mark the node as unschedulable:
								</p><pre class="programlisting language-terminal">$ oc adm cordon &lt;node1&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">node/&lt;node1&gt; cordoned</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Check that the node status is <code class="literal">Ready,SchedulingDisabled</code>:
								</p><pre class="programlisting language-terminal">$ oc get node &lt;node1&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME        STATUS                     ROLES     AGE       VERSION
&lt;node1&gt;     Ready,SchedulingDisabled   worker    1d        v1.26.0</pre>

									</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Evacuate the pods using one of the following methods:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Evacuate all or selected pods on one or more nodes:
								</p><pre class="programlisting language-terminal">$ oc adm drain &lt;node1&gt; &lt;node2&gt; [--pod-selector=&lt;pod_selector&gt;]</pre></li><li class="listitem"><p class="simpara">
									Force the deletion of bare pods using the <code class="literal">--force</code> option. When set to <code class="literal">true</code>, deletion continues even if there are pods not managed by a replication controller, replica set, job, daemon set, or stateful set:
								</p><pre class="programlisting language-terminal">$ oc adm drain &lt;node1&gt; &lt;node2&gt; --force=true</pre></li><li class="listitem"><p class="simpara">
									Set a period of time in seconds for each pod to terminate gracefully, use <code class="literal">--grace-period</code>. If negative, the default value specified in the pod will be used:
								</p><pre class="programlisting language-terminal">$ oc adm drain &lt;node1&gt; &lt;node2&gt; --grace-period=-1</pre></li><li class="listitem"><p class="simpara">
									Ignore pods managed by daemon sets using the <code class="literal">--ignore-daemonsets</code> flag set to <code class="literal">true</code>:
								</p><pre class="programlisting language-terminal">$ oc adm drain &lt;node1&gt; &lt;node2&gt; --ignore-daemonsets=true</pre></li><li class="listitem"><p class="simpara">
									Set the length of time to wait before giving up using the <code class="literal">--timeout</code> flag. A value of <code class="literal">0</code> sets an infinite length of time:
								</p><pre class="programlisting language-terminal">$ oc adm drain &lt;node1&gt; &lt;node2&gt; --timeout=5s</pre></li><li class="listitem"><p class="simpara">
									Delete pods even if there are pods using <code class="literal">emptyDir</code> volumes by setting the <code class="literal">--delete-emptydir-data</code> flag to <code class="literal">true</code>. Local data is deleted when the node is drained:
								</p><pre class="programlisting language-terminal">$ oc adm drain &lt;node1&gt; &lt;node2&gt; --delete-emptydir-data=true</pre></li><li class="listitem"><p class="simpara">
									List objects that will be migrated without actually performing the evacuation, using the <code class="literal">--dry-run</code> option set to <code class="literal">true</code>:
								</p><pre class="programlisting language-terminal">$ oc adm drain &lt;node1&gt; &lt;node2&gt;  --dry-run=true</pre><p class="simpara">
									Instead of specifying specific node names (for example, <code class="literal">&lt;node1&gt; &lt;node2&gt;</code>), you can use the <code class="literal">--selector=&lt;node_selector&gt;</code> option to evacuate pods on selected nodes.
								</p></li></ul></div></li><li class="listitem"><p class="simpara">
							Mark the node as schedulable when done.
						</p><pre class="programlisting language-terminal">$ oc adm uncordon &lt;node1&gt;</pre></li></ol></div></section><section class="section" id="nodes-nodes-working-updating_nodes-nodes-working"><div class="titlepage"><div><div><h3 class="title">6.2.2. Understanding how to update labels on nodes</h3></div></div></div><p>
					You can update any label on a node.
				</p><p>
					Node labels are not persisted after a node is deleted even if the node is backed up by a Machine.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Any change to a <code class="literal">MachineSet</code> object is not applied to existing machines owned by the compute machine set. For example, labels edited or added to an existing <code class="literal">MachineSet</code> object are not propagated to existing machines and nodes associated with the compute machine set.
					</p></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							The following command adds or updates labels on a node:
						</p><pre class="programlisting language-terminal">$ oc label node &lt;node&gt; &lt;key_1&gt;=&lt;value_1&gt; ... &lt;key_n&gt;=&lt;value_n&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc label nodes webconsole-7f7f6 unhealthy=true</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can alternatively apply the following YAML to apply the label:
						</p><pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  name: webconsole-7f7f6
  labels:
    unhealthy: 'true'
#...</pre></div></div></li><li class="listitem"><p class="simpara">
							The following command updates all pods in the namespace:
						</p><pre class="programlisting language-terminal">$ oc label pods --all &lt;key_1&gt;=&lt;value_1&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc label pods --all status=unhealthy</pre></li></ul></div></section><section class="section" id="nodes-nodes-working-marking_nodes-nodes-working"><div class="titlepage"><div><div><h3 class="title">6.2.3. Understanding how to mark nodes as unschedulable or schedulable</h3></div></div></div><p>
					By default, healthy nodes with a <code class="literal">Ready</code> status are marked as schedulable, which means that you can place new pods on the node. Manually marking a node as unschedulable blocks any new pods from being scheduled on the node. Existing pods on the node are not affected.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							The following command marks a node or nodes as unschedulable:
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">$ oc adm cordon &lt;node&gt;</pre>

							</p></div><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc adm cordon node1.example.com</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">node/node1.example.com cordoned

NAME                 LABELS                                        STATUS
node1.example.com    kubernetes.io/hostname=node1.example.com      Ready,SchedulingDisabled</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							The following command marks a currently unschedulable node or nodes as schedulable:
						</p><pre class="programlisting language-terminal">$ oc adm uncordon &lt;node1&gt;</pre><p class="simpara">
							Alternatively, instead of specifying specific node names (for example, <code class="literal">&lt;node&gt;</code>), you can use the <code class="literal">--selector=&lt;node_selector&gt;</code> option to mark selected nodes as schedulable or unschedulable.
						</p></li></ul></div></section><section class="section" id="sno-clusters-reboot-without-drain_nodes-nodes-working"><div class="titlepage"><div><div><h3 class="title">6.2.4. Handling errors in single-node OpenShift clusters when the node reboots without draining application pods</h3></div></div></div><p>
					In single-node OpenShift clusters and in OpenShift Container Platform clusters in general, a situation can arise where a node reboot occurs without first draining the node. This can occur where an application pod requesting devices fails with the <code class="literal">UnexpectedAdmissionError</code> error. <code class="literal">Deployment</code>, <code class="literal">ReplicaSet</code>, or <code class="literal">DaemonSet</code> errors are reported because the application pods that require those devices start before the pod serving those devices. You cannot control the order of pod restarts.
				</p><p>
					While this behavior is to be expected, it can cause a pod to remain on the cluster even though it has failed to deploy successfully. The pod continues to report <code class="literal">UnexpectedAdmissionError</code>. This issue is mitigated by the fact that application pods are typically included in a <code class="literal">Deployment</code>, <code class="literal">ReplicaSet</code>, or <code class="literal">DaemonSet</code>. If a pod is in this error state, it is of little concern because another instance should be running. Belonging to a <code class="literal">Deployment</code>, <code class="literal">ReplicaSet</code>, or <code class="literal">DaemonSet</code> guarantees the successful creation and execution of subsequent pods and ensures the successful deployment of the application.
				</p><p>
					There is ongoing work upstream to ensure that such pods are gracefully terminated. Until that work is resolved, run the following command for a single-node OpenShift cluster to remove the failed pods:
				</p><pre class="programlisting language-terminal">$ oc delete pods --field-selector status.phase=Failed -n <span class="emphasis"><em>&lt;POD_NAMESPACE&gt;</em></span></pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The option to drain the node is unavailable for single-node OpenShift clusters.
					</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-working-evacuating_nodes-nodes-working">Understanding how to evacuate pods on nodes</a>
						</li></ul></div></section><section class="section" id="deleting-nodes"><div class="titlepage"><div><div><h3 class="title">6.2.5. Deleting nodes</h3></div></div></div><section class="section" id="nodes-nodes-working-deleting_nodes-nodes-working"><div class="titlepage"><div><div><h4 class="title">6.2.5.1. Deleting nodes from a cluster</h4></div></div></div><p>
						When you delete a node using the CLI, the node object is deleted in Kubernetes, but the pods that exist on the node are not deleted. Any bare pods not backed by a replication controller become inaccessible to OpenShift Container Platform. Pods backed by replication controllers are rescheduled to other available nodes. You must delete local manifest pods.
					</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To delete a node from the OpenShift Container Platform cluster, edit the appropriate <code class="literal">MachineSet</code> object:
						</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you are running cluster on bare metal, you cannot delete a node by editing <code class="literal">MachineSet</code> objects. Compute machine sets are only available when a cluster is integrated with a cloud provider. Instead you must unschedule and drain the node before manually deleting it.
						</p></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								View the compute machine sets that are in the cluster:
							</p><pre class="programlisting language-terminal">$ oc get machinesets -n openshift-machine-api</pre><p class="simpara">
								The compute machine sets are listed in the form of &lt;clusterid&gt;-worker-&lt;aws-region-az&gt;.
							</p></li><li class="listitem"><p class="simpara">
								Scale the compute machine set:
							</p><pre class="programlisting language-terminal">$ oc scale --replicas=2 machineset &lt;machineset&gt; -n openshift-machine-api</pre><p class="simpara">
								Or:
							</p><pre class="programlisting language-terminal">$ oc edit machineset &lt;machineset&gt; -n openshift-machine-api</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
								You can alternatively apply the following YAML to scale the compute machine set:
							</p><pre class="programlisting language-yaml">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: &lt;machineset&gt;
  namespace: openshift-machine-api
spec:
  replicas: 2
#...</pre></div></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								For more information on scaling your cluster using a MachineSet, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machineset-manually-scaling-manually-scaling-machineset">Manually scaling a MachineSet</a>.
							</li></ul></div></section><section class="section" id="nodes-nodes-working-deleting-bare-metal_nodes-nodes-working"><div class="titlepage"><div><div><h4 class="title">6.2.5.2. Deleting nodes from a bare metal cluster</h4></div></div></div><p>
						When you delete a node using the CLI, the node object is deleted in Kubernetes, but the pods that exist on the node are not deleted. Any bare pods not backed by a replication controller become inaccessible to OpenShift Container Platform. Pods backed by replication controllers are rescheduled to other available nodes. You must delete local manifest pods.
					</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							Delete a node from an OpenShift Container Platform cluster running on bare metal by completing the following steps:
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Mark the node as unschedulable:
							</p><pre class="programlisting language-terminal">$ oc adm cordon &lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
								Drain all pods on the node:
							</p><pre class="programlisting language-terminal">$ oc adm drain &lt;node_name&gt; --force=true</pre><p class="simpara">
								This step might fail if the node is offline or unresponsive. Even if the node does not respond, it might still be running a workload that writes to shared storage. To avoid data corruption, power down the physical hardware before you proceed.
							</p></li><li class="listitem"><p class="simpara">
								Delete the node from the cluster:
							</p><pre class="programlisting language-terminal">$ oc delete node &lt;node_name&gt;</pre><p class="simpara">
								Although the node object is now deleted from the cluster, it can still rejoin the cluster after reboot or if the kubelet service is restarted. To permanently delete the node and all its data, you must <a class="link" href="https://access.redhat.com/solutions/84663">decommission the node</a>.
							</p></li><li class="listitem">
								If you powered down the physical hardware, turn it back on so that the node can rejoin the cluster.
							</li></ol></div></section></section></section><section class="section" id="nodes-nodes-managing"><div class="titlepage"><div><div><h2 class="title">6.3. Managing nodes</h2></div></div></div><p>
				OpenShift Container Platform uses a KubeletConfig custom resource (CR) to manage the configuration of nodes. By creating an instance of a <code class="literal">KubeletConfig</code> object, a managed machine config is created to override setting on the node.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					<span class="strong strong"><strong>Logging in to remote machines for the purpose of changing their configuration is not supported.</strong></span>
				</p></div></div><section class="section" id="nodes-nodes-managing-about_nodes-nodes-managing"><div class="titlepage"><div><div><h3 class="title">6.3.1. Modifying nodes</h3></div></div></div><p>
					To make configuration changes to a cluster, or machine pool, you must create a custom resource definition (CRD), or <code class="literal">kubeletConfig</code> object. OpenShift Container Platform uses the Machine Config Controller to watch for changes introduced through the CRD to apply the changes to the cluster.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Because the fields in a <code class="literal">kubeletConfig</code> object are passed directly to the kubelet from upstream Kubernetes, the validation of those fields is handled directly by the kubelet itself. Please refer to the relevant Kubernetes documentation for the valid values for these fields. Invalid values in the <code class="literal">kubeletConfig</code> object can render cluster nodes unusable.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the label associated with the static CRD, Machine Config Pool, for the type of node you want to configure. Perform one of the following steps:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Check current labels of the desired machine config pool.
								</p><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$  oc get machineconfigpool  --show-labels</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME      CONFIG                                             UPDATED   UPDATING   DEGRADED   LABELS
master    rendered-master-e05b81f5ca4db1d249a1bf32f9ec24fd   True      False      False      operator.machineconfiguration.openshift.io/required-for-upgrade=
worker    rendered-worker-f50e78e1bc06d8e82327763145bfcf62   True      False      False</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Add a custom label to the desired machine config pool.
								</p><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc label machineconfigpool worker custom-kubelet=enabled</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">kubeletconfig</code> custom resource (CR) for your configuration change.
						</p><p class="simpara">
							For example:
						</p><div class="formalpara"><p class="title"><strong>Sample configuration for a <span class="strong strong"><strong>custom-config</strong></span> CR</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: custom-config <span id="CO112-1"><!--Empty--></span><span class="callout">1</span>
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: enabled <span id="CO112-2"><!--Empty--></span><span class="callout">2</span>
  kubeletConfig: <span id="CO112-3"><!--Empty--></span><span class="callout">3</span>
    podsPerCore: 10
    maxPods: 250
    systemReserved:
      cpu: 2000m
      memory: 1Gi
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO112-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Assign a name to CR.
								</div></dd><dt><a href="#CO112-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the label to apply the configuration change, this is the label you added to the machine config pool.
								</div></dd><dt><a href="#CO112-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify the new value(s) you want to change.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the CR object.
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc create -f master-kube-config.yaml</pre></li></ol></div><p>
					Most <a class="link" href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/">Kubelet Configuration options</a> can be set by the user. The following options are not allowed to be overwritten:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							CgroupDriver
						</li><li class="listitem">
							ClusterDNS
						</li><li class="listitem">
							ClusterDomain
						</li><li class="listitem">
							StaticPodPath
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If a single node contains more than 50 images, pod scheduling might be imbalanced across nodes. This is because the list of images on a node is shortened to 50 by default. You can disable the image limit by editing the <code class="literal">KubeletConfig</code> object and setting the value of <code class="literal">nodeStatusMaxImages</code> to <code class="literal">-1</code>.
					</p></div></div></section><section class="section" id="nodes-nodes-working-master-schedulable_nodes-nodes-managing"><div class="titlepage"><div><div><h3 class="title">6.3.2. Configuring control plane nodes as schedulable</h3></div></div></div><p>
					You can configure control plane nodes to be schedulable, meaning that new pods are allowed for placement on the master nodes. By default, control plane nodes are not schedulable.
				</p><p>
					You can set the masters to be schedulable, but must retain the worker nodes.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can deploy OpenShift Container Platform with no worker nodes on a bare metal cluster. In this case, the control plane nodes are marked schedulable by default.
					</p></div></div><p>
					You can allow or disallow control plane nodes to be schedulable by configuring the <code class="literal">mastersSchedulable</code> field.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						When you configure control plane nodes from the default unschedulable to schedulable, additional subscriptions are required. This is because control plane nodes then become worker nodes.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">schedulers.config.openshift.io</code> resource.
						</p><pre class="programlisting language-terminal">$ oc edit schedulers.config.openshift.io cluster</pre></li><li class="listitem"><p class="simpara">
							Configure the <code class="literal">mastersSchedulable</code> field.
						</p><pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  creationTimestamp: "2019-09-10T03:04:05Z"
  generation: 1
  name: cluster
  resourceVersion: "433"
  selfLink: /apis/config.openshift.io/v1/schedulers/cluster
  uid: a636d30a-d377-11e9-88d4-0a60097bee62
spec:
  mastersSchedulable: false <span id="CO113-1"><!--Empty--></span><span class="callout">1</span>
status: {}
#...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO113-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Set to <code class="literal">true</code> to allow control plane nodes to be schedulable, or <code class="literal">false</code> to disallow control plane nodes to be schedulable.
								</div></dd></dl></div></li><li class="listitem">
							Save the file to apply the changes.
						</li></ol></div></section><section class="section" id="nodes-nodes-working-setting-booleans_nodes-nodes-managing"><div class="titlepage"><div><div><h3 class="title">6.3.3. Setting SELinux booleans</h3></div></div></div><p>
					OpenShift Container Platform allows you to enable and disable an SELinux boolean on a Red Hat Enterprise Linux CoreOS (RHCOS) node. The following procedure explains how to modify SELinux booleans on nodes using the Machine Config Operator (MCO). This procedure uses <code class="literal">container_manage_cgroup</code> as the example boolean. You can modify this value to whichever boolean you need.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (oc).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a new YAML file with a <code class="literal">MachineConfig</code> object, displayed in the following example:
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 99-worker-setsebool
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
      - contents: |
          [Unit]
          Description=Set SELinux booleans
          Before=kubelet.service

          [Service]
          Type=oneshot
          ExecStart=/sbin/setsebool container_manage_cgroup=on
          RemainAfterExit=true

          [Install]
          WantedBy=multi-user.target graphical.target
        enabled: true
        name: setsebool.service
#...</pre></li><li class="listitem"><p class="simpara">
							Create the new <code class="literal">MachineConfig</code> object by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f 99-worker-setsebool.yaml</pre></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Applying any changes to the <code class="literal">MachineConfig</code> object causes all affected nodes to gracefully reboot after the change is applied.
					</p></div></div></section><section class="section" id="nodes-nodes-kernel-arguments_nodes-nodes-managing"><div class="titlepage"><div><div><h3 class="title">6.3.4. Adding kernel arguments to nodes</h3></div></div></div><p>
					In some special cases, you might want to add kernel arguments to a set of nodes in your cluster. This should only be done with caution and clear understanding of the implications of the arguments you set.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Improper use of kernel arguments can result in your systems becoming unbootable.
					</p></div></div><p>
					Examples of kernel arguments you could set include:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<span class="strong strong"><strong>enforcing=0</strong></span>: Configures Security Enhanced Linux (SELinux) to run in permissive mode. In permissive mode, the system acts as if SELinux is enforcing the loaded security policy, including labeling objects and emitting access denial entries in the logs, but it does not actually deny any operations. While not supported for production systems, permissive mode can be helpful for debugging.
						</li><li class="listitem">
							<span class="strong strong"><strong>nosmt</strong></span>: Disables symmetric multithreading (SMT) in the kernel. Multithreading allows multiple logical threads for each CPU. You could consider <code class="literal">nosmt</code> in multi-tenant environments to reduce risks from potential cross-thread attacks. By disabling SMT, you essentially choose security over performance.
						</li><li class="listitem">
							<span class="strong strong"><strong>systemd.unified_cgroup_hierarchy</strong></span>: Enables <a class="link" href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html">Linux control group version 2</a> (cgroup v2). cgroup v2 is the next version of the kernel <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/ch01">control group</a> and offers multiple improvements.
						</li></ul></div><p>
					See <a class="link" href="https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt">Kernel.org kernel parameters</a> for a list and descriptions of kernel arguments.
				</p><p>
					In the following procedure, you create a <code class="literal">MachineConfig</code> object that identifies:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							A set of machines to which you want to add the kernel argument. In this case, machines with a worker role.
						</li><li class="listitem">
							Kernel arguments that are appended to the end of the existing kernel arguments.
						</li><li class="listitem">
							A label that indicates where in the list of machine configs the change is applied.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Have administrative privilege to a working OpenShift Container Platform cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							List existing <code class="literal">MachineConfig</code> objects for your OpenShift Container Platform cluster to determine how to label your machine config:
						</p><pre class="programlisting language-terminal">$ oc get MachineConfig</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
00-master                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
00-worker                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-ssh                                                                                 3.2.0             40m
99-worker-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-worker-ssh                                                                                 3.2.0             40m
rendered-master-23e785de7587df95a4b517e0647e5ab7   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
rendered-worker-5d596d9293ca3ea80c896a1191735bb1   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">MachineConfig</code> object file that identifies the kernel argument (for example, <code class="literal">05-worker-kernelarg-selinuxpermissive.yaml</code>)
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker<span id="CO114-1"><!--Empty--></span><span class="callout">1</span>
  name: 05-worker-kernelarg-selinuxpermissive<span id="CO114-2"><!--Empty--></span><span class="callout">2</span>
spec:
  kernelArguments:
    - enforcing=0<span id="CO114-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO114-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Applies the new kernel argument only to worker nodes.
								</div></dd><dt><a href="#CO114-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Named to identify where it fits among the machine configs (05) and what it does (adds a kernel argument to configure SELinux permissive mode).
								</div></dd><dt><a href="#CO114-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Identifies the exact kernel argument as <code class="literal">enforcing=0</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the new machine config:
						</p><pre class="programlisting language-terminal">$ oc create -f 05-worker-kernelarg-selinuxpermissive.yaml</pre></li><li class="listitem"><p class="simpara">
							Check the machine configs to see that the new one was added:
						</p><pre class="programlisting language-terminal">$ oc get MachineConfig</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
00-master                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
00-worker                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
05-worker-kernelarg-selinuxpermissive                                                         3.2.0             105s
99-master-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-ssh                                                                                 3.2.0             40m
99-worker-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-worker-ssh                                                                                 3.2.0             40m
rendered-master-23e785de7587df95a4b517e0647e5ab7   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
rendered-worker-5d596d9293ca3ea80c896a1191735bb1   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check the nodes:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                           STATUS                     ROLES    AGE   VERSION
ip-10-0-136-161.ec2.internal   Ready                      worker   28m   v1.26.0
ip-10-0-136-243.ec2.internal   Ready                      master   34m   v1.26.0
ip-10-0-141-105.ec2.internal   Ready,SchedulingDisabled   worker   28m   v1.26.0
ip-10-0-142-249.ec2.internal   Ready                      master   34m   v1.26.0
ip-10-0-153-11.ec2.internal    Ready                      worker   28m   v1.26.0
ip-10-0-153-150.ec2.internal   Ready                      master   34m   v1.26.0</pre>

							</p></div><p class="simpara">
							You can see that scheduling on each worker node is disabled as the change is being applied.
						</p></li><li class="listitem"><p class="simpara">
							Check that the kernel argument worked by going to one of the worker nodes and listing the kernel command line arguments (in <code class="literal">/proc/cmdline</code> on the host):
						</p><pre class="programlisting language-terminal">$ oc debug node/ip-10-0-141-105.ec2.internal</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Starting pod/ip-10-0-141-105ec2internal-debug ...
To use host binaries, run `chroot /host`

sh-4.2# cat /host/proc/cmdline
BOOT_IMAGE=/ostree/rhcos-... console=tty0 console=ttyS0,115200n8
rootflags=defaults,prjquota rw root=UUID=fd0... ostree=/ostree/boot.0/rhcos/16...
coreos.oem.id=qemu coreos.oem.id=ec2 ignition.platform.id=ec2 enforcing=0

sh-4.2# exit</pre>

							</p></div><p class="simpara">
							You should see the <code class="literal">enforcing=0</code> argument added to the other kernel arguments.
						</p></li></ol></div></section><section class="section" id="nodes-nodes-swap-memory_nodes-nodes-managing"><div class="titlepage"><div><div><h3 class="title">6.3.5. Enabling swap memory use on nodes</h3></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Enabling swap memory use on nodes is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
					</p><p>
						For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
					</p></div></div><p>
					You can enable swap memory use for OpenShift Container Platform workloads on a per-node basis.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Enabling swap memory can negatively impact workload performance and out-of-resource handling. Do not enable swap memory on control plane nodes.
					</p></div></div><p>
					To enable swap memory, create a <code class="literal">kubeletconfig</code> custom resource (CR) to set the <code class="literal">swapbehavior</code> parameter. You can set limited or unlimited swap memory:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Limited: Use the <code class="literal">LimitedSwap</code> value to limit how much swap memory workloads can use. Any workloads on the node that are not managed by OpenShift Container Platform can still use swap memory. The <code class="literal">LimitedSwap</code> behavior depends on whether the node is running with Linux control groups <a class="link" href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v1/index.html">version 1 (cgroups v1)</a> or <a class="link" href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html">version 2 (cgroup v2)</a>:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									cgroup v1: OpenShift Container Platform workloads can use any combination of memory and swap, up to the pod’s memory limit, if set.
								</li><li class="listitem">
									cgroup v2: OpenShift Container Platform workloads cannot use swap memory.
								</li></ul></div></li><li class="listitem">
							Unlimited: Use the <code class="literal">UnlimitedSwap</code> value to allow workloads to use as much swap memory as they request, up to the system limit.
						</li></ul></div><p>
					Because the kubelet will not start in the presence of swap memory without this configuration, you must enable swap memory in OpenShift Container Platform before enabling swap memory on the nodes. If there is no swap memory present on a node, enabling swap memory in OpenShift Container Platform has no effect.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have a running OpenShift Container Platform cluster that uses version 4.10 or later.
						</li><li class="listitem">
							You are logged in to the cluster as a user with administrative privileges.
						</li><li class="listitem"><p class="simpara">
							You have enabled the <code class="literal">TechPreviewNoUpgrade</code> feature set on the cluster (see <span class="emphasis"><em>Nodes → Working with clusters → Enabling features using feature gates</em></span>).
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Enabling the <code class="literal">TechPreviewNoUpgrade</code> feature set cannot be undone and prevents minor version updates. These feature sets are not recommended on production clusters.
							</p></div></div></li><li class="listitem">
							If cgroup v2 is enabled on a node, you must enable swap accounting on the node, by setting the <code class="literal">swapaccount=1</code> kernel argument.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Apply a custom label to the machine config pool where you want to allow swap memory.
						</p><pre class="programlisting language-terminal">$ oc label machineconfigpool worker kubelet-swap=enabled</pre></li><li class="listitem"><p class="simpara">
							Create a custom resource (CR) to enable and configure swap settings.
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: swap-config
spec:
  machineConfigPoolSelector:
    matchLabels:
      kubelet-swap: enabled
  kubeletConfig:
    failSwapOn: false <span id="CO115-1"><!--Empty--></span><span class="callout">1</span>
    memorySwap:
      swapBehavior: LimitedSwap <span id="CO115-2"><!--Empty--></span><span class="callout">2</span>
#...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO115-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Set to <code class="literal">false</code> to enable swap memory use on the associated nodes. Set to <code class="literal">true</code> to disable swap memory use.
								</div></dd><dt><a href="#CO115-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the swap memory behavior. If unspecified, the default is <code class="literal">LimitedSwap</code>.
								</div></dd></dl></div></li><li class="listitem">
							Enable swap memory on the machines.
						</li></ol></div></section><section class="section" id="nodes-control-plane-osp-migrating_nodes-nodes-managing"><div class="titlepage"><div><div><h3 class="title">6.3.6. Migrating control plane nodes from one RHOSP host to another</h3></div></div></div><p>
					You can run a script that moves a control plane node from one Red Hat OpenStack Platform (RHOSP) node to another.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The environment variable <code class="literal">OS_CLOUD</code> refers to a <code class="literal">clouds</code> entry that has administrative credentials in a <code class="literal">clouds.yaml</code> file.
						</li><li class="listitem">
							The environment variable <code class="literal">KUBECONFIG</code> refers to a configuration that contains administrative OpenShift Container Platform credentials.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							From a command line, run the following script:
						</li></ul></div><pre class="programlisting language-bash">#!/usr/bin/env bash

set -Eeuo pipefail

if [ $# -lt 1 ]; then
	echo "Usage: '$0 node_name'"
	exit 64
fi

# Check for admin OpenStack credentials
openstack server list --all-projects &gt;/dev/null || { &gt;&amp;2 echo "The script needs OpenStack admin credentials. Exiting"; exit 77; }

# Check for admin OpenShift credentials
oc adm top node &gt;/dev/null || { &gt;&amp;2 echo "The script needs OpenShift admin credentials. Exiting"; exit 77; }

set -x

declare -r node_name="$1"
declare server_id
server_id="$(openstack server list --all-projects -f value -c ID -c Name | grep "$node_name" | cut -d' ' -f1)"
readonly server_id

# Drain the node
oc adm cordon "$node_name"
oc adm drain "$node_name" --delete-emptydir-data --ignore-daemonsets --force

# Power off the server
oc debug "node/${node_name}" -- chroot /host shutdown -h 1

# Verify the server is shut off
until openstack server show "$server_id" -f value -c status | grep -q 'SHUTOFF'; do sleep 5; done

# Migrate the node
openstack server migrate --wait "$server_id"

# Resize the VM
openstack server resize confirm "$server_id"

# Wait for the resize confirm to finish
until openstack server show "$server_id" -f value -c status | grep -q 'SHUTOFF'; do sleep 5; done

# Restart the VM
openstack server start "$server_id"

# Wait for the node to show up as Ready:
until oc get node "$node_name" | grep -q "^${node_name}[[:space:]]\+Ready"; do sleep 5; done

# Uncordon the node
oc adm uncordon "$node_name"

# Wait for cluster operators to stabilize
until oc get co -o go-template='statuses: {{ range .items }}{{ range .status.conditions }}{{ if eq .type "Degraded" }}{{ if ne .status "False" }}DEGRADED{{ end }}{{ else if eq .type "Progressing"}}{{ if ne .status "False" }}PROGRESSING{{ end }}{{ else if eq .type "Available"}}{{ if ne .status "True" }}NOTAVAILABLE{{ end }}{{ end }}{{ end }}{{ end }}' | grep -qv '\(DEGRADED\|PROGRESSING\|NOTAVAILABLE\)'; do sleep 5; done</pre><p>
					If the script completes, the control plane machine is migrated to a new RHOSP node.
				</p></section></section><section class="section" id="nodes-nodes-graceful-shutdown"><div class="titlepage"><div><div><h2 class="title">6.4. Managing graceful node shutdown</h2></div></div></div><p>
				Graceful node shutdown enables the kubelet to delay forcible eviction of pods during a node shutdown. When you configure a graceful node shutdown, you can define a time period for pods to complete running workloads before shutting down. This grace period minimizes interruption to critical workloads during unexpected node shutdown events. Using priority classes, you can also specify the order of pod shutdown.
			</p><section class="section" id="nodes-nodes-cluster-timeout-graceful-shutdown_nodes-nodes-graceful-shutdown"><div class="titlepage"><div><div><h3 class="title">6.4.1. About graceful node shutdown</h3></div></div></div><p>
					During a graceful node shutdown, the kubelet sends a termination signal to pods running on the node and postpones the node shutdown until all the pods evicted. If a node unexpectedly shuts down, the graceful node shutdown feature minimizes interruption to workloads running on these pods.
				</p><p>
					During a graceful node shutdown, the kubelet stops pods in two phases:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Regular pod termination
						</li><li class="listitem">
							Critical pod termination
						</li></ul></div><p>
					You can define shutdown grace periods for regular and critical pods by configuring the following specifications in the <code class="literal">KubeletConfig</code> custom resource:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">shutdownGracePeriod</code>: Specifies the total duration for pod termination for regular and critical pods.
						</li><li class="listitem">
							<code class="literal">shutdownGracePeriodCriticalPods</code>: Specifies the duration for critical pod termination. This value must be less than the <code class="literal">shutdownGracePeriod</code> value.
						</li></ul></div><p>
					For example, if the <code class="literal">shutdownGracePeriod</code> value is <code class="literal">30s</code>, and the <code class="literal">shutdownGracePeriodCriticalPods</code> value is <code class="literal">10s</code>, the kubelet delays the node shutdown by 30 seconds. During the shutdown, the first 20 (30-10) seconds are reserved for gracefully shutting down regular pods, and the last 10 seconds are reserved for gracefully shutting down critical pods.
				</p><p>
					To define a critical pod, assign a pod priority value greater than or equal to <code class="literal">2000000000</code>. To define a regular pod, assign a pod priority value of less than <code class="literal">2000000000</code>.
				</p><p>
					For more information about how to define a priority value for pods, see the <span class="emphasis"><em>Additional resources</em></span> section.
				</p></section><section class="section" id="nodes-nodes-configuring-graceful-shutdown_nodes-nodes-graceful-shutdown"><div class="titlepage"><div><div><h3 class="title">6.4.2. Configuring graceful node shutdown</h3></div></div></div><p>
					To configure graceful node shutdown, create a <code class="literal">KubeletConfig</code> custom resource (CR) to specify a shutdown grace period for pods on a set of nodes. The graceful node shutdown feature minimizes interruption to workloads that run on these pods.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you do not configure graceful node shutdown, the default grace period is <code class="literal">0</code> and the pod is forcefully evicted from the node.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							You have defined priority classes for pods that require critical or regular classification.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Define shutdown grace periods in the <code class="literal">KubeletConfig</code> CR by saving the following YAML in the <code class="literal">kubelet-gns.yaml</code> file:
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: graceful-shutdown
  namespace: openshift-machine-config-operator
spec:
  machineConfigPoolSelector:
    matchLabels:
      pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO116-1"><!--Empty--></span><span class="callout">1</span>
  kubeletConfig:
    shutdownGracePeriod: "3m" <span id="CO116-2"><!--Empty--></span><span class="callout">2</span>
    shutdownGracePeriodCriticalPods: "2m" <span id="CO116-3"><!--Empty--></span><span class="callout">3</span>
#...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO116-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									This example applies shutdown grace periods to nodes with the <code class="literal">worker</code> role.
								</div></dd><dt><a href="#CO116-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Define a time period for regular pods to shut down.
								</div></dd><dt><a href="#CO116-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Define a time period for critical pods to shut down.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">KubeletConfig</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f kubelet-gns.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">kubeletconfig.machineconfiguration.openshift.io/graceful-shutdown created</pre>

							</p></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the kubelet logs for a node to verify the grace period configuration by using the command line or by viewing the <code class="literal">kubelet.conf</code> file.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Ensure that the log messages for <code class="literal">shutdownGracePeriodRequested</code> and <code class="literal">shutdownGracePeriodCriticalPods</code> match the values set in the <code class="literal">KubeletConfig</code> CR.
							</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									To view the logs by using the command line, run the following command, replacing <code class="literal">&lt;node_name&gt;</code> with the name of the node:
								</p><pre class="programlisting language-bash">$ oc adm node-logs &lt;node_name&gt; -u kubelet</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">Sep 12 22:13:46
ci-ln-qv5pvzk-72292-xvkd9-worker-a-dmbr4
hyperkube[22317]: I0912 22:13:46.687472
22317 nodeshutdown_manager_linux.go:134]
"Creating node shutdown manager"
shutdownGracePeriodRequested="3m0s" <span id="CO117-1"><!--Empty--></span><span class="callout">1</span>
shutdownGracePeriodCriticalPods="2m0s"
shutdownGracePeriodByPodPriority=[
{Priority:0
ShutdownGracePeriodSeconds:1200}
{Priority:2000000000
ShutdownGracePeriodSeconds:600}]
...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO117-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Ensure that the log messages for <code class="literal">shutdownGracePeriodRequested</code> and <code class="literal">shutdownGracePeriodCriticalPods</code> match the values set in the <code class="literal">KubeletConfig</code> CR.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To view the logs in the <code class="literal">kubelet.conf</code> file on a node, run the following commands to enter a debug session on the node:
								</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt;</pre><pre class="programlisting language-terminal">$ chroot /host</pre><pre class="programlisting language-terminal">$ cat /etc/kubernetes/kubelet.conf</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">#...
“memorySwap”: {},
 “containerLogMaxSize”: “50Mi”,
 “logging”: {
  “flushFrequency”: 0,
  “verbosity”: 0,
  “options”: {
   “json”: {
    “infoBufferSize”: “0”
   }
  }
 },
 “shutdownGracePeriod”: “10m0s”, <span id="CO118-1"><!--Empty--></span><span class="callout">1</span>
 “shutdownGracePeriodCriticalPods”: “3m0s”
}
#...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO118-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Ensure that the log messages for <code class="literal">shutdownGracePeriodRequested</code> and <code class="literal">shutdownGracePeriodCriticalPods</code> match the values set in the <code class="literal">KubeletConfig</code> CR.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							During a graceful node shutdown, you can verify that a pod was gracefully shut down by running the following command, replacing <code class="literal">&lt;pod_name&gt;</code> with the name of the pod:
						</p><pre class="programlisting language-terminal">$ oc describe pod &lt;pod_name&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Reason:         Terminated
Message:        Pod was terminated in response to imminent node shutdown.</pre>

							</p></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-priority">Understanding pod priority</a>
						</li></ul></div></section></section><section class="section" id="nodes-nodes-managing-max-pods"><div class="titlepage"><div><div><h2 class="title">6.5. Managing the maximum number of pods per node</h2></div></div></div><p>
				In OpenShift Container Platform, you can configure the number of pods that can run on a node based on the number of processor cores on the node, a hard limit or both. If you use both options, the lower of the two limits the number of pods on a node.
			</p><p>
				Exceeding these values can result in:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Increased CPU utilization by OpenShift Container Platform.
					</li><li class="listitem">
						Slow pod scheduling.
					</li><li class="listitem">
						Potential out-of-memory scenarios, depending on the amount of memory in the node.
					</li><li class="listitem">
						Exhausting the IP address pool.
					</li><li class="listitem">
						Resource overcommitting, leading to poor user application performance.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					A pod that is holding a single container actually uses two containers. The second container sets up networking prior to the actual container starting. As a result, a node running 10 pods actually has 20 containers running.
				</p></div></div><p>
				The <code class="literal">podsPerCore</code> parameter limits the number of pods the node can run based on the number of processor cores on the node. For example, if <code class="literal">podsPerCore</code> is set to <code class="literal">10</code> on a node with 4 processor cores, the maximum number of pods allowed on the node is 40.
			</p><p>
				The <code class="literal">maxPods</code> parameter limits the number of pods the node can run to a fixed value, regardless of the properties of the node.
			</p><section class="section" id="nodes-nodes-managing-max-pods-proc_nodes-nodes-managing-max-pods"><div class="titlepage"><div><div><h3 class="title">6.5.1. Configuring the maximum number of pods per node</h3></div></div></div><p>
					Two parameters control the maximum number of pods that can be scheduled to a node: <code class="literal">podsPerCore</code> and <code class="literal">maxPods</code>. If you use both options, the lower of the two limits the number of pods on a node.
				</p><p>
					For example, if <code class="literal">podsPerCore</code> is set to <code class="literal">10</code> on a node with 4 processor cores, the maximum number of pods allowed on the node will be 40.
				</p><div class="orderedlist"><p class="title"><strong>Prerequisites</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the label associated with the static <code class="literal">MachineConfigPool</code> CRD for the type of node you want to configure by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc edit machineconfigpool &lt;name&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc edit machineconfigpool worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: "2022-11-16T15:34:25Z"
  generation: 4
  labels:
    pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO119-1"><!--Empty--></span><span class="callout">1</span>
  name: worker
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO119-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The label appears under Labels.
								</div></dd></dl></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							If the label is not present, add a key/value pair such as:
						</p><pre class="screen">$ oc label machineconfigpool worker custom-kubelet=small-pods</pre></div></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a custom resource (CR) for your configuration change.
						</p><div class="formalpara"><p class="title"><strong>Sample configuration for a <code class="literal">max-pods</code> CR</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-max-pods <span id="CO120-1"><!--Empty--></span><span class="callout">1</span>
spec:
  machineConfigPoolSelector:
    matchLabels:
      pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO120-2"><!--Empty--></span><span class="callout">2</span>
  kubeletConfig:
    podsPerCore: 10 <span id="CO120-3"><!--Empty--></span><span class="callout">3</span>
    maxPods: 250 <span id="CO120-4"><!--Empty--></span><span class="callout">4</span>
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO120-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Assign a name to CR.
								</div></dd><dt><a href="#CO120-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the label from the machine config pool.
								</div></dd><dt><a href="#CO120-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify the number of pods the node can run based on the number of processor cores on the node.
								</div></dd><dt><a href="#CO120-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specify the number of pods the node can run to a fixed value, regardless of the properties of the node.
								</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Setting <code class="literal">podsPerCore</code> to <code class="literal">0</code> disables this limit.
							</p></div></div><p class="simpara">
							In the above example, the default value for <code class="literal">podsPerCore</code> is <code class="literal">10</code> and the default value for <code class="literal">maxPods</code> is <code class="literal">250</code>. This means that unless the node has 25 cores or more, by default, <code class="literal">podsPerCore</code> will be the limiting factor.
						</p></li><li class="listitem"><p class="simpara">
							Run the following command to create the CR:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							List the <code class="literal">MachineConfigPool</code> CRDs to see if the change is applied. The <code class="literal">UPDATING</code> column reports <code class="literal">True</code> if the change is picked up by the Machine Config Controller:
						</p><pre class="programlisting language-terminal">$ oc get machineconfigpools</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME     CONFIG                        UPDATED   UPDATING   DEGRADED
master   master-9cc2c72f205e103bb534   False     False      False
worker   worker-8cecd1236b33ee3f8a5e   False     True       False</pre>

							</p></div><p class="simpara">
							Once the change is complete, the <code class="literal">UPDATED</code> column reports <code class="literal">True</code>.
						</p><pre class="programlisting language-terminal">$ oc get machineconfigpools</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME     CONFIG                        UPDATED   UPDATING   DEGRADED
master   master-9cc2c72f205e103bb534   False     True       False
worker   worker-8cecd1236b33ee3f8a5e   True      False      False</pre>

							</p></div></li></ol></div></section></section><section class="section" id="nodes-node-tuning-operator"><div class="titlepage"><div><div><h2 class="title">6.6. Using the Node Tuning Operator</h2></div></div></div><p>
				Learn about the Node Tuning Operator and how you can use it to manage node-level tuning by orchestrating the tuned daemon.
			</p><h5 id="about-node-tuning-operator_nodes-node-tuning-operator">Purpose</h5><p>
				The Node Tuning Operator helps you manage node-level tuning by orchestrating the TuneD daemon and achieves low latency performance by using the Performance Profile controller. The majority of high-performance applications require some level of kernel tuning. The Node Tuning Operator provides a unified management interface to users of node-level sysctls and more flexibility to add custom tuning specified by user needs.
			</p><p>
				The Operator manages the containerized TuneD daemon for OpenShift Container Platform as a Kubernetes daemon set. It ensures the custom tuning specification is passed to all containerized TuneD daemons running in the cluster in the format that the daemons understand. The daemons run on all nodes in the cluster, one per node.
			</p><p>
				Node-level settings applied by the containerized TuneD daemon are rolled back on an event that triggers a profile change or when the containerized TuneD daemon is terminated gracefully by receiving and handling a termination signal.
			</p><p>
				The Node Tuning Operator uses the Performance Profile controller to implement automatic tuning to achieve low latency performance for OpenShift Container Platform applications.
			</p><p>
				The cluster administrator configures a performance profile to define node-level settings such as the following:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Updating the kernel to kernel-rt.
					</li><li class="listitem">
						Choosing CPUs for housekeeping.
					</li><li class="listitem">
						Choosing CPUs for running workloads.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Currently, disabling CPU load balancing is not supported by cgroup v2. As a result, you might not get the desired behavior from performance profiles if you have cgroup v2 enabled. Enabling cgroup v2 is not recommended if you are using performance profiles.
				</p></div></div><p>
				The Node Tuning Operator is part of a standard OpenShift Container Platform installation in version 4.1 and later.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					In earlier versions of OpenShift Container Platform, the Performance Addon Operator was used to implement automatic tuning to achieve low latency performance for OpenShift applications. In OpenShift Container Platform 4.11 and later, this functionality is part of the Node Tuning Operator.
				</p></div></div><section class="section" id="accessing-an-example-node-tuning-operator-specification_nodes-node-tuning-operator"><div class="titlepage"><div><div><h3 class="title">6.6.1. Accessing an example Node Tuning Operator specification</h3></div></div></div><p>
					Use this process to access an example Node Tuning Operator specification.
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Run the following command to access an example Node Tuning Operator specification:
						</p><pre class="programlisting language-terminal">$ oc get Tuned/default -o yaml -n openshift-cluster-node-tuning-operator</pre></li></ul></div><p>
					The default CR is meant for delivering standard node-level tuning for the OpenShift Container Platform platform and it can only be modified to set the Operator Management state. Any other custom changes to the default CR will be overwritten by the Operator. For custom tuning, create your own Tuned CRs. Newly created CRs will be combined with the default CR and custom tuning applied to OpenShift Container Platform nodes based on node or pod labels and profile priorities.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						While in certain situations the support for pod labels can be a convenient way of automatically delivering required tuning, this practice is discouraged and strongly advised against, especially in large-scale clusters. The default Tuned CR ships without pod label matching. If a custom profile is created with pod label matching, then the functionality will be enabled at that time. The pod label functionality will be deprecated in future versions of the Node Tuning Operator.
					</p></div></div></section><section class="section" id="custom-tuning-specification_nodes-node-tuning-operator"><div class="titlepage"><div><div><h3 class="title">6.6.2. Custom tuning specification</h3></div></div></div><p>
					The custom resource (CR) for the Operator has two major sections. The first section, <code class="literal">profile:</code>, is a list of TuneD profiles and their names. The second, <code class="literal">recommend:</code>, defines the profile selection logic.
				</p><p>
					Multiple custom tuning specifications can co-exist as multiple CRs in the Operator’s namespace. The existence of new CRs or the deletion of old CRs is detected by the Operator. All existing custom tuning specifications are merged and appropriate objects for the containerized TuneD daemons are updated.
				</p><p>
					<span class="strong strong"><strong>Management state</strong></span>
				</p><p>
					The Operator Management state is set by adjusting the default Tuned CR. By default, the Operator is in the Managed state and the <code class="literal">spec.managementState</code> field is not present in the default Tuned CR. Valid values for the Operator Management state are as follows:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Managed: the Operator will update its operands as configuration resources are updated
						</li><li class="listitem">
							Unmanaged: the Operator will ignore changes to the configuration resources
						</li><li class="listitem">
							Removed: the Operator will remove its operands and resources the Operator provisioned
						</li></ul></div><p>
					<span class="strong strong"><strong>Profile data</strong></span>
				</p><p>
					The <code class="literal">profile:</code> section lists TuneD profiles and their names.
				</p><pre class="programlisting language-yaml">profile:
- name: tuned_profile_1
  data: |
    # TuneD profile specification
    [main]
    summary=Description of tuned_profile_1 profile

    [sysctl]
    net.ipv4.ip_forward=1
    # ... other sysctl's or other TuneD daemon plugins supported by the containerized TuneD

# ...

- name: tuned_profile_n
  data: |
    # TuneD profile specification
    [main]
    summary=Description of tuned_profile_n profile

    # tuned_profile_n profile settings</pre><p>
					<span class="strong strong"><strong>Recommended profiles</strong></span>
				</p><p>
					The <code class="literal">profile:</code> selection logic is defined by the <code class="literal">recommend:</code> section of the CR. The <code class="literal">recommend:</code> section is a list of items to recommend the profiles based on a selection criteria.
				</p><pre class="programlisting language-yaml">recommend:
&lt;recommend-item-1&gt;
# ...
&lt;recommend-item-n&gt;</pre><p>
					The individual items of the list:
				</p><pre class="programlisting language-yaml">- machineConfigLabels: <span id="CO121-1"><!--Empty--></span><span class="callout">1</span>
    &lt;mcLabels&gt; <span id="CO121-2"><!--Empty--></span><span class="callout">2</span>
  match: <span id="CO121-3"><!--Empty--></span><span class="callout">3</span>
    &lt;match&gt; <span id="CO121-4"><!--Empty--></span><span class="callout">4</span>
  priority: &lt;priority&gt; <span id="CO121-5"><!--Empty--></span><span class="callout">5</span>
  profile: &lt;tuned_profile_name&gt; <span id="CO121-6"><!--Empty--></span><span class="callout">6</span>
  operand: <span id="CO121-7"><!--Empty--></span><span class="callout">7</span>
    debug: &lt;bool&gt; <span id="CO121-8"><!--Empty--></span><span class="callout">8</span>
    tunedConfig:
      reapply_sysctl: &lt;bool&gt; <span id="CO121-9"><!--Empty--></span><span class="callout">9</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO121-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Optional.
						</div></dd><dt><a href="#CO121-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							A dictionary of key/value <code class="literal">MachineConfig</code> labels. The keys must be unique.
						</div></dd><dt><a href="#CO121-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							If omitted, profile match is assumed unless a profile with a higher priority matches first or <code class="literal">machineConfigLabels</code> is set.
						</div></dd><dt><a href="#CO121-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							An optional list.
						</div></dd><dt><a href="#CO121-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Profile ordering priority. Lower numbers mean higher priority (<code class="literal">0</code> is the highest priority).
						</div></dd><dt><a href="#CO121-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							A TuneD profile to apply on a match. For example <code class="literal">tuned_profile_1</code>.
						</div></dd><dt><a href="#CO121-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							Optional operand configuration.
						</div></dd><dt><a href="#CO121-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							Turn debugging on or off for the TuneD daemon. Options are <code class="literal">true</code> for on or <code class="literal">false</code> for off. The default is <code class="literal">false</code>.
						</div></dd><dt><a href="#CO121-9"><span class="callout">9</span></a> </dt><dd><div class="para">
							Turn <code class="literal">reapply_sysctl</code> functionality on or off for the TuneD daemon. Options are <code class="literal">true</code> for on and <code class="literal">false</code> for off.
						</div></dd></dl></div><p>
					<code class="literal">&lt;match&gt;</code> is an optional list recursively defined as follows:
				</p><pre class="programlisting language-yaml">- label: &lt;label_name&gt; <span id="CO122-1"><!--Empty--></span><span class="callout">1</span>
  value: &lt;label_value&gt; <span id="CO122-2"><!--Empty--></span><span class="callout">2</span>
  type: &lt;label_type&gt; <span id="CO122-3"><!--Empty--></span><span class="callout">3</span>
    &lt;match&gt; <span id="CO122-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO122-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Node or pod label name.
						</div></dd><dt><a href="#CO122-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Optional node or pod label value. If omitted, the presence of <code class="literal">&lt;label_name&gt;</code> is enough to match.
						</div></dd><dt><a href="#CO122-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Optional object type (<code class="literal">node</code> or <code class="literal">pod</code>). If omitted, <code class="literal">node</code> is assumed.
						</div></dd><dt><a href="#CO122-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							An optional <code class="literal">&lt;match&gt;</code> list.
						</div></dd></dl></div><p>
					If <code class="literal">&lt;match&gt;</code> is not omitted, all nested <code class="literal">&lt;match&gt;</code> sections must also evaluate to <code class="literal">true</code>. Otherwise, <code class="literal">false</code> is assumed and the profile with the respective <code class="literal">&lt;match&gt;</code> section will not be applied or recommended. Therefore, the nesting (child <code class="literal">&lt;match&gt;</code> sections) works as logical AND operator. Conversely, if any item of the <code class="literal">&lt;match&gt;</code> list matches, the entire <code class="literal">&lt;match&gt;</code> list evaluates to <code class="literal">true</code>. Therefore, the list acts as logical OR operator.
				</p><p>
					If <code class="literal">machineConfigLabels</code> is defined, machine config pool based matching is turned on for the given <code class="literal">recommend:</code> list item. <code class="literal">&lt;mcLabels&gt;</code> specifies the labels for a machine config. The machine config is created automatically to apply host settings, such as kernel boot parameters, for the profile <code class="literal">&lt;tuned_profile_name&gt;</code>. This involves finding all machine config pools with machine config selector matching <code class="literal">&lt;mcLabels&gt;</code> and setting the profile <code class="literal">&lt;tuned_profile_name&gt;</code> on all nodes that are assigned the found machine config pools. To target nodes that have both master and worker roles, you must use the master role.
				</p><p>
					The list items <code class="literal">match</code> and <code class="literal">machineConfigLabels</code> are connected by the logical OR operator. The <code class="literal">match</code> item is evaluated first in a short-circuit manner. Therefore, if it evaluates to <code class="literal">true</code>, the <code class="literal">machineConfigLabels</code> item is not considered.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						When using machine config pool based matching, it is advised to group nodes with the same hardware configuration into the same machine config pool. Not following this practice might result in TuneD operands calculating conflicting kernel parameters for two or more nodes sharing the same machine config pool.
					</p></div></div><div class="formalpara"><p class="title"><strong>Example: node or pod label based matching</strong></p><p>
						
<pre class="programlisting language-yaml">- match:
  - label: tuned.openshift.io/elasticsearch
    match:
    - label: node-role.kubernetes.io/master
    - label: node-role.kubernetes.io/infra
    type: pod
  priority: 10
  profile: openshift-control-plane-es
- match:
  - label: node-role.kubernetes.io/master
  - label: node-role.kubernetes.io/infra
  priority: 20
  profile: openshift-control-plane
- priority: 30
  profile: openshift-node</pre>

					</p></div><p>
					The CR above is translated for the containerized TuneD daemon into its <code class="literal">recommend.conf</code> file based on the profile priorities. The profile with the highest priority (<code class="literal">10</code>) is <code class="literal">openshift-control-plane-es</code> and, therefore, it is considered first. The containerized TuneD daemon running on a given node looks to see if there is a pod running on the same node with the <code class="literal">tuned.openshift.io/elasticsearch</code> label set. If not, the entire <code class="literal">&lt;match&gt;</code> section evaluates as <code class="literal">false</code>. If there is such a pod with the label, in order for the <code class="literal">&lt;match&gt;</code> section to evaluate to <code class="literal">true</code>, the node label also needs to be <code class="literal">node-role.kubernetes.io/master</code> or <code class="literal">node-role.kubernetes.io/infra</code>.
				</p><p>
					If the labels for the profile with priority <code class="literal">10</code> matched, <code class="literal">openshift-control-plane-es</code> profile is applied and no other profile is considered. If the node/pod label combination did not match, the second highest priority profile (<code class="literal">openshift-control-plane</code>) is considered. This profile is applied if the containerized TuneD pod runs on a node with labels <code class="literal">node-role.kubernetes.io/master</code> or <code class="literal">node-role.kubernetes.io/infra</code>.
				</p><p>
					Finally, the profile <code class="literal">openshift-node</code> has the lowest priority of <code class="literal">30</code>. It lacks the <code class="literal">&lt;match&gt;</code> section and, therefore, will always match. It acts as a profile catch-all to set <code class="literal">openshift-node</code> profile, if no other profile with higher priority matches on a given node.
				</p><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Nodes-en-US/images/b350c395f7c7262cec5e5d9d7404ce73/node-tuning-operator-workflow-revised.png" alt="Decision workflow"/></div></div><div class="formalpara"><p class="title"><strong>Example: machine config pool based matching</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: openshift-node-custom
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=Custom OpenShift node profile with an additional kernel parameter
      include=openshift-node
      [bootloader]
      cmdline_openshift_node_custom=+skew_tick=1
    name: openshift-node-custom

  recommend:
  - machineConfigLabels:
      machineconfiguration.openshift.io/role: "worker-custom"
    priority: 20
    profile: openshift-node-custom</pre>

					</p></div><p>
					To minimize node reboots, label the target nodes with a label the machine config pool’s node selector will match, then create the Tuned CR above and finally create the custom machine config pool itself.
				</p><p>
					<span class="strong strong"><strong>Cloud provider-specific TuneD profiles</strong></span>
				</p><p>
					With this functionality, all Cloud provider-specific nodes can conveniently be assigned a TuneD profile specifically tailored to a given Cloud provider on a OpenShift Container Platform cluster. This can be accomplished without adding additional node labels or grouping nodes into machine config pools.
				</p><p>
					This functionality takes advantage of <code class="literal">spec.providerID</code> node object values in the form of <code class="literal">&lt;cloud-provider&gt;://&lt;cloud-provider-specific-id&gt;</code> and writes the file <code class="literal">/var/lib/tuned/provider</code> with the value <code class="literal">&lt;cloud-provider&gt;</code> in NTO operand containers. The content of this file is then used by TuneD to load <code class="literal">provider-&lt;cloud-provider&gt;</code> profile if such profile exists.
				</p><p>
					The <code class="literal">openshift</code> profile that both <code class="literal">openshift-control-plane</code> and <code class="literal">openshift-node</code> profiles inherit settings from is now updated to use this functionality through the use of conditional profile loading. Neither NTO nor TuneD currently include any Cloud provider-specific profiles. However, it is possible to create a custom profile <code class="literal">provider-&lt;cloud-provider&gt;</code> that will be applied to all Cloud provider-specific cluster nodes.
				</p><div class="formalpara"><p class="title"><strong>Example GCE Cloud provider profile</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: provider-gce
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=GCE Cloud provider-specific profile
      # Your tuning for GCE Cloud provider goes here.
    name: provider-gce</pre>

					</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Due to profile inheritance, any setting specified in the <code class="literal">provider-&lt;cloud-provider&gt;</code> profile will be overwritten by the <code class="literal">openshift</code> profile and its child profiles.
					</p></div></div></section><section class="section" id="custom-tuning-default-profiles-set_nodes-node-tuning-operator"><div class="titlepage"><div><div><h3 class="title">6.6.3. Default profiles set on a cluster</h3></div></div></div><p>
					The following are the default profiles set on a cluster.
				</p><pre class="programlisting language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: default
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=Optimize systems running OpenShift (provider specific parent profile)
      include=-provider-${f:exec:cat:/var/lib/tuned/provider},openshift
    name: openshift
  recommend:
  - profile: openshift-control-plane
    priority: 30
    match:
    - label: node-role.kubernetes.io/master
    - label: node-role.kubernetes.io/infra
  - profile: openshift-node
    priority: 40</pre><p>
					Starting with OpenShift Container Platform 4.9, all OpenShift TuneD profiles are shipped with the TuneD package. You can use the <code class="literal">oc exec</code> command to view the contents of these profiles:
				</p><pre class="programlisting language-terminal">$ oc exec $tuned_pod -n openshift-cluster-node-tuning-operator -- find /usr/lib/tuned/openshift{,-control-plane,-node} -name tuned.conf -exec grep -H ^ {} \;</pre></section><section class="section" id="supported-tuned-daemon-plug-ins_nodes-node-tuning-operator"><div class="titlepage"><div><div><h3 class="title">6.6.4. Supported TuneD daemon plugins</h3></div></div></div><p>
					Excluding the <code class="literal">[main]</code> section, the following TuneD plugins are supported when using custom profiles defined in the <code class="literal">profile:</code> section of the Tuned CR:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							audio
						</li><li class="listitem">
							cpu
						</li><li class="listitem">
							disk
						</li><li class="listitem">
							eeepc_she
						</li><li class="listitem">
							modules
						</li><li class="listitem">
							mounts
						</li><li class="listitem">
							net
						</li><li class="listitem">
							scheduler
						</li><li class="listitem">
							scsi_host
						</li><li class="listitem">
							selinux
						</li><li class="listitem">
							sysctl
						</li><li class="listitem">
							sysfs
						</li><li class="listitem">
							usb
						</li><li class="listitem">
							video
						</li><li class="listitem">
							vm
						</li><li class="listitem">
							bootloader
						</li></ul></div><p>
					There is some dynamic tuning functionality provided by some of these plugins that is not supported. The following TuneD plugins are currently not supported:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							script
						</li><li class="listitem">
							systemd
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The TuneD bootloader plugin only supports Red Hat Enterprise Linux CoreOS (RHCOS) worker nodes.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/customizing-tuned-profiles_monitoring-and-managing-system-status-and-performance#available-tuned-plug-ins_customizing-tuned-profiles">Available TuneD Plugins</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/getting-started-with-tuned_monitoring-and-managing-system-status-and-performance">Getting Started with TuneD</a>
						</li></ul></div></section></section><section class="section" id="nodes-remediating-fencing-maintaining-rhwa"><div class="titlepage"><div><div><h2 class="title">6.7. Remediating, fencing, and maintaining nodes</h2></div></div></div><p>
				When node-level failures occur, such as the kernel hangs or network interface controllers (NICs) fail, the work required from the cluster does not decrease, and workloads from affected nodes need to be restarted somewhere. Failures affecting these workloads risk data loss, corruption, or both. It is important to isolate the node, known as <code class="literal">fencing</code>, before initiating recovery of the workload, known as <code class="literal">remediation</code>, and recovery of the node.
			</p><p>
				For more information on remediation, fencing, and maintaining nodes, see the <a class="link" href="https://access.redhat.com/documentation/en-us/workload_availability_for_red_hat_openshift/23.2/html-single/remediation_fencing_and_maintenance/index#about-remediation-fencing-maintenance">Workload Availability for Red Hat OpenShift</a> documentation.
			</p></section><section class="section" id="nodes-nodes-rebooting"><div class="titlepage"><div><div><h2 class="title">6.8. Understanding node rebooting</h2></div></div></div><p>
				To reboot a node without causing an outage for applications running on the platform, it is important to first evacuate the pods. For pods that are made highly available by the routing tier, nothing else needs to be done. For other pods needing storage, typically databases, it is critical to ensure that they can remain in operation with one pod temporarily going offline. While implementing resiliency for stateful pods is different for each application, in all cases it is important to configure the scheduler to use node anti-affinity to ensure that the pods are properly spread across available nodes.
			</p><p>
				Another challenge is how to handle nodes that are running critical infrastructure such as the router or the registry. The same node evacuation process applies, though it is important to understand certain edge cases.
			</p><section class="section" id="nodes-nodes-rebooting-infrastructure_nodes-nodes-rebooting"><div class="titlepage"><div><div><h3 class="title">6.8.1. About rebooting nodes running critical infrastructure</h3></div></div></div><p>
					When rebooting nodes that host critical OpenShift Container Platform infrastructure components, such as router pods, registry pods, and monitoring pods, ensure that there are at least three nodes available to run these components.
				</p><p>
					The following scenario demonstrates how service interruptions can occur with applications running on OpenShift Container Platform when only two nodes are available:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Node A is marked unschedulable and all pods are evacuated.
						</li><li class="listitem">
							The registry pod running on that node is now redeployed on node B. Node B is now running both registry pods.
						</li><li class="listitem">
							Node B is now marked unschedulable and is evacuated.
						</li><li class="listitem">
							The service exposing the two pod endpoints on node B loses all endpoints, for a brief period of time, until they are redeployed to node A.
						</li></ul></div><p>
					When using three nodes for infrastructure components, this process does not result in a service disruption. However, due to pod scheduling, the last node that is evacuated and brought back into rotation does not have a registry pod. One of the other nodes has two registry pods. To schedule the third registry pod on the last node, use pod anti-affinity to prevent the scheduler from locating two registry pods on the same node.
				</p><div class="itemizedlist"><p class="title"><strong>Additional information</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							For more information on pod anti-affinity, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-pod-affinity">Placing pods relative to other pods using affinity and anti-affinity rules</a>.
						</li></ul></div></section><section class="section" id="nodes-nodes-rebooting-affinity_nodes-nodes-rebooting"><div class="titlepage"><div><div><h3 class="title">6.8.2. Rebooting a node using pod anti-affinity</h3></div></div></div><p>
					Pod anti-affinity is slightly different than node anti-affinity. Node anti-affinity can be violated if there are no other suitable locations to deploy a pod. Pod anti-affinity can be set to either required or preferred.
				</p><p>
					With this in place, if only two infrastructure nodes are available and one is rebooted, the container image registry pod is prevented from running on the other node. <code class="literal"><span class="strong strong"><strong>oc get pods</strong></span></code> reports the pod as unready until a suitable node is available. Once a node is available and all pods are back in ready state, the next node can be restarted.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To reboot a node using pod anti-affinity:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the node specification to configure pod anti-affinity:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: with-pod-antiaffinity
spec:
  affinity:
    podAntiAffinity: <span id="CO123-1"><!--Empty--></span><span class="callout">1</span>
      preferredDuringSchedulingIgnoredDuringExecution: <span id="CO123-2"><!--Empty--></span><span class="callout">2</span>
      - weight: 100 <span id="CO123-3"><!--Empty--></span><span class="callout">3</span>
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: registry <span id="CO123-4"><!--Empty--></span><span class="callout">4</span>
              operator: In <span id="CO123-5"><!--Empty--></span><span class="callout">5</span>
              values:
              - default
          topologyKey: kubernetes.io/hostname
#...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO123-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Stanza to configure pod anti-affinity.
								</div></dd><dt><a href="#CO123-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Defines a preferred rule.
								</div></dd><dt><a href="#CO123-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specifies a weight for a preferred rule. The node with the highest weight is preferred.
								</div></dd><dt><a href="#CO123-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Description of the pod label that determines when the anti-affinity rule applies. Specify a key and value for the label.
								</div></dd><dt><a href="#CO123-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									The operator represents the relationship between the label on the existing pod and the set of values in the <code class="literal">matchExpression</code> parameters in the specification for the new pod. Can be <code class="literal">In</code>, <code class="literal">NotIn</code>, <code class="literal">Exists</code>, or <code class="literal">DoesNotExist</code>.
								</div></dd></dl></div><p class="simpara">
							This example assumes the container image registry pod has a label of <code class="literal">registry=default</code>. Pod anti-affinity can use any Kubernetes match expression.
						</p></li><li class="listitem">
							Enable the <code class="literal">MatchInterPodAffinity</code> scheduler predicate in the scheduling policy file.
						</li><li class="listitem">
							Perform a graceful restart of the node.
						</li></ol></div></section><section class="section" id="nodes-nodes-rebooting-router_nodes-nodes-rebooting"><div class="titlepage"><div><div><h3 class="title">6.8.3. Understanding how to reboot nodes running routers</h3></div></div></div><p>
					In most cases, a pod running an OpenShift Container Platform router exposes a host port.
				</p><p>
					The <code class="literal">PodFitsPorts</code> scheduler predicate ensures that no router pods using the same port can run on the same node, and pod anti-affinity is achieved. If the routers are relying on IP failover for high availability, there is nothing else that is needed.
				</p><p>
					For router pods relying on an external service such as AWS Elastic Load Balancing for high availability, it is that service’s responsibility to react to router pod restarts.
				</p><p>
					In rare cases, a router pod may not have a host port configured. In those cases, it is important to follow the recommended restart process for infrastructure nodes.
				</p></section><section class="section" id="nodes-nodes-rebooting-gracefully_nodes-nodes-rebooting"><div class="titlepage"><div><div><h3 class="title">6.8.4. Rebooting a node gracefully</h3></div></div></div><p>
					Before rebooting a node, it is recommended to backup etcd data to avoid any data loss on the node.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						For single-node OpenShift clusters that require users to perform the <code class="literal">oc login</code> command rather than having the certificates in <code class="literal">kubeconfig</code> file to manage the cluster, the <code class="literal">oc adm</code> commands might not be available after cordoning and draining the node. This is because the <code class="literal">openshift-oauth-apiserver</code> pod is not running due to the cordon. You can use SSH to access the nodes as indicated in the following procedure.
					</p><p>
						In a single-node OpenShift cluster, pods cannot be rescheduled when cordoning and draining. However, doing so gives the pods, especially your workload pods, time to properly stop and release associated resources.
					</p></div></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To perform a graceful restart of a node:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Mark the node as unschedulable:
						</p><pre class="programlisting language-terminal">$ oc adm cordon &lt;node1&gt;</pre></li><li class="listitem"><p class="simpara">
							Drain the node to remove all the running pods:
						</p><pre class="programlisting language-terminal">$ oc adm drain &lt;node1&gt; --ignore-daemonsets --delete-emptydir-data --force</pre><p class="simpara">
							You might receive errors that pods associated with custom pod disruption budgets (PDB) cannot be evicted.
						</p><div class="formalpara"><p class="title"><strong>Example error</strong></p><p>
								
<pre class="programlisting language-terminal">error when evicting pods/"rails-postgresql-example-1-72v2w" -n "rails" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.</pre>

							</p></div><p class="simpara">
							In this case, run the drain command again, adding the <code class="literal">disable-eviction</code> flag, which bypasses the PDB checks:
						</p><pre class="programlisting language-terminal">$ oc adm drain &lt;node1&gt; --ignore-daemonsets --delete-emptydir-data --force --disable-eviction</pre></li><li class="listitem"><p class="simpara">
							Access the node in debug mode:
						</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node1&gt;</pre></li><li class="listitem"><p class="simpara">
							Change your root directory to <code class="literal">/host</code>:
						</p><pre class="programlisting language-terminal">$ chroot /host</pre></li><li class="listitem"><p class="simpara">
							Restart the node:
						</p><pre class="programlisting language-terminal">$ systemctl reboot</pre><p class="simpara">
							In a moment, the node enters the <code class="literal">NotReady</code> state.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								With some single-node OpenShift clusters, the <code class="literal">oc</code> commands might not be available after you cordon and drain the node because the <code class="literal">openshift-oauth-apiserver</code> pod is not running. You can use SSH to connect to the node and perform the reboot.
							</p><pre class="programlisting language-terminal">$ ssh core@&lt;master-node&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;</pre><pre class="programlisting language-terminal">$ sudo systemctl reboot</pre></div></div></li><li class="listitem"><p class="simpara">
							After the reboot is complete, mark the node as schedulable by running the following command:
						</p><pre class="programlisting language-terminal">$ oc adm uncordon &lt;node1&gt;</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								With some single-node OpenShift clusters, the <code class="literal">oc</code> commands might not be available after you cordon and drain the node because the <code class="literal">openshift-oauth-apiserver</code> pod is not running. You can use SSH to connect to the node and uncordon it.
							</p><pre class="programlisting language-terminal">$ ssh core@&lt;target_node&gt;</pre><pre class="programlisting language-terminal">$ sudo oc adm uncordon &lt;node&gt; --kubeconfig /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig</pre></div></div></li><li class="listitem"><p class="simpara">
							Verify that the node is ready:
						</p><pre class="programlisting language-terminal">$ oc get node &lt;node1&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME    STATUS  ROLES    AGE     VERSION
&lt;node1&gt; Ready   worker   6d22h   v1.18.3+b0068a8</pre>

							</p></div></li></ol></div><div class="formalpara"><p class="title"><strong>Additional information</strong></p><p>
						For information on etcd data backup, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/backup_and_restore/#backup-etcd">Backing up etcd data</a>.
					</p></div></section></section><section class="section" id="nodes-nodes-garbage-collection"><div class="titlepage"><div><div><h2 class="title">6.9. Freeing node resources using garbage collection</h2></div></div></div><p>
				As an administrator, you can use OpenShift Container Platform to ensure that your nodes are running efficiently by freeing up resources through garbage collection.
			</p><p>
				The OpenShift Container Platform node performs two types of garbage collection:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Container garbage collection: Removes terminated containers.
					</li><li class="listitem">
						Image garbage collection: Removes images not referenced by any running pods.
					</li></ul></div><section class="section" id="nodes-nodes-garbage-collection-containers_nodes-nodes-configuring"><div class="titlepage"><div><div><h3 class="title">6.9.1. Understanding how terminated containers are removed through garbage collection</h3></div></div></div><p>
					Container garbage collection removes terminated containers by using eviction thresholds.
				</p><p>
					When eviction thresholds are set for garbage collection, the node tries to keep any container for any pod accessible from the API. If the pod has been deleted, the containers will be as well. Containers are preserved as long the pod is not deleted and the eviction threshold is not reached. If the node is under disk pressure, it will remove containers and their logs will no longer be accessible using <code class="literal">oc logs</code>.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<span class="strong strong"><strong>eviction-soft</strong></span> - A soft eviction threshold pairs an eviction threshold with a required administrator-specified grace period.
						</li><li class="listitem">
							<span class="strong strong"><strong>eviction-hard</strong></span> - A hard eviction threshold has no grace period, and if observed, OpenShift Container Platform takes immediate action.
						</li></ul></div><p>
					The following table lists the eviction thresholds:
				</p><div class="table" id="idm140232225847648"><p class="title"><strong>Table 6.2. Variables for configuring container garbage collection</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232225841968" scope="col">Node condition</th><th align="left" valign="top" id="idm140232225840880" scope="col">Eviction signal</th><th align="left" valign="top" id="idm140232225839792" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232225841968"> <p>
									MemoryPressure
								</p>
								 </td><td align="left" valign="top" headers="idm140232225840880"> <p>
									<code class="literal">memory.available</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232225839792"> <p>
									The available memory on the node.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232225841968"> <p>
									DiskPressure
								</p>
								 </td><td align="left" valign="top" headers="idm140232225840880"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">nodefs.available</code>
										</li><li class="listitem">
											<code class="literal">nodefs.inodesFree</code>
										</li><li class="listitem">
											<code class="literal">imagefs.available</code>
										</li><li class="listitem">
											<code class="literal">imagefs.inodesFree</code>
										</li></ul></div>
								 </td><td align="left" valign="top" headers="idm140232225839792"> <p>
									The available disk space or inodes on the node root file system, <code class="literal">nodefs</code>, or image file system, <code class="literal">imagefs</code>.
								</p>
								 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						For <code class="literal">evictionHard</code> you must specify all of these parameters. If you do not specify all parameters, only the specified parameters are applied and the garbage collection will not function properly.
					</p></div></div><p>
					If a node is oscillating above and below a soft eviction threshold, but not exceeding its associated grace period, the corresponding node would constantly oscillate between <code class="literal">true</code> and <code class="literal">false</code>. As a consequence, the scheduler could make poor scheduling decisions.
				</p><p>
					To protect against this oscillation, use the <code class="literal">eviction-pressure-transition-period</code> flag to control how long OpenShift Container Platform must wait before transitioning out of a pressure condition. OpenShift Container Platform will not set an eviction threshold as being met for the specified pressure condition for the period specified before toggling the condition back to false.
				</p></section><section class="section" id="nodes-nodes-garbage-collection-images_nodes-nodes-configuring"><div class="titlepage"><div><div><h3 class="title">6.9.2. Understanding how images are removed through garbage collection</h3></div></div></div><p>
					Image garbage collection removes images that are not referenced by any running pods.
				</p><p>
					OpenShift Container Platform determines which images to remove from a node based on the disk usage that is reported by <span class="strong strong"><strong>cAdvisor</strong></span>.
				</p><p>
					The policy for image garbage collection is based on two conditions:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The percent of disk usage (expressed as an integer) which triggers image garbage collection. The default is <span class="strong strong"><strong>85</strong></span>.
						</li><li class="listitem">
							The percent of disk usage (expressed as an integer) to which image garbage collection attempts to free. Default is <span class="strong strong"><strong>80</strong></span>.
						</li></ul></div><p>
					For image garbage collection, you can modify any of the following variables using a custom resource.
				</p><div class="table" id="idm140232226079040"><p class="title"><strong>Table 6.3. Variables for configuring image garbage collection</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 75%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232226074176" scope="col">Setting</th><th align="left" valign="top" id="idm140232226073088" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232226074176"> <p>
									<code class="literal">imageMinimumGCAge</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226073088"> <p>
									The minimum age for an unused image before the image is removed by garbage collection. The default is <span class="strong strong"><strong>2m</strong></span>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226074176"> <p>
									<code class="literal">imageGCHighThresholdPercent</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226073088"> <p>
									The percent of disk usage, expressed as an integer, which triggers image garbage collection. The default is <span class="strong strong"><strong>85</strong></span>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226074176"> <p>
									<code class="literal">imageGCLowThresholdPercent</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226073088"> <p>
									The percent of disk usage, expressed as an integer, to which image garbage collection attempts to free. The default is <span class="strong strong"><strong>80</strong></span>.
								</p>
								 </td></tr></tbody></table></div></div><p>
					Two lists of images are retrieved in each garbage collector run:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							A list of images currently running in at least one pod.
						</li><li class="listitem">
							A list of images available on a host.
						</li></ol></div><p>
					As new containers are run, new images appear. All images are marked with a time stamp. If the image is running (the first list above) or is newly detected (the second list above), it is marked with the current time. The remaining images are already marked from the previous spins. All images are then sorted by the time stamp.
				</p><p>
					Once the collection starts, the oldest images get deleted first until the stopping criterion is met.
				</p></section><section class="section" id="nodes-nodes-garbage-collection-configuring_nodes-nodes-configuring"><div class="titlepage"><div><div><h3 class="title">6.9.3. Configuring garbage collection for containers and images</h3></div></div></div><p>
					As an administrator, you can configure how OpenShift Container Platform performs garbage collection by creating a <code class="literal">kubeletConfig</code> object for each machine config pool.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						OpenShift Container Platform supports only one <code class="literal">kubeletConfig</code> object for each machine config pool.
					</p></div></div><p>
					You can configure any combination of the following:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Soft eviction for containers
						</li><li class="listitem">
							Hard eviction for containers
						</li><li class="listitem">
							Eviction for images
						</li></ul></div><p>
					Container garbage collection removes terminated containers. Image garbage collection removes images that are not referenced by any running pods.
				</p><div class="orderedlist"><p class="title"><strong>Prerequisites</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the label associated with the static <code class="literal">MachineConfigPool</code> CRD for the type of node you want to configure by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc edit machineconfigpool &lt;name&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc edit machineconfigpool worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: "2022-11-16T15:34:25Z"
  generation: 4
  labels:
    pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO124-1"><!--Empty--></span><span class="callout">1</span>
  name: worker
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO124-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The label appears under Labels.
								</div></dd></dl></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							If the label is not present, add a key/value pair such as:
						</p><pre class="screen">$ oc label machineconfigpool worker custom-kubelet=small-pods</pre></div></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a custom resource (CR) for your configuration change.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								If there is one file system, or if <code class="literal">/var/lib/kubelet</code> and <code class="literal">/var/lib/containers/</code> are in the same file system, the settings with the highest values trigger evictions, as those are met first. The file system triggers the eviction.
							</p></div></div><div class="formalpara"><p class="title"><strong>Sample configuration for a container garbage collection CR:</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: worker-kubeconfig <span id="CO125-1"><!--Empty--></span><span class="callout">1</span>
spec:
  machineConfigPoolSelector:
    matchLabels:
      pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO125-2"><!--Empty--></span><span class="callout">2</span>
  kubeletConfig:
    evictionSoft: <span id="CO125-3"><!--Empty--></span><span class="callout">3</span>
      memory.available: "500Mi" <span id="CO125-4"><!--Empty--></span><span class="callout">4</span>
      nodefs.available: "10%"
      nodefs.inodesFree: "5%"
      imagefs.available: "15%"
      imagefs.inodesFree: "10%"
    evictionSoftGracePeriod:  <span id="CO125-5"><!--Empty--></span><span class="callout">5</span>
      memory.available: "1m30s"
      nodefs.available: "1m30s"
      nodefs.inodesFree: "1m30s"
      imagefs.available: "1m30s"
      imagefs.inodesFree: "1m30s"
    evictionHard: <span id="CO125-6"><!--Empty--></span><span class="callout">6</span>
      memory.available: "200Mi"
      nodefs.available: "5%"
      nodefs.inodesFree: "4%"
      imagefs.available: "10%"
      imagefs.inodesFree: "5%"
    evictionPressureTransitionPeriod: 0s <span id="CO125-7"><!--Empty--></span><span class="callout">7</span>
    imageMinimumGCAge: 5m <span id="CO125-8"><!--Empty--></span><span class="callout">8</span>
    imageGCHighThresholdPercent: 80 <span id="CO125-9"><!--Empty--></span><span class="callout">9</span>
    imageGCLowThresholdPercent: 75 <span id="CO125-10"><!--Empty--></span><span class="callout">10</span>
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO125-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Name for the object.
								</div></dd><dt><a href="#CO125-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the label from the machine config pool.
								</div></dd><dt><a href="#CO125-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									For container garbage collection: Type of eviction: <code class="literal">evictionSoft</code> or <code class="literal">evictionHard</code>.
								</div></dd><dt><a href="#CO125-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									For container garbage collection: Eviction thresholds based on a specific eviction trigger signal.
								</div></dd><dt><a href="#CO125-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									For container garbage collection: Grace periods for the soft eviction. This parameter does not apply to <code class="literal">eviction-hard</code>.
								</div></dd><dt><a href="#CO125-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									For container garbage collection: Eviction thresholds based on a specific eviction trigger signal. For <code class="literal">evictionHard</code> you must specify all of these parameters. If you do not specify all parameters, only the specified parameters are applied and the garbage collection will not function properly.
								</div></dd><dt><a href="#CO125-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									For container garbage collection: The duration to wait before transitioning out of an eviction pressure condition.
								</div></dd><dt><a href="#CO125-8"><span class="callout">8</span></a> </dt><dd><div class="para">
									For image garbage collection: The minimum age for an unused image before the image is removed by garbage collection.
								</div></dd><dt><a href="#CO125-9"><span class="callout">9</span></a> </dt><dd><div class="para">
									For image garbage collection: The percent of disk usage (expressed as an integer) that triggers image garbage collection.
								</div></dd><dt><a href="#CO125-10"><span class="callout">10</span></a> </dt><dd><div class="para">
									For image garbage collection: The percent of disk usage (expressed as an integer) that image garbage collection attempts to free.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Run the following command to create the CR:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc create -f gc-container.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">kubeletconfig.machineconfiguration.openshift.io/gc-container created</pre>

							</p></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Verify that garbage collection is active by entering the following command. The Machine Config Pool you specified in the custom resource appears with <code class="literal">UPDATING</code> as 'true` until the change is fully implemented:
						</p><pre class="programlisting language-terminal">$ oc get machineconfigpool</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME     CONFIG                                   UPDATED   UPDATING
master   rendered-master-546383f80705bd5aeaba93   True      False
worker   rendered-worker-b4c51bb33ccaae6fc4a6a5   False     True</pre>

							</p></div></li></ol></div></section></section><section class="section" id="nodes-nodes-resources-configuring"><div class="titlepage"><div><div><h2 class="title">6.10. Allocating resources for nodes in an OpenShift Container Platform cluster</h2></div></div></div><p>
				To provide more reliable scheduling and minimize node resource overcommitment, reserve a portion of the CPU and memory resources for use by the underlying node components, such as <code class="literal">kubelet</code> and <code class="literal">kube-proxy</code>, and the remaining system components, such as <code class="literal">sshd</code> and <code class="literal">NetworkManager</code>. By specifying the resources to reserve, you provide the scheduler with more information about the remaining CPU and memory resources that a node has available for use by pods. You can allow OpenShift Container Platform to <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-resources-configuring-auto_nodes-nodes-resources-configuring">automatically determine the optimal <code class="literal">system-reserved</code> CPU and memory resources</a> for your nodes or you can <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-resources-configuring-setting_nodes-nodes-resources-configuring">manually determine and set the best resources</a> for your nodes.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					To manually set resource values, you must use a kubelet config CR. You cannot use a machine config CR.
				</p></div></div><section class="section" id="nodes-nodes-resources-configuring-about_nodes-nodes-resources-configuring"><div class="titlepage"><div><div><h3 class="title">6.10.1. Understanding how to allocate resources for nodes</h3></div></div></div><p>
					CPU and memory resources reserved for node components in OpenShift Container Platform are based on two node settings:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 67%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232226128640" scope="col">Setting</th><th align="left" valign="top" id="idm140232226127552" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232226128640"> <p>
									<code class="literal">kube-reserved</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226127552"> <p>
									This setting is not used with OpenShift Container Platform. Add the CPU and memory resources that you planned to reserve to the <code class="literal">system-reserved</code> setting.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226128640"> <p>
									<code class="literal">system-reserved</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226127552"> <p>
									This setting identifies the resources to reserve for the node components and system components, such as CRI-O and Kubelet. The default settings depend on the OpenShift Container Platform and Machine Config Operator versions. Confirm the default <code class="literal">systemReserved</code> parameter on the <code class="literal">machine-config-operator</code> repository.
								</p>
								 </td></tr></tbody></table></div><p>
					If a flag is not set, the defaults are used. If none of the flags are set, the allocated resource is set to the node’s capacity as it was before the introduction of allocatable resources.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Any CPUs specifically reserved using the <code class="literal">reservedSystemCPUs</code> parameter are not available for allocation using <code class="literal">kube-reserved</code> or <code class="literal">system-reserved</code>.
					</p></div></div><section class="section" id="computing-allocated-resources_nodes-nodes-resources-configuring"><div class="titlepage"><div><div><h4 class="title">6.10.1.1. How OpenShift Container Platform computes allocated resources</h4></div></div></div><p>
						An allocated amount of a resource is computed based on the following formula:
					</p><pre class="screen">[Allocatable] = [Node Capacity] - [system-reserved] - [Hard-Eviction-Thresholds]</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The withholding of <code class="literal">Hard-Eviction-Thresholds</code> from <code class="literal">Allocatable</code> improves system reliability because the value for <code class="literal">Allocatable</code> is enforced for pods at the node level.
						</p></div></div><p>
						If <code class="literal">Allocatable</code> is negative, it is set to <code class="literal">0</code>.
					</p><p>
						Each node reports the system resources that are used by the container runtime and kubelet. To simplify configuring the <code class="literal">system-reserved</code> parameter, view the resource use for the node by using the node summary API. The node summary is available at <code class="literal">/api/v1/nodes/&lt;node&gt;/proxy/stats/summary</code>.
					</p></section><section class="section" id="allocate-node-enforcement_nodes-nodes-resources-configuring"><div class="titlepage"><div><div><h4 class="title">6.10.1.2. How nodes enforce resource constraints</h4></div></div></div><p>
						The node is able to limit the total amount of resources that pods can consume based on the configured allocatable value. This feature significantly improves the reliability of the node by preventing pods from using CPU and memory resources that are needed by system services such as the container runtime and node agent. To improve node reliability, administrators should reserve resources based on a target for resource use.
					</p><p>
						The node enforces resource constraints by using a new cgroup hierarchy that enforces quality of service. All pods are launched in a dedicated cgroup hierarchy that is separate from system daemons.
					</p><p>
						Administrators should treat system daemons similar to pods that have a guaranteed quality of service. System daemons can burst within their bounding control groups and this behavior must be managed as part of cluster deployments. Reserve CPU and memory resources for system daemons by specifying the amount of CPU and memory resources in <code class="literal">system-reserved</code>.
					</p><p>
						Enforcing <code class="literal">system-reserved</code> limits can prevent critical system services from receiving CPU and memory resources. As a result, a critical system service can be ended by the out-of-memory killer. The recommendation is to enforce <code class="literal">system-reserved</code> only if you have profiled the nodes exhaustively to determine precise estimates and you are confident that critical system services can recover if any process in that group is ended by the out-of-memory killer.
					</p></section><section class="section" id="allocate-eviction-thresholds_nodes-nodes-resources-configuring"><div class="titlepage"><div><div><h4 class="title">6.10.1.3. Understanding Eviction Thresholds</h4></div></div></div><p>
						If a node is under memory pressure, it can impact the entire node and all pods running on the node. For example, a system daemon that uses more than its reserved amount of memory can trigger an out-of-memory event. To avoid or reduce the probability of system out-of-memory events, the node provides out-of-resource handling.
					</p><p>
						You can reserve some memory using the <code class="literal">--eviction-hard</code> flag. The node attempts to evict pods whenever memory availability on the node drops below the absolute value or percentage. If system daemons do not exist on a node, pods are limited to the memory <code class="literal">capacity - eviction-hard</code>. For this reason, resources set aside as a buffer for eviction before reaching out of memory conditions are not available for pods.
					</p><p>
						The following is an example to illustrate the impact of node allocatable for memory:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Node capacity is <code class="literal">32Gi</code>
							</li><li class="listitem">
								--system-reserved is <code class="literal">3Gi</code>
							</li><li class="listitem">
								--eviction-hard is set to <code class="literal">100Mi</code>.
							</li></ul></div><p>
						For this node, the effective node allocatable value is <code class="literal">28.9Gi</code>. If the node and system components use all their reservation, the memory available for pods is <code class="literal">28.9Gi</code>, and kubelet evicts pods when it exceeds this threshold.
					</p><p>
						If you enforce node allocatable, <code class="literal">28.9Gi</code>, with top-level cgroups, then pods can never exceed <code class="literal">28.9Gi</code>. Evictions are not performed unless system daemons consume more than <code class="literal">3.1Gi</code> of memory.
					</p><p>
						If system daemons do not use up all their reservation, with the above example, pods would face memcg OOM kills from their bounding cgroup before node evictions kick in. To better enforce QoS under this situation, the node applies the hard eviction thresholds to the top-level cgroup for all pods to be <code class="literal">Node Allocatable + Eviction Hard Thresholds</code>.
					</p><p>
						If system daemons do not use up all their reservation, the node will evict pods whenever they consume more than <code class="literal">28.9Gi</code> of memory. If eviction does not occur in time, a pod will be OOM killed if pods consume <code class="literal">29Gi</code> of memory.
					</p></section><section class="section" id="allocate-scheduler-policy_nodes-nodes-resources-configuring"><div class="titlepage"><div><div><h4 class="title">6.10.1.4. How the scheduler determines resource availability</h4></div></div></div><p>
						The scheduler uses the value of <code class="literal">node.Status.Allocatable</code> instead of <code class="literal">node.Status.Capacity</code> to decide if a node will become a candidate for pod scheduling.
					</p><p>
						By default, the node will report its machine capacity as fully schedulable by the cluster.
					</p></section></section><section class="section" id="nodes-nodes-resources-configuring-auto_nodes-nodes-resources-configuring"><div class="titlepage"><div><div><h3 class="title">6.10.2. Automatically allocating resources for nodes</h3></div></div></div><p>
					OpenShift Container Platform can automatically determine the optimal <code class="literal">system-reserved</code> CPU and memory resources for nodes associated with a specific machine config pool and update the nodes with those values when the nodes start. By default, the <code class="literal">system-reserved</code> CPU is <code class="literal">500m</code> and <code class="literal">system-reserved</code> memory is <code class="literal">1Gi</code>.
				</p><p>
					To automatically determine and allocate the <code class="literal">system-reserved</code> resources on nodes, create a <code class="literal">KubeletConfig</code> custom resource (CR) to set the <code class="literal">autoSizingReserved: true</code> parameter. A script on each node calculates the optimal values for the respective reserved resources based on the installed CPU and memory capacity on each node. The script takes into account that increased capacity requires a corresponding increase in the reserved resources.
				</p><p>
					Automatically determining the optimal <code class="literal">system-reserved</code> settings ensures that your cluster is running efficiently and prevents node failure due to resource starvation of system components, such as CRI-O and kubelet, without your needing to manually calculate and update the values.
				</p><p>
					This feature is disabled by default.
				</p><div class="orderedlist"><p class="title"><strong>Prerequisites</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the label associated with the static <code class="literal">MachineConfigPool</code> object for the type of node you want to configure by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc edit machineconfigpool &lt;name&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc edit machineconfigpool worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: "2022-11-16T15:34:25Z"
  generation: 4
  labels:
    pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO126-1"><!--Empty--></span><span class="callout">1</span>
  name: worker
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO126-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The label appears under <code class="literal">Labels</code>.
								</div></dd></dl></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							If an appropriate label is not present, add a key/value pair such as:
						</p><pre class="screen">$ oc label machineconfigpool worker custom-kubelet=small-pods</pre></div></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a custom resource (CR) for your configuration change:
						</p><div class="formalpara"><p class="title"><strong>Sample configuration for a resource allocation CR</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: dynamic-node <span id="CO127-1"><!--Empty--></span><span class="callout">1</span>
spec:
  autoSizingReserved: true <span id="CO127-2"><!--Empty--></span><span class="callout">2</span>
  machineConfigPoolSelector:
    matchLabels:
      pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO127-3"><!--Empty--></span><span class="callout">3</span>
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO127-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Assign a name to CR.
								</div></dd><dt><a href="#CO127-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Add the <code class="literal">autoSizingReserved</code> parameter set to <code class="literal">true</code> to allow OpenShift Container Platform to automatically determine and allocate the <code class="literal">system-reserved</code> resources on the nodes associated with the specified label. To disable automatic allocation on those nodes, set this parameter to <code class="literal">false</code>.
								</div></dd><dt><a href="#CO127-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify the label from the machine config pool that you configured in the "Prerequisites" section. You can choose any desired labels for the machine config pool, such as <code class="literal">custom-kubelet: small-pods</code>, or the default label, <code class="literal">pools.operator.machineconfiguration.openshift.io/worker: ""</code>.
								</div></dd></dl></div><p class="simpara">
							The previous example enables automatic resource allocation on all worker nodes. OpenShift Container Platform drains the nodes, applies the kubelet config, and restarts the nodes.
						</p></li><li class="listitem"><p class="simpara">
							Create the CR by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Log in to a node you configured by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Set <code class="literal">/host</code> as the root directory within the debug shell:
						</p><pre class="programlisting language-terminal"># chroot /host</pre></li><li class="listitem"><p class="simpara">
							View the <code class="literal">/etc/node-sizing.env</code> file:
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">SYSTEM_RESERVED_MEMORY=3Gi
SYSTEM_RESERVED_CPU=0.08</pre>

							</p></div><p class="simpara">
							The kubelet uses the <code class="literal">system-reserved</code> values in the <code class="literal">/etc/node-sizing.env</code> file. In the previous example, the worker nodes are allocated <code class="literal">0.08</code> CPU and 3 Gi of memory. It can take several minutes for the optimal values to appear.
						</p></li></ol></div></section><section class="section" id="nodes-nodes-resources-configuring-setting_nodes-nodes-resources-configuring"><div class="titlepage"><div><div><h3 class="title">6.10.3. Manually allocating resources for nodes</h3></div></div></div><p>
					OpenShift Container Platform supports the CPU and memory resource types for allocation. The <code class="literal">ephemeral-resource</code> resource type is also supported. For the <code class="literal">cpu</code> type, you specify the resource quantity in units of cores, such as <code class="literal">200m</code>, <code class="literal">0.5</code>, or <code class="literal">1</code>. For <code class="literal">memory</code> and <code class="literal">ephemeral-storage</code>, you specify the resource quantity in units of bytes, such as <code class="literal">200Ki</code>, <code class="literal">50Mi</code>, or <code class="literal">5Gi</code>. By default, the <code class="literal">system-reserved</code> CPU is <code class="literal">500m</code> and <code class="literal">system-reserved</code> memory is <code class="literal">1Gi</code>.
				</p><p>
					As an administrator, you can set these values by using a kubelet config custom resource (CR) through a set of <code class="literal">&lt;resource_type&gt;=&lt;resource_quantity&gt;</code> pairs (e.g., <code class="literal">cpu=200m,memory=512Mi</code>).
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						You must use a kubelet config CR to manually set resource values. You cannot use a machine config CR.
					</p></div></div><p>
					For details on the recommended <code class="literal">system-reserved</code> values, refer to the <a class="link" href="https://access.redhat.com/solutions/5843241">recommended system-reserved values</a>.
				</p><div class="orderedlist"><p class="title"><strong>Prerequisites</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the label associated with the static <code class="literal">MachineConfigPool</code> CRD for the type of node you want to configure by entering the following command:
						</p><pre class="programlisting language-terminal">$ oc edit machineconfigpool &lt;name&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc edit machineconfigpool worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: "2022-11-16T15:34:25Z"
  generation: 4
  labels:
    pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO128-1"><!--Empty--></span><span class="callout">1</span>
  name: worker
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO128-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The label appears under Labels.
								</div></dd></dl></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							If the label is not present, add a key/value pair such as:
						</p><pre class="screen">$ oc label machineconfigpool worker custom-kubelet=small-pods</pre></div></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a custom resource (CR) for your configuration change.
						</p><div class="formalpara"><p class="title"><strong>Sample configuration for a resource allocation CR</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-allocatable <span id="CO129-1"><!--Empty--></span><span class="callout">1</span>
spec:
  machineConfigPoolSelector:
    matchLabels:
      pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO129-2"><!--Empty--></span><span class="callout">2</span>
  kubeletConfig:
    systemReserved: <span id="CO129-3"><!--Empty--></span><span class="callout">3</span>
      cpu: 1000m
      memory: 1Gi
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO129-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Assign a name to CR.
								</div></dd><dt><a href="#CO129-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the label from the machine config pool.
								</div></dd><dt><a href="#CO129-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify the resources to reserve for the node components and system components.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Run the following command to create the CR:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div></section></section><section class="section" id="nodes-nodes-resources-cpus"><div class="titlepage"><div><div><h2 class="title">6.11. Allocating specific CPUs for nodes in a cluster</h2></div></div></div><p>
				When using the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#using-cpu-manager-and-topology-manager">static CPU Manager policy</a>, you can reserve specific CPUs for use by specific nodes in your cluster. For example, on a system with 24 CPUs, you could reserve CPUs numbered 0 - 3 for the control plane allowing the compute nodes to use CPUs 4 - 23.
			</p><section class="section" id="nodes-nodes-resources-cpus-reserve_nodes-nodes-resources-cpus"><div class="titlepage"><div><div><h3 class="title">6.11.1. Reserving CPUs for nodes</h3></div></div></div><p>
					To explicitly define a list of CPUs that are reserved for specific nodes, create a <code class="literal">KubeletConfig</code> custom resource (CR) to define the <code class="literal">reservedSystemCPUs</code> parameter. This list supersedes the CPUs that might be reserved using the <code class="literal">systemReserved</code> and <code class="literal">kubeReserved</code> parameters.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the label associated with the machine config pool (MCP) for the type of node you want to configure:
						</p><pre class="programlisting language-terminal">$ oc describe machineconfigpool &lt;name&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc describe machineconfigpool worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">Name:         worker
Namespace:
Labels:       machineconfiguration.openshift.io/mco-built-in=
              pools.operator.machineconfiguration.openshift.io/worker= <span id="CO130-1"><!--Empty--></span><span class="callout">1</span>
Annotations:  &lt;none&gt;
API Version:  machineconfiguration.openshift.io/v1
Kind:         MachineConfigPool
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO130-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Get the MCP label.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create a YAML file for the <code class="literal">KubeletConfig</code> CR:
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-reserved-cpus <span id="CO131-1"><!--Empty--></span><span class="callout">1</span>
spec:
  kubeletConfig:
    reservedSystemCPUs: "0,1,2,3" <span id="CO131-2"><!--Empty--></span><span class="callout">2</span>
  machineConfigPoolSelector:
    matchLabels:
      pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO131-3"><!--Empty--></span><span class="callout">3</span>
#...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO131-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify a name for the CR.
								</div></dd><dt><a href="#CO131-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the core IDs of the CPUs you want to reserve for the nodes associated with the MCP.
								</div></dd><dt><a href="#CO131-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify the label from the MCP.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the CR object:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For more information on the <code class="literal">systemReserved</code> and <code class="literal">kubeReserved</code> parameters, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-resources-configuring-about_nodes-nodes-resources-configuring">Allocating resources for nodes in an OpenShift Container Platform cluster</a>.
						</li></ul></div></section></section><section class="section" id="nodes-nodes-tls"><div class="titlepage"><div><div><h2 class="title">6.12. Enabling TLS security profiles for the kubelet</h2></div></div></div><p>
				You can use a TLS (Transport Layer Security) security profile to define which TLS ciphers are required by the kubelet when it is acting as an HTTP server. The kubelet uses its HTTP/GRPC server to communicate with the Kubernetes API server, which sends commands to pods, gathers logs, and run exec commands on pods through the kubelet.
			</p><p>
				A TLS security profile defines the TLS ciphers that the Kubernetes API server must use when connecting with the kubelet to protect communication between the kubelet and the Kubernetes API server.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					By default, when the kubelet acts as a client with the Kubernetes API server, it automatically negotiates the TLS parameters with the API server.
				</p></div></div><section class="section" id="tls-profiles-understanding_nodes-nodes-tls"><div class="titlepage"><div><div><h3 class="title">6.12.1. Understanding TLS security profiles</h3></div></div></div><p>
					You can use a TLS (Transport Layer Security) security profile to define which TLS ciphers are required by various OpenShift Container Platform components. The OpenShift Container Platform TLS security profiles are based on <a class="link" href="https://wiki.mozilla.org/Security/Server_Side_TLS">Mozilla recommended configurations</a>.
				</p><p>
					You can specify one of the following TLS security profiles for each component:
				</p><div class="table" id="idm140232228142528"><p class="title"><strong>Table 6.4. TLS security profiles</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 67%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232228137696" scope="col">Profile</th><th align="left" valign="top" id="idm140232228136608" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232228137696"> <p>
									<code class="literal">Old</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228136608"> <p>
									This profile is intended for use with legacy clients or libraries. The profile is based on the <a class="link" href="https://wiki.mozilla.org/Security/Server_Side_TLS#Old_backward_compatibility">Old backward compatibility</a> recommended configuration.
								</p>
								 <p>
									The <code class="literal">Old</code> profile requires a minimum TLS version of 1.0.
								</p>
								 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
										For the Ingress Controller, the minimum TLS version is converted from 1.0 to 1.1.
									</p></div></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228137696"> <p>
									<code class="literal">Intermediate</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228136608"> <p>
									This profile is the recommended configuration for the majority of clients. It is the default TLS security profile for the Ingress Controller, kubelet, and control plane. The profile is based on the <a class="link" href="https://wiki.mozilla.org/Security/Server_Side_TLS#Intermediate_compatibility_.28recommended.29">Intermediate compatibility</a> recommended configuration.
								</p>
								 <p>
									The <code class="literal">Intermediate</code> profile requires a minimum TLS version of 1.2.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228137696"> <p>
									<code class="literal">Modern</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228136608"> <p>
									This profile is intended for use with modern clients that have no need for backwards compatibility. This profile is based on the <a class="link" href="https://wiki.mozilla.org/Security/Server_Side_TLS#Modern_compatibility">Modern compatibility</a> recommended configuration.
								</p>
								 <p>
									The <code class="literal">Modern</code> profile requires a minimum TLS version of 1.3.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228137696"> <p>
									<code class="literal">Custom</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228136608"> <p>
									This profile allows you to define the TLS version and ciphers to use.
								</p>
								 <div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
										Use caution when using a <code class="literal">Custom</code> profile, because invalid configurations can cause problems.
									</p></div></div>
								 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When using one of the predefined profile types, the effective profile configuration is subject to change between releases. For example, given a specification to use the Intermediate profile deployed on release X.Y.Z, an upgrade to release X.Y.Z+1 might cause a new profile configuration to be applied, resulting in a rollout.
					</p></div></div></section><section class="section" id="tls-profiles-kubelet-configuring_nodes-nodes-tls"><div class="titlepage"><div><div><h3 class="title">6.12.2. Configuring the TLS security profile for the kubelet</h3></div></div></div><p>
					To configure a TLS security profile for the kubelet when it is acting as an HTTP server, create a <code class="literal">KubeletConfig</code> custom resource (CR) to specify a predefined or custom TLS security profile for specific nodes. If a TLS security profile is not configured, the default TLS security profile is <code class="literal">Intermediate</code>.
				</p><div class="formalpara"><p class="title"><strong>Sample <code class="literal">KubeletConfig</code> CR that configures the <code class="literal">Old</code> TLS security profile on worker nodes</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: KubeletConfig
 ...
spec:
  tlsSecurityProfile:
    old: {}
    type: Old
  machineConfigPoolSelector:
    matchLabels:
      pools.operator.machineconfiguration.openshift.io/worker: ""
#...</pre>

					</p></div><p>
					You can see the ciphers and the minimum TLS version of the configured TLS security profile in the <code class="literal">kubelet.conf</code> file on a configured node.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">KubeletConfig</code> CR to configure the TLS security profile:
						</p><div class="formalpara"><p class="title"><strong>Sample <code class="literal">KubeletConfig</code> CR for a <code class="literal">Custom</code> profile</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-kubelet-tls-security-profile
spec:
  tlsSecurityProfile:
    type: Custom <span id="CO132-1"><!--Empty--></span><span class="callout">1</span>
    custom: <span id="CO132-2"><!--Empty--></span><span class="callout">2</span>
      ciphers: <span id="CO132-3"><!--Empty--></span><span class="callout">3</span>
      - ECDHE-ECDSA-CHACHA20-POLY1305
      - ECDHE-RSA-CHACHA20-POLY1305
      - ECDHE-RSA-AES128-GCM-SHA256
      - ECDHE-ECDSA-AES128-GCM-SHA256
      minTLSVersion: VersionTLS11
  machineConfigPoolSelector:
    matchLabels:
      pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO132-4"><!--Empty--></span><span class="callout">4</span>
#...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO132-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the TLS security profile type (<code class="literal">Old</code>, <code class="literal">Intermediate</code>, or <code class="literal">Custom</code>). The default is <code class="literal">Intermediate</code>.
								</div></dd><dt><a href="#CO132-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the appropriate field for the selected type:
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">old: {}</code>
										</li><li class="listitem">
											<code class="literal">intermediate: {}</code>
										</li><li class="listitem">
											<code class="literal">custom:</code>
										</li></ul></div></dd><dt><a href="#CO132-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									For the <code class="literal">custom</code> type, specify a list of TLS ciphers and minimum accepted TLS version.
								</div></dd><dt><a href="#CO132-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Optional: Specify the machine config pool label for the nodes you want to apply the TLS security profile.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">KubeletConfig</code> object:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;filename&gt;</pre><p class="simpara">
							Depending on the number of worker nodes in the cluster, wait for the configured nodes to be rebooted one by one.
						</p></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						To verify that the profile is set, perform the following steps after the nodes are in the <code class="literal">Ready</code> state:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Start a debug session for a configured node:
						</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Set <code class="literal">/host</code> as the root directory within the debug shell:
						</p><pre class="programlisting language-terminal">sh-4.4# chroot /host</pre></li><li class="listitem"><p class="simpara">
							View the <code class="literal">kubelet.conf</code> file:
						</p><pre class="programlisting language-terminal">sh-4.4# cat /etc/kubernetes/kubelet.conf</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">  "kind": "KubeletConfiguration",
  "apiVersion": "kubelet.config.k8s.io/v1beta1",
#...
  "tlsCipherSuites": [
    "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256",
    "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256",
    "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384",
    "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384",
    "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256",
    "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256"
  ],
  "tlsMinVersion": "VersionTLS12",
#...</pre>

							</p></div></li></ol></div></section></section><section class="section" id="machine-config-daemon-metrics"><div class="titlepage"><div><div><h2 class="title">6.13. Machine Config Daemon metrics</h2></div></div></div><p>
				The Machine Config Daemon is a part of the Machine Config Operator. It runs on every node in the cluster. The Machine Config Daemon manages configuration changes and updates on each of the nodes.
			</p><section class="section" id="machine-config-daemon-metrics_machine-config-operator"><div class="titlepage"><div><div><h3 class="title">6.13.1. Machine Config Daemon metrics</h3></div></div></div><p>
					Beginning with OpenShift Container Platform 4.3, the Machine Config Daemon provides a set of metrics. These metrics can be accessed using the Prometheus Cluster Monitoring stack.
				</p><p>
					The following table describes this set of metrics. Some entries contain commands for getting specific logs. Hpwever, the most comprehensive set of logs is available using the <code class="literal">oc adm must-gather</code> command.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Metrics marked with <code class="literal">*</code> in the <span class="strong strong"><strong>Name</strong></span> and <span class="strong strong"><strong>Description</strong></span> columns represent serious errors that might cause performance problems. Such problems might prevent updates and upgrades from proceeding.
					</p></div></div><div class="table" id="idm140232228815728"><p class="title"><strong>Table 6.5. MCO metrics</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 17%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col><col style="width: 33%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232228809040" scope="col">Name</th><th align="left" valign="top" id="idm140232228807952" scope="col">Format</th><th align="left" valign="top" id="idm140232228806864" scope="col">Description</th><th align="left" valign="top" id="idm140232228805776" scope="col">Notes</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232228809040"> <p>
									<code class="literal">mcd_host_os_and_version</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228807952"> <p>
									<code class="literal">[]string{"os", "version"}</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228806864"> <p>
									Shows the OS that MCD is running on, such as RHCOS or RHEL. In case of RHCOS, the version is provided.
								</p>
								 </td><td align="left" valign="top" headers="idm140232228805776"> </td></tr><tr><td align="left" valign="top" headers="idm140232228809040"> <p>
									<code class="literal">mcd_drain_err*</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228807952"> </td><td align="left" valign="top" headers="idm140232228806864"> <p>
									Logs errors received during failed drain. *
								</p>
								 </td><td align="left" valign="top" headers="idm140232228805776"> <p>
									While drains might need multiple tries to succeed, terminal failed drains prevent updates from proceeding. The <code class="literal">drain_time</code> metric, which shows how much time the drain took, might help with troubleshooting.
								</p>
								 <p>
									For further investigation, see the logs by running:
								</p>
								 <p>
									<code class="literal">$ oc logs -f -n openshift-machine-config-operator machine-config-daemon-&lt;hash&gt; -c machine-config-daemon</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228809040"> <p>
									<code class="literal">mcd_pivot_err*</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228807952"> <p>
									<code class="literal">[]string{"err", "node", "pivot_target"}</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228806864"> <p>
									Logs errors encountered during pivot. *
								</p>
								 </td><td align="left" valign="top" headers="idm140232228805776"> <p>
									Pivot errors might prevent OS upgrades from proceeding.
								</p>
								 <p>
									For further investigation, run this command to see the logs from the <code class="literal">machine-config-daemon</code> container:
								</p>
								 <p>
									<code class="literal">$ oc logs -f -n openshift-machine-config-operator machine-config-daemon-&lt;hash&gt; -c machine-config-daemon</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228809040"> <p>
									<code class="literal">mcd_state</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228807952"> <p>
									<code class="literal">[]string{"state", "reason"}</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228806864"> <p>
									State of Machine Config Daemon for the indicated node. Possible states are "Done", "Working", and "Degraded". In case of "Degraded", the reason is included.
								</p>
								 </td><td align="left" valign="top" headers="idm140232228805776"> <p>
									For further investigation, see the logs by running:
								</p>
								 <p>
									<code class="literal">$ oc logs -f -n openshift-machine-config-operator machine-config-daemon-&lt;hash&gt; -c machine-config-daemon</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228809040"> <p>
									<code class="literal">mcd_kubelet_state*</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228807952"> </td><td align="left" valign="top" headers="idm140232228806864"> <p>
									Logs kubelet health failures. *
								</p>
								 </td><td align="left" valign="top" headers="idm140232228805776"> <p>
									This is expected to be empty, with failure count of 0. If failure count exceeds 2, the error indicating threshold is exceeded. This indicates a possible issue with the health of the kubelet.
								</p>
								 <p>
									For further investigation, run this command to access the node and see all its logs:
								</p>
								 <p>
									<code class="literal">$ oc debug node/&lt;node&gt; — chroot /host journalctl -u kubelet</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228809040"> <p>
									<code class="literal">mcd_reboot_err*</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228807952"> <p>
									<code class="literal">[]string{"message", "err", "node"}</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228806864"> <p>
									Logs the failed reboots and the corresponding errors. *
								</p>
								 </td><td align="left" valign="top" headers="idm140232228805776"> <p>
									This is expected to be empty, which indicates a successful reboot.
								</p>
								 <p>
									For further investigation, see the logs by running:
								</p>
								 <p>
									<code class="literal">$ oc logs -f -n openshift-machine-config-operator machine-config-daemon-&lt;hash&gt; -c machine-config-daemon</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228809040"> <p>
									<code class="literal">mcd_update_state</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228807952"> <p>
									<code class="literal">[]string{"config", "err"}</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228806864"> <p>
									Logs success or failure of configuration updates and the corresponding errors.
								</p>
								 </td><td align="left" valign="top" headers="idm140232228805776"> <p>
									The expected value is <code class="literal">rendered-master/rendered-worker-XXXX</code>. If the update fails, an error is present.
								</p>
								 <p>
									For further investigation, see the logs by running:
								</p>
								 <p>
									<code class="literal">$ oc logs -f -n openshift-machine-config-operator machine-config-daemon-&lt;hash&gt; -c machine-config-daemon</code>
								</p>
								 </td></tr></tbody></table></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/monitoring/#monitoring-overview">Monitoring overview</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#gathering-cluster-data">Gathering data about your cluster</a>
						</li></ul></div></section></section><section class="section" id="nodes-nodes-creating-infrastructure-nodes"><div class="titlepage"><div><div><h2 class="title">6.14. Creating infrastructure nodes</h2></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.
				</p><p>
					Clusters with the infrastructure platform type <code class="literal">none</code> cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.
				</p><p>
					To view the platform type for your cluster, run the following command:
				</p><pre class="programlisting language-terminal">$ oc get infrastructure cluster -o jsonpath='{.status.platform}'</pre></div></div><p>
				You can use infrastructure machine sets to create machines that host only infrastructure components, such as the default router, the integrated container image registry, and the components for cluster metrics and monitoring. These infrastructure machines are not counted toward the total number of subscriptions that are required to run the environment.
			</p><p>
				In a production deployment, it is recommended that you deploy at least three machine sets to hold infrastructure components. Both OpenShift Logging and Red Hat OpenShift Service Mesh deploy Elasticsearch, which requires three instances to be installed on different nodes. Each of these nodes can be deployed to different availability zones for high availability. This configuration requires three different machine sets, one for each availability zone. In global Azure regions that do not have multiple availability zones, you can use availability sets to ensure high availability.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					After adding the <code class="literal">NoSchedule</code> taint on the infrastructure node, existing DNS pods running on that node are marked as <code class="literal">misscheduled</code>. You must either delete or <a class="link" href="https://access.redhat.com/solutions/6592171">add toleration on <code class="literal">misscheduled</code> DNS pods</a>.
				</p></div></div><section class="section" id="infrastructure-components_creating-infrastructure-nodes"><div class="titlepage"><div><div><h3 class="title">6.14.1. OpenShift Container Platform infrastructure components</h3></div></div></div><p>
					The following infrastructure workloads do not incur OpenShift Container Platform worker subscriptions:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Kubernetes and OpenShift Container Platform control plane services that run on masters
						</li><li class="listitem">
							The default router
						</li><li class="listitem">
							The integrated container image registry
						</li><li class="listitem">
							The HAProxy-based Ingress Controller
						</li><li class="listitem">
							The cluster metrics collection, or monitoring service, including components for monitoring user-defined projects
						</li><li class="listitem">
							Cluster aggregated logging
						</li><li class="listitem">
							Service brokers
						</li><li class="listitem">
							Red Hat Quay
						</li><li class="listitem">
							Red Hat OpenShift Data Foundation
						</li><li class="listitem">
							Red Hat Advanced Cluster Manager
						</li><li class="listitem">
							Red Hat Advanced Cluster Security for Kubernetes
						</li><li class="listitem">
							Red Hat OpenShift GitOps
						</li><li class="listitem">
							Red Hat OpenShift Pipelines
						</li></ul></div><p>
					Any node that runs any other container, pod, or component is a worker node that your subscription must cover.
				</p><p>
					For information about infrastructure nodes and which components can run on infrastructure nodes, see the "Red Hat OpenShift control plane and infrastructure nodes" section in the <a class="link" href="https://www.redhat.com/en/resources/openshift-subscription-sizing-guide">OpenShift sizing and subscription guide for enterprise Kubernetes</a> document.
				</p><p>
					To create an infrastructure node, you can <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#machineset-creating_creating-infrastructure-machinesets">use a machine set</a>, <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#creating-an-infra-node_creating-infrastructure-nodes">label the node</a>, or <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#creating-infra-machines_creating-infrastructure-machinesets">use a machine config pool</a>.
				</p><section class="section" id="creating-an-infra-node_creating-infrastructure-nodes"><div class="titlepage"><div><div><h4 class="title">6.14.1.1. Creating an infrastructure node</h4></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							See Creating infrastructure machine sets for installer-provisioned infrastructure environments or for any cluster where the control plane nodes are managed by the machine API.
						</p></div></div><p>
						Requirements of the cluster dictate that infrastructure, also called <code class="literal">infra</code> nodes, be provisioned. The installer only provides provisions for control plane and worker nodes. Worker nodes can be designated as infrastructure nodes or application, also called <code class="literal">app</code>, nodes through labeling.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Add a label to the worker node that you want to act as application node:
							</p><pre class="programlisting language-terminal">$ oc label node &lt;node-name&gt; node-role.kubernetes.io/app=""</pre></li><li class="listitem"><p class="simpara">
								Add a label to the worker nodes that you want to act as infrastructure nodes:
							</p><pre class="programlisting language-terminal">$ oc label node &lt;node-name&gt; node-role.kubernetes.io/infra=""</pre></li><li class="listitem"><p class="simpara">
								Check to see if applicable nodes now have the <code class="literal">infra</code> role and <code class="literal">app</code> roles:
							</p><pre class="programlisting language-terminal">$ oc get nodes</pre></li><li class="listitem"><p class="simpara">
								Create a default cluster-wide node selector. The default node selector is applied to pods created in all namespaces. This creates an intersection with any existing node selectors on a pod, which additionally constrains the pod’s selector.
							</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									If the default node selector key conflicts with the key of a pod’s label, then the default node selector is not applied.
								</p><p>
									However, do not set a default node selector that might cause a pod to become unschedulable. For example, setting the default node selector to a specific node role, such as <code class="literal">node-role.kubernetes.io/infra=""</code>, when a pod’s label is set to a different node role, such as <code class="literal">node-role.kubernetes.io/master=""</code>, can cause the pod to become unschedulable. For this reason, use caution when setting the default node selector to specific node roles.
								</p><p>
									You can alternatively use a project node selector to avoid cluster-wide node selector key conflicts.
								</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Edit the <code class="literal">Scheduler</code> object:
									</p><pre class="programlisting language-terminal">$ oc edit scheduler cluster</pre></li><li class="listitem"><p class="simpara">
										Add the <code class="literal">defaultNodeSelector</code> field with the appropriate node selector:
									</p><pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  name: cluster
spec:
  defaultNodeSelector: topology.kubernetes.io/region=us-east-1 <span id="CO133-1"><!--Empty--></span><span class="callout">1</span>
# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO133-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												This example node selector deploys pods on nodes in the <code class="literal">us-east-1</code> region by default.
											</div></dd></dl></div></li><li class="listitem">
										Save the file to apply the changes.
									</li></ol></div></li></ol></div><p>
						You can now move infrastructure resources to the newly labeled <code class="literal">infra</code> nodes.
					</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#moving-resources-to-infrastructure-machinesets">Moving resources to infrastructure machine sets</a>
							</li></ul></div></section></section></section></section><section class="chapter" id="working-with-containers"><div class="titlepage"><div><div><h1 class="title">Chapter 7. Working with containers</h1></div></div></div><section class="section" id="nodes-containers-using"><div class="titlepage"><div><div><h2 class="title">7.1. Understanding Containers</h2></div></div></div><p>
				The basic units of OpenShift Container Platform applications are called <span class="emphasis"><em>containers</em></span>. <a class="link" href="https://www.redhat.com/en/topics/containers#overview">Linux container technologies</a> are lightweight mechanisms for isolating running processes so that they are limited to interacting with only their designated resources.
			</p><p>
				Many application instances can be running in containers on a single host without visibility into each others' processes, files, network, and so on. Typically, each container provides a single service (often called a "micro-service"), such as a web server or a database, though containers can be used for arbitrary workloads.
			</p><p>
				The Linux kernel has been incorporating capabilities for container technologies for years. OpenShift Container Platform and Kubernetes add the ability to orchestrate containers across multi-host installations.
			</p><section class="section" id="nodes-containers-memory"><div class="titlepage"><div><div><h3 class="title">7.1.1. About containers and RHEL kernel memory</h3></div></div></div><p>
					Due to Red Hat Enterprise Linux (RHEL) behavior, a container on a node with high CPU usage might seem to consume more memory than expected. The higher memory consumption could be caused by the <code class="literal">kmem_cache</code> in the RHEL kernel. The RHEL kernel creates a <code class="literal">kmem_cache</code> for each cgroup. For added performance, the <code class="literal">kmem_cache</code> contains a <code class="literal">cpu_cache</code>, and a node cache for any NUMA nodes. These caches all consume kernel memory.
				</p><p>
					The amount of memory stored in those caches is proportional to the number of CPUs that the system uses. As a result, a higher number of CPUs results in a greater amount of kernel memory being held in these caches. Higher amounts of kernel memory in these caches can cause OpenShift Container Platform containers to exceed the configured memory limits, resulting in the container being killed.
				</p><p>
					To avoid losing containers due to kernel memory issues, ensure that the containers request sufficient memory. You can use the following formula to estimate the amount of memory consumed by the <code class="literal">kmem_cache</code>, where <code class="literal">nproc</code> is the number of processing units available that are reported by the <code class="literal">nproc</code> command. The lower limit of container requests should be this value plus the container memory requirements:
				</p><pre class="programlisting language-terminal">$(nproc) X 1/2 MiB</pre></section><section class="section" id="nodes-containers-runtimes"><div class="titlepage"><div><div><h3 class="title">7.1.2. About the container engine and container runtime</h3></div></div></div><p>
					A <span class="emphasis"><em>container engine</em></span> is a piece of software that processes user requests, including command line options and image pulls. The container engine uses a <span class="emphasis"><em>container runtime</em></span>, also called a <span class="emphasis"><em>lower-level container runtime</em></span>, to run and manage the components required to deploy and operate containers. You likely will not need to interact with the container engine or container runtime.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The OpenShift Container Platform documentation uses the term <span class="emphasis"><em>container runtime</em></span> to refer to the lower-level container runtime. Other documentation can refer to the container engine as the container runtime.
					</p></div></div><p>
					OpenShift Container Platform uses CRI-O as the container engine and runC or crun as the container runtime. The default container runtime is runC. Both container runtimes adhere to the <a class="link" href="https://www.opencontainers.org/">Open Container Initiative (OCI)</a> runtime specifications.
				</p><p>
					CRI-O is a Kubernetes-native container engine implementation that integrates closely with the operating system to deliver an efficient and optimized Kubernetes experience. The CRI-O container engine runs as a systemd service on each OpenShift Container Platform cluster node.
				</p><p>
					runC, developed by Docker and maintained by the Open Container Project, is a lightweight, portable container runtime written in Go. crun, developed by Red Hat, is a fast and low-memory container runtime fully written in C. As of OpenShift Container Platform 4.13, you can select between the two.
				</p><p>
					crun has several improvements over runC, including:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Smaller binary
						</li><li class="listitem">
							Quicker processing
						</li><li class="listitem">
							Lower memory footprint
						</li></ul></div><p>
					runC has some benefits over crun, including:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Most popular OCI container runtime.
						</li><li class="listitem">
							Longer tenure in production.
						</li><li class="listitem">
							Default container runtime of CRI-O.
						</li></ul></div><p>
					You can move between the two container runtimes as needed.
				</p><p>
					For information on setting which container runtime to use, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#create-a-containerruntimeconfig_post-install-machine-configuration-tasks">Creating a <code class="literal">ContainerRuntimeConfig</code> CR to edit CRI-O parameters</a>.
				</p></section></section><section class="section" id="nodes-containers-init"><div class="titlepage"><div><div><h2 class="title">7.2. Using Init Containers to perform tasks before a pod is deployed</h2></div></div></div><p>
				OpenShift Container Platform provides <span class="emphasis"><em>init containers</em></span>, which are specialized containers that run before application containers and can contain utilities or setup scripts not present in an app image.
			</p><section class="section" id="nodes-containers-init-about_nodes-containers-init"><div class="titlepage"><div><div><h3 class="title">7.2.1. Understanding Init Containers</h3></div></div></div><p>
					You can use an Init Container resource to perform tasks before the rest of a pod is deployed.
				</p><p>
					A pod can have Init Containers in addition to application containers. Init containers allow you to reorganize setup scripts and binding code.
				</p><p>
					An Init Container can:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Contain and run utilities that are not desirable to include in the app Container image for security reasons.
						</li><li class="listitem">
							Contain utilities or custom code for setup that is not present in an app image. For example, there is no requirement to make an image FROM another image just to use a tool like sed, awk, python, or dig during setup.
						</li><li class="listitem">
							Use Linux namespaces so that they have different filesystem views from app containers, such as access to secrets that application containers are not able to access.
						</li></ul></div><p>
					Each Init Container must complete successfully before the next one is started. So, Init Containers provide an easy way to block or delay the startup of app containers until some set of preconditions are met.
				</p><p>
					For example, the following are some ways you can use Init Containers:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Wait for a service to be created with a shell command like:
						</p><pre class="programlisting language-terminal">for i in {1..100}; do sleep 1; if dig myservice; then exit 0; fi; done; exit 1</pre></li><li class="listitem"><p class="simpara">
							Register this pod with a remote server from the downward API with a command like:
						</p><pre class="programlisting language-terminal">$ curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d ‘instance=$()&amp;ip=$()’</pre></li><li class="listitem">
							Wait for some time before starting the app Container with a command like <code class="literal">sleep 60</code>.
						</li><li class="listitem">
							Clone a git repository into a volume.
						</li><li class="listitem">
							Place values into a configuration file and run a template tool to dynamically generate a configuration file for the main app Container. For example, place the POD_IP value in a configuration and generate the main app configuration file using Jinja.
						</li></ul></div><p>
					See the <a class="link" href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">Kubernetes documentation</a> for more information.
				</p></section><section class="section" id="nodes-containers-init-creating_nodes-containers-init"><div class="titlepage"><div><div><h3 class="title">7.2.2. Creating Init Containers</h3></div></div></div><p>
					The following example outlines a simple pod which has two Init Containers. The first waits for <code class="literal">myservice</code> and the second waits for <code class="literal">mydb</code>. After both containers complete, the pod begins.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the pod for the Init Container:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: registry.access.redhat.com/ubi9/ubi:latest
    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']
  initContainers:
  - name: init-myservice
    image: registry.access.redhat.com/ubi9/ubi:latest
    command: ['sh', '-c', 'until getent hosts myservice; do echo waiting for myservice; sleep 2; done;']
  - name: init-mydb
    image: registry.access.redhat.com/ubi9/ubi:latest
    command: ['sh', '-c', 'until getent hosts mydb; do echo waiting for mydb; sleep 2; done;']
# ...</pre></li><li class="listitem"><p class="simpara">
									Create the pod:
								</p><pre class="programlisting language-terminal">$ oc create -f myapp.yaml</pre></li><li class="listitem"><p class="simpara">
									View the status of the pod:
								</p><pre class="programlisting language-terminal">$ oc get pods</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                          READY     STATUS              RESTARTS   AGE
myapp-pod                     0/1       Init:0/2            0          5s</pre>

									</p></div><p class="simpara">
									The pod status, <code class="literal">Init:0/2</code>, indicates it is waiting for the two services.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">myservice</code> service.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file similar to the following:
								</p><pre class="programlisting language-yaml">kind: Service
apiVersion: v1
metadata:
  name: myservice
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376</pre></li><li class="listitem"><p class="simpara">
									Create the pod:
								</p><pre class="programlisting language-terminal">$ oc create -f myservice.yaml</pre></li><li class="listitem"><p class="simpara">
									View the status of the pod:
								</p><pre class="programlisting language-terminal">$ oc get pods</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                          READY     STATUS              RESTARTS   AGE
myapp-pod                     0/1       Init:1/2            0          5s</pre>

									</p></div><p class="simpara">
									The pod status, <code class="literal">Init:1/2</code>, indicates it is waiting for one service, in this case the <code class="literal">mydb</code> service.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">mydb</code> service:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file similar to the following:
								</p><pre class="programlisting language-yaml">kind: Service
apiVersion: v1
metadata:
  name: mydb
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9377</pre></li><li class="listitem"><p class="simpara">
									Create the pod:
								</p><pre class="programlisting language-terminal">$ oc create -f mydb.yaml</pre></li><li class="listitem"><p class="simpara">
									View the status of the pod:
								</p><pre class="programlisting language-terminal">$ oc get pods</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                          READY     STATUS              RESTARTS   AGE
myapp-pod                     1/1       Running             0          2m</pre>

									</p></div><p class="simpara">
									The pod status indicated that it is no longer waiting for the services and is running.
								</p></li></ol></div></li></ol></div></section></section><section class="section" id="nodes-containers-volumes"><div class="titlepage"><div><div><h2 class="title">7.3. Using volumes to persist container data</h2></div></div></div><p>
				Files in a container are ephemeral. As such, when a container crashes or stops, the data is lost. You can use <span class="emphasis"><em>volumes</em></span> to persist the data used by the containers in a pod. A volume is directory, accessible to the Containers in a pod, where data is stored for the life of the pod.
			</p><section class="section" id="nodes-containers-volumes-about_nodes-containers-volumes"><div class="titlepage"><div><div><h3 class="title">7.3.1. Understanding volumes</h3></div></div></div><p>
					Volumes are mounted file systems available to pods and their containers which may be backed by a number of host-local or network attached storage endpoints. Containers are not persistent by default; on restart, their contents are cleared.
				</p><p>
					To ensure that the file system on the volume contains no errors and, if errors are present, to repair them when possible, OpenShift Container Platform invokes the <code class="literal">fsck</code> utility prior to the <code class="literal">mount</code> utility. This occurs when either adding a volume or updating an existing volume.
				</p><p>
					The simplest volume type is <code class="literal">emptyDir</code>, which is a temporary directory on a single machine. Administrators may also allow you to request a persistent volume that is automatically attached to your pods.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						<code class="literal">emptyDir</code> volume storage may be restricted by a quota based on the pod’s FSGroup, if the FSGroup parameter is enabled by your cluster administrator.
					</p></div></div></section><section class="section" id="nodes-containers-volumes-cli_nodes-containers-volumes"><div class="titlepage"><div><div><h3 class="title">7.3.2. Working with volumes using the OpenShift Container Platform CLI</h3></div></div></div><p>
					You can use the CLI command <code class="literal">oc set volume</code> to add and remove volumes and volume mounts for any object that has a pod template like replication controllers or deployment configs. You can also list volumes in pods or any object that has a pod template.
				</p><p>
					The <code class="literal">oc set volume</code> command uses the following general syntax:
				</p><pre class="programlisting language-terminal">$ oc set volume &lt;object_selection&gt; &lt;operation&gt; &lt;mandatory_parameters&gt; &lt;options&gt;</pre><div class="variablelist"><dl class="variablelist"><dt><span class="term">Object selection</span></dt><dd>
								Specify one of the following for the <code class="literal">object_selection</code> parameter in the <code class="literal">oc set volume</code> command:
							</dd></dl></div><div class="table" id="vol-object-selection_nodes-containers-volumes"><p class="title"><strong>Table 7.1. Object Selection</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232229615296" scope="col">Syntax</th><th align="left" valign="top" id="idm140232229614208" scope="col">Description</th><th align="left" valign="top" id="idm140232229613120" scope="col">Example</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232229615296"> <p>
									<code class="literal"><span class="emphasis"><em>&lt;object_type&gt;</em></span> <span class="emphasis"><em>&lt;name&gt;</em></span></code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229614208"> <p>
									Selects <code class="literal"><span class="emphasis"><em>&lt;name&gt;</em></span></code> of type <code class="literal"><span class="emphasis"><em>&lt;object_type&gt;</em></span></code>.
								</p>
								 </td><td align="left" valign="top" headers="idm140232229613120"> <p>
									<code class="literal">deploymentConfig registry</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229615296"> <p>
									<code class="literal"><span class="emphasis"><em>&lt;object_type&gt;</em></span>/<span class="emphasis"><em>&lt;name&gt;</em></span></code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229614208"> <p>
									Selects <code class="literal"><span class="emphasis"><em>&lt;name&gt;</em></span></code> of type <code class="literal"><span class="emphasis"><em>&lt;object_type&gt;</em></span></code>.
								</p>
								 </td><td align="left" valign="top" headers="idm140232229613120"> <p>
									<code class="literal">deploymentConfig/registry</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229615296"> <p>
									<code class="literal"><span class="emphasis"><em>&lt;object_type&gt;</em></span></code><code class="literal">--selector=<span class="emphasis"><em>&lt;object_label_selector&gt;</em></span></code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229614208"> <p>
									Selects resources of type <code class="literal"><span class="emphasis"><em>&lt;object_type&gt;</em></span></code> that matched the given label selector.
								</p>
								 </td><td align="left" valign="top" headers="idm140232229613120"> <p>
									<code class="literal">deploymentConfig</code><code class="literal">--selector="name=registry"</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229615296"> <p>
									<code class="literal"><span class="emphasis"><em>&lt;object_type&gt;</em></span> --all</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229614208"> <p>
									Selects all resources of type <code class="literal"><span class="emphasis"><em>&lt;object_type&gt;</em></span></code>.
								</p>
								 </td><td align="left" valign="top" headers="idm140232229613120"> <p>
									<code class="literal">deploymentConfig --all</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229615296"> <p>
									<code class="literal">-f</code> or <code class="literal">--filename=<span class="emphasis"><em>&lt;file_name&gt;</em></span></code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229614208"> <p>
									File name, directory, or URL to file to use to edit the resource.
								</p>
								 </td><td align="left" valign="top" headers="idm140232229613120"> <p>
									<code class="literal">-f registry-deployment-config.json</code>
								</p>
								 </td></tr></tbody></table></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Operation</span></dt><dd>
								Specify <code class="literal">--add</code> or <code class="literal">--remove</code> for the <code class="literal">operation</code> parameter in the <code class="literal">oc set volume</code> command.
							</dd><dt><span class="term">Mandatory parameters</span></dt><dd>
								Any mandatory parameters are specific to the selected operation and are discussed in later sections.
							</dd><dt><span class="term">Options</span></dt><dd>
								Any options are specific to the selected operation and are discussed in later sections.
							</dd></dl></div></section><section class="section" id="nodes-containers-volumes-listing_nodes-containers-volumes"><div class="titlepage"><div><div><h3 class="title">7.3.3. Listing volumes and volume mounts in a pod</h3></div></div></div><p>
					You can list volumes and volume mounts in pods or pod templates:
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To list volumes:
					</p></div><pre class="programlisting language-terminal">$ oc set volume &lt;object_type&gt;/&lt;name&gt; [options]</pre><p>
					List volume supported options:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232227397296" scope="col">Option</th><th align="left" valign="top" id="idm140232227396208" scope="col">Description</th><th align="left" valign="top" id="idm140232227395120" scope="col">Default</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232227397296"> <p>
									<code class="literal">--name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227396208"> <p>
									Name of the volume.
								</p>
								 </td><td align="left" valign="top" headers="idm140232227395120"> </td></tr><tr><td align="left" valign="top" headers="idm140232227397296"> <p>
									<code class="literal">-c, --containers</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227396208"> <p>
									Select containers by name. It can also take wildcard <code class="literal">'*'</code> that matches any character.
								</p>
								 </td><td align="left" valign="top" headers="idm140232227395120"> <p>
									<code class="literal">'*'</code>
								</p>
								 </td></tr></tbody></table></div><p>
					For example:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To list all volumes for pod <span class="strong strong"><strong>p1</strong></span>:
						</p><pre class="programlisting language-terminal">$ oc set volume pod/p1</pre></li><li class="listitem"><p class="simpara">
							To list volume <span class="strong strong"><strong>v1</strong></span> defined on all deployment configs:
						</p><pre class="programlisting language-terminal">$ oc set volume dc --all --name=v1</pre></li></ul></div></section><section class="section" id="nodes-containers-volumes-adding_nodes-containers-volumes"><div class="titlepage"><div><div><h3 class="title">7.3.4. Adding volumes to a pod</h3></div></div></div><p>
					You can add volumes and volume mounts to a pod.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To add a volume, a volume mount, or both to pod templates:
					</p></div><pre class="programlisting language-terminal">$ oc set volume &lt;object_type&gt;/&lt;name&gt; --add [options]</pre><div class="table" id="idm140232227369808"><p class="title"><strong>Table 7.2. Supported Options for Adding Volumes</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232229170528" scope="col">Option</th><th align="left" valign="top" id="idm140232229169440" scope="col">Description</th><th align="left" valign="top" id="idm140232229168352" scope="col">Default</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232229170528"> <p>
									<code class="literal">--name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229169440"> <p>
									Name of the volume.
								</p>
								 </td><td align="left" valign="top" headers="idm140232229168352"> <p>
									Automatically generated, if not specified.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229170528"> <p>
									<code class="literal">-t, --type</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229169440"> <p>
									Name of the volume source. Supported values: <code class="literal">emptyDir</code>, <code class="literal">hostPath</code>, <code class="literal">secret</code>, <code class="literal">configmap</code>, <code class="literal">persistentVolumeClaim</code> or <code class="literal">projected</code>.
								</p>
								 </td><td align="left" valign="top" headers="idm140232229168352"> <p>
									<code class="literal">emptyDir</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229170528"> <p>
									<code class="literal">-c, --containers</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229169440"> <p>
									Select containers by name. It can also take wildcard <code class="literal">'*'</code> that matches any character.
								</p>
								 </td><td align="left" valign="top" headers="idm140232229168352"> <p>
									<code class="literal">'*'</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229170528"> <p>
									<code class="literal">-m, --mount-path</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229169440"> <p>
									Mount path inside the selected containers. Do not mount to the container root, <code class="literal">/</code>, or any path that is the same in the host and the container. This can corrupt your host system if the container is sufficiently privileged, such as the host <code class="literal">/dev/pts</code> files. It is safe to mount the host by using <code class="literal">/host</code>.
								</p>
								 </td><td align="left" valign="top" headers="idm140232229168352"> </td></tr><tr><td align="left" valign="top" headers="idm140232229170528"> <p>
									<code class="literal">--path</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229169440"> <p>
									Host path. Mandatory parameter for <code class="literal">--type=hostPath</code>. Do not mount to the container root, <code class="literal">/</code>, or any path that is the same in the host and the container. This can corrupt your host system if the container is sufficiently privileged, such as the host <code class="literal">/dev/pts</code> files. It is safe to mount the host by using <code class="literal">/host</code>.
								</p>
								 </td><td align="left" valign="top" headers="idm140232229168352"> </td></tr><tr><td align="left" valign="top" headers="idm140232229170528"> <p>
									<code class="literal">--secret-name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229169440"> <p>
									Name of the secret. Mandatory parameter for <code class="literal">--type=secret</code>.
								</p>
								 </td><td align="left" valign="top" headers="idm140232229168352"> </td></tr><tr><td align="left" valign="top" headers="idm140232229170528"> <p>
									<code class="literal">--configmap-name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229169440"> <p>
									Name of the configmap. Mandatory parameter for <code class="literal">--type=configmap</code>.
								</p>
								 </td><td align="left" valign="top" headers="idm140232229168352"> </td></tr><tr><td align="left" valign="top" headers="idm140232229170528"> <p>
									<code class="literal">--claim-name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229169440"> <p>
									Name of the persistent volume claim. Mandatory parameter for <code class="literal">--type=persistentVolumeClaim</code>.
								</p>
								 </td><td align="left" valign="top" headers="idm140232229168352"> </td></tr><tr><td align="left" valign="top" headers="idm140232229170528"> <p>
									<code class="literal">--source</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229169440"> <p>
									Details of volume source as a JSON string. Recommended if the desired volume source is not supported by <code class="literal">--type</code>.
								</p>
								 </td><td align="left" valign="top" headers="idm140232229168352"> </td></tr><tr><td align="left" valign="top" headers="idm140232229170528"> <p>
									<code class="literal">-o, --output</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229169440"> <p>
									Display the modified objects instead of updating them on the server. Supported values: <code class="literal">json</code>, <code class="literal">yaml</code>.
								</p>
								 </td><td align="left" valign="top" headers="idm140232229168352"> </td></tr><tr><td align="left" valign="top" headers="idm140232229170528"> <p>
									<code class="literal">--output-version</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229169440"> <p>
									Output the modified objects with the given version.
								</p>
								 </td><td align="left" valign="top" headers="idm140232229168352"> <p>
									<code class="literal">api-version</code>
								</p>
								 </td></tr></tbody></table></div></div><p>
					For example:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To add a new volume source <span class="strong strong"><strong>emptyDir</strong></span> to the <span class="strong strong"><strong>registry</strong></span> <code class="literal">DeploymentConfig</code> object:
						</p><pre class="programlisting language-terminal">$ oc set volume dc/registry --add</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can alternatively apply the following YAML to add the volume:
						</p><div class="example" id="idm140232227784976"><p class="title"><strong>Example 7.1. Sample deployment config with an added volume</strong></p><div class="example-contents"><pre class="programlisting language-yaml">kind: DeploymentConfig
apiVersion: apps.openshift.io/v1
metadata:
  name: registry
  namespace: registry
spec:
  replicas: 3
  selector:
    app: httpd
  template:
    metadata:
      labels:
        app: httpd
    spec:
      volumes: <span id="CO134-1"><!--Empty--></span><span class="callout">1</span>
        - name: volume-pppsw
          emptyDir: {}
      containers:
        - name: httpd
          image: &gt;-
            image-registry.openshift-image-registry.svc:5000/openshift/httpd:latest
          ports:
            - containerPort: 8080
              protocol: TCP</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO134-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Add the volume source <span class="strong strong"><strong>emptyDir</strong></span>.
									</div></dd></dl></div></div></div></div></div></li><li class="listitem"><p class="simpara">
							To add volume <span class="strong strong"><strong>v1</strong></span> with secret <span class="strong strong"><strong>secret1</strong></span> for replication controller <span class="strong strong"><strong>r1</strong></span> and mount inside the containers at <span class="strong strong"><strong><span class="emphasis"><em>/data</em></span></strong></span>:
						</p><pre class="programlisting language-terminal">$ oc set volume rc/r1 --add --name=v1 --type=secret --secret-name='secret1' --mount-path=/data</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can alternatively apply the following YAML to add the volume:
						</p><div class="example" id="idm140232227772080"><p class="title"><strong>Example 7.2. Sample replication controller with added volume and secret</strong></p><div class="example-contents"><pre class="programlisting language-yaml">kind: ReplicationController
apiVersion: v1
metadata:
  name: example-1
  namespace: example
spec:
  replicas: 0
  selector:
    app: httpd
    deployment: example-1
    deploymentconfig: example
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: httpd
        deployment: example-1
        deploymentconfig: example
    spec:
      volumes: <span id="CO135-1"><!--Empty--></span><span class="callout">1</span>
        - name: v1
          secret:
            secretName: secret1
            defaultMode: 420
      containers:
        - name: httpd
          image: &gt;-
            image-registry.openshift-image-registry.svc:5000/openshift/httpd:latest
          volumeMounts: <span id="CO135-2"><!--Empty--></span><span class="callout">2</span>
            - name: v1
              mountPath: /data</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO135-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Add the volume and secret.
									</div></dd><dt><a href="#CO135-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Add the container mount path.
									</div></dd></dl></div></div></div></div></div></li><li class="listitem"><p class="simpara">
							To add existing persistent volume <span class="strong strong"><strong>v1</strong></span> with claim name <span class="strong strong"><strong>pvc1</strong></span> to deployment configuration <span class="strong strong"><strong><span class="emphasis"><em>dc.json</em></span></strong></span> on disk, mount the volume on container <span class="strong strong"><strong>c1</strong></span> at <span class="strong strong"><strong><span class="emphasis"><em>/data</em></span></strong></span>, and update the <code class="literal">DeploymentConfig</code> object on the server:
						</p><pre class="programlisting language-terminal">$ oc set volume -f dc.json --add --name=v1 --type=persistentVolumeClaim \
  --claim-name=pvc1 --mount-path=/data --containers=c1</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can alternatively apply the following YAML to add the volume:
						</p><div class="example" id="idm140232226798704"><p class="title"><strong>Example 7.3. Sample deployment config with persistent volume added</strong></p><div class="example-contents"><pre class="programlisting language-yaml">kind: DeploymentConfig
apiVersion: apps.openshift.io/v1
metadata:
  name: example
  namespace: example
spec:
  replicas: 3
  selector:
    app: httpd
  template:
    metadata:
      labels:
        app: httpd
    spec:
      volumes:
        - name: volume-pppsw
          emptyDir: {}
        - name: v1 <span id="CO136-1"><!--Empty--></span><span class="callout">1</span>
          persistentVolumeClaim:
            claimName: pvc1
      containers:
        - name: httpd
          image: &gt;-
            image-registry.openshift-image-registry.svc:5000/openshift/httpd:latest
          ports:
            - containerPort: 8080
              protocol: TCP
          volumeMounts: <span id="CO136-2"><!--Empty--></span><span class="callout">2</span>
            - name: v1
              mountPath: /data</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO136-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Add the persistent volume claim named `pvc1.
									</div></dd><dt><a href="#CO136-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Add the container mount path.
									</div></dd></dl></div></div></div></div></div></li><li class="listitem"><p class="simpara">
							To add a volume <span class="strong strong"><strong>v1</strong></span> based on Git repository <span class="strong strong"><strong>https://github.com/namespace1/project1</strong></span> with revision <span class="strong strong"><strong>5125c45f9f563</strong></span> for all replication controllers:
						</p><pre class="programlisting language-terminal">$ oc set volume rc --all --add --name=v1 \
  --source='{"gitRepo": {
                "repository": "https://github.com/namespace1/project1",
                "revision": "5125c45f9f563"
            }}'</pre></li></ul></div></section><section class="section" id="nodes-containers-volumes-updating_nodes-containers-volumes"><div class="titlepage"><div><div><h3 class="title">7.3.5. Updating volumes and volume mounts in a pod</h3></div></div></div><p>
					You can modify the volumes and volume mounts in a pod.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						Updating existing volumes using the <code class="literal">--overwrite</code> option:
					</p></div><pre class="programlisting language-terminal">$ oc set volume &lt;object_type&gt;/&lt;name&gt; --add --overwrite [options]</pre><p>
					For example:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To replace existing volume <span class="strong strong"><strong>v1</strong></span> for replication controller <span class="strong strong"><strong>r1</strong></span> with existing persistent volume claim <span class="strong strong"><strong>pvc1</strong></span>:
						</p><pre class="programlisting language-terminal">$ oc set volume rc/r1 --add --overwrite --name=v1 --type=persistentVolumeClaim --claim-name=pvc1</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can alternatively apply the following YAML to replace the volume:
						</p><div class="example" id="idm140232226774288"><p class="title"><strong>Example 7.4. Sample replication controller with persistent volume claim named <code class="literal">pvc1</code></strong></p><div class="example-contents"><pre class="programlisting language-yaml">kind: ReplicationController
apiVersion: v1
metadata:
  name: example-1
  namespace: example
spec:
  replicas: 0
  selector:
    app: httpd
    deployment: example-1
    deploymentconfig: example
  template:
    metadata:
      labels:
        app: httpd
        deployment: example-1
        deploymentconfig: example
    spec:
      volumes:
        - name: v1 <span id="CO137-1"><!--Empty--></span><span class="callout">1</span>
          persistentVolumeClaim:
            claimName: pvc1
      containers:
        - name: httpd
          image: &gt;-
            image-registry.openshift-image-registry.svc:5000/openshift/httpd:latest
          ports:
            - containerPort: 8080
              protocol: TCP
          volumeMounts:
            - name: v1
              mountPath: /data</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO137-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Set persistent volume claim to <code class="literal">pvc1</code>.
									</div></dd></dl></div></div></div></div></div></li><li class="listitem"><p class="simpara">
							To change the <code class="literal">DeploymentConfig</code> object <span class="strong strong"><strong>d1</strong></span> mount point to <span class="strong strong"><strong><span class="emphasis"><em>/opt</em></span></strong></span> for volume <span class="strong strong"><strong>v1</strong></span>:
						</p><pre class="programlisting language-terminal">$ oc set volume dc/d1 --add --overwrite --name=v1 --mount-path=/opt</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can alternatively apply the following YAML to change the mount point:
						</p><div class="example" id="idm140232226761024"><p class="title"><strong>Example 7.5. Sample deployment config with mount point set to <code class="literal">opt</code>.</strong></p><div class="example-contents"><pre class="programlisting language-yaml">kind: DeploymentConfig
apiVersion: apps.openshift.io/v1
metadata:
  name: example
  namespace: example
spec:
  replicas: 3
  selector:
    app: httpd
  template:
    metadata:
      labels:
        app: httpd
    spec:
      volumes:
        - name: volume-pppsw
          emptyDir: {}
        - name: v2
          persistentVolumeClaim:
            claimName: pvc1
        - name: v1
          persistentVolumeClaim:
            claimName: pvc1
      containers:
        - name: httpd
          image: &gt;-
            image-registry.openshift-image-registry.svc:5000/openshift/httpd:latest
          ports:
            - containerPort: 8080
              protocol: TCP
          volumeMounts: <span id="CO138-1"><!--Empty--></span><span class="callout">1</span>
            - name: v1
              mountPath: /opt</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO138-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Set the mount point to <code class="literal">/opt</code>.
									</div></dd></dl></div></div></div></div></div></li></ul></div></section><section class="section" id="nodes-containers-volumes-removing_nodes-containers-volumes"><div class="titlepage"><div><div><h3 class="title">7.3.6. Removing volumes and volume mounts from a pod</h3></div></div></div><p>
					You can remove a volume or volume mount from a pod.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To remove a volume from pod templates:
					</p></div><pre class="programlisting language-terminal">$ oc set volume &lt;object_type&gt;/&lt;name&gt; --remove [options]</pre><div class="table" id="idm140232226252112"><p class="title"><strong>Table 7.3. Supported options for removing volumes</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232226246336" scope="col">Option</th><th align="left" valign="top" id="idm140232226245248" scope="col">Description</th><th align="left" valign="top" id="idm140232226244160" scope="col">Default</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232226246336"> <p>
									<code class="literal">--name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226245248"> <p>
									Name of the volume.
								</p>
								 </td><td align="left" valign="top" headers="idm140232226244160"> </td></tr><tr><td align="left" valign="top" headers="idm140232226246336"> <p>
									<code class="literal">-c, --containers</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226245248"> <p>
									Select containers by name. It can also take wildcard <code class="literal">'*'</code> that matches any character.
								</p>
								 </td><td align="left" valign="top" headers="idm140232226244160"> <p>
									<code class="literal">'*'</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226246336"> <p>
									<code class="literal">--confirm</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226245248"> <p>
									Indicate that you want to remove multiple volumes at once.
								</p>
								 </td><td align="left" valign="top" headers="idm140232226244160"> </td></tr><tr><td align="left" valign="top" headers="idm140232226246336"> <p>
									<code class="literal">-o, --output</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226245248"> <p>
									Display the modified objects instead of updating them on the server. Supported values: <code class="literal">json</code>, <code class="literal">yaml</code>.
								</p>
								 </td><td align="left" valign="top" headers="idm140232226244160"> </td></tr><tr><td align="left" valign="top" headers="idm140232226246336"> <p>
									<code class="literal">--output-version</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226245248"> <p>
									Output the modified objects with the given version.
								</p>
								 </td><td align="left" valign="top" headers="idm140232226244160"> <p>
									<code class="literal">api-version</code>
								</p>
								 </td></tr></tbody></table></div></div><p>
					For example:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To remove a volume <span class="strong strong"><strong>v1</strong></span> from the <code class="literal">DeploymentConfig</code> object <span class="strong strong"><strong>d1</strong></span>:
						</p><pre class="programlisting language-terminal">$ oc set volume dc/d1 --remove --name=v1</pre></li><li class="listitem"><p class="simpara">
							To unmount volume <span class="strong strong"><strong>v1</strong></span> from container <span class="strong strong"><strong>c1</strong></span> for the <code class="literal">DeploymentConfig</code> object <span class="strong strong"><strong>d1</strong></span> and remove the volume <span class="strong strong"><strong>v1</strong></span> if it is not referenced by any containers on <span class="strong strong"><strong>d1</strong></span>:
						</p><pre class="programlisting language-terminal">$ oc set volume dc/d1 --remove --name=v1 --containers=c1</pre></li><li class="listitem"><p class="simpara">
							To remove all volumes for replication controller <span class="strong strong"><strong>r1</strong></span>:
						</p><pre class="programlisting language-terminal">$ oc set volume rc/r1 --remove --confirm</pre></li></ul></div></section><section class="section" id="nodes-containers-volumes-subpath_nodes-containers-volumes"><div class="titlepage"><div><div><h3 class="title">7.3.7. Configuring volumes for multiple uses in a pod</h3></div></div></div><p>
					You can configure a volume to allows you to share one volume for multiple uses in a single pod using the <code class="literal">volumeMounts.subPath</code> property to specify a <code class="literal">subPath</code> value inside a volume instead of the volume’s root.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You cannot add a <code class="literal">subPath</code> parameter to an existing scheduled pod.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To view the list of files in the volume, run the <code class="literal">oc rsh</code> command:
						</p><pre class="programlisting language-terminal">$ oc rsh &lt;pod&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">sh-4.2$ ls /path/to/volume/subpath/mount
example_file1 example_file2 example_file3</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Specify the <code class="literal">subPath</code>:
						</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">Pod</code> spec with <code class="literal">subPath</code> parameter</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-site
spec:
    containers:
    - name: mysql
      image: mysql
      volumeMounts:
      - mountPath: /var/lib/mysql
        name: site-data
        subPath: mysql <span id="CO139-1"><!--Empty--></span><span class="callout">1</span>
    - name: php
      image: php
      volumeMounts:
      - mountPath: /var/www/html
        name: site-data
        subPath: html <span id="CO139-2"><!--Empty--></span><span class="callout">2</span>
    volumes:
    - name: site-data
      persistentVolumeClaim:
        claimName: my-site-data</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO139-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Databases are stored in the <code class="literal">mysql</code> folder.
								</div></dd><dt><a href="#CO139-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									HTML content is stored in the <code class="literal">html</code> folder.
								</div></dd></dl></div></li></ol></div></section></section><section class="section" id="nodes-containers-projected-volumes"><div class="titlepage"><div><div><h2 class="title">7.4. Mapping volumes using projected volumes</h2></div></div></div><p>
				A <span class="emphasis"><em>projected volume</em></span> maps several existing volume sources into the same directory.
			</p><p>
				The following types of volume sources can be projected:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Secrets
					</li><li class="listitem">
						Config Maps
					</li><li class="listitem">
						Downward API
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					All sources are required to be in the same namespace as the pod.
				</p></div></div><section class="section" id="nodes-containers-projected-volumes-about_nodes-containers-projected-volumes"><div class="titlepage"><div><div><h3 class="title">7.4.1. Understanding projected volumes</h3></div></div></div><p>
					Projected volumes can map any combination of these volume sources into a single directory, allowing the user to:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							automatically populate a single volume with the keys from multiple secrets, config maps, and with downward API information, so that I can synthesize a single directory with various sources of information;
						</li><li class="listitem">
							populate a single volume with the keys from multiple secrets, config maps, and with downward API information, explicitly specifying paths for each item, so that I can have full control over the contents of that volume.
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						When the <code class="literal">RunAsUser</code> permission is set in the security context of a Linux-based pod, the projected files have the correct permissions set, including container user ownership. However, when the Windows equivalent <code class="literal">RunAsUsername</code> permission is set in a Windows pod, the kubelet is unable to correctly set ownership on the files in the projected volume.
					</p><p>
						Therefore, the <code class="literal">RunAsUsername</code> permission set in the security context of a Windows pod is not honored for Windows projected volumes running in OpenShift Container Platform.
					</p></div></div><p>
					The following general scenarios show how you can use projected volumes.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><span class="strong strong"><strong>Config map, secrets, Downward API.</strong></span></span></dt><dd>
								Projected volumes allow you to deploy containers with configuration data that includes passwords. An application using these resources could be deploying Red Hat OpenStack Platform (RHOSP) on Kubernetes. The configuration data might have to be assembled differently depending on if the services are going to be used for production or for testing. If a pod is labeled with production or testing, the downward API selector <code class="literal">metadata.labels</code> can be used to produce the correct RHOSP configs.
							</dd><dt><span class="term"><span class="strong strong"><strong>Config map + secrets.</strong></span></span></dt><dd>
								Projected volumes allow you to deploy containers involving configuration data and passwords. For example, you might execute a config map with some sensitive encrypted tasks that are decrypted using a vault password file.
							</dd><dt><span class="term"><span class="strong strong"><strong>ConfigMap + Downward API.</strong></span></span></dt><dd>
								Projected volumes allow you to generate a config including the pod name (available via the <code class="literal">metadata.name</code> selector). This application can then pass the pod name along with requests to easily determine the source without using IP tracking.
							</dd><dt><span class="term"><span class="strong strong"><strong>Secrets + Downward API.</strong></span></span></dt><dd>
								Projected volumes allow you to use a secret as a public key to encrypt the namespace of the pod (available via the <code class="literal">metadata.namespace</code> selector). This example allows the Operator to use the application to deliver the namespace information securely without using an encrypted transport.
							</dd></dl></div><section class="section" id="projected-volumes-examples_nodes-containers-projected-volumes"><div class="titlepage"><div><div><h4 class="title">7.4.1.1. Example Pod specs</h4></div></div></div><p>
						The following are examples of <code class="literal">Pod</code> specs for creating projected volumes.
					</p><div class="formalpara"><p class="title"><strong>Pod with a secret, a Downward API, and a config map</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts: <span id="CO140-1"><!--Empty--></span><span class="callout">1</span>
    - name: all-in-one
      mountPath: "/projected-volume"<span id="CO140-2"><!--Empty--></span><span class="callout">2</span>
      readOnly: true <span id="CO140-3"><!--Empty--></span><span class="callout">3</span>
  volumes: <span id="CO140-4"><!--Empty--></span><span class="callout">4</span>
  - name: all-in-one <span id="CO140-5"><!--Empty--></span><span class="callout">5</span>
    projected:
      defaultMode: 0400 <span id="CO140-6"><!--Empty--></span><span class="callout">6</span>
      sources:
      - secret:
          name: mysecret <span id="CO140-7"><!--Empty--></span><span class="callout">7</span>
          items:
            - key: username
              path: my-group/my-username <span id="CO140-8"><!--Empty--></span><span class="callout">8</span>
      - downwardAPI: <span id="CO140-9"><!--Empty--></span><span class="callout">9</span>
          items:
            - path: "labels"
              fieldRef:
                fieldPath: metadata.labels
            - path: "cpu_limit"
              resourceFieldRef:
                containerName: container-test
                resource: limits.cpu
      - configMap: <span id="CO140-10"><!--Empty--></span><span class="callout">10</span>
          name: myconfigmap
          items:
            - key: config
              path: my-group/my-config
              mode: 0777 <span id="CO140-11"><!--Empty--></span><span class="callout">11</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO140-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Add a <code class="literal">volumeMounts</code> section for each container that needs the secret.
							</div></dd><dt><a href="#CO140-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specify a path to an unused directory where the secret will appear.
							</div></dd><dt><a href="#CO140-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Set <code class="literal">readOnly</code> to <code class="literal">true</code>.
							</div></dd><dt><a href="#CO140-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Add a <code class="literal">volumes</code> block to list each projected volume source.
							</div></dd><dt><a href="#CO140-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Specify any name for the volume.
							</div></dd><dt><a href="#CO140-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								Set the execute permission on the files.
							</div></dd><dt><a href="#CO140-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								Add a secret. Enter the name of the secret object. Each secret you want to use must be listed.
							</div></dd><dt><a href="#CO140-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								Specify the path to the secrets file under the <code class="literal">mountPath</code>. Here, the secrets file is in <span class="strong strong"><strong><span class="emphasis"><em>/projected-volume/my-group/my-username</em></span></strong></span>.
							</div></dd><dt><a href="#CO140-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								Add a Downward API source.
							</div></dd><dt><a href="#CO140-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								Add a ConfigMap source.
							</div></dd><dt><a href="#CO140-11"><span class="callout">11</span></a> </dt><dd><div class="para">
								Set the mode for the specific projection
							</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If there are multiple containers in the pod, each container needs a <code class="literal">volumeMounts</code> section, but only one <code class="literal">volumes</code> section is needed.
						</p></div></div><div class="formalpara"><p class="title"><strong>Pod with multiple secrets with a non-default permission mode set</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts:
    - name: all-in-one
      mountPath: "/projected-volume"
      readOnly: true
  volumes:
  - name: all-in-one
    projected:
      defaultMode: 0755
      sources:
      - secret:
          name: mysecret
          items:
            - key: username
              path: my-group/my-username
      - secret:
          name: mysecret2
          items:
            - key: password
              path: my-group/my-password
              mode: 511</pre>

						</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The <code class="literal">defaultMode</code> can only be specified at the projected level and not for each volume source. However, as illustrated above, you can explicitly set the <code class="literal">mode</code> for each individual projection.
						</p></div></div></section><section class="section" id="projected-volumes-pathing_nodes-containers-projected-volumes"><div class="titlepage"><div><div><h4 class="title">7.4.1.2. Pathing Considerations</h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term"><span class="strong strong"><strong>Collisions Between Keys when Configured Paths are Identical</strong></span></span></dt><dd><p class="simpara">
									If you configure any keys with the same path, the pod spec will not be accepted as valid. In the following example, the specified path for <code class="literal">mysecret</code> and <code class="literal">myconfigmap</code> are the same:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts:
    - name: all-in-one
      mountPath: "/projected-volume"
      readOnly: true
  volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:
          name: mysecret
          items:
            - key: username
              path: my-group/data
      - configMap:
          name: myconfigmap
          items:
            - key: config
              path: my-group/data</pre></dd></dl></div><p>
						Consider the following situations related to the volume file paths.
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><span class="strong strong"><strong>Collisions Between Keys without Configured Paths</strong></span></span></dt><dd>
									The only run-time validation that can occur is when all the paths are known at pod creation, similar to the above scenario. Otherwise, when a conflict occurs the most recent specified resource will overwrite anything preceding it (this is true for resources that are updated after pod creation as well).
								</dd><dt><span class="term"><span class="strong strong"><strong>Collisions when One Path is Explicit and the Other is Automatically Projected</strong></span></span></dt><dd>
									In the event that there is a collision due to a user specified path matching data that is automatically projected, the latter resource will overwrite anything preceding it as before
								</dd></dl></div></section></section><section class="section" id="nodes-containers-projected-volumes-creating_nodes-containers-projected-volumes"><div class="titlepage"><div><div><h3 class="title">7.4.2. Configuring a Projected Volume for a Pod</h3></div></div></div><p>
					When creating projected volumes, consider the volume file path situations described in <span class="emphasis"><em>Understanding projected volumes</em></span>.
				</p><p>
					The following example shows how to use a projected volume to mount an existing secret volume source. The steps can be used to create a user name and password secrets from local files. You then create a pod that runs one container, using a projected volume to mount the secrets into the same shared directory.
				</p><p>
					The user name and password values can be any valid string that is <span class="strong strong"><strong>base64</strong></span> encoded.
				</p><p>
					The following example shows <code class="literal">admin</code> in base64:
				</p><pre class="programlisting language-terminal">$ echo -n "admin" | base64</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">YWRtaW4=</pre>

					</p></div><p>
					The following example shows the password <code class="literal">1f2d1e2e67df</code> in base64:
				</p><pre class="programlisting language-terminal">$ echo -n "1f2d1e2e67df" | base64</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">MWYyZDFlMmU2N2Rm</pre>

					</p></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To use a projected volume to mount an existing secret volume source.
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the secret:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file similar to the following, replacing the password and user information as appropriate:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  pass: MWYyZDFlMmU2N2Rm
  user: YWRtaW4=</pre></li><li class="listitem"><p class="simpara">
									Use the following command to create the secret:
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;secrets-filename&gt;</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc create -f secret.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">secret "mysecret" created</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									You can check that the secret was created using the following commands:
								</p><pre class="programlisting language-terminal">$ oc get secret &lt;secret-name&gt;</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc get secret mysecret</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME       TYPE      DATA      AGE
mysecret   Opaque    2         17h</pre>

									</p></div><pre class="programlisting language-terminal">$ oc get secret &lt;secret-name&gt; -o yaml</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc get secret mysecret -o yaml</pre><pre class="programlisting language-yaml">apiVersion: v1
data:
  pass: MWYyZDFlMmU2N2Rm
  user: YWRtaW4=
kind: Secret
metadata:
  creationTimestamp: 2017-05-30T20:21:38Z
  name: mysecret
  namespace: default
  resourceVersion: "2107"
  selfLink: /api/v1/namespaces/default/secrets/mysecret
  uid: 959e0424-4575-11e7-9f97-fa163e4bd54c
type: Opaque</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a pod with a projected volume.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file similar to the following, including a <code class="literal">volumes</code> section:
								</p><pre class="programlisting language-yaml">kind: Pod
metadata:
  name: test-projected-volume
spec:
  containers:
  - name: test-projected-volume
    image: busybox
    args:
    - sleep
    - "86400"
    volumeMounts:
    - name: all-in-one
      mountPath: "/projected-volume"
      readOnly: true
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
            - ALL
  volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:
          name: mysecret <span id="CO141-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO141-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The name of the secret you created.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Create the pod from the configuration file:
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;your_yaml_file&gt;.yaml</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc create -f secret-pod.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">pod "test-projected-volume" created</pre>

									</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Verify that the pod container is running, and then watch for changes to the pod:
						</p><pre class="programlisting language-terminal">$ oc get pod &lt;name&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc get pod test-projected-volume</pre><p class="simpara">
							The output should appear similar to the following:
						</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                    READY     STATUS    RESTARTS   AGE
test-projected-volume   1/1       Running   0          14s</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							In another terminal, use the <code class="literal">oc exec</code> command to open a shell to the running container:
						</p><pre class="programlisting language-terminal">$ oc exec -it &lt;pod&gt; &lt;command&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc exec -it test-projected-volume -- /bin/sh</pre></li><li class="listitem"><p class="simpara">
							In your shell, verify that the <code class="literal">projected-volumes</code> directory contains your projected sources:
						</p><pre class="programlisting language-terminal">/ # ls</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">bin               home              root              tmp
dev               proc              run               usr
etc               projected-volume  sys               var</pre>

							</p></div></li></ol></div></section></section><section class="section" id="nodes-containers-downward-api"><div class="titlepage"><div><div><h2 class="title">7.5. Allowing containers to consume API objects</h2></div></div></div><p>
				The <span class="emphasis"><em>Downward API</em></span> is a mechanism that allows containers to consume information about API objects without coupling to OpenShift Container Platform. Such information includes the pod’s name, namespace, and resource values. Containers can consume information from the downward API using environment variables or a volume plugin.
			</p><section class="section" id="nodes-containers-projected-volumes-about_nodes-containers-downward-api"><div class="titlepage"><div><div><h3 class="title">7.5.1. Expose pod information to Containers using the Downward API</h3></div></div></div><p>
					The Downward API contains such information as the pod’s name, project, and resource values. Containers can consume information from the downward API using environment variables or a volume plugin.
				</p><p>
					Fields within the pod are selected using the <code class="literal">FieldRef</code> API type. <code class="literal">FieldRef</code> has two fields:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232227934880" scope="col">Field</th><th align="left" valign="top" id="idm140232227933792" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232227934880"> <p>
									<code class="literal">fieldPath</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227933792"> <p>
									The path of the field to select, relative to the pod.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227934880"> <p>
									<code class="literal">apiVersion</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227933792"> <p>
									The API version to interpret the <code class="literal">fieldPath</code> selector within.
								</p>
								 </td></tr></tbody></table></div><p>
					Currently, the valid selectors in the v1 API include:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232227917968" scope="col">Selector</th><th align="left" valign="top" id="idm140232227916880" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232227917968"> <p>
									<code class="literal">metadata.name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227916880"> <p>
									The pod’s name. This is supported in both environment variables and volumes.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227917968"> <p>
									<code class="literal">metadata.namespace</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227916880"> <p>
									The pod’s namespace.This is supported in both environment variables and volumes.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227917968"> <p>
									<code class="literal">metadata.labels</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227916880"> <p>
									The pod’s labels. This is only supported in volumes and not in environment variables.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227917968"> <p>
									<code class="literal">metadata.annotations</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227916880"> <p>
									The pod’s annotations. This is only supported in volumes and not in environment variables.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227917968"> <p>
									<code class="literal">status.podIP</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227916880"> <p>
									The pod’s IP. This is only supported in environment variables and not volumes.
								</p>
								 </td></tr></tbody></table></div><p>
					The <code class="literal">apiVersion</code> field, if not specified, defaults to the API version of the enclosing pod template.
				</p></section><section class="section" id="nodes-containers-downward-api-container-values_nodes-containers-downward-api"><div class="titlepage"><div><div><h3 class="title">7.5.2. Understanding how to consume container values using the downward API</h3></div></div></div><p>
					You containers can consume API values using environment variables or a volume plugin. Depending on the method you choose, containers can consume:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Pod name
						</li><li class="listitem">
							Pod project/namespace
						</li><li class="listitem">
							Pod annotations
						</li><li class="listitem">
							Pod labels
						</li></ul></div><p>
					Annotations and labels are available using only a volume plugin.
				</p><section class="section" id="nodes-containers-downward-api-container-values-envars_nodes-containers-downward-api"><div class="titlepage"><div><div><h4 class="title">7.5.2.1. Consuming container values using environment variables</h4></div></div></div><p>
						When using a container’s environment variables, use the <code class="literal">EnvVar</code> type’s <code class="literal">valueFrom</code> field (of type <code class="literal">EnvVarSource</code>) to specify that the variable’s value should come from a <code class="literal">FieldRef</code> source instead of the literal value specified by the <code class="literal">value</code> field.
					</p><p>
						Only constant attributes of the pod can be consumed this way, as environment variables cannot be updated once a process is started in a way that allows the process to be notified that the value of a variable has changed. The fields supported using environment variables are:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Pod name
							</li><li class="listitem">
								Pod project/namespace
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a new pod spec that contains the environment variables you want the container to consume:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Create a <code class="literal">pod.yaml</code> file similar to the following:
									</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: dapi-env-test-pod
spec:
  containers:
    - name: env-test-container
      image: gcr.io/google_containers/busybox
      command: [ "/bin/sh", "-c", "env" ]
      env:
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
  restartPolicy: Never
# ...</pre></li><li class="listitem"><p class="simpara">
										Create the pod from the <code class="literal">pod.yaml</code> file:
									</p><pre class="programlisting language-terminal">$ oc create -f pod.yaml</pre></li></ol></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Check the container’s logs for the <code class="literal">MY_POD_NAME</code> and <code class="literal">MY_POD_NAMESPACE</code> values:
							</p><pre class="programlisting language-terminal">$ oc logs -p dapi-env-test-pod</pre></li></ul></div></section><section class="section" id="nodes-containers-downward-api-container-values-plugin_nodes-containers-downward-api"><div class="titlepage"><div><div><h4 class="title">7.5.2.2. Consuming container values using a volume plugin</h4></div></div></div><p>
						You containers can consume API values using a volume plugin.
					</p><p>
						Containers can consume:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Pod name
							</li><li class="listitem">
								Pod project/namespace
							</li><li class="listitem">
								Pod annotations
							</li><li class="listitem">
								Pod labels
							</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To use the volume plugin:
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a new pod spec that contains the environment variables you want the container to consume:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Create a <code class="literal">volume-pod.yaml</code> file similar to the following:
									</p><pre class="programlisting language-yaml">kind: Pod
apiVersion: v1
metadata:
  labels:
    zone: us-east-coast
    cluster: downward-api-test-cluster1
    rack: rack-123
  name: dapi-volume-test-pod
  annotations:
    annotation1: "345"
    annotation2: "456"
spec:
  containers:
    - name: volume-test-container
      image: gcr.io/google_containers/busybox
      command: ["sh", "-c", "cat /tmp/etc/pod_labels /tmp/etc/pod_annotations"]
      volumeMounts:
        - name: podinfo
          mountPath: /tmp/etc
          readOnly: false
  volumes:
  - name: podinfo
    downwardAPI:
      defaultMode: 420
      items:
      - fieldRef:
          fieldPath: metadata.name
        path: pod_name
      - fieldRef:
          fieldPath: metadata.namespace
        path: pod_namespace
      - fieldRef:
          fieldPath: metadata.labels
        path: pod_labels
      - fieldRef:
          fieldPath: metadata.annotations
        path: pod_annotations
  restartPolicy: Never
# ...</pre></li><li class="listitem"><p class="simpara">
										Create the pod from the <code class="literal">volume-pod.yaml</code> file:
									</p><pre class="programlisting language-terminal">$ oc create -f volume-pod.yaml</pre></li></ol></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Check the container’s logs and verify the presence of the configured fields:
							</p><pre class="programlisting language-terminal">$ oc logs -p dapi-volume-test-pod</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">cluster=downward-api-test-cluster1
rack=rack-123
zone=us-east-coast
annotation1=345
annotation2=456
kubernetes.io/config.source=api</pre>

								</p></div></li></ul></div></section></section><section class="section" id="nodes-containers-downward-api-container-resources-api_nodes-containers-downward-api"><div class="titlepage"><div><div><h3 class="title">7.5.3. Understanding how to consume container resources using the Downward API</h3></div></div></div><p>
					When creating pods, you can use the Downward API to inject information about computing resource requests and limits so that image and application authors can correctly create an image for specific environments.
				</p><p>
					You can do this using environment variable or a volume plugin.
				</p><section class="section" id="nodes-containers-downward-api-container-resources-envars_nodes-containers-downward-api"><div class="titlepage"><div><div><h4 class="title">7.5.3.1. Consuming container resources using environment variables</h4></div></div></div><p>
						When creating pods, you can use the Downward API to inject information about computing resource requests and limits using environment variables.
					</p><p>
						When creating the pod configuration, specify environment variables that correspond to the contents of the <code class="literal">resources</code> field in the <code class="literal"><span class="strong strong"><strong>spec.container</strong></span></code> field.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If the resource limits are not included in the container configuration, the downward API defaults to the node’s CPU and memory allocatable values.
						</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a new pod spec that contains the resources you want to inject:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Create a <code class="literal">pod.yaml</code> file similar to the following:
									</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: dapi-env-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox:1.24
      command: [ "/bin/sh", "-c", "env" ]
      resources:
        requests:
          memory: "32Mi"
          cpu: "125m"
        limits:
          memory: "64Mi"
          cpu: "250m"
      env:
        - name: MY_CPU_REQUEST
          valueFrom:
            resourceFieldRef:
              resource: requests.cpu
        - name: MY_CPU_LIMIT
          valueFrom:
            resourceFieldRef:
              resource: limits.cpu
        - name: MY_MEM_REQUEST
          valueFrom:
            resourceFieldRef:
              resource: requests.memory
        - name: MY_MEM_LIMIT
          valueFrom:
            resourceFieldRef:
              resource: limits.memory
# ...</pre></li><li class="listitem"><p class="simpara">
										Create the pod from the <code class="literal">pod.yaml</code> file:
									</p><pre class="programlisting language-terminal">$ oc create -f pod.yaml</pre></li></ol></div></li></ol></div></section><section class="section" id="nodes-containers-downward-api-container-resources-plugin_nodes-containers-downward-api"><div class="titlepage"><div><div><h4 class="title">7.5.3.2. Consuming container resources using a volume plugin</h4></div></div></div><p>
						When creating pods, you can use the Downward API to inject information about computing resource requests and limits using a volume plugin.
					</p><p>
						When creating the pod configuration, use the <code class="literal">spec.volumes.downwardAPI.items</code> field to describe the desired resources that correspond to the <code class="literal">spec.resources</code> field.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If the resource limits are not included in the container configuration, the Downward API defaults to the node’s CPU and memory allocatable values.
						</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a new pod spec that contains the resources you want to inject:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Create a <code class="literal">pod.yaml</code> file similar to the following:
									</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: dapi-env-test-pod
spec:
  containers:
    - name: client-container
      image: gcr.io/google_containers/busybox:1.24
      command: ["sh", "-c", "while true; do echo; if [[ -e /etc/cpu_limit ]]; then cat /etc/cpu_limit; fi; if [[ -e /etc/cpu_request ]]; then cat /etc/cpu_request; fi; if [[ -e /etc/mem_limit ]]; then cat /etc/mem_limit; fi; if [[ -e /etc/mem_request ]]; then cat /etc/mem_request; fi; sleep 5; done"]
      resources:
        requests:
          memory: "32Mi"
          cpu: "125m"
        limits:
          memory: "64Mi"
          cpu: "250m"
      volumeMounts:
        - name: podinfo
          mountPath: /etc
          readOnly: false
  volumes:
    - name: podinfo
      downwardAPI:
        items:
          - path: "cpu_limit"
            resourceFieldRef:
              containerName: client-container
              resource: limits.cpu
          - path: "cpu_request"
            resourceFieldRef:
              containerName: client-container
              resource: requests.cpu
          - path: "mem_limit"
            resourceFieldRef:
              containerName: client-container
              resource: limits.memory
          - path: "mem_request"
            resourceFieldRef:
              containerName: client-container
              resource: requests.memory
# ...</pre></li><li class="listitem"><p class="simpara">
										Create the pod from the <code class="literal"><span class="strong strong"><strong><span class="emphasis"><em>volume-pod.yaml</em></span></strong></span></code> file:
									</p><pre class="programlisting language-terminal">$ oc create -f volume-pod.yaml</pre></li></ol></div></li></ol></div></section></section><section class="section" id="nodes-containers-downward-api-container-secrets_nodes-containers-downward-api"><div class="titlepage"><div><div><h3 class="title">7.5.4. Consuming secrets using the Downward API</h3></div></div></div><p>
					When creating pods, you can use the downward API to inject secrets so image and application authors can create an image for specific environments.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a secret to inject:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a <code class="literal">secret.yaml</code> file similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: mysecret
data:
  password: &lt;password&gt;
  username: &lt;username&gt;
type: kubernetes.io/basic-auth</pre></li><li class="listitem"><p class="simpara">
									Create the secret object from the <code class="literal">secret.yaml</code> file:
								</p><pre class="programlisting language-terminal">$ oc create -f secret.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a pod that references the <code class="literal">username</code> field from the above <code class="literal">Secret</code> object:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a <code class="literal">pod.yaml</code> file similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: dapi-env-test-pod
spec:
  containers:
    - name: env-test-container
      image: gcr.io/google_containers/busybox
      command: [ "/bin/sh", "-c", "env" ]
      env:
        - name: MY_SECRET_USERNAME
          valueFrom:
            secretKeyRef:
              name: mysecret
              key: username
  restartPolicy: Never
# ...</pre></li><li class="listitem"><p class="simpara">
									Create the pod from the <code class="literal">pod.yaml</code> file:
								</p><pre class="programlisting language-terminal">$ oc create -f pod.yaml</pre></li></ol></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Check the container’s logs for the <code class="literal">MY_SECRET_USERNAME</code> value:
						</p><pre class="programlisting language-terminal">$ oc logs -p dapi-env-test-pod</pre></li></ul></div></section><section class="section" id="nodes-containers-downward-api-container-configmaps_nodes-containers-downward-api"><div class="titlepage"><div><div><h3 class="title">7.5.5. Consuming configuration maps using the Downward API</h3></div></div></div><p>
					When creating pods, you can use the Downward API to inject configuration map values so image and application authors can create an image for specific environments.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a config map with the values to inject:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a <code class="literal"><span class="strong strong"><strong><span class="emphasis"><em>configmap.yaml</em></span></strong></span></code> file similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: myconfigmap
data:
  mykey: myvalue</pre></li><li class="listitem"><p class="simpara">
									Create the config map from the <code class="literal">configmap.yaml</code> file:
								</p><pre class="programlisting language-terminal">$ oc create -f configmap.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a pod that references the above config map:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a <code class="literal">pod.yaml</code> file similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: dapi-env-test-pod
spec:
  containers:
    - name: env-test-container
      image: gcr.io/google_containers/busybox
      command: [ "/bin/sh", "-c", "env" ]
      env:
        - name: MY_CONFIGMAP_VALUE
          valueFrom:
            configMapKeyRef:
              name: myconfigmap
              key: mykey
  restartPolicy: Always
# ...</pre></li><li class="listitem"><p class="simpara">
									Create the pod from the <code class="literal">pod.yaml</code> file:
								</p><pre class="programlisting language-terminal">$ oc create -f pod.yaml</pre></li></ol></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Check the container’s logs for the <code class="literal">MY_CONFIGMAP_VALUE</code> value:
						</p><pre class="programlisting language-terminal">$ oc logs -p dapi-env-test-pod</pre></li></ul></div></section><section class="section" id="nodes-containers-downward-api-container-envars_nodes-containers-downward-api"><div class="titlepage"><div><div><h3 class="title">7.5.6. Referencing environment variables</h3></div></div></div><p>
					When creating pods, you can reference the value of a previously defined environment variable by using the <code class="literal">$()</code> syntax. If the environment variable reference can not be resolved, the value will be left as the provided string.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a pod that references an existing environment variable:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a <code class="literal">pod.yaml</code> file similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: dapi-env-test-pod
spec:
  containers:
    - name: env-test-container
      image: gcr.io/google_containers/busybox
      command: [ "/bin/sh", "-c", "env" ]
      env:
        - name: MY_EXISTING_ENV
          value: my_value
        - name: MY_ENV_VAR_REF_ENV
          value: $(MY_EXISTING_ENV)
  restartPolicy: Never
# ...</pre></li><li class="listitem"><p class="simpara">
									Create the pod from the <code class="literal"><span class="strong strong"><strong><span class="emphasis"><em>pod.yaml</em></span></strong></span></code> file:
								</p><pre class="programlisting language-terminal">$ oc create -f pod.yaml</pre></li></ol></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Check the container’s logs for the <code class="literal">MY_ENV_VAR_REF_ENV</code> value:
						</p><pre class="programlisting language-terminal">$ oc logs -p dapi-env-test-pod</pre></li></ul></div></section><section class="section" id="nodes-containers-downward-api-container-escaping_nodes-containers-downward-api"><div class="titlepage"><div><div><h3 class="title">7.5.7. Escaping environment variable references</h3></div></div></div><p>
					When creating a pod, you can escape an environment variable reference by using a double dollar sign. The value will then be set to a single dollar sign version of the provided value.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a pod that references an existing environment variable:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a <code class="literal">pod.yaml</code> file similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: dapi-env-test-pod
spec:
  containers:
    - name: env-test-container
      image: gcr.io/google_containers/busybox
      command: [ "/bin/sh", "-c", "env" ]
      env:
        - name: MY_NEW_ENV
          value: $$(SOME_OTHER_ENV)
  restartPolicy: Never
# ...</pre></li><li class="listitem"><p class="simpara">
									Create the pod from the <code class="literal"><span class="strong strong"><strong><span class="emphasis"><em>pod.yaml</em></span></strong></span></code> file:
								</p><pre class="programlisting language-terminal">$ oc create -f pod.yaml</pre></li></ol></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Check the container’s logs for the <code class="literal">MY_NEW_ENV</code> value:
						</p><pre class="programlisting language-terminal">$ oc logs -p dapi-env-test-pod</pre></li></ul></div></section></section><section class="section" id="nodes-containers-copying-files"><div class="titlepage"><div><div><h2 class="title">7.6. Copying files to or from an OpenShift Container Platform container</h2></div></div></div><p>
				You can use the CLI to copy local files to or from a remote directory in a container using the <code class="literal">rsync</code> command.
			</p><section class="section" id="nodes-containers-copying-files-about_nodes-containers-copying-files"><div class="titlepage"><div><div><h3 class="title">7.6.1. Understanding how to copy files</h3></div></div></div><p>
					The <code class="literal">oc rsync</code> command, or remote sync, is a useful tool for copying database archives to and from your pods for backup and restore purposes. You can also use <code class="literal">oc rsync</code> to copy source code changes into a running pod for development debugging, when the running pod supports hot reload of source files.
				</p><pre class="programlisting language-terminal">$ oc rsync &lt;source&gt; &lt;destination&gt; [-c &lt;container&gt;]</pre><section class="section" id="requirements"><div class="titlepage"><div><div><h4 class="title">7.6.1.1. Requirements</h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Specifying the Copy Source</span></dt><dd><p class="simpara">
									The source argument of the <code class="literal">oc rsync</code> command must point to either a local directory or a pod directory. Individual files are not supported.
								</p><p class="simpara">
									When specifying a pod directory the directory name must be prefixed with the pod name:
								</p><pre class="programlisting language-terminal">&lt;pod name&gt;:&lt;dir&gt;</pre><p class="simpara">
									If the directory name ends in a path separator (<code class="literal">/</code>), only the contents of the directory are copied to the destination. Otherwise, the directory and its contents are copied to the destination.
								</p></dd><dt><span class="term">Specifying the Copy Destination</span></dt><dd>
									The destination argument of the <code class="literal">oc rsync</code> command must point to a directory. If the directory does not exist, but <code class="literal">rsync</code> is used for copy, the directory is created for you.
								</dd><dt><span class="term">Deleting Files at the Destination</span></dt><dd>
									The <code class="literal">--delete</code> flag may be used to delete any files in the remote directory that are not in the local directory.
								</dd><dt><span class="term">Continuous Syncing on File Change</span></dt><dd><p class="simpara">
									Using the <code class="literal">--watch</code> option causes the command to monitor the source path for any file system changes, and synchronizes changes when they occur. With this argument, the command runs forever.
								</p><p class="simpara">
									Synchronization occurs after short quiet periods to ensure a rapidly changing file system does not result in continuous synchronization calls.
								</p><p class="simpara">
									When using the <code class="literal">--watch</code> option, the behavior is effectively the same as manually invoking <code class="literal">oc rsync</code> repeatedly, including any arguments normally passed to <code class="literal">oc rsync</code>. Therefore, you can control the behavior via the same flags used with manual invocations of <code class="literal">oc rsync</code>, such as <code class="literal">--delete</code>.
								</p></dd></dl></div></section></section><section class="section" id="nodes-containers-copying-files-procedure_nodes-containers-copying-files"><div class="titlepage"><div><div><h3 class="title">7.6.2. Copying files to and from containers</h3></div></div></div><p>
					Support for copying local files to or from a container is built into the CLI.
				</p><div class="formalpara"><p class="title"><strong>Prerequisites</strong></p><p>
						When working with <code class="literal">oc rsync</code>, note the following:
					</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							rsync must be installed. The <code class="literal">oc rsync</code> command uses the local <code class="literal">rsync</code> tool, if present on the client machine and the remote container.
						</p><p class="simpara">
							If <code class="literal">rsync</code> is not found locally or in the remote container, a <span class="strong strong"><strong>tar</strong></span> archive is created locally and sent to the container where the <span class="strong strong"><strong>tar</strong></span> utility is used to extract the files. If <span class="strong strong"><strong>tar</strong></span> is not available in the remote container, the copy will fail.
						</p><p class="simpara">
							The <span class="strong strong"><strong>tar</strong></span> copy method does not provide the same functionality as <code class="literal">oc rsync</code>. For example, <code class="literal">oc rsync</code> creates the destination directory if it does not exist and only sends files that are different between the source and the destination.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								In Windows, the <code class="literal">cwRsync</code> client should be installed and added to the PATH for use with the <code class="literal">oc rsync</code> command.
							</p></div></div></li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To copy a local directory to a pod directory:
						</p><pre class="programlisting language-terminal">$ oc rsync &lt;local-dir&gt; &lt;pod-name&gt;:/&lt;remote-dir&gt; -c &lt;container-name&gt;</pre><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc rsync /home/user/source devpod1234:/src -c user-container</pre></li><li class="listitem"><p class="simpara">
							To copy a pod directory to a local directory:
						</p><pre class="programlisting language-terminal">$ oc rsync devpod1234:/src /home/user/source</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">$ oc rsync devpod1234:/src/status.txt /home/user/</pre>

							</p></div></li></ul></div></section><section class="section" id="nodes-containers-copying-files-rsync_nodes-containers-copying-files"><div class="titlepage"><div><div><h3 class="title">7.6.3. Using advanced Rsync features</h3></div></div></div><p>
					The <code class="literal">oc rsync</code> command exposes fewer command line options than standard <code class="literal">rsync</code>. In the case that you want to use a standard <code class="literal">rsync</code> command line option that is not available in <code class="literal">oc rsync</code>, for example the <code class="literal">--exclude-from=FILE</code> option, it might be possible to use standard <code class="literal">rsync</code> 's <code class="literal">--rsh</code> (<code class="literal">-e</code>) option or <code class="literal">RSYNC_RSH</code> environment variable as a workaround, as follows:
				</p><pre class="programlisting language-terminal">$ rsync --rsh='oc rsh' --exclude-from=&lt;file_name&gt; &lt;local-dir&gt; &lt;pod-name&gt;:/&lt;remote-dir&gt;</pre><p>
					or:
				</p><p>
					Export the <code class="literal">RSYNC_RSH</code> variable:
				</p><pre class="programlisting language-terminal">$ export RSYNC_RSH='oc rsh'</pre><p>
					Then, run the rsync command:
				</p><pre class="programlisting language-terminal">$ rsync --exclude-from=&lt;file_name&gt; &lt;local-dir&gt; &lt;pod-name&gt;:/&lt;remote-dir&gt;</pre><p>
					Both of the above examples configure standard <code class="literal">rsync</code> to use <code class="literal">oc rsh</code> as its remote shell program to enable it to connect to the remote pod, and are an alternative to running <code class="literal">oc rsync</code>.
				</p></section></section><section class="section" id="nodes-containers-remote-commands"><div class="titlepage"><div><div><h2 class="title">7.7. Executing remote commands in an OpenShift Container Platform container</h2></div></div></div><p>
				You can use the CLI to execute remote commands in an OpenShift Container Platform container.
			</p><section class="section" id="nodes-containers-remote-commands-about_nodes-containers-remote-commands"><div class="titlepage"><div><div><h3 class="title">7.7.1. Executing remote commands in containers</h3></div></div></div><p>
					Support for remote container command execution is built into the CLI.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To run a command in a container:
					</p></div><pre class="programlisting language-terminal">$ oc exec &lt;pod&gt; [-c &lt;container&gt;] -- &lt;command&gt; [&lt;arg_1&gt; ... &lt;arg_n&gt;]</pre><p>
					For example:
				</p><pre class="programlisting language-terminal">$ oc exec mypod date</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">Thu Apr  9 02:21:53 UTC 2015</pre>

					</p></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						<a class="link" href="https://access.redhat.com/errata/RHSA-2015:1650">For security purposes</a>, the <code class="literal">oc exec</code> command does not work when accessing privileged containers except when the command is executed by a <code class="literal">cluster-admin</code> user.
					</p></div></div></section><section class="section" id="nodes-containers-remote-commands-protocol_nodes-containers-remote-commands"><div class="titlepage"><div><div><h3 class="title">7.7.2. Protocol for initiating a remote command from a client</h3></div></div></div><p>
					Clients initiate the execution of a remote command in a container by issuing a request to the Kubernetes API server:
				</p><pre class="programlisting language-terminal">/proxy/nodes/&lt;node_name&gt;/exec/&lt;namespace&gt;/&lt;pod&gt;/&lt;container&gt;?command=&lt;command&gt;</pre><p>
					In the above URL:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">&lt;node_name&gt;</code> is the FQDN of the node.
						</li><li class="listitem">
							<code class="literal">&lt;namespace&gt;</code> is the project of the target pod.
						</li><li class="listitem">
							<code class="literal">&lt;pod&gt;</code> is the name of the target pod.
						</li><li class="listitem">
							<code class="literal">&lt;container&gt;</code> is the name of the target container.
						</li><li class="listitem">
							<code class="literal">&lt;command&gt;</code> is the desired command to be executed.
						</li></ul></div><p>
					For example:
				</p><pre class="programlisting language-terminal">/proxy/nodes/node123.openshift.com/exec/myns/mypod/mycontainer?command=date</pre><p>
					Additionally, the client can add parameters to the request to indicate if:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							the client should send input to the remote container’s command (stdin).
						</li><li class="listitem">
							the client’s terminal is a TTY.
						</li><li class="listitem">
							the remote container’s command should send output from stdout to the client.
						</li><li class="listitem">
							the remote container’s command should send output from stderr to the client.
						</li></ul></div><p>
					After sending an <code class="literal">exec</code> request to the API server, the client upgrades the connection to one that supports multiplexed streams; the current implementation uses <span class="strong strong"><strong>HTTP/2</strong></span>.
				</p><p>
					The client creates one stream each for stdin, stdout, and stderr. To distinguish among the streams, the client sets the <code class="literal">streamType</code> header on the stream to one of <code class="literal">stdin</code>, <code class="literal">stdout</code>, or <code class="literal">stderr</code>.
				</p><p>
					The client closes all streams, the upgraded connection, and the underlying connection when it is finished with the remote command execution request.
				</p></section></section><section class="section" id="nodes-containers-port-forwarding"><div class="titlepage"><div><div><h2 class="title">7.8. Using port forwarding to access applications in a container</h2></div></div></div><p>
				OpenShift Container Platform supports port forwarding to pods.
			</p><section class="section" id="nodes-containers-port-forwarding-about_nodes-containers-port-forwarding"><div class="titlepage"><div><div><h3 class="title">7.8.1. Understanding port forwarding</h3></div></div></div><p>
					You can use the CLI to forward one or more local ports to a pod. This allows you to listen on a given or random port locally, and have data forwarded to and from given ports in the pod.
				</p><p>
					Support for port forwarding is built into the CLI:
				</p><pre class="programlisting language-terminal">$ oc port-forward &lt;pod&gt; [&lt;local_port&gt;:]&lt;remote_port&gt; [...[&lt;local_port_n&gt;:]&lt;remote_port_n&gt;]</pre><p>
					The CLI listens on each local port specified by the user, forwarding using the protocol described below.
				</p><p>
					Ports may be specified using the following formats:
				</p><div class="horizontal"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 15%; "><!--Empty--></col><col style="width: 85%; "><!--Empty--></col></colgroup><tbody valign="top"><tr><td valign="top"> <p>
									<code class="literal">5000</code>
								</p>
								 </td><td valign="top"> <p>
									The client listens on port 5000 locally and forwards to 5000 in the pod.
								</p>
								 </td></tr><tr><td valign="top"> <p>
									<code class="literal">6000:5000</code>
								</p>
								 </td><td valign="top"> <p>
									The client listens on port 6000 locally and forwards to 5000 in the pod.
								</p>
								 </td></tr><tr><td valign="top"> <p>
									<code class="literal">:5000</code> or <code class="literal">0:5000</code>
								</p>
								 </td><td valign="top"> <p>
									The client selects a free local port and forwards to 5000 in the pod.
								</p>
								 </td></tr></tbody></table></div><p>
					OpenShift Container Platform handles port-forward requests from clients. Upon receiving a request, OpenShift Container Platform upgrades the response and waits for the client to create port-forwarding streams. When OpenShift Container Platform receives a new stream, it copies data between the stream and the pod’s port.
				</p><p>
					Architecturally, there are options for forwarding to a pod’s port. The supported OpenShift Container Platform implementation invokes <code class="literal">nsenter</code> directly on the node host to enter the pod’s network namespace, then invokes <code class="literal">socat</code> to copy data between the stream and the pod’s port. However, a custom implementation could include running a <span class="emphasis"><em>helper</em></span> pod that then runs <code class="literal">nsenter</code> and <code class="literal">socat</code>, so that those binaries are not required to be installed on the host.
				</p></section><section class="section" id="nodes-containers-port-forwarding-using_nodes-containers-port-forwarding"><div class="titlepage"><div><div><h3 class="title">7.8.2. Using port forwarding</h3></div></div></div><p>
					You can use the CLI to port-forward one or more local ports to a pod.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						Use the following command to listen on the specified port in a pod:
					</p></div><pre class="programlisting language-terminal">$ oc port-forward &lt;pod&gt; [&lt;local_port&gt;:]&lt;remote_port&gt; [...[&lt;local_port_n&gt;:]&lt;remote_port_n&gt;]</pre><p>
					For example:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Use the following command to listen on ports <code class="literal">5000</code> and <code class="literal">6000</code> locally and forward data to and from ports <code class="literal">5000</code> and <code class="literal">6000</code> in the pod:
						</p><pre class="programlisting language-terminal">$ oc port-forward &lt;pod&gt; 5000 6000</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Forwarding from 127.0.0.1:5000 -&gt; 5000
Forwarding from [::1]:5000 -&gt; 5000
Forwarding from 127.0.0.1:6000 -&gt; 6000
Forwarding from [::1]:6000 -&gt; 6000</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Use the following command to listen on port <code class="literal">8888</code> locally and forward to <code class="literal">5000</code> in the pod:
						</p><pre class="programlisting language-terminal">$ oc port-forward &lt;pod&gt; 8888:5000</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Forwarding from 127.0.0.1:8888 -&gt; 5000
Forwarding from [::1]:8888 -&gt; 5000</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Use the following command to listen on a free port locally and forward to <code class="literal">5000</code> in the pod:
						</p><pre class="programlisting language-terminal">$ oc port-forward &lt;pod&gt; :5000</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Forwarding from 127.0.0.1:42390 -&gt; 5000
Forwarding from [::1]:42390 -&gt; 5000</pre>

							</p></div><p class="simpara">
							Or:
						</p><pre class="programlisting language-terminal">$ oc port-forward &lt;pod&gt; 0:5000</pre></li></ul></div></section><section class="section" id="nodes-containers-port-forwarding-protocol_nodes-containers-port-forwarding"><div class="titlepage"><div><div><h3 class="title">7.8.3. Protocol for initiating port forwarding from a client</h3></div></div></div><p>
					Clients initiate port forwarding to a pod by issuing a request to the Kubernetes API server:
				</p><pre class="screen">/proxy/nodes/&lt;node_name&gt;/portForward/&lt;namespace&gt;/&lt;pod&gt;</pre><p>
					In the above URL:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">&lt;node_name&gt;</code> is the FQDN of the node.
						</li><li class="listitem">
							<code class="literal">&lt;namespace&gt;</code> is the namespace of the target pod.
						</li><li class="listitem">
							<code class="literal">&lt;pod&gt;</code> is the name of the target pod.
						</li></ul></div><p>
					For example:
				</p><pre class="screen">/proxy/nodes/node123.openshift.com/portForward/myns/mypod</pre><p>
					After sending a port forward request to the API server, the client upgrades the connection to one that supports multiplexed streams; the current implementation uses <a class="link" href="https://httpwg.org/specs/rfc7540.html"><span class="strong strong"><strong>Hyptertext Transfer Protocol Version 2 (HTTP/2)</strong></span></a>.
				</p><p>
					The client creates a stream with the <code class="literal">port</code> header containing the target port in the pod. All data written to the stream is delivered via the kubelet to the target pod and port. Similarly, all data sent from the pod for that forwarded connection is delivered back to the same stream in the client.
				</p><p>
					The client closes all streams, the upgraded connection, and the underlying connection when it is finished with the port forwarding request.
				</p></section></section><section class="section" id="nodes-containers-sysctls"><div class="titlepage"><div><div><h2 class="title">7.9. Using sysctls in containers</h2></div></div></div><p>
				Sysctl settings are exposed through Kubernetes, allowing users to modify certain kernel parameters at runtime. Only sysctls that are namespaced can be set independently on pods. If a sysctl is not namespaced, called <span class="emphasis"><em>node-level</em></span>, you must use another method of setting the sysctl, such as by using the Node Tuning Operator.
			</p><p>
				Network sysctls are a special category of sysctl. Network sysctls include:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						System-wide sysctls, for example <code class="literal">net.ipv4.ip_local_port_range</code>, that are valid for all networking. You can set these independently for each pod on a node.
					</li><li class="listitem">
						Interface-specific sysctls, for example <code class="literal">net.ipv4.conf.IFNAME.accept_local</code>, that only apply to a specific additional network interface for a given pod. You can set these independently for each additional network configuration. You set these by using a configuration in the <code class="literal">tuning-cni</code> after the network interfaces are created.
					</li></ul></div><p>
				Moreover, only those sysctls considered <span class="emphasis"><em>safe</em></span> are whitelisted by default; you can manually enable other <span class="emphasis"><em>unsafe</em></span> sysctls on the node to be available to the user.
			</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#using-node-tuning-operator">Node Tuning Operator</a>
					</li></ul></div><section class="section" id="nodes-containers-sysctls-about_nodes-containers-using"><div class="titlepage"><div><div><h3 class="title">7.9.1. About sysctls</h3></div></div></div><p>
					In Linux, the sysctl interface allows an administrator to modify kernel parameters at runtime. Parameters are available from the <code class="literal"><span class="emphasis"><em>/proc/sys/</em></span></code> virtual process file system. The parameters cover various subsystems, such as:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							kernel (common prefix: <code class="literal"><span class="emphasis"><em>kernel.</em></span></code>)
						</li><li class="listitem">
							networking (common prefix: <code class="literal"><span class="emphasis"><em>net.</em></span></code>)
						</li><li class="listitem">
							virtual memory (common prefix: <code class="literal"><span class="emphasis"><em>vm.</em></span></code>)
						</li><li class="listitem">
							MDADM (common prefix: <code class="literal"><span class="emphasis"><em>dev.</em></span></code>)
						</li></ul></div><p>
					More subsystems are described in <a class="link" href="https://www.kernel.org/doc/Documentation/sysctl/README">Kernel documentation</a>. To get a list of all parameters, run:
				</p><pre class="programlisting language-terminal">$ sudo sysctl -a</pre></section><section class="section" id="namespaced-and-node-level-sysctls"><div class="titlepage"><div><div><h3 class="title">7.9.2. Namespaced and node-level sysctls</h3></div></div></div><p>
					A number of sysctls are <span class="emphasis"><em>namespaced</em></span> in the Linux kernels. This means that you can set them independently for each pod on a node. Being namespaced is a requirement for sysctls to be accessible in a pod context within Kubernetes.
				</p><p>
					The following sysctls are known to be namespaced:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal"><span class="emphasis"><em>kernel.shm*</em></span></code>
						</li><li class="listitem">
							<code class="literal"><span class="emphasis"><em>kernel.msg*</em></span></code>
						</li><li class="listitem">
							<code class="literal"><span class="emphasis"><em>kernel.sem</em></span></code>
						</li><li class="listitem">
							<code class="literal"><span class="emphasis"><em>fs.mqueue.*</em></span></code>
						</li></ul></div><p>
					Additionally, most of the sysctls in the <code class="literal">net.*</code> group are known to be namespaced. Their namespace adoption differs based on the kernel version and distributor.
				</p><p>
					Sysctls that are not namespaced are called <span class="emphasis"><em>node-level</em></span> and must be set manually by the cluster administrator, either by means of the underlying Linux distribution of the nodes, such as by modifying the <code class="literal"><span class="emphasis"><em>/etc/sysctls.conf</em></span></code> file, or by using a daemon set with privileged containers. You can use the Node Tuning Operator to set <span class="emphasis"><em>node-level</em></span> sysctls.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Consider marking nodes with special sysctls as tainted. Only schedule pods onto them that need those sysctl settings. Use the taints and toleration feature to mark the nodes.
					</p></div></div></section><section class="section" id="safe_and_unsafe_sysctls_nodes-containers-using"><div class="titlepage"><div><div><h3 class="title">7.9.3. Safe and unsafe sysctls</h3></div></div></div><p>
					Sysctls are grouped into <span class="emphasis"><em>safe</em></span> and <span class="emphasis"><em>unsafe</em></span> sysctls.
				</p><p>
					For system-wide sysctls to be considered safe, they must be namespaced. A namespaced sysctl ensures there is isolation between namespaces and therefore pods. If you set a sysctl for one pod it must not add any of the following:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Influence any other pod on the node
						</li><li class="listitem">
							Harm the node health
						</li><li class="listitem">
							Gain CPU or memory resources outside of the resource limits of a pod
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Being namespaced alone is not sufficient for the sysctl to be considered safe.
					</p></div></div><p>
					Any sysctl that is not added to the allowed list on OpenShift Container Platform is considered unsafe for OpenShift Container Platform.
				</p><p>
					Unsafe sysctls are not allowed by default. For system-wide sysctls the cluster administrator must manually enable them on a per-node basis. Pods with disabled unsafe sysctls are scheduled but do not launch.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You cannot manually enable interface-specific unsafe sysctls.
					</p></div></div><p>
					OpenShift Container Platform adds the following system-wide and interface-specific safe sysctls to an allowed safe list:
				</p><div class="table" id="idm140232226829536"><p class="title"><strong>Table 7.4. System-wide safe sysctls</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 30%; " class="col_1"><!--Empty--></col><col style="width: 70%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232226824688" scope="col">sysctl</th><th align="left" valign="top" id="idm140232226823600" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232226824688"> <p>
									<code class="literal">kernel.shm_rmid_forced</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226823600"> <p>
									When set to <code class="literal">1</code>, all shared memory objects in current IPC namespace are automatically forced to use IPC_RMID. For more information, see <a class="link" href="https://docs.kernel.org/admin-guide/sysctl/kernel.html?highlight=shm_rmid_forced#shm-rmid-forced">shm_rmid_forced</a>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226824688"> <p>
									<code class="literal">net.ipv4.ip_local_port_range</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226823600"> <p>
									Defines the local port range that is used by TCP and UDP to choose the local port. The first number is the first port number, and the second number is the last local port number. If possible, it is better if these numbers have different parity (one even and one odd value). They must be greater than or equal to <code class="literal">ip_unprivileged_port_start</code>. The default values are <code class="literal">32768</code> and <code class="literal">60999</code> respectively. For more information, see <a class="link" href="https://docs.kernel.org/networking/ip-sysctl.html?highlight=ip_local_port_range#ip-variables">ip_local_port_range</a>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226824688"> <p>
									<code class="literal">net.ipv4.tcp_syncookies</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226823600"> <p>
									When <code class="literal">net.ipv4.tcp_syncookies</code> is set, the kernel handles TCP SYN packets normally until the half-open connection queue is full, at which time, the SYN cookie functionality kicks in. This functionality allows the system to keep accepting valid connections, even if under a denial-of-service attack. For more information, see <a class="link" href="https://docs.kernel.org/networking/ip-sysctl.html?highlight=tcp_syncookies#tcp-variables">tcp_syncookies</a>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226824688"> <p>
									<code class="literal">net.ipv4.ping_group_range</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226823600"> <p>
									This restricts <code class="literal">ICMP_PROTO</code> datagram sockets to users in the group range. The default is <code class="literal">1 0</code>, meaning that nobody, not even root, can create ping sockets. For more information, see <a class="link" href="https://docs.kernel.org/networking/ip-sysctl.html?highlight=ping_group_range#ip-variables">ping_group_range</a>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226824688"> <p>
									<code class="literal">net.ipv4.ip_unprivileged_port_start</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226823600"> <p>
									This defines the first unprivileged port in the network namespace. To disable all privileged ports, set this to <code class="literal">0</code>. Privileged ports must not overlap with the <code class="literal">ip_local_port_range</code>. For more information, see <a class="link" href="https://docs.kernel.org/networking/ip-sysctl.html?highlight=ip_unprivileged_port_start#ip-variables#ip-variables">ip_unprivileged_port_start</a>.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140232228234928"><p class="title"><strong>Table 7.5. Interface-specific safe sysctls</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 30%; " class="col_1"><!--Empty--></col><col style="width: 70%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232228230080" scope="col">sysctl</th><th align="left" valign="top" id="idm140232228228992" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232228230080"> <p>
									<code class="literal">net.ipv4.conf.IFNAME.accept_redirects</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228228992"> <p>
									Accept IPv4 ICMP redirect messages.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228230080"> <p>
									<code class="literal">net.ipv4.conf.IFNAME.accept_source_route</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228228992"> <p>
									Accept IPv4 packets with strict source route (SRR) option.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228230080"> <p>
									<code class="literal">net.ipv4.conf.IFNAME.arp_accept</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228228992"> <p>
									Define behavior for gratuitous ARP frames with an IPv4 address that is not already present in the ARP table:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">0</code> - Do not create new entries in the ARP table.
										</li><li class="listitem">
											<code class="literal">1</code> - Create new entries in the ARP table.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228230080"> <p>
									<code class="literal">net.ipv4.conf.IFNAME.arp_notify</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228228992"> <p>
									Define mode for notification of IPv4 address and device changes.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228230080"> <p>
									<code class="literal">net.ipv4.conf.IFNAME.disable_policy</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228228992"> <p>
									Disable IPSEC policy (SPD) for this IPv4 interface.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228230080"> <p>
									<code class="literal">net.ipv4.conf.IFNAME.secure_redirects</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228228992"> <p>
									Accept ICMP redirect messages only to gateways listed in the interface’s current gateway list.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228230080"> <p>
									<code class="literal">net.ipv4.conf.IFNAME.send_redirects</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228228992"> <p>
									Send redirects is enabled only if the node acts as a router. That is, a host should not send an ICMP redirect message. It is used by routers to notify the host about a better routing path that is available for a particular destination.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228230080"> <p>
									<code class="literal">net.ipv6.conf.IFNAME.accept_ra</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228228992"> <p>
									Accept IPv6 Router advertisements; autoconfigure using them. It also determines whether or not to transmit router solicitations. Router solicitations are transmitted only if the functional setting is to accept router advertisements.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228230080"> <p>
									<code class="literal">net.ipv6.conf.IFNAME.accept_redirects</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228228992"> <p>
									Accept IPv6 ICMP redirect messages.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228230080"> <p>
									<code class="literal">net.ipv6.conf.IFNAME.accept_source_route</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228228992"> <p>
									Accept IPv6 packets with SRR option.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228230080"> <p>
									<code class="literal">net.ipv6.conf.IFNAME.arp_accept</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228228992"> <p>
									Define behavior for gratuitous ARP frames with an IPv6 address that is not already present in the ARP table:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">0</code> - Do not create new entries in the ARP table.
										</li><li class="listitem">
											<code class="literal">1</code> - Create new entries in the ARP table.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228230080"> <p>
									<code class="literal">net.ipv6.conf.IFNAME.arp_notify</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228228992"> <p>
									Define mode for notification of IPv6 address and device changes.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228230080"> <p>
									<code class="literal">net.ipv6.neigh.IFNAME.base_reachable_time_ms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228228992"> <p>
									This parameter controls the hardware address to IP mapping lifetime in the neighbour table for IPv6.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232228230080"> <p>
									<code class="literal">net.ipv6.neigh.IFNAME.retrans_time_ms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232228228992"> <p>
									Set the retransmit timer for neighbor discovery messages.
								</p>
								 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When setting these values using the <code class="literal">tuning</code> CNI plugin, use the value <code class="literal">IFNAME</code> literally. The interface name is represented by the <code class="literal">IFNAME</code> token, and is replaced with the actual name of the interface at runtime.
					</p></div></div></section><section class="section" id="updating-interface-specific-safe-sysctls-list_nodes-containers-using"><div class="titlepage"><div><div><h3 class="title">7.9.4. Updating the interface-specific safe sysctls list</h3></div></div></div><p>
					OpenShift Container Platform includes a predefined list of safe interface-specific <code class="literal">sysctls</code>. You can modify this list by updating the <code class="literal">cni-sysctl-allowlist</code> in the <code class="literal">openshift-multus</code> namespace.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The support for updating the interface-specific safe sysctls list is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
					</p><p>
						For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
					</p></div></div><p>
					Follow this procedure to modify the predefined list of safe <code class="literal">sysctls</code>. This procedure describes how to extend the default allow list.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the existing predefined list by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get cm -n openshift-multus cni-sysctl-allowlist -oyaml</pre><div class="formalpara"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">apiVersion: v1
data:
  allowlist.conf: |-
    ^net.ipv4.conf.IFNAME.accept_redirects$
    ^net.ipv4.conf.IFNAME.accept_source_route$
    ^net.ipv4.conf.IFNAME.arp_accept$
    ^net.ipv4.conf.IFNAME.arp_notify$
    ^net.ipv4.conf.IFNAME.disable_policy$
    ^net.ipv4.conf.IFNAME.secure_redirects$
    ^net.ipv4.conf.IFNAME.send_redirects$
    ^net.ipv6.conf.IFNAME.accept_ra$
    ^net.ipv6.conf.IFNAME.accept_redirects$
    ^net.ipv6.conf.IFNAME.accept_source_route$
    ^net.ipv6.conf.IFNAME.arp_accept$
    ^net.ipv6.conf.IFNAME.arp_notify$
    ^net.ipv6.neigh.IFNAME.base_reachable_time_ms$
    ^net.ipv6.neigh.IFNAME.retrans_time_ms$
kind: ConfigMap
metadata:
  annotations:
    kubernetes.io/description: |
      Sysctl allowlist for nodes.
    release.openshift.io/version: 4.13.0-0.nightly-2022-11-16-003434
  creationTimestamp: "2022-11-17T14:09:27Z"
  name: cni-sysctl-allowlist
  namespace: openshift-multus
  resourceVersion: "2422"
  uid: 96d138a3-160e-4943-90ff-6108fa7c50c3</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Edit the list by using the following command:
						</p><pre class="programlisting language-terminal">$ oc edit cm -n openshift-multus cni-sysctl-allowlist -oyaml</pre><p class="simpara">
							For example, to allow you to be able to implement stricter reverse path forwarding you need to add <code class="literal">^net.ipv4.conf.IFNAME.rp_filter$</code> and <code class="literal">^net.ipv6.conf.IFNAME.rp_filter$</code> to the list as shown here:
						</p><pre class="programlisting language-terminal"># Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  allowlist.conf: |-
    ^net.ipv4.conf.IFNAME.accept_redirects$
    ^net.ipv4.conf.IFNAME.accept_source_route$
    ^net.ipv4.conf.IFNAME.arp_accept$
    ^net.ipv4.conf.IFNAME.arp_notify$
    ^net.ipv4.conf.IFNAME.disable_policy$
    ^net.ipv4.conf.IFNAME.secure_redirects$
    ^net.ipv4.conf.IFNAME.send_redirects$
    ^net.ipv4.conf.IFNAME.rp_filter$
    ^net.ipv6.conf.IFNAME.accept_ra$
    ^net.ipv6.conf.IFNAME.accept_redirects$
    ^net.ipv6.conf.IFNAME.accept_source_route$
    ^net.ipv6.conf.IFNAME.arp_accept$
    ^net.ipv6.conf.IFNAME.arp_notify$
    ^net.ipv6.neigh.IFNAME.base_reachable_time_ms$
    ^net.ipv6.neigh.IFNAME.retrans_time_ms$
    ^net.ipv6.conf.IFNAME.rp_filter$</pre></li><li class="listitem"><p class="simpara">
							Save the changes to the file and exit.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The removal of <code class="literal">sysctls</code> is also supported. Edit the file, remove the <code class="literal">sysctl</code> or <code class="literal">sysctls</code> then save the changes and exit.
							</p></div></div></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						Follow this procedure to enforce stricter reverse path forwarding for IPv4. For more information on reverse path forwarding see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/security_guide/sect-security_guide-server_security-reverse_path_forwarding">Reverse Path Forwarding </a>.
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a network attachment definition, such as <code class="literal">reverse-path-fwd-example.yaml</code>, with the following content:
						</p><pre class="programlisting language-yaml">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: tuningnad
  namespace: default
spec:
  config: '{
    "cniVersion": "0.4.0",
    "name": "tuningnad",
    "plugins": [{
      "type": "bridge"
      },
      {
      "type": "tuning",
      "sysctl": {
         "net.ipv4.conf.IFNAME.rp_filter": "1"
        }
    }
  ]
}'</pre></li><li class="listitem"><p class="simpara">
							Apply the yaml by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f reverse-path-fwd-example.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">networkattachmentdefinition.k8.cni.cncf.io/tuningnad created</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create a pod such as <code class="literal">examplepod.yaml</code> using the following YAML:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: example
  labels:
    app: httpd
  namespace: default
  annotations:
    k8s.v1.cni.cncf.io/networks: tuningnad  <span id="CO142-1"><!--Empty--></span><span class="callout">1</span>
spec:
  securityContext:
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
  containers:
    - name: httpd
      image: 'image-registry.openshift-image-registry.svc:5000/openshift/httpd:latest'
      ports:
        - containerPort: 8080
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
            - ALL</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO142-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the name of the configured <code class="literal">NetworkAttachmentDefinition</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Apply the yaml by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f examplepod.yaml</pre></li><li class="listitem"><p class="simpara">
							Verify that the pod is created by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get pod</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      READY   STATUS    RESTARTS   AGE
example   1/1     Running   0          47s</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Log in to the pod by running the following command:
						</p><pre class="programlisting language-terminal">$ oc rsh example</pre></li><li class="listitem"><p class="simpara">
							Verify the value of the configured sysctl flag. For example, find the value <code class="literal">net.ipv4.conf.net1.rp_filter</code> by running the following command:
						</p><pre class="programlisting language-terminal">sh-4.4# sysctl net.ipv4.conf.net1.rp_filter</pre><div class="formalpara"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">net.ipv4.conf.net1.rp_filter = 1</pre>

							</p></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-configuring-tuning-cni_set-networkinterface-sysctls">Configuring the tuning CNI</a>
						</li><li class="listitem">
							<a class="link" href="https://docs.kernel.org/networking/ip-sysctl.html">Linux networking documentation</a>
						</li></ul></div></section><section class="section" id="nodes-starting-pod-safe-sysctls_nodes-containers-using"><div class="titlepage"><div><div><h3 class="title">7.9.5. Starting a pod with safe sysctls</h3></div></div></div><p>
					You can set sysctls on pods using the pod’s <code class="literal">securityContext</code>. The <code class="literal">securityContext</code> applies to all containers in the same pod.
				</p><p>
					Safe sysctls are allowed by default.
				</p><p>
					This example uses the pod <code class="literal">securityContext</code> to set the following safe sysctls:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">kernel.shm_rmid_forced</code>
						</li><li class="listitem">
							<code class="literal">net.ipv4.ip_local_port_range</code>
						</li><li class="listitem">
							<code class="literal">net.ipv4.tcp_syncookies</code>
						</li><li class="listitem">
							<code class="literal">net.ipv4.ping_group_range</code>
						</li></ul></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						To avoid destabilizing your operating system, modify sysctl parameters only after you understand their effects.
					</p></div></div><p>
					Use this procedure to start a pod with the configured sysctl settings.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						In most cases you modify an existing pod definition and add the <code class="literal">securityContext</code> spec.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a YAML file <code class="literal">sysctl_pod.yaml</code> that defines an example pod and add the <code class="literal">securityContext</code> spec, as shown in the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: sysctl-example
  namespace: default
spec:
  containers:
  - name: podexample
    image: centos
    command: ["bin/bash", "-c", "sleep INF"]
    securityContext:
      runAsUser: 2000 <span id="CO143-1"><!--Empty--></span><span class="callout">1</span>
      runAsGroup: 3000 <span id="CO143-2"><!--Empty--></span><span class="callout">2</span>
      allowPrivilegeEscalation: false <span id="CO143-3"><!--Empty--></span><span class="callout">3</span>
      capabilities: <span id="CO143-4"><!--Empty--></span><span class="callout">4</span>
        drop: ["ALL"]
  securityContext:
    runAsNonRoot: true <span id="CO143-5"><!--Empty--></span><span class="callout">5</span>
    seccompProfile: <span id="CO143-6"><!--Empty--></span><span class="callout">6</span>
      type: RuntimeDefault
    sysctls:
    - name: kernel.shm_rmid_forced
      value: "1"
    - name: net.ipv4.ip_local_port_range
      value: "32770       60666"
    - name: net.ipv4.tcp_syncookies
      value: "0"
    - name: net.ipv4.ping_group_range
      value: "0           200000000"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO143-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal">runAsUser</code> controls which user ID the container is run with.
								</div></dd><dt><a href="#CO143-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									<code class="literal">runAsGroup</code> controls which primary group ID the containers is run with.
								</div></dd><dt><a href="#CO143-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									<code class="literal">allowPrivilegeEscalation</code> determines if a pod can request to allow privilege escalation. If unspecified, it defaults to true. This boolean directly controls whether the <code class="literal">no_new_privs</code> flag gets set on the container process.
								</div></dd><dt><a href="#CO143-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									<code class="literal">capabilities</code> permit privileged actions without giving full root access. This policy ensures all capabilities are dropped from the pod.
								</div></dd><dt><a href="#CO143-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									<code class="literal">runAsNonRoot: true</code> requires that the container will run with a user with any UID other than 0.
								</div></dd><dt><a href="#CO143-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									<code class="literal">RuntimeDefault</code> enables the default seccomp profile for a pod or container workload.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the pod by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f sysctl_pod.yaml</pre></li><li class="listitem"><p class="simpara">
							Verify that the pod is created by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get pod</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME              READY   STATUS            RESTARTS   AGE
sysctl-example    1/1     Running           0          14s</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Log in to the pod by running the following command:
						</p><pre class="programlisting language-terminal">$ oc rsh sysctl-example</pre></li><li class="listitem"><p class="simpara">
							Verify the values of the configured sysctl flags. For example, find the value <code class="literal">kernel.shm_rmid_forced</code> by running the following command:
						</p><pre class="programlisting language-terminal">sh-4.4# sysctl kernel.shm_rmid_forced</pre><div class="formalpara"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">kernel.shm_rmid_forced = 1</pre>

							</p></div></li></ol></div></section><section class="section" id="nodes-containers-starting-pod-with-unsafe-sysctls_nodes-containers-using"><div class="titlepage"><div><div><h3 class="title">7.9.6. Starting a pod with unsafe sysctls</h3></div></div></div><p>
					A pod with unsafe sysctls fails to launch on any node unless the cluster administrator explicitly enables unsafe sysctls for that node. As with node-level sysctls, use the taints and toleration feature or labels on nodes to schedule those pods onto the right nodes.
				</p><p>
					The following example uses the pod <code class="literal">securityContext</code> to set a safe sysctl <code class="literal">kernel.shm_rmid_forced</code> and two unsafe sysctls, <code class="literal">net.core.somaxconn</code> and <code class="literal">kernel.msgmax</code>. There is no distinction between <span class="emphasis"><em>safe</em></span> and <span class="emphasis"><em>unsafe</em></span> sysctls in the specification.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						To avoid destabilizing your operating system, modify sysctl parameters only after you understand their effects.
					</p></div></div><p>
					The following example illustrates what happens when you add safe and unsafe sysctls to a pod specification:
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a YAML file <code class="literal">sysctl-example-unsafe.yaml</code> that defines an example pod and add the <code class="literal">securityContext</code> specification, as shown in the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: sysctl-example-unsafe
spec:
  containers:
  - name: podexample
    image: centos
    command: ["bin/bash", "-c", "sleep INF"]
    securityContext:
      runAsUser: 2000
      runAsGroup: 3000
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
  securityContext:
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
    sysctls:
    - name: kernel.shm_rmid_forced
      value: "0"
    - name: net.core.somaxconn
      value: "1024"
    - name: kernel.msgmax
      value: "65536"</pre></li><li class="listitem"><p class="simpara">
							Create the pod using the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f sysctl-example-unsafe.yaml</pre></li><li class="listitem"><p class="simpara">
							Verify that the pod is scheduled but does not deploy because unsafe sysctls are not allowed for the node using the following command:
						</p><pre class="programlisting language-terminal">$ oc get pod</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                       READY             STATUS            RESTARTS   AGE
sysctl-example-unsafe      0/1               SysctlForbidden   0          14s</pre>

							</p></div></li></ol></div></section><section class="section" id="nodes-containers-sysctls-unsafe_nodes-containers-using"><div class="titlepage"><div><div><h3 class="title">7.9.7. Enabling unsafe sysctls</h3></div></div></div><p>
					A cluster administrator can allow certain unsafe sysctls for very special situations such as high performance or real-time application tuning.
				</p><p>
					If you want to use unsafe sysctls, a cluster administrator must enable them individually for a specific type of node. The sysctls must be namespaced.
				</p><p>
					You can further control which sysctls are set in pods by specifying lists of sysctls or sysctl patterns in the <code class="literal">allowedUnsafeSysctls</code> field of the Security Context Constraints.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The <code class="literal">allowedUnsafeSysctls</code> option controls specific needs such as high performance or real-time application tuning.
						</li></ul></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Due to their nature of being unsafe, the use of unsafe sysctls is at-your-own-risk and can lead to severe problems, such as improper behavior of containers, resource shortage, or breaking a node.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							List existing MachineConfig objects for your OpenShift Container Platform cluster to decide how to label your machine config by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineconfigpool</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master   rendered-master-bfb92f0cd1684e54d8e234ab7423cc96   True      False      False      3              3                   3                     0                      42m
worker   rendered-worker-21b6cb9a0f8919c88caf39db80ac1fce   True      False      False      3              3                   3                     0                      42m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Add a label to the machine config pool where the containers with the unsafe sysctls will run by running the following command:
						</p><pre class="programlisting language-terminal">$ oc label machineconfigpool worker custom-kubelet=sysctl</pre></li><li class="listitem"><p class="simpara">
							Create a YAML file <code class="literal">set-sysctl-worker.yaml</code> that defines a <code class="literal">KubeletConfig</code> custom resource (CR):
						</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: custom-kubelet
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: sysctl <span id="CO144-1"><!--Empty--></span><span class="callout">1</span>
  kubeletConfig:
    allowedUnsafeSysctls: <span id="CO144-2"><!--Empty--></span><span class="callout">2</span>
      - "kernel.msg*"
      - "net.core.somaxconn"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO144-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the label from the machine config pool.
								</div></dd><dt><a href="#CO144-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									List the unsafe sysctls you want to allow.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the object by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f set-sysctl-worker.yaml</pre></li><li class="listitem"><p class="simpara">
							Wait for the Machine Config Operator to generate the new rendered configuration and apply it to the machines by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get machineconfigpool worker -w</pre><p class="simpara">
							After some minutes the <code class="literal">UPDATING</code> status changes from True to False:
						</p><pre class="programlisting language-terminal">NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
worker   rendered-worker-f1704a00fc6f30d3a7de9a15fd68a800   False     True       False      3              2                   2                     0                      71m
worker   rendered-worker-f1704a00fc6f30d3a7de9a15fd68a800   False     True       False      3              2                   3                     0                      72m
worker   rendered-worker-0188658afe1f3a183ec8c4f14186f4d5   True      False      False      3              3                   3                     0                      72m</pre></li><li class="listitem"><p class="simpara">
							Create a YAML file <code class="literal">sysctl-example-safe-unsafe.yaml</code> that defines an example pod and add the <code class="literal">securityContext</code> spec, as shown in the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: sysctl-example-safe-unsafe
spec:
  containers:
  - name: podexample
    image: centos
    command: ["bin/bash", "-c", "sleep INF"]
    securityContext:
      runAsUser: 2000
      runAsGroup: 3000
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
  securityContext:
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
    sysctls:
    - name: kernel.shm_rmid_forced
      value: "0"
    - name: net.core.somaxconn
      value: "1024"
    - name: kernel.msgmax
      value: "65536"</pre></li><li class="listitem"><p class="simpara">
							Create the pod by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f sysctl-example-safe-unsafe.yaml</pre><div class="formalpara"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">Warning: would violate PodSecurity "restricted:latest": forbidden sysctls (net.core.somaxconn, kernel.msgmax)
pod/sysctl-example-safe-unsafe created</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify that the pod is created by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get pod</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                         READY   STATUS    RESTARTS   AGE
sysctl-example-safe-unsafe   1/1     Running   0          19s</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Log in to the pod by running the following command:
						</p><pre class="programlisting language-terminal">$ oc rsh sysctl-example-safe-unsafe</pre></li><li class="listitem"><p class="simpara">
							Verify the values of the configured sysctl flags. For example, find the value <code class="literal">net.core.somaxconn</code> by running the following command:
						</p><pre class="programlisting language-terminal">sh-4.4# sysctl net.core.somaxconn</pre><div class="formalpara"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">net.core.somaxconn = 1024</pre>

							</p></div></li></ol></div><p>
					The unsafe sysctl is now allowed and the value is set as defined in the <code class="literal">securityContext</code> spec of the updated pod specification.
				</p></section><section class="section _additional-resources" id="additional-resources_nodes-containers-sysctls"><div class="titlepage"><div><div><h3 class="title">7.9.8. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nodes-setting-interface-level-network-sysctls">Setting interface-level network sysctls</a>
						</li></ul></div></section></section></section><section class="chapter" id="working-with-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 8. Working with clusters</h1></div></div></div><section class="section" id="nodes-containers-events"><div class="titlepage"><div><div><h2 class="title">8.1. Viewing system event information in an OpenShift Container Platform cluster</h2></div></div></div><p>
				Events in OpenShift Container Platform are modeled based on events that happen to API objects in an OpenShift Container Platform cluster.
			</p><section class="section" id="nodes-containers-events-about_nodes-containers-events"><div class="titlepage"><div><div><h3 class="title">8.1.1. Understanding events</h3></div></div></div><p>
					Events allow OpenShift Container Platform to record information about real-world events in a resource-agnostic manner. They also allow developers and administrators to consume information about system components in a unified way.
				</p></section><section class="section" id="nodes-containers-events-viewing-cli_nodes-containers-events"><div class="titlepage"><div><div><h3 class="title">8.1.2. Viewing events using the CLI</h3></div></div></div><p>
					You can get a list of events in a given project using the CLI.
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To view events in a project use the following command:
						</p><pre class="programlisting language-terminal">$ oc get events [-n &lt;project&gt;] <span id="CO145-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO145-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the project.
								</div></dd></dl></div><p class="simpara">
							For example:
						</p><pre class="programlisting language-terminal">$ oc get events -n openshift-config</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">LAST SEEN   TYPE      REASON                   OBJECT                      MESSAGE
97m         Normal    Scheduled                pod/dapi-env-test-pod       Successfully assigned openshift-config/dapi-env-test-pod to ip-10-0-171-202.ec2.internal
97m         Normal    Pulling                  pod/dapi-env-test-pod       pulling image "gcr.io/google_containers/busybox"
97m         Normal    Pulled                   pod/dapi-env-test-pod       Successfully pulled image "gcr.io/google_containers/busybox"
97m         Normal    Created                  pod/dapi-env-test-pod       Created container
9m5s        Warning   FailedCreatePodSandBox   pod/dapi-volume-test-pod    Failed create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_dapi-volume-test-pod_openshift-config_6bc60c1f-452e-11e9-9140-0eec59c23068_0(748c7a40db3d08c07fb4f9eba774bd5effe5f0d5090a242432a73eee66ba9e22): Multus: Err adding pod to network "openshift-sdn": cannot set "openshift-sdn" ifname to "eth0": no netns: failed to Statfs "/proc/33366/ns/net": no such file or directory
8m31s       Normal    Scheduled                pod/dapi-volume-test-pod    Successfully assigned openshift-config/dapi-volume-test-pod to ip-10-0-171-202.ec2.internal</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To view events in your project from the OpenShift Container Platform console.
						</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
									Launch the OpenShift Container Platform console.
								</li><li class="listitem">
									Click <span class="strong strong"><strong>Home</strong></span> → <span class="strong strong"><strong>Events</strong></span> and select your project.
								</li><li class="listitem"><p class="simpara">
									Move to resource that you want to see events. For example: <span class="strong strong"><strong>Home</strong></span> → <span class="strong strong"><strong>Projects</strong></span> → &lt;project-name&gt; → &lt;resource-name&gt;.
								</p><p class="simpara">
									Many objects, such as pods and deployments, have their own <span class="strong strong"><strong>Events</strong></span> tab as well, which shows events related to that object.
								</p></li></ol></div></li></ul></div></section><section class="section" id="nodes-containers-events-list_nodes-containers-events"><div class="titlepage"><div><div><h3 class="title">8.1.3. List of events</h3></div></div></div><p>
					This section describes the events of OpenShift Container Platform.
				</p><div class="table" id="idm140232227299088"><p class="title"><strong>Table 8.1. Configuration events</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232227294256" scope="col">Name</th><th align="left" valign="top" id="idm140232227293168" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232227294256"> <p>
									<code class="literal">FailedValidation</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227293168"> <p>
									Failed pod configuration validation.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140232227286896"><p class="title"><strong>Table 8.2. Container events</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232227282064" scope="col">Name</th><th align="left" valign="top" id="idm140232227280976" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232227282064"> <p>
									<code class="literal">BackOff</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227280976"> <p>
									Back-off restarting failed the container.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227282064"> <p>
									<code class="literal">Created</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227280976"> <p>
									Container created.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227282064"> <p>
									<code class="literal">Failed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227280976"> <p>
									Pull/Create/Start failed.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227282064"> <p>
									<code class="literal">Killing</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227280976"> <p>
									Killing the container.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227282064"> <p>
									<code class="literal">Started</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227280976"> <p>
									Container started.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227282064"> <p>
									<code class="literal">Preempting</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227280976"> <p>
									Preempting other pods.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227282064"> <p>
									<code class="literal">ExceededGracePeriod</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227280976"> <p>
									Container runtime did not stop the pod within specified grace period.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140232227249840"><p class="title"><strong>Table 8.3. Health events</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232227245008" scope="col">Name</th><th align="left" valign="top" id="idm140232227243920" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232227245008"> <p>
									<code class="literal">Unhealthy</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227243920"> <p>
									Container is unhealthy.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140232227237584"><p class="title"><strong>Table 8.4. Image events</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232227232752" scope="col">Name</th><th align="left" valign="top" id="idm140232227231664" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232227232752"> <p>
									<code class="literal">BackOff</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227231664"> <p>
									Back off Ctr Start, image pull.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227232752"> <p>
									<code class="literal">ErrImageNeverPull</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227231664"> <p>
									The image’s <span class="strong strong"><strong>NeverPull Policy</strong></span> is violated.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227232752"> <p>
									<code class="literal">Failed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227231664"> <p>
									Failed to pull the image.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227232752"> <p>
									<code class="literal">InspectFailed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227231664"> <p>
									Failed to inspect the image.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227232752"> <p>
									<code class="literal">Pulled</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227231664"> <p>
									Successfully pulled the image or the container image is already present on the machine.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227232752"> <p>
									<code class="literal">Pulling</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227231664"> <p>
									Pulling the image.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140232226314064"><p class="title"><strong>Table 8.5. Image Manager events</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232226309232" scope="col">Name</th><th align="left" valign="top" id="idm140232226308144" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232226309232"> <p>
									<code class="literal">FreeDiskSpaceFailed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226308144"> <p>
									Free disk space failed.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226309232"> <p>
									<code class="literal">InvalidDiskCapacity</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226308144"> <p>
									Invalid disk capacity.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140232226297680"><p class="title"><strong>Table 8.6. Node events</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232226292848" scope="col">Name</th><th align="left" valign="top" id="idm140232226291760" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">FailedMount</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Volume mount failed.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">HostNetworkNotSupported</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Host network not supported.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">HostPortConflict</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Host/port conflict.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">KubeletSetupFailed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Kubelet setup failed.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">NilShaper</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Undefined shaper.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">NodeNotReady</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Node is not ready.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">NodeNotSchedulable</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Node is not schedulable.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">NodeReady</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Node is ready.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">NodeSchedulable</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Node is schedulable.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">NodeSelectorMismatching</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Node selector mismatch.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">OutOfDisk</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Out of disk.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">Rebooted</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Node rebooted.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">Starting</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Starting kubelet.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">FailedAttachVolume</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Failed to attach volume.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">FailedDetachVolume</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Failed to detach volume.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">VolumeResizeFailed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Failed to expand/reduce volume.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">VolumeResizeSuccessful</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Successfully expanded/reduced volume.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">FileSystemResizeFailed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Failed to expand/reduce file system.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">FileSystemResizeSuccessful</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Successfully expanded/reduced file system.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">FailedUnMount</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Failed to unmount volume.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">FailedMapVolume</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Failed to map a volume.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">FailedUnmapDevice</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Failed unmaped device.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">AlreadyMountedVolume</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Volume is already mounted.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">SuccessfulDetachVolume</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Volume is successfully detached.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">SuccessfulMountVolume</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Volume is successfully mounted.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">SuccessfulUnMountVolume</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Volume is successfully unmounted.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">ContainerGCFailed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Container garbage collection failed.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">ImageGCFailed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Image garbage collection failed.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">FailedNodeAllocatableEnforcement</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Failed to enforce System Reserved Cgroup limit.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">NodeAllocatableEnforced</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Enforced System Reserved Cgroup limit.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">UnsupportedMountOption</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Unsupported mount option.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">SandboxChanged</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Pod sandbox changed.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">FailedCreatePodSandBox</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Failed to create pod sandbox.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226292848"> <p>
									<code class="literal">FailedPodSandBoxStatus</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226291760"> <p>
									Failed pod sandbox status.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140232229090560"><p class="title"><strong>Table 8.7. Pod worker events</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232229085728" scope="col">Name</th><th align="left" valign="top" id="idm140232229084640" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232229085728"> <p>
									<code class="literal">FailedSync</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229084640"> <p>
									Pod sync failed.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140232229078320"><p class="title"><strong>Table 8.8. System Events</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232229073488" scope="col">Name</th><th align="left" valign="top" id="idm140232229072400" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232229073488"> <p>
									<code class="literal">SystemOOM</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229072400"> <p>
									There is an OOM (out of memory) situation on the cluster.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140232229066032"><p class="title"><strong>Table 8.9. Pod events</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232229061200" scope="col">Name</th><th align="left" valign="top" id="idm140232229060112" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232229061200"> <p>
									<code class="literal">FailedKillPod</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229060112"> <p>
									Failed to stop a pod.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229061200"> <p>
									<code class="literal">FailedCreatePodContainer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229060112"> <p>
									Failed to create a pod container.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229061200"> <p>
									<code class="literal">Failed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229060112"> <p>
									Failed to make pod data directories.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229061200"> <p>
									<code class="literal">NetworkNotReady</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229060112"> <p>
									Network is not ready.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229061200"> <p>
									<code class="literal">FailedCreate</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229060112"> <p>
									Error creating: <code class="literal">&lt;error-msg&gt;</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229061200"> <p>
									<code class="literal">SuccessfulCreate</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229060112"> <p>
									Created pod: <code class="literal">&lt;pod-name&gt;</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229061200"> <p>
									<code class="literal">FailedDelete</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229060112"> <p>
									Error deleting: <code class="literal">&lt;error-msg&gt;</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229061200"> <p>
									<code class="literal">SuccessfulDelete</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229060112"> <p>
									Deleted pod: <code class="literal">&lt;pod-id&gt;</code>.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140232226030992"><p class="title"><strong>Table 8.10. Horizontal Pod AutoScaler events</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232226026144" scope="col">Name</th><th align="left" valign="top" id="idm140232226025056" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232226026144"> <p>
									SelectorRequired
								</p>
								 </td><td align="left" valign="top" headers="idm140232226025056"> <p>
									Selector is required.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226026144"> <p>
									<code class="literal">InvalidSelector</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226025056"> <p>
									Could not convert selector into a corresponding internal selector object.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226026144"> <p>
									<code class="literal">FailedGetObjectMetric</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226025056"> <p>
									HPA was unable to compute the replica count.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226026144"> <p>
									<code class="literal">InvalidMetricSourceType</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226025056"> <p>
									Unknown metric source type.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226026144"> <p>
									<code class="literal">ValidMetricFound</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226025056"> <p>
									HPA was able to successfully calculate a replica count.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226026144"> <p>
									<code class="literal">FailedConvertHPA</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226025056"> <p>
									Failed to convert the given HPA.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226026144"> <p>
									<code class="literal">FailedGetScale</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226025056"> <p>
									HPA controller was unable to get the target’s current scale.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226026144"> <p>
									<code class="literal">SucceededGetScale</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226025056"> <p>
									HPA controller was able to get the target’s current scale.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226026144"> <p>
									<code class="literal">FailedComputeMetricsReplicas</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226025056"> <p>
									Failed to compute desired number of replicas based on listed metrics.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226026144"> <p>
									<code class="literal">FailedRescale</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226025056"> <p>
									New size: <code class="literal">&lt;size&gt;</code>; reason: <code class="literal">&lt;msg&gt;</code>; error: <code class="literal">&lt;error-msg&gt;</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226026144"> <p>
									<code class="literal">SuccessfulRescale</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226025056"> <p>
									New size: <code class="literal">&lt;size&gt;</code>; reason: <code class="literal">&lt;msg&gt;</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232226026144"> <p>
									<code class="literal">FailedUpdateStatus</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232226025056"> <p>
									Failed to update status.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140232225971056"><p class="title"><strong>Table 8.11. Network events (openshift-sdn)</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232225966208" scope="col">Name</th><th align="left" valign="top" id="idm140232225965120" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232225966208"> <p>
									<code class="literal">Starting</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232225965120"> <p>
									Starting OpenShift SDN.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232225966208"> <p>
									<code class="literal">NetworkFailed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232225965120"> <p>
									The pod’s network interface has been lost and the pod will be stopped.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140232229901184"><p class="title"><strong>Table 8.12. Network events (kube-proxy)</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232229896384" scope="col">Name</th><th align="left" valign="top" id="idm140232229895296" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232229896384"> <p>
									<code class="literal">NeedPods</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229895296"> <p>
									The service-port <code class="literal">&lt;serviceName&gt;:&lt;port&gt;</code> needs pods.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140232229888480"><p class="title"><strong>Table 8.13. Volume events</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232229883648" scope="col">Name</th><th align="left" valign="top" id="idm140232229882560" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232229883648"> <p>
									<code class="literal">FailedBinding</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229882560"> <p>
									There are no persistent volumes available and no storage class is set.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229883648"> <p>
									<code class="literal">VolumeMismatch</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229882560"> <p>
									Volume size or class is different from what is requested in claim.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229883648"> <p>
									<code class="literal">VolumeFailedRecycle</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229882560"> <p>
									Error creating recycler pod.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229883648"> <p>
									<code class="literal">VolumeRecycled</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229882560"> <p>
									Occurs when volume is recycled.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229883648"> <p>
									<code class="literal">RecyclerPod</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229882560"> <p>
									Occurs when pod is recycled.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229883648"> <p>
									<code class="literal">VolumeDelete</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229882560"> <p>
									Occurs when volume is deleted.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229883648"> <p>
									<code class="literal">VolumeFailedDelete</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229882560"> <p>
									Error when deleting the volume.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229883648"> <p>
									<code class="literal">ExternalProvisioning</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229882560"> <p>
									Occurs when volume for the claim is provisioned either manually or via external software.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229883648"> <p>
									<code class="literal">ProvisioningFailed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229882560"> <p>
									Failed to provision volume.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229883648"> <p>
									<code class="literal">ProvisioningCleanupFailed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229882560"> <p>
									Error cleaning provisioned volume.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229883648"> <p>
									<code class="literal">ProvisioningSucceeded</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229882560"> <p>
									Occurs when the volume is provisioned successfully.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229883648"> <p>
									<code class="literal">WaitForFirstConsumer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229882560"> <p>
									Delay binding until pod scheduling.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140232229830640"><p class="title"><strong>Table 8.14. Lifecycle hooks</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232229825808" scope="col">Name</th><th align="left" valign="top" id="idm140232229824720" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232229825808"> <p>
									<code class="literal">FailedPostStartHook</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229824720"> <p>
									Handler failed for pod start.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229825808"> <p>
									<code class="literal">FailedPreStopHook</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229824720"> <p>
									Handler failed for pre-stop.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232229825808"> <p>
									<code class="literal">UnfinishedPreStopHook</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232229824720"> <p>
									Pre-stop hook unfinished.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140232227907024"><p class="title"><strong>Table 8.15. Deployments</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232227902192" scope="col">Name</th><th align="left" valign="top" id="idm140232227901104" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232227902192"> <p>
									<code class="literal">DeploymentCancellationFailed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227901104"> <p>
									Failed to cancel deployment.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227902192"> <p>
									<code class="literal">DeploymentCancelled</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227901104"> <p>
									Canceled deployment.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227902192"> <p>
									<code class="literal">DeploymentCreated</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227901104"> <p>
									Created new replication controller.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227902192"> <p>
									<code class="literal">IngressIPRangeFull</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227901104"> <p>
									No available Ingress IP to allocate to service.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140232227882416"><p class="title"><strong>Table 8.16. Scheduler events</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232227877584" scope="col">Name</th><th align="left" valign="top" id="idm140232227876496" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232227877584"> <p>
									<code class="literal">FailedScheduling</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227876496"> <p>
									Failed to schedule pod: <code class="literal">&lt;pod-namespace&gt;/&lt;pod-name&gt;</code>. This event is raised for multiple reasons, for example: <code class="literal">AssumePodVolumes</code> failed, Binding rejected etc.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227877584"> <p>
									<code class="literal">Preempted</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227876496"> <p>
									By <code class="literal">&lt;preemptor-namespace&gt;/&lt;preemptor-name&gt;</code> on node <code class="literal">&lt;node-name&gt;</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227877584"> <p>
									<code class="literal">Scheduled</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227876496"> <p>
									Successfully assigned <code class="literal">&lt;pod-name&gt;</code> to <code class="literal">&lt;node-name&gt;</code>.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140232227858976"><p class="title"><strong>Table 8.17. Daemon set events</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232227854144" scope="col">Name</th><th align="left" valign="top" id="idm140232227853056" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232227854144"> <p>
									<code class="literal">SelectingAll</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227853056"> <p>
									This daemon set is selecting all pods. A non-empty selector is required.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227854144"> <p>
									<code class="literal">FailedPlacement</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227853056"> <p>
									Failed to place pod on <code class="literal">&lt;node-name&gt;</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227854144"> <p>
									<code class="literal">FailedDaemonPod</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227853056"> <p>
									Found failed daemon pod <code class="literal">&lt;pod-name&gt;</code> on node <code class="literal">&lt;node-name&gt;</code>, will try to kill it.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140232227836992"><p class="title"><strong>Table 8.18. LoadBalancer service events</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 80%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232227832144" scope="col">Name</th><th align="left" valign="top" id="idm140232227831056" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232227832144"> <p>
									<code class="literal">CreatingLoadBalancerFailed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227831056"> <p>
									Error creating load balancer.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227832144"> <p>
									<code class="literal">DeletingLoadBalancer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227831056"> <p>
									Deleting load balancer.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227832144"> <p>
									<code class="literal">EnsuringLoadBalancer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227831056"> <p>
									Ensuring load balancer.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227832144"> <p>
									<code class="literal">EnsuredLoadBalancer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227831056"> <p>
									Ensured load balancer.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227832144"> <p>
									<code class="literal">UnAvailableLoadBalancer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227831056"> <p>
									There are no available nodes for <code class="literal">LoadBalancer</code> service.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227832144"> <p>
									<code class="literal">LoadBalancerSourceRanges</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227831056"> <p>
									Lists the new <code class="literal">LoadBalancerSourceRanges</code>. For example, <code class="literal">&lt;old-source-range&gt; → &lt;new-source-range&gt;</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227832144"> <p>
									<code class="literal">LoadbalancerIP</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227831056"> <p>
									Lists the new IP address. For example, <code class="literal">&lt;old-ip&gt; → &lt;new-ip&gt;</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227832144"> <p>
									<code class="literal">ExternalIP</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227831056"> <p>
									Lists external IP address. For example, <code class="literal">Added: &lt;external-ip&gt;</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227832144"> <p>
									<code class="literal">UID</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227831056"> <p>
									Lists the new UID. For example, <code class="literal">&lt;old-service-uid&gt; → &lt;new-service-uid&gt;</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227832144"> <p>
									<code class="literal">ExternalTrafficPolicy</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227831056"> <p>
									Lists the new <code class="literal">ExternalTrafficPolicy</code>. For example, <code class="literal">&lt;old-policy&gt; → &lt;new-policy&gt;</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227832144"> <p>
									<code class="literal">HealthCheckNodePort</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227831056"> <p>
									Lists the new <code class="literal">HealthCheckNodePort</code>. For example, <code class="literal">&lt;old-node-port&gt; → new-node-port&gt;</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227832144"> <p>
									<code class="literal">UpdatedLoadBalancer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227831056"> <p>
									Updated load balancer with new hosts.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227832144"> <p>
									<code class="literal">LoadBalancerUpdateFailed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227831056"> <p>
									Error updating load balancer with new hosts.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227832144"> <p>
									<code class="literal">DeletingLoadBalancer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227831056"> <p>
									Deleting load balancer.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227832144"> <p>
									<code class="literal">DeletingLoadBalancerFailed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227831056"> <p>
									Error deleting load balancer.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232227832144"> <p>
									<code class="literal">DeletedLoadBalancer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140232227831056"> <p>
									Deleted load balancer.
								</p>
								 </td></tr></tbody></table></div></div></section></section><section class="section" id="nodes-cluster-resource-levels"><div class="titlepage"><div><div><h2 class="title">8.2. Estimating the number of pods your OpenShift Container Platform nodes can hold</h2></div></div></div><p>
				As a cluster administrator, you can use the OpenShift Cluster Capacity Tool to view the number of pods that can be scheduled to increase the current resources before they become exhausted, and to ensure any future pods can be scheduled. This capacity comes from an individual node host in a cluster, and includes CPU, memory, disk space, and others.
			</p><section class="section" id="nodes-cluster-resource-levels-about_nodes-cluster-resource-levels"><div class="titlepage"><div><div><h3 class="title">8.2.1. Understanding the OpenShift Cluster Capacity Tool</h3></div></div></div><p>
					The OpenShift Cluster Capacity Tool simulates a sequence of scheduling decisions to determine how many instances of an input pod can be scheduled on the cluster before it is exhausted of resources to provide a more accurate estimation.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The remaining allocatable capacity is a rough estimation, because it does not count all of the resources being distributed among nodes. It analyzes only the remaining resources and estimates the available capacity that is still consumable in terms of a number of instances of a pod with given requirements that can be scheduled in a cluster.
					</p><p>
						Also, pods might only have scheduling support on particular sets of nodes based on its selection and affinity criteria. As a result, the estimation of which remaining pods a cluster can schedule can be difficult.
					</p></div></div><p>
					You can run the OpenShift Cluster Capacity Tool as a stand-alone utility from the command line, or as a job in a pod inside an OpenShift Container Platform cluster. Running the tool as job inside of a pod enables you to run it multiple times without intervention.
				</p></section><section class="section" id="nodes-cluster-resource-levels-command_nodes-cluster-resource-levels"><div class="titlepage"><div><div><h3 class="title">8.2.2. Running the OpenShift Cluster Capacity Tool on the command line</h3></div></div></div><p>
					You can run the OpenShift Cluster Capacity Tool from the command line to estimate the number of pods that can be scheduled onto your cluster.
				</p><p>
					You create a sample pod spec file, which the tool uses for estimating resource usage. The pod spec specifies its resource requirements as <code class="literal">limits</code> or <code class="literal">requests</code>. The cluster capacity tool takes the pod’s resource requirements into account for its estimation analysis.
				</p><div class="orderedlist"><p class="title"><strong>Prerequisites</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Run the <a class="link" href="https://catalog.redhat.com/software/containers/openshift4/ose-cluster-capacity/5cca0324d70cc57c44ae8eb6?container-tabs=overview">OpenShift Cluster Capacity Tool</a>, which is available as a container image from the Red Hat Ecosystem Catalog.
						</li><li class="listitem"><p class="simpara">
							Create a sample pod spec file:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: small-pod
  labels:
    app: guestbook
    tier: frontend
spec:
  containers:
  - name: php-redis
    image: gcr.io/google-samples/gb-frontend:v4
    imagePullPolicy: Always
    resources:
      limits:
        cpu: 150m
        memory: 100Mi
      requests:
        cpu: 150m
        memory: 100Mi</pre></li><li class="listitem"><p class="simpara">
									Create the cluster role:
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc create -f pod-spec.yaml</pre></li></ol></div></li></ol></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To use the cluster capacity tool on the command line:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							From the terminal, log in to the Red Hat Registry:
						</p><pre class="programlisting language-terminal">$ podman login registry.redhat.io</pre></li><li class="listitem"><p class="simpara">
							Pull the cluster capacity tool image:
						</p><pre class="programlisting language-terminal">$ podman pull registry.redhat.io/openshift4/ose-cluster-capacity</pre></li><li class="listitem"><p class="simpara">
							Run the cluster capacity tool:
						</p><pre class="programlisting language-terminal">$ podman run -v $HOME/.kube:/kube:Z -v $(pwd):/cc:Z  ose-cluster-capacity \
/bin/cluster-capacity --kubeconfig /kube/config --&lt;pod_spec&gt;.yaml /cc/&lt;pod_spec&gt;.yaml \
--verbose</pre><p class="simpara">
							where:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;pod_spec&gt;.yaml</span></dt><dd>
										Specifies the pod spec to use.
									</dd><dt><span class="term">verbose</span></dt><dd>
										Outputs a detailed description of how many pods can be scheduled on each node in the cluster.
									</dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">small-pod pod requirements:
	- CPU: 150m
	- Memory: 100Mi

The cluster can schedule 88 instance(s) of the pod small-pod.

Termination reason: Unschedulable: 0/5 nodes are available: 2 Insufficient cpu,
3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't
tolerate.

Pod distribution among nodes:
small-pod
	- 192.168.124.214: 45 instance(s)
	- 192.168.124.120: 43 instance(s)</pre>

							</p></div><p class="simpara">
							In the above example, the number of estimated pods that can be scheduled onto the cluster is 88.
						</p></li></ol></div></section><section class="section" id="nodes-cluster-resource-levels-job_nodes-cluster-resource-levels"><div class="titlepage"><div><div><h3 class="title">8.2.3. Running the OpenShift Cluster Capacity Tool as a job inside a pod</h3></div></div></div><p>
					Running the OpenShift Cluster Capacity Tool as a job inside of a pod allows you to run the tool multiple times without needing user intervention. You run the OpenShift Cluster Capacity Tool as a job by using a <code class="literal">ConfigMap</code> object.
				</p><div class="formalpara"><p class="title"><strong>Prerequisites</strong></p><p>
						Download and install <a class="link" href="https://github.com/openshift/cluster-capacity">OpenShift Cluster Capacity Tool</a>.
					</p></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To run the cluster capacity tool:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the cluster role:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file similar to the following:
								</p><pre class="programlisting language-yaml">kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: cluster-capacity-role
rules:
- apiGroups: [""]
  resources: ["pods", "nodes", "persistentvolumeclaims", "persistentvolumes", "services", "replicationcontrollers"]
  verbs: ["get", "watch", "list"]
- apiGroups: ["apps"]
  resources: ["replicasets", "statefulsets"]
  verbs: ["get", "watch", "list"]
- apiGroups: ["policy"]
  resources: ["poddisruptionbudgets"]
  verbs: ["get", "watch", "list"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "watch", "list"]</pre></li><li class="listitem"><p class="simpara">
									Create the cluster role by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc create sa cluster-capacity-sa</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create the service account:
						</p><pre class="programlisting language-terminal">$ oc create sa cluster-capacity-sa -n default</pre></li><li class="listitem"><p class="simpara">
							Add the role to the service account:
						</p><pre class="programlisting language-terminal">$ oc adm policy add-cluster-role-to-user cluster-capacity-role \
    system:serviceaccount:&lt;namespace&gt;:cluster-capacity-sa</pre><p class="simpara">
							where:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;namespace&gt;</span></dt><dd>
										Specifies the namespace where the pod is located.
									</dd></dl></div></li><li class="listitem"><p class="simpara">
							Define and create the pod spec:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: small-pod
  labels:
    app: guestbook
    tier: frontend
spec:
  containers:
  - name: php-redis
    image: gcr.io/google-samples/gb-frontend:v4
    imagePullPolicy: Always
    resources:
      limits:
        cpu: 150m
        memory: 100Mi
      requests:
        cpu: 150m
        memory: 100Mi</pre></li><li class="listitem"><p class="simpara">
									Create the pod by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre><p class="simpara">
									For example:
								</p><pre class="programlisting language-terminal">$ oc create -f pod.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Created a config map object by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create configmap cluster-capacity-configmap \
    --from-file=pod.yaml=pod.yaml</pre><p class="simpara">
							The cluster capacity analysis is mounted in a volume using a config map object named <code class="literal">cluster-capacity-configmap</code> to mount the input pod spec file <code class="literal">pod.yaml</code> into a volume <code class="literal">test-volume</code> at the path <code class="literal">/test-pod</code>.
						</p></li><li class="listitem"><p class="simpara">
							Create the job using the below example of a job specification file:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: batch/v1
kind: Job
metadata:
  name: cluster-capacity-job
spec:
  parallelism: 1
  completions: 1
  template:
    metadata:
      name: cluster-capacity-pod
    spec:
        containers:
        - name: cluster-capacity
          image: openshift/origin-cluster-capacity
          imagePullPolicy: "Always"
          volumeMounts:
          - mountPath: /test-pod
            name: test-volume
          env:
          - name: CC_INCLUSTER <span id="CO146-1"><!--Empty--></span><span class="callout">1</span>
            value: "true"
          command:
          - "/bin/sh"
          - "-ec"
          - |
            /bin/cluster-capacity --podspec=/test-pod/pod.yaml --verbose
        restartPolicy: "Never"
        serviceAccountName: cluster-capacity-sa
        volumes:
        - name: test-volume
          configMap:
            name: cluster-capacity-configmap</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO146-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											A required environment variable letting the cluster capacity tool know that it is running inside a cluster as a pod. <br/> The <code class="literal">pod.yaml</code> key of the <code class="literal">ConfigMap</code> object is the same as the <code class="literal">Pod</code> spec file name, though it is not required. By doing this, the input pod spec file can be accessed inside the pod as <code class="literal">/test-pod/pod.yaml</code>.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Run the cluster capacity image as a job in a pod by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f cluster-capacity-job.yaml</pre></li></ol></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check the job logs to find the number of pods that can be scheduled in the cluster:
						</p><pre class="programlisting language-terminal">$ oc logs jobs/cluster-capacity-job</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">small-pod pod requirements:
        - CPU: 150m
        - Memory: 100Mi

The cluster can schedule 52 instance(s) of the pod small-pod.

Termination reason: Unschedulable: No nodes are available that match all of the
following predicates:: Insufficient cpu (2).

Pod distribution among nodes:
small-pod
        - 192.168.124.214: 26 instance(s)
        - 192.168.124.120: 26 instance(s)</pre>

							</p></div></li></ol></div></section></section><section class="section" id="nodes-cluster-pods-configuring"><div class="titlepage"><div><div><h2 class="title">8.3. Configuring an OpenShift Container Platform cluster for pods</h2></div></div></div><p>
				As an administrator, you can create and maintain an efficient cluster for pods.
			</p><p>
				By keeping your cluster efficient, you can provide a better environment for your developers using such tools as what a pod does when it exits, ensuring that the required number of pods is always running, when to restart pods designed to run only once, limit the bandwidth available to pods, and how to keep pods running during disruptions.
			</p><section class="section" id="nodes-pods-configuring-restart_nodes-cluster-pods"><div class="titlepage"><div><div><h3 class="title">8.3.1. Configuring how pods behave after restart</h3></div></div></div><p>
					A pod restart policy determines how OpenShift Container Platform responds when Containers in that pod exit. The policy applies to all Containers in that pod.
				</p><p>
					The possible values are:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">Always</code> - Tries restarting a successfully exited Container on the pod continuously, with an exponential back-off delay (10s, 20s, 40s) capped at 5 minutes. The default is <code class="literal">Always</code>.
						</li><li class="listitem">
							<code class="literal">OnFailure</code> - Tries restarting a failed Container on the pod with an exponential back-off delay (10s, 20s, 40s) capped at 5 minutes.
						</li><li class="listitem">
							<code class="literal">Never</code> - Does not try to restart exited or failed Containers on the pod. Pods immediately fail and exit.
						</li></ul></div><p>
					After the pod is bound to a node, the pod will never be bound to another node. This means that a controller is necessary in order for a pod to survive node failure:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232230127328" scope="col">Condition</th><th align="left" valign="top" id="idm140232230126240" scope="col">Controller Type</th><th align="left" valign="top" id="idm140232230125152" scope="col">Restart Policy</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232230127328"> <p>
									Pods that are expected to terminate (such as batch computations)
								</p>
								 </td><td align="left" valign="top" headers="idm140232230126240"> <p>
									Job
								</p>
								 </td><td align="left" valign="top" headers="idm140232230125152"> <p>
									<code class="literal">OnFailure</code> or <code class="literal">Never</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232230127328"> <p>
									Pods that are expected to not terminate (such as web servers)
								</p>
								 </td><td align="left" valign="top" headers="idm140232230126240"> <p>
									Replication controller
								</p>
								 </td><td align="left" valign="top" headers="idm140232230125152"> <p>
									<code class="literal">Always</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140232230127328"> <p>
									Pods that must run one-per-machine
								</p>
								 </td><td align="left" valign="top" headers="idm140232230126240"> <p>
									Daemon set
								</p>
								 </td><td align="left" valign="top" headers="idm140232230125152"> <p>
									Any
								</p>
								 </td></tr></tbody></table></div><p>
					If a Container on a pod fails and the restart policy is set to <code class="literal">OnFailure</code>, the pod stays on the node and the Container is restarted. If you do not want the Container to restart, use a restart policy of <code class="literal">Never</code>.
				</p><p>
					If an entire pod fails, OpenShift Container Platform starts a new pod. Developers must address the possibility that applications might be restarted in a new pod. In particular, applications must handle temporary files, locks, incomplete output, and so forth caused by previous runs.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Kubernetes architecture expects reliable endpoints from cloud providers. When a cloud provider is down, the kubelet prevents OpenShift Container Platform from restarting.
					</p><p>
						If the underlying cloud provider endpoints are not reliable, do not install a cluster using cloud provider integration. Install the cluster as if it was in a no-cloud environment. It is not recommended to toggle cloud provider integration on or off in an installed cluster.
					</p></div></div><p>
					For details on how OpenShift Container Platform uses restart policy with failed Containers, see the <a class="link" href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#example-states">Example States</a> in the Kubernetes documentation.
				</p></section><section class="section" id="nodes-pods-configuring-bandwidth_nodes-cluster-pods"><div class="titlepage"><div><div><h3 class="title">8.3.2. Limiting the bandwidth available to pods</h3></div></div></div><p>
					You can apply quality-of-service traffic shaping to a pod and effectively limit its available bandwidth. Egress traffic (from the pod) is handled by policing, which simply drops packets in excess of the configured rate. Ingress traffic (to the pod) is handled by shaping queued packets to effectively handle data. The limits you place on a pod do not affect the bandwidth of other pods.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To limit the bandwidth on a pod:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Write an object definition JSON file, and specify the data traffic speed using <code class="literal">kubernetes.io/ingress-bandwidth</code> and <code class="literal">kubernetes.io/egress-bandwidth</code> annotations. For example, to limit both pod egress and ingress bandwidth to 10M/s:
						</p><div class="formalpara"><p class="title"><strong>Limited <code class="literal">Pod</code> object definition</strong></p><p>
								
<pre class="programlisting language-json">{
    "kind": "Pod",
    "spec": {
        "containers": [
            {
                "image": "openshift/hello-openshift",
                "name": "hello-openshift"
            }
        ]
    },
    "apiVersion": "v1",
    "metadata": {
        "name": "iperf-slow",
        "annotations": {
            "kubernetes.io/ingress-bandwidth": "10M",
            "kubernetes.io/egress-bandwidth": "10M"
        }
    }
}</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create the pod using the object definition:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_or_dir_path&gt;</pre></li></ol></div></section><section class="section" id="nodes-pods-pod-distruption-about_nodes-cluster-pods"><div class="titlepage"><div><div><h3 class="title">8.3.3. Understanding how to use pod disruption budgets to specify the number of pods that must be up</h3></div></div></div><p>
					A <span class="emphasis"><em>pod disruption budget</em></span> allows the specification of safety constraints on pods during operations, such as draining a node for maintenance.
				</p><p>
					<code class="literal">PodDisruptionBudget</code> is an API object that specifies the minimum number or percentage of replicas that must be up at a time. Setting these in projects can be helpful during node maintenance (such as scaling a cluster down or a cluster upgrade) and is only honored on voluntary evictions (not on node failures).
				</p><p>
					A <code class="literal">PodDisruptionBudget</code> object’s configuration consists of the following key parts:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							A label selector, which is a label query over a set of pods.
						</li><li class="listitem"><p class="simpara">
							An availability level, which specifies the minimum number of pods that must be available simultaneously, either:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<code class="literal">minAvailable</code> is the number of pods must always be available, even during a disruption.
								</li><li class="listitem">
									<code class="literal">maxUnavailable</code> is the number of pods can be unavailable during a disruption.
								</li></ul></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						<code class="literal">Available</code> refers to the number of pods that has condition <code class="literal">Ready=True</code>. <code class="literal">Ready=True</code> refers to the pod that is able to serve requests and should be added to the load balancing pools of all matching services.
					</p><p>
						A <code class="literal">maxUnavailable</code> of <code class="literal">0%</code> or <code class="literal">0</code> or a <code class="literal">minAvailable</code> of <code class="literal">100%</code> or equal to the number of replicas is permitted but can block nodes from being drained.
					</p></div></div><p>
					You can check for pod disruption budgets across all projects with the following:
				</p><pre class="programlisting language-terminal">$ oc get poddisruptionbudget --all-namespaces</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">NAMESPACE                              NAME                                    MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
openshift-apiserver                    openshift-apiserver-pdb                 N/A             1                 1                     121m
openshift-cloud-controller-manager     aws-cloud-controller-manager            1               N/A               1                     125m
openshift-cloud-credential-operator    pod-identity-webhook                    1               N/A               1                     117m
openshift-cluster-csi-drivers          aws-ebs-csi-driver-controller-pdb       N/A             1                 1                     121m
openshift-cluster-storage-operator     csi-snapshot-controller-pdb             N/A             1                 1                     122m
openshift-cluster-storage-operator     csi-snapshot-webhook-pdb                N/A             1                 1                     122m
openshift-console                      console                                 N/A             1                 1                     116m
#...</pre>

					</p></div><p>
					The <code class="literal">PodDisruptionBudget</code> is considered healthy when there are at least <code class="literal">minAvailable</code> pods running in the system. Every pod above that limit can be evicted.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Depending on your pod priority and preemption settings, lower-priority pods might be removed despite their pod disruption budget requirements.
					</p></div></div><section class="section" id="nodes-pods-pod-disruption-configuring_nodes-cluster-pods"><div class="titlepage"><div><div><h4 class="title">8.3.3.1. Specifying the number of pods that must be up with pod disruption budgets</h4></div></div></div><p>
						You can use a <code class="literal">PodDisruptionBudget</code> object to specify the minimum number or percentage of replicas that must be up at a time.
					</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To configure a pod disruption budget:
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a YAML file with the an object definition similar to the following:
							</p><pre class="programlisting language-yaml">apiVersion: policy/v1 <span id="CO147-1"><!--Empty--></span><span class="callout">1</span>
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  minAvailable: 2  <span id="CO147-2"><!--Empty--></span><span class="callout">2</span>
  selector:  <span id="CO147-3"><!--Empty--></span><span class="callout">3</span>
    matchLabels:
      name: my-pod</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO147-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										<code class="literal">PodDisruptionBudget</code> is part of the <code class="literal">policy/v1</code> API group.
									</div></dd><dt><a href="#CO147-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The minimum number of pods that must be available simultaneously. This can be either an integer or a string specifying a percentage, for example, <code class="literal">20%</code>.
									</div></dd><dt><a href="#CO147-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										A label query over a set of resources. The result of <code class="literal">matchLabels</code> and <code class="literal">matchExpressions</code> are logically conjoined. Leave this parameter blank, for example <code class="literal">selector {}</code>, to select all pods in the project.
									</div></dd></dl></div><p class="simpara">
								Or:
							</p><pre class="programlisting language-yaml">apiVersion: policy/v1 <span id="CO148-1"><!--Empty--></span><span class="callout">1</span>
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  maxUnavailable: 25% <span id="CO148-2"><!--Empty--></span><span class="callout">2</span>
  selector: <span id="CO148-3"><!--Empty--></span><span class="callout">3</span>
    matchLabels:
      name: my-pod</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO148-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										<code class="literal">PodDisruptionBudget</code> is part of the <code class="literal">policy/v1</code> API group.
									</div></dd><dt><a href="#CO148-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The maximum number of pods that can be unavailable simultaneously. This can be either an integer or a string specifying a percentage, for example, <code class="literal">20%</code>.
									</div></dd><dt><a href="#CO148-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										A label query over a set of resources. The result of <code class="literal">matchLabels</code> and <code class="literal">matchExpressions</code> are logically conjoined. Leave this parameter blank, for example <code class="literal">selector {}</code>, to select all pods in the project.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Run the following command to add the object to project:
							</p><pre class="programlisting language-terminal">$ oc create -f &lt;/path/to/file&gt; -n &lt;project_name&gt;</pre></li></ol></div></section><section class="section" id="pod-disruption-eviction-policy_nodes-cluster-pods"><div class="titlepage"><div><div><h4 class="title">8.3.3.2. Specifying the eviction policy for unhealthy pods</h4></div></div></div><p>
						When you use pod disruption budgets (PDBs) to specify how many pods must be available simultaneously, you can also define the criteria for how unhealthy pods are considered for eviction.
					</p><p>
						You can choose one of the following policies:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">IfHealthyBudget</span></dt><dd>
									Running pods that are not yet healthy can be evicted only if the guarded application is not disrupted.
								</dd><dt><span class="term">AlwaysAllow</span></dt><dd>
									Running pods that are not yet healthy can be evicted regardless of whether the criteria in the pod disruption budget is met. This policy can help evict malfunctioning applications, such as ones with pods stuck in the <code class="literal">CrashLoopBackOff</code> state or failing to report the <code class="literal">Ready</code> status.
								</dd></dl></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Specifying the unhealthy pod eviction policy for pod disruption budgets is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
						</p><p>
							For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
						</p></div></div><p>
						To use this Technology Preview feature, you must have enabled the <code class="literal">TechPreviewNoUpgrade</code> feature set.
					</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							Enabling the <code class="literal">TechPreviewNoUpgrade</code> feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.
						</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a YAML file that defines a <code class="literal">PodDisruptionBudget</code> object and specify the unhealthy pod eviction policy:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">pod-disruption-budget.yaml</code> file</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      name: my-pod
  unhealthyPodEvictionPolicy: AlwaysAllow <span id="CO149-1"><!--Empty--></span><span class="callout">1</span></pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO149-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Choose either <code class="literal">IfHealthyBudget</code> or <code class="literal">AlwaysAllow</code> as the unhealthy pod eviction policy. The default is <code class="literal">IfHealthyBudget</code> when the <code class="literal">unhealthyPodEvictionPolicy</code> field is empty.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">PodDisruptionBudget</code> object by running the following command:
							</p><pre class="programlisting language-terminal">$ oc create -f pod-disruption-budget.yaml</pre></li></ol></div><p>
						With a PDB that has the <code class="literal">AlwaysAllow</code> unhealthy pod eviction policy set, you can now drain nodes and evict the pods for a malfunctioning application guarded by this PDB.
					</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-cluster-enabling">Enabling features using feature gates</a>
							</li><li class="listitem">
								<a class="link" href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy">Unhealthy Pod Eviction Policy</a> in the Kubernetes documentation
							</li></ul></div></section></section><section class="section" id="nodes-pods-configuring-critical_nodes-cluster-pods"><div class="titlepage"><div><div><h3 class="title">8.3.4. Preventing pod removal using critical pods</h3></div></div></div><p>
					There are a number of core components that are critical to a fully functional cluster, but, run on a regular cluster node rather than the master. A cluster might stop working properly if a critical add-on is evicted.
				</p><p>
					Pods marked as critical are not allowed to be evicted.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To make a pod critical:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">Pod</code> spec or edit existing pods to include the <code class="literal">system-cluster-critical</code> priority class:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-pdb
spec:
  template:
    metadata:
      name: critical-pod
    priorityClassName: system-cluster-critical <span id="CO150-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO150-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Default priority class for pods that should never be evicted from a node.
								</div></dd></dl></div><p class="simpara">
							Alternatively, you can specify <code class="literal">system-node-critical</code> for pods that are important to the cluster but can be removed if necessary.
						</p></li><li class="listitem"><p class="simpara">
							Create the pod:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre></li></ol></div></section></section><section class="section" id="nodes-cluster-limit-ranges"><div class="titlepage"><div><div><h2 class="title">8.4. Restrict resource consumption with limit ranges</h2></div></div></div><p>
				By default, containers run with unbounded compute resources on an OpenShift Container Platform cluster. With limit ranges, you can restrict resource consumption for specific objects in a project:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						pods and containers: You can set minimum and maximum requirements for CPU and memory for pods and their containers.
					</li><li class="listitem">
						Image streams: You can set limits on the number of images and tags in an <code class="literal">ImageStream</code> object.
					</li><li class="listitem">
						Images: You can limit the size of images that can be pushed to an internal registry.
					</li><li class="listitem">
						Persistent volume claims (PVC): You can restrict the size of the PVCs that can be requested.
					</li></ul></div><p>
				If a pod does not meet the constraints imposed by the limit range, the pod cannot be created in the namespace.
			</p><section class="section" id="nodes-cluster-limit-ranges-about_nodes-cluster-limit-ranges"><div class="titlepage"><div><div><h3 class="title">8.4.1. About limit ranges</h3></div></div></div><p>
					A limit range, defined by a <code class="literal">LimitRange</code> object, restricts resource consumption in a project. In the project you can set specific resource limits for a pod, container, image, image stream, or persistent volume claim (PVC).
				</p><p>
					All requests to create and modify resources are evaluated against each <code class="literal">LimitRange</code> object in the project. If the resource violates any of the enumerated constraints, the resource is rejected.
				</p><p>
					The following shows a limit range object for all components: pod, container, image, image stream, or PVC. You can configure limits for any or all of these components in the same object. You create a different limit range object for each project where you want to control resources.
				</p><div class="formalpara"><p class="title"><strong>Sample limit range object for a container</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: "v1"
kind: "LimitRange"
metadata:
  name: "resource-limits"
spec:
  limits:
    - type: "Container"
      max:
        cpu: "2"
        memory: "1Gi"
      min:
        cpu: "100m"
        memory: "4Mi"
      default:
        cpu: "300m"
        memory: "200Mi"
      defaultRequest:
        cpu: "200m"
        memory: "100Mi"
      maxLimitRequestRatio:
        cpu: "10"</pre>

					</p></div><section class="section" id="nodes-cluster-limit-ranges-limits_nodes-cluster-limit-ranges"><div class="titlepage"><div><div><h4 class="title">8.4.1.1. About component limits</h4></div></div></div><p>
						The following examples show limit range parameters for each component. The examples are broken out for clarity. You can create a single <code class="literal">LimitRange</code> object for any or all components as necessary.
					</p><section class="section" id="nodes-cluster-limit-container-limits"><div class="titlepage"><div><div><h5 class="title">8.4.1.1.1. Container limits</h5></div></div></div><p>
							A limit range allows you to specify the minimum and maximum CPU and memory that each container in a pod can request for a specific project. If a container is created in the project, the container CPU and memory requests in the <code class="literal">Pod</code> spec must comply with the values set in the <code class="literal">LimitRange</code> object. If not, the pod does not get created.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The container CPU or memory request and limit must be greater than or equal to the <code class="literal">min</code> resource constraint for containers that are specified in the <code class="literal">LimitRange</code> object.
								</li><li class="listitem"><p class="simpara">
									The container CPU or memory request and limit must be less than or equal to the <code class="literal">max</code> resource constraint for containers that are specified in the <code class="literal">LimitRange</code> object.
								</p><p class="simpara">
									If the <code class="literal">LimitRange</code> object defines a <code class="literal">max</code> CPU, you do not need to define a CPU <code class="literal">request</code> value in the <code class="literal">Pod</code> spec. But you must specify a CPU <code class="literal">limit</code> value that satisfies the maximum CPU constraint specified in the limit range.
								</p></li><li class="listitem"><p class="simpara">
									The ratio of the container limits to requests must be less than or equal to the <code class="literal">maxLimitRequestRatio</code> value for containers that is specified in the <code class="literal">LimitRange</code> object.
								</p><p class="simpara">
									If the <code class="literal">LimitRange</code> object defines a <code class="literal">maxLimitRequestRatio</code> constraint, any new containers must have both a <code class="literal">request</code> and a <code class="literal">limit</code> value. OpenShift Container Platform calculates the limit-to-request ratio by dividing the <code class="literal">limit</code> by the <code class="literal">request</code>. This value should be a non-negative integer greater than 1.
								</p><p class="simpara">
									For example, if a container has <code class="literal">cpu: 500</code> in the <code class="literal">limit</code> value, and <code class="literal">cpu: 100</code> in the <code class="literal">request</code> value, the limit-to-request ratio for <code class="literal">cpu</code> is <code class="literal">5</code>. This ratio must be less than or equal to the <code class="literal">maxLimitRequestRatio</code>.
								</p></li></ul></div><p>
							If the <code class="literal">Pod</code> spec does not specify a container resource memory or limit, the <code class="literal">default</code> or <code class="literal">defaultRequest</code> CPU and memory values for containers specified in the limit range object are assigned to the container.
						</p><div class="formalpara"><p class="title"><strong>Container <code class="literal">LimitRange</code> object definition</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: "v1"
kind: "LimitRange"
metadata:
  name: "resource-limits" <span id="CO151-1"><!--Empty--></span><span class="callout">1</span>
spec:
  limits:
    - type: "Container"
      max:
        cpu: "2" <span id="CO151-2"><!--Empty--></span><span class="callout">2</span>
        memory: "1Gi" <span id="CO151-3"><!--Empty--></span><span class="callout">3</span>
      min:
        cpu: "100m" <span id="CO151-4"><!--Empty--></span><span class="callout">4</span>
        memory: "4Mi" <span id="CO151-5"><!--Empty--></span><span class="callout">5</span>
      default:
        cpu: "300m" <span id="CO151-6"><!--Empty--></span><span class="callout">6</span>
        memory: "200Mi" <span id="CO151-7"><!--Empty--></span><span class="callout">7</span>
      defaultRequest:
        cpu: "200m" <span id="CO151-8"><!--Empty--></span><span class="callout">8</span>
        memory: "100Mi" <span id="CO151-9"><!--Empty--></span><span class="callout">9</span>
      maxLimitRequestRatio:
        cpu: "10" <span id="CO151-10"><!--Empty--></span><span class="callout">10</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO151-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the LimitRange object.
								</div></dd><dt><a href="#CO151-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The maximum amount of CPU that a single container in a pod can request.
								</div></dd><dt><a href="#CO151-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The maximum amount of memory that a single container in a pod can request.
								</div></dd><dt><a href="#CO151-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The minimum amount of CPU that a single container in a pod can request.
								</div></dd><dt><a href="#CO151-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									The minimum amount of memory that a single container in a pod can request.
								</div></dd><dt><a href="#CO151-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									The default amount of CPU that a container can use if not specified in the <code class="literal">Pod</code> spec.
								</div></dd><dt><a href="#CO151-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									The default amount of memory that a container can use if not specified in the <code class="literal">Pod</code> spec.
								</div></dd><dt><a href="#CO151-8"><span class="callout">8</span></a> </dt><dd><div class="para">
									The default amount of CPU that a container can request if not specified in the <code class="literal">Pod</code> spec.
								</div></dd><dt><a href="#CO151-9"><span class="callout">9</span></a> </dt><dd><div class="para">
									The default amount of memory that a container can request if not specified in the <code class="literal">Pod</code> spec.
								</div></dd><dt><a href="#CO151-10"><span class="callout">10</span></a> </dt><dd><div class="para">
									The maximum limit-to-request ratio for a container.
								</div></dd></dl></div></section><section class="section" id="nodes-cluster-limit-pod-limits"><div class="titlepage"><div><div><h5 class="title">8.4.1.1.2. Pod limits</h5></div></div></div><p>
							A limit range allows you to specify the minimum and maximum CPU and memory limits for all containers across a pod in a given project. To create a container in the project, the container CPU and memory requests in the <code class="literal">Pod</code> spec must comply with the values set in the <code class="literal">LimitRange</code> object. If not, the pod does not get created.
						</p><p>
							If the <code class="literal">Pod</code> spec does not specify a container resource memory or limit, the <code class="literal">default</code> or <code class="literal">defaultRequest</code> CPU and memory values for containers specified in the limit range object are assigned to the container.
						</p><p>
							Across all containers in a pod, the following must hold true:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The container CPU or memory request and limit must be greater than or equal to the <code class="literal">min</code> resource constraints for pods that are specified in the <code class="literal">LimitRange</code> object.
								</li><li class="listitem">
									The container CPU or memory request and limit must be less than or equal to the <code class="literal">max</code> resource constraints for pods that are specified in the <code class="literal">LimitRange</code> object.
								</li><li class="listitem">
									The ratio of the container limits to requests must be less than or equal to the <code class="literal">maxLimitRequestRatio</code> constraint specified in the <code class="literal">LimitRange</code> object.
								</li></ul></div><div class="formalpara"><p class="title"><strong>Pod <code class="literal">LimitRange</code> object definition</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: "v1"
kind: "LimitRange"
metadata:
  name: "resource-limits" <span id="CO152-1"><!--Empty--></span><span class="callout">1</span>
spec:
  limits:
    - type: "Pod"
      max:
        cpu: "2" <span id="CO152-2"><!--Empty--></span><span class="callout">2</span>
        memory: "1Gi" <span id="CO152-3"><!--Empty--></span><span class="callout">3</span>
      min:
        cpu: "200m" <span id="CO152-4"><!--Empty--></span><span class="callout">4</span>
        memory: "6Mi" <span id="CO152-5"><!--Empty--></span><span class="callout">5</span>
      maxLimitRequestRatio:
        cpu: "10" <span id="CO152-6"><!--Empty--></span><span class="callout">6</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO152-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the limit range object.
								</div></dd><dt><a href="#CO152-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The maximum amount of CPU that a pod can request across all containers.
								</div></dd><dt><a href="#CO152-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The maximum amount of memory that a pod can request across all containers.
								</div></dd><dt><a href="#CO152-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The minimum amount of CPU that a pod can request across all containers.
								</div></dd><dt><a href="#CO152-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									The minimum amount of memory that a pod can request across all containers.
								</div></dd><dt><a href="#CO152-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									The maximum limit-to-request ratio for a container.
								</div></dd></dl></div></section><section class="section" id="nodes-cluster-limit-image-limits"><div class="titlepage"><div><div><h5 class="title">8.4.1.1.3. Image limits</h5></div></div></div><p>
							A <code class="literal">LimitRange</code> object allows you to specify the maximum size of an image that can be pushed to an OpenShift image registry.
						</p><p>
							When pushing images to an OpenShift image registry, the following must hold true:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The size of the image must be less than or equal to the <code class="literal">max</code> size for images that is specified in the <code class="literal">LimitRange</code> object.
								</li></ul></div><div class="formalpara"><p class="title"><strong>Image <code class="literal">LimitRange</code> object definition</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: "v1"
kind: "LimitRange"
metadata:
  name: "resource-limits" <span id="CO153-1"><!--Empty--></span><span class="callout">1</span>
spec:
  limits:
    - type: openshift.io/Image
      max:
        storage: 1Gi <span id="CO153-2"><!--Empty--></span><span class="callout">2</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO153-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the <code class="literal">LimitRange</code> object.
								</div></dd><dt><a href="#CO153-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The maximum size of an image that can be pushed to an OpenShift image registry.
								</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								To prevent blobs that exceed the limit from being uploaded to the registry, the registry must be configured to enforce quotas.
							</p></div></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
								The image size is not always available in the manifest of an uploaded image. This is especially the case for images built with Docker 1.10 or higher and pushed to a v2 registry. If such an image is pulled with an older Docker daemon, the image manifest is converted by the registry to schema v1 lacking all the size information. No storage limit set on images prevent it from being uploaded.
							</p><p>
								<a class="link" href="https://github.com/openshift/origin/issues/7706">The issue</a> is being addressed.
							</p></div></div></section><section class="section" id="nodes-cluster-limit-stream-limits"><div class="titlepage"><div><div><h5 class="title">8.4.1.1.4. Image stream limits</h5></div></div></div><p>
							A <code class="literal">LimitRange</code> object allows you to specify limits for image streams.
						</p><p>
							For each image stream, the following must hold true:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The number of image tags in an <code class="literal">ImageStream</code> specification must be less than or equal to the <code class="literal">openshift.io/image-tags</code> constraint in the <code class="literal">LimitRange</code> object.
								</li><li class="listitem">
									The number of unique references to images in an <code class="literal">ImageStream</code> specification must be less than or equal to the <code class="literal">openshift.io/images</code> constraint in the limit range object.
								</li></ul></div><div class="formalpara"><p class="title"><strong>Imagestream <code class="literal">LimitRange</code> object definition</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: "v1"
kind: "LimitRange"
metadata:
  name: "resource-limits" <span id="CO154-1"><!--Empty--></span><span class="callout">1</span>
spec:
  limits:
    - type: openshift.io/ImageStream
      max:
        openshift.io/image-tags: 20 <span id="CO154-2"><!--Empty--></span><span class="callout">2</span>
        openshift.io/images: 30 <span id="CO154-3"><!--Empty--></span><span class="callout">3</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO154-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the <code class="literal">LimitRange</code> object.
								</div></dd><dt><a href="#CO154-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The maximum number of unique image tags in the <code class="literal">imagestream.spec.tags</code> parameter in imagestream spec.
								</div></dd><dt><a href="#CO154-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The maximum number of unique image references in the <code class="literal">imagestream.status.tags</code> parameter in the <code class="literal">imagestream</code> spec.
								</div></dd></dl></div><p>
							The <code class="literal">openshift.io/image-tags</code> resource represents unique image references. Possible references are an <code class="literal"><span class="strong strong"><strong>ImageStreamTag</strong></span></code>, an <code class="literal"><span class="strong strong"><strong>ImageStreamImage</strong></span></code> and a <code class="literal"><span class="strong strong"><strong>DockerImage</strong></span></code>. Tags can be created using the <code class="literal">oc tag</code> and <code class="literal">oc import-image</code> commands. No distinction is made between internal and external references. However, each unique reference tagged in an <code class="literal">ImageStream</code> specification is counted just once. It does not restrict pushes to an internal container image registry in any way, but is useful for tag restriction.
						</p><p>
							The <code class="literal">openshift.io/images</code> resource represents unique image names recorded in image stream status. It allows for restriction of a number of images that can be pushed to the OpenShift image registry. Internal and external references are not distinguished.
						</p></section><section class="section" id="nodes-cluster-limit-pvc-limits"><div class="titlepage"><div><div><h5 class="title">8.4.1.1.5. Persistent volume claim limits</h5></div></div></div><p>
							A <code class="literal">LimitRange</code> object allows you to restrict the storage requested in a persistent volume claim (PVC).
						</p><p>
							Across all persistent volume claims in a project, the following must hold true:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The resource request in a persistent volume claim (PVC) must be greater than or equal the <code class="literal">min</code> constraint for PVCs that is specified in the <code class="literal">LimitRange</code> object.
								</li><li class="listitem">
									The resource request in a persistent volume claim (PVC) must be less than or equal the <code class="literal">max</code> constraint for PVCs that is specified in the <code class="literal">LimitRange</code> object.
								</li></ul></div><div class="formalpara"><p class="title"><strong>PVC <code class="literal">LimitRange</code> object definition</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: "v1"
kind: "LimitRange"
metadata:
  name: "resource-limits" <span id="CO155-1"><!--Empty--></span><span class="callout">1</span>
spec:
  limits:
    - type: "PersistentVolumeClaim"
      min:
        storage: "2Gi" <span id="CO155-2"><!--Empty--></span><span class="callout">2</span>
      max:
        storage: "50Gi" <span id="CO155-3"><!--Empty--></span><span class="callout">3</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO155-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the <code class="literal">LimitRange</code> object.
								</div></dd><dt><a href="#CO155-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The minimum amount of storage that can be requested in a persistent volume claim.
								</div></dd><dt><a href="#CO155-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The maximum amount of storage that can be requested in a persistent volume claim.
								</div></dd></dl></div></section></section></section><section class="section" id="nodes-cluster-limit-creating_nodes-cluster-limit-ranges"><div class="titlepage"><div><div><h3 class="title">8.4.2. Creating a Limit Range</h3></div></div></div><p>
					To apply a limit range to a project:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">LimitRange</code> object with your required specifications:
						</p><pre class="programlisting language-yaml">apiVersion: "v1"
kind: "LimitRange"
metadata:
  name: "resource-limits" <span id="CO156-1"><!--Empty--></span><span class="callout">1</span>
spec:
  limits:
    - type: "Pod" <span id="CO156-2"><!--Empty--></span><span class="callout">2</span>
      max:
        cpu: "2"
        memory: "1Gi"
      min:
        cpu: "200m"
        memory: "6Mi"
    - type: "Container" <span id="CO156-3"><!--Empty--></span><span class="callout">3</span>
      max:
        cpu: "2"
        memory: "1Gi"
      min:
        cpu: "100m"
        memory: "4Mi"
      default: <span id="CO156-4"><!--Empty--></span><span class="callout">4</span>
        cpu: "300m"
        memory: "200Mi"
      defaultRequest: <span id="CO156-5"><!--Empty--></span><span class="callout">5</span>
        cpu: "200m"
        memory: "100Mi"
      maxLimitRequestRatio: <span id="CO156-6"><!--Empty--></span><span class="callout">6</span>
        cpu: "10"
    - type: openshift.io/Image <span id="CO156-7"><!--Empty--></span><span class="callout">7</span>
      max:
        storage: 1Gi
    - type: openshift.io/ImageStream <span id="CO156-8"><!--Empty--></span><span class="callout">8</span>
      max:
        openshift.io/image-tags: 20
        openshift.io/images: 30
    - type: "PersistentVolumeClaim" <span id="CO156-9"><!--Empty--></span><span class="callout">9</span>
      min:
        storage: "2Gi"
      max:
        storage: "50Gi"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO156-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify a name for the <code class="literal">LimitRange</code> object.
								</div></dd><dt><a href="#CO156-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									To set limits for a pod, specify the minimum and maximum CPU and memory requests as needed.
								</div></dd><dt><a href="#CO156-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									To set limits for a container, specify the minimum and maximum CPU and memory requests as needed.
								</div></dd><dt><a href="#CO156-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Optional. For a container, specify the default amount of CPU or memory that a container can use, if not specified in the <code class="literal">Pod</code> spec.
								</div></dd><dt><a href="#CO156-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Optional. For a container, specify the default amount of CPU or memory that a container can request, if not specified in the <code class="literal">Pod</code> spec.
								</div></dd><dt><a href="#CO156-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Optional. For a container, specify the maximum limit-to-request ratio that can be specified in the <code class="literal">Pod</code> spec.
								</div></dd><dt><a href="#CO156-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									To set limits for an Image object, set the maximum size of an image that can be pushed to an OpenShift image registry.
								</div></dd><dt><a href="#CO156-8"><span class="callout">8</span></a> </dt><dd><div class="para">
									To set limits for an image stream, set the maximum number of image tags and references that can be in the <code class="literal">ImageStream</code> object file, as needed.
								</div></dd><dt><a href="#CO156-9"><span class="callout">9</span></a> </dt><dd><div class="para">
									To set limits for a persistent volume claim, set the minimum and maximum amount of storage that can be requested.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the object:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;limit_range_file&gt; -n &lt;project&gt; <span id="CO157-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO157-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the name of the YAML file you created and the project where you want the limits to apply.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="nodes-cluster-limit-viewing_nodes-cluster-limit-ranges"><div class="titlepage"><div><div><h3 class="title">8.4.3. Viewing a limit</h3></div></div></div><p>
					You can view any limits defined in a project by navigating in the web console to the project’s <span class="strong strong"><strong>Quota</strong></span> page.
				</p><p>
					You can also use the CLI to view limit range details:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Get the list of <code class="literal">LimitRange</code> object defined in the project. For example, for a project called <span class="strong strong"><strong>demoproject</strong></span>:
						</p><pre class="programlisting language-terminal">$ oc get limits -n demoproject</pre><pre class="programlisting language-terminal">NAME              CREATED AT
resource-limits   2020-07-15T17:14:23Z</pre></li><li class="listitem"><p class="simpara">
							Describe the <code class="literal">LimitRange</code> object you are interested in, for example the <code class="literal">resource-limits</code> limit range:
						</p><pre class="programlisting language-terminal">$ oc describe limits resource-limits -n demoproject</pre><pre class="programlisting language-terminal">Name:                           resource-limits
Namespace:                      demoproject
Type                            Resource                Min     Max     Default Request Default Limit   Max Limit/Request Ratio
----                            --------                ---     ---     --------------- -------------   -----------------------
Pod                             cpu                     200m    2       -               -               -
Pod                             memory                  6Mi     1Gi     -               -               -
Container                       cpu                     100m    2       200m            300m            10
Container                       memory                  4Mi     1Gi     100Mi           200Mi           -
openshift.io/Image              storage                 -       1Gi     -               -               -
openshift.io/ImageStream        openshift.io/image      -       12      -               -               -
openshift.io/ImageStream        openshift.io/image-tags -       10      -               -               -
PersistentVolumeClaim           storage                 -       50Gi    -               -               -</pre></li></ol></div></section><section class="section" id="nodes-cluster-limit-ranges-deleting_nodes-cluster-limit-ranges"><div class="titlepage"><div><div><h3 class="title">8.4.4. Deleting a Limit Range</h3></div></div></div><p>
					To remove any active <code class="literal">LimitRange</code> object to no longer enforce the limits in a project:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Run the following command:
						</p><pre class="programlisting language-terminal">$ oc delete limits &lt;limit_name&gt;</pre></li></ul></div></section></section><section class="section" id="nodes-cluster-resource-configure"><div class="titlepage"><div><div><h2 class="title">8.5. Configuring cluster memory to meet container memory and risk requirements</h2></div></div></div><p>
				As a cluster administrator, you can help your clusters operate efficiently through managing application memory by:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Determining the memory and risk requirements of a containerized application component and configuring the container memory parameters to suit those requirements.
					</li><li class="listitem">
						Configuring containerized application runtimes (for example, OpenJDK) to adhere optimally to the configured container memory parameters.
					</li><li class="listitem">
						Diagnosing and resolving memory-related error conditions associated with running in a container.
					</li></ul></div><section class="section" id="nodes-cluster-resource-configure-about_nodes-cluster-resource-configure"><div class="titlepage"><div><div><h3 class="title">8.5.1. Understanding managing application memory</h3></div></div></div><p>
					It is recommended to fully read the overview of how OpenShift Container Platform manages Compute Resources before proceeding.
				</p><p>
					For each kind of resource (memory, CPU, storage), OpenShift Container Platform allows optional <span class="strong strong"><strong>request</strong></span> and <span class="strong strong"><strong>limit</strong></span> values to be placed on each container in a pod.
				</p><p>
					Note the following about memory requests and memory limits:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							<span class="strong strong"><strong>Memory request</strong></span>
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									The memory request value, if specified, influences the OpenShift Container Platform scheduler. The scheduler considers the memory request when scheduling a container to a node, then fences off the requested memory on the chosen node for the use of the container.
								</li><li class="listitem">
									If a node’s memory is exhausted, OpenShift Container Platform prioritizes evicting its containers whose memory usage most exceeds their memory request. In serious cases of memory exhaustion, the node OOM killer may select and kill a process in a container based on a similar metric.
								</li><li class="listitem">
									The cluster administrator can assign quota or assign default values for the memory request value.
								</li><li class="listitem">
									The cluster administrator can override the memory request values that a developer specifies, to manage cluster overcommit.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							<span class="strong strong"><strong>Memory limit</strong></span>
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									The memory limit value, if specified, provides a hard limit on the memory that can be allocated across all the processes in a container.
								</li><li class="listitem">
									If the memory allocated by all of the processes in a container exceeds the memory limit, the node Out of Memory (OOM) killer will immediately select and kill a process in the container.
								</li><li class="listitem">
									If both memory request and limit are specified, the memory limit value must be greater than or equal to the memory request.
								</li><li class="listitem">
									The cluster administrator can assign quota or assign default values for the memory limit value.
								</li><li class="listitem">
									The minimum memory limit is 12 MB. If a container fails to start due to a <code class="literal">Cannot allocate memory</code> pod event, the memory limit is too low. Either increase or remove the memory limit. Removing the limit allows pods to consume unbounded node resources.
								</li></ul></div></li></ul></div><section class="section" id="nodes-cluster-resource-configure-about-memory_nodes-cluster-resource-configure"><div class="titlepage"><div><div><h4 class="title">8.5.1.1. Managing application memory strategy</h4></div></div></div><p>
						The steps for sizing application memory on OpenShift Container Platform are as follows:
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								<span class="strong strong"><strong>Determine expected container memory usage</strong></span>
							</p><p class="simpara">
								Determine expected mean and peak container memory usage, empirically if necessary (for example, by separate load testing). Remember to consider all the processes that may potentially run in parallel in the container: for example, does the main application spawn any ancillary scripts?
							</p></li><li class="listitem"><p class="simpara">
								<span class="strong strong"><strong>Determine risk appetite</strong></span>
							</p><p class="simpara">
								Determine risk appetite for eviction. If the risk appetite is low, the container should request memory according to the expected peak usage plus a percentage safety margin. If the risk appetite is higher, it may be more appropriate to request memory according to the expected mean usage.
							</p></li><li class="listitem"><p class="simpara">
								<span class="strong strong"><strong>Set container memory request</strong></span>
							</p><p class="simpara">
								Set container memory request based on the above. The more accurately the request represents the application memory usage, the better. If the request is too high, cluster and quota usage will be inefficient. If the request is too low, the chances of application eviction increase.
							</p></li><li class="listitem"><p class="simpara">
								<span class="strong strong"><strong>Set container memory limit, if required</strong></span>
							</p><p class="simpara">
								Set container memory limit, if required. Setting a limit has the effect of immediately killing a container process if the combined memory usage of all processes in the container exceeds the limit, and is therefore a mixed blessing. On the one hand, it may make unanticipated excess memory usage obvious early ("fail fast"); on the other hand it also terminates processes abruptly.
							</p><p class="simpara">
								Note that some OpenShift Container Platform clusters may require a limit value to be set; some may override the request based on the limit; and some application images rely on a limit value being set as this is easier to detect than a request value.
							</p><p class="simpara">
								If the memory limit is set, it should not be set to less than the expected peak container memory usage plus a percentage safety margin.
							</p></li><li class="listitem"><p class="simpara">
								<span class="strong strong"><strong>Ensure application is tuned</strong></span>
							</p><p class="simpara">
								Ensure application is tuned with respect to configured request and limit values, if appropriate. This step is particularly relevant to applications which pool memory, such as the JVM. The rest of this page discusses this.
							</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-cluster-overcommit-reserving-memory_nodes-cluster-overcommit">Understanding compute resources and containers</a>
							</li></ul></div></section></section><section class="section" id="nodes-cluster-resource-configure-jdk_nodes-cluster-resource-configure"><div class="titlepage"><div><div><h3 class="title">8.5.2. Understanding OpenJDK settings for OpenShift Container Platform</h3></div></div></div><p>
					The default OpenJDK settings do not work well with containerized environments. As a result, some additional Java memory settings must always be provided whenever running the OpenJDK in a container.
				</p><p>
					The JVM memory layout is complex, version dependent, and describing it in detail is beyond the scope of this documentation. However, as a starting point for running OpenJDK in a container, at least the following three memory-related tasks are key:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Overriding the JVM maximum heap size.
						</li><li class="listitem">
							Encouraging the JVM to release unused memory to the operating system, if appropriate.
						</li><li class="listitem">
							Ensuring all JVM processes within a container are appropriately configured.
						</li></ol></div><p>
					Optimally tuning JVM workloads for running in a container is beyond the scope of this documentation, and may involve setting multiple additional JVM options.
				</p><section class="section" id="nodes-cluster-resource-configure-jdk-heap_nodes-cluster-resource-configure"><div class="titlepage"><div><div><h4 class="title">8.5.2.1. Understanding how to override the JVM maximum heap size</h4></div></div></div><p>
						For many Java workloads, the JVM heap is the largest single consumer of memory. Currently, the OpenJDK defaults to allowing up to 1/4 (1/<code class="literal">-XX:MaxRAMFraction</code>) of the compute node’s memory to be used for the heap, regardless of whether the OpenJDK is running in a container or not. It is therefore <span class="strong strong"><strong>essential</strong></span> to override this behavior, especially if a container memory limit is also set.
					</p><p>
						There are at least two ways the above can be achieved:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								If the container memory limit is set and the experimental options are supported by the JVM, set <code class="literal">-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap</code>.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									The <code class="literal">UseCGroupMemoryLimitForHeap</code> option has been removed in JDK 11. Use <code class="literal">-XX:+UseContainerSupport</code> instead.
								</p></div></div><p class="simpara">
								This sets <code class="literal">-XX:MaxRAM</code> to the container memory limit, and the maximum heap size (<code class="literal">-XX:MaxHeapSize</code> / <code class="literal">-Xmx</code>) to 1/<code class="literal">-XX:MaxRAMFraction</code> (1/4 by default).
							</p></li><li class="listitem"><p class="simpara">
								Directly override one of <code class="literal">-XX:MaxRAM</code>, <code class="literal">-XX:MaxHeapSize</code> or <code class="literal">-Xmx</code>.
							</p><p class="simpara">
								This option involves hard-coding a value, but has the advantage of allowing a safety margin to be calculated.
							</p></li></ul></div></section><section class="section" id="nodes-cluster-resource-configure-jdk-unused_nodes-cluster-resource-configure"><div class="titlepage"><div><div><h4 class="title">8.5.2.2. Understanding how to encourage the JVM to release unused memory to the operating system</h4></div></div></div><p>
						By default, the OpenJDK does not aggressively return unused memory to the operating system. This may be appropriate for many containerized Java workloads, but notable exceptions include workloads where additional active processes co-exist with a JVM within a container, whether those additional processes are native, additional JVMs, or a combination of the two.
					</p><p>
						Java-based agents can use the following JVM arguments to encourage the JVM to release unused memory to the operating system:
					</p><pre class="programlisting language-terminal">-XX:+UseParallelGC
-XX:MinHeapFreeRatio=5 -XX:MaxHeapFreeRatio=10 -XX:GCTimeRatio=4
-XX:AdaptiveSizePolicyWeight=90.</pre><p>
						These arguments are intended to return heap memory to the operating system whenever allocated memory exceeds 110% of in-use memory (<code class="literal">-XX:MaxHeapFreeRatio</code>), spending up to 20% of CPU time in the garbage collector (<code class="literal">-XX:GCTimeRatio</code>). At no time will the application heap allocation be less than the initial heap allocation (overridden by <code class="literal">-XX:InitialHeapSize</code> / <code class="literal">-Xms</code>). Detailed additional information is available <a class="link" href="https://developers.redhat.com/blog/2014/07/15/dude-wheres-my-paas-memory-tuning-javas-footprint-in-openshift-part-1/">Tuning Java’s footprint in OpenShift (Part 1)</a>, <a class="link" href="https://developers.redhat.com/blog/2014/07/22/dude-wheres-my-paas-memory-tuning-javas-footprint-in-openshift-part-2/">Tuning Java’s footprint in OpenShift (Part 2)</a>, and at <a class="link" href="https://developers.redhat.com/blog/2017/04/04/openjdk-and-containers/">OpenJDK and Containers</a>.
					</p></section><section class="section" id="nodes-cluster-resource-configure-jdk-proc_nodes-cluster-resource-configure"><div class="titlepage"><div><div><h4 class="title">8.5.2.3. Understanding how to ensure all JVM processes within a container are appropriately configured</h4></div></div></div><p>
						In the case that multiple JVMs run in the same container, it is essential to ensure that they are all configured appropriately. For many workloads it will be necessary to grant each JVM a percentage memory budget, leaving a perhaps substantial additional safety margin.
					</p><p>
						Many Java tools use different environment variables (<code class="literal">JAVA_OPTS</code>, <code class="literal">GRADLE_OPTS</code>, and so on) to configure their JVMs and it can be challenging to ensure that the right settings are being passed to the right JVM.
					</p><p>
						The <code class="literal">JAVA_TOOL_OPTIONS</code> environment variable is always respected by the OpenJDK, and values specified in <code class="literal">JAVA_TOOL_OPTIONS</code> will be overridden by other options specified on the JVM command line. By default, to ensure that these options are used by default for all JVM workloads run in the Java-based agent image, the OpenShift Container Platform Jenkins Maven agent image sets:
					</p><pre class="programlisting language-terminal">JAVA_TOOL_OPTIONS="-XX:+UnlockExperimentalVMOptions
-XX:+UseCGroupMemoryLimitForHeap -Dsun.zip.disableMemoryMapping=true"</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The <code class="literal">UseCGroupMemoryLimitForHeap</code> option has been removed in JDK 11. Use <code class="literal">-XX:+UseContainerSupport</code> instead.
						</p></div></div><p>
						This does not guarantee that additional options are not required, but is intended to be a helpful starting point.
					</p></section></section><section class="section" id="nodes-cluster-resource-configure-request-limit_nodes-cluster-resource-configure"><div class="titlepage"><div><div><h3 class="title">8.5.3. Finding the memory request and limit from within a pod</h3></div></div></div><p>
					An application wishing to dynamically discover its memory request and limit from within a pod should use the Downward API.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Configure the pod to add the <code class="literal">MEMORY_REQUEST</code> and <code class="literal">MEMORY_LIMIT</code> stanzas:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file similar to the following:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
  - name: test
    image: fedora:latest
    command:
    - sleep
    - "3600"
    env:
    - name: MEMORY_REQUEST <span id="CO158-1"><!--Empty--></span><span class="callout">1</span>
      valueFrom:
        resourceFieldRef:
          containerName: test
          resource: requests.memory
    - name: MEMORY_LIMIT <span id="CO158-2"><!--Empty--></span><span class="callout">2</span>
      valueFrom:
        resourceFieldRef:
          containerName: test
          resource: limits.memory
    resources:
      requests:
        memory: 384Mi
      limits:
        memory: 512Mi</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO158-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Add this stanza to discover the application memory request value.
										</div></dd><dt><a href="#CO158-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Add this stanza to discover the application memory limit value.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Create the pod by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre></li></ol></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Access the pod using a remote shell:
						</p><pre class="programlisting language-terminal">$ oc rsh test</pre></li><li class="listitem"><p class="simpara">
							Check that the requested values were applied:
						</p><pre class="programlisting language-terminal">$ env | grep MEMORY | sort</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">MEMORY_LIMIT=536870912
MEMORY_REQUEST=402653184</pre>

							</p></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The memory limit value can also be read from inside the container by the <code class="literal">/sys/fs/cgroup/memory/memory.limit_in_bytes</code> file.
					</p></div></div></section><section class="section" id="nodes-cluster-resource-configure-oom_nodes-cluster-resource-configure"><div class="titlepage"><div><div><h3 class="title">8.5.4. Understanding OOM kill policy</h3></div></div></div><p>
					OpenShift Container Platform can kill a process in a container if the total memory usage of all the processes in the container exceeds the memory limit, or in serious cases of node memory exhaustion.
				</p><p>
					When a process is Out of Memory (OOM) killed, this might result in the container exiting immediately. If the container PID 1 process receives the <span class="strong strong"><strong>SIGKILL</strong></span>, the container will exit immediately. Otherwise, the container behavior is dependent on the behavior of the other processes.
				</p><p>
					For example, a container process exited with code 137, indicating it received a SIGKILL signal.
				</p><p>
					If the container does not exit immediately, an OOM kill is detectable as follows:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Access the pod using a remote shell:
						</p><pre class="programlisting language-terminal"># oc rsh test</pre></li><li class="listitem"><p class="simpara">
							Run the following command to see the current OOM kill count in <code class="literal">/sys/fs/cgroup/memory/memory.oom_control</code>:
						</p><pre class="programlisting language-terminal">$ grep '^oom_kill ' /sys/fs/cgroup/memory/memory.oom_control</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">oom_kill 0</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Run the following command to provoke an OOM kill:
						</p><pre class="programlisting language-terminal">$ sed -e '' &lt;/dev/zero</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Killed</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Run the following command to view the exit status of the <code class="literal">sed</code> command:
						</p><pre class="programlisting language-terminal">$ echo $?</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">137</pre>

							</p></div><p class="simpara">
							The <code class="literal">137</code> code indicates the container process exited with code 137, indicating it received a SIGKILL signal.
						</p></li><li class="listitem"><p class="simpara">
							Run the following command to see that the OOM kill counter in <code class="literal">/sys/fs/cgroup/memory/memory.oom_control</code> incremented:
						</p><pre class="programlisting language-terminal">$ grep '^oom_kill ' /sys/fs/cgroup/memory/memory.oom_control</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">oom_kill 1</pre>

							</p></div><p class="simpara">
							If one or more processes in a pod are OOM killed, when the pod subsequently exits, whether immediately or not, it will have phase <span class="strong strong"><strong>Failed</strong></span> and reason <span class="strong strong"><strong>OOMKilled</strong></span>. An OOM-killed pod might be restarted depending on the value of <code class="literal">restartPolicy</code>. If not restarted, controllers such as the replication controller will notice the pod’s failed status and create a new pod to replace the old one.
						</p><p class="simpara">
							Use the follwing command to get the pod status:
						</p><pre class="programlisting language-terminal">$ oc get pod test</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      READY     STATUS      RESTARTS   AGE
test      0/1       OOMKilled   0          1m</pre>

							</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									If the pod has not restarted, run the following command to view the pod:
								</p><pre class="programlisting language-terminal">$ oc get pod test -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">...
status:
  containerStatuses:
  - name: test
    ready: false
    restartCount: 0
    state:
      terminated:
        exitCode: 137
        reason: OOMKilled
  phase: Failed</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									If restarted, run the following command to view the pod:
								</p><pre class="programlisting language-terminal">$ oc get pod test -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">...
status:
  containerStatuses:
  - name: test
    ready: true
    restartCount: 1
    lastState:
      terminated:
        exitCode: 137
        reason: OOMKilled
    state:
      running:
  phase: Running</pre>

									</p></div></li></ul></div></li></ol></div></section><section class="section" id="nodes-cluster-resource-configure-evicted_nodes-cluster-resource-configure"><div class="titlepage"><div><div><h3 class="title">8.5.5. Understanding pod eviction</h3></div></div></div><p>
					OpenShift Container Platform may evict a pod from its node when the node’s memory is exhausted. Depending on the extent of memory exhaustion, the eviction may or may not be graceful. Graceful eviction implies the main process (PID 1) of each container receiving a SIGTERM signal, then some time later a SIGKILL signal if the process has not exited already. Non-graceful eviction implies the main process of each container immediately receiving a SIGKILL signal.
				</p><p>
					An evicted pod has phase <span class="strong strong"><strong>Failed</strong></span> and reason <span class="strong strong"><strong>Evicted</strong></span>. It will not be restarted, regardless of the value of <code class="literal">restartPolicy</code>. However, controllers such as the replication controller will notice the pod’s failed status and create a new pod to replace the old one.
				</p><pre class="programlisting language-terminal">$ oc get pod test</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">NAME      READY     STATUS    RESTARTS   AGE
test      0/1       Evicted   0          1m</pre>

					</p></div><pre class="programlisting language-terminal">$ oc get pod test -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
						
<pre class="programlisting language-terminal">...
status:
  message: 'Pod The node was low on resource: [MemoryPressure].'
  phase: Failed
  reason: Evicted</pre>

					</p></div></section></section><section class="section" id="nodes-cluster-overcommit"><div class="titlepage"><div><div><h2 class="title">8.6. Configuring your cluster to place pods on overcommitted nodes</h2></div></div></div><p>
				In an <span class="emphasis"><em>overcommitted</em></span> state, the sum of the container compute resource requests and limits exceeds the resources available on the system. For example, you might want to use overcommitment in development environments where a trade-off of guaranteed performance for capacity is acceptable.
			</p><p>
				Containers can specify compute resource requests and limits. Requests are used for scheduling your container and provide a minimum service guarantee. Limits constrain the amount of compute resource that can be consumed on your node.
			</p><p>
				The scheduler attempts to optimize the compute resource use across all nodes in your cluster. It places pods onto specific nodes, taking the pods' compute resource requests and nodes' available capacity into consideration.
			</p><p>
				OpenShift Container Platform administrators can control the level of overcommit and manage container density on nodes. You can configure cluster-level overcommit using the <a class="link" href="#nodes-cluster-resource-override_nodes-cluster-overcommit" title="8.6.2. Cluster-level overcommit using the Cluster Resource Override Operator">ClusterResourceOverride Operator</a> to override the ratio between requests and limits set on developer containers. In conjunction with <a class="link" href="#nodes-cluster-node-overcommit_nodes-cluster-overcommit" title="8.6.3. Node-level overcommit">node overcommit</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/building_applications/#deployments-setting-resources_deployment-operations">project memory and CPU limits and defaults</a>, you can adjust the resource limit and request to achieve the desired level of overcommit.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					In OpenShift Container Platform, you must enable cluster-level overcommit. Node overcommitment is enabled by default. See <a class="link" href="#nodes-cluster-overcommit-node-disable_nodes-cluster-overcommit" title="8.6.3.7. Disabling overcommitment for a node">Disabling overcommitment for a node</a>.
				</p></div></div><section class="section" id="nodes-cluster-overcommit-resource-requests_nodes-cluster-overcommit"><div class="titlepage"><div><div><h3 class="title">8.6.1. Resource requests and overcommitment</h3></div></div></div><p>
					For each compute resource, a container may specify a resource request and limit. Scheduling decisions are made based on the request to ensure that a node has enough capacity available to meet the requested value. If a container specifies limits, but omits requests, the requests are defaulted to the limits. A container is not able to exceed the specified limit on the node.
				</p><p>
					The enforcement of limits is dependent upon the compute resource type. If a container makes no request or limit, the container is scheduled to a node with no resource guarantees. In practice, the container is able to consume as much of the specified resource as is available with the lowest local priority. In low resource situations, containers that specify no resource requests are given the lowest quality of service.
				</p><p>
					Scheduling is based on resources requested, while quota and hard limits refer to resource limits, which can be set higher than requested resources. The difference between request and limit determines the level of overcommit; for instance, if a container is given a memory request of 1Gi and a memory limit of 2Gi, it is scheduled based on the 1Gi request being available on the node, but could use up to 2Gi; so it is 200% overcommitted.
				</p></section><section class="section" id="nodes-cluster-resource-override_nodes-cluster-overcommit"><div class="titlepage"><div><div><h3 class="title">8.6.2. Cluster-level overcommit using the Cluster Resource Override Operator</h3></div></div></div><p>
					The Cluster Resource Override Operator is an admission webhook that allows you to control the level of overcommit and manage container density across all the nodes in your cluster. The Operator controls how nodes in specific projects can exceed defined memory and CPU limits.
				</p><p>
					You must install the Cluster Resource Override Operator using the OpenShift Container Platform console or CLI as shown in the following sections. During the installation, you create a <code class="literal">ClusterResourceOverride</code> custom resource (CR), where you set the level of overcommit, as shown in the following example:
				</p><pre class="programlisting language-yaml">apiVersion: operator.autoscaling.openshift.io/v1
kind: ClusterResourceOverride
metadata:
    name: cluster <span id="CO159-1"><!--Empty--></span><span class="callout">1</span>
spec:
  podResourceOverride:
    spec:
      memoryRequestToLimitPercent: 50 <span id="CO159-2"><!--Empty--></span><span class="callout">2</span>
      cpuRequestToLimitPercent: 25 <span id="CO159-3"><!--Empty--></span><span class="callout">3</span>
      limitCPUToMemoryPercent: 200 <span id="CO159-4"><!--Empty--></span><span class="callout">4</span>
# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO159-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The name must be <code class="literal">cluster</code>.
						</div></dd><dt><a href="#CO159-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Optional. If a container memory limit has been specified or defaulted, the memory request is overridden to this percentage of the limit, between 1-100. The default is 50.
						</div></dd><dt><a href="#CO159-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Optional. If a container CPU limit has been specified or defaulted, the CPU request is overridden to this percentage of the limit, between 1-100. The default is 25.
						</div></dd><dt><a href="#CO159-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Optional. If a container memory limit has been specified or defaulted, the CPU limit is overridden to a percentage of the memory limit, if specified. Scaling 1Gi of RAM at 100 percent is equal to 1 CPU core. This is processed prior to overriding the CPU request (if configured). The default is 200.
						</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The Cluster Resource Override Operator overrides have no effect if limits have not been set on containers. Create a <code class="literal">LimitRange</code> object with default limits per individual project or configure limits in <code class="literal">Pod</code> specs for the overrides to apply.
					</p></div></div><p>
					When configured, overrides can be enabled per-project by applying the following label to the Namespace object for each project:
				</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:

# ...

  labels:
    clusterresourceoverrides.admission.autoscaling.openshift.io/enabled: "true"

# ...</pre><p>
					The Operator watches for the <code class="literal">ClusterResourceOverride</code> CR and ensures that the <code class="literal">ClusterResourceOverride</code> admission webhook is installed into the same namespace as the operator.
				</p><section class="section" id="nodes-cluster-resource-override-deploy-console_nodes-cluster-overcommit"><div class="titlepage"><div><div><h4 class="title">8.6.2.1. Installing the Cluster Resource Override Operator using the web console</h4></div></div></div><p>
						You can use the OpenShift Container Platform web console to install the Cluster Resource Override Operator to help control overcommit in your cluster.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								The Cluster Resource Override Operator has no effect if limits have not been set on containers. You must specify default limits for a project using a <code class="literal">LimitRange</code> object or configure limits in <code class="literal">Pod</code> specs for the overrides to apply.
							</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To install the Cluster Resource Override Operator using the OpenShift Container Platform web console:
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong>Home</strong></span> → <span class="strong strong"><strong>Projects</strong></span>
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Click <span class="strong strong"><strong>Create Project</strong></span>.
									</li><li class="listitem">
										Specify <code class="literal">clusterresourceoverride-operator</code> as the name of the project.
									</li><li class="listitem">
										Click <span class="strong strong"><strong>Create</strong></span>.
									</li></ol></div></li><li class="listitem"><p class="simpara">
								Navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span>.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Choose <span class="strong strong"><strong>ClusterResourceOverride Operator</strong></span> from the list of available Operators and click <span class="strong strong"><strong>Install</strong></span>.
									</li><li class="listitem">
										On the <span class="strong strong"><strong>Install Operator</strong></span> page, make sure <span class="strong strong"><strong>A specific Namespace on the cluster</strong></span> is selected for <span class="strong strong"><strong>Installation Mode</strong></span>.
									</li><li class="listitem">
										Make sure <span class="strong strong"><strong>clusterresourceoverride-operator</strong></span> is selected for <span class="strong strong"><strong>Installed Namespace</strong></span>.
									</li><li class="listitem">
										Select an <span class="strong strong"><strong>Update Channel</strong></span> and <span class="strong strong"><strong>Approval Strategy</strong></span>.
									</li><li class="listitem">
										Click <span class="strong strong"><strong>Install</strong></span>.
									</li></ol></div></li><li class="listitem"><p class="simpara">
								On the <span class="strong strong"><strong>Installed Operators</strong></span> page, click <span class="strong strong"><strong>ClusterResourceOverride</strong></span>.
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										On the <span class="strong strong"><strong>ClusterResourceOverride Operator</strong></span> details page, click <span class="strong strong"><strong>Create ClusterResourceOverride</strong></span>.
									</li><li class="listitem"><p class="simpara">
										On the <span class="strong strong"><strong>Create ClusterResourceOverride</strong></span> page, click <span class="strong strong"><strong>YAML view</strong></span> and edit the YAML template to set the overcommit values as needed:
									</p><pre class="programlisting language-yaml">apiVersion: operator.autoscaling.openshift.io/v1
kind: ClusterResourceOverride
metadata:
  name: cluster <span id="CO160-1"><!--Empty--></span><span class="callout">1</span>
spec:
  podResourceOverride:
    spec:
      memoryRequestToLimitPercent: 50 <span id="CO160-2"><!--Empty--></span><span class="callout">2</span>
      cpuRequestToLimitPercent: 25 <span id="CO160-3"><!--Empty--></span><span class="callout">3</span>
      limitCPUToMemoryPercent: 200 <span id="CO160-4"><!--Empty--></span><span class="callout">4</span>
# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO160-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												The name must be <code class="literal">cluster</code>.
											</div></dd><dt><a href="#CO160-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Optional. Specify the percentage to override the container memory limit, if used, between 1-100. The default is 50.
											</div></dd><dt><a href="#CO160-3"><span class="callout">3</span></a> </dt><dd><div class="para">
												Optional. Specify the percentage to override the container CPU limit, if used, between 1-100. The default is 25.
											</div></dd><dt><a href="#CO160-4"><span class="callout">4</span></a> </dt><dd><div class="para">
												Optional. Specify the percentage to override the container memory limit, if used. Scaling 1Gi of RAM at 100 percent is equal to 1 CPU core. This is processed prior to overriding the CPU request, if configured. The default is 200.
											</div></dd></dl></div></li><li class="listitem">
										Click <span class="strong strong"><strong>Create</strong></span>.
									</li></ol></div></li><li class="listitem"><p class="simpara">
								Check the current state of the admission webhook by checking the status of the cluster custom resource:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										On the <span class="strong strong"><strong>ClusterResourceOverride Operator</strong></span> page, click <span class="strong strong"><strong>cluster</strong></span>.
									</li><li class="listitem"><p class="simpara">
										On the <span class="strong strong"><strong>ClusterResourceOverride Details</strong></span> page, click <span class="strong strong"><strong>YAML</strong></span>. The <code class="literal">mutatingWebhookConfigurationRef</code> section appears when the webhook is called.
									</p><pre class="programlisting language-yaml">apiVersion: operator.autoscaling.openshift.io/v1
kind: ClusterResourceOverride
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"operator.autoscaling.openshift.io/v1","kind":"ClusterResourceOverride","metadata":{"annotations":{},"name":"cluster"},"spec":{"podResourceOverride":{"spec":{"cpuRequestToLimitPercent":25,"limitCPUToMemoryPercent":200,"memoryRequestToLimitPercent":50}}}}
  creationTimestamp: "2019-12-18T22:35:02Z"
  generation: 1
  name: cluster
  resourceVersion: "127622"
  selfLink: /apis/operator.autoscaling.openshift.io/v1/clusterresourceoverrides/cluster
  uid: 978fc959-1717-4bd1-97d0-ae00ee111e8d
spec:
  podResourceOverride:
    spec:
      cpuRequestToLimitPercent: 25
      limitCPUToMemoryPercent: 200
      memoryRequestToLimitPercent: 50
status:

# ...

    mutatingWebhookConfigurationRef: <span id="CO161-1"><!--Empty--></span><span class="callout">1</span>
      apiVersion: admissionregistration.k8s.io/v1
      kind: MutatingWebhookConfiguration
      name: clusterresourceoverrides.admission.autoscaling.openshift.io
      resourceVersion: "127621"
      uid: 98b3b8ae-d5ce-462b-8ab5-a729ea8f38f3

# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO161-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Reference to the <code class="literal">ClusterResourceOverride</code> admission webhook.
											</div></dd></dl></div></li></ol></div></li></ol></div></section><section class="section" id="nodes-cluster-resource-override-deploy-cli_nodes-cluster-overcommit"><div class="titlepage"><div><div><h4 class="title">8.6.2.2. Installing the Cluster Resource Override Operator using the CLI</h4></div></div></div><p>
						You can use the OpenShift Container Platform CLI to install the Cluster Resource Override Operator to help control overcommit in your cluster.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								The Cluster Resource Override Operator has no effect if limits have not been set on containers. You must specify default limits for a project using a <code class="literal">LimitRange</code> object or configure limits in <code class="literal">Pod</code> specs for the overrides to apply.
							</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To install the Cluster Resource Override Operator using the CLI:
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a namespace for the Cluster Resource Override Operator:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Create a <code class="literal">Namespace</code> object YAML file (for example, <code class="literal">cro-namespace.yaml</code>) for the Cluster Resource Override Operator:
									</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: clusterresourceoverride-operator</pre></li><li class="listitem"><p class="simpara">
										Create the namespace:
									</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre><p class="simpara">
										For example:
									</p><pre class="programlisting language-terminal">$ oc create -f cro-namespace.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Create an Operator group:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Create an <code class="literal">OperatorGroup</code> object YAML file (for example, cro-og.yaml) for the Cluster Resource Override Operator:
									</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: clusterresourceoverride-operator
  namespace: clusterresourceoverride-operator
spec:
  targetNamespaces:
    - clusterresourceoverride-operator</pre></li><li class="listitem"><p class="simpara">
										Create the Operator Group:
									</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre><p class="simpara">
										For example:
									</p><pre class="programlisting language-terminal">$ oc create -f cro-og.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Create a subscription:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Create a <code class="literal">Subscription</code> object YAML file (for example, cro-sub.yaml) for the Cluster Resource Override Operator:
									</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: clusterresourceoverride
  namespace: clusterresourceoverride-operator
spec:
  channel: "4.13"
  name: clusterresourceoverride
  source: redhat-operators
  sourceNamespace: openshift-marketplace</pre></li><li class="listitem"><p class="simpara">
										Create the subscription:
									</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre><p class="simpara">
										For example:
									</p><pre class="programlisting language-terminal">$ oc create -f cro-sub.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Create a <code class="literal">ClusterResourceOverride</code> custom resource (CR) object in the <code class="literal">clusterresourceoverride-operator</code> namespace:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Change to the <code class="literal">clusterresourceoverride-operator</code> namespace.
									</p><pre class="programlisting language-terminal">$ oc project clusterresourceoverride-operator</pre></li><li class="listitem"><p class="simpara">
										Create a <code class="literal">ClusterResourceOverride</code> object YAML file (for example, cro-cr.yaml) for the Cluster Resource Override Operator:
									</p><pre class="programlisting language-yaml">apiVersion: operator.autoscaling.openshift.io/v1
kind: ClusterResourceOverride
metadata:
    name: cluster <span id="CO162-1"><!--Empty--></span><span class="callout">1</span>
spec:
  podResourceOverride:
    spec:
      memoryRequestToLimitPercent: 50 <span id="CO162-2"><!--Empty--></span><span class="callout">2</span>
      cpuRequestToLimitPercent: 25 <span id="CO162-3"><!--Empty--></span><span class="callout">3</span>
      limitCPUToMemoryPercent: 200 <span id="CO162-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO162-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												The name must be <code class="literal">cluster</code>.
											</div></dd><dt><a href="#CO162-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Optional. Specify the percentage to override the container memory limit, if used, between 1-100. The default is 50.
											</div></dd><dt><a href="#CO162-3"><span class="callout">3</span></a> </dt><dd><div class="para">
												Optional. Specify the percentage to override the container CPU limit, if used, between 1-100. The default is 25.
											</div></dd><dt><a href="#CO162-4"><span class="callout">4</span></a> </dt><dd><div class="para">
												Optional. Specify the percentage to override the container memory limit, if used. Scaling 1Gi of RAM at 100 percent is equal to 1 CPU core. This is processed prior to overriding the CPU request, if configured. The default is 200.
											</div></dd></dl></div></li><li class="listitem"><p class="simpara">
										Create the <code class="literal">ClusterResourceOverride</code> object:
									</p><pre class="programlisting language-terminal">$ oc create -f &lt;file-name&gt;.yaml</pre><p class="simpara">
										For example:
									</p><pre class="programlisting language-terminal">$ oc create -f cro-cr.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Verify the current state of the admission webhook by checking the status of the cluster custom resource.
							</p><pre class="programlisting language-terminal">$ oc get clusterresourceoverride cluster -n clusterresourceoverride-operator -o yaml</pre><p class="simpara">
								The <code class="literal">mutatingWebhookConfigurationRef</code> section appears when the webhook is called.
							</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: operator.autoscaling.openshift.io/v1
kind: ClusterResourceOverride
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"operator.autoscaling.openshift.io/v1","kind":"ClusterResourceOverride","metadata":{"annotations":{},"name":"cluster"},"spec":{"podResourceOverride":{"spec":{"cpuRequestToLimitPercent":25,"limitCPUToMemoryPercent":200,"memoryRequestToLimitPercent":50}}}}
  creationTimestamp: "2019-12-18T22:35:02Z"
  generation: 1
  name: cluster
  resourceVersion: "127622"
  selfLink: /apis/operator.autoscaling.openshift.io/v1/clusterresourceoverrides/cluster
  uid: 978fc959-1717-4bd1-97d0-ae00ee111e8d
spec:
  podResourceOverride:
    spec:
      cpuRequestToLimitPercent: 25
      limitCPUToMemoryPercent: 200
      memoryRequestToLimitPercent: 50
status:

# ...

    mutatingWebhookConfigurationRef: <span id="CO163-1"><!--Empty--></span><span class="callout">1</span>
      apiVersion: admissionregistration.k8s.io/v1
      kind: MutatingWebhookConfiguration
      name: clusterresourceoverrides.admission.autoscaling.openshift.io
      resourceVersion: "127621"
      uid: 98b3b8ae-d5ce-462b-8ab5-a729ea8f38f3

# ...</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO163-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Reference to the <code class="literal">ClusterResourceOverride</code> admission webhook.
									</div></dd></dl></div></li></ol></div></section><section class="section" id="nodes-cluster-resource-configure_nodes-cluster-overcommit"><div class="titlepage"><div><div><h4 class="title">8.6.2.3. Configuring cluster-level overcommit</h4></div></div></div><p>
						The Cluster Resource Override Operator requires a <code class="literal">ClusterResourceOverride</code> custom resource (CR) and a label for each project where you want the Operator to control overcommit.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								The Cluster Resource Override Operator has no effect if limits have not been set on containers. You must specify default limits for a project using a <code class="literal">LimitRange</code> object or configure limits in <code class="literal">Pod</code> specs for the overrides to apply.
							</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To modify cluster-level overcommit:
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">ClusterResourceOverride</code> CR:
							</p><pre class="programlisting language-yaml">apiVersion: operator.autoscaling.openshift.io/v1
kind: ClusterResourceOverride
metadata:
    name: cluster
spec:
  podResourceOverride:
    spec:
      memoryRequestToLimitPercent: 50 <span id="CO164-1"><!--Empty--></span><span class="callout">1</span>
      cpuRequestToLimitPercent: 25 <span id="CO164-2"><!--Empty--></span><span class="callout">2</span>
      limitCPUToMemoryPercent: 200 <span id="CO164-3"><!--Empty--></span><span class="callout">3</span>
# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO164-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Optional. Specify the percentage to override the container memory limit, if used, between 1-100. The default is 50.
									</div></dd><dt><a href="#CO164-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Optional. Specify the percentage to override the container CPU limit, if used, between 1-100. The default is 25.
									</div></dd><dt><a href="#CO164-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Optional. Specify the percentage to override the container memory limit, if used. Scaling 1Gi of RAM at 100 percent is equal to 1 CPU core. This is processed prior to overriding the CPU request, if configured. The default is 200.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Ensure the following label has been added to the Namespace object for each project where you want the Cluster Resource Override Operator to control overcommit:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:

# ...

  labels:
    clusterresourceoverrides.admission.autoscaling.openshift.io/enabled: "true" <span id="CO165-1"><!--Empty--></span><span class="callout">1</span>

# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO165-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Add this label to each project.
									</div></dd></dl></div></li></ol></div></section></section><section class="section" id="nodes-cluster-node-overcommit_nodes-cluster-overcommit"><div class="titlepage"><div><div><h3 class="title">8.6.3. Node-level overcommit</h3></div></div></div><p>
					You can use various ways to control overcommit on specific nodes, such as quality of service (QOS) guarantees, CPU limits, or reserve resources. You can also disable overcommit for specific nodes and specific projects.
				</p><section class="section" id="nodes-cluster-overcommit-reserving-memory_nodes-cluster-overcommit"><div class="titlepage"><div><div><h4 class="title">8.6.3.1. Understanding compute resources and containers</h4></div></div></div><p>
						The node-enforced behavior for compute resources is specific to the resource type.
					</p><section class="section" id="understanding-container-CPU-requests_nodes-cluster-overcommit"><div class="titlepage"><div><div><h5 class="title">8.6.3.1.1. Understanding container CPU requests</h5></div></div></div><p>
							A container is guaranteed the amount of CPU it requests and is additionally able to consume excess CPU available on the node, up to any limit specified by the container. If multiple containers are attempting to use excess CPU, CPU time is distributed based on the amount of CPU requested by each container.
						</p><p>
							For example, if one container requested 500m of CPU time and another container requested 250m of CPU time, then any extra CPU time available on the node is distributed among the containers in a 2:1 ratio. If a container specified a limit, it will be throttled not to use more CPU than the specified limit. CPU requests are enforced using the CFS shares support in the Linux kernel. By default, CPU limits are enforced using the CFS quota support in the Linux kernel over a 100ms measuring interval, though this can be disabled.
						</p></section><section class="section" id="understanding-memory-requests-container_nodes-cluster-overcommit"><div class="titlepage"><div><div><h5 class="title">8.6.3.1.2. Understanding container memory requests</h5></div></div></div><p>
							A container is guaranteed the amount of memory it requests. A container can use more memory than requested, but once it exceeds its requested amount, it could be terminated in a low memory situation on the node. If a container uses less memory than requested, it will not be terminated unless system tasks or daemons need more memory than was accounted for in the node’s resource reservation. If a container specifies a limit on memory, it is immediately terminated if it exceeds the limit amount.
						</p></section></section><section class="section" id="nodes-cluster-overcommit-qos-about_nodes-cluster-overcommit"><div class="titlepage"><div><div><h4 class="title">8.6.3.2. Understanding overcomitment and quality of service classes</h4></div></div></div><p>
						A node is <span class="emphasis"><em>overcommitted</em></span> when it has a pod scheduled that makes no request, or when the sum of limits across all pods on that node exceeds available machine capacity.
					</p><p>
						In an overcommitted environment, it is possible that the pods on the node will attempt to use more compute resource than is available at any given point in time. When this occurs, the node must give priority to one pod over another. The facility used to make this decision is referred to as a Quality of Service (QoS) Class.
					</p><p>
						A pod is designated as one of three QoS classes with decreasing order of priority:
					</p><div class="table" id="idm140232230545920"><p class="title"><strong>Table 8.19. Quality of Service Classes</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 14%; " class="col_1"><!--Empty--></col><col style="width: 14%; " class="col_2"><!--Empty--></col><col style="width: 72%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232230540144" scope="col">Priority</th><th align="left" valign="top" id="idm140232230539056" scope="col">Class Name</th><th align="left" valign="top" id="idm140232230537968" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232230540144"> <p>
										1 (highest)
									</p>
									 </td><td align="left" valign="top" headers="idm140232230539056"> <p>
										<span class="strong strong"><strong>Guaranteed</strong></span>
									</p>
									 </td><td align="left" valign="top" headers="idm140232230537968"> <p>
										If limits and optionally requests are set (not equal to 0) for all resources and they are equal, then the pod is classified as <span class="strong strong"><strong>Guaranteed</strong></span>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140232230540144"> <p>
										2
									</p>
									 </td><td align="left" valign="top" headers="idm140232230539056"> <p>
										<span class="strong strong"><strong>Burstable</strong></span>
									</p>
									 </td><td align="left" valign="top" headers="idm140232230537968"> <p>
										If requests and optionally limits are set (not equal to 0) for all resources, and they are not equal, then the pod is classified as <span class="strong strong"><strong>Burstable</strong></span>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140232230540144"> <p>
										3 (lowest)
									</p>
									 </td><td align="left" valign="top" headers="idm140232230539056"> <p>
										<span class="strong strong"><strong>BestEffort</strong></span>
									</p>
									 </td><td align="left" valign="top" headers="idm140232230537968"> <p>
										If requests and limits are not set for any of the resources, then the pod is classified as <span class="strong strong"><strong>BestEffort</strong></span>.
									</p>
									 </td></tr></tbody></table></div></div><p>
						Memory is an incompressible resource, so in low memory situations, containers that have the lowest priority are terminated first:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<span class="strong strong"><strong>Guaranteed</strong></span> containers are considered top priority, and are guaranteed to only be terminated if they exceed their limits, or if the system is under memory pressure and there are no lower priority containers that can be evicted.
							</li><li class="listitem">
								<span class="strong strong"><strong>Burstable</strong></span> containers under system memory pressure are more likely to be terminated once they exceed their requests and no other <span class="strong strong"><strong>BestEffort</strong></span> containers exist.
							</li><li class="listitem">
								<span class="strong strong"><strong>BestEffort</strong></span> containers are treated with the lowest priority. Processes in these containers are first to be terminated if the system runs out of memory.
							</li></ul></div><section class="section" id="qos-about-reserve_nodes-cluster-overcommit"><div class="titlepage"><div><div><h5 class="title">8.6.3.2.1. Understanding how to reserve memory across quality of service tiers</h5></div></div></div><p>
							You can use the <code class="literal">qos-reserved</code> parameter to specify a percentage of memory to be reserved by a pod in a particular QoS level. This feature attempts to reserve requested resources to exclude pods from lower OoS classes from using resources requested by pods in higher QoS classes.
						</p><p>
							OpenShift Container Platform uses the <code class="literal">qos-reserved</code> parameter as follows:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									A value of <code class="literal">qos-reserved=memory=100%</code> will prevent the <code class="literal">Burstable</code> and <code class="literal">BestEffort</code> QoS classes from consuming memory that was requested by a higher QoS class. This increases the risk of inducing OOM on <code class="literal">BestEffort</code> and <code class="literal">Burstable</code> workloads in favor of increasing memory resource guarantees for <code class="literal">Guaranteed</code> and <code class="literal">Burstable</code> workloads.
								</li><li class="listitem">
									A value of <code class="literal">qos-reserved=memory=50%</code> will allow the <code class="literal">Burstable</code> and <code class="literal">BestEffort</code> QoS classes to consume half of the memory requested by a higher QoS class.
								</li><li class="listitem">
									A value of <code class="literal">qos-reserved=memory=0%</code> will allow a <code class="literal">Burstable</code> and <code class="literal">BestEffort</code> QoS classes to consume up to the full node allocatable amount if available, but increases the risk that a <code class="literal">Guaranteed</code> workload will not have access to requested memory. This condition effectively disables this feature.
								</li></ul></div></section></section><section class="section" id="nodes-qos-about-swap_nodes-cluster-overcommit"><div class="titlepage"><div><div><h4 class="title">8.6.3.3. Understanding swap memory and QOS</h4></div></div></div><p>
						You can disable swap by default on your nodes to preserve quality of service (QOS) guarantees. Otherwise, physical resources on a node can oversubscribe, affecting the resource guarantees the Kubernetes scheduler makes during pod placement.
					</p><p>
						For example, if two guaranteed pods have reached their memory limit, each container could start using swap memory. Eventually, if there is not enough swap space, processes in the pods can be terminated due to the system being oversubscribed.
					</p><p>
						Failing to disable swap results in nodes not recognizing that they are experiencing <span class="strong strong"><strong>MemoryPressure</strong></span>, resulting in pods not receiving the memory they made in their scheduling request. As a result, additional pods are placed on the node to further increase memory pressure, ultimately increasing your risk of experiencing a system out of memory (OOM) event.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							If swap is enabled, any out-of-resource handling eviction thresholds for available memory will not work as expected. Take advantage of out-of-resource handling to allow pods to be evicted from a node when it is under memory pressure, and rescheduled on an alternative node that has no such pressure.
						</p></div></div></section><section class="section" id="nodes-cluster-overcommit-configure-nodes_nodes-cluster-overcommit"><div class="titlepage"><div><div><h4 class="title">8.6.3.4. Understanding nodes overcommitment</h4></div></div></div><p>
						In an overcommitted environment, it is important to properly configure your node to provide best system behavior.
					</p><p>
						When the node starts, it ensures that the kernel tunable flags for memory management are set properly. The kernel should never fail memory allocations unless it runs out of physical memory.
					</p><p>
						To ensure this behavior, OpenShift Container Platform configures the kernel to always overcommit memory by setting the <code class="literal">vm.overcommit_memory</code> parameter to <code class="literal">1</code>, overriding the default operating system setting.
					</p><p>
						OpenShift Container Platform also configures the kernel not to panic when it runs out of memory by setting the <code class="literal">vm.panic_on_oom</code> parameter to <code class="literal">0</code>. A setting of 0 instructs the kernel to call oom_killer in an Out of Memory (OOM) condition, which kills processes based on priority
					</p><p>
						You can view the current setting by running the following commands on your nodes:
					</p><pre class="programlisting language-terminal">$ sysctl -a |grep commit</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">#...
vm.overcommit_memory = 0
#...</pre>

						</p></div><pre class="programlisting language-terminal">$ sysctl -a |grep panic</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">#...
vm.panic_on_oom = 0
#...</pre>

						</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The above flags should already be set on nodes, and no further action is required.
						</p></div></div><p>
						You can also perform the following configurations for each node:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Disable or enforce CPU limits using CPU CFS quotas
							</li><li class="listitem">
								Reserve resources for system processes
							</li><li class="listitem">
								Reserve memory across quality of service tiers
							</li></ul></div></section><section class="section" id="nodes-cluster-overcommit-node-enforcing_nodes-cluster-overcommit"><div class="titlepage"><div><div><h4 class="title">8.6.3.5. Disabling or enforcing CPU limits using CPU CFS quotas</h4></div></div></div><p>
						Nodes by default enforce specified CPU limits using the Completely Fair Scheduler (CFS) quota support in the Linux kernel.
					</p><p>
						If you disable CPU limit enforcement, it is important to understand the impact on your node:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								If a container has a CPU request, the request continues to be enforced by CFS shares in the Linux kernel.
							</li><li class="listitem">
								If a container does not have a CPU request, but does have a CPU limit, the CPU request defaults to the specified CPU limit, and is enforced by CFS shares in the Linux kernel.
							</li><li class="listitem">
								If a container has both a CPU request and limit, the CPU request is enforced by CFS shares in the Linux kernel, and the CPU limit has no impact on the node.
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Obtain the label associated with the static <code class="literal">MachineConfigPool</code> CRD for the type of node you want to configure by entering the following command:
							</p><pre class="programlisting language-terminal">$ oc edit machineconfigpool &lt;name&gt;</pre><p class="simpara">
								For example:
							</p><pre class="programlisting language-terminal">$ oc edit machineconfigpool worker</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: "2022-11-16T15:34:25Z"
  generation: 4
  labels:
    pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO166-1"><!--Empty--></span><span class="callout">1</span>
  name: worker</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO166-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The label appears under Labels.
									</div></dd></dl></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
								If the label is not present, add a key/value pair such as:
							</p><pre class="programlisting language-terminal">$ oc label machineconfigpool worker custom-kubelet=small-pods</pre></div></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a custom resource (CR) for your configuration change.
							</p><div class="formalpara"><p class="title"><strong>Sample configuration for a disabling CPU limits</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: disable-cpu-units <span id="CO167-1"><!--Empty--></span><span class="callout">1</span>
spec:
  machineConfigPoolSelector:
    matchLabels:
      pools.operator.machineconfiguration.openshift.io/worker: "" <span id="CO167-2"><!--Empty--></span><span class="callout">2</span>
  kubeletConfig:
    cpuCfsQuota: false <span id="CO167-3"><!--Empty--></span><span class="callout">3</span></pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO167-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Assign a name to CR.
									</div></dd><dt><a href="#CO167-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specify the label from the machine config pool.
									</div></dd><dt><a href="#CO167-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Set the <code class="literal">cpuCfsQuota</code> parameter to <code class="literal">false</code>.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Run the following command to create the CR:
							</p><pre class="programlisting language-terminal">$ oc create -f &lt;file_name&gt;.yaml</pre></li></ol></div></section><section class="section" id="nodes-cluster-overcommit-node-resources_nodes-cluster-overcommit"><div class="titlepage"><div><div><h4 class="title">8.6.3.6. Reserving resources for system processes</h4></div></div></div><p>
						To provide more reliable scheduling and minimize node resource overcommitment, each node can reserve a portion of its resources for use by system daemons that are required to run on your node for your cluster to function. In particular, it is recommended that you reserve resources for incompressible resources such as memory.
					</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To explicitly reserve resources for non-pod processes, allocate node resources by specifying resources available for scheduling. For more details, see Allocating Resources for Nodes.
						</p></div></section><section class="section" id="nodes-cluster-overcommit-node-disable_nodes-cluster-overcommit"><div class="titlepage"><div><div><h4 class="title">8.6.3.7. Disabling overcommitment for a node</h4></div></div></div><p>
						When enabled, overcommitment can be disabled on each node.
					</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To disable overcommitment in a node run the following command on that node:
						</p></div><pre class="programlisting language-terminal">$ sysctl -w vm.overcommit_memory=0</pre></section></section><section class="section" id="nodes-cluster-project-overcommit_nodes-cluster-overcommit"><div class="titlepage"><div><div><h3 class="title">8.6.4. Project-level limits</h3></div></div></div><p>
					To help control overcommit, you can set per-project resource limit ranges, specifying memory and CPU limits and defaults for a project that overcommit cannot exceed.
				</p><p>
					For information on project-level resource limits, see Additional resources.
				</p><p>
					Alternatively, you can disable overcommitment for specific projects.
				</p><section class="section" id="nodes-cluster-overcommit-project-disable_nodes-cluster-overcommit"><div class="titlepage"><div><div><h4 class="title">8.6.4.1. Disabling overcommitment for a project</h4></div></div></div><p>
						When enabled, overcommitment can be disabled per-project. For example, you can allow infrastructure components to be configured independently of overcommitment.
					</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To disable overcommitment in a project:
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the namespace object to add the following annotation:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
  annotations:
    quota.openshift.io/cluster-resource-override-enabled: "false" <span id="CO168-1"><!--Empty--></span><span class="callout">1</span>
# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO168-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Setting this annotation to <code class="literal">false</code> disables overcommit for this namespace.
									</div></dd></dl></div></li></ol></div></section></section><section class="section _additional-resources" id="nodes-cluster-overcommit-addtl-resources"><div class="titlepage"><div><div><h3 class="title">8.6.5. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/building_applications/#deployments-triggers_deployment-operations">Setting deployment resources</a>.
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-resources-configuring-setting_nodes-nodes-resources-configuring">Allocating resources for nodes</a>.
						</li></ul></div></section></section><section class="section" id="nodes-cluster-cgroups-2"><div class="titlepage"><div><div><h2 class="title">8.7. Configuring the Linux cgroup version on your nodes</h2></div></div></div><p>
				By default, OpenShift Container Platform uses <a class="link" href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v1.html">Linux control group version 1</a> (cgroup v1) in your cluster. You can switch to <a class="link" href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html">Linux control group version 2</a> (cgroup v2), if needed, by editing the <code class="literal">node.config</code> object. Enabling cgroup v2 in OpenShift Container Platform disables all cgroup version 1 controllers and hierarchies in your cluster.
			</p><p>
				cgroup v2 is the next version of the Linux cgroup API. cgroup v2 offers several improvements over cgroup v1, including a unified hierarchy, safer sub-tree delegation, new features such as <a class="link" href="https://www.kernel.org/doc/html/latest/accounting/psi.html">Pressure Stall Information</a>, and enhanced resource management and isolation.
			</p><section class="section" id="nodes-clusters-cgroups-2_nodes-cluster-cgroups-2"><div class="titlepage"><div><div><h3 class="title">8.7.1. Configuring Linux cgroup</h3></div></div></div><p>
					You can enable <a class="link" href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v1.html">Linux control group version 1</a> (cgroup v1) or <a class="link" href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html">Linux control group version 2</a> (cgroup v2) by editing the <code class="literal">node.config</code> object. The default is cgroup v1.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Currently, disabling CPU load balancing is not supported by cgroup v2. As a result, you might not get the desired behavior from performance profiles if you have cgroup v2 enabled. Enabling cgroup v2 is not recommended if you are using performance profiles.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have a running OpenShift Container Platform cluster that uses version 4.12 or later.
						</li><li class="listitem">
							You are logged in to the cluster as a user with administrative privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Enable cgroup v2 on nodes:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Edit the <code class="literal">node.config</code> object:
								</p><pre class="programlisting language-terminal">$ oc edit nodes.config/cluster</pre></li><li class="listitem"><p class="simpara">
									Edit the <code class="literal">spec.cgroupMode</code> parameter:
								</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">node.config</code> object</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Node
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2022-07-08T16:02:51Z"
  generation: 1
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    kind: ClusterVersion
    name: version
    uid: 36282574-bf9f-409e-a6cd-3032939293eb
  resourceVersion: "1865"
  uid: 0c0f7a4c-4307-4187-b591-6155695ac85b
spec:
  cgroupMode: "v2" <span id="CO169-1"><!--Empty--></span><span class="callout">1</span>
...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO169-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify <code class="literal">v2</code> to enable cgroup v2 or <code class="literal">v1</code> for cgroup v1.
										</div></dd></dl></div></li></ol></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check the machine configs to see that the new machine configs were added:
						</p><pre class="programlisting language-terminal">$ oc get mc</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
00-master                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
00-worker                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
97-master-generated-kubelet                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-worker-generated-kubelet                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-ssh                                                                                 3.2.0             40m
99-worker-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-worker-ssh                                                                                 3.2.0             40m
rendered-master-23d4317815a5f854bd3553d689cfe2e9   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             10s <span id="CO170-1"><!--Empty--></span><span class="callout">1</span>
rendered-master-23e785de7587df95a4b517e0647e5ab7   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
rendered-worker-5d596d9293ca3ea80c896a1191735bb1   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
rendered-worker-dcc7f1b92892d34db74d6832bcc9ccd4   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             10s</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO170-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									New machine configs are created, as expected.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Check that the new <code class="literal">kernelArguments</code> were added to the new machine configs:
						</p><pre class="programlisting language-terminal">$ oc describe mc &lt;name&gt;</pre><div class="formalpara"><p class="title"><strong>Example output for cgroup v1</strong></p><p>
								
<pre class="programlisting language-terminal">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 05-worker-kernelarg-selinuxpermissive
spec:
  kernelArguments:
    systemd.unified_cgroup_hierarchy=0 <span id="CO171-1"><!--Empty--></span><span class="callout">1</span>
    systemd.legacy_systemd_cgroup_controller=1 <span id="CO171-2"><!--Empty--></span><span class="callout">2</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO171-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Enables cgroup v1 in systemd.
								</div></dd><dt><a href="#CO171-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Disables cgroup v2.
								</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example output for cgroup v2</strong></p><p>
								
<pre class="programlisting language-terminal">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 05-worker-kernelarg-selinuxpermissive
spec:
  kernelArguments:
  - systemd_unified_cgroup_hierarchy=1 <span id="CO172-1"><!--Empty--></span><span class="callout">1</span>
  - cgroup_no_v1="all" <span id="CO172-2"><!--Empty--></span><span class="callout">2</span>
  - psi=1 <span id="CO172-3"><!--Empty--></span><span class="callout">3</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO172-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Enables cgroup v2 in systemd.
								</div></dd><dt><a href="#CO172-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Disables cgroup v1.
								</div></dd><dt><a href="#CO172-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Enables the Linux Pressure Stall Information (PSI) feature.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Check the nodes to see that scheduling on the nodes is disabled. This indicates that the change is being applied:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                       STATUS                     ROLES    AGE   VERSION
ci-ln-fm1qnwt-72292-99kt6-master-0         Ready,SchedulingDisabled   master   58m   v1.26.0
ci-ln-fm1qnwt-72292-99kt6-master-1         Ready                      master   58m   v1.26.0
ci-ln-fm1qnwt-72292-99kt6-master-2         Ready                      master   58m   v1.26.0
ci-ln-fm1qnwt-72292-99kt6-worker-a-h5gt4   Ready,SchedulingDisabled   worker   48m   v1.26.0
ci-ln-fm1qnwt-72292-99kt6-worker-b-7vtmd   Ready                      worker   48m   v1.26.0
ci-ln-fm1qnwt-72292-99kt6-worker-c-rhzkv   Ready                      worker   48m   v1.26.0</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							After a node returns to the <code class="literal">Ready</code> state, start a debug session for that node:
						</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Set <code class="literal">/host</code> as the root directory within the debug shell:
						</p><pre class="programlisting language-terminal">sh-4.4# chroot /host</pre></li><li class="listitem"><p class="simpara">
							Check that the <code class="literal">sys/fs/cgroup/cgroup2fs</code> or <code class="literal">sys/fs/cgroup/tmpfs</code> file is present on your nodes:
						</p><pre class="programlisting language-terminal">$ stat -c %T -f /sys/fs/cgroup</pre><div class="formalpara"><p class="title"><strong>Example output for cgroup v1</strong></p><p>
								
<pre class="programlisting language-terminal">tmp2fs</pre>

							</p></div><div class="formalpara"><p class="title"><strong>Example output for cgroup v2</strong></p><p>
								
<pre class="programlisting language-terminal">cgroup2fs</pre>

							</p></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#ocp-installation-overview">OpenShift Container Platform installation overview</a>
						</li></ul></div></section></section><section class="section" id="nodes-cluster-enabling"><div class="titlepage"><div><div><h2 class="title">8.8. Enabling features using feature gates</h2></div></div></div><p>
				As an administrator, you can use feature gates to enable features that are not part of the default set of features.
			</p><section class="section" id="nodes-cluster-enabling-features-about_nodes-cluster-enabling"><div class="titlepage"><div><div><h3 class="title">8.8.1. Understanding feature gates</h3></div></div></div><p>
					You can use the <code class="literal">FeatureGate</code> custom resource (CR) to enable specific feature sets in your cluster. A feature set is a collection of OpenShift Container Platform features that are not enabled by default.
				</p><p>
					You can activate the following feature set by using the <code class="literal">FeatureGate</code> CR:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							<code class="literal">TechPreviewNoUpgrade</code>. This feature set is a subset of the current Technology Preview features. This feature set allows you to enable these Technology Preview features on test clusters, where you can fully test them, while leaving the features disabled on production clusters.
						</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
								Enabling the <code class="literal">TechPreviewNoUpgrade</code> feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.
							</p></div></div><p class="simpara">
							The following Technology Preview features are enabled by this feature set:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									External cloud providers. Enables support for external cloud providers for clusters on vSphere, AWS, Azure, and GCP. Support for OpenStack is GA. This is an internal feature that most users do not need to interact with. (<code class="literal">ExternalCloudProvider</code>)
								</li><li class="listitem">
									Shared Resources CSI Driver and Build CSI Volumes in OpenShift Builds. Enables the Container Storage Interface (CSI). (<code class="literal">CSIDriverSharedResource</code>)
								</li><li class="listitem">
									CSI volumes. Enables CSI volume support for the OpenShift Container Platform build system. (<code class="literal">BuildCSIVolumes</code>)
								</li><li class="listitem">
									Swap memory on nodes. Enables swap memory use for OpenShift Container Platform workloads on a per-node basis. (<code class="literal">NodeSwap</code>)
								</li><li class="listitem">
									OpenStack Machine API Provider. This gate has no effect and is planned to be removed from this feature set in a future release. (<code class="literal">MachineAPIProviderOpenStack</code>)
								</li><li class="listitem">
									Insights Operator. Enables the Insights Operator, which gathers OpenShift Container Platform configuration data and sends it to Red Hat. (<code class="literal">InsightsConfigAPI</code>)
								</li><li class="listitem">
									Pod topology spread constraints. Enables the <code class="literal">matchLabelKeys</code> parameter for pod topology constraints. The parameter is list of pod label keys to select the pods over which spreading will be calculated. (<code class="literal">MatchLabelKeysInPodTopologySpread</code>)
								</li><li class="listitem">
									Retroactive Default Storage Class. Enables OpenShift Container Platform to retroactively assign the default storage class to PVCs if there was no default storage class when the PVC was created.(<code class="literal">RetroactiveDefaultStorageClass</code>)
								</li><li class="listitem">
									Pod disruption budget (PDB) unhealthy pod eviction policy. Enables support for specifying how unhealthy pods are considered for eviction when using PDBs. (<code class="literal">PDBUnhealthyPodEvictionPolicy</code>)
								</li><li class="listitem">
									Dynamic Resource Allocation API. Enables a new API for requesting and sharing resources between pods and containers. This is an internal feature that most users do not need to interact with. (<code class="literal">DynamicResourceAllocation</code>)
								</li><li class="listitem">
									Pod security admission enforcement. Enables the restricted enforcement mode for pod security admission. Instead of only logging a warning, pods are rejected if they violate pod security standards. (<code class="literal">OpenShiftPodSecurityAdmission</code>)
								</li></ul></div></li></ul></div><p>
					For more information about the features activated by the <code class="literal">TechPreviewNoUpgrade</code> feature gate, see the following topics:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/cicd/#builds-running-entitled-builds-with-sharedsecret-objects_running-entitled-builds">Shared Resources CSI Driver and Build CSI Volumes in OpenShift Builds</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/storage/#ephemeral-storage-csi-inline">CSI inline ephemeral volumes</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-swap-memory_nodes-nodes-managing">Swap memory on nodes</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#using-insights-operator">Using Insights Operator</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/machine_management/#capi-machine-management">Managing machines with the Cluster API</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-pod-topology-spread-constraints">Controlling pod placement by using pod topology spread constraints</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/storage/#persistent-storage-csi-sc-manage">Managing the default storage class</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#pod-disruption-eviction-policy_nodes-pods-configuring">Specifying the eviction policy for unhealthy pods</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#understanding-and-managing-pod-security-admission">Pod security admission enforcement</a>.
						</li></ul></div></section><section class="section" id="nodes-cluster-enabling-features-install_nodes-cluster-enabling"><div class="titlepage"><div><div><h3 class="title">8.8.2. Enabling feature sets at installation</h3></div></div></div><p>
					You can enable feature sets for all nodes in the cluster by editing the <code class="literal">install-config.yaml</code> file before you deploy the cluster.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have an <code class="literal">install-config.yaml</code> file.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Use the <code class="literal">featureSet</code> parameter to specify the name of the feature set you want to enable, such as <code class="literal">TechPreviewNoUpgrade</code>:
						</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
								Enabling the <code class="literal">TechPreviewNoUpgrade</code> feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.
							</p></div></div><div class="formalpara"><p class="title"><strong>Sample <code class="literal">install-config.yaml</code> file with an enabled feature set</strong></p><p>
								
<pre class="programlisting language-yaml">compute:
- hyperthreading: Enabled
  name: worker
  platform:
    aws:
      rootVolume:
        iops: 2000
        size: 500
        type: io1
      metadataService:
        authentication: Optional
      type: c5.4xlarge
      zones:
      - us-west-2c
  replicas: 3
featureSet: TechPreviewNoUpgrade</pre>

							</p></div></li><li class="listitem">
							Save the file and reference it when using the installation program to deploy the cluster.
						</li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						You can verify that the feature gates are enabled by looking at the <code class="literal">kubelet.conf</code> file on a node after the nodes return to the ready state.
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							From the <span class="strong strong"><strong>Administrator</strong></span> perspective in the web console, navigate to <span class="strong strong"><strong>Compute</strong></span> → <span class="strong strong"><strong>Nodes</strong></span>.
						</li><li class="listitem">
							Select a node.
						</li><li class="listitem">
							In the <span class="strong strong"><strong>Node details</strong></span> page, click <span class="strong strong"><strong>Terminal</strong></span>.
						</li><li class="listitem"><p class="simpara">
							In the terminal window, change your root directory to <code class="literal">/host</code>:
						</p><pre class="programlisting language-terminal">sh-4.2# chroot /host</pre></li><li class="listitem"><p class="simpara">
							View the <code class="literal">kubelet.conf</code> file:
						</p><pre class="programlisting language-terminal">sh-4.2# cat /etc/kubernetes/kubelet.conf</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
								
<pre class="programlisting language-terminal"># ...
featureGates:
  InsightsOperatorPullingSCA: true,
  LegacyNodeRoleBehavior: false
# ...</pre>

							</p></div><p class="simpara">
							The features that are listed as <code class="literal">true</code> are enabled on your cluster.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The features listed vary depending upon the OpenShift Container Platform version.
							</p></div></div></li></ol></div></section><section class="section" id="nodes-cluster-enabling-features-console_nodes-cluster-enabling"><div class="titlepage"><div><div><h3 class="title">8.8.3. Enabling feature sets using the web console</h3></div></div></div><p>
					You can use the OpenShift Container Platform web console to enable feature sets for all of the nodes in a cluster by editing the <code class="literal">FeatureGate</code> custom resource (CR).
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To enable feature sets:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							In the OpenShift Container Platform web console, switch to the <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Custom Resource Definitions</strong></span> page.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>Custom Resource Definitions</strong></span> page, click <span class="strong strong"><strong>FeatureGate</strong></span>.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>Custom Resource Definition Details</strong></span> page, click the <span class="strong strong"><strong>Instances</strong></span> tab.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>cluster</strong></span> feature gate, then click the <span class="strong strong"><strong>YAML</strong></span> tab.
						</li><li class="listitem"><p class="simpara">
							Edit the <span class="strong strong"><strong>cluster</strong></span> instance to add specific feature sets:
						</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
								Enabling the <code class="literal">TechPreviewNoUpgrade</code> feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.
							</p></div></div><div class="formalpara"><p class="title"><strong>Sample Feature Gate custom resource</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: FeatureGate
metadata:
  name: cluster <span id="CO173-1"><!--Empty--></span><span class="callout">1</span>
# ...
spec:
  featureSet: TechPreviewNoUpgrade <span id="CO173-2"><!--Empty--></span><span class="callout">2</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO173-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the <code class="literal">FeatureGate</code> CR must be <code class="literal">cluster</code>.
								</div></dd><dt><a href="#CO173-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Add the feature set that you want to enable:
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">TechPreviewNoUpgrade</code> enables specific Technology Preview features.
										</li></ul></div></dd></dl></div><p class="simpara">
							After you save the changes, new machine configs are created, the machine config pools are updated, and scheduling on each node is disabled while the change is being applied.
						</p></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						You can verify that the feature gates are enabled by looking at the <code class="literal">kubelet.conf</code> file on a node after the nodes return to the ready state.
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							From the <span class="strong strong"><strong>Administrator</strong></span> perspective in the web console, navigate to <span class="strong strong"><strong>Compute</strong></span> → <span class="strong strong"><strong>Nodes</strong></span>.
						</li><li class="listitem">
							Select a node.
						</li><li class="listitem">
							In the <span class="strong strong"><strong>Node details</strong></span> page, click <span class="strong strong"><strong>Terminal</strong></span>.
						</li><li class="listitem"><p class="simpara">
							In the terminal window, change your root directory to <code class="literal">/host</code>:
						</p><pre class="programlisting language-terminal">sh-4.2# chroot /host</pre></li><li class="listitem"><p class="simpara">
							View the <code class="literal">kubelet.conf</code> file:
						</p><pre class="programlisting language-terminal">sh-4.2# cat /etc/kubernetes/kubelet.conf</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
								
<pre class="programlisting language-terminal"># ...
featureGates:
  InsightsOperatorPullingSCA: true,
  LegacyNodeRoleBehavior: false
# ...</pre>

							</p></div><p class="simpara">
							The features that are listed as <code class="literal">true</code> are enabled on your cluster.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The features listed vary depending upon the OpenShift Container Platform version.
							</p></div></div></li></ol></div></section><section class="section" id="nodes-cluster-enabling-features-cli_nodes-cluster-enabling"><div class="titlepage"><div><div><h3 class="title">8.8.4. Enabling feature sets using the CLI</h3></div></div></div><p>
					You can use the OpenShift CLI (<code class="literal">oc</code>) to enable feature sets for all of the nodes in a cluster by editing the <code class="literal">FeatureGate</code> custom resource (CR).
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To enable feature sets:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">FeatureGate</code> CR named <code class="literal">cluster</code>:
						</p><pre class="programlisting language-terminal">$ oc edit featuregate cluster</pre><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
								Enabling the <code class="literal">TechPreviewNoUpgrade</code> feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.
							</p></div></div><div class="formalpara"><p class="title"><strong>Sample FeatureGate custom resource</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: FeatureGate
metadata:
  name: cluster <span id="CO174-1"><!--Empty--></span><span class="callout">1</span>
# ...
spec:
  featureSet: TechPreviewNoUpgrade <span id="CO174-2"><!--Empty--></span><span class="callout">2</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO174-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the <code class="literal">FeatureGate</code> CR must be <code class="literal">cluster</code>.
								</div></dd><dt><a href="#CO174-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Add the feature set that you want to enable:
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">TechPreviewNoUpgrade</code> enables specific Technology Preview features.
										</li></ul></div></dd></dl></div><p class="simpara">
							After you save the changes, new machine configs are created, the machine config pools are updated, and scheduling on each node is disabled while the change is being applied.
						</p></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						You can verify that the feature gates are enabled by looking at the <code class="literal">kubelet.conf</code> file on a node after the nodes return to the ready state.
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							From the <span class="strong strong"><strong>Administrator</strong></span> perspective in the web console, navigate to <span class="strong strong"><strong>Compute</strong></span> → <span class="strong strong"><strong>Nodes</strong></span>.
						</li><li class="listitem">
							Select a node.
						</li><li class="listitem">
							In the <span class="strong strong"><strong>Node details</strong></span> page, click <span class="strong strong"><strong>Terminal</strong></span>.
						</li><li class="listitem"><p class="simpara">
							In the terminal window, change your root directory to <code class="literal">/host</code>:
						</p><pre class="programlisting language-terminal">sh-4.2# chroot /host</pre></li><li class="listitem"><p class="simpara">
							View the <code class="literal">kubelet.conf</code> file:
						</p><pre class="programlisting language-terminal">sh-4.2# cat /etc/kubernetes/kubelet.conf</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
								
<pre class="programlisting language-terminal"># ...
featureGates:
  InsightsOperatorPullingSCA: true,
  LegacyNodeRoleBehavior: false
# ...</pre>

							</p></div><p class="simpara">
							The features that are listed as <code class="literal">true</code> are enabled on your cluster.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The features listed vary depending upon the OpenShift Container Platform version.
							</p></div></div></li></ol></div></section></section><section class="section" id="nodes-cluster-worker-latency-profiles"><div class="titlepage"><div><div><h2 class="title">8.9. Improving cluster stability in high latency environments using worker latency profiles</h2></div></div></div><p>
				All nodes send heartbeats to the Kubernetes Controller Manager Operator (kube controller) in the OpenShift Container Platform cluster every 10 seconds, by default. If the cluster does not receive heartbeats from a node, OpenShift Container Platform responds using several default mechanisms.
			</p><p>
				For example, if the Kubernetes Controller Manager Operator loses contact with a node after a configured period:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						The node controller on the control plane updates the node health to <code class="literal">Unhealthy</code> and marks the node <code class="literal">Ready</code> condition as <code class="literal">Unknown</code>.
					</li><li class="listitem">
						In response, the scheduler stops scheduling pods to that node.
					</li><li class="listitem">
						The on-premise node controller adds a <code class="literal">node.kubernetes.io/unreachable</code> taint with a <code class="literal">NoExecute</code> effect to the node and schedules any pods on the node for eviction after five minutes, by default.
					</li></ol></div><p>
				This behavior can cause problems if your network is prone to latency issues, especially if you have nodes at the network edge. In some cases, the Kubernetes Controller Manager Operator might not receive an update from a healthy node due to network latency. The Kubernetes Controller Manager Operator would then evict pods from the node even though the node is healthy. To avoid this problem, you can use <span class="emphasis"><em>worker latency profiles</em></span> to adjust the frequency that the kubelet and the Kubernetes Controller Manager Operator wait for status updates before taking action. These adjustments help to ensure that your cluster runs properly in the event that network latency between the control plane and the worker nodes is not optimal.
			</p><p>
				These worker latency profiles are three sets of parameters that are pre-defined with carefully tuned values that let you control the reaction of the cluster to latency issues without needing to determine the best values manually.
			</p><p>
				You can configure worker latency profiles when installing a cluster or at any time you notice increased latency in your cluster network.
			</p><section class="section" id="nodes-cluster-worker-latency-profiles-about_nodes-cluster-worker-latency-profiles"><div class="titlepage"><div><div><h3 class="title">8.9.1. Understanding worker latency profiles</h3></div></div></div><p>
					Worker latency profiles are multiple sets of carefully-tuned values for the <code class="literal">node-status-update-frequency</code>, <code class="literal">node-monitor-grace-period</code>, <code class="literal">default-not-ready-toleration-seconds</code> and <code class="literal">default-unreachable-toleration-seconds</code> parameters. These parameters let you control the reaction of the cluster to latency issues without needing to determine the best values manually.
				</p><p>
					All worker latency profiles configure the following parameters:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">node-status-update-frequency</code>. Specifies the amount of time in seconds that a kubelet updates its status to the Kubernetes Controller Manager Operator.
						</li><li class="listitem">
							<code class="literal">node-monitor-grace-period</code>. Specifies the amount of time in seconds that the Kubernetes Controller Manager Operator waits for an update from a kubelet before marking the node unhealthy and adding the <code class="literal">node.kubernetes.io/not-ready</code> or <code class="literal">node.kubernetes.io/unreachable</code> taint to the node.
						</li><li class="listitem">
							<code class="literal">default-not-ready-toleration-seconds</code>. Specifies the amount of time in seconds after marking a node unhealthy that the Kubernetes Controller Manager Operator waits before evicting pods from that node.
						</li><li class="listitem">
							<code class="literal">default-unreachable-toleration-seconds</code>. Specifies the amount of time in seconds after marking a node unreachable that the Kubernetes Controller Manager Operator waits before evicting pods from that node.
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Manually modifying the <code class="literal">node-monitor-grace-period</code> parameter is not supported.
					</p></div></div><p>
					The following Operators monitor the changes to the worker latency profiles and respond accordingly:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The Machine Config Operator (MCO) updates the <code class="literal">node-status-update-frequency</code> parameter on the worker nodes.
						</li><li class="listitem">
							The Kubernetes Controller Manager Operator updates the <code class="literal">node-monitor-grace-period</code> parameter on the control plane nodes.
						</li><li class="listitem">
							The Kubernetes API Server Operator updates the <code class="literal">default-not-ready-toleration-seconds</code> and <code class="literal">default-unreachable-toleration-seconds</code> parameters on the control plance nodes.
						</li></ul></div><p>
					While the default configuration works in most cases, OpenShift Container Platform offers two other worker latency profiles for situations where the network is experiencing higher latency than usual. The three worker latency profiles are described in the following sections:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Default worker latency profile</span></dt><dd><p class="simpara">
								With the <code class="literal">Default</code> profile, each kubelet reports its node status to the Kubelet Controller Manager Operator (kube controller) every 10 seconds. The Kubelet Controller Manager Operator checks the kubelet for a status every 5 seconds.
							</p><p class="simpara">
								The Kubernetes Controller Manager Operator waits 40 seconds for a status update before considering that node unhealthy. It marks the node with the <code class="literal">node.kubernetes.io/not-ready</code> or <code class="literal">node.kubernetes.io/unreachable</code> taint and evicts the pods on that node. If a pod on that node has the <code class="literal">NoExecute</code> toleration, the pod gets evicted in 300 seconds. If the pod has the <code class="literal">tolerationSeconds</code> parameter, the eviction waits for the period specified by that parameter.
							</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col><col style="width: 17%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th style="text-align: left; vertical-align: top; " id="idm140232237923136" scope="col">Profile</th><th style="text-align: left; vertical-align: top; " id="idm140232237922048" scope="col">Component</th><th style="text-align: left; vertical-align: top; " id="idm140232237920960" scope="col">Parameter</th><th style="text-align: left; vertical-align: top; " id="idm140232237919872" scope="col">Value</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; " rowspan="4" headers="idm140232237923136"> <p>
												Default
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237922048"> <p>
												kubelet
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237920960"> <p>
												<code class="literal">node-status-update-frequency</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237919872"> <p>
												10s
											</p>
											 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm140232237922048"> <p>
												Kubelet Controller Manager
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237920960"> <p>
												<code class="literal">node-monitor-grace-period</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237919872"> <p>
												40s
											</p>
											 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm140232237922048"> <p>
												Kubernetes API Server
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237920960"> <p>
												<code class="literal">default-not-ready-toleration-seconds</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237919872"> <p>
												300s
											</p>
											 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm140232237922048"> <p>
												Kubernetes API Server
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237920960"> <p>
												<code class="literal">default-unreachable-toleration-seconds</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237919872"> <p>
												300s
											</p>
											 </td></tr></tbody></table></div></dd><dt><span class="term">Medium worker latency profile</span></dt><dd><p class="simpara">
								Use the <code class="literal">MediumUpdateAverageReaction</code> profile if the network latency is slightly higher than usual.
							</p><p class="simpara">
								The <code class="literal">MediumUpdateAverageReaction</code> profile reduces the frequency of kubelet updates to 20 seconds and changes the period that the Kubernetes Controller Manager Operator waits for those updates to 2 minutes. The pod eviction period for a pod on that node is reduced to 60 seconds. If the pod has the <code class="literal">tolerationSeconds</code> parameter, the eviction waits for the period specified by that parameter.
							</p><p class="simpara">
								The Kubernetes Controller Manager Operator waits for 2 minutes to consider a node unhealthy. In another minute, the eviction process starts.
							</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col><col style="width: 17%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th style="text-align: left; vertical-align: top; " id="idm140232237881744" scope="col">Profile</th><th style="text-align: left; vertical-align: top; " id="idm140232237880656" scope="col">Component</th><th style="text-align: left; vertical-align: top; " id="idm140232237879568" scope="col">Parameter</th><th style="text-align: left; vertical-align: top; " id="idm140232237878480" scope="col">Value</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; " rowspan="4" headers="idm140232237881744"> <p>
												MediumUpdateAverageReaction
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237880656"> <p>
												kubelet
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237879568"> <p>
												<code class="literal">node-status-update-frequency</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237878480"> <p>
												20s
											</p>
											 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm140232237880656"> <p>
												Kubelet Controller Manager
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237879568"> <p>
												<code class="literal">node-monitor-grace-period</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237878480"> <p>
												2m
											</p>
											 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm140232237880656"> <p>
												Kubernetes API Server
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237879568"> <p>
												<code class="literal">default-not-ready-toleration-seconds</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237878480"> <p>
												60s
											</p>
											 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm140232237880656"> <p>
												Kubernetes API Server
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237879568"> <p>
												<code class="literal">default-unreachable-toleration-seconds</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237878480"> <p>
												60s
											</p>
											 </td></tr></tbody></table></div></dd><dt><span class="term">Low worker latency profile</span></dt><dd><p class="simpara">
								Use the <code class="literal">LowUpdateSlowReaction</code> profile if the network latency is extremely high.
							</p><p class="simpara">
								The <code class="literal">LowUpdateSlowReaction</code> profile reduces the frequency of kubelet updates to 1 minute and changes the period that the Kubernetes Controller Manager Operator waits for those updates to 5 minutes. The pod eviction period for a pod on that node is reduced to 60 seconds. If the pod has the <code class="literal">tolerationSeconds</code> parameter, the eviction waits for the period specified by that parameter.
							</p><p class="simpara">
								The Kubernetes Controller Manager Operator waits for 5 minutes to consider a node unhealthy. In another minute, the eviction process starts.
							</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col><col style="width: 17%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th style="text-align: left; vertical-align: top; " id="idm140232237840432" scope="col">Profile</th><th style="text-align: left; vertical-align: top; " id="idm140232237839344" scope="col">Component</th><th style="text-align: left; vertical-align: top; " id="idm140232237838256" scope="col">Parameter</th><th style="text-align: left; vertical-align: top; " id="idm140232237837168" scope="col">Value</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; " rowspan="4" headers="idm140232237840432"> <p>
												LowUpdateSlowReaction
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237839344"> <p>
												kubelet
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237838256"> <p>
												<code class="literal">node-status-update-frequency</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237837168"> <p>
												1m
											</p>
											 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm140232237839344"> <p>
												Kubelet Controller Manager
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237838256"> <p>
												<code class="literal">node-monitor-grace-period</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237837168"> <p>
												5m
											</p>
											 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm140232237839344"> <p>
												Kubernetes API Server
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237838256"> <p>
												<code class="literal">default-not-ready-toleration-seconds</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237837168"> <p>
												60s
											</p>
											 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm140232237839344"> <p>
												Kubernetes API Server
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237838256"> <p>
												<code class="literal">default-unreachable-toleration-seconds</code>
											</p>
											 </td><td style="text-align: left; vertical-align: top; " headers="idm140232237837168"> <p>
												60s
											</p>
											 </td></tr></tbody></table></div></dd></dl></div></section><section class="section" id="nodes-cluster-worker-latency-profiles-using_nodes-cluster-worker-latency-profiles"><div class="titlepage"><div><div><h3 class="title">8.9.2. Using worker latency profiles</h3></div></div></div><p>
					To implement a worker latency profile to deal with network latency, edit the <code class="literal">node.config</code> object to add the name of the profile. You can change the profile at any time as latency increases or decreases.
				</p><p>
					You must move one worker latency profile at a time. For example, you cannot move directly from the <code class="literal">Default</code> profile to the <code class="literal">LowUpdateSlowReaction</code> worker latency profile. You must move from the <code class="literal">default</code> worker latency profile to the <code class="literal">MediumUpdateAverageReaction</code> profile first, then to <code class="literal">LowUpdateSlowReaction</code>. Similarly, when returning to the default profile, you must move from the low profile to the medium profile first, then to the default.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can also configure worker latency profiles upon installing an OpenShift Container Platform cluster.
					</p></div></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To move from the default worker latency profile:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Move to the medium worker latency profile:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Edit the <code class="literal">node.config</code> object:
								</p><pre class="programlisting language-terminal">$ oc edit nodes.config/cluster</pre></li><li class="listitem"><p class="simpara">
									Add <code class="literal">spec.workerLatencyProfile: MediumUpdateAverageReaction</code>:
								</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">node.config</code> object</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Node
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2022-07-08T16:02:51Z"
  generation: 1
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    kind: ClusterVersion
    name: version
    uid: 36282574-bf9f-409e-a6cd-3032939293eb
  resourceVersion: "1865"
  uid: 0c0f7a4c-4307-4187-b591-6155695ac85b
spec:
  workerLatencyProfile: MediumUpdateAverageReaction <span id="CO175-1"><!--Empty--></span><span class="callout">1</span>

# ...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO175-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specifies the medium worker latency policy.
										</div></dd></dl></div><p class="simpara">
									Scheduling on each worker node is disabled as the change is being applied.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Optional: Move to the low worker latency profile:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Edit the <code class="literal">node.config</code> object:
								</p><pre class="programlisting language-terminal">$ oc edit nodes.config/cluster</pre></li><li class="listitem"><p class="simpara">
									Change the <code class="literal">spec.workerLatencyProfile</code> value to <code class="literal">LowUpdateSlowReaction</code>:
								</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">node.config</code> object</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Node
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2022-07-08T16:02:51Z"
  generation: 1
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    kind: ClusterVersion
    name: version
    uid: 36282574-bf9f-409e-a6cd-3032939293eb
  resourceVersion: "1865"
  uid: 0c0f7a4c-4307-4187-b591-6155695ac85b
spec:
  workerLatencyProfile: LowUpdateSlowReaction <span id="CO176-1"><!--Empty--></span><span class="callout">1</span>

# ...</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO176-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specifies to use the low worker latency policy.
										</div></dd></dl></div><p class="simpara">
									Scheduling on each worker node is disabled as the change is being applied.
								</p></li></ol></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							When all nodes return to the <code class="literal">Ready</code> condition, you can use the following command to look in the Kubernetes Controller Manager to ensure it was applied:
						</p><pre class="programlisting language-terminal">$ oc get KubeControllerManager -o yaml | grep -i workerlatency -A 5 -B 5</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal"># ...
    - lastTransitionTime: "2022-07-11T19:47:10Z"
      reason: ProfileUpdated
      status: "False"
      type: WorkerLatencyProfileProgressing
    - lastTransitionTime: "2022-07-11T19:47:10Z" <span id="CO177-1"><!--Empty--></span><span class="callout">1</span>
      message: all static pod revision(s) have updated latency profile
      reason: ProfileUpdated
      status: "True"
      type: WorkerLatencyProfileComplete
    - lastTransitionTime: "2022-07-11T19:20:11Z"
      reason: AsExpected
      status: "False"
      type: WorkerLatencyProfileDegraded
    - lastTransitionTime: "2022-07-11T19:20:36Z"
      status: "False"
# ...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO177-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specifies that the profile is applied and active.
								</div></dd></dl></div></li></ul></div><p>
					To change the low profile to medium or change the medium to low, edit the <code class="literal">node.config</code> object and set the <code class="literal">spec.workerLatencyProfile</code> parameter to the appropriate value.
				</p></section></section></section><section class="chapter" id="remote-worker-nodes-on-the-network-edge"><div class="titlepage"><div><div><h1 class="title">Chapter 9. Remote worker nodes on the network edge</h1></div></div></div><section class="section" id="nodes-edge-remote-workers"><div class="titlepage"><div><div><h2 class="title">9.1. Using remote worker nodes at the network edge</h2></div></div></div><p>
				You can configure OpenShift Container Platform clusters with nodes located at your network edge. In this topic, they are called <span class="emphasis"><em>remote worker nodes</em></span>. A typical cluster with remote worker nodes combines on-premise master and worker nodes with worker nodes in other locations that connect to the cluster. This topic is intended to provide guidance on best practices for using remote worker nodes and does not contain specific configuration details.
			</p><p>
				There are multiple use cases across different industries, such as telecommunications, retail, manufacturing, and government, for using a deployment pattern with remote worker nodes. For example, you can separate and isolate your projects and workloads by combining the remote worker nodes into <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-edge-remote-workers-strategies-zones_nodes-edge-remote-workers">Kubernetes zones</a>.
			</p><p>
				However, having remote worker nodes can introduce higher latency, intermittent loss of network connectivity, and other issues. Among the challenges in a cluster with remote worker node are:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<span class="strong strong"><strong>Network separation</strong></span>: The OpenShift Container Platform control plane and the remote worker nodes must be able communicate with each other. Because of the distance between the control plane and the remote worker nodes, network issues could prevent this communication. See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-edge-remote-workers-network_nodes-edge-remote-workers">Network separation with remote worker nodes</a> for information on how OpenShift Container Platform responds to network separation and for methods to diminish the impact to your cluster.
					</li><li class="listitem">
						<span class="strong strong"><strong>Power outage</strong></span>: Because the control plane and remote worker nodes are in separate locations, a power outage at the remote location or at any point between the two can negatively impact your cluster. See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-edge-remote-workers-power_nodes-edge-remote-workers">Power loss on remote worker nodes</a> for information on how OpenShift Container Platform responds to a node losing power and for methods to diminish the impact to your cluster.
					</li><li class="listitem">
						<span class="strong strong"><strong>Latency spikes or temporary reduction in throughput</strong></span>: As with any network, any changes in network conditions between your cluster and the remote worker nodes can negatively impact your cluster. OpenShift Container Platform offers multiple <span class="emphasis"><em>worker latency profiles</em></span> that let you control the reaction of the cluster to latency issues.
					</li></ul></div><p>
				Note the following limitations when planning a cluster with remote worker nodes:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						OpenShift Container Platform does not support remote worker nodes that use a different cloud provider than the on-premise cluster uses.
					</li><li class="listitem">
						Moving workloads from one Kubernetes zone to a different Kubernetes zone can be problematic due to system and environment issues, such as a specific type of memory not being available in a different zone.
					</li><li class="listitem">
						Proxies and firewalls can present additional limitations that are beyond the scope of this document. See the relevant OpenShift Container Platform documentation for how to address such limitations, such as <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#configuring-firewall">Configuring your firewall</a>.
					</li><li class="listitem">
						You are responsible for configuring and maintaining L2/L3-level network connectivity between the control plane and the network-edge nodes.
					</li></ul></div><section class="section" id="nodes-rwn_con_adding-remote-worker-nodes_nodes-edge-remote-workers"><div class="titlepage"><div><div><h3 class="title">9.1.1. Adding remote worker nodes</h3></div></div></div><p>
					Adding remote worker nodes to a cluster involves some additional considerations.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							You must ensure that a route or a default gateway is in place to route traffic between the control plane and every remote worker node.
						</li><li class="listitem">
							You must place the Ingress VIP on the control plane.
						</li><li class="listitem">
							Adding remote worker nodes with user-provisioned infrastructure is identical to adding other worker nodes.
						</li><li class="listitem">
							To add remote worker nodes to an installer-provisioned cluster at install time, specify the subnet for each worker node in the <code class="literal">install-config.yaml</code> file before installation. There are no additional settings required for the DHCP server. You must use virtual media, because the remote worker nodes will not have access to the local provisioning network.
						</li><li class="listitem">
							To add remote worker nodes to an installer-provisioned cluster deployed with a provisioning network, ensure that <code class="literal">virtualMediaViaExternalNetwork</code> flag is set to <code class="literal">true</code> in the <code class="literal">install-config.yaml</code> file so that it will add the nodes using virtual media. Remote worker nodes will not have access to the local provisioning network. They must be deployed with virtual media rather than PXE. Additionally, specify each subnet for each group of remote worker nodes and the control plane nodes in the DHCP server.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#ipi-install-establishing-communication-between-subnets_ipi-install-installation-workflow">Establishing communications between subnets</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#ipi-install-configuring-host-network-interfaces-for-subnets_ipi-install-installation-workflow">Configuring host network interfaces for subnets</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#configure-network-components-to-run-on-the-control-plane_ipi-install-installation-workflow">Configuring network components to run on the control plane</a>
						</li></ul></div></section><section class="section" id="nodes-edge-remote-workers-network_nodes-edge-remote-workers"><div class="titlepage"><div><div><h3 class="title">9.1.2. Network separation with remote worker nodes</h3></div></div></div><p>
					All nodes send heartbeats to the Kubernetes Controller Manager Operator (kube controller) in the OpenShift Container Platform cluster every 10 seconds. If the cluster does not receive heartbeats from a node, OpenShift Container Platform responds using several default mechanisms.
				</p><p>
					OpenShift Container Platform is designed to be resilient to network partitions and other disruptions. You can mitigate some of the more common disruptions, such as interruptions from software upgrades, network splits, and routing issues. Mitigation strategies include ensuring that pods on remote worker nodes request the correct amount of CPU and memory resources, configuring an appropriate replication policy, using redundancy across zones, and using Pod Disruption Budgets on workloads.
				</p><p>
					If the kube controller loses contact with a node after a configured period, the node controller on the control plane updates the node health to <code class="literal">Unhealthy</code> and marks the node <code class="literal">Ready</code> condition as <code class="literal">Unknown</code>. In response, the scheduler stops scheduling pods to that node. The on-premise node controller adds a <code class="literal">node.kubernetes.io/unreachable</code> taint with a <code class="literal">NoExecute</code> effect to the node and schedules pods on the node for eviction after five minutes, by default.
				</p><p>
					If a workload controller, such as a <code class="literal">Deployment</code> object or <code class="literal">StatefulSet</code> object, is directing traffic to pods on the unhealthy node and other nodes can reach the cluster, OpenShift Container Platform routes the traffic away from the pods on the node. Nodes that cannot reach the cluster do not get updated with the new traffic routing. As a result, the workloads on those nodes might continue to attempt to reach the unhealthy node.
				</p><p>
					You can mitigate the effects of connection loss by:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							using daemon sets to create pods that tolerate the taints
						</li><li class="listitem">
							using static pods that automatically restart if a node goes down
						</li><li class="listitem">
							using Kubernetes zones to control pod eviction
						</li><li class="listitem">
							configuring pod tolerations to delay or avoid pod eviction
						</li><li class="listitem">
							configuring the kubelet to control the timing of when it marks nodes as unhealthy.
						</li></ul></div><p>
					For more information on using these objects in a cluster with remote worker nodes, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-edge-remote-workers-strategies_nodes-edge-remote-workers">About remote worker node strategies</a>.
				</p></section><section class="section" id="nodes-edge-remote-workers-power_nodes-edge-remote-workers"><div class="titlepage"><div><div><h3 class="title">9.1.3. Power loss on remote worker nodes</h3></div></div></div><p>
					If a remote worker node loses power or restarts ungracefully, OpenShift Container Platform responds using several default mechanisms.
				</p><p>
					If the Kubernetes Controller Manager Operator (kube controller) loses contact with a node after a configured period, the control plane updates the node health to <code class="literal">Unhealthy</code> and marks the node <code class="literal">Ready</code> condition as <code class="literal">Unknown</code>. In response, the scheduler stops scheduling pods to that node. The on-premise node controller adds a <code class="literal">node.kubernetes.io/unreachable</code> taint with a <code class="literal">NoExecute</code> effect to the node and schedules pods on the node for eviction after five minutes, by default.
				</p><p>
					On the node, the pods must be restarted when the node recovers power and reconnects with the control plane.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you want the pods to restart immediately upon restart, use static pods.
					</p></div></div><p>
					After the node restarts, the kubelet also restarts and attempts to restart the pods that were scheduled on the node. If the connection to the control plane takes longer than the default five minutes, the control plane cannot update the node health and remove the <code class="literal">node.kubernetes.io/unreachable</code> taint. On the node, the kubelet terminates any running pods. When these conditions are cleared, the scheduler can start scheduling pods to that node.
				</p><p>
					You can mitigate the effects of power loss by:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							using daemon sets to create pods that tolerate the taints
						</li><li class="listitem">
							using static pods that automatically restart with a node
						</li><li class="listitem">
							configuring pods tolerations to delay or avoid pod eviction
						</li><li class="listitem">
							configuring the kubelet to control the timing of when the node controller marks nodes as unhealthy.
						</li></ul></div><p>
					For more information on using these objects in a cluster with remote worker nodes, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-edge-remote-workers-strategies_nodes-edge-remote-workers">About remote worker node strategies</a>.
				</p></section><section class="section" id="nodes-edge-remote-workers-latency"><div class="titlepage"><div><div><h3 class="title">9.1.4. Latency spikes or temporary reduction in throughput to remote workers</h3></div></div></div><p>
					All nodes send heartbeats to the Kubernetes Controller Manager Operator (kube controller) in the OpenShift Container Platform cluster every 10 seconds, by default. If the cluster does not receive heartbeats from a node, OpenShift Container Platform responds using several default mechanisms.
				</p><p>
					For example, if the Kubernetes Controller Manager Operator loses contact with a node after a configured period:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							The node controller on the control plane updates the node health to <code class="literal">Unhealthy</code> and marks the node <code class="literal">Ready</code> condition as <code class="literal">Unknown</code>.
						</li><li class="listitem">
							In response, the scheduler stops scheduling pods to that node.
						</li><li class="listitem">
							The on-premise node controller adds a <code class="literal">node.kubernetes.io/unreachable</code> taint with a <code class="literal">NoExecute</code> effect to the node and schedules any pods on the node for eviction after five minutes, by default.
						</li></ol></div><p>
					This behavior can cause problems if your network is prone to latency issues, especially if you have nodes at the network edge. In some cases, the Kubernetes Controller Manager Operator might not receive an update from a healthy node due to network latency. The Kubernetes Controller Manager Operator would then evict pods from the node even though the node is healthy. To avoid this problem, you can use <span class="emphasis"><em>worker latency profiles</em></span> to adjust the frequency that the kubelet and the Kubernetes Controller Manager Operator wait for status updates before taking action. These adjustments help to ensure that your cluster runs properly in the event that network latency between the control plane and the worker nodes is not optimal.
				</p><p>
					These worker latency profiles are three sets of parameters that are pre-defined with carefully tuned values that let you control the reaction of the cluster to latency issues without needing to determine the best values manually.
				</p><div class="itemizedlist"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-cluster-worker-latency-profiles">Improving cluster stability in high latency environments using worker latency profiles </a>
						</li></ul></div></section><section class="section" id="nodes-edge-remote-workers-strategies_nodes-edge-remote-workers"><div class="titlepage"><div><div><h3 class="title">9.1.5. Remote worker node strategies</h3></div></div></div><p>
					If you use remote worker nodes, consider which objects to use to run your applications.
				</p><p>
					It is recommended to use daemon sets or static pods based on the behavior you want in the event of network issues or power loss. In addition, you can use Kubernetes zones and tolerations to control or avoid pod evictions if the control plane cannot reach remote worker nodes.
				</p><div class="variablelist" id="nodes-edge-remote-workers-strategies-daemonsets_nodes-edge-remote-workers"><dl class="variablelist"><dt><span class="term">Daemon sets</span></dt><dd>
								Daemon sets are the best approach to managing pods on remote worker nodes for the following reasons:
							</dd></dl></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Daemon sets do not typically need rescheduling behavior. If a node disconnects from the cluster, pods on the node can continue to run. OpenShift Container Platform does not change the state of daemon set pods, and leaves the pods in the state they last reported. For example, if a daemon set pod is in the <code class="literal">Running</code> state, when a node stops communicating, the pod keeps running and is assumed to be running by OpenShift Container Platform.
						</li><li class="listitem"><p class="simpara">
							Daemon set pods, by default, are created with <code class="literal">NoExecute</code> tolerations for the <code class="literal">node.kubernetes.io/unreachable</code> and <code class="literal">node.kubernetes.io/not-ready</code> taints with no <code class="literal">tolerationSeconds</code> value. These default values ensure that daemon set pods are never evicted if the control plane cannot reach a node. For example:
						</p><div class="formalpara"><p class="title"><strong>Tolerations added to daemon set pods by default</strong></p><p>
								
<pre class="programlisting language-yaml">  tolerations:
    - key: node.kubernetes.io/not-ready
      operator: Exists
      effect: NoExecute
    - key: node.kubernetes.io/unreachable
      operator: Exists
      effect: NoExecute
    - key: node.kubernetes.io/disk-pressure
      operator: Exists
      effect: NoSchedule
    - key: node.kubernetes.io/memory-pressure
      operator: Exists
      effect: NoSchedule
    - key: node.kubernetes.io/pid-pressure
      operator: Exists
      effect: NoSchedule
    - key: node.kubernetes.io/unschedulable
      operator: Exists
      effect: NoSchedule</pre>

							</p></div></li><li class="listitem">
							Daemon sets can use labels to ensure that a workload runs on a matching worker node.
						</li><li class="listitem">
							You can use an OpenShift Container Platform service endpoint to load balance daemon set pods.
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Daemon sets do not schedule pods after a reboot of the node if OpenShift Container Platform cannot reach the node.
					</p></div></div><div class="variablelist" id="nodes-edge-remote-workers-strategies-static_nodes-edge-remote-workers"><dl class="variablelist"><dt><span class="term">Static pods</span></dt><dd>
								If you want pods restart if a node reboots, after a power loss for example, consider <a class="link" href="https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/">static pods</a>. The kubelet on a node automatically restarts static pods as node restarts.
							</dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Static pods cannot use secrets and config maps.
					</p></div></div><div class="variablelist" id="nodes-edge-remote-workers-strategies-zones_nodes-edge-remote-workers"><dl class="variablelist"><dt><span class="term">Kubernetes zones</span></dt><dd>
								<a class="link" href="https://kubernetes.io/docs/setup/best-practices/multiple-zones/">Kubernetes zones</a> can slow down the rate or, in some cases, completely stop pod evictions.
							</dd></dl></div><p>
					When the control plane cannot reach a node, the node controller, by default, applies <code class="literal">node.kubernetes.io/unreachable</code> taints and evicts pods at a rate of 0.1 nodes per second. However, in a cluster that uses Kubernetes zones, pod eviction behavior is altered.
				</p><p>
					If a zone is fully disrupted, where all nodes in the zone have a <code class="literal">Ready</code> condition that is <code class="literal">False</code> or <code class="literal">Unknown</code>, the control plane does not apply the <code class="literal">node.kubernetes.io/unreachable</code> taint to the nodes in that zone.
				</p><p>
					For partially disrupted zones, where more than 55% of the nodes have a <code class="literal">False</code> or <code class="literal">Unknown</code> condition, the pod eviction rate is reduced to 0.01 nodes per second. Nodes in smaller clusters, with fewer than 50 nodes, are not tainted. Your cluster must have more than three zones for these behavior to take effect.
				</p><p>
					You assign a node to a specific zone by applying the <code class="literal">topology.kubernetes.io/region</code> label in the node specification.
				</p><div class="formalpara"><p class="title"><strong>Sample node labels for Kubernetes zones</strong></p><p>
						
<pre class="programlisting language-yaml">kind: Node
apiVersion: v1
metadata:
  labels:
    topology.kubernetes.io/region=east</pre>

					</p></div><div class="variablelist" id="nodes-edge-remote-workers-strategies-kubeconfig_nodes-edge-remote-workers"><dl class="variablelist"><dt><span class="term"><code class="literal">KubeletConfig</code> objects</span></dt><dd><!--Empty--></dd></dl></div><p>
					You can adjust the amount of time that the kubelet checks the state of each node.
				</p><p>
					To set the interval that affects the timing of when the on-premise node controller marks nodes with the <code class="literal">Unhealthy</code> or <code class="literal">Unreachable</code> condition, create a <code class="literal">KubeletConfig</code> object that contains the <code class="literal">node-status-update-frequency</code> and <code class="literal">node-status-report-frequency</code> parameters.
				</p><p>
					The kubelet on each node determines the node status as defined by the <code class="literal">node-status-update-frequency</code> setting and reports that status to the cluster based on the <code class="literal">node-status-report-frequency</code> setting. By default, the kubelet determines the pod status every 10 seconds and reports the status every minute. However, if the node state changes, the kubelet reports the change to the cluster immediately. OpenShift Container Platform uses the <code class="literal">node-status-report-frequency</code> setting only when the Node Lease feature gate is enabled, which is the default state in OpenShift Container Platform clusters. If the Node Lease feature gate is disabled, the node reports its status based on the <code class="literal">node-status-update-frequency</code> setting.
				</p><div class="formalpara"><p class="title"><strong>Example kubelet config</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: disable-cpu-units
spec:
  machineConfigPoolSelector:
    matchLabels:
      machineconfiguration.openshift.io/role: worker <span id="CO178-1"><!--Empty--></span><span class="callout">1</span>
  kubeletConfig:
    node-status-update-frequency: <span id="CO178-2"><!--Empty--></span><span class="callout">2</span>
      - "10s"
    node-status-report-frequency: <span id="CO178-3"><!--Empty--></span><span class="callout">3</span>
      - "1m"</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO178-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Specify the type of node type to which this <code class="literal">KubeletConfig</code> object applies using the label from the <code class="literal">MachineConfig</code> object.
						</div></dd><dt><a href="#CO178-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Specify the frequency that the kubelet checks the status of a node associated with this <code class="literal">MachineConfig</code> object. The default value is <code class="literal">10s</code>. If you change this default, the <code class="literal">node-status-report-frequency</code> value is changed to the same value.
						</div></dd><dt><a href="#CO178-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Specify the frequency that the kubelet reports the status of a node associated with this <code class="literal">MachineConfig</code> object. The default value is <code class="literal">1m</code>.
						</div></dd></dl></div><p>
					The <code class="literal">node-status-update-frequency</code> parameter works with the <code class="literal">node-monitor-grace-period</code> and <code class="literal">pod-eviction-timeout</code> parameters.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The <code class="literal">node-monitor-grace-period</code> parameter specifies how long OpenShift Container Platform waits after a node associated with a <code class="literal">MachineConfig</code> object is marked <code class="literal">Unhealthy</code> if the controller manager does not receive the node heartbeat. Workloads on the node continue to run after this time. If the remote worker node rejoins the cluster after <code class="literal">node-monitor-grace-period</code> expires, pods continue to run. New pods can be scheduled to that node. The <code class="literal">node-monitor-grace-period</code> interval is <code class="literal">40s</code>. The <code class="literal">node-status-update-frequency</code> value must be lower than the <code class="literal">node-monitor-grace-period</code> value.
						</li><li class="listitem">
							The <code class="literal">pod-eviction-timeout</code> parameter specifies the amount of time OpenShift Container Platform waits after marking a node that is associated with a <code class="literal">MachineConfig</code> object as <code class="literal">Unreachable</code> to start marking pods for eviction. Evicted pods are rescheduled on other nodes. If the remote worker node rejoins the cluster after <code class="literal">pod-eviction-timeout</code> expires, the pods running on the remote worker node are terminated because the node controller has evicted the pods on-premise. Pods can then be rescheduled to that node. The <code class="literal">pod-eviction-timeout</code> interval is <code class="literal">5m0s</code>.
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Modifying the <code class="literal">node-monitor-grace-period</code> and <code class="literal">pod-eviction-timeout</code> parameters is not supported.
					</p></div></div><div class="variablelist" id="nodes-edge-remote-workers-strategies-tolerations_nodes-edge-remote-workers"><dl class="variablelist"><dt><span class="term">Tolerations</span></dt><dd>
								You can use pod tolerations to mitigate the effects if the on-premise node controller adds a <code class="literal">node.kubernetes.io/unreachable</code> taint with a <code class="literal">NoExecute</code> effect to a node it cannot reach.
							</dd></dl></div><p>
					A taint with the <code class="literal">NoExecute</code> effect affects pods that are running on the node in the following ways:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Pods that do not tolerate the taint are queued for eviction.
						</li><li class="listitem">
							Pods that tolerate the taint without specifying a <code class="literal">tolerationSeconds</code> value in their toleration specification remain bound forever.
						</li><li class="listitem">
							Pods that tolerate the taint with a specified <code class="literal">tolerationSeconds</code> value remain bound for the specified amount of time. After the time elapses, the pods are queued for eviction.
						</li></ul></div><p>
					You can delay or avoid pod eviction by configuring pods tolerations with the <code class="literal">NoExecute</code> effect for the <code class="literal">node.kubernetes.io/unreachable</code> and <code class="literal">node.kubernetes.io/not-ready</code> taints.
				</p><div class="formalpara"><p class="title"><strong>Example toleration in a pod spec</strong></p><p>
						
<pre class="programlisting language-yaml">...
tolerations:
- key: "node.kubernetes.io/unreachable"
  operator: "Exists"
  effect: "NoExecute" <span id="CO179-1"><!--Empty--></span><span class="callout">1</span>
- key: "node.kubernetes.io/not-ready"
  operator: "Exists"
  effect: "NoExecute" <span id="CO179-2"><!--Empty--></span><span class="callout">2</span>
  tolerationSeconds: 600
...</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO179-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The <code class="literal">NoExecute</code> effect without <code class="literal">tolerationSeconds</code> lets pods remain forever if the control plane cannot reach the node.
						</div></dd><dt><a href="#CO179-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							The <code class="literal">NoExecute</code> effect with <code class="literal">tolerationSeconds</code>: 600 lets pods remain for 10 minutes if the control plane marks the node as <code class="literal">Unhealthy</code>.
						</div></dd></dl></div><p>
					OpenShift Container Platform uses the <code class="literal">tolerationSeconds</code> value after the <code class="literal">pod-eviction-timeout</code> value elapses.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Other types of OpenShift Container Platform objects</span></dt><dd>
								You can use replica sets, deployments, and replication controllers. The scheduler can reschedule these pods onto other nodes after the node is disconnected for five minutes. Rescheduling onto other nodes can be beneficial for some workloads, such as REST APIs, where an administrator can guarantee a specific number of pods are running and accessible.
							</dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When working with remote worker nodes, rescheduling pods on different nodes might not be acceptable if remote worker nodes are intended to be reserved for specific functions.
					</p></div></div><p id="nodes-edge-remote-workers-strategies-statefulset_nodes-edge-remote-workers">
					<a class="link" href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">stateful sets</a> do not get restarted when there is an outage. The pods remain in the <code class="literal">terminating</code> state until the control plane can acknowledge that the pods are terminated.
				</p><p>
					To avoid scheduling a to a node that does not have access to the same type of persistent storage, OpenShift Container Platform cannot migrate pods that require persistent volumes to other zones in the case of network separation.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For more information on Daemonesets, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-daemonsets">DaemonSets</a>.
						</li><li class="listitem">
							For more information on taints and tolerations, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-taints-tolerations-about_nodes-scheduler-taints-tolerations">Controlling pod placement using node taints</a>.
						</li><li class="listitem">
							For more information on configuring <code class="literal">KubeletConfig</code> objects, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#create-a-kubeletconfig-crd-to-edit-kubelet-parameters_post-install-node-tasks">Creating a KubeletConfig CRD</a>.
						</li><li class="listitem">
							For more information on replica sets, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/building_applications/#deployments-repliasets_what-deployments-are">ReplicaSets</a>.
						</li><li class="listitem">
							For more information on deployments, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/building_applications/#deployments-kube-deployments_what-deployments-are">Deployments</a>.
						</li><li class="listitem">
							For more information on replication controllers, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/building_applications/#deployments-replicationcontrollers_what-deployments-are">Replication controllers</a>.
						</li><li class="listitem">
							For more information on the controller manager, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#kube-controller-manager-operator_cluster-operators-ref">Kubernetes Controller Manager Operator</a>.
						</li></ul></div></section></section></section><section class="chapter" id="worker-nodes-for-single-node-openshift-clusters"><div class="titlepage"><div><div><h1 class="title">Chapter 10. Worker nodes for single-node OpenShift clusters</h1></div></div></div><section class="section" id="nodes-sno-worker-nodes"><div class="titlepage"><div><div><h2 class="title">10.1. Adding worker nodes to single-node OpenShift clusters</h2></div></div></div><p>
				Single-node OpenShift clusters reduce the host prerequisites for deployment to a single host. This is useful for deployments in constrained environments or at the network edge. However, sometimes you need to add additional capacity to your cluster, for example, in telecommunications and network edge scenarios. In these scenarios, you can add worker nodes to the single-node cluster.
			</p><p>
				There are several ways that you can add worker nodes to a single-node cluster. You can add worker nodes to a cluster manually, using <a class="link" href="https://console.redhat.com/openshift/assisted-installer/clusters">Red Hat OpenShift Cluster Manager</a>, or by using the Assisted Installer REST API directly.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Adding worker nodes does not expand the cluster control plane, and it does not provide high availability to your cluster. For single-node OpenShift clusters, high availability is handled by failing over to another site. It is not recommended to add a large number of worker nodes to a single-node cluster.
				</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Unlike multi-node clusters, by default all ingress traffic is routed to the single control-plane node, even after adding additional worker nodes.
				</p></div></div><section class="section" id="ai-sno-requirements-for-installing-worker-nodes_add-workers"><div class="titlepage"><div><div><h3 class="title">10.1.1. Requirements for installing single-node OpenShift worker nodes</h3></div></div></div><p>
					To install a single-node OpenShift worker node, you must address the following requirements:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<span class="strong strong"><strong>Administration host:</strong></span> You must have a computer to prepare the ISO and to monitor the installation.
						</li><li class="listitem"><p class="simpara">
							<span class="strong strong"><strong>Production-grade server:</strong></span> Installing single-node OpenShift worker nodes requires a server with sufficient resources to run OpenShift Container Platform services and a production workload.
						</p><div class="table" id="idm140232237546144"><p class="title"><strong>Table 10.1. Minimum resource requirements</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232237539440" scope="col">Profile</th><th align="left" valign="top" id="idm140232237538352" scope="col">vCPU</th><th align="left" valign="top" id="idm140232237537264" scope="col">Memory</th><th align="left" valign="top" id="idm140232237536176" scope="col">Storage</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232237539440"> <p>
											Minimum
										</p>
										 </td><td align="left" valign="top" headers="idm140232237538352"> <p>
											2 vCPU cores
										</p>
										 </td><td align="left" valign="top" headers="idm140232237537264"> <p>
											8GB of RAM
										</p>
										 </td><td align="left" valign="top" headers="idm140232237536176"> <p>
											100GB
										</p>
										 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								One vCPU is equivalent to one physical core when simultaneous multithreading (SMT), or hyperthreading, is not enabled. When enabled, use the following formula to calculate the corresponding ratio:
							</p><p>
								(threads per core × cores) × sockets = vCPUs
							</p></div></div><p class="simpara">
							The server must have a Baseboard Management Controller (BMC) when booting with virtual media.
						</p></li><li class="listitem"><p class="simpara">
							<span class="strong strong"><strong>Networking:</strong></span> The worker node server must have access to the internet or access to a local registry if it is not connected to a routable network. The worker node server must have a DHCP reservation or a static IP address and be able to access the single-node OpenShift cluster Kubernetes API, ingress route, and cluster node domain names. You must configure the DNS to resolve the IP address to each of the following fully qualified domain names (FQDN) for the single-node OpenShift cluster:
						</p><div class="table" id="idm140232237522624"><p class="title"><strong>Table 10.2. Required DNS records</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140232237516864" scope="col">Usage</th><th align="left" valign="top" id="idm140232237515776" scope="col">FQDN</th><th align="left" valign="top" id="idm140232237514688" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140232237516864"> <p>
											Kubernetes API
										</p>
										 </td><td align="left" valign="top" headers="idm140232237515776"> <p>
											<code class="literal">api.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140232237514688"> <p>
											Add a DNS A/AAAA or CNAME record. This record must be resolvable by clients external to the cluster.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140232237516864"> <p>
											Internal API
										</p>
										 </td><td align="left" valign="top" headers="idm140232237515776"> <p>
											<code class="literal">api-int.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140232237514688"> <p>
											Add a DNS A/AAAA or CNAME record when creating the ISO manually. This record must be resolvable by nodes within the cluster.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140232237516864"> <p>
											Ingress route
										</p>
										 </td><td align="left" valign="top" headers="idm140232237515776"> <p>
											<code class="literal">*.apps.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140232237514688"> <p>
											Add a wildcard DNS A/AAAA or CNAME record that targets the node. This record must be resolvable by clients external to the cluster.
										</p>
										 </td></tr></tbody></table></div></div><p class="simpara">
							Without persistent IP addresses, communications between the <code class="literal">apiserver</code> and <code class="literal">etcd</code> might fail.
						</p></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-minimum-resource-requirements_installing-restricted-networks-bare-metal">Minimum resource requirements for cluster installation</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#recommended-scale-practices_cluster-scaling">Recommended practices for scaling the cluster</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-dns-user-infra_installing-bare-metal-network-customizations">User-provisioned DNS requirements</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-with-usb-media_install-sno-installing-sno-with-the-assisted-installer">Creating a bootable ISO image on a USB drive</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#install-booting-from-an-iso-over-http-redfish_install-sno-installing-sno-with-the-assisted-installer">Booting from an ISO image served over HTTP using the Redfish API</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-working-deleting_nodes-nodes-working">Deleting nodes from a cluster</a>
						</li></ul></div></section><section class="section" id="sno-adding-worker-nodes-to-sno-clusters_add-workers"><div class="titlepage"><div><div><h3 class="title">10.1.2. Adding worker nodes using the Assisted Installer and OpenShift Cluster Manager</h3></div></div></div><p>
					You can add worker nodes to single-node OpenShift clusters that were created on <a class="link" href="https://console.redhat.com">Red Hat OpenShift Cluster Manager</a> using the <a class="link" href="https://console.redhat.com/openshift/assisted-installer/clusters/~new">Assisted Installer</a>.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Adding worker nodes to single-node OpenShift clusters is only supported for clusters running OpenShift Container Platform version 4.11 and up.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Have access to a single-node OpenShift cluster installed using <a class="link" href="https://console.redhat.com/openshift/assisted-installer/clusters/~new">Assisted Installer</a>.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							Ensure that all the required DNS records exist for the cluster that you are adding the worker node to.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to <a class="link" href="https://console.redhat.com/openshift">OpenShift Cluster Manager</a> and click the single-node cluster that you want to add a worker node to.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Add hosts</strong></span>, and download the discovery ISO for the new worker node, adding SSH public key and configuring cluster-wide proxy settings as required.
						</li><li class="listitem">
							Boot the target host using the discovery ISO, and wait for the host to be discovered in the console. After the host is discovered, start the installation.
						</li><li class="listitem"><p class="simpara">
							As the installation proceeds, the installation generates pending certificate signing requests (CSRs) for the worker node. When prompted, approve the pending CSRs to complete the installation.
						</p><p class="simpara">
							When the worker node is sucessfully installed, it is listed as a worker node in the cluster web console.
						</p></li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						New worker nodes will be encrypted using the same method as the original cluster.
					</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-dns-user-infra_installing-bare-metal-network-customizations">User-provisioned DNS requirements</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#installation-approve-csrs_add-workers">Approving the certificate signing requests for your machines</a>
						</li></ul></div></section><section class="section" id="adding-worker-nodes-using-the-assisted-installer-api"><div class="titlepage"><div><div><h3 class="title">10.1.3. Adding worker nodes using the Assisted Installer API</h3></div></div></div><p>
					You can add worker nodes to single-node OpenShift clusters using the Assisted Installer REST API. Before you add worker nodes, you must log in to <a class="link" href="https://console.redhat.com/openshift/token/show">OpenShift Cluster Manager</a> and authenticate against the API.
				</p><section class="section" id="ai-authenticating-against-ai-rest-api_add-workers"><div class="titlepage"><div><div><h4 class="title">10.1.3.1. Authenticating against the Assisted Installer REST API</h4></div></div></div><p>
						Before you can use the Assisted Installer REST API, you must authenticate against the API using a JSON web token (JWT) that you generate.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Log in to <a class="link" href="https://console.redhat.com/openshift/assisted-installer/clusters">OpenShift Cluster Manager</a> as a user with cluster creation privileges.
							</li><li class="listitem">
								Install <code class="literal">jq</code>.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Log in to <a class="link" href="https://console.redhat.com/openshift/token/show">OpenShift Cluster Manager</a> and copy your API token.
							</li><li class="listitem"><p class="simpara">
								Set the <code class="literal">$OFFLINE_TOKEN</code> variable using the copied API token by running the following command:
							</p><pre class="programlisting language-terminal">$ export OFFLINE_TOKEN=&lt;copied_api_token&gt;</pre></li><li class="listitem"><p class="simpara">
								Set the <code class="literal">$JWT_TOKEN</code> variable using the previously set <code class="literal">$OFFLINE_TOKEN</code> variable:
							</p><pre class="programlisting language-terminal">$ export JWT_TOKEN=$(
  curl \
  --silent \
  --header "Accept: application/json" \
  --header "Content-Type: application/x-www-form-urlencoded" \
  --data-urlencode "grant_type=refresh_token" \
  --data-urlencode "client_id=cloud-services" \
  --data-urlencode "refresh_token=${OFFLINE_TOKEN}" \
  "https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token" \
  | jq --raw-output ".access_token"
)</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									The JWT token is valid for 15 minutes only.
								</p></div></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Optional: Check that you can access the API by running the following command:
							</p><pre class="programlisting language-terminal">$ curl -s https://api.openshift.com/api/assisted-install/v2/component-versions -H "Authorization: Bearer ${JWT_TOKEN}" | jq</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-yaml">{
    "release_tag": "v2.5.1",
    "versions":
    {
        "assisted-installer": "registry.redhat.io/rhai-tech-preview/assisted-installer-rhel8:v1.0.0-175",
        "assisted-installer-controller": "registry.redhat.io/rhai-tech-preview/assisted-installer-reporter-rhel8:v1.0.0-223",
        "assisted-installer-service": "quay.io/app-sre/assisted-service:ac87f93",
        "discovery-agent": "registry.redhat.io/rhai-tech-preview/assisted-installer-agent-rhel8:v1.0.0-156"
    }
}</pre>

								</p></div></li></ul></div></section><section class="section" id="ai-adding-worker-nodes-to-cluster_add-workers"><div class="titlepage"><div><div><h4 class="title">10.1.3.2. Adding worker nodes using the Assisted Installer REST API</h4></div></div></div><p>
						You can add worker nodes to clusters using the Assisted Installer REST API.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								Install the OpenShift Cluster Manager CLI (<code class="literal">ocm</code>).
							</li><li class="listitem">
								Log in to <a class="link" href="https://console.redhat.com/openshift/assisted-installer/clusters">OpenShift Cluster Manager</a> as a user with cluster creation privileges.
							</li><li class="listitem">
								Install <code class="literal">jq</code>.
							</li><li class="listitem">
								Ensure that all the required DNS records exist for the cluster that you are adding the worker node to.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Authenticate against the Assisted Installer REST API and generate a JSON web token (JWT) for your session. The generated JWT token is valid for 15 minutes only.
							</li><li class="listitem"><p class="simpara">
								Set the <code class="literal">$API_URL</code> variable by running the following command:
							</p><pre class="programlisting language-terminal">$ export API_URL=&lt;api_url&gt; <span id="CO180-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO180-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;api_url&gt;</code> with the Assisted Installer API URL, for example, <code class="literal"><a class="link" href="https://api.openshift.com">https://api.openshift.com</a></code>
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Import the single-node OpenShift cluster by running the following commands:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										Set the <code class="literal">$OPENSHIFT_CLUSTER_ID</code> variable. Log in to the cluster and run the following command:
									</p><pre class="programlisting language-terminal">$ export OPENSHIFT_CLUSTER_ID=$(oc get clusterversion -o jsonpath='{.items[].spec.clusterID}')</pre></li><li class="listitem"><p class="simpara">
										Set the <code class="literal">$CLUSTER_REQUEST</code> variable that is used to import the cluster:
									</p><pre class="programlisting language-terminal">$ export CLUSTER_REQUEST=$(jq --null-input --arg openshift_cluster_id "$OPENSHIFT_CLUSTER_ID" '{
  "api_vip_dnsname": "&lt;api_vip&gt;", <span id="CO181-1"><!--Empty--></span><span class="callout">1</span>
  "openshift_cluster_id": $openshift_cluster_id,
  "name": "&lt;openshift_cluster_name&gt;" <span id="CO181-2"><!--Empty--></span><span class="callout">2</span>
}')</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO181-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Replace <code class="literal">&lt;api_vip&gt;</code> with the hostname for the cluster’s API server. This can be the DNS domain for the API server or the IP address of the single node which the worker node can reach. For example, <code class="literal">api.compute-1.example.com</code>.
											</div></dd><dt><a href="#CO181-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Replace <code class="literal">&lt;openshift_cluster_name&gt;</code> with the plain text name for the cluster. The cluster name should match the cluster name that was set during the Day 1 cluster installation.
											</div></dd></dl></div></li><li class="listitem"><p class="simpara">
										Import the cluster and set the <code class="literal">$CLUSTER_ID</code> variable. Run the following command:
									</p><pre class="programlisting language-terminal">$ CLUSTER_ID=$(curl "$API_URL/api/assisted-install/v2/clusters/import" -H "Authorization: Bearer ${JWT_TOKEN}" -H 'accept: application/json' -H 'Content-Type: application/json' \
  -d "$CLUSTER_REQUEST" | tee /dev/stderr | jq -r '.id')</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Generate the <code class="literal">InfraEnv</code> resource for the cluster and set the <code class="literal">$INFRA_ENV_ID</code> variable by running the following commands:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Download the pull secret file from Red Hat OpenShift Cluster Manager at <a class="link" href="console.redhat.com/openshift/install/pull-secret">console.redhat.com</a>.
									</li><li class="listitem"><p class="simpara">
										Set the <code class="literal">$INFRA_ENV_REQUEST</code> variable:
									</p><pre class="programlisting language-terminal">export INFRA_ENV_REQUEST=$(jq --null-input \
    --slurpfile pull_secret &lt;path_to_pull_secret_file&gt; \<span id="CO182-1"><!--Empty--></span><span class="callout">1</span>
    --arg ssh_pub_key "$(cat &lt;path_to_ssh_pub_key&gt;)" \<span id="CO182-2"><!--Empty--></span><span class="callout">2</span>
    --arg cluster_id "$CLUSTER_ID" '{
  "name": "&lt;infraenv_name&gt;", <span id="CO182-3"><!--Empty--></span><span class="callout">3</span>
  "pull_secret": $pull_secret[0] | tojson,
  "cluster_id": $cluster_id,
  "ssh_authorized_key": $ssh_pub_key,
  "image_type": "&lt;iso_image_type&gt;" <span id="CO182-4"><!--Empty--></span><span class="callout">4</span>
}')</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO182-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Replace <code class="literal">&lt;path_to_pull_secret_file&gt;</code> with the path to the local file containing the downloaded pull secret from Red Hat OpenShift Cluster Manager at <a class="link" href="console.redhat.com/openshift/install/pull-secret">console.redhat.com</a>.
											</div></dd><dt><a href="#CO182-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Replace <code class="literal">&lt;path_to_ssh_pub_key&gt;</code> with the path to the public SSH key required to access the host. If you do not set this value, you cannot access the host while in discovery mode.
											</div></dd><dt><a href="#CO182-3"><span class="callout">3</span></a> </dt><dd><div class="para">
												Replace <code class="literal">&lt;infraenv_name&gt;</code> with the plain text name for the <code class="literal">InfraEnv</code> resource.
											</div></dd><dt><a href="#CO182-4"><span class="callout">4</span></a> </dt><dd><div class="para">
												Replace <code class="literal">&lt;iso_image_type&gt;</code> with the ISO image type, either <code class="literal">full-iso</code> or <code class="literal">minimal-iso</code>.
											</div></dd></dl></div></li><li class="listitem"><p class="simpara">
										Post the <code class="literal">$INFRA_ENV_REQUEST</code> to the <a class="link" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service#/installer/RegisterInfraEnv">/v2/infra-envs</a> API and set the <code class="literal">$INFRA_ENV_ID</code> variable:
									</p><pre class="programlisting language-terminal">$ INFRA_ENV_ID=$(curl "$API_URL/api/assisted-install/v2/infra-envs" -H "Authorization: Bearer ${JWT_TOKEN}" -H 'accept: application/json' -H 'Content-Type: application/json' -d "$INFRA_ENV_REQUEST" | tee /dev/stderr | jq -r '.id')</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Get the URL of the discovery ISO for the cluster worker node by running the following command:
							</p><pre class="programlisting language-terminal">$ curl -s "$API_URL/api/assisted-install/v2/infra-envs/$INFRA_ENV_ID" -H "Authorization: Bearer ${JWT_TOKEN}" | jq -r '.download_url'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">https://api.openshift.com/api/assisted-images/images/41b91e72-c33e-42ee-b80f-b5c5bbf6431a?arch=x86_64&amp;image_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2NTYwMjYzNzEsInN1YiI6IjQxYjkxZTcyLWMzM2UtNDJlZS1iODBmLWI1YzViYmY2NDMxYSJ9.1EX_VGaMNejMhrAvVRBS7PDPIQtbOOc8LtG8OukE1a4&amp;type=minimal-iso&amp;version=4.13</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Download the ISO:
							</p><pre class="programlisting language-terminal">$ curl -L -s '&lt;iso_url&gt;' --output rhcos-live-minimal.iso <span id="CO183-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO183-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;iso_url&gt;</code> with the URL for the ISO from the previous step.
									</div></dd></dl></div></li><li class="listitem">
								Boot the new worker host from the downloaded <code class="literal">rhcos-live-minimal.iso</code>.
							</li><li class="listitem"><p class="simpara">
								Get the list of hosts in the cluster that are <span class="emphasis"><em>not</em></span> installed. Keep running the following command until the new host shows up:
							</p><pre class="programlisting language-terminal">$ curl -s "$API_URL/api/assisted-install/v2/clusters/$CLUSTER_ID" -H "Authorization: Bearer ${JWT_TOKEN}" | jq -r '.hosts[] | select(.status != "installed").id'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">2294ba03-c264-4f11-ac08-2f1bb2f8c296</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Set the <code class="literal">$HOST_ID</code> variable for the new worker node, for example:
							</p><pre class="programlisting language-terminal">$ HOST_ID=&lt;host_id&gt; <span id="CO184-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO184-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;host_id&gt;</code> with the host ID from the previous step.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Check that the host is ready to install by running the following command:
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Ensure that you copy the entire command including the complete <code class="literal">jq</code> expression.
								</p></div></div><pre class="programlisting language-terminal">$ curl -s $API_URL/api/assisted-install/v2/clusters/$CLUSTER_ID -H "Authorization: Bearer ${JWT_TOKEN}" | jq '
def host_name($host):
    if (.suggested_hostname // "") == "" then
        if (.inventory // "") == "" then
            "Unknown hostname, please wait"
        else
            .inventory | fromjson | .hostname
        end
    else
        .suggested_hostname
    end;

def is_notable($validation):
    ["failure", "pending", "error"] | any(. == $validation.status);

def notable_validations($validations_info):
    [
        $validations_info // "{}"
        | fromjson
        | to_entries[].value[]
        | select(is_notable(.))
    ];

{
    "Hosts validations": {
        "Hosts": [
            .hosts[]
            | select(.status != "installed")
            | {
                "id": .id,
                "name": host_name(.),
                "status": .status,
                "notable_validations": notable_validations(.validations_info)
            }
        ]
    },
    "Cluster validations info": {
        "notable_validations": notable_validations(.validations_info)
    }
}
' -r</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">{
  "Hosts validations": {
    "Hosts": [
      {
        "id": "97ec378c-3568-460c-bc22-df54534ff08f",
        "name": "localhost.localdomain",
        "status": "insufficient",
        "notable_validations": [
          {
            "id": "ntp-synced",
            "status": "failure",
            "message": "Host couldn't synchronize with any NTP server"
          },
          {
            "id": "api-domain-name-resolved-correctly",
            "status": "error",
            "message": "Parse error for domain name resolutions result"
          },
          {
            "id": "api-int-domain-name-resolved-correctly",
            "status": "error",
            "message": "Parse error for domain name resolutions result"
          },
          {
            "id": "apps-domain-name-resolved-correctly",
            "status": "error",
            "message": "Parse error for domain name resolutions result"
          }
        ]
      }
    ]
  },
  "Cluster validations info": {
    "notable_validations": []
  }
}</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								When the previous command shows that the host is ready, start the installation using the <a class="link" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service#/installer/v2InstallHost">/v2/infra-envs/{infra_env_id}/hosts/{host_id}/actions/install</a> API by running the following command:
							</p><pre class="programlisting language-terminal">$ curl -X POST -s "$API_URL/api/assisted-install/v2/infra-envs/$INFRA_ENV_ID/hosts/$HOST_ID/actions/install"  -H "Authorization: Bearer ${JWT_TOKEN}"</pre></li><li class="listitem"><p class="simpara">
								As the installation proceeds, the installation generates pending certificate signing requests (CSRs) for the worker node.
							</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									You must approve the CSRs to complete the installation.
								</p></div></div><p class="simpara">
								Keep running the following API call to monitor the cluster installation:
							</p><pre class="programlisting language-terminal">$ curl -s "$API_URL/api/assisted-install/v2/clusters/$CLUSTER_ID" -H "Authorization: Bearer ${JWT_TOKEN}" | jq '{
    "Cluster day-2 hosts":
        [
            .hosts[]
            | select(.status != "installed")
            | {id, requested_hostname, status, status_info, progress, status_updated_at, updated_at, infra_env_id, cluster_id, created_at}
        ]
}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">{
  "Cluster day-2 hosts": [
    {
      "id": "a1c52dde-3432-4f59-b2ae-0a530c851480",
      "requested_hostname": "control-plane-1",
      "status": "added-to-existing-cluster",
      "status_info": "Host has rebooted and no further updates will be posted. Please check console for progress and to possibly approve pending CSRs",
      "progress": {
        "current_stage": "Done",
        "installation_percentage": 100,
        "stage_started_at": "2022-07-08T10:56:20.476Z",
        "stage_updated_at": "2022-07-08T10:56:20.476Z"
      },
      "status_updated_at": "2022-07-08T10:56:20.476Z",
      "updated_at": "2022-07-08T10:57:15.306369Z",
      "infra_env_id": "b74ec0c3-d5b5-4717-a866-5b6854791bd3",
      "cluster_id": "8f721322-419d-4eed-aa5b-61b50ea586ae",
      "created_at": "2022-07-06T22:54:57.161614Z"
    }
  ]
}</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Optional: Run the following command to see all the events for the cluster:
							</p><pre class="programlisting language-terminal">$ curl -s "$API_URL/api/assisted-install/v2/events?cluster_id=$CLUSTER_ID" -H "Authorization: Bearer ${JWT_TOKEN}" | jq -c '.[] | {severity, message, event_time, host_id}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">{"severity":"info","message":"Host compute-0: updated status from insufficient to known (Host is ready to be installed)","event_time":"2022-07-08T11:21:46.346Z","host_id":"9d7b3b44-1125-4ad0-9b14-76550087b445"}
{"severity":"info","message":"Host compute-0: updated status from known to installing (Installation is in progress)","event_time":"2022-07-08T11:28:28.647Z","host_id":"9d7b3b44-1125-4ad0-9b14-76550087b445"}
{"severity":"info","message":"Host compute-0: updated status from installing to installing-in-progress (Starting installation)","event_time":"2022-07-08T11:28:52.068Z","host_id":"9d7b3b44-1125-4ad0-9b14-76550087b445"}
{"severity":"info","message":"Uploaded logs for host compute-0 cluster 8f721322-419d-4eed-aa5b-61b50ea586ae","event_time":"2022-07-08T11:29:47.802Z","host_id":"9d7b3b44-1125-4ad0-9b14-76550087b445"}
{"severity":"info","message":"Host compute-0: updated status from installing-in-progress to added-to-existing-cluster (Host has rebooted and no further updates will be posted. Please check console for progress and to possibly approve pending CSRs)","event_time":"2022-07-08T11:29:48.259Z","host_id":"9d7b3b44-1125-4ad0-9b14-76550087b445"}
{"severity":"info","message":"Host: compute-0, reached installation stage Rebooting","event_time":"2022-07-08T11:29:48.261Z","host_id":"9d7b3b44-1125-4ad0-9b14-76550087b445"}</pre>

								</p></div></li><li class="listitem">
								Log in to the cluster and approve the pending CSRs to complete the installation.
							</li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Check that the new worker node was successfully added to the cluster with a status of <code class="literal">Ready</code>:
							</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                           STATUS   ROLES           AGE   VERSION
control-plane-1.example.com    Ready    master,worker   56m   v1.26.0
compute-1.example.com          Ready    worker          11m   v1.26.0</pre>

								</p></div></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-dns-user-infra_installing-bare-metal-network-customizations">User-provisioned DNS requirements</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#installation-approve-csrs_add-workers">Approving the certificate signing requests for your machines</a>
							</li></ul></div></section></section><section class="section" id="sno-adding-worker-nodes-to-single-node-clusters-manually_add-workers"><div class="titlepage"><div><div><h3 class="title">10.1.4. Adding worker nodes to single-node OpenShift clusters manually</h3></div></div></div><p>
					You can add a worker node to a single-node OpenShift cluster manually by booting the worker node from Red Hat Enterprise Linux CoreOS (RHCOS) ISO and by using the cluster <code class="literal">worker.ign</code> file to join the new worker node to the cluster.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install a single-node OpenShift cluster on bare metal.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							Ensure that all the required DNS records exist for the cluster that you are adding the worker node to.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Set the OpenShift Container Platform version:
						</p><pre class="programlisting language-terminal">$ OCP_VERSION=&lt;ocp_version&gt; <span id="CO185-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO185-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal">&lt;ocp_version&gt;</code> with the current version, for example, <code class="literal">latest-4.13</code>
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Set the host architecture:
						</p><pre class="programlisting language-terminal">$ ARCH=&lt;architecture&gt; <span id="CO186-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO186-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal">&lt;architecture&gt;</code> with the target host architecture, for example, <code class="literal">aarch64</code> or <code class="literal">x86_64</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Get the <code class="literal">worker.ign</code> data from the running single-node cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ oc extract -n openshift-machine-api secret/worker-user-data-managed --keys=userData --to=- &gt; worker.ign</pre></li><li class="listitem">
							Host the <code class="literal">worker.ign</code> file on a web server accessible from your network.
						</li><li class="listitem"><p class="simpara">
							Download the OpenShift Container Platform installer and make it available for use by running the following commands:
						</p><pre class="programlisting language-terminal">$ curl -k https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$OCP_VERSION/openshift-install-linux.tar.gz &gt; openshift-install-linux.tar.gz</pre><pre class="programlisting language-terminal">$ tar zxvf openshift-install-linux.tar.gz</pre><pre class="programlisting language-terminal">$ chmod +x openshift-install</pre></li><li class="listitem"><p class="simpara">
							Retrieve the RHCOS ISO URL:
						</p><pre class="programlisting language-terminal">$ ISO_URL=$(./openshift-install coreos print-stream-json | grep location | grep $ARCH | grep iso | cut -d\" -f4)</pre></li><li class="listitem"><p class="simpara">
							Download the RHCOS ISO:
						</p><pre class="programlisting language-terminal">$ curl -L $ISO_URL -o rhcos-live.iso</pre></li><li class="listitem"><p class="simpara">
							Use the RHCOS ISO and the hosted <code class="literal">worker.ign</code> file to install the worker node:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Boot the target host with the RHCOS ISO and your preferred method of installation.
								</li><li class="listitem">
									When the target host has booted from the RHCOS ISO, open a console on the target host.
								</li><li class="listitem"><p class="simpara">
									If your local network does not have DHCP enabled, you need to create an ignition file with the new hostname and configure the worker node static IP address before running the RHCOS installation. Perform the following steps:
								</p><div class="orderedlist"><ol class="orderedlist" type="i"><li class="listitem"><p class="simpara">
											Configure the worker host network connection with a static IP. Run the following command on the target host console:
										</p><pre class="programlisting language-terminal">$ nmcli con mod &lt;network_interface&gt; ipv4.method manual /
ipv4.addresses &lt;static_ip&gt; ipv4.gateway &lt;network_gateway&gt; ipv4.dns &lt;dns_server&gt; /
802-3-ethernet.mtu 9000</pre><p class="simpara">
											where:
										</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;static_ip&gt;</span></dt><dd>
														Is the host static IP address and CIDR, for example, <code class="literal">10.1.101.50/24</code>
													</dd><dt><span class="term">&lt;network_gateway&gt;</span></dt><dd>
														Is the network gateway, for example, <code class="literal">10.1.101.1</code>
													</dd></dl></div></li><li class="listitem"><p class="simpara">
											Activate the modified network interface:
										</p><pre class="programlisting language-terminal">$ nmcli con up &lt;network_interface&gt;</pre></li><li class="listitem"><p class="simpara">
											Create a new ignition file <code class="literal">new-worker.ign</code> that includes a reference to the original <code class="literal">worker.ign</code> and an additional instruction that the <code class="literal">coreos-installer</code> program uses to populate the <code class="literal">/etc/hostname</code> file on the new worker host. For example:
										</p><pre class="programlisting language-json">{
  "ignition":{
    "version":"3.2.0",
    "config":{
      "merge":[
        {
          "source":"&lt;hosted_worker_ign_file&gt;" <span id="CO187-1"><!--Empty--></span><span class="callout">1</span>
        }
      ]
    }
  },
  "storage":{
    "files":[
      {
        "path":"/etc/hostname",
        "contents":{
          "source":"data:,&lt;new_fqdn&gt;" <span id="CO187-2"><!--Empty--></span><span class="callout">2</span>
        },
        "mode":420,
        "overwrite":true,
        "path":"/etc/hostname"
      }
    ]
  }
}</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO187-1"><span class="callout">1</span></a> </dt><dd><div class="para">
													<code class="literal">&lt;hosted_worker_ign_file&gt;</code> is the locally accessible URL for the original <code class="literal">worker.ign</code> file. For example, <code class="literal"><a class="link" href="http://webserver.example.com/worker.ign">http://webserver.example.com/worker.ign</a></code>
												</div></dd><dt><a href="#CO187-2"><span class="callout">2</span></a> </dt><dd><div class="para">
													<code class="literal">&lt;new_fqdn&gt;</code> is the new FQDN that you set for the worker node. For example, <code class="literal">new-worker.example.com</code>.
												</div></dd></dl></div></li><li class="listitem">
											Host the <code class="literal">new-worker.ign</code> file on a web server accessible from your network.
										</li><li class="listitem"><p class="simpara">
											Run the following <code class="literal">coreos-installer</code> command, passing in the <code class="literal">ignition-url</code> and hard disk details:
										</p><pre class="programlisting language-terminal">$ sudo coreos-installer install --copy-network /
--ignition-url=&lt;new_worker_ign_file&gt; &lt;hard_disk&gt; --insecure-ignition</pre><p class="simpara">
											where:
										</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;new_worker_ign_file&gt;</span></dt><dd>
														is the locally accessible URL for the hosted <code class="literal">new-worker.ign</code> file, for example, <code class="literal"><a class="link" href="http://webserver.example.com/new-worker.ign">http://webserver.example.com/new-worker.ign</a></code>
													</dd><dt><span class="term">&lt;hard_disk&gt;</span></dt><dd>
														Is the hard disk where you install RHCOS, for example, <code class="literal">/dev/sda</code>
													</dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
									For networks that have DHCP enabled, you do not need to set a static IP. Run the following <code class="literal">coreos-installer</code> command from the target host console to install the system:
								</p><pre class="programlisting language-terminal">$ coreos-installer install --ignition-url=&lt;hosted_worker_ign_file&gt; &lt;hard_disk&gt;</pre></li><li class="listitem"><p class="simpara">
									To manually enable DHCP, apply the following <code class="literal">NMStateConfig</code> CR to the single-node OpenShift cluster:
								</p><pre class="programlisting language-yaml">apiVersion: agent-install.openshift.io/v1
kind: NMStateConfig
metadata:
  name: nmstateconfig-dhcp
  namespace: example-sno
  labels:
    nmstate_config_cluster_name: &lt;nmstate_config_cluster_label&gt;
spec:
  config:
    interfaces:
      - name: eth0
        type: ethernet
        state: up
        ipv4:
          enabled: true
          dhcp: true
        ipv6:
          enabled: false
  interfaces:
    - name: "eth0"
      macAddress: "AA:BB:CC:DD:EE:11"</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
										The <code class="literal">NMStateConfig</code> CR is required for successful deployments of worker nodes with static IP addresses and for adding a worker node with a dynamic IP address if the single-node OpenShift was deployed with a static IP address. The cluster network DHCP does not automatically set these network settings for the new worker node.
									</p></div></div></li></ol></div></li><li class="listitem">
							As the installation proceeds, the installation generates pending certificate signing requests (CSRs) for the worker node. When prompted, approve the pending CSRs to complete the installation.
						</li><li class="listitem">
							When the install is complete, reboot the host. The host joins the cluster as a new worker node.
						</li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Check that the new worker node was successfully added to the cluster with a status of <code class="literal">Ready</code>:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                           STATUS   ROLES           AGE   VERSION
control-plane-1.example.com    Ready    master,worker   56m   v1.26.0
compute-1.example.com          Ready    worker          11m   v1.26.0</pre>

							</p></div></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-dns-user-infra_installing-bare-metal-network-customizations">User-provisioned DNS requirements</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#installation-approve-csrs_add-workers">Approving the certificate signing requests for your machines</a>
						</li></ul></div></section><section class="section" id="installation-approve-csrs_add-workers"><div class="titlepage"><div><div><h3 class="title">10.1.5. Approving the certificate signing requests for your machines</h3></div></div></div><p>
					When you add machines to a cluster, two pending certificate signing requests (CSRs) are generated for each machine that you added. You must confirm that these CSRs are approved or, if necessary, approve them yourself. The client requests must be approved first, followed by the server requests.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You added machines to your cluster.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Confirm that the cluster recognizes the machines:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  63m  v1.26.0
master-1  Ready     master  63m  v1.26.0
master-2  Ready     master  64m  v1.26.0</pre>

							</p></div><p class="simpara">
							The output lists all of the machines that you created.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The preceding output might not include the compute nodes, also known as worker nodes, until some CSRs are approved.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Review the pending CSRs and ensure that you see the client requests with the <code class="literal">Pending</code> or <code class="literal">Approved</code> status for each machine that you added to the cluster:
						</p><pre class="programlisting language-terminal">$ oc get csr</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-8b2br   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-8vnps   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
...</pre>

							</p></div><p class="simpara">
							In this example, two machines are joining the cluster. You might see more approved CSRs in the list.
						</p></li><li class="listitem"><p class="simpara">
							If the CSRs were not approved, after all of the pending CSRs for the machines you added are in <code class="literal">Pending</code> status, approve the CSRs for your cluster machines:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Because the CSRs rotate automatically, approve your CSRs within an hour of adding the machines to the cluster. If you do not approve them within an hour, the certificates will rotate, and more than two certificates will be present for each node. You must approve all of these certificates. After the client CSR is approved, the Kubelet creates a secondary CSR for the serving certificate, which requires manual approval. Then, subsequent serving certificate renewal requests are automatically approved by the <code class="literal">machine-approver</code> if the Kubelet requests a new certificate with identical parameters.
							</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								For clusters running on platforms that are not machine API enabled, such as bare metal and other user-provisioned infrastructure, you must implement a method of automatically approving the kubelet serving certificate requests (CSRs). If a request is not approved, then the <code class="literal">oc exec</code>, <code class="literal">oc rsh</code>, and <code class="literal">oc logs</code> commands cannot succeed, because a serving certificate is required when the API server connects to the kubelet. Any operation that contacts the Kubelet endpoint requires this certificate approval to be in place. The method must watch for new CSRs, confirm that the CSR was submitted by the <code class="literal">node-bootstrapper</code> service account in the <code class="literal">system:node</code> or <code class="literal">system:admin</code> groups, and confirm the identity of the node.
							</p></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To approve them individually, run the following command for each valid CSR:
								</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt; <span id="CO188-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO188-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											<code class="literal">&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To approve all pending CSRs, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs --no-run-if-empty oc adm certificate approve</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Some Operators might not become available until some CSRs are approved.
									</p></div></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Now that your client requests are approved, you must review the server requests for each machine that you added to the cluster:
						</p><pre class="programlisting language-terminal">$ oc get csr</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-bfd72   5m26s   system:node:ip-10-0-50-126.us-east-2.compute.internal                       Pending
csr-c57lv   5m26s   system:node:ip-10-0-95-157.us-east-2.compute.internal                       Pending
...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							If the remaining CSRs are not approved, and are in the <code class="literal">Pending</code> status, approve the CSRs for your cluster machines:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To approve them individually, run the following command for each valid CSR:
								</p><pre class="programlisting language-terminal">$ oc adm certificate approve &lt;csr_name&gt; <span id="CO189-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO189-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											<code class="literal">&lt;csr_name&gt;</code> is the name of a CSR from the list of current CSRs.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To approve all pending CSRs, run the following command:
								</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs oc adm certificate approve</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							After all client and server CSRs have been approved, the machines have the <code class="literal">Ready</code> status. Verify this by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  73m  v1.26.0
master-1  Ready     master  73m  v1.26.0
master-2  Ready     master  74m  v1.26.0
worker-0  Ready     worker  11m  v1.26.0
worker-1  Ready     worker  11m  v1.26.0</pre>

							</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								It can take a few minutes after approval of the server CSRs for the machines to transition to the <code class="literal">Ready</code> status.
							</p></div></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Additional information</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							For more information on CSRs, see <a class="link" href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/">Certificate Signing Requests</a>.
						</li></ul></div></section></section></section><div><div xml:lang="en-US" class="legalnotice" id="idm140232224692304"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"><!--Empty--></span>© 2023 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div>


  <nav class="pvof-doc__book-nav">
  <ol class="book-nav__list">
              </ol>
</nav>


          </div>
              </div>
              <div id="comments-footer" class="book-comments">
          

  

        </div>
          </div>
  </article>
<meta itemscope="" itemref="md1">



    </div>
      <!-- CP_PRIMER_FOOTER -->            </div>
        </main>
    </div>
    <!--googleoff: all-->
    <div id="to-top"><a class="btn_slideto" href="#masthead" aria-label="Back to Top"><span class="web-icon-upload"></span></a></div>
    <footer class="footer-main">
        <div class="footer-top">
            <div class="container">

              <div class="brand">
                <a href="https://redhat.com">
                  <svg class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 613 145">
                    <defs>
                      <style>
                        .rh-logo-hat {
                          fill: #e00;
                        }
                        .rh-logo-type {
                          fill: #fff;
                        }
                      </style>
                    </defs>
                    <title>Red Hat</title>
                    <path
                      class="rh-logo-hat"
                      d="M127.47,83.49c12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42l-7.45-32.36c-1.72-7.12-3.23-10.35-15.73-16.6C124.89,8.69,103.76.5,97.51.5,91.69.5,90,8,83.06,8c-6.68,0-11.64-5.6-17.89-5.6-6,0-9.91,4.09-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33C22.27,52,.5,55,.5,74.22c0,31.48,74.59,70.28,133.65,70.28,45.28,0,56.7-20.48,56.7-36.65,0-12.72-11-27.16-30.83-35.78"/>
                      <path class="rh-logo-band"
                      d="M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45,12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/>
                      <path
                      class="rh-logo-type"
                      d="M579.74,92.8c0,11.89,7.15,17.67,20.19,17.67a52.11,52.11,0,0,0,11.89-1.68V95a24.84,24.84,0,0,1-7.68,1.16c-5.37,0-7.36-1.68-7.36-6.73V68.3h15.56V54.1H596.78v-18l-17,3.68V54.1H568.49V68.3h11.25Zm-53,.32c0-3.68,3.69-5.47,9.26-5.47a43.12,43.12,0,0,1,10.1,1.26v7.15a21.51,21.51,0,0,1-10.63,2.63c-5.46,0-8.73-2.1-8.73-5.57m5.2,17.56c6,0,10.84-1.26,15.36-4.31v3.37h16.82V74.08c0-13.56-9.14-21-24.39-21-8.52,0-16.94,2-26,6.1l6.1,12.52c6.52-2.74,12-4.42,16.83-4.42,7,0,10.62,2.73,10.62,8.31v2.73a49.53,49.53,0,0,0-12.62-1.58c-14.31,0-22.93,6-22.93,16.73,0,9.78,7.78,17.24,20.19,17.24m-92.44-.94h18.09V80.92h30.29v28.82H506V36.12H487.93V64.41H457.64V36.12H439.55ZM370.62,81.87c0-8,6.31-14.1,14.62-14.1A17.22,17.22,0,0,1,397,72.09V91.54A16.36,16.36,0,0,1,385.24,96c-8.2,0-14.62-6.1-14.62-14.09m26.61,27.87h16.83V32.44l-17,3.68V57.05a28.3,28.3,0,0,0-14.2-3.68c-16.19,0-28.92,12.51-28.92,28.5a28.25,28.25,0,0,0,28.4,28.6,25.12,25.12,0,0,0,14.93-4.83ZM320,67c5.36,0,9.88,3.47,11.67,8.83H308.47C310.15,70.3,314.36,67,320,67M291.33,82c0,16.2,13.25,28.82,30.28,28.82,9.36,0,16.2-2.53,23.25-8.42l-11.26-10c-2.63,2.74-6.52,4.21-11.14,4.21a14.39,14.39,0,0,1-13.68-8.83h39.65V83.55c0-17.67-11.88-30.39-28.08-30.39a28.57,28.57,0,0,0-29,28.81M262,51.58c6,0,9.36,3.78,9.36,8.31S268,68.2,262,68.2H244.11V51.58Zm-36,58.16h18.09V82.92h13.77l13.89,26.82H292l-16.2-29.45a22.27,22.27,0,0,0,13.88-20.72c0-13.25-10.41-23.45-26-23.45H226Z"/>
                  </svg>
                </a>
              </div>

              <div role="navigation" aria-label="quick">
                  <h3>Quick Links</h3>
                  <ul>
                      <li><a class="download-software" href="https://access.redhat.com/downloads/">Downloads</a></li>
                      <li><a class="manage-subscriptions" href="https://access.redhat.com/management">Subscriptions</a></li>
                      <li><a class="support-cases" href="https://access.redhat.com/support">Support Cases</a></li>
                      <li><a class="customer-service" href="https://access.redhat.com/support/customer-service">Customer Service</a></li>
                      <li><a class="quick-docs" href="https://access.redhat.com/documentation">Product Documentation</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="help">
                  <h3>Help</h3>
                  <ul>
                      <li><a class="contact-us" href="https://access.redhat.com/support/contact/">Contact Us</a></li>
                      <li><a class="cp-faqs" href="https://access.redhat.com/articles/33844">Customer Portal FAQ</a></li>
                      <li><a class="login-problems" href="https://access.redhat.com/help/login_assistance">Log-in Assistance</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="site">
                  <h3>Site Info</h3>
                  <ul>
                      <li><a class="trust-red-hat" href="https://www.redhat.com/en/trust">Trust Red Hat</a></li>
                      <li><a class="browser-support-policy" href="https://www.redhat.com/en/about/browser-support">Browser Support Policy</a></li>
                      <li><a class="accessibility" href="https://www.redhat.com/en/about/digital-accessibility">Accessibility</a></li>
                      <li><a class="recognition" href="https://access.redhat.com/recognition/">Awards and Recognition</a></li>
                      <li><a class="colophon" href="https://access.redhat.com/help/colophon/">Colophon</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="other">
                  <h3>Related Sites</h3>
                  <ul>
                      <li><a href="https://www.redhat.com/" class="red-hat-com">redhat.com</a></li>
                      <li><a href="http://developers.redhat.com/" class="red-hat-developers">developers.redhat.com</a></li>
                      <li><a href="https://connect.redhat.com/" class="partner-connect">connect.redhat.com</a></li>
                      <li><a href="https://cloud.redhat.com/" class="cloud-com">cloud.redhat.com</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="about">
                  <h3>About</h3>
                  <ul>
                      <li><a href="https://access.redhat.com/subscription-value" class="subscription-value">Red Hat Subscription Value</a></li>
                      <li><a href="https://www.redhat.com/about/" class="about-red-hat">About Red Hat</a></li>
                      <li><a href="http://jobs.redhat.com" class="about-jobs">Red Hat Jobs</a></li>
                  </ul>
              </div>

            </div>
        </div>

        <div class="anchor">
            <div class="container">
                <div class="status-legal">
                    <a hidden href="https://status.redhat.com" class="status-page-widget">
                        <span class="status-description"></span>
                        <span class="status-dot shape-circle"></span>
                    </a>
                    <div class="legal-copyright">
                        <div class="copyright">Copyright © 2023 Red Hat, Inc.</div>
                        <div role="navigation" aria-label="legal" class="legal">
                            <ul>
                                <li><a href="http://www.redhat.com/en/about/privacy-policy" class="privacy-policy">Privacy Statement</a></li>
                                <li><a href="https://www.redhat.com/en/about/terms-use" class="terms-of-use">Terms of Use</a></li>
                                <li><a href="http://www.redhat.com/en/about/all-policies-guidelines" class="all-policies">All Policies and Guidelines</a></li>
                                <li><a id="teconsent"></a></li>
                            </ul>
                            <div id="privacy_policy">We've updated our <a href='http://www.redhat.com/en/about/privacy-policy' class='privacy-policy'>Privacy Statement</a> effective September 15, 2023.
                            </div>
                          </div>
                        </div>
                </div>
                <div class="social">
                    <a href="http://www.redhat.com/summit/" class="summit">
                        <img src="https://access.redhat.com/chrome_themes/nimbus/img/rh-summit-red-a.svg" alt="Red Hat Summit" /> <span class="offscreen">Red Hat Summit</span>
                    </a>

                    <div class="social-media">
                        <a href="https://twitter.com/RedHat" class="sm-icon twitter"><span class="nicon-twitter"></span><span class="offscreen">Twitter</span></a>                        
                    </div>
                </div>
            </div>
        </div>
    </footer>
    <!-- TrustArc -->
    <div id="consent_blackbar"></div> 
    <!--googleon: all-->
</div>
<!-- /CP_PRIMER_FOOTER -->


  </div>

    
  </body>
</html>
