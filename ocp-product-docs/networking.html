<!DOCTYPE html>
<html lang="en" dir="ltr" prefix="og: https://ogp.me/ns#">
  <head>
    <meta charset="utf-8" />
<link rel="canonical" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/index" />
<link rel="shortlink" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/index" />
<meta property="og:site_name" content="Red Hat Customer Portal" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/index" />
<meta property="og:title" content="Networking OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<meta property="og:description" content="This document provides instructions for configuring and managing your OpenShift Container Platform cluster network, including DNS, ingress, and the Pod network." />
<meta property="og:image" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:description" content="This document provides instructions for configuring and managing your OpenShift Container Platform cluster network, including DNS, ingress, and the Pod network." />
<meta name="twitter:title" content="Networking OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<meta name="twitter:url" content="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/index" />
<meta name="twitter:image" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
<meta name="title" content="Networking OpenShift Container Platform 4.13 | Red Hat Customer Portal" />
<link rel="alternate" hreflang="en" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/index" />
<link rel="alternate" hreflang="ko" href="https://access.redhat.com/documentation/ko-kr/openshift_container_platform/4.13/html-single/networking/index" />
<link rel="alternate" hreflang="zh-hans" href="https://access.redhat.com/documentation/zh-cn/openshift_container_platform/4.13/html-single/networking/index" />
<link rel="alternate" hreflang="ja" href="https://access.redhat.com/documentation/ja-jp/openshift_container_platform/4.13/html-single/networking/index" />
<link rel="alternate" hreflang="es" href="https://access.redhat.com/documentation/es-es/openshift_container_platform/4.13/html-single/networking/index" />
<link rel="alternate" hreflang="ru" href="https://access.redhat.com/documentation/ru-ru/openshift_container_platform/4.13/html-single/networking/index" />
<link rel="alternate" hreflang="pt-br" href="https://access.redhat.com/documentation/pt-br/openshift_container_platform/4.13/html-single/networking/index" />
<link rel="alternate" hreflang="it" href="https://access.redhat.com/documentation/it-it/openshift_container_platform/4.13/html-single/networking/index" />
<link rel="alternate" hreflang="de" href="https://access.redhat.com/documentation/de-de/openshift_container_platform/4.13/html-single/networking/index" />
<link rel="alternate" hreflang="fr" href="https://access.redhat.com/documentation/fr-fr/openshift_container_platform/4.13/html-single/networking/index" />
<link rel="alternate" hreflang="zh-hant" href="https://access.redhat.com/documentation/zh-tw/openshift_container_platform/4.13/html-single/networking/index" />
<link rel="alternate" hreflang="id" href="https://access.redhat.com/documentation/id-id/openshift_container_platform/4.13/html-single/networking/index" />
<link rel="alternate" hreflang="th" href="https://access.redhat.com/documentation/th-th/openshift_container_platform/4.13/html-single/networking/index" />
<link rel="alternate" hreflang="vi" href="https://access.redhat.com/documentation/vi-vn/openshift_container_platform/4.13/html-single/networking/index" />
<meta name="Generator" content="Drupal 9 (https://www.drupal.org)" />
<meta name="MobileOptimized" content="width" />
<meta name="HandheldFriendly" content="true" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="revision" product="b0738f19-59ac-47eb-9512-8a439cd6dfb0" title="5b11808b-a659-4764-9b1d-6c9e7b49d658" page="870ca63e-3f06-4110-81fb-8694399b7555" revision="dccd48b1f252cc22dad43e9ae6252b9287d98ca1:en-us" body="b3bd800eead00ef5d846e9e2633da7a3.html" toc="36d1a4d713360519ead32069e964acf4.json" />

    <title>Networking OpenShift Container Platform 4.13 | Red Hat Customer Portal</title>
    <link rel="stylesheet" media="all" href="/sites/dxp-docs/files/css/css_87GMcmxT1ib8ziQiU2KUAnTDFtZQV6iP-KGslA9LigM.css" />
<link rel="stylesheet" media="all" href="/sites/dxp-docs/files/css/css__Xq4GfgPDJw9K_yYJFmlRZGJeCENu3R3r4s0K7Tr_9g.css" />

    
    <script type="application/json" data-drupal-selector="drupal-settings-json">{"path":{"baseUrl":"\/","scriptPath":null,"pathPrefix":"","currentPath":"documentation\/en-us\/openshift_container_platform\/4.13\/html-single\/networking\/index","currentPathIsAdmin":false,"isFront":false,"currentLanguage":"en"},"pluralDelimiter":"\u0003","suppressDeprecationErrors":true,"red_hat_jwt":{"client_id":"customer-portal","cookie_name":"rh_jwt","leeway":"0","realm":"redhat-external","sso_host":"https:\/\/sso.redhat.com\/","user_integration":1,"user_plugin":"drupal_user_auth","use_external_js":0,"use_internal_js":0,"use_in_admin":0},"user":{"uid":0,"permissionsHash":"d8ea0bce2d740dacbdfe0257cf55baa0e33f7fb8468a26d055ce75daaaa2d315"}}</script>
<script src="/sites/dxp-docs/files/js/js_EQWKo9EokWkWS99x_e1oM-NEM0zlKyTkp_83mGdm5Ks.js"></script>

    <!-- CP_PRIMER_HEAD -->  <!-- TrustArc & DTM -->
  <script src="//static.redhat.com/libs/redhat/marketing/latest/trustarc/trustarc.js"></script>
  <script src="//www.redhat.com/dtm.js"></script><meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<!--[if IEMobile]><meta http-equiv="cleartype" content="on"><![endif]-->

<!-- metaInclude -->
<meta name="avalon-host-info" content="dxp-kbase-prod-139-77b4fb8768-25dr9" />
<meta name="avalon-version" content="27861f77" />
<meta name="cp-chrome-build-date" content="2023-10-06T19:17:59.039Z" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<!-- Chrome, Firefox OS and Opera -->
<meta name="theme-color" content="#000000">
<!-- Windows Phone -->
<meta name="msapplication-navbutton-color" content="#000000">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-status-bar-style" content="#000000">
<link rel="manifest" href="https://access.redhat.com/webassets/avalon/j/manifest.json">
<!-- Open Search - Tap to search -->
<link rel="search" type="application/opensearchdescription+xml" title="Red Hat Customer Portal" href="https://access.redhat.com/webassets/avalon/j/opensearch.xml" />
<!-- title -->
<title>Red Hat Customer Portal - Access to 24x7 support and knowledge</title>
<!-- /title -->
<script type="text/javascript">
    window.portal = {
        analytics : {},
        host      : "https://access.redhat.com",
        idp_url   : "https://sso.redhat.com",
        lang      : "en", 
        version   : "27861f77",
        builddate : "2023-10-06T19:17:59.039Z",        fetchdate : "2023-10-10T17:45:08-0400",        nrid      : "NOLONGERSUPPORTED",
        nrlk      : "NOLONGERSUPPORTED"
    };
</script>
<script type="text/javascript">
    if (!/\/logout.*/.test(location.pathname) && portal.host === location.origin && document.cookie.indexOf('rh_sso_session') >= 0 && !(document.cookie.indexOf('rh_jwt') >= 0)) window.location = '/login?redirectTo=' + encodeURIComponent(window.location.href);
</script>
<!-- cssInclude -->

<link rel="shortcut icon" href="https://access.redhat.com/webassets/avalon/g/favicon.ico" /><link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/bootstrap.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/bootstrap-grid.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/main.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/components.css?v=27861f77" />
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/pages.css?v=27861f77" />

<link href="https://access.redhat.com/webassets/avalon/s/chosen.css?v=27861f77" rel="stylesheet" type="text/css" />

<!--[if lte IE 9]>
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/ie.css" />
<![endif]--><noscript>
    <style type="text/css" media="screen"> .primary-nav { display: block; } </style>
</noscript>
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/webassets/avalon/j/public_modules/node_modules/@cpelements/pfe-navigation/dist/pfe-navigation--lightdom.min.css" />
<!-- /cssInclude -->
<script src="https://access.redhat.com/webassets/avalon/j/public_modules/node_modules/@cpelements/pfe-navigation/dist/ie-polyfills.js?v=27861f77"></script>

<script async>
  if (!HTMLScriptElement.supports || !HTMLScriptElement.supports('importmap')) {
    import("https://www.redhatstatic.com/dx/v1-alpha/es-module-shims@1.7.3.js");
  }
</script>
<script type="importmap">
{
  "imports": {
    "@patternfly/elements/" : "https://www.redhatstatic.com/dx/v1-alpha/@patternfly/elements@2.2.2/",
    "@rhds/elements/":"https://www.redhatstatic.com/dx/v1-alpha/@rhds/elements@1.1.0/elements/",
    "@rhds/elements/lib/":"https://www.redhatstatic.com/dx/v1-alpha/@rhds/elements@1.1.0/lib/",
    "@cpelements/elements/":"https://www.redhatstatic.com/dx/v1-alpha/@cpelements/elements@2.0.0-alpha.7/elements/"
  }
}
</script><script type="text/javascript" src="https://access.redhat.com/webassets/avalon/j/lib/require.js?v=27861f77" data-main="/webassets/avalon/j/"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<script src="https://access.redhat.com/chrome_themes/nimbus/js/ie8.js"></script>
<![endif]-->
<script type="text/javascript" src="https://access.redhat.com/chrome_themes/nimbus/js/new-nav.js?v=27861f77" ></script>
<!-- /CP_PRIMER_HEAD -->

  </head>
  <body>
    
      <div class="dialog-off-canvas-main-canvas" data-off-canvas-main-canvas>
      <!-- CP_PRIMER_HEADER -->
<div id="page-wrap" class="page-wrap">
    <div id="pers-top-page-wrap" class="top-page-wrap pers-loader-bg">

      <div id="hero-bg-top-left" class="summit-bg-shapes"></div>
      <div id="hero-bg-top-right" class="summit-bg-shapes"></div>

        <!--googleoff: all-->
        <header class="masthead" id="masthead">

            <a href="#pfe-navigation" id="global-skip-to-nav" class="skip-link visually-hidden">Skip to navigation</a>
            <a href="#cp-main" class="skip-link visually-hidden">Skip to main content</a>            <nav id="portal-utility-nav" class="utility-navigation utility-navigation--bar hidden-at-mobile" data-analytics-region="utility" aria-labelledby="nav__utility-nav--desktop">
                <h3 id="nav__utility-nav--desktop" class="element-invisible">Utilities
                </h3>
                <ul aria-labelledby="nav__utility-nav--desktop">
                    <li id="nav-subscription" data-portal-tour-1="1">
                        <a class="top-nav-subscriptions" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Subscription" href="https://access.redhat.com/management/" >Subscriptions
                        </a>
                    </li>
                    <li id="nav-downloads" data-portal-tour-1="2">
                        <a class="top-nav-downloads" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Downloads" href="https://access.redhat.com/downloads/" >Downloads
                        </a>
                    </li>
                    <li id="nav-containers">
                        <a class="top-nav-containers" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Containers" href="https://catalog.redhat.com/software/containers/explore/" >Containers
                        </a>
                    </li>
                    <li id="nav-support" data-portal-tour-1="3">
                        <a class="top-nav-support-cases" data-analytics-level="2" data-analytics-category="Utilities" data-analytics-text="Support Cases" href="https://access.redhat.com/support/cases/" >Support Cases
                        </a>
                    </li>
                </ul>
            </nav>

            <pfe-navigation id="pfe-navigation" data-analytics-region="mega menu">
                <div class="pfe-navigation__logo-wrapper" id="pfe-navigation__logo-wrapper">
                    <a href="https://access.redhat.com/" class="pfe-navigation__logo-link" data-analytics-text="logo" data-analytics-category="MM|logo">
                        <img class="pfe-navigation__logo-image" alt="Red Hat Customer Portal" src="https://access.redhat.com/chrome_themes/nimbus/img/red-hat-customer-portal.svg" />
                    </a>
                </div>

                <nav class="pfe-navigation" aria-label="Main Navigation" data-analytics-region="main nav">
                    <ul class="pfe-navigation__menu" id="pfe-navigation__menu">                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-subscription--mobile" data-portal-tour-1="1">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Subscription" href="https://access.redhat.com/management/" >Subscriptions
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-downloads--mobile" data-portal-tour-1="2">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Downloads" href="https://access.redhat.com/downloads/" >Downloads
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-containers--mobile">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Containers" href="https://catalog.redhat.com/software/containers/explore/" >Containers
                            </a>
                        </li>
                        <li class="pfe-navigation__menu-item hidden-at-tablet hidden-at-desktop" id="nav-support--mobile" data-portal-tour-1="3">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Support Cases" href="https://access.redhat.com/support/cases/" >Support Cases
                            </a>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a href="https://access.redhat.com/products/" class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Products and Services">Products &amp; Services
                            </a>
                            <div class="pfe-navigation__dropdown has-primary-detail">                                <div class="desktop-col-span-2 tablet-col-span-all">
                                    <h3>
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Products" href="https://access.redhat.com/products/">Products
                                        </a>
                                    </h3>
                                    <slot name="main-menu__dropdown--product__product-listing"></slot>
                                </div>                                <div>
                                    <h3 id="nav__products__support">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Support" href="https://access.redhat.com/support">Support
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__support">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Production Support" href="https://access.redhat.com/support/offerings/production/">Production Support
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Development Support" href="https://access.redhat.com/support/offerings/developer/">Development Support
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Support" data-analytics-text="Product Life Cycles" href="https://access.redhat.com/product-life-cycles/">Product Life Cycles
                                                    </a></li>
                                    </ul>

                                    <h3 id="nav__products__services">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Services" href="https://www.redhat.com/en/services">Services
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__services">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Consulting" href="https://www.redhat.com/en/services/consulting">Consulting
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Technical Account Management" href="https://access.redhat.com/support/offerings/tam/">Technical Account Management
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Services" data-analytics-text="Training and Certifications" href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications
                                                    </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__products__documentation">
                                        <a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Documentation" href="https://access.redhat.com/documentation">Documentation
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__products__documentation">
                                        <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat Enterprise Linux" href="https://access.redhat.com/documentation/en/red_hat_enterprise_linux">Red Hat Enterprise Linux
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat JBoss Enterprise Application Platform" href="https://access.redhat.com/documentation/en/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat OpenStack Platform" href="https://access.redhat.com/documentation/en/red_hat_openstack_platform">Red Hat OpenStack Platform
                                                    </a></li>
                                                    <li><a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="Red Hat OpenShift Container Platform" href="https://access.redhat.com/documentation/en/openshift_container_platform">Red Hat OpenShift Container Platform
                                                        </a></li>
                                    </ul>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Products and Services|Documentation" data-analytics-text="All Documentation" data-analytics-linkType="cta" href="https://access.redhat.com/documentation">All Documentation
                                        </a>
                                    </pfe-cta>

                                    <h3 id="nav__products__catalog"><a data-analytics-level="2" data-analytics-category="Products and Services" data-analytics-text="Ecosystem Catalog" href="https://catalog.redhat.com/">Ecosystem Catalog
                                        </a></h3>
                                        <ul aria-labelledby="nav__products__catalog">
                                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Ecosystem Catalog" data-analytics-text="Red Hat Partner Ecosystem" href="https://access.redhat.com/ecosystem/">Red Hat Partner Ecosystem
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Products and Services|Ecosystem Catalog" data-analytics-text="Partner Resources" href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources
                                                    </a></li>
                                        </ul>
                                </div>
                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Tools" href="https://access.redhat.com/labs/">Tools
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="nav__tools__tools" data-analytics-level="2" data-analytics-text="Tools" data-analytics-category="Tools">Tools
                                    </h3>
                                    <ul aria-labelledby="nav__tools__tools">
                                        <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Solution Engine" href="https://access.redhat.com/support/cases/#/troubleshoot">Troubleshoot a product issue
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Packages" href="https://access.redhat.com/downloads/content/package-browser">Packages
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Tools|Tools" data-analytics-text="Errata" href="https://access.redhat.com/errata/">Errata
                                                    </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__tools__labs">
                                        <a data-analytics-level="2" data-analytics-category="Tools" data-analytics-text="Customer Portal Labs" href="https://access.redhat.com/labs/">Customer Portal Labs
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__tools__labs">
                                        <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Configuration" href="https://access.redhat.com/labs/#!?type=config">Configuration
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Deployment" href="https://access.redhat.com/labs/#!?type=deploy">Deployment
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Security" href="https://access.redhat.com/labs/#!?type=security">Security
                                                    </a></li>                                                    <li><a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="Troubleshooting" href="https://access.redhat.com/labs/#!?type=troubleshoot">Troubleshoot
                                                        </a></li>
                                    </ul>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Tools|Customer Portal Labs" data-analytics-text="All Labs" data-analytics-linkType="cta" href="https://access.redhat.com/labs/">All labs
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h4 id="nav__tools__red-hat-insights">
                                        <a data-analytics-level="2" data-analytics-category="Tools" data-analytics-text="Red Hat Insights" href="//www.redhat.com/en/technologies/management/insights">Red Hat Insights
                                        </a>
                                    </h4>
                                    <p>Increase visibility into IT operations to detect and resolve technical issues before they impact your business.</p>
                                    <a data-analytics-level="3" data-analytics-category="Tools|Red Hat Insights" data-analytics-text="Learn more" href="https://www.redhat.com/en/technologies/management/insights">Learn More
                                    </a>
                                    <br>
                                    <a data-analytics-level="3" data-analytics-category="Tools|Red Hat Insights" data-analytics-text="Go to Insights" href="https://cloud.redhat.com/insights">Go to Insights
                                    </a>
                                </div>
                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Security" href="https://access.redhat.com/security/">Security
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="security__security-center">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Red Hat Product Security Center" href="https://access.redhat.com/security">Red Hat Product Security Center
                                        </a>
                                    </h3>
                                    <p>Engage with our Red Hat Product Security team, access security updates, and ensure your environments are not exposed to any known security vulnerabilities.
                                    </p>
                                    <pfe-cta pfe-priority="primary">
                                        <a data-analytics-level="3" data-analytics-category="Security|Red Hat Product Security Center" data-analytics-text="Product Security Center" data-analytics-linkType="cta" href="https://access.redhat.com/security/">Product Security Center
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__security__updates" data-analytics-level="2" data-analytics-text="Security Updates" data-analytics-category="Security">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Security Updates" href="/security">Security Updates
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__security__updates">
                                        <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Security Advisories" href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Red Hat CVE Database" href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="Security Labs" href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs
                                                    </a></li>
                                    </ul>
                                    <p class="margin-top-xl">Keep your systems secure with Red Hat&#039;s specialized responses to security vulnerabilities.
                                    </p>
                                    <pfe-cta>
                                        <a data-analytics-level="3" data-analytics-category="Security|Security Updates" data-analytics-text="View Responses" data-analytics-linkType="cta" href="https://access.redhat.com/security/vulnerability">View Responses
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__security__resources">
                                        <a data-analytics-level="2" data-analytics-category="Security" data-analytics-text="Resources" href="https://access.redhat.com/security/overview">Resources
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__security__resources">                                            <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Security Blog" href="//redhat.com/en/blog/channel/security">Security Blog
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Security Measurement" href="https://www.redhat.com/security/data/metrics/">Security Measurement
                                                    </a></li>
                                                    <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Severity Ratings" href="https://access.redhat.com/security/updates/classification/">Severity Ratings
                                                        </a></li>
                                                        <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Backporting Policies" href="https://access.redhat.com/security/updates/backporting/">Backporting Policies
                                                            </a></li>
                                                            <li><a data-analytics-level="3" data-analytics-category="Security|Resources" data-analytics-text="Product Signing (GPG) Keys" href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys
                                                                </a></li>
                                    </ul>
                                </div>

                            </div>
                        </li>                        <li class="pfe-navigation__menu-item">
                            <a href="https://access.redhat.com/community/" class="pfe-navigation__menu-link" data-analytics-level="1" data-analytics-text="Community">Community
                            </a>
                            <div class="pfe-navigation__dropdown pfe-navigation__dropdown--3-column">                                <div>
                                    <h3 id="nav__community__cp-community">
                                        <a href="https://access.redhat.com/community" data-analytics-level="2" data-analytics-text="Customer Portal Community" data-analytics-text="Customer Portal Community" data-analytics-category="Community">Customer Portal Community
                                        </a>
                                    </h3>
                                    <ul aria-labelledby="nav__community__cp-community">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Discussions" href="https://access.redhat.com/discussions">Discussions
                                            </a></li>                                                <li><a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Private Groups" href="https://access.redhat.com/groups/">Private Groups
                                                    </a></li>
                                    </ul>

                                    <pfe-cta pfe-priority="primary">
                                        <a data-analytics-level="3" data-analytics-category="Community|Customer Portal Community" data-analytics-text="Community Activity" data-analytics-linkType="cta" href="https://access.redhat.com/community/">Community Activity
                                        </a>
                                    </pfe-cta>
                                </div>                                <div>
                                    <h3 id="nav__community__events" data-analytics-level="2" data-analytics-text="Customer Events" data-analytics-category="Community">Customer Events
                                    </h3>
                                    <ul aria-labelledby="nav__community__events">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Customer Events" data-analytics-text="Red Hat Convergence" href="https://access.redhat.com/convergence/">Red Hat Convergence
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-category="Community|Customer Events" data-analytics-text="Red Hat Summit" href="http://www.redhat.com/summit/">Red Hat Summit
                                                </a></li>
                                    </ul>
                                </div>                                <div>
                                    <h3 id="nav__community__stories" data-analytics-level="2" data-analytics-text="Stories" data-analytics-category="Community">Stories
                                    </h3>
                                    <ul aria-labelledby="nav__community__stories">
                                        <li><a data-analytics-level="3" data-analytics-category="Community|Stories" data-analytics-text="Red Hat Subscription Value" href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value
                                            </a></li>
                                            <li><a data-analytics-level="3" data-analytics-text="You Asked. We Acted." data-analytics-category="Community|Stories" href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.
                                                </a></li>
                                                <li><a data-analytics-level="3" data-analytics-category="Community|Stories" data-analytics-text="Open Source Communities" href="http://www.redhat.com/en/open-source">Open Source Communities
                                                    </a></li>
                                    </ul>
                                </div>
                            </div>
                        </li>
                    </ul>                </nav>                <div id="site-search" slot="search" class="utility-link site-search">
                    <div class="content">
                        <form class="ng-pristine ng-valid topSearchForm" id="topSearchForm" name="topSearchForm" action="/search/browse/search/" method="get" enctype="application/x-www-form-urlencoded">
                            <cp-search-autocomplete class="push-bottom" path="/webassets/avalon/j/data.json"></cp-search-autocomplete>                            <div>Or <a href="/support/cases/#/troubleshoot">troubleshoot an issue</a>.
                            </div>
                        </form>
                    </div>
                </div>


                <div slot="secondary-links" id="localesMenu">
                    <button class="pfe-navigation__secondary-link">
                        <pfe-icon icon="web-icon-globe" size="sm" aria-hidden="true"></pfe-icon>English
                    </button>

                    <pfe-navigation-dropdown dropdown-width="single">
                        <h2 class="utility-header">Select Your Language
                        </h2>
                        <ul class="reset">
                            <li><a href="https://access.redhat.com/changeLanguage?language=en" data-lang="en" id="en" data-analytics-text="English">English</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=ko" data-lang="ko" id="ko" data-analytics-text="Korean">한국어</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=ja"    data-lang="ja"    id="ja" data-analytics-text="Japanese">日本語</a></li>
                            <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" data-lang="zh_CN" id="zh_CN" data-analytics-text="Chinese">中文 (中国)</a></li>
                        </ul>

                    </pfe-navigation-dropdown>
                </div>                <rh-account-dropdown slot="account"></rh-account-dropdown>                <pfe-primary-detail breakpoint-width="600" class="main-menu__dropdown--product__product-listing" slot="main-menu__dropdown--product__product-listing" consistent-height>
                    <h3 slot="details-nav">Infrastructure and Management                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Enterprise Linux" href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Satellite" href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Subscription Management" href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Insights" href="https://access.redhat.com/products/red-hat-insights/">Red Hat Insights
                                </a>
                            </li>
                            <li><a data-analytics-level="3" data-analytics-category="Products and Services|Products:Infrastructure and Management" data-analytics-text="Red Hat Ansible Automation Platform" href="https://access.redhat.com/products/red-hat-ansible-automation-platform/">Red Hat Ansible Automation Platform
                                </a></li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Cloud Computing                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift" href="https://access.redhat.com/products/openshift">Red Hat OpenShift
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenStack Platform" href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat OpenShift Container Platform" href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat OpenShift Data Science" href="https://access.redhat.com/products/red-hat-openshift-data-science/">Red Hat OpenShift Data Science
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift Dedicated" href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing Platform" data-analytics-text="Red Hat Advanced Cluster Security for Kubernetes" href="https://access.redhat.com/products/red-hat-advanced-cluster-security-for-kubernetes/">Red Hat Advanced Cluster Security for Kubernetes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat Advanced Cluster Management for Kubernetes" href="https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes/">Red Hat Advanced Cluster Management for Kubernetes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat Quay" href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat CodeReady Workspaces" href="https://access.redhat.com/products/red-hat-codeready-workspaces/">OpenShift Dev Spaces
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Cloud Computing" data-analytics-text="Red Hat OpenShift Service on AWS" href="https://access.redhat.com/products/red-hat-openshift-service-aws">Red Hat OpenShift Service on AWS
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Storage                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Gluster Storage" href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Hyperconverged Infrastructure" href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Ceph Storage" href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Storage" data-analytics-text="Red Hat Openshift Container Storage" href="https://access.redhat.com/products/red-hat-openshift-data-foundation">Red Hat OpenShift Data Foundation
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Runtimes                    </h3>
                    <div slot="details">
                        <ul>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Runtimes" href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat JBoss Enterprise Application Platform" href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Data Grid" href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat JBoss Web Server" href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat Single Sign On" href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat support for Spring Boot" href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat build of Node.js" href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js
                                </a>
                            </li>                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Runtimes" data-analytics-text="Red Hat build of Quarkus" href="https://access.redhat.com/products/quarkus/">Red Hat build of Quarkus
                                </a>
                            </li>
                        </ul>
                    </div>

                    <h3 slot="details-nav">Integration and Automation                    </h3>
                    <div slot="details">
                        <ul class="border-bottom" id="portal-menu-border-bottom">
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat Application Foundations" href="https://access.redhat.com/products/red-hat-application-foundations/">Red Hat Application Foundations
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat Fuse" href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat AMQ" href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ
                                </a>
                            </li>
                            <li>
                                <a data-analytics-level="3" data-analytics-category="Products and Services|Products:Integration and Automation" data-analytics-text="Red Hat 3scale API Management" href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management
                                </a>
                            </li>
                        </ul>
                    </div>
                    <div slot="details-nav--footer">
                        <pfe-cta pfe-priority="primary">
                            <a href="https://access.redhat.com/products/" class="pfe-navigation__menu-link" data-analytics-level="2" data-analytics-text="All Products" data-analytics-category="Products and Services|Products:" data-analytics-linkType="cta">All Products
                            </a>
                        </pfe-cta>
                    </div>
                </pfe-primary-detail>

            </pfe-navigation>

            <div id="scroll-anchor"></div>

            <!--[if IE 8]>
                <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                <span class="icon-warning alert-icon" aria-hidden="true"></span>
                You are using an unsupported web browser. Update to a supported browser for the best experience. <a href="/announcements/2120951">Read the announcement</a>.
                </div>
                </div>
            <![endif]-->
            <!--[if IE 9]>
                <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                <span class="icon-warning alert-icon" aria-hidden="true"></span>As of March 1, 2016, the Red Hat Customer Portal will no longer support Internet Explorer 9. See our new <a href="/help/browsers">browser support policy</a> for more information.
                </div>
                </div>
            <![endif]-->
            <div id="site-section"></div>
        </header>
        <!--googleon: all-->

        <main id="cp-main" class="portal-content-area">
            <div id="cp-content" class="main-content">                            <!-- /CP_PRIMER_HEADER -->

      <div class="container">
        

                                                                                                        <script>breadcrumbs = [["Products & Services","\/products\/"],["Product Documentation","\/documentation"],["OpenShift Container Platform","\/documentation\/en-us\/openshift_container_platform"],["4.13","\/documentation\/en-us\/openshift_container_platform\/4.13"],["Networking","\/documentation\/en-us\/openshift_container_platform\/4.13\/html\/networking"],["Networking","\/documentation\/en-us\/openshift_container_platform\/4.13\/html\/networking\/--single-page-document--"]]</script>

<div data-drupal-messages-fallback class="hidden"></div>


    </div>
        <div class="container">
        

  

  


  <article class="pvof-doc__content-wrapper__outer pvof-doc__content-wrapper__outer--css-not-removed">
    <script>
      'use strict';

            var $outerWrapper = document.querySelector('.pvof-doc__content-wrapper__outer');
      if ($outerWrapper && $outerWrapper.closest) {
        var $containerWrapper = $outerWrapper.closest('.container');
        if ($containerWrapper) {
          $containerWrapper.classList.remove('container');
          $containerWrapper.classList.add('j-chrome-content-container');
        }
      }

            var cssRemoved = false;
      try {
        var $crapCss = document.querySelectorAll(
          'link[href*="/chrome_themes/nimbus/css/pages.css"], link[href*="/chrome_themes/nimbus/css/components.css"]'
        );
        if ($crapCss.length) {
          for (let index = 0; index < $crapCss.length; index++) {
            const $stylesheet = $crapCss[index];
            $stylesheet.remove();
          }
        }
        cssRemoved = true;
      }
      catch (error) {
        console.error('Ran into an issue while trying to retheme page', error);
        cssRemoved = false;
      }

            if (cssRemoved) {
        var $pvofOuterWrapper = document.querySelector('.pvof-doc__content-wrapper__outer--css-not-removed');
        if ($pvofOuterWrapper) {
          $pvofOuterWrapper.classList.remove('pvof-doc__content-wrapper__outer--css-not-removed');
        }
      }
    </script>
    <div itemscope="" itemtype="https://schema.org/TechArticle" itemref="techArticle-md1 techArticle-md2 techArticle-md3"></div>
    <div itemscope="" itemtype="https://schema.org/SoftwareApplication" itemref="softwareApplication-md1 softwareApplication-md2 softwareApplication-md3 softwareApplication-md4"></div>
    <div class="pvof-doc__content-wrapper pvof-doc__content-wrapper--has-sidebar">
                                <div class="pvof-doc__content-wrapper__inner j-superdoc j-superdoc--has-nav">
                            <div class="pvof-sidebar__wrapper j-doc-nav j-superdoc__nav">
            <div class="j-sidebar__menu-container">
              <button class="j-sidebar__menu-trigger content-expander__trigger">
                <span class="j-sidebar__menu-trigger__open-text">Jump To</span>
                <span class="j-sidebar__menu-trigger__close-text">Close</span>
              </button>

              <div class="pvof-sidebar__inner-wrapper j-doc-nav__wrapper content-expander">
                <div class="j-sidebar__menu-details-container">
                  <button class="j-sidebar__menu-details-button j-sidebar__menu-details-button--expand">
                    Expand all
                  </button>
                  <button class="j-sidebar__menu-details-button j-sidebar__menu-details-button--collapse">
                    Collapse all
                  </button>
                </div>
                

  <nav id="pvof-doc__toc" class="pvof-doc__toc">
  <h2 class="j-doc-nav__title" id="j-doc-nav__title">
    Table of contents
  </h2>
  <div class="pvof-doc__toc-inner">
              <ol class="j-doc-nav__list" aria-labelledby="j-doc-nav__title">
                  <li class="j-doc-nav__list-item">
                                    

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking" class="j-doc-nav__link ">
    Networking
  </a>
  
          </li>
                  <li class="j-doc-nav__list-item">
                                    

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#about-networking" class="j-doc-nav__link ">
    1. About networking
  </a>
  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#understanding-networking" class="j-doc-nav__link j-doc-nav__link--has-children">
    2. Understanding networking
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2. Understanding networking"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2. Understanding networking"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ne-openshift-dns_understanding-networking" class="j-doc-nav__link ">
    2.1. OpenShift Container Platform DNS
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ne-openshift-ingress_understanding-networking" class="j-doc-nav__link j-doc-nav__link--has-children">
    2.2. OpenShift Container Platform Ingress Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "2.2. OpenShift Container Platform Ingress Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "2.2. OpenShift Container Platform Ingress Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ne-comparing-ingress-route_understanding-networking" class="j-doc-nav__link ">
    2.2.1. Comparing routes and Ingress
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networking-glossary-terms_understanding-networking" class="j-doc-nav__link ">
    2.3. Glossary of common terms for OpenShift Container Platform networking
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#accessing-hosts" class="j-doc-nav__link j-doc-nav__link--has-children">
    3. Accessing hosts
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "3. Accessing hosts"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "3. Accessing hosts"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#accessing-hosts-on-aws_accessing-hosts" class="j-doc-nav__link ">
    3.1. Accessing hosts on Amazon Web Services in an installer-provisioned infrastructure cluster
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#networking-operators-overview" class="j-doc-nav__link j-doc-nav__link--has-children">
    4. Networking Operators overview
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "4. Networking Operators overview"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "4. Networking Operators overview"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#networking-operators-overview-cluster-network-operator" class="j-doc-nav__link ">
    4.1. Cluster Network Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#networking-operators-overview-dns-operator" class="j-doc-nav__link ">
    4.2. DNS Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#networking-operators-overview-ingress-operator" class="j-doc-nav__link ">
    4.3. Ingress Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#networking-operators-overview-external-dns-operator" class="j-doc-nav__link ">
    4.4. External DNS Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ingress-node-firewall-operator-1" class="j-doc-nav__link ">
    4.5. Ingress Node Firewall Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-overview-operator" class="j-doc-nav__link ">
    4.6. Network Observability Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cluster-network-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    5. Cluster Network Operator in OpenShift Container Platform
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "5. Cluster Network Operator in OpenShift Container Platform"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "5. Cluster Network Operator in OpenShift Container Platform"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-cluster-network-operator_cluster-network-operator" class="j-doc-nav__link ">
    5.1. Cluster Network Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-cno-view_cluster-network-operator" class="j-doc-nav__link ">
    5.2. Viewing the cluster network configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-cno-status_cluster-network-operator" class="j-doc-nav__link ">
    5.3. Viewing Cluster Network Operator status
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-cno-logs_cluster-network-operator" class="j-doc-nav__link ">
    5.4. Viewing Cluster Network Operator logs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-operator-cr_cluster-network-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    5.5. Cluster Network Operator configuration
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "5.5. Cluster Network Operator configuration"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "5.5. Cluster Network Operator configuration"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-operator-cr-cno-object_cluster-network-operator" class="j-doc-nav__link ">
    5.5.1. Cluster Network Operator configuration object
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-operator-example-cr_cluster-network-operator" class="j-doc-nav__link ">
    5.5.2. Cluster Network Operator example configuration
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cluster-network-operator-additional-resources" class="j-doc-nav__link ">
    5.6. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#dns-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    6. DNS Operator in OpenShift Container Platform
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "6. DNS Operator in OpenShift Container Platform"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "6. DNS Operator in OpenShift Container Platform"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-dns-operator_dns-operator" class="j-doc-nav__link ">
    6.1. DNS Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-dns-operator-managementState_dns-operator" class="j-doc-nav__link ">
    6.2. Changing the DNS Operator managementState
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-controlling-dns-pod-placement_dns-operator" class="j-doc-nav__link ">
    6.3. Controlling DNS pod placement
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-dns-view_dns-operator" class="j-doc-nav__link ">
    6.4. View the default DNS
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-dns-forward_dns-operator" class="j-doc-nav__link ">
    6.5. Using DNS forwarding
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-dns-operator-status_dns-operator" class="j-doc-nav__link ">
    6.6. DNS Operator status
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-dns-operator-logs_dns-operator" class="j-doc-nav__link ">
    6.7. DNS Operator logs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-dns-loglevel_dns-operator" class="j-doc-nav__link ">
    6.8. Setting the CoreDNS log level
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-dns-operatorloglevel_dns-operator" class="j-doc-nav__link ">
    6.9. Setting the CoreDNS Operator log level
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-dns-cache-tuning_dns-operator" class="j-doc-nav__link ">
    6.10. Tuning the CoreDNS cache
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ingress" class="j-doc-nav__link j-doc-nav__link--has-children">
    7. Ingress Operator in OpenShift Container Platform
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7. Ingress Operator in OpenShift Container Platform"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7. Ingress Operator in OpenShift Container Platform"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ne-openshift-ingress_configuring-ingress" class="j-doc-nav__link ">
    7.1. OpenShift Container Platform Ingress Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-installation-ingress-config-asset_configuring-ingress" class="j-doc-nav__link ">
    7.2. The Ingress configuration asset
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-controller-configuration-parameters_configuring-ingress" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.3. Ingress Controller configuration parameters
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.3. Ingress Controller configuration parameters"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.3. Ingress Controller configuration parameters"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ingress-controller-tls" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.3.1. Ingress Controller TLS security profiles
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.3.1. Ingress Controller TLS security profiles"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.3.1. Ingress Controller TLS security profiles"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#tls-profiles-understanding_configuring-ingress" class="j-doc-nav__link ">
    7.3.1.1. Understanding TLS security profiles
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#tls-profiles-ingress-configuring_configuring-ingress" class="j-doc-nav__link ">
    7.3.1.2. Configuring the TLS security profile for the Ingress Controller
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-mutual-tls-auth_configuring-ingress" class="j-doc-nav__link ">
    7.3.1.3. Configuring mutual TLS authentication
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-view_configuring-ingress" class="j-doc-nav__link ">
    7.4. View the default Ingress Controller
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-operator-status_configuring-ingress" class="j-doc-nav__link ">
    7.5. View Ingress Operator status
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-operator-logs_configuring-ingress" class="j-doc-nav__link ">
    7.6. View Ingress Controller logs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-controller-status_configuring-ingress" class="j-doc-nav__link ">
    7.7. View Ingress Controller status
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ingress-controller" class="j-doc-nav__link j-doc-nav__link--has-children">
    7.8. Configuring the Ingress Controller
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "7.8. Configuring the Ingress Controller"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "7.8. Configuring the Ingress Controller"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-setting-a-custom-default-certificate_configuring-ingress" class="j-doc-nav__link ">
    7.8.1. Setting a custom default certificate
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-custom-default-certificate-remove_configuring-ingress" class="j-doc-nav__link ">
    7.8.2. Removing a custom default certificate
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-autoscaling-ingress-controller_configuring-ingress" class="j-doc-nav__link ">
    7.8.3. Autoscaling an Ingress Controller
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-controller-configuration_configuring-ingress" class="j-doc-nav__link ">
    7.8.4. Scaling an Ingress Controller
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-configure-ingress-access-logging_configuring-ingress" class="j-doc-nav__link ">
    7.8.5. Configuring Ingress access logging
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-setting-thread-count_configuring-ingress" class="j-doc-nav__link ">
    7.8.6. Setting Ingress Controller thread count
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-setting-internal-lb_configuring-ingress" class="j-doc-nav__link ">
    7.8.7. Configuring an Ingress Controller to use an internal load balancer
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-controller-configuration-gcp-global-access_configuring-ingress" class="j-doc-nav__link ">
    7.8.8. Configuring global access for an Ingress Controller on GCP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-controller-config-tuningoptions-healthcheckinterval_configuring-ingress" class="j-doc-nav__link ">
    7.8.9. Setting the Ingress Controller health check interval
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-default-internal_configuring-ingress" class="j-doc-nav__link ">
    7.8.10. Configuring the default Ingress Controller for your cluster to be internal
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-route-admission-policy_configuring-ingress" class="j-doc-nav__link ">
    7.8.11. Configuring the route admission policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#using-wildcard-routes_configuring-ingress" class="j-doc-nav__link ">
    7.8.12. Using wildcard routes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-using-ingress-forwarded_configuring-ingress" class="j-doc-nav__link ">
    7.8.13. Using X-Forwarded headers
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-http2-haproxy_configuring-ingress" class="j-doc-nav__link ">
    7.8.14. Enabling HTTP/2 Ingress connectivity
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-controller-configuration-proxy-protocol_configuring-ingress" class="j-doc-nav__link ">
    7.8.15. Configuring the PROXY protocol for an Ingress Controller
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-configuring-application-domain_configuring-ingress" class="j-doc-nav__link ">
    7.8.16. Specifying an alternative cluster domain using the appsDomain option
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-converting-http-header-case_configuring-ingress" class="j-doc-nav__link ">
    7.8.17. Converting HTTP header case
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-configuring-router-compression_configuring-ingress" class="j-doc-nav__link ">
    7.8.18. Using router compression
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-exposing-router-metrics_configuring-ingress" class="j-doc-nav__link ">
    7.8.19. Exposing router metrics
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-customize-ingress-error-pages_configuring-ingress" class="j-doc-nav__link ">
    7.8.20. Customizing HAProxy error code response pages
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-setting-max-connections_configuring-ingress" class="j-doc-nav__link ">
    7.8.21. Setting the Ingress Controller maximum connections
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources-1" class="j-doc-nav__link ">
    7.9. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ingress-sharding" class="j-doc-nav__link j-doc-nav__link--has-children">
    8. Ingress sharding in OpenShift Container Platform
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8. Ingress sharding in OpenShift Container Platform"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8. Ingress sharding in OpenShift Container Platform"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-sharding_ingress-sharding" class="j-doc-nav__link j-doc-nav__link--has-children">
    8.1. Ingress Controller sharding
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "8.1. Ingress Controller sharding"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "8.1. Ingress Controller sharding"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#traditional-sharding-example" class="j-doc-nav__link ">
    8.1.1. Traditional sharding example
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#overlapped-sharding-example" class="j-doc-nav__link ">
    8.1.2. Overlapped sharding example
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-sharding-default_ingress-sharding" class="j-doc-nav__link ">
    8.1.3. Sharding the default Ingress Controller
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-sharding-dns_ingress-sharding" class="j-doc-nav__link ">
    8.1.4. Ingress sharding and DNS
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-sharding-route-labels_ingress-sharding" class="j-doc-nav__link ">
    8.1.5. Configuring Ingress Controller sharding by using route labels
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-sharding-namespace-labels_ingress-sharding" class="j-doc-nav__link ">
    8.1.6. Configuring Ingress Controller sharding by using namespace labels
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-sharding-route-configuration_ingress-sharding" class="j-doc-nav__link ">
    8.2. Creating a route for Ingress Controller sharding
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ingress-node-firewall-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    9. Ingress Node Firewall Operator in OpenShift Container Platform
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9. Ingress Node Firewall Operator in OpenShift Container Platform"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9. Ingress Node Firewall Operator in OpenShift Container Platform"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#installing-infw-operator_ingress-node-firewall-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    9.1. Installing the Ingress Node Firewall Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9.1. Installing the Ingress Node Firewall Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9.1. Installing the Ingress Node Firewall Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#install-operator-cli_ingress-node-firewall-operator" class="j-doc-nav__link ">
    9.1.1. Installing the Ingress Node Firewall Operator using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#install-operator-web-console_ingress-node-firewall-operator" class="j-doc-nav__link ">
    9.1.2. Installing the Ingress Node Firewall Operator using the web console
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-infw-operator-cr_ingress-node-firewall-operator" class="j-doc-nav__link ">
    9.2. Ingress Node Firewall Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-infw-operator-deploying_ingress-node-firewall-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    9.3. Deploying Ingress Node Firewall Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "9.3. Deploying Ingress Node Firewall Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "9.3. Deploying Ingress Node Firewall Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-infw-operator-config-object_ingress-node-firewall-operator" class="j-doc-nav__link ">
    9.3.1. Ingress Node Firewall configuration object
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-node-firewall-operator-rules-object_ingress-node-firewall-operator" class="j-doc-nav__link ">
    9.3.2. Ingress Node Firewall rules object
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-infw-operator-viewing_ingress-node-firewall-operator" class="j-doc-nav__link ">
    9.4. Viewing Ingress Node Firewall Operator rules
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-infw-operator-troubleshooting_ingress-node-firewall-operator" class="j-doc-nav__link ">
    9.5. Troubleshooting the Ingress Node Firewall Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ingress-controller-dnsmgt" class="j-doc-nav__link j-doc-nav__link--has-children">
    10. Configuring an Ingress Controller for manual DNS Management
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "10. Configuring an Ingress Controller for manual DNS Management"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "10. Configuring an Ingress Controller for manual DNS Management"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#literal-managed-literal-dns-management-policy" class="j-doc-nav__link ">
    10.1. Managed DNS management policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#literal-unmanaged-literal-dns-management-policy" class="j-doc-nav__link ">
    10.2. Unmanaged DNS management policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#creating-a-custom-ingress-controller_ingress-controller-dnsmgt" class="j-doc-nav__link ">
    10.3. Creating a custom Ingress Controller with the Unmanaged DNS management policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#modifying-an-existing-ingress-controller_ingress-controller-dnsmgt" class="j-doc-nav__link ">
    10.4. Modifying an existing Ingress Controller
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ingress-controller-dns-management-additional-resources" class="j-doc-nav__link ">
    10.5. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-controller-endpoint-publishing-strategies" class="j-doc-nav__link j-doc-nav__link--has-children">
    11. Configuring the Ingress Controller endpoint publishing strategy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11. Configuring the Ingress Controller endpoint publishing strategy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11. Configuring the Ingress Controller endpoint publishing strategy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-controller-endpoint-publishing-strategies_nw-ingress-controller-endpoint-publishing-strategies" class="j-doc-nav__link j-doc-nav__link--has-children">
    11.1. Ingress Controller endpoint publishing strategy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "11.1. Ingress Controller endpoint publishing strategy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "11.1. Ingress Controller endpoint publishing strategy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingresscontroller-change-internal_nw-ingress-controller-endpoint-publishing-strategies" class="j-doc-nav__link ">
    11.1.1. Configuring the Ingress Controller endpoint publishing scope to Internal
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingresscontroller-change-external_nw-ingress-controller-endpoint-publishing-strategies" class="j-doc-nav__link ">
    11.1.2. Configuring the Ingress Controller endpoint publishing scope to External
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources_nw-ingress-controller-endpoint-publishing-strategies" class="j-doc-nav__link ">
    11.2. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#verifying-connectivity-endpoint" class="j-doc-nav__link j-doc-nav__link--has-children">
    12. Verifying connectivity to an endpoint
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "12. Verifying connectivity to an endpoint"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "12. Verifying connectivity to an endpoint"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-pod-network-connectivity-checks_verifying-connectivity-endpoint" class="j-doc-nav__link ">
    12.1. Connection health checks performed
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-pod-network-connectivity-implementation_verifying-connectivity-endpoint" class="j-doc-nav__link ">
    12.2. Implementation of connection health checks
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-pod-network-connectivity-check-object_verifying-connectivity-endpoint" class="j-doc-nav__link ">
    12.3. PodNetworkConnectivityCheck object fields
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-pod-network-connectivity-verify_verifying-connectivity-endpoint" class="j-doc-nav__link ">
    12.4. Verifying network connectivity for an endpoint
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#changing-cluster-network-mtu" class="j-doc-nav__link j-doc-nav__link--has-children">
    13. Changing the MTU for the cluster network
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13. Changing the MTU for the cluster network"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13. Changing the MTU for the cluster network"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-cluster-mtu-change-about_changing-cluster-network-mtu" class="j-doc-nav__link j-doc-nav__link--has-children">
    13.1. About the cluster MTU
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "13.1. About the cluster MTU"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "13.1. About the cluster MTU"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#service-interruption-considerations_changing-cluster-network-mtu" class="j-doc-nav__link ">
    13.1.1. Service interruption considerations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#mtu-value-selection_changing-cluster-network-mtu" class="j-doc-nav__link ">
    13.1.2. MTU value selection
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#how-the-migration-process-works_changing-cluster-network-mtu" class="j-doc-nav__link ">
    13.1.3. How the migration process works
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-cluster-mtu-change_changing-cluster-network-mtu" class="j-doc-nav__link ">
    13.2. Changing the cluster MTU
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#changing-cluster-network-mtu-additional-resources" class="j-doc-nav__link ">
    13.3. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-node-port-service-range" class="j-doc-nav__link j-doc-nav__link--has-children">
    14. Configuring the node port service range
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "14. Configuring the node port service range"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "14. Configuring the node port service range"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-node-port-service-range-prerequisites" class="j-doc-nav__link ">
    14.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-nodeport-service-range-edit_configuring-node-port-service-range" class="j-doc-nav__link ">
    14.2. Expanding the node port range
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-node-port-service-range-additional-resources" class="j-doc-nav__link ">
    14.3. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-cluster-network-range" class="j-doc-nav__link j-doc-nav__link--has-children">
    15. Configuring the cluster network range
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "15. Configuring the cluster network range"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "15. Configuring the cluster network range"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-cluster-network-range-edit_configuring-cluster-network-range" class="j-doc-nav__link ">
    15.1. Expanding the cluster network IP address range
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-cluster-network-range-additional-resources" class="j-doc-nav__link ">
    15.2. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ipfailover" class="j-doc-nav__link j-doc-nav__link--has-children">
    16. Configuring IP failover
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "16. Configuring IP failover"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "16. Configuring IP failover"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ipfailover-environment-variables_configuring-ipfailover" class="j-doc-nav__link ">
    16.1. IP failover environment variables
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ipfailover-configuration_configuring-ipfailover" class="j-doc-nav__link ">
    16.2. Configuring IP failover
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ipfailover-virtual-ip-addresses-concept_configuring-ipfailover" class="j-doc-nav__link ">
    16.3. About virtual IP addresses
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ipfailover-configuring-check-notify-scripts_configuring-ipfailover" class="j-doc-nav__link ">
    16.4. Configuring check and notify scripts
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ipfailover-configuring-vrrp-preemption_configuring-ipfailover" class="j-doc-nav__link ">
    16.5. Configuring VRRP preemption
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ipfailover-vrrp-ip-offset_configuring-ipfailover" class="j-doc-nav__link ">
    16.6. About VRRP ID offset
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ipfailover-configuring-more-than-254_configuring-ipfailover" class="j-doc-nav__link ">
    16.7. Configuring IP failover for more than 254 addresses
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ipfailover-cluster-ha-ingress_configuring-ipfailover" class="j-doc-nav__link ">
    16.8. High availability For ingressIP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ipfailover-remove_configuring-ipfailover" class="j-doc-nav__link ">
    16.9. Removing IP failover
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nodes-setting-interface-level-network-sysctls" class="j-doc-nav__link j-doc-nav__link--has-children">
    17. Configuring interface-level network sysctls
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "17. Configuring interface-level network sysctls"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "17. Configuring interface-level network sysctls"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-configuring-tuning-cni_set-networkinterface-sysctls" class="j-doc-nav__link ">
    17.1. Configuring the tuning CNI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources_nodes-setting-interface-level-network-sysctls" class="j-doc-nav__link ">
    17.2. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#using-sctp" class="j-doc-nav__link j-doc-nav__link--has-children">
    18. Using the Stream Control Transmission Protocol (SCTP) on a bare metal cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "18. Using the Stream Control Transmission Protocol (SCTP) on a bare metal cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "18. Using the Stream Control Transmission Protocol (SCTP) on a bare metal cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sctp-about_using-sctp" class="j-doc-nav__link j-doc-nav__link--has-children">
    18.1. Support for Stream Control Transmission Protocol (SCTP) on OpenShift Container Platform
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "18.1. Support for Stream Control Transmission Protocol (SCTP) on OpenShift Container Platform"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "18.1. Support for Stream Control Transmission Protocol (SCTP) on OpenShift Container Platform"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#example_configurations_using-sctp" class="j-doc-nav__link ">
    18.1.1. Example configurations using SCTP protocol
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sctp-enabling_using-sctp" class="j-doc-nav__link ">
    18.2. Enabling Stream Control Transmission Protocol (SCTP)
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sctp-verifying_using-sctp" class="j-doc-nav__link ">
    18.3. Verifying Stream Control Transmission Protocol (SCTP) is enabled
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#using-ptp" class="j-doc-nav__link j-doc-nav__link--has-children">
    19. Using PTP hardware
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "19. Using PTP hardware"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "19. Using PTP hardware"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#about-using-ptp-hardware" class="j-doc-nav__link ">
    19.1. About PTP hardware
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ptp-introduction_using-ptp" class="j-doc-nav__link j-doc-nav__link--has-children">
    19.2. About PTP
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "19.2. About PTP"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "19.2. About PTP"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ptp-elements_using-ptp" class="j-doc-nav__link ">
    19.2.1. Elements of a PTP domain
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ptp-advantages-over-ntp_using-ptp" class="j-doc-nav__link ">
    19.2.2. Advantages of PTP over NTP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ptp-dual-nics_using-ptp" class="j-doc-nav__link ">
    19.2.3. Using PTP with dual NIC hardware
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ptp-linuxptp-introduction_using-ptp" class="j-doc-nav__link ">
    19.3. Overview of linuxptp in OpenShift Container Platform nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#install-ptp-operator-cli_using-ptp" class="j-doc-nav__link ">
    19.4. Installing the PTP Operator using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#install-ptp-operator-web-console_using-ptp" class="j-doc-nav__link ">
    19.5. Installing the PTP Operator using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ptp-devices" class="j-doc-nav__link j-doc-nav__link--has-children">
    19.6. Configuring PTP devices
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "19.6. Configuring PTP devices"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "19.6. Configuring PTP devices"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#discover-ptp-devices_using-ptp" class="j-doc-nav__link ">
    19.6.1. Discovering PTP capable network devices in your cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-linuxptp-services-as-grandmaster-clock_using-ptp" class="j-doc-nav__link j-doc-nav__link--has-children">
    19.6.2. Configuring linuxptp services as a grandmaster clock
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "19.6.2. Configuring linuxptp services as a grandmaster clock"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "19.6.2. Configuring linuxptp services as a grandmaster clock"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ptp-grandmaster-clock-configuration-reference_using-ptp" class="j-doc-nav__link ">
    19.6.2.1. Grandmaster clock PtpConfig configuration reference
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-linuxptp-services-as-ordinary-clock_using-ptp" class="j-doc-nav__link ">
    19.6.3. Configuring linuxptp services as an ordinary clock
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-linuxptp-services-as-boundary-clock_using-ptp" class="j-doc-nav__link ">
    19.6.4. Configuring linuxptp services as a boundary clock
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ptp-configuring-linuxptp-services-as-bc-for-dual-nic_using-ptp" class="j-doc-nav__link ">
    19.6.5. Configuring linuxptp services as boundary clocks for dual NIC hardware
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-columbiaville-ptp-config-refererence_using-ptp" class="j-doc-nav__link ">
    19.6.6. Intel Columbiaville E800 series NIC as PTP ordinary clock reference
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-configuring-fifo-priority-scheduling-for-ptp_using-ptp" class="j-doc-nav__link ">
    19.6.7. Configuring FIFO priority scheduling for PTP hardware
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-configuring-log-filtering-for-linuxptp_using-ptp" class="j-doc-nav__link ">
    19.6.8. Configuring log filtering for linuxptp services
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-troubleshooting-common-ptp-operator-issues_using-ptp" class="j-doc-nav__link j-doc-nav__link--has-children">
    19.7. Troubleshooting common PTP Operator issues
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "19.7. Troubleshooting common PTP Operator issues"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "19.7. Troubleshooting common PTP Operator issues"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-about-collecting-nro-data_using-ptp" class="j-doc-nav__link ">
    19.7.1. Collecting Precision Time Protocol (PTP) Operator data
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ptp-hardware-fast-event-notifications-framework" class="j-doc-nav__link j-doc-nav__link--has-children">
    19.8. PTP hardware fast event notifications framework
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "19.8. PTP hardware fast event notifications framework"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "19.8. PTP hardware fast event notifications framework"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-about-ptp-and-clock-synchronization_using-ptp" class="j-doc-nav__link ">
    19.8.1. About PTP and clock synchronization error events
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-about-ptp-fast-event-notifications-framework_using-ptp" class="j-doc-nav__link ">
    19.8.2. About the PTP fast event notifications framework
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-configuring-the-ptp-fast-event-publisher_using-ptp" class="j-doc-nav__link ">
    19.8.3. Configuring the PTP fast event notifications publisher
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-migrating-from-amqp-to-http-transport_using-ptp" class="j-doc-nav__link ">
    19.8.4. Migrating consumer applications to use HTTP transport for PTP or bare-metal events
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-installing-amq-interconnect-messaging-bus_using-ptp" class="j-doc-nav__link ">
    19.8.5. Installing the AMQ messaging bus
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-fast-event-notifications-api-refererence_using-ptp" class="j-doc-nav__link j-doc-nav__link--has-children">
    19.8.6. Subscribing DU applications to PTP events REST API reference
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "19.8.6. Subscribing DU applications to PTP events REST API reference"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "19.8.6. Subscribing DU applications to PTP events REST API reference"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#api-ocloudnotifications-v1-subscriptions" class="j-doc-nav__link ">
    19.8.6.1. api/ocloudNotifications/v1/subscriptions
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#api-ocloudnotifications-v1-subscriptions-subscription_id" class="j-doc-nav__link ">
    19.8.6.2. api/ocloudNotifications/v1/subscriptions/&lt;subscription_id&gt;
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#api-ocloudnotifications-v1-subscriptions-status-subscription_id" class="j-doc-nav__link ">
    19.8.6.3. api/ocloudNotifications/v1/subscriptions/status/&lt;subscription_id&gt;
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#api-ocloudnotifications-v1-health" class="j-doc-nav__link ">
    19.8.6.4. api/ocloudNotifications/v1/health/
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#api-ocloudnotifications-v1-publishers" class="j-doc-nav__link ">
    19.8.6.5. api/ocloudNotifications/v1/publishers
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#api-ocloudnotifications-v1-resource_address-currentstate" class="j-doc-nav__link ">
    19.8.6.6. /api/ocloudnotifications/v1/&lt;resource_address&gt;/CurrentState
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-monitoring-fast-events-metrics_using-ptp" class="j-doc-nav__link ">
    19.8.7. Monitoring PTP fast event metrics
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ptp-cloud-events-consumer-dev-reference" class="j-doc-nav__link j-doc-nav__link--has-children">
    20. Developing PTP events consumer applications
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "20. Developing PTP events consumer applications"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "20. Developing PTP events consumer applications"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ptp-events-consumer-application_ptp-consumer" class="j-doc-nav__link ">
    20.1. PTP events consumer application reference
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ptp-reference-deployment-and-service-crs_ptp-consumer" class="j-doc-nav__link ">
    20.2. Reference cloud-event-proxy deployment and service CRs
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ptp-cloud-event-proxy-sidecar-api_ptp-consumer" class="j-doc-nav__link ">
    20.3. PTP events available from the cloud-event-proxy sidecar REST API
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ptp-subscribing-consumer-app-to-events_ptp-consumer" class="j-doc-nav__link j-doc-nav__link--has-children">
    20.4. Subscribing the consumer application to PTP events
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "20.4. Subscribing the consumer application to PTP events"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "20.4. Subscribing the consumer application to PTP events"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ptp-sub-lock-state-events_ptp-consumer" class="j-doc-nav__link ">
    20.4.1. Subscribing to PTP lock-state events
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ptp-sub-os-clock-sync-state_ptp-consumer" class="j-doc-nav__link ">
    20.4.2. Subscribing to PTP os-clock-sync-state events
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ptp-sub-ptp-clock-class-change_ptp-consumer" class="j-doc-nav__link ">
    20.4.3. Subscribing to PTP ptp-clock-class-change events
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ptp-getting-the-current-ptp-clock-status_ptp-consumer" class="j-doc-nav__link ">
    20.5. Getting the current PTP clock status
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ptp-verifying-events-consumer-app-is-receiving-events_ptp-consumer" class="j-doc-nav__link ">
    20.6. Verifying that the PTP events consumer application is receiving events
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#external-dns-operator-1" class="j-doc-nav__link j-doc-nav__link--has-children">
    21. External DNS Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "21. External DNS Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "21. External DNS Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#external-dns-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    21.1. External DNS Operator in OpenShift Container Platform
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "21.1. External DNS Operator in OpenShift Container Platform"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "21.1. External DNS Operator in OpenShift Container Platform"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-external-dns-operator_external-dns-operator" class="j-doc-nav__link ">
    21.1.1. External DNS Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-external-dns-operator-logs_external-dns-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    21.1.2. External DNS Operator logs
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "21.1.2. External DNS Operator logs"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "21.1.2. External DNS Operator logs"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#external-dns-operator-domain-name-limitations" class="j-doc-nav__link ">
    21.1.2.1. External DNS Operator domain name limitations
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#installing-external-dns-on-cloud-providers" class="j-doc-nav__link j-doc-nav__link--has-children">
    21.2. Installing External DNS Operator on cloud providers
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "21.2. Installing External DNS Operator on cloud providers"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "21.2. Installing External DNS Operator on cloud providers"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-installing-external-dns-operator_installing-external-dns-on-cloud-providers" class="j-doc-nav__link ">
    21.2.1. Installing the External DNS Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#external-dns-operator-configuration-parameters" class="j-doc-nav__link j-doc-nav__link--has-children">
    21.3. External DNS Operator configuration parameters
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "21.3. External DNS Operator configuration parameters"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "21.3. External DNS Operator configuration parameters"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-external-dns-operator-configuration-parameters_external-dns-operator-configuration-parameters" class="j-doc-nav__link ">
    21.3.1. External DNS Operator configuration parameters
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#creating-dns-records-on-aws" class="j-doc-nav__link j-doc-nav__link--has-children">
    21.4. Creating DNS records on AWS
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "21.4. Creating DNS records on AWS"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "21.4. Creating DNS records on AWS"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-control-dns-records-public-hosted-zone-aws_creating-dns-records-on-aws" class="j-doc-nav__link ">
    21.4.1. Creating DNS records on an public hosted zone for AWS by using Red Hat External DNS Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#creating-dns-records-on-azure" class="j-doc-nav__link j-doc-nav__link--has-children">
    21.5. Creating DNS records on Azure
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "21.5. Creating DNS records on Azure"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "21.5. Creating DNS records on Azure"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-control-dns-records-public-hosted-zone-azure_creating-dns-records-on-azure" class="j-doc-nav__link ">
    21.5.1. Creating DNS records on an public DNS zone for Azure by using Red Hat External DNS Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#creating-dns-records-on-gcp" class="j-doc-nav__link j-doc-nav__link--has-children">
    21.6. Creating DNS records on GCP
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "21.6. Creating DNS records on GCP"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "21.6. Creating DNS records on GCP"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-control-dns-records-public-managed-zone-gcp_creating-dns-records-on-gcp" class="j-doc-nav__link ">
    21.6.1. Creating DNS records on an public managed zone for GCP by using Red Hat External DNS Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#creating-dns-records-on-infoblox" class="j-doc-nav__link j-doc-nav__link--has-children">
    21.7. Creating DNS records on Infoblox
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "21.7. Creating DNS records on Infoblox"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "21.7. Creating DNS records on Infoblox"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-control-dns-records-public-dns-zone-infoblox_creating-dns-records-on-infoblox" class="j-doc-nav__link ">
    21.7.1. Creating DNS records on a public DNS zone on Infoblox
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#external-dns-operator-cluster-wide-proxy" class="j-doc-nav__link j-doc-nav__link--has-children">
    21.8. Configuring the cluster-wide proxy on the External DNS Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "21.8. Configuring the cluster-wide proxy on the External DNS Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "21.8. Configuring the cluster-wide proxy on the External DNS Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-configuring-cluster-wide-proxy_external-dns-operator-cluster-wide-proxy" class="j-doc-nav__link ">
    21.8.1. Configuring the External DNS Operator to trust the certificate authority of the cluster-wide proxy
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-policy" class="j-doc-nav__link j-doc-nav__link--has-children">
    22. Network policy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "22. Network policy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "22. Network policy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#about-network-policy" class="j-doc-nav__link j-doc-nav__link--has-children">
    22.1. About network policy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "22.1. About network policy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "22.1. About network policy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-about_about-network-policy" class="j-doc-nav__link j-doc-nav__link--has-children">
    22.1.1. About network policy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "22.1.1. About network policy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "22.1.1. About network policy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-allow-from-router_about-network-policy" class="j-doc-nav__link ">
    22.1.1.1. Using the allow-from-router network policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-allow-from-hostnetwork_about-network-policy" class="j-doc-nav__link ">
    22.1.1.2. Using the allow-from-hostnetwork network policy
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-optimize-sdn_about-network-policy" class="j-doc-nav__link ">
    22.1.2. Optimizations for network policy with OpenShift SDN
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-optimize-ovn_about-network-policy" class="j-doc-nav__link ">
    22.1.3. Optimizations for network policy with OVN-Kubernetes network plugin
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#about-network-policy-next-steps" class="j-doc-nav__link ">
    22.1.4. Next steps
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#about-network-policy-additional-resources" class="j-doc-nav__link ">
    22.1.5. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#creating-network-policy" class="j-doc-nav__link j-doc-nav__link--has-children">
    22.2. Creating a network policy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "22.2. Creating a network policy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "22.2. Creating a network policy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-object_creating-network-policy" class="j-doc-nav__link ">
    22.2.1. Example NetworkPolicy object
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-create-cli_creating-network-policy" class="j-doc-nav__link ">
    22.2.2. Creating a network policy using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-deny-all-multi-network-policy_creating-network-policy" class="j-doc-nav__link ">
    22.2.3. Creating a default deny all network policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-allow-external-clients_creating-network-policy" class="j-doc-nav__link ">
    22.2.4. Creating a network policy to allow traffic from external clients
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-allow-traffic-from-all-applications_creating-network-policy" class="j-doc-nav__link ">
    22.2.5. Creating a network policy allowing traffic to an application from all namespaces
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-allow-traffic-from-a-namespace_creating-network-policy" class="j-doc-nav__link ">
    22.2.6. Creating a network policy allowing traffic to an application from a namespace
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources-2" class="j-doc-nav__link ">
    22.2.7. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#viewing-network-policy" class="j-doc-nav__link j-doc-nav__link--has-children">
    22.3. Viewing a network policy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "22.3. Viewing a network policy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "22.3. Viewing a network policy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-object_viewing-network-policy" class="j-doc-nav__link ">
    22.3.1. Example NetworkPolicy object
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-view-cli_viewing-network-policy" class="j-doc-nav__link ">
    22.3.2. Viewing network policies using the CLI
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#editing-network-policy" class="j-doc-nav__link j-doc-nav__link--has-children">
    22.4. Editing a network policy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "22.4. Editing a network policy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "22.4. Editing a network policy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-edit_editing-network-policy" class="j-doc-nav__link ">
    22.4.1. Editing a network policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-object_editing-network-policy" class="j-doc-nav__link ">
    22.4.2. Example NetworkPolicy object
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#editing-network-policy-additional-resources" class="j-doc-nav__link ">
    22.4.3. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#deleting-network-policy" class="j-doc-nav__link j-doc-nav__link--has-children">
    22.5. Deleting a network policy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "22.5. Deleting a network policy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "22.5. Deleting a network policy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-delete-cli_deleting-network-policy" class="j-doc-nav__link ">
    22.5.1. Deleting a network policy using the CLI
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#default-network-policy" class="j-doc-nav__link j-doc-nav__link--has-children">
    22.6. Defining a default network policy for projects
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "22.6. Defining a default network policy for projects"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "22.6. Defining a default network policy for projects"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#modifying-template-for-new-projects_default-network-policy" class="j-doc-nav__link ">
    22.6.1. Modifying the template for new projects
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-project-defaults_default-network-policy" class="j-doc-nav__link ">
    22.6.2. Adding network policies to the new project template
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#multitenant-network-policy" class="j-doc-nav__link j-doc-nav__link--has-children">
    22.7. Configuring multitenant isolation with network policy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "22.7. Configuring multitenant isolation with network policy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "22.7. Configuring multitenant isolation with network policy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-multitenant-isolation_multitenant-network-policy" class="j-doc-nav__link ">
    22.7.1. Configuring multitenant isolation by using network policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#multitenant-network-policy-next-steps" class="j-doc-nav__link ">
    22.7.2. Next steps
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#multitenant-network-policy-additional-resources" class="j-doc-nav__link ">
    22.7.3. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#aws-load-balancer-operator-1" class="j-doc-nav__link j-doc-nav__link--has-children">
    23. AWS Load Balancer Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "23. AWS Load Balancer Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "23. AWS Load Balancer Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#aws-load-balancer-operator-release-notes" class="j-doc-nav__link j-doc-nav__link--has-children">
    23.1. AWS Load Balancer Operator release notes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "23.1. AWS Load Balancer Operator release notes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "23.1. AWS Load Balancer Operator release notes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#aws-load-balancer-operator-release-notes-1.0.0" class="j-doc-nav__link j-doc-nav__link--has-children">
    23.1.1. AWS Load Balancer Operator 1.0.0
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "23.1.1. AWS Load Balancer Operator 1.0.0"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "23.1.1. AWS Load Balancer Operator 1.0.0"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#aws-load-balancer-operator-1.0.0-notable-changes" class="j-doc-nav__link ">
    23.1.1.1. Notable changes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#aws-load-balancer-operator-1.0.0-bug-fixes" class="j-doc-nav__link ">
    23.1.1.2. Bug fixes
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#aws-load-balancer-operator-release-notes-earlier-versions" class="j-doc-nav__link ">
    23.1.2. Earlier versions
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#aws-load-balancer-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    23.2. AWS Load Balancer Operator in OpenShift Container Platform
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "23.2. AWS Load Balancer Operator in OpenShift Container Platform"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "23.2. AWS Load Balancer Operator in OpenShift Container Platform"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-aws-load-balancer-operator-considerations_aws-load-balancer-operator" class="j-doc-nav__link ">
    23.2.1. AWS Load Balancer Operator considerations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-aws-load-balancer-operator_aws-load-balancer-operator" class="j-doc-nav__link ">
    23.2.2. AWS Load Balancer Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-aws-load-balancer-operator-logs_aws-load-balancer-operator" class="j-doc-nav__link ">
    23.2.3. AWS Load Balancer Operator logs
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-aws-load-balancer-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    23.3. Understanding AWS Load Balancer Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "23.3. Understanding AWS Load Balancer Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "23.3. Understanding AWS Load Balancer Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-installing-aws-load-balancer-operator_aws-load-balancer-operator" class="j-doc-nav__link ">
    23.3.1. Installing the AWS Load Balancer Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#albo-sts-cluster" class="j-doc-nav__link j-doc-nav__link--has-children">
    23.4. Installing AWS Load Balancer Operator on a Security Token Service cluster
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "23.4. Installing AWS Load Balancer Operator on a Security Token Service cluster"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "23.4. Installing AWS Load Balancer Operator on a Security Token Service cluster"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-bootstra-albo-on-sts-cluster_albo-sts-cluster" class="j-doc-nav__link ">
    23.4.1. Bootstrapping AWS Load Balancer Operator on Security Token Service cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-installing-albo-on-sts-cluster_albo-sts-cluster" class="j-doc-nav__link ">
    23.4.2. Configuring AWS Load Balancer Operator on Security Token Service cluster by using managed CredentialsRequest objects
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-installing-albo-on-sts-cluster-predefined-credentials_albo-sts-cluster" class="j-doc-nav__link ">
    23.4.3. Configuring the AWS Load Balancer Operator on Security Token Service cluster by using specific credentials
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources-3" class="j-doc-nav__link ">
    23.4.4. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-create-instance-aws-load-balancer" class="j-doc-nav__link j-doc-nav__link--has-children">
    23.5. Creating an instance of AWS Load Balancer Controller
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "23.5. Creating an instance of AWS Load Balancer Controller"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "23.5. Creating an instance of AWS Load Balancer Controller"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-creating-instance-aws-load-balancer-controller_create-instance-aws-load-balancer" class="j-doc-nav__link ">
    23.5.1. Creating an instance of the AWS Load Balancer Controller using AWS Load Balancer Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multiple-ingress-through-single-alb" class="j-doc-nav__link j-doc-nav__link--has-children">
    23.6. Creating multiple ingresses
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "23.6. Creating multiple ingresses"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "23.6. Creating multiple ingresses"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-creating-multiple-ingress-through-single-alb_multiple-ingress-through-single-alb" class="j-doc-nav__link ">
    23.6.1. Creating multiple ingresses through a single AWS Load Balancer
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-adding-tls-termination" class="j-doc-nav__link j-doc-nav__link--has-children">
    23.7. Adding TLS termination
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "23.7. Adding TLS termination"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "23.7. Adding TLS termination"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-adding-tls-termination_adding-tls-termination" class="j-doc-nav__link ">
    23.7.1. Adding TLS termination on the AWS Load Balancer
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-aws-load-balancer-operator-cluster-wide-proxy" class="j-doc-nav__link j-doc-nav__link--has-children">
    23.8. Configuring cluster-wide proxy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "23.8. Configuring cluster-wide proxy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "23.8. Configuring cluster-wide proxy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-configuring-cluster-wide-proxy_aws-load-balancer-operator" class="j-doc-nav__link ">
    23.8.1. Configuring the AWS Load Balancer Operator to trust the certificate authority of the cluster-wide proxy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources-4" class="j-doc-nav__link ">
    23.8.2. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#multiple-networks" class="j-doc-nav__link j-doc-nav__link--has-children">
    24. Multiple networks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24. Multiple networks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24. Multiple networks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#understanding-multiple-networks" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.1. Understanding multiple networks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.1. Understanding multiple networks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.1. Understanding multiple networks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-network-considerations" class="j-doc-nav__link ">
    24.1.1. Usage scenarios for an additional network
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-networks-provided" class="j-doc-nav__link ">
    24.1.2. Additional networks in OpenShift Container Platform
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-additional-network" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.2. Configuring an additional network
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.2. Configuring an additional network"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.2. Configuring an additional network"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-additional-network_approaches-managing-additional-network" class="j-doc-nav__link ">
    24.2.1. Approaches to managing an additional network
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-additional-network_configuration-additional-network-attachment" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.2.2. Configuration for an additional network attachment
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.2.2. Configuration for an additional network attachment"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.2.2. Configuration for an additional network attachment"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-additional-network_configuration-additional-network-cno" class="j-doc-nav__link ">
    24.2.2.1. Configuration of an additional network through the Cluster Network Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-additional-network_configuration-additional-network-yaml" class="j-doc-nav__link ">
    24.2.2.2. Configuration of an additional network from a YAML manifest
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-additional-network_configuration-additional-network-types" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.2.3. Configurations for additional network types
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.2.3. Configurations for additional network types"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.2.3. Configurations for additional network types"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-bridge-object_configuring-additional-network" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.2.3.1. Configuration for a bridge additional network
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.2.3.1. Configuration for a bridge additional network"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.2.3.1. Configuration for a bridge additional network"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-bridge-config-example_configuring-additional-network" class="j-doc-nav__link ">
    24.2.3.1.1. bridge configuration example
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-host-device-object_configuring-additional-network" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.2.3.2. Configuration for a host device additional network
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.2.3.2. Configuration for a host device additional network"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.2.3.2. Configuration for a host device additional network"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-hostdev-config-example_configuring-additional-network" class="j-doc-nav__link ">
    24.2.3.2.1. host-device configuration example
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-vlan-object_configuring-additional-network" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.2.3.3. Configuration for an VLAN additional network
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.2.3.3. Configuration for an VLAN additional network"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.2.3.3. Configuration for an VLAN additional network"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-vlan-config-example_configuring-additional-network" class="j-doc-nav__link ">
    24.2.3.3.1. vlan configuration example
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-ipvlan-object_configuring-additional-network" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.2.3.4. Configuration for an IPVLAN additional network
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.2.3.4. Configuration for an IPVLAN additional network"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.2.3.4. Configuration for an IPVLAN additional network"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-ipvlan-config-example_configuring-additional-network" class="j-doc-nav__link ">
    24.2.3.4.1. ipvlan configuration example
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-macvlan-object_configuring-additional-network" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.2.3.5. Configuration for a MACVLAN additional network
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.2.3.5. Configuration for a MACVLAN additional network"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.2.3.5. Configuration for a MACVLAN additional network"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-macvlan-config-example_configuring-additional-network" class="j-doc-nav__link ">
    24.2.3.5.1. macvlan configuration example
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuration-ovnk-additional-networks_configuring-additional-network" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.2.3.6. Configuration for an OVN-Kubernetes additional network
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.2.3.6. Configuration for an OVN-Kubernetes additional network"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.2.3.6. Configuration for an OVN-Kubernetes additional network"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuration-ovnk-network-plugin-json-object_configuring-additional-network" class="j-doc-nav__link ">
    24.2.3.6.1. OVN-Kubernetes network plugin JSON configuration table
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuration-layer-two-switched-topology_configuring-additional-network" class="j-doc-nav__link ">
    24.2.3.6.2. Configuration for a switched topology
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-pods-secondary-network_configuring-additional-network" class="j-doc-nav__link ">
    24.2.3.6.3. Configuring pods for additional networks
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-pods-static-ip_configuring-additional-network" class="j-doc-nav__link ">
    24.2.3.6.4. Configuring pods with a static IP address
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-ipam-object_configuring-additional-network" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.2.4. Configuration of IP address assignment for an additional network
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.2.4. Configuration of IP address assignment for an additional network"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.2.4. Configuration of IP address assignment for an additional network"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-static_configuring-additional-network" class="j-doc-nav__link ">
    24.2.4.1. Static IP address assignment configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-dhcp_configuring-additional-network" class="j-doc-nav__link ">
    24.2.4.2. Dynamic IP address (DHCP) assignment configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-whereabouts_configuring-additional-network" class="j-doc-nav__link ">
    24.2.4.3. Dynamic IP address assignment configuration with Whereabouts
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-creating-whereabouts-reconciler-daemon-set_configuring-additional-network" class="j-doc-nav__link ">
    24.2.4.4. Creating a Whereabouts reconciler daemon set
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-create-network_configuring-additional-network" class="j-doc-nav__link ">
    24.2.5. Creating an additional network attachment with the Cluster Network Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-create-network-apply_configuring-additional-network" class="j-doc-nav__link ">
    24.2.6. Creating an additional network attachment by applying a YAML manifest
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#about-virtual-routing-and-forwarding" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.3. About virtual routing and forwarding
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.3. About virtual routing and forwarding"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.3. About virtual routing and forwarding"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-about-virtual-routing-and-forwarding_about-virtual-routing-and-forwarding" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.3.1. About virtual routing and forwarding
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.3.1. About virtual routing and forwarding"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.3.1. About virtual routing and forwarding"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-benefits-secondary-networks-telecommunications-operators_about-virtual-routing-and-forwarding" class="j-doc-nav__link ">
    24.3.1.1. Benefits of secondary networks for pods for telecommunications operators
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-multi-network-policy" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.4. Configuring multi-network policy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.4. Configuring multi-network policy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.4. Configuring multi-network policy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multi-network-policy-differences_configuring-multi-network-policy" class="j-doc-nav__link ">
    24.4.1. Differences between multi-network policy and network policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multi-network-policy-enable_configuring-multi-network-policy" class="j-doc-nav__link ">
    24.4.2. Enabling multi-network policy for the cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-multi-network-policy_working-with-multi-network-policy" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.4.3. Working with multi-network policy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.4.3. Working with multi-network policy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.4.3. Working with multi-network policy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-multi-network-policy_prerequisites" class="j-doc-nav__link ">
    24.4.3.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-create-cli_configuring-multi-network-policy" class="j-doc-nav__link ">
    24.4.3.2. Creating a multi-network policy using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-edit_configuring-multi-network-policy" class="j-doc-nav__link ">
    24.4.3.3. Editing a multi-network policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-view-cli_configuring-multi-network-policy" class="j-doc-nav__link ">
    24.4.3.4. Viewing multi-network policies using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-delete-cli_configuring-multi-network-policy" class="j-doc-nav__link ">
    24.4.3.5. Deleting a multi-network policy using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-deny-all-multi-network-policy_configuring-multi-network-policy" class="j-doc-nav__link ">
    24.4.3.6. Creating a default deny all multi-network policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-allow-external-clients_configuring-multi-network-policy" class="j-doc-nav__link ">
    24.4.3.7. Creating a multi-network policy to allow traffic from external clients
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-allow-traffic-from-all-applications_configuring-multi-network-policy" class="j-doc-nav__link ">
    24.4.3.8. Creating a multi-network policy allowing traffic to an application from all namespaces
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-allow-traffic-from-a-namespace_configuring-multi-network-policy" class="j-doc-nav__link ">
    24.4.3.9. Creating a multi-network policy allowing traffic to an application from a namespace
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-multi-network-policy_additional-resources" class="j-doc-nav__link ">
    24.4.4. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#attaching-pod" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.5. Attaching a pod to an additional network
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.5. Attaching a pod to an additional network"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.5. Attaching a pod to an additional network"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-add-pod_attaching-pod" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.5.1. Adding a pod to an additional network
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.5.1. Adding a pod to an additional network"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.5.1. Adding a pod to an additional network"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-advanced-annotations_attaching-pod" class="j-doc-nav__link ">
    24.5.1.1. Specifying pod-specific addressing and routing options
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#removing-pod" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.6. Removing a pod from an additional network
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.6. Removing a pod from an additional network"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.6. Removing a pod from an additional network"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-remove-pod_removing-pod" class="j-doc-nav__link ">
    24.6.1. Removing a pod from an additional network
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#edit-additional-network" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.7. Editing an additional network
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.7. Editing an additional network"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.7. Editing an additional network"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-edit-network_edit-additional-network" class="j-doc-nav__link ">
    24.7.1. Modifying an additional network attachment definition
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#remove-additional-network" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.8. Removing an additional network
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.8. Removing an additional network"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.8. Removing an additional network"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-delete-network_remove-additional-network" class="j-doc-nav__link ">
    24.8.1. Removing an additional network attachment definition
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#assigning-a-secondary-network-to-a-vrf" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.9. Assigning a secondary network to a VRF
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.9. Assigning a secondary network to a VRF"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.9. Assigning a secondary network to a VRF"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-assigning-a-secondary-network-to-a-vrf_assigning-a-secondary-network-to-a-vrf" class="j-doc-nav__link j-doc-nav__link--has-children">
    24.9.1. Assigning a secondary network to a VRF
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "24.9.1. Assigning a secondary network to a VRF"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "24.9.1. Assigning a secondary network to a VRF"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-creating-an-additional-network-attachment-with-the-cni-vrf-plug-in_assigning-a-secondary-network-to-a-vrf" class="j-doc-nav__link ">
    24.9.1.1. Creating an additional network attachment with the CNI VRF plugin
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#hardware-networks" class="j-doc-nav__link j-doc-nav__link--has-children">
    25. Hardware networks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25. Hardware networks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25. Hardware networks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#about-sriov" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.1. About Single Root I/O Virtualization (SR-IOV) hardware networks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.1. About Single Root I/O Virtualization (SR-IOV) hardware networks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.1. About Single Root I/O Virtualization (SR-IOV) hardware networks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#components-sr-iov-network-devices" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.1.1. Components that manage SR-IOV network devices
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.1.1. Components that manage SR-IOV network devices"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.1.1. Components that manage SR-IOV network devices"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-supported-platforms_about-sriov" class="j-doc-nav__link ">
    25.1.1.1. Supported platforms
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#supported-devices_about-sriov" class="j-doc-nav__link ">
    25.1.1.2. Supported devices
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#discover-sr-iov-devices_about-sriov" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.1.1.3. Automated discovery of SR-IOV network devices
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.1.1.3. Automated discovery of SR-IOV network devices"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.1.1.3. Automated discovery of SR-IOV network devices"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#example-sriovnetworknodestate_about-sriov" class="j-doc-nav__link ">
    25.1.1.3.1. Example SriovNetworkNodeState object
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#example-vf-use-in-pod_about-sriov" class="j-doc-nav__link ">
    25.1.1.4. Example use of a virtual function in a pod
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-app-netutil_about-sriov" class="j-doc-nav__link ">
    25.1.1.5. DPDK library for use with container applications
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-hugepages_about-sriov" class="j-doc-nav__link ">
    25.1.1.6. Huge pages resource injection for Downward API
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configure-multi-networks-additional-resources" class="j-doc-nav__link ">
    25.1.2. Additional resources
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#about-sriov-next-steps" class="j-doc-nav__link ">
    25.1.3. Next steps
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#installing-sriov-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.2. Installing the SR-IOV Network Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.2. Installing the SR-IOV Network Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.2. Installing the SR-IOV Network Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#installing-sr-iov-operator_installing-sriov-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.2.1. Installing SR-IOV Network Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.2.1. Installing SR-IOV Network Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.2.1. Installing SR-IOV Network Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#install-operator-cli_installing-sriov-operator" class="j-doc-nav__link ">
    25.2.1.1. CLI: Installing the SR-IOV Network Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#install-operator-web-console_installing-sriov-operator" class="j-doc-nav__link ">
    25.2.1.2. Web console: Installing the SR-IOV Network Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#installing-sriov-operator-next-steps" class="j-doc-nav__link ">
    25.2.2. Next steps
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-sriov-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.3. Configuring the SR-IOV Network Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.3. Configuring the SR-IOV Network Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.3. Configuring the SR-IOV Network Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-configuring-operator_configuring-sriov-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.3.1. Configuring the SR-IOV Network Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.3.1. Configuring the SR-IOV Network Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.3.1. Configuring the SR-IOV Network Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-operator-cr_configuring-sriov-operator" class="j-doc-nav__link ">
    25.3.1.1. SR-IOV Network Operator config custom resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#about-network-resource-injector_configuring-sriov-operator" class="j-doc-nav__link ">
    25.3.1.2. About the Network Resources Injector
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#about-sr-iov-operator-admission-control-webhook_configuring-sriov-operator" class="j-doc-nav__link ">
    25.3.1.3. About the SR-IOV Network Operator admission controller webhook
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#about-custom-node-selectors_configuring-sriov-operator" class="j-doc-nav__link ">
    25.3.1.4. About custom node selectors
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#disable-enable-network-resource-injector_configuring-sriov-operator" class="j-doc-nav__link ">
    25.3.1.5. Disabling or enabling the Network Resources Injector
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#disable-enable-sr-iov-operator-admission-control-webhook_configuring-sriov-operator" class="j-doc-nav__link ">
    25.3.1.6. Disabling or enabling the SR-IOV Network Operator admission controller webhook
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-custom-nodeselector_configuring-sriov-operator" class="j-doc-nav__link ">
    25.3.1.7. Configuring a custom NodeSelector for the SR-IOV Network Config daemon
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configure-sr-iov-operator-single-node_configuring-sriov-operator" class="j-doc-nav__link ">
    25.3.1.8. Configuring the SR-IOV Network Operator for single node installations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#sriov-operator-hosted-control-planes_configuring-sriov-operator" class="j-doc-nav__link ">
    25.3.1.9. Deploying the SR-IOV Operator for hosted control planes
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-sriov-operator-next-steps" class="j-doc-nav__link ">
    25.3.2. Next steps
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-sriov-device" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.4. Configuring an SR-IOV network device
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.4. Configuring an SR-IOV network device"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.4. Configuring an SR-IOV network device"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-networknodepolicy-object_configuring-sriov-device" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.4.1. SR-IOV network node configuration object
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.4.1. SR-IOV network node configuration object"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.4.1. SR-IOV network node configuration object"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#sr-iov-network-node-configuration-examples_configuring-sriov-device" class="j-doc-nav__link ">
    25.4.1.1. SR-IOV network node configuration examples
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-nic-partitioning_configuring-sriov-device" class="j-doc-nav__link ">
    25.4.1.2. Virtual function (VF) partitioning for SR-IOV devices
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-configuring-device_configuring-sriov-device" class="j-doc-nav__link ">
    25.4.2. Configuring SR-IOV network devices
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-troubleshooting_configuring-sriov-device" class="j-doc-nav__link ">
    25.4.3. Troubleshooting SR-IOV configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-assigning-a-sriov-network-to-a-vrf_configuring-sriov-device" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.4.4. Assigning an SR-IOV network to a VRF
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.4.4. Assigning an SR-IOV network to a VRF"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.4.4. Assigning an SR-IOV network to a VRF"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-creating-an-additional-sriov-network-with-vrf-plug-in_configuring-sriov-device" class="j-doc-nav__link ">
    25.4.4.1. Creating an additional SR-IOV network attachment with the CNI VRF plugin
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-exclude-topology-manager_configuring-sriov-device" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.4.5. Exclude the SR-IOV network topology for NUMA-aware scheduling
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.4.5. Exclude the SR-IOV network topology for NUMA-aware scheduling"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.4.5. Exclude the SR-IOV network topology for NUMA-aware scheduling"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-configure-exclude-topology-manager_configuring-sriov-device" class="j-doc-nav__link ">
    25.4.5.1. Excluding the SR-IOV network topology for NUMA-aware scheduling
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-sriov-device-next-steps" class="j-doc-nav__link ">
    25.4.6. Next steps
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-sriov-net-attach" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.5. Configuring an SR-IOV Ethernet network attachment
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.5. Configuring an SR-IOV Ethernet network attachment"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.5. Configuring an SR-IOV Ethernet network attachment"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-network-object_configuring-sriov-net-attach" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.5.1. Ethernet device configuration object
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.5.1. Ethernet device configuration object"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.5.1. Ethernet device configuration object"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-ipam-object_configuring-sriov-net-attach" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.5.1.1. Configuration of IP address assignment for an additional network
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.5.1.1. Configuration of IP address assignment for an additional network"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.5.1.1. Configuration of IP address assignment for an additional network"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-static_configuring-sriov-net-attach" class="j-doc-nav__link ">
    25.5.1.1.1. Static IP address assignment configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-dhcp_configuring-sriov-net-attach" class="j-doc-nav__link ">
    25.5.1.1.2. Dynamic IP address (DHCP) assignment configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-whereabouts_configuring-sriov-net-attach" class="j-doc-nav__link ">
    25.5.1.1.3. Dynamic IP address assignment configuration with Whereabouts
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-creating-whereabouts-reconciler-daemon-set_configuring-sriov-net-attach" class="j-doc-nav__link ">
    25.5.1.1.4. Creating a Whereabouts reconciler daemon set
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-network-attachment_configuring-sriov-net-attach" class="j-doc-nav__link ">
    25.5.2. Configuring SR-IOV additional network
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-sriov-net-attach-next-steps" class="j-doc-nav__link ">
    25.5.3. Next steps
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-sriov-net-attach-additional-resources" class="j-doc-nav__link ">
    25.5.4. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-sriov-ib-attach" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.6. Configuring an SR-IOV InfiniBand network attachment
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.6. Configuring an SR-IOV InfiniBand network attachment"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.6. Configuring an SR-IOV InfiniBand network attachment"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-ibnetwork-object_configuring-sriov-ib-attach" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.6.1. InfiniBand device configuration object
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.6.1. InfiniBand device configuration object"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.6.1. InfiniBand device configuration object"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-ipam-object_configuring-sriov-ib-attach" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.6.1.1. Configuration of IP address assignment for an additional network
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.6.1.1. Configuration of IP address assignment for an additional network"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.6.1.1. Configuration of IP address assignment for an additional network"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-static_configuring-sriov-ib-attach" class="j-doc-nav__link ">
    25.6.1.1.1. Static IP address assignment configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-dhcp_configuring-sriov-ib-attach" class="j-doc-nav__link ">
    25.6.1.1.2. Dynamic IP address (DHCP) assignment configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-whereabouts_configuring-sriov-ib-attach" class="j-doc-nav__link ">
    25.6.1.1.3. Dynamic IP address assignment configuration with Whereabouts
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-creating-whereabouts-reconciler-daemon-set_configuring-sriov-ib-attach" class="j-doc-nav__link ">
    25.6.1.1.4. Creating a Whereabouts reconciler daemon set
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-network-attachment_configuring-sriov-ib-attach" class="j-doc-nav__link ">
    25.6.2. Configuring SR-IOV additional network
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-sriov-ib-attach-next-steps" class="j-doc-nav__link ">
    25.6.3. Next steps
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-sriov-ib-attach-additional-resources" class="j-doc-nav__link ">
    25.6.4. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#add-pod" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.7. Adding a pod to an SR-IOV additional network
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.7. Adding a pod to an SR-IOV additional network"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.7. Adding a pod to an SR-IOV additional network"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-runtime-config_configuring-sr-iov" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.7.1. Runtime configuration for a network attachment
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.7.1. Runtime configuration for a network attachment"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.7.1. Runtime configuration for a network attachment"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#runtime-config-ethernet_configuring-sr-iov" class="j-doc-nav__link ">
    25.7.1.1. Runtime configuration for an Ethernet-based SR-IOV attachment
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#runtime-config-infiniband_configuring-sr-iov" class="j-doc-nav__link ">
    25.7.1.2. Runtime configuration for an InfiniBand-based SR-IOV attachment
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multus-add-pod_configuring-sr-iov" class="j-doc-nav__link ">
    25.7.2. Adding a pod to an additional network
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-topology-manager_configuring-sr-iov" class="j-doc-nav__link ">
    25.7.3. Creating a non-uniform memory access (NUMA) aligned SR-IOV pod
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-openstack-ovs-sr-iov-testpmd-pod_configuring-sr-iov" class="j-doc-nav__link ">
    25.7.4. A test pod template for clusters that use SR-IOV on OpenStack
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#add-pod-additional-resources" class="j-doc-nav__link ">
    25.7.5. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-interface-level-sysctl-settings-sriov-device" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.8. Configuring interface-level network sysctl settings for SR-IOV networks
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.8. Configuring interface-level network sysctl settings for SR-IOV networks"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.8. Configuring interface-level network sysctl settings for SR-IOV networks"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-labeling-sriov-enabled-nodes_configuring-sysctl-interface-sriov-device" class="j-doc-nav__link ">
    25.8.1. Labeling nodes with an SR-IOV enabled NIC
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-setting-one-sysctl-flag_configuring-sysctl-interface-sriov-device" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.8.2. Setting one sysctl flag
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.8.2. Setting one sysctl flag"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.8.2. Setting one sysctl flag"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-basic-example-setting-one-sysctl-flag-node-policy_configuring-sysctl-interface-sriov-device" class="j-doc-nav__link ">
    25.8.2.1. Setting one sysctl flag on nodes with SR-IOV network devices
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-sysctl-on-sriov-network_configuring-sysctl-interface-sriov-device" class="j-doc-nav__link ">
    25.8.2.2. Configuring sysctl on a SR-IOV network
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-configure-sysctl-settings-flag-bonded_configuring-sysctl-interface-sriov-device" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.8.3. Configuring sysctl settings for pods associated with bonded SR-IOV interface flag
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.8.3. Configuring sysctl settings for pods associated with bonded SR-IOV interface flag"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.8.3. Configuring sysctl settings for pods associated with bonded SR-IOV interface flag"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-setting-all-sysctls-flag-node-policy-bonded_configuring-sysctl-interface-sriov-device" class="j-doc-nav__link ">
    25.8.3.1. Setting all sysctl flag on nodes with bonded SR-IOV network devices
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-sysctl-on-bonded-sriov-network_configuring-sysctl-interface-sriov-device" class="j-doc-nav__link ">
    25.8.3.2. Configuring sysctl on a bonded SR-IOV network
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#using-sriov-multicast" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.9. Using high performance multicast
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.9. Using high performance multicast"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.9. Using high performance multicast"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-high-performance-multicast_using-sriov-multicast" class="j-doc-nav__link ">
    25.9.1. High performance multicast
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-using-an-sriov-interface-for-multicast_using-sriov-multicast" class="j-doc-nav__link ">
    25.9.2. Configuring an SR-IOV interface for multicast
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#using-dpdk-and-rdma" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.10. Using DPDK and RDMA
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.10. Using DPDK and RDMA"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.10. Using DPDK and RDMA"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#example-vf-use-in-dpdk-mode-intel_using-dpdk-and-rdma" class="j-doc-nav__link ">
    25.10.1. Using a virtual function in DPDK mode with an Intel NIC
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#example-vf-use-in-dpdk-mode-mellanox_using-dpdk-and-rdma" class="j-doc-nav__link ">
    25.10.2. Using a virtual function in DPDK mode with a Mellanox NIC
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-example-dpdk-line-rate_using-dpdk-and-rdma" class="j-doc-nav__link ">
    25.10.3. Overview of achieving a specific DPDK line rate
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-example-dpdk-line-rate_using-dpdk-and-rdma" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.10.4. Using SR-IOV and the Node Tuning Operator to achieve a DPDK line rate
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.10.4. Using SR-IOV and the Node Tuning Operator to achieve a DPDK line rate"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.10.4. Using SR-IOV and the Node Tuning Operator to achieve a DPDK line rate"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-network-operator_using-dpdk-and-rdma" class="j-doc-nav__link ">
    25.10.4.1. Example SR-IOV Network Operator for virtual functions
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-create-object_using-dpdk-and-rdma" class="j-doc-nav__link ">
    25.10.4.2. Example SR-IOV network operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-dpdk-base-workload_using-dpdk-and-rdma" class="j-doc-nav__link ">
    25.10.4.3. Example DPDK base workload
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-dpdk-running-testpmd_using-dpdk-and-rdma" class="j-doc-nav__link ">
    25.10.4.4. Example testpmd script
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#example-vf-use-in-rdma-mode-mellanox_using-dpdk-and-rdma" class="j-doc-nav__link ">
    25.10.5. Using a virtual function in RDMA mode with a Mellanox NIC
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-openstack-ovs-dpdk-testpmd-pod_using-dpdk-and-rdma" class="j-doc-nav__link ">
    25.10.6. A test pod template for clusters that use OVS-DPDK on OpenStack
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-openstack-hw-offload-testpmd-pod_using-dpdk-and-rdma" class="j-doc-nav__link ">
    25.10.7. A test pod template for clusters that use OVS hardware offloading on OpenStack
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources_using-dpdk-and-rdma" class="j-doc-nav__link ">
    25.10.8. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#using-pod-level-bonding" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.11. Using pod-level bonding
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.11. Using pod-level bonding"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.11. Using pod-level bonding"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-cfg-bond-interface-with-virtual-functions_using-pod-level-bonding" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.11.1. Configuring a bond interface from two SR-IOV interfaces
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.11.1. Configuring a bond interface from two SR-IOV interfaces"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.11.1. Configuring a bond interface from two SR-IOV interfaces"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-cfg-creating-bond-network-attachment-definition_using-pod-level-bonding" class="j-doc-nav__link ">
    25.11.1.1. Creating a bond network attachment definition
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-cfg-creating-pod-using-interface_using-pod-level-bonding" class="j-doc-nav__link ">
    25.11.1.2. Creating a pod using a bond interface
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-hardware-offloading" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.12. Configuring hardware offloading
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.12. Configuring hardware offloading"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.12. Configuring hardware offloading"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#about-hardware-offloading_configuring-hardware-offloading" class="j-doc-nav__link ">
    25.12.1. About hardware offloading
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#supported_devices_configuring-hardware-offloading" class="j-doc-nav__link ">
    25.12.2. Supported devices
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-hardware-offloading-prerequisites" class="j-doc-nav__link ">
    25.12.3. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-machine-config-pool_configuring-hardware-offloading" class="j-doc-nav__link ">
    25.12.4. Configuring a machine config pool for hardware offloading
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configure-sriov-node-policy_configuring-hardware-offloading" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.12.5. Configuring the SR-IOV network node policy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.12.5. Configuring the SR-IOV network node policy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.12.5. Configuring the SR-IOV network node policy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-hwol-ref-openstack-sriov-policy_configuring-hardware-offloading" class="j-doc-nav__link ">
    25.12.5.1. An example SR-IOV network node policy for OpenStack
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#create-network-attachment-definition_configuring-hardware-offloading" class="j-doc-nav__link ">
    25.12.6. Creating a network attachment definition
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#adding-network-attachment-definition-to-pods_configuring-hardware-offloading" class="j-doc-nav__link ">
    25.12.7. Adding the network attachment definition to your pods
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#switching-bf2-nic-dpu" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.13. Switching Bluefield-2 from DPU to NIC
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.13. Switching Bluefield-2 from DPU to NIC"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.13. Switching Bluefield-2 from DPU to NIC"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#proc-switching-bf2-nic_switching-bf2-nic-dpu" class="j-doc-nav__link ">
    25.13.1. Switching Bluefield-2 from DPU mode to NIC mode
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#uninstalling-sriov-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    25.14. Uninstalling the SR-IOV Network Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "25.14. Uninstalling the SR-IOV Network Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "25.14. Uninstalling the SR-IOV Network Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-sriov-operator-uninstall_uninstalling-sr-iov-operator" class="j-doc-nav__link ">
    25.14.1. Uninstalling the SR-IOV Network Operator
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ovn-kubernetes-network-plugin" class="j-doc-nav__link j-doc-nav__link--has-children">
    26. OVN-Kubernetes network plugin
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26. OVN-Kubernetes network plugin"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26. OVN-Kubernetes network plugin"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#about-ovn-kubernetes" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.1. About the OVN-Kubernetes network plugin
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.1. About the OVN-Kubernetes network plugin"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.1. About the OVN-Kubernetes network plugin"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-purpose_about-ovn-kubernetes" class="j-doc-nav__link ">
    26.1.1. OVN-Kubernetes purpose
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-matrix_about-ovn-kubernetes" class="j-doc-nav__link ">
    26.1.2. Supported network plugin feature matrix
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-limitations_about-ovn-kubernetes" class="j-doc-nav__link ">
    26.1.3. OVN-Kubernetes IPv6 and dual-stack limitations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-session-affinity_about-ovn-kubernetes" class="j-doc-nav__link ">
    26.1.4. Session affinity
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ovn-kubernetes-architecture-assembly" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.2. OVN-Kubernetes architecture
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.2. OVN-Kubernetes architecture"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.2. OVN-Kubernetes architecture"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ovn-kubernetes-architecture-con" class="j-doc-nav__link ">
    26.2.1. Introduction to OVN-Kubernetes architecture
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-list-resources_ovn-kubernetes-architecture" class="j-doc-nav__link ">
    26.2.2. Listing all resources in the OVN-Kubernetes project
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-list-database-contents_ovn-kubernetes-architecture" class="j-doc-nav__link ">
    26.2.3. Listing the OVN-Kubernetes northbound database contents
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-examine-nb-database-contents-ref_ovn-kubernetes-architecture" class="j-doc-nav__link ">
    26.2.4. Command line arguments for ovn-nbctl to examine northbound database contents
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-list-southbound-database-contents_ovn-kubernetes-architecture" class="j-doc-nav__link ">
    26.2.5. Listing the OVN-Kubernetes southbound database contents
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-examine-sb-database-contents-ref_ovn-kubernetes-architecture" class="j-doc-nav__link ">
    26.2.6. Command line arguments for ovn-sbctl to examine southbound database contents
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ovn-kubernetes-logical-architecture-con_ovn-kubernetes-architecture" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.2.7. OVN-Kubernetes logical architecture
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.2.7. OVN-Kubernetes logical architecture"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.2.7. OVN-Kubernetes logical architecture"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-installing-network-tools_ovn-kubernetes-architecture" class="j-doc-nav__link ">
    26.2.7.1. Installing network-tools on local host
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-running-network-tools_ovn-kubernetes-architecture" class="j-doc-nav__link ">
    26.2.7.2. Running network-tools
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources_ovn-kubernetes-architecture" class="j-doc-nav__link ">
    26.2.8. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ovn-kubernetes-troubleshooting-sources" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.3. Troubleshooting OVN-Kubernetes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.3. Troubleshooting OVN-Kubernetes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.3. Troubleshooting OVN-Kubernetes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-readiness-probes_ovn-kubernetes-sources-of-troubleshooting-information" class="j-doc-nav__link ">
    26.3.1. Monitoring OVN-Kubernetes health by using readiness probes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-alerts-console_ovn-kubernetes-sources-of-troubleshooting-information" class="j-doc-nav__link ">
    26.3.2. Viewing OVN-Kubernetes alerts in the console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-alerts-cli_ovn-kubernetes-sources-of-troubleshooting-information" class="j-doc-nav__link ">
    26.3.3. Viewing OVN-Kubernetes alerts in the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-logs-cli_ovn-kubernetes-sources-of-troubleshooting-information" class="j-doc-nav__link ">
    26.3.4. Viewing the OVN-Kubernetes logs using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-logs-console_ovn-kubernetes-sources-of-troubleshooting-information" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.3.5. Viewing the OVN-Kubernetes logs using the web console
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.3.5. Viewing the OVN-Kubernetes logs using the web console"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.3.5. Viewing the OVN-Kubernetes logs using the web console"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-change-log-levels_ovn-kubernetes-sources-of-troubleshooting-information" class="j-doc-nav__link ">
    26.3.5.1. Changing the OVN-Kubernetes log levels
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-pod-connectivity-checks_ovn-kubernetes-sources-of-troubleshooting-information" class="j-doc-nav__link ">
    26.3.6. Checking the OVN-Kubernetes pod network connectivity
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources_ovn-kubernetes-sources-of-troubleshooting-information" class="j-doc-nav__link ">
    26.3.7. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#ovn-kubernetes-tracing-using-ovntrace" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.4. Tracing Openflow with ovnkube-trace
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.4. Tracing Openflow with ovnkube-trace"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.4. Tracing Openflow with ovnkube-trace"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-install-ovnkube-trace-local_ovn-kubernetes-tracing-with-ovnkube" class="j-doc-nav__link ">
    26.4.1. Installing the ovnkube-trace on local host
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-running-ovnkube-trace_ovn-kubernetes-tracing-with-ovnkube" class="j-doc-nav__link ">
    26.4.2. Running ovnkube-trace
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources_ovn-kubernetes-tracing-with-ovnkube" class="j-doc-nav__link ">
    26.4.3. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#migrate-from-openshift-sdn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.5. Migrating from the OpenShift SDN network plugin
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.5. Migrating from the OpenShift SDN network plugin"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.5. Migrating from the OpenShift SDN network plugin"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-migration-about_migrate-from-openshift-sdn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.5.1. Migration to the OVN-Kubernetes network plugin
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.5.1. Migration to the OVN-Kubernetes network plugin"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.5.1. Migration to the OVN-Kubernetes network plugin"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#considerations-migrating-ovn-kubernetes-network-provider_migrate-from-openshift-sdn" class="j-doc-nav__link ">
    26.5.1.1. Considerations for migrating to the OVN-Kubernetes network plugin
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#how-the-migration-process-works_migrate-from-openshift-sdn" class="j-doc-nav__link ">
    26.5.1.2. How the migration process works
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-migration_migrate-from-openshift-sdn" class="j-doc-nav__link ">
    26.5.2. Migrating to the OVN-Kubernetes network plugin
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#migrate-from-openshift-sdn-additional-resources" class="j-doc-nav__link ">
    26.5.3. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#rollback-to-openshift-sdn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.6. Rolling back to the OpenShift SDN network provider
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.6. Rolling back to the OpenShift SDN network provider"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.6. Rolling back to the OpenShift SDN network provider"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-rollback_rollback-to-openshift-sdn" class="j-doc-nav__link ">
    26.6.1. Migrating to the OpenShift SDN network plugin
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#migrate-from-kuryr-sdn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.7. Migrating from the Kuryr network plugin to the OVN-Kubernetes network plugin
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.7. Migrating from the Kuryr network plugin to the OVN-Kubernetes network plugin"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.7. Migrating from the Kuryr network plugin to the OVN-Kubernetes network plugin"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-kuryr-ovn-kubernetes-migration-about_migrate-from-kuryr-sdn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.7.1. Migration to the OVN-Kubernetes network provider
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.7.1. Migration to the OVN-Kubernetes network provider"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.7.1. Migration to the OVN-Kubernetes network provider"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#considerations-kuryr-migrating-network-provider_migrate-from-kuryr-sdn" class="j-doc-nav__link ">
    26.7.1.1. Considerations when migrating to the OVN-Kubernetes network provider
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#how-the-kuryr-migration-process-works_migrate-from-kuryr-sdn" class="j-doc-nav__link ">
    26.7.1.2. How the migration process works
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-kuryr-migration_migrate-from-kuryr-sdn" class="j-doc-nav__link ">
    26.7.2. Migrating to the OVN-Kubernetes network plugin
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-kuryr-cleanup_migrate-from-kuryr-sdn" class="j-doc-nav__link ">
    26.7.3. Cleaning up resources after migration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#migrate-from-kuryr-additional-resources" class="j-doc-nav__link ">
    26.7.4. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#converting-to-dual-stack" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.8. Converting to IPv4/IPv6 dual-stack networking
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.8. Converting to IPv4/IPv6 dual-stack networking"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.8. Converting to IPv4/IPv6 dual-stack networking"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-dual-stack-convert_converting-to-dual-stack" class="j-doc-nav__link ">
    26.8.1. Converting to a dual-stack cluster network
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-dual-stack-convert-back-single-stack_converting-to-dual-stack" class="j-doc-nav__link ">
    26.8.2. Converting to a single-stack cluster network
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#logging-network-policy" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.9. Logging for egress firewall and network policy rules
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.9. Logging for egress firewall and network policy rules"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.9. Logging for egress firewall and network policy rules"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-audit-concept_logging-network-policy" class="j-doc-nav__link ">
    26.9.1. Audit logging
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-policy-audit-configuration-logging-network-policy" class="j-doc-nav__link ">
    26.9.2. Audit configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-audit-configure_logging-network-policy" class="j-doc-nav__link ">
    26.9.3. Configuring egress firewall and network policy auditing for a cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-audit-enable_logging-network-policy" class="j-doc-nav__link ">
    26.9.4. Enabling egress firewall and network policy audit logging for a namespace
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-audit-disable_logging-network-policy" class="j-doc-nav__link ">
    26.9.5. Disabling egress firewall and network policy audit logging for a namespace
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#logging-network-policy-additional-resources" class="j-doc-nav__link ">
    26.9.6. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ipsec-ovn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.10. Configuring IPsec encryption
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.10. Configuring IPsec encryption"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.10. Configuring IPsec encryption"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ipsec-ovn-prerequisites" class="j-doc-nav__link ">
    26.10.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-ipsec-traffic_configuring-ipsec-ovn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.10.2. Types of network traffic flows encrypted by IPsec
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.10.2. Types of network traffic flows encrypted by IPsec"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.10.2. Types of network traffic flows encrypted by IPsec"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-connectivity-requirements-when-ipsec-is-enabled" class="j-doc-nav__link ">
    26.10.2.1. Network connectivity requirements when IPsec is enabled
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-ipsec-encryption_configuring-ipsec-ovn" class="j-doc-nav__link ">
    26.10.3. Encryption protocol and IPsec mode
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-ipsec-certificates_configuring-ipsec-ovn" class="j-doc-nav__link ">
    26.10.4. Security certificate generation and rotation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-ipsec-enable_configuring-ipsec-ovn" class="j-doc-nav__link ">
    26.10.5. Enabling IPsec encryption
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-ipsec-verification_configuring-ipsec-ovn" class="j-doc-nav__link ">
    26.10.6. Verifying that IPsec is enabled
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-ipsec-disable_configuring-ipsec-ovn" class="j-doc-nav__link ">
    26.10.7. Disabling IPsec encryption
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ipsec-ovn_additional-resources" class="j-doc-nav__link ">
    26.10.8. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-egress-firewall-ovn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.11. Configuring an egress firewall for a project
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.11. Configuring an egress firewall for a project"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.11. Configuring an egress firewall for a project"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egressnetworkpolicy-about_configuring-egress-firewall-ovn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.11.1. How an egress firewall works in a project
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.11.1. How an egress firewall works in a project"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.11.1. How an egress firewall works in a project"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#limitations-of-an-egress-firewall_configuring-egress-firewall-ovn" class="j-doc-nav__link ">
    26.11.1.1. Limitations of an egress firewall
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#policy-rule-order_configuring-egress-firewall-ovn" class="j-doc-nav__link ">
    26.11.1.2. Matching order for egress firewall policy rules
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#domain-name-server-resolution_configuring-egress-firewall-ovn" class="j-doc-nav__link ">
    26.11.1.3. How Domain Name Server (DNS) resolution works
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egressnetworkpolicy-object_configuring-egress-firewall-ovn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.11.2. EgressFirewall custom resource (CR) object
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.11.2. EgressFirewall custom resource (CR) object"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.11.2. EgressFirewall custom resource (CR) object"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#egressnetworkpolicy-rules_configuring-egress-firewall-ovn" class="j-doc-nav__link ">
    26.11.2.1. EgressFirewall rules
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#egressnetworkpolicy-example_configuring-egress-firewall-ovn" class="j-doc-nav__link ">
    26.11.2.2. Example EgressFirewall CR objects
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuringNodeSelector-example_configuring-egress-firewall-ovn" class="j-doc-nav__link ">
    26.11.2.3. Example nodeSelector for EgressFirewall
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-create_configuring-egress-firewall-ovn" class="j-doc-nav__link ">
    26.11.3. Creating an egress firewall policy object
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#viewing-egress-firewall-ovn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.12. Viewing an egress firewall for a project
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.12. Viewing an egress firewall for a project"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.12. Viewing an egress firewall for a project"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egressnetworkpolicy-view_viewing-egress-firewall-ovn" class="j-doc-nav__link ">
    26.12.1. Viewing an EgressFirewall object
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#editing-egress-firewall-ovn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.13. Editing an egress firewall for a project
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.13. Editing an egress firewall for a project"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.13. Editing an egress firewall for a project"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egressnetworkpolicy-edit_editing-egress-firewall-ovn" class="j-doc-nav__link ">
    26.13.1. Editing an EgressFirewall object
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#removing-egress-firewall-ovn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.14. Removing an egress firewall from a project
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.14. Removing an egress firewall from a project"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.14. Removing an egress firewall from a project"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egressnetworkpolicy-delete_removing-egress-firewall-ovn" class="j-doc-nav__link ">
    26.14.1. Removing an EgressFirewall object
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-egress-ips-ovn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.15. Configuring an egress IP address
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.15. Configuring an egress IP address"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.15. Configuring an egress IP address"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-about_configuring-egress-ips-ovn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.15.1. Egress IP address architectural design and implementation
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.15.1. Egress IP address architectural design and implementation"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.15.1. Egress IP address architectural design and implementation"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-platform-support_configuring-egress-ips-ovn" class="j-doc-nav__link ">
    26.15.1.1. Platform support
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-public-cloud-platform-considerations_configuring-egress-ips-ovn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.15.1.2. Public cloud platform considerations
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.15.1.2. Public cloud platform considerations"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.15.1.2. Public cloud platform considerations"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-capacity-aws_configuring-egress-ips-ovn" class="j-doc-nav__link ">
    26.15.1.2.1. Amazon Web Services (AWS) IP address capacity limits
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-capacity-gcp_configuring-egress-ips-ovn" class="j-doc-nav__link ">
    26.15.1.2.2. Google Cloud Platform (GCP) IP address capacity limits
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-capacity-azure_configuring-egress-ips-ovn" class="j-doc-nav__link ">
    26.15.1.2.3. Microsoft Azure IP address capacity limits
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-considerations_configuring-egress-ips-ovn" class="j-doc-nav__link ">
    26.15.1.3. Assignment of egress IPs to pods
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-node-assignment_configuring-egress-ips-ovn" class="j-doc-nav__link ">
    26.15.1.4. Assignment of egress IPs to nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-node-architecture_configuring-egress-ips-ovn" class="j-doc-nav__link ">
    26.15.1.5. Architectural diagram of an egress IP address configuration
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-object_configuring-egress-ips-ovn" class="j-doc-nav__link ">
    26.15.2. EgressIP object
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-config-object_configuring-egress-ips-ovn" class="j-doc-nav__link ">
    26.15.3. EgressIPconfig object
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-node_configuring-egress-ips-ovn" class="j-doc-nav__link ">
    26.15.4. Labeling a node to host egress IP addresses
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-egress-ips-next-steps" class="j-doc-nav__link ">
    26.15.5. Next steps
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-egress-ips-additional-resources" class="j-doc-nav__link ">
    26.15.6. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#assigning-egress-ips-ovn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.16. Assigning an egress IP address
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.16. Assigning an egress IP address"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.16. Assigning an egress IP address"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-assign_assigning-egress-ips-ovn" class="j-doc-nav__link ">
    26.16.1. Assigning an egress IP address to a namespace
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#assigning-egress-ips-additional-resources" class="j-doc-nav__link ">
    26.16.2. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#using-an-egress-router-ovn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.17. Considerations for the use of an egress router pod
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.17. Considerations for the use of an egress router pod"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.17. Considerations for the use of an egress router pod"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-about_using-an-egress-router-ovn" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.17.1. About an egress router pod
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.17.1. About an egress router pod"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.17.1. About an egress router pod"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-about-modes_using-an-egress-router-ovn" class="j-doc-nav__link ">
    26.17.1.1. Egress router modes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-about-router-pod-implementation_using-an-egress-router-ovn" class="j-doc-nav__link ">
    26.17.1.2. Egress router pod implementation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-about-deployments_using-an-egress-router-ovn" class="j-doc-nav__link ">
    26.17.1.3. Deployment considerations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-about-failover_using-an-egress-router-ovn" class="j-doc-nav__link ">
    26.17.1.4. Failover configuration
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#using-an-egress-router-ovn-additional-resources" class="j-doc-nav__link ">
    26.17.2. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#deploying-egress-router-ovn-redirection" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.18. Deploying an egress router pod in redirect mode
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.18. Deploying an egress router pod in redirect mode"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.18. Deploying an egress router pod in redirect mode"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-ovn-cr_deploying-egress-router-ovn-redirection" class="j-doc-nav__link ">
    26.18.1. Egress router custom resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-redirect-mode-ovn_deploying-egress-router-ovn-redirection" class="j-doc-nav__link ">
    26.18.2. Deploying an egress router in redirect mode
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-enabling-multicast" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.19. Enabling multicast for a project
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.19. Enabling multicast for a project"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.19. Enabling multicast for a project"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-about-multicast_ovn-kubernetes-enabling-multicast" class="j-doc-nav__link ">
    26.19.1. About multicast
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-enabling-multicast_ovn-kubernetes-enabling-multicast" class="j-doc-nav__link ">
    26.19.2. Enabling multicast between pods
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-disabling-multicast" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.20. Disabling multicast for a project
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.20. Disabling multicast for a project"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.20. Disabling multicast for a project"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-disabling-multicast_ovn-kubernetes-disabling-multicast" class="j-doc-nav__link ">
    26.20.1. Disabling multicast between pods
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#tracking-network-flows" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.21. Tracking network flows
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.21. Tracking network flows"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.21. Tracking network flows"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-network-flows-object_tracking-network-flows" class="j-doc-nav__link ">
    26.21.1. Network object configuration for tracking network flows
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-network-flows-create_tracking-network-flows" class="j-doc-nav__link ">
    26.21.2. Adding destinations for network flows collectors
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-network-flows-delete_tracking-network-flows" class="j-doc-nav__link ">
    26.21.3. Deleting all destinations for network flows collectors
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources_tracking-network-flows" class="j-doc-nav__link ">
    26.21.4. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-hybrid-networking" class="j-doc-nav__link j-doc-nav__link--has-children">
    26.22. Configuring hybrid networking
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "26.22. Configuring hybrid networking"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "26.22. Configuring hybrid networking"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-hybrid-ovnkubernetes_configuring-hybrid-networking" class="j-doc-nav__link ">
    26.22.1. Configuring hybrid networking with OVN-Kubernetes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-hybrid-networking-additional-resources" class="j-doc-nav__link ">
    26.22.2. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#openshift-sdn-network-plugin" class="j-doc-nav__link j-doc-nav__link--has-children">
    27. OpenShift SDN network plugin
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27. OpenShift SDN network plugin"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27. OpenShift SDN network plugin"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#about-openshift-sdn" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.1. About the OpenShift SDN network plugin
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.1. About the OpenShift SDN network plugin"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.1. About the OpenShift SDN network plugin"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-openshift-sdn-modes_about-openshift-sdn" class="j-doc-nav__link ">
    27.1.1. OpenShift SDN network isolation modes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-matrix_about-openshift-sdn" class="j-doc-nav__link ">
    27.1.2. Supported network plugin feature matrix
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#migrate-to-openshift-sdn" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.2. Migrating to the OpenShift SDN network plugin
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.2. Migrating to the OpenShift SDN network plugin"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.2. Migrating to the OpenShift SDN network plugin"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#how-the-migration-process-works_migrate-to-openshift-sdn" class="j-doc-nav__link ">
    27.2.1. How the migration process works
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-rollback_migrate-to-openshift-sdn" class="j-doc-nav__link ">
    27.2.2. Migrating to the OpenShift SDN network plugin
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#migrate-to-openshift-sdn-additional-resources" class="j-doc-nav__link ">
    27.2.3. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#roll-back-to-ovn-kubernetes" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.3. Rolling back to the OVN-Kubernetes network plugin
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.3. Rolling back to the OVN-Kubernetes network plugin"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.3. Rolling back to the OVN-Kubernetes network plugin"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ovn-kubernetes-migration_roll-back-to-ovn-kubernetes" class="j-doc-nav__link ">
    27.3.1. Migrating to the OVN-Kubernetes network plugin
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#assigning-egress-ips" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.4. Configuring egress IPs for a project
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.4. Configuring egress IPs for a project"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.4. Configuring egress IPs for a project"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-about_egress-ips" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.4.1. Egress IP address architectural design and implementation
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.4.1. Egress IP address architectural design and implementation"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.4.1. Egress IP address architectural design and implementation"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-platform-support_egress-ips" class="j-doc-nav__link ">
    27.4.1.1. Platform support
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-public-cloud-platform-considerations_egress-ips" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.4.1.2. Public cloud platform considerations
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.4.1.2. Public cloud platform considerations"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.4.1.2. Public cloud platform considerations"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-capacity-aws_egress-ips" class="j-doc-nav__link ">
    27.4.1.2.1. Amazon Web Services (AWS) IP address capacity limits
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-capacity-gcp_egress-ips" class="j-doc-nav__link ">
    27.4.1.2.2. Google Cloud Platform (GCP) IP address capacity limits
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-capacity-azure_egress-ips" class="j-doc-nav__link ">
    27.4.1.2.3. Microsoft Azure IP address capacity limits
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-limitations_egress-ips" class="j-doc-nav__link ">
    27.4.1.3. Limitations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#automatic-manual-assignment-approaches" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.4.1.4. IP address assignment approaches
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.4.1.4. IP address assignment approaches"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.4.1.4. IP address assignment approaches"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#considerations-automatic-egress-ips" class="j-doc-nav__link ">
    27.4.1.4.1. Considerations when using automatically assigned egress IP addresses
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#considerations-manual-egress-ips" class="j-doc-nav__link ">
    27.4.1.4.2. Considerations when using manually assigned egress IP addresses
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-automatic_egress-ips" class="j-doc-nav__link ">
    27.4.2. Configuring automatically assigned egress IP addresses for a namespace
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-ips-static_egress-ips" class="j-doc-nav__link ">
    27.4.3. Configuring manually assigned egress IP addresses for a namespace
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#egress-ips-additional-resources" class="j-doc-nav__link ">
    27.4.4. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-egress-firewall" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.5. Configuring an egress firewall for a project
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.5. Configuring an egress firewall for a project"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.5. Configuring an egress firewall for a project"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egressnetworkpolicy-about_openshift-sdn-egress-firewall" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.5.1. How an egress firewall works in a project
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.5.1. How an egress firewall works in a project"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.5.1. How an egress firewall works in a project"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#limitations-of-an-egress-firewall_openshift-sdn-egress-firewall" class="j-doc-nav__link ">
    27.5.1.1. Limitations of an egress firewall
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#policy-rule-order_openshift-sdn-egress-firewall" class="j-doc-nav__link ">
    27.5.1.2. Matching order for egress firewall policy rules
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#domain-name-server-resolution_openshift-sdn-egress-firewall" class="j-doc-nav__link ">
    27.5.1.3. How Domain Name Server (DNS) resolution works
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egressnetworkpolicy-object_openshift-sdn-egress-firewall" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.5.2. EgressNetworkPolicy custom resource (CR) object
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.5.2. EgressNetworkPolicy custom resource (CR) object"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.5.2. EgressNetworkPolicy custom resource (CR) object"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#egressnetworkpolicy-rules_openshift-sdn-egress-firewall" class="j-doc-nav__link ">
    27.5.2.1. EgressNetworkPolicy rules
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#egressnetworkpolicy-example_openshift-sdn-egress-firewall" class="j-doc-nav__link ">
    27.5.2.2. Example EgressNetworkPolicy CR objects
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-networkpolicy-create_openshift-sdn-egress-firewall" class="j-doc-nav__link ">
    27.5.3. Creating an egress firewall policy object
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#openshift-sdn-viewing-egress-firewall" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.6. Editing an egress firewall for a project
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.6. Editing an egress firewall for a project"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.6. Editing an egress firewall for a project"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egressnetworkpolicy-view_openshift-sdn-viewing-egress-firewall" class="j-doc-nav__link ">
    27.6.1. Viewing an EgressNetworkPolicy object
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#editing-egress-firewall" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.7. Editing an egress firewall for a project
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.7. Editing an egress firewall for a project"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.7. Editing an egress firewall for a project"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egressnetworkpolicy-edit_openshift-sdn-egress-firewall" class="j-doc-nav__link ">
    27.7.1. Editing an EgressNetworkPolicy object
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#removing-egress-firewall" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.8. Removing an egress firewall from a project
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.8. Removing an egress firewall from a project"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.8. Removing an egress firewall from a project"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egressnetworkpolicy-delete_openshift-sdn-egress-firewall" class="j-doc-nav__link ">
    27.8.1. Removing an EgressNetworkPolicy object
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#using-an-egress-router" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.9. Considerations for the use of an egress router pod
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.9. Considerations for the use of an egress router pod"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.9. Considerations for the use of an egress router pod"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-about_using-an-egress-router" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.9.1. About an egress router pod
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.9.1. About an egress router pod"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.9.1. About an egress router pod"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-about-modes_using-an-egress-router" class="j-doc-nav__link ">
    27.9.1.1. Egress router modes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-about-router-pod-implementation_using-an-egress-router" class="j-doc-nav__link ">
    27.9.1.2. Egress router pod implementation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-about-deployments_using-an-egress-router" class="j-doc-nav__link ">
    27.9.1.3. Deployment considerations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-about-failover_using-an-egress-router" class="j-doc-nav__link ">
    27.9.1.4. Failover configuration
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#using-an-egress-router-additional-resources" class="j-doc-nav__link ">
    27.9.2. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#deploying-egress-router-layer3-redirection" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.10. Deploying an egress router pod in redirect mode
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.10. Deploying an egress router pod in redirect mode"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.10. Deploying an egress router pod in redirect mode"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-pod_deploying-egress-router-layer3-redirection" class="j-doc-nav__link ">
    27.10.1. Egress router pod specification for redirect mode
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-dest-var_deploying-egress-router-layer3-redirection" class="j-doc-nav__link ">
    27.10.2. Egress destination configuration format
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-redirect-mode_deploying-egress-router-layer3-redirection" class="j-doc-nav__link ">
    27.10.3. Deploying an egress router pod in redirect mode
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#deploying-egress-router-layer3-redirection-additional-resources" class="j-doc-nav__link ">
    27.10.4. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#deploying-egress-router-http-redirection" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.11. Deploying an egress router pod in HTTP proxy mode
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.11. Deploying an egress router pod in HTTP proxy mode"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.11. Deploying an egress router pod in HTTP proxy mode"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-pod_deploying-egress-router-http-redirection" class="j-doc-nav__link ">
    27.11.1. Egress router pod specification for HTTP mode
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-dest-var_deploying-egress-router-http-redirection" class="j-doc-nav__link ">
    27.11.2. Egress destination configuration format
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-http-proxy-mode_deploying-egress-router-http-redirection" class="j-doc-nav__link ">
    27.11.3. Deploying an egress router pod in HTTP proxy mode
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#deploying-egress-router-http-redirection-additional-resources" class="j-doc-nav__link ">
    27.11.4. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#deploying-egress-router-dns-redirection" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.12. Deploying an egress router pod in DNS proxy mode
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.12. Deploying an egress router pod in DNS proxy mode"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.12. Deploying an egress router pod in DNS proxy mode"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-pod_deploying-egress-router-dns-redirection" class="j-doc-nav__link ">
    27.12.1. Egress router pod specification for DNS mode
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-dest-var_deploying-egress-router-dns-redirection" class="j-doc-nav__link ">
    27.12.2. Egress destination configuration format
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-egress-router-dns-mode_deploying-egress-router-dns-redirection" class="j-doc-nav__link ">
    27.12.3. Deploying an egress router pod in DNS proxy mode
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#deploying-egress-router-dns-redirection-additional-resources" class="j-doc-nav__link ">
    27.12.4. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-egress-router-configmap" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.13. Configuring an egress router pod destination list from a config map
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.13. Configuring an egress router pod destination list from a config map"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.13. Configuring an egress router pod destination list from a config map"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-egress-router-configmap_configuring-egress-router-configmap" class="j-doc-nav__link ">
    27.13.1. Configuring an egress router destination mappings with a config map
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-egress-router-configmap-additional-resources" class="j-doc-nav__link ">
    27.13.2. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#enabling-multicast" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.14. Enabling multicast for a project
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.14. Enabling multicast for a project"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.14. Enabling multicast for a project"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-about-multicast_openshift-sdn-enabling-multicast" class="j-doc-nav__link ">
    27.14.1. About multicast
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-enabling-multicast_openshift-sdn-enabling-multicast" class="j-doc-nav__link ">
    27.14.2. Enabling multicast between pods
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#disabling-multicast" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.15. Disabling multicast for a project
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.15. Disabling multicast for a project"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.15. Disabling multicast for a project"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-disabling-multicast_openshift-sdn-disabling-multicast" class="j-doc-nav__link ">
    27.15.1. Disabling multicast between pods
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-multitenant-isolation" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.16. Configuring network isolation using OpenShift SDN
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.16. Configuring network isolation using OpenShift SDN"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.16. Configuring network isolation using OpenShift SDN"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#prerequisites" class="j-doc-nav__link ">
    27.16.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multitenant-joining_multitenant-isolation" class="j-doc-nav__link ">
    27.16.2. Joining projects
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multitenant-isolation_multitenant-isolation" class="j-doc-nav__link ">
    27.16.3. Isolating a project
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-multitenant-global_multitenant-isolation" class="j-doc-nav__link ">
    27.16.4. Disabling network isolation for a project
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-kube-proxy" class="j-doc-nav__link j-doc-nav__link--has-children">
    27.17. Configuring kube-proxy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "27.17. Configuring kube-proxy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "27.17. Configuring kube-proxy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-kube-proxy-sync_configuring-kube-proxy" class="j-doc-nav__link ">
    27.17.1. About iptables rules synchronization
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-kube-proxy-config_configuring-kube-proxy" class="j-doc-nav__link ">
    27.17.2. kube-proxy configuration parameters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-kube-proxy-configuring_configuring-kube-proxy" class="j-doc-nav__link ">
    27.17.3. Modifying the kube-proxy configuration
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-routes" class="j-doc-nav__link j-doc-nav__link--has-children">
    28. Configuring Routes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "28. Configuring Routes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "28. Configuring Routes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#route-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    28.1. Route configuration
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "28.1. Route configuration"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "28.1. Route configuration"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-creating-a-route_route-configuration" class="j-doc-nav__link ">
    28.1.1. Creating an HTTP-based route
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-sharding-route-configuration_route-configuration" class="j-doc-nav__link ">
    28.1.2. Creating a route for Ingress Controller sharding
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-configuring-route-timeouts_route-configuration" class="j-doc-nav__link ">
    28.1.3. Configuring route timeouts
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-enabling-hsts_route-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    28.1.4. HTTP Strict Transport Security
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "28.1.4. HTTP Strict Transport Security"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "28.1.4. HTTP Strict Transport Security"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-enabling-hsts-per-route_route-configuration" class="j-doc-nav__link ">
    28.1.4.1. Enabling HTTP Strict Transport Security per-route
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-disabling-hsts_route-configuration" class="j-doc-nav__link ">
    28.1.4.2. Disabling HTTP Strict Transport Security per-route
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-enforcing-hsts-per-domain_route-configuration" class="j-doc-nav__link ">
    28.1.4.3. Enforcing HTTP Strict Transport Security per-domain
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-throughput-troubleshoot_route-configuration" class="j-doc-nav__link ">
    28.1.5. Throughput issue troubleshooting methods
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-using-cookies-keep-route-statefulness_route-configuration" class="j-doc-nav__link j-doc-nav__link--has-children">
    28.1.6. Using cookies to keep route statefulness
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "28.1.6. Using cookies to keep route statefulness"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "28.1.6. Using cookies to keep route statefulness"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-annotating-a-route-with-a-cookie-name_route-configuration" class="j-doc-nav__link ">
    28.1.6.1. Annotating a route with a cookie
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-path-based-routes_route-configuration" class="j-doc-nav__link ">
    28.1.7. Path-based routes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-route-specific-annotations_route-configuration" class="j-doc-nav__link ">
    28.1.8. Route-specific annotations
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-route-admission-policy_route-configuration" class="j-doc-nav__link ">
    28.1.9. Configuring the route admission policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-creating-a-route-via-an-ingress_route-configuration" class="j-doc-nav__link ">
    28.1.10. Creating a route through an Ingress object
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#creating-edge-route-with-default-certificate_route-configuration" class="j-doc-nav__link ">
    28.1.11. Creating a route using the default certificate through an Ingress object
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#creating-re-encrypt-route-with-custom-certificate_route-configuration" class="j-doc-nav__link ">
    28.1.12. Creating a route using the destination CA certificate in the Ingress annotation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-router-configuring-dual-stack_route-configuration" class="j-doc-nav__link ">
    28.1.13. Configuring the OpenShift Container Platform Ingress Controller for dual-stack networking
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-default-certificate" class="j-doc-nav__link j-doc-nav__link--has-children">
    28.2. Secured routes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "28.2. Secured routes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "28.2. Secured routes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-creating-a-reencrypt-route-with-a-custom-certificate_secured-routes" class="j-doc-nav__link ">
    28.2.1. Creating a re-encrypt route with a custom certificate
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-creating-an-edge-route-with-a-custom-certificate_secured-routes" class="j-doc-nav__link ">
    28.2.2. Creating an edge route with a custom certificate
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-creating-a-passthrough-route_secured-routes" class="j-doc-nav__link ">
    28.2.3. Creating a passthrough route
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ingress-cluster-traffic" class="j-doc-nav__link j-doc-nav__link--has-children">
    29. Configuring ingress cluster traffic
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "29. Configuring ingress cluster traffic"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "29. Configuring ingress cluster traffic"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#overview-traffic" class="j-doc-nav__link j-doc-nav__link--has-children">
    29.1. Configuring ingress cluster traffic overview
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "29.1. Configuring ingress cluster traffic overview"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "29.1. Configuring ingress cluster traffic overview"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#overview-traffic-comparision_overview-traffic" class="j-doc-nav__link ">
    29.1.1. Comparision: Fault tolerant access to external IP addresses
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-externalip" class="j-doc-nav__link j-doc-nav__link--has-children">
    29.2. Configuring ExternalIPs for services
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "29.2. Configuring ExternalIPs for services"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "29.2. Configuring ExternalIPs for services"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#prerequisites-2" class="j-doc-nav__link ">
    29.2.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-externalip-about_configuring-externalip" class="j-doc-nav__link j-doc-nav__link--has-children">
    29.2.2. About ExternalIP
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "29.2.2. About ExternalIP"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "29.2.2. About ExternalIP"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuration-externalip_configuring-externalip" class="j-doc-nav__link ">
    29.2.2.1. Configuration for ExternalIP
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#restrictions-on-ip-assignment_configuring-externalip" class="j-doc-nav__link ">
    29.2.2.2. Restrictions on the assignment of an external IP address
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#example-policy-objects_configuring-externalip" class="j-doc-nav__link ">
    29.2.2.3. Example policy objects
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-externalip-object_configuring-externalip" class="j-doc-nav__link ">
    29.2.3. ExternalIP address block configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-externalip-configuring_configuring-externalip" class="j-doc-nav__link ">
    29.2.4. Configure external IP address blocks for your cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-externalip-next-steps" class="j-doc-nav__link ">
    29.2.5. Next steps
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ingress-cluster-traffic-ingress-controller" class="j-doc-nav__link j-doc-nav__link--has-children">
    29.3. Configuring ingress cluster traffic using an Ingress Controller
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "29.3. Configuring ingress cluster traffic using an Ingress Controller"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "29.3. Configuring ingress cluster traffic using an Ingress Controller"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-using-ingress-and-routes_configuring-ingress-cluster-traffic-ingress-controller" class="j-doc-nav__link ">
    29.3.1. Using Ingress Controllers and routes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#prerequisites-3" class="j-doc-nav__link ">
    29.3.2. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-creating-project-and-service_configuring-ingress-cluster-traffic-ingress-controller" class="j-doc-nav__link ">
    29.3.3. Creating a project and service
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-exposing-service_configuring-ingress-cluster-traffic-ingress-controller" class="j-doc-nav__link ">
    29.3.4. Exposing the service by creating a route
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-sharding-route-labels_configuring-ingress-cluster-traffic-ingress-controller" class="j-doc-nav__link ">
    29.3.5. Configuring Ingress Controller sharding by using route labels
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-sharding-namespace-labels_configuring-ingress-cluster-traffic-ingress-controller" class="j-doc-nav__link ">
    29.3.6. Configuring Ingress Controller sharding by using namespace labels
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-ingress-sharding-route-configuration_configuring-ingress-cluster-traffic-ingress-controller" class="j-doc-nav__link ">
    29.3.7. Creating a route for Ingress Controller sharding
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources-5" class="j-doc-nav__link ">
    29.3.8. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ingress-cluster-traffic-load-balancer" class="j-doc-nav__link j-doc-nav__link--has-children">
    29.4. Configuring ingress cluster traffic using a load balancer
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "29.4. Configuring ingress cluster traffic using a load balancer"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "29.4. Configuring ingress cluster traffic using a load balancer"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-using-load-balancer-getting-traffic_configuring-ingress-cluster-traffic-load-balancer" class="j-doc-nav__link ">
    29.4.1. Using a load balancer to get traffic into the cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#prerequisites-4" class="j-doc-nav__link ">
    29.4.2. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-creating-project-and-service_configuring-ingress-cluster-traffic-load-balancer" class="j-doc-nav__link ">
    29.4.3. Creating a project and service
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-exposing-service_configuring-ingress-cluster-traffic-load-balancer" class="j-doc-nav__link ">
    29.4.4. Exposing the service by creating a route
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-create-load-balancer-service_configuring-ingress-cluster-traffic-load-balancer" class="j-doc-nav__link ">
    29.4.5. Creating a load balancer service
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ingress-cluster-traffic-aws" class="j-doc-nav__link j-doc-nav__link--has-children">
    29.5. Configuring ingress cluster traffic on AWS
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "29.5. Configuring ingress cluster traffic on AWS"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "29.5. Configuring ingress cluster traffic on AWS"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-configuring-elb-timeouts-aws-classic_configuring-ingress-cluster-traffic-aws" class="j-doc-nav__link j-doc-nav__link--has-children">
    29.5.1. Configuring Classic Load Balancer timeouts on AWS
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "29.5.1. Configuring Classic Load Balancer timeouts on AWS"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "29.5.1. Configuring Classic Load Balancer timeouts on AWS"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-configuring-route-timeouts_configuring-ingress-cluster-traffic-aws" class="j-doc-nav__link ">
    29.5.1.1. Configuring route timeouts
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-configuring-clb-timeouts_configuring-ingress-cluster-traffic-aws" class="j-doc-nav__link ">
    29.5.1.2. Configuring Classic Load Balancer timeouts
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-configuring-ingress-cluster-traffic-aws-network-load-balancer_configuring-ingress-cluster-traffic-aws" class="j-doc-nav__link j-doc-nav__link--has-children">
    29.5.2. Configuring ingress cluster traffic on AWS using a Network Load Balancer
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "29.5.2. Configuring ingress cluster traffic on AWS using a Network Load Balancer"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "29.5.2. Configuring ingress cluster traffic on AWS using a Network Load Balancer"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-aws-switching-clb-with-nlb_configuring-ingress-cluster-traffic-aws" class="j-doc-nav__link ">
    29.5.2.1. Switching the Ingress Controller from using a Classic Load Balancer to a Network Load Balancer
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-aws-switching-nlb-with-clb_configuring-ingress-cluster-traffic-aws" class="j-doc-nav__link ">
    29.5.2.2. Switching the Ingress Controller from using a Network Load Balancer to a Classic Load Balancer
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-aws-replacing-clb-with-nlb_configuring-ingress-cluster-traffic-aws" class="j-doc-nav__link ">
    29.5.2.3. Replacing Ingress Controller Classic Load Balancer with Network Load Balancer
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-aws-nlb-existing-cluster_configuring-ingress-cluster-traffic-aws" class="j-doc-nav__link ">
    29.5.2.4. Configuring an Ingress Controller Network Load Balancer on an existing AWS cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-aws-nlb-new-cluster_configuring-ingress-cluster-traffic-aws" class="j-doc-nav__link ">
    29.5.2.5. Configuring an Ingress Controller Network Load Balancer on a new AWS cluster
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources_configuring-ingress-cluster-traffic-aws" class="j-doc-nav__link ">
    29.5.3. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ingress-cluster-traffic-service-external-ip" class="j-doc-nav__link j-doc-nav__link--has-children">
    29.6. Configuring ingress cluster traffic for a service external IP
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "29.6. Configuring ingress cluster traffic for a service external IP"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "29.6. Configuring ingress cluster traffic for a service external IP"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ingress-cluster-traffic-service-external-ip-prerequisites" class="j-doc-nav__link ">
    29.6.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-service-externalip-create_configuring-ingress-cluster-traffic-service-external-ip" class="j-doc-nav__link ">
    29.6.2. Attaching an ExternalIP to a service
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ingress-cluster-traffic-service-external-ip-additional-resources" class="j-doc-nav__link ">
    29.6.3. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ingress-cluster-traffic-nodeport" class="j-doc-nav__link j-doc-nav__link--has-children">
    29.7. Configuring ingress cluster traffic using a NodePort
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "29.7. Configuring ingress cluster traffic using a NodePort"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "29.7. Configuring ingress cluster traffic using a NodePort"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-using-nodeport_configuring-ingress-cluster-traffic-nodeport" class="j-doc-nav__link ">
    29.7.1. Using a NodePort to get traffic into the cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#prerequisites-5" class="j-doc-nav__link ">
    29.7.2. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-creating-project-and-service_configuring-ingress-cluster-traffic-nodeport" class="j-doc-nav__link ">
    29.7.3. Creating a project and service
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-exposing-service_configuring-ingress-cluster-traffic-nodeport" class="j-doc-nav__link ">
    29.7.4. Exposing the service by creating a route
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ingress-cluster-traffic-nodeport-additional-resources" class="j-doc-nav__link ">
    29.7.5. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-ingress-cluster-traffic-lb-allowed-source-ranges" class="j-doc-nav__link j-doc-nav__link--has-children">
    29.8. Configuring ingress cluster traffic using load balancer allowed source ranges
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "29.8. Configuring ingress cluster traffic using load balancer allowed source ranges"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "29.8. Configuring ingress cluster traffic using load balancer allowed source ranges"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-configuring-lb-allowed-source-ranges_configuring-ingress-cluster-traffic-lb-allowed-source-ranges" class="j-doc-nav__link ">
    29.8.1. Configuring load balancer allowed source ranges
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-configuring-lb-allowed-source-ranges-migration_configuring-ingress-cluster-traffic-lb-allowed-source-ranges" class="j-doc-nav__link ">
    29.8.2. Migrating to load balancer allowed source ranges
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources-6" class="j-doc-nav__link ">
    29.8.3. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#kubernetes-nmstate" class="j-doc-nav__link j-doc-nav__link--has-children">
    30. Kubernetes NMState
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "30. Kubernetes NMState"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "30. Kubernetes NMState"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#k8s-nmstate-about-the-k8s-nmstate-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    30.1. About the Kubernetes NMState Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "30.1. About the Kubernetes NMState Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "30.1. About the Kubernetes NMState Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#installing-the-kubernetes-nmstate-operator-cli" class="j-doc-nav__link j-doc-nav__link--has-children">
    30.1.1. Installing the Kubernetes NMState Operator
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "30.1.1. Installing the Kubernetes NMState Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "30.1.1. Installing the Kubernetes NMState Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#installing-the-kubernetes-nmstate-operator-web-console_k8s-nmstate-operator" class="j-doc-nav__link ">
    30.1.1.1. Installing the Kubernetes NMState Operator using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#installing-the-kubernetes-nmstate-operator-CLI_k8s-nmstate-operator" class="j-doc-nav__link ">
    30.1.1.2. Installing the Kubernetes NMState Operator using the CLI
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#k8s-nmstate-observing-node-network-state" class="j-doc-nav__link j-doc-nav__link--has-children">
    30.2. Observing node network state
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "30.2. Observing node network state"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "30.2. Observing node network state"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-about-nmstate_k8s-nmstate-observing-node-network-state" class="j-doc-nav__link ">
    30.2.1. About nmstate
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-viewing-network-state-of-node_k8s-nmstate-observing-node-network-state" class="j-doc-nav__link ">
    30.2.2. Viewing the network state of a node
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#k8s-nmstate-updating-node-network-config" class="j-doc-nav__link j-doc-nav__link--has-children">
    30.3. Updating node network configuration
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "30.3. Updating node network configuration"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "30.3. Updating node network configuration"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-about-nmstate_k8s_nmstate-updating-node-network-config" class="j-doc-nav__link ">
    30.3.1. About nmstate
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-creating-interface-on-nodes_k8s_nmstate-updating-node-network-config" class="j-doc-nav__link ">
    30.3.2. Creating an interface on nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-confirming-policy-updates-on-nodes_k8s_nmstate-updating-node-network-config" class="j-doc-nav__link ">
    30.3.3. Confirming node network policy updates on nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-removing-interface-from-nodes_k8s_nmstate-updating-node-network-config" class="j-doc-nav__link ">
    30.3.4. Removing an interface from nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-nmstate-example-policy-configurations" class="j-doc-nav__link j-doc-nav__link--has-children">
    30.3.5. Example policy configurations for different interfaces
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "30.3.5. Example policy configurations for different interfaces"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "30.3.5. Example policy configurations for different interfaces"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-example-bridge-nncp_k8s_nmstate-updating-node-network-config" class="j-doc-nav__link ">
    30.3.5.1. Example: Linux bridge interface node network configuration policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-example-vlan-nncp_k8s_nmstate-updating-node-network-config" class="j-doc-nav__link ">
    30.3.5.2. Example: VLAN interface node network configuration policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-example-bond-nncp_k8s_nmstate-updating-node-network-config" class="j-doc-nav__link ">
    30.3.5.3. Example: Bond interface node network configuration policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-example-ethernet-nncp_k8s_nmstate-updating-node-network-config" class="j-doc-nav__link ">
    30.3.5.4. Example: Ethernet interface node network configuration policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-example-nmstate-multiple-interfaces_k8s_nmstate-updating-node-network-config" class="j-doc-nav__link ">
    30.3.5.5. Example: Multiple interfaces in the same node network configuration policy
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#capturing-nic-static-ip_k8s-nmstate-updating-node-network-config" class="j-doc-nav__link j-doc-nav__link--has-children">
    30.3.6. Capturing the static IP of a NIC attached to a bridge
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "30.3.6. Capturing the static IP of a NIC attached to a bridge"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "30.3.6. Capturing the static IP of a NIC attached to a bridge"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-example-inherit-static-ip-from-nic_k8s_nmstate-updating-node-network-config" class="j-doc-nav__link ">
    30.3.6.1. Example: Linux bridge interface node network configuration policy to inherit static IP address from the NIC attached to the bridge
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-example-nmstate-IP-management_k8s_nmstate-updating-node-network-config" class="j-doc-nav__link j-doc-nav__link--has-children">
    30.3.7. Examples: IP management
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "30.3.7. Examples: IP management"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "30.3.7. Examples: IP management"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-example-nmstate-IP-management-static_k8s_nmstate-updating-node-network-config" class="j-doc-nav__link ">
    30.3.7.1. Static
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-example-nmstate-IP-management-no-ip_k8s_nmstate-updating-node-network-config" class="j-doc-nav__link ">
    30.3.7.2. No IP address
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-example-nmstate-IP-management-dhcp_k8s_nmstate-updating-node-network-config" class="j-doc-nav__link ">
    30.3.7.3. Dynamic host configuration
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-example-nmstate-IP-management-dns_k8s_nmstate-updating-node-network-config" class="j-doc-nav__link ">
    30.3.7.4. DNS
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-example-nmstate-IP-management-static-routing_k8s_nmstate-updating-node-network-config" class="j-doc-nav__link ">
    30.3.7.5. Static routing
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#k8s-nmstate-troubleshooting-node-network" class="j-doc-nav__link j-doc-nav__link--has-children">
    30.4. Troubleshooting node network configuration
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "30.4. Troubleshooting node network configuration"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "30.4. Troubleshooting node network configuration"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#virt-troubleshooting-incorrect-policy-config_k8s-nmstate-troubleshooting-node-network" class="j-doc-nav__link ">
    30.4.1. Troubleshooting an incorrect node network configuration policy configuration
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#enable-cluster-wide-proxy" class="j-doc-nav__link j-doc-nav__link--has-children">
    31. Configuring the cluster-wide proxy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "31. Configuring the cluster-wide proxy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "31. Configuring the cluster-wide proxy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#prerequisites-6" class="j-doc-nav__link ">
    31.1. Prerequisites
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-proxy-configure-object_config-cluster-wide-proxy" class="j-doc-nav__link ">
    31.2. Enabling the cluster-wide proxy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-proxy-remove_config-cluster-wide-proxy" class="j-doc-nav__link ">
    31.3. Removing the cluster-wide proxy
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-a-custom-pki" class="j-doc-nav__link j-doc-nav__link--has-children">
    32. Configuring a custom PKI
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "32. Configuring a custom PKI"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "32. Configuring a custom PKI"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#installation-configure-proxy_configuring-a-custom-pki" class="j-doc-nav__link ">
    32.1. Configuring the cluster-wide proxy during installation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-proxy-configure-object_configuring-a-custom-pki" class="j-doc-nav__link ">
    32.2. Enabling the cluster-wide proxy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#certificate-injection-using-operators_configuring-a-custom-pki" class="j-doc-nav__link ">
    32.3. Certificate injection using Operators
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#load-balancing-openstack" class="j-doc-nav__link j-doc-nav__link--has-children">
    33. Load balancing on RHOSP
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "33. Load balancing on RHOSP"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "33. Load balancing on RHOSP"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-osp-loadbalancer-limitations_load-balancing-openstack" class="j-doc-nav__link j-doc-nav__link--has-children">
    33.1. Limitations of load balancer services
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "33.1. Limitations of load balancer services"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "33.1. Limitations of load balancer services"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-osp-loadbalancer-etp-local_load-balancing-openstack" class="j-doc-nav__link ">
    33.1.1. Local external traffic policies
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-osp-loadbalancer-source-ranges_load-balancing-openstack" class="j-doc-nav__link ">
    33.1.2. Load balancer source ranges
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#installation-osp-kuryr-octavia-configure_load-balancing-openstack" class="j-doc-nav__link ">
    33.2. Using the Octavia OVN load balancer provider driver with Kuryr SDN
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#installation-osp-api-octavia_load-balancing-openstack" class="j-doc-nav__link j-doc-nav__link--has-children">
    33.3. Scaling clusters for application traffic by using Octavia
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "33.3. Scaling clusters for application traffic by using Octavia"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "33.3. Scaling clusters for application traffic by using Octavia"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#installation-osp-api-scaling_load-balancing-openstack" class="j-doc-nav__link ">
    33.3.1. Scaling clusters by using Octavia
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#installation-osp-kuryr-api-scaling_load-balancing-openstack" class="j-doc-nav__link ">
    33.3.2. Scaling clusters that use Kuryr by using Octavia
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#installation-osp-kuryr-octavia-scale_load-balancing-openstack" class="j-doc-nav__link ">
    33.4. Scaling for ingress traffic by using RHOSP Octavia
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-osp-configuring-external-load-balancer_load-balancing-openstack" class="j-doc-nav__link ">
    33.5. Configuring an external load balancer
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#load-balancing-with-metallb" class="j-doc-nav__link j-doc-nav__link--has-children">
    34. Load balancing with MetalLB
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34. Load balancing with MetalLB"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34. Load balancing with MetalLB"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#about-metallb" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.1. About MetalLB and the MetalLB Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.1. About MetalLB and the MetalLB Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.1. About MetalLB and the MetalLB Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-when-metallb_about-metallb-and-metallb-operator" class="j-doc-nav__link ">
    34.1.1. When to use MetalLB
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-operator-custom-resources_about-metallb-and-metallb-operator" class="j-doc-nav__link ">
    34.1.2. MetalLB Operator custom resources
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-software-components_about-metallb-and-metallb-operator" class="j-doc-nav__link ">
    34.1.3. MetalLB software components
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-extern-traffic-pol_about-metallb-and-metallb-operator" class="j-doc-nav__link ">
    34.1.4. MetalLB and external traffic policy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-layer2_about-metallb-and-metallb-operator" class="j-doc-nav__link ">
    34.1.5. MetalLB concepts for layer 2 mode
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-bgp_about-metallb-and-metallb-operator" class="j-doc-nav__link ">
    34.1.6. MetalLB concepts for BGP mode
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#limitations-and-restrictions_about-metallb-and-metallb-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.1.7. Limitations and restrictions
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.1.7. Limitations and restrictions"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.1.7. Limitations and restrictions"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-infra-considerations_about-metallb-and-metallb-operator" class="j-doc-nav__link ">
    34.1.7.1. Infrastructure considerations for MetalLB
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-layer2-limitations_about-metallb-and-metallb-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.1.7.2. Limitations for layer 2 mode
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.1.7.2. Limitations for layer 2 mode"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.1.7.2. Limitations for layer 2 mode"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-layer2-limitations-bottleneck_about-metallb-and-metallb-operator" class="j-doc-nav__link ">
    34.1.7.2.1. Single-node bottleneck
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-layer2-limitations-failover_about-metallb-and-metallb-operator" class="j-doc-nav__link ">
    34.1.7.2.2. Slow failover performance
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-bgp-limitations_about-metallb-and-metallb-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.1.7.3. Limitations for BGP mode
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.1.7.3. Limitations for BGP mode"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.1.7.3. Limitations for BGP mode"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-bgp-limitations-break-connections_about-metallb-and-metallb-operator" class="j-doc-nav__link ">
    34.1.7.3.1. Node failure can break all active connections
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-bgp-limitations-single-asn_about-metallb-and-metallb-operator" class="j-doc-nav__link ">
    34.1.7.3.2. Support for a single ASN and a single router ID only
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources_about-metallb-and-metallb-operator" class="j-doc-nav__link ">
    34.1.8. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#metallb-operator-install" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.2. Installing the MetalLB Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.2. Installing the MetalLB Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.2. Installing the MetalLB Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#installing-the-metallb-operator-using-web-console_metallb-operator-install" class="j-doc-nav__link ">
    34.2.1. Installing the MetalLB Operator from the OperatorHub using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-installing-operator-cli_metallb-operator-install" class="j-doc-nav__link ">
    34.2.2. Installing from OperatorHub using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-operator-initial-config_metallb-operator-install" class="j-doc-nav__link ">
    34.2.3. Starting MetalLB on your cluster
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-operator-deployment-specifications-for-metallb_metallb-operator-install" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.2.4. Deployment specifications for MetalLB
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.2.4. Deployment specifications for MetalLB"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.2.4. Deployment specifications for MetalLB"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-operator-limit-speaker-to-nodes_metallb-operator-install" class="j-doc-nav__link ">
    34.2.4.1. Limit speaker pods to specific nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-operator-setting-runtimeclass_metallb-operator-install" class="j-doc-nav__link ">
    34.2.4.2. Configuring a container runtime class in a MetalLB deployment
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-operator-setting-pod-priority-affinity_metallb-operator-install" class="j-doc-nav__link ">
    34.2.4.3. Configuring pod priority and pod affinity in a MetalLB deployment
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-operator-setting-pod-CPU-limits_metallb-operator-install" class="j-doc-nav__link ">
    34.2.4.4. Configuring pod CPU limits in a MetalLB deployment
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources_metallb-operator-install" class="j-doc-nav__link ">
    34.2.5. Additional resources
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#next-steps_metallb-operator-install" class="j-doc-nav__link ">
    34.2.6. Next steps
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#metallb-upgrading-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.3. Upgrading the MetalLB
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.3. Upgrading the MetalLB"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.3. Upgrading the MetalLB"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#olm-deleting-metallb-operator-from-a-cluster-using-web-console_metallb-upgrading-operator" class="j-doc-nav__link ">
    34.3.1. Deleting the MetalLB Operator from a cluster using the web console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#olm-deleting-metallb-operator-from-a-cluster-using-cli_metallb-upgrading-operator" class="j-doc-nav__link ">
    34.3.2. Deleting MetalLB Operator from a cluster using the CLI
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#upgrading-metallb-operator_metallb-upgrading-operator" class="j-doc-nav__link ">
    34.3.3. Upgrading the MetalLB Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources" class="j-doc-nav__link ">
    34.3.4. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#metallb-configure-address-pools" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.4. Configuring MetalLB address pools
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.4. Configuring MetalLB address pools"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.4. Configuring MetalLB address pools"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-ipaddresspool-cr_configure-metallb-address-pools" class="j-doc-nav__link ">
    34.4.1. About the IPAddressPool custom resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-configure-address-pool_configure-metallb-address-pools" class="j-doc-nav__link ">
    34.4.2. Configuring an address pool
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-example-addresspool_configure-metallb-address-pools" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.4.3. Example address pool configurations
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.4.3. Example address pool configurations"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.4.3. Example address pool configurations"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#example-ipv4-and-cidr-ranges" class="j-doc-nav__link ">
    34.4.3.1. Example: IPv4 and CIDR ranges
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#example-reserve-ip-addresses" class="j-doc-nav__link ">
    34.4.3.2. Example: Reserve IP addresses
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#example-ipv4-and-ipv6-addresses" class="j-doc-nav__link ">
    34.4.3.3. Example: IPv4 and IPv6 addresses
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#example-assign-ip-address-pools-to-services-or-namespaces" class="j-doc-nav__link ">
    34.4.3.4. Example: Assign IP address pools to services or namespaces
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources_metallb-configure-address-pools" class="j-doc-nav__link ">
    34.4.4. Additional resources
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#next-steps_configure-metallb-address-pools" class="j-doc-nav__link ">
    34.4.5. Next steps
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#about-advertise-for-ipaddress-pools" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.5. About advertising for the IP address pools
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.5. About advertising for the IP address pools"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.5. About advertising for the IP address pools"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-bgpadvertisement-cr_about-advertising-ip-address-pool" class="j-doc-nav__link ">
    34.5.1. About the BGPAdvertisement custom resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-configure-BGP-advertisement-basic-use-case_about-advertising-ip-address-pool" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.5.2. Configuring MetalLB with a BGP advertisement and a basic use case
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.5.2. Configuring MetalLB with a BGP advertisement and a basic use case"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.5.2. Configuring MetalLB with a BGP advertisement and a basic use case"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-advertise-a-basic-address-pool-configuration-bgp_about-advertising-ip-address-pool" class="j-doc-nav__link ">
    34.5.2.1. Example: Advertise a basic address pool configuration with BGP
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-configure-BGP-advertisement-advanced-use-case_about-advertising-ip-address-pool" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.5.3. Configuring MetalLB with a BGP advertisement and an advanced use case
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.5.3. Configuring MetalLB with a BGP advertisement and an advanced use case"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.5.3. Configuring MetalLB with a BGP advertisement and an advanced use case"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-advertise-an-advanced-address-pool-configuration-bgp_about-advertising-ip-address-pool" class="j-doc-nav__link ">
    34.5.3.1. Example: Advertise an advanced address pool configuration with BGP
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-advertise-ip-pools-to-node-subset_about-advertising-ip-address-pool" class="j-doc-nav__link ">
    34.5.4. Advertising an IP address pool from a subset of nodes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-l2padvertisement-cr_about-advertising-ip-address-pool" class="j-doc-nav__link ">
    34.5.5. About the L2Advertisement custom resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-configure-with-L2-advertisement_about-advertising-ip-address-pool" class="j-doc-nav__link ">
    34.5.6. Configuring MetalLB with an L2 advertisement
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-configure-with-L2-advertisement-label_about-advertising-ip-address-pool" class="j-doc-nav__link ">
    34.5.7. Configuring MetalLB with a L2 advertisement and label
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-configure-with-L2-advertisement-interface_about-advertising-ip-address-pool" class="j-doc-nav__link ">
    34.5.8. Configuring MetalLB with an L2 advertisement for selected interfaces
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#additional-resources_about-advertiseipaddress" class="j-doc-nav__link ">
    34.5.9. Additional resources
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#metallb-configure-bgp-peers" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.6. Configuring MetalLB BGP peers
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.6. Configuring MetalLB BGP peers"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.6. Configuring MetalLB BGP peers"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-bgppeer-cr_configure-metallb-bgp-peers" class="j-doc-nav__link ">
    34.6.1. About the BGP peer custom resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-configure-bgppeer_configure-metallb-bgp-peers" class="j-doc-nav__link ">
    34.6.2. Configuring a BGP peer
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-example-assign-specific-address-pools-specific-bgp-peers_configure-metallb-bgp-peers" class="j-doc-nav__link ">
    34.6.3. Configure a specific set of BGP peers for a given address pool
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-example-bgppeer_configure-metallb-bgp-peers" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.6.4. Example BGP peer configurations
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.6.4. Example BGP peer configurations"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.6.4. Example BGP peer configurations"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-example-limit-nodes-bgppeer_configure-metallb-bgp-peers" class="j-doc-nav__link ">
    34.6.4.1. Example: Limit which nodes connect to a BGP peer
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-example-specify-bfd-profile_configure-metallb-bgp-peers" class="j-doc-nav__link ">
    34.6.4.2. Example: Specify a BFD profile for a BGP peer
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-example-dual-stack_configure-metallb-bgp-peers" class="j-doc-nav__link ">
    34.6.4.3. Example: Specify BGP peers for dual-stack networking
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#next-steps_configure-metallb-bgp-peers" class="j-doc-nav__link ">
    34.6.5. Next steps
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#metallb-configure-community-alias" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.7. Configuring community alias
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.7. Configuring community alias"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.7. Configuring community alias"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-community-cr_configure-community-alias" class="j-doc-nav__link ">
    34.7.1. About the community custom resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-configure-BGP-advertisement-community-alias_configure-community-alias" class="j-doc-nav__link ">
    34.7.2. Configuring MetalLB with a BGP advertisement and community alias
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#metallb-configure-bfd-profiles" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.8. Configuring MetalLB BFD profiles
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.8. Configuring MetalLB BFD profiles"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.8. Configuring MetalLB BFD profiles"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-bfdprofile-cr_configure-metallb-bfd-profiles" class="j-doc-nav__link ">
    34.8.1. About the BFD profile custom resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-configure-bfdprofile_configure-metallb-bfd-profiles" class="j-doc-nav__link ">
    34.8.2. Configuring a BFD profile
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#next-steps_configure-metallb-bfd-profiles" class="j-doc-nav__link ">
    34.8.3. Next steps
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#metallb-configure-services" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.9. Configuring services to use MetalLB
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.9. Configuring services to use MetalLB"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.9. Configuring services to use MetalLB"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#request-specific-ip-address_configure-services-metallb" class="j-doc-nav__link ">
    34.9.1. Request a specific IP address
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#request-ip-address-from-pool_configure-services-metallb" class="j-doc-nav__link ">
    34.9.2. Request an IP address from a specific pool
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#accept-any-ip-address_configure-services-metallb" class="j-doc-nav__link ">
    34.9.3. Accept any IP address
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#share-specific-ip-address_configure-services-metallb" class="j-doc-nav__link ">
    34.9.4. Share a specific IP address
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-configure-svc_configure-services-metallb" class="j-doc-nav__link ">
    34.9.5. Configuring a service with MetalLB
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#metallb-logging-troubleshooting-support" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.10. MetalLB logging, troubleshooting, and support
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.10. MetalLB logging, troubleshooting, and support"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.10. MetalLB logging, troubleshooting, and support"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-setting-metalb-logging-levels_metallb-troubleshoot-support" class="j-doc-nav__link j-doc-nav__link--has-children">
    34.10.1. Setting the MetalLB logging levels
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "34.10.1. Setting the MetalLB logging levels"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "34.10.1. Setting the MetalLB logging levels"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#frr-log-levels_metallb-troubleshoot-support" class="j-doc-nav__link ">
    34.10.1.1. FRRouting (FRR) log levels
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-troubleshoot-bgp_metallb-troubleshoot-support" class="j-doc-nav__link ">
    34.10.2. Troubleshooting BGP issues
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-troubleshoot-bfd_metallb-troubleshoot-support" class="j-doc-nav__link ">
    34.10.3. Troubleshooting BFD issues
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-metrics_metallb-troubleshoot-support" class="j-doc-nav__link ">
    34.10.4. MetalLB metrics for BGP and BFD
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-metallb-collecting-data_metallb-troubleshoot-support" class="j-doc-nav__link ">
    34.10.5. About collecting MetalLB data
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#associating-secondary-interfaces-metrics-to-network-attachments" class="j-doc-nav__link j-doc-nav__link--has-children">
    35. Associating secondary interfaces metrics to network attachments
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "35. Associating secondary interfaces metrics to network attachments"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "35. Associating secondary interfaces metrics to network attachments"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-associating-secondary-interfaces-metrics-to-network-attachments_secondary-interfaces-metrics" class="j-doc-nav__link j-doc-nav__link--has-children">
    35.1. Extending secondary network metrics for monitoring
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "35.1. Extending secondary network metrics for monitoring"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "35.1. Extending secondary network metrics for monitoring"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-associating-secondary-interfaces-metrics-to-network-attachments-network-metrics-daemon_secondary-interfaces-metrics" class="j-doc-nav__link ">
    35.1.1. Network Metrics Daemon
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#cnf-associating-secondary-interfaces-metrics-with-network-name_secondary-interfaces-metrics" class="j-doc-nav__link ">
    35.1.2. Metrics with network name
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                      

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability" class="j-doc-nav__link j-doc-nav__link--has-children">
    36. Network Observability
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36. Network Observability"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36. Network Observability"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-release-notes" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.1. Network Observability Operator release notes
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.1. Network Observability Operator release notes"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.1. Network Observability Operator release notes"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-release-notes-1-4" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.1.1. Network Observability Operator 1.4.0
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.1.1. Network Observability Operator 1.4.0"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.1.1. Network Observability Operator 1.4.0"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-channel-removal-1.4" class="j-doc-nav__link ">
    36.1.1.1. Channel removal
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-1.4.0-features-enhancements" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.1.1.2. New features and enhancements
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.1.1.2. New features and enhancements"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.1.1.2. New features and enhancements"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-enhanced-configuration-and-ui-1.4" class="j-doc-nav__link ">
    36.1.1.2.1. Notable enhancements
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-without-loki-1.4" class="j-doc-nav__link ">
    36.1.1.2.2. Network Observability without Loki
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-dns-tracking-1.4" class="j-doc-nav__link ">
    36.1.1.2.3. DNS tracking
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#SR-IOV-configuration-1.4" class="j-doc-nav__link ">
    36.1.1.2.4. SR-IOV support
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#IPFIX-support-1.4" class="j-doc-nav__link ">
    36.1.1.2.5. IPFIX exporter support
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-packet-drop-1.4" class="j-doc-nav__link ">
    36.1.1.2.6. Packet drops
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#s390x-architecture-support" class="j-doc-nav__link ">
    36.1.1.2.7. s390x architecture support
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-1.4.0-bug-fixes" class="j-doc-nav__link ">
    36.1.1.3. Bug fixes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-1.4.0-known-issues" class="j-doc-nav__link ">
    36.1.1.4. Known issues
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-release-notes-1-3" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.1.2. Network Observability Operator 1.3.0
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.1.2. Network Observability Operator 1.3.0"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.1.2. Network Observability Operator 1.3.0"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-channel-deprecation" class="j-doc-nav__link ">
    36.1.2.1. Channel deprecation
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-1.3.0-features-enhancements" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.1.2.2. New features and enhancements
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.1.2.2. New features and enhancements"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.1.2.2. New features and enhancements"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#multi-tenancy-1.3" class="j-doc-nav__link ">
    36.1.2.2.1. Multi-tenancy in Network Observability
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#flow-based-dashboard-1.3" class="j-doc-nav__link ">
    36.1.2.2.2. Flow-based metrics dashboard
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#must-gather-1.3" class="j-doc-nav__link ">
    36.1.2.2.3. Troubleshooting with the must-gather tool
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#multi-arch-1.3" class="j-doc-nav__link ">
    36.1.2.2.4. Multiple architectures now supported
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#deprecated-features-1.3" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.1.2.3. Deprecated features
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.1.2.3. Deprecated features"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.1.2.3. Deprecated features"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#authToken-host" class="j-doc-nav__link ">
    36.1.2.3.1. Deprecated configuration parameter setting
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-1.3.0-bug-fixes" class="j-doc-nav__link ">
    36.1.2.4. Bug fixes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-1.3.0-known-issues" class="j-doc-nav__link ">
    36.1.2.5. Known issues
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-release-notes-1-2" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.1.3. Network Observability Operator 1.2.0
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.1.3. Network Observability Operator 1.2.0"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.1.3. Network Observability Operator 1.2.0"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-preparing-to-update" class="j-doc-nav__link ">
    36.1.3.1. Preparing for the next update
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-1.2.0-features-enhancements" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.1.3.2. New features and enhancements
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.1.3.2. New features and enhancements"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.1.3.2. New features and enhancements"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#histogram-feature-1.2" class="j-doc-nav__link ">
    36.1.3.2.1. Histogram in Traffic Flows view
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#conversation-tracking-feature-1.2" class="j-doc-nav__link ">
    36.1.3.2.2. Conversation tracking
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#health-alerts-feature-1.2" class="j-doc-nav__link ">
    36.1.3.2.3. Network Observability health alerts
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-1.2.0-bug-fixes" class="j-doc-nav__link ">
    36.1.3.3. Bug fixes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-1.2.0-known-issues" class="j-doc-nav__link ">
    36.1.3.4. Known issue
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-1.2.0-notable-technical-changes" class="j-doc-nav__link ">
    36.1.3.5. Notable technical changes
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-release-notes-1-1" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.1.4. Network Observability Operator 1.1.0
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.1.4. Network Observability Operator 1.1.0"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.1.4. Network Observability Operator 1.1.0"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-1.1.0-bug-fixes" class="j-doc-nav__link ">
    36.1.4.1. Bug fix
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-overview" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.2. About Network Observability
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.2. About Network Observability"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.2. About Network Observability"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#dependency-network-observability" class="j-doc-nav__link ">
    36.2.1. Dependency of Network Observability Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#optional-dependency-network-observability" class="j-doc-nav__link ">
    36.2.2. Optional dependencies of the Network Observability Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator" class="j-doc-nav__link ">
    36.2.3. Network Observability Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#no-console-integration" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.2.4. OpenShift Container Platform console integration
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.2.4. OpenShift Container Platform console integration"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.2.4. OpenShift Container Platform console integration"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-dashboards" class="j-doc-nav__link ">
    36.2.4.1. Network Observability metrics dashboards
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-topology-views" class="j-doc-nav__link ">
    36.2.4.2. Network Observability topology views
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#traffic-flow-tables" class="j-doc-nav__link ">
    36.2.4.3. Traffic flow tables
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#installing-network-observability-operators" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.3. Installing the Network Observability Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.3. Installing the Network Observability Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.3. Installing the Network Observability Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-without-loki_network_observability" class="j-doc-nav__link ">
    36.3.1. Network Observability without Loki
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-loki-installation_network_observability" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.3.2. Installing the Loki Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.3.2. Installing the Loki Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.3.2. Installing the Loki Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-loki-secret_network_observability" class="j-doc-nav__link ">
    36.3.2.1. Creating a secret for Loki storage
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-lokistack-create_network_observability" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.3.2.2. Creating a LokiStack custom resource
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.3.2.2. Creating a LokiStack custom resource"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.3.2.2. Creating a LokiStack custom resource"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#deployment-sizing_network_observability" class="j-doc-nav__link ">
    36.3.2.2.1. Deployment Sizing
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-lokistack-configuring-ingestionnetwork_observability" class="j-doc-nav__link ">
    36.3.2.3. LokiStack ingestion limits and health alerts
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-auth-mutli-tenancy_network_observability" class="j-doc-nav__link ">
    36.3.3. Configuring authorization and multi-tenancy
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-multi-tenancynetwork_observability" class="j-doc-nav__link ">
    36.3.4. Enabling multi-tenancy in Network Observability
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-kafka-option_network_observability" class="j-doc-nav__link ">
    36.3.5. Installing Kafka (optional)
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-installation_network_observability" class="j-doc-nav__link ">
    36.3.6. Installing the Network Observability Operator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-uninstall_network_observability" class="j-doc-nav__link ">
    36.3.7. Uninstalling the Network Observability Operator
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-network-observability-operator" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.4. Network Observability Operator in OpenShift Container Platform
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.4. Network Observability Operator in OpenShift Container Platform"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.4. Network Observability Operator in OpenShift Container Platform"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-network-observability-operator_nw-network-observability-operator" class="j-doc-nav__link ">
    36.4.1. Viewing statuses
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-architecture_nw-network-observability-operator" class="j-doc-nav__link ">
    36.4.2. Network Observablity Operator architecture
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-status-configuration-network-observability-operator_nw-network-observability-operator" class="j-doc-nav__link ">
    36.4.3. Viewing Network Observability Operator status and configuration
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configuring-network-observability-operators" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.5. Configuring the Network Observability Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.5. Configuring the Network Observability Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.5. Configuring the Network Observability Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-flowcollector-view_network_observability" class="j-doc-nav__link ">
    36.5.1. View the FlowCollector resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-flowcollector-kafka-config_network_observability" class="j-doc-nav__link ">
    36.5.2. Configuring the Flow Collector resource with Kafka
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-enriched-flows_network_observability" class="j-doc-nav__link ">
    36.5.3. Export enriched network flow data
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-config-FLP-sampling_network_observability" class="j-doc-nav__link ">
    36.5.4. Updating the Flow Collector resource
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-config-quick-filters_network_observability" class="j-doc-nav__link ">
    36.5.5. Configuring quick filters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-SR-IOV-config_network_observability" class="j-doc-nav__link ">
    36.5.6. Configuring monitoring for SR-IOV interface traffic
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-resource-recommendations_network_observability" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.5.7. Resource management and performance considerations
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.5.7. Resource management and performance considerations"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.5.7. Resource management and performance considerations"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-resources-table_network_observability" class="j-doc-nav__link ">
    36.5.7.1. Resource considerations
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-network-policy" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.6. Network Policy
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.6. Network Policy"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.6. Network Policy"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-network-policy_network_observability" class="j-doc-nav__link ">
    36.6.1. Creating a network policy for Network Observability
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-sample-network-policy_network_observability" class="j-doc-nav__link ">
    36.6.2. Example network policy
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#nw-observe-network-traffic" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.7. Observing the network traffic
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.7. Observing the network traffic"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.7. Observing the network traffic"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-overview_nw-observe-network-traffic" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.7.1. Observing the network traffic from the Overview view
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.7.1. Observing the network traffic from the Overview view"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.7.1. Observing the network traffic from the Overview view"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-working-with-overview_nw-observe-network-traffic" class="j-doc-nav__link ">
    36.7.1.1. Working with the Overview view
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-configuring-options-overview_nw-observe-network-traffic" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.7.1.2. Configuring advanced options for the Overview view
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.7.1.2. Configuring advanced options for the Overview view"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.7.1.2. Configuring advanced options for the Overview view"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-cao-managing-panels-overview_nw-observe-network-traffic" class="j-doc-nav__link ">
    36.7.1.2.1. Managing panels
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-pktdrop-overview_nw-observe-network-traffic" class="j-doc-nav__link ">
    36.7.1.2.2. Packet drop tracking
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-dns-overview_nw-observe-network-traffic" class="j-doc-nav__link ">
    36.7.1.2.3. DNS tracking
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-trafficflow_nw-observe-network-traffic" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.7.2. Observing the network traffic from the Traffic flows view
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.7.2. Observing the network traffic from the Traffic flows view"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.7.2. Observing the network traffic from the Traffic flows view"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-working-with-trafficflow_nw-observe-network-traffic" class="j-doc-nav__link ">
    36.7.2.1. Working with the Traffic flows view
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-configuring-options-trafficflow_nw-observe-network-traffic" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.7.2.2. Configuring advanced options for the Traffic flows view
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.7.2.2. Configuring advanced options for the Traffic flows view"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.7.2.2. Configuring advanced options for the Traffic flows view"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-cao-managing-columns-trafficflownw-observe-network-traffic" class="j-doc-nav__link ">
    36.7.2.2.1. Managing columns
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-cao-export-trafficflow_nw-observe-network-traffic" class="j-doc-nav__link ">
    36.7.2.2.2. Exporting the traffic flow data
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-working-with-conversations_nw-observe-network-traffic" class="j-doc-nav__link ">
    36.7.2.3. Working with conversation tracking
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-packet-drops_nw-observe-network-traffic" class="j-doc-nav__link ">
    36.7.2.4. Working with packet drops
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-dns-tracking_nw-observe-network-traffic" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.7.2.5. Working with DNS tracking
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.7.2.5. Working with DNS tracking"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.7.2.5. Working with DNS tracking"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-histogram-trafficflow_nw-observe-network-traffic" class="j-doc-nav__link ">
    36.7.2.5.1. Using the histogram
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-topology_nw-observe-network-traffic" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.7.3. Observing the network traffic from the Topology view
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.7.3. Observing the network traffic from the Topology view"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.7.3. Observing the network traffic from the Topology view"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-working-with-topology_nw-observe-network-traffic" class="j-doc-nav__link ">
    36.7.3.1. Working with the Topology view
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-configuring-options-topology_nw-observe-network-traffic" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.7.3.2. Configuring the advanced options for the Topology view
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.7.3.2. Configuring the advanced options for the Topology view"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.7.3.2. Configuring the advanced options for the Topology view"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-cao-export-topology_nw-observe-network-traffic" class="j-doc-nav__link ">
    36.7.3.2.1. Exporting the topology view
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-quickfilternw-observe-network-traffic" class="j-doc-nav__link ">
    36.7.4. Filtering the network traffic
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-operator-monitoring" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.8. Monitoring the Network Observability Operator
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.8. Monitoring the Network Observability Operator"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.8. Monitoring the Network Observability Operator"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-alert-dashboard_network_observability" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.8.1. Viewing health information
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.8.1. Viewing health information"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.8.1. Viewing health information"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-disable-alerts_network_observability" class="j-doc-nav__link ">
    36.8.1.1. Disabling health alerts
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-netobserv-dashboard-rate-limit-alerts_network_observability" class="j-doc-nav__link ">
    36.8.2. Creating Loki rate limit alerts for the NetObserv dashboard
  </a>
  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#flowcollector-api" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.9. FlowCollector configuration parameters
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.9. FlowCollector configuration parameters"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.9. FlowCollector configuration parameters"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-flowcollector-api-specifications_network_observability" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.9.1. FlowCollector API specifications
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.9.1. FlowCollector API specifications"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.9.1. FlowCollector API specifications"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#metadata" class="j-doc-nav__link ">
    36.9.1.1. .metadata
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec" class="j-doc-nav__link ">
    36.9.1.2. .spec
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-agent" class="j-doc-nav__link ">
    36.9.1.3. .spec.agent
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-agent-ebpf" class="j-doc-nav__link ">
    36.9.1.4. .spec.agent.ebpf
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-agent-ebpf-debug" class="j-doc-nav__link ">
    36.9.1.5. .spec.agent.ebpf.debug
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-agent-ebpf-resources" class="j-doc-nav__link ">
    36.9.1.6. .spec.agent.ebpf.resources
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-agent-ipfix" class="j-doc-nav__link ">
    36.9.1.7. .spec.agent.ipfix
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-agent-ipfix-clusternetworkoperator" class="j-doc-nav__link ">
    36.9.1.8. .spec.agent.ipfix.clusterNetworkOperator
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-agent-ipfix-ovnkubernetes" class="j-doc-nav__link ">
    36.9.1.9. .spec.agent.ipfix.ovnKubernetes
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-consoleplugin" class="j-doc-nav__link ">
    36.9.1.10. .spec.consolePlugin
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-consoleplugin-autoscaler" class="j-doc-nav__link ">
    36.9.1.11. .spec.consolePlugin.autoscaler
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-consoleplugin-portnaming" class="j-doc-nav__link ">
    36.9.1.12. .spec.consolePlugin.portNaming
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-consoleplugin-quickfilters" class="j-doc-nav__link ">
    36.9.1.13. .spec.consolePlugin.quickFilters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-consoleplugin-quickfilters-2" class="j-doc-nav__link ">
    36.9.1.14. .spec.consolePlugin.quickFilters[]
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-consoleplugin-resources" class="j-doc-nav__link ">
    36.9.1.15. .spec.consolePlugin.resources
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-exporters" class="j-doc-nav__link ">
    36.9.1.16. .spec.exporters
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-exporters-2" class="j-doc-nav__link ">
    36.9.1.17. .spec.exporters[]
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-exporters-ipfix" class="j-doc-nav__link ">
    36.9.1.18. .spec.exporters[].ipfix
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-exporters-kafka" class="j-doc-nav__link ">
    36.9.1.19. .spec.exporters[].kafka
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-exporters-kafka-sasl" class="j-doc-nav__link ">
    36.9.1.20. .spec.exporters[].kafka.sasl
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-exporters-kafka-sasl-clientidreference" class="j-doc-nav__link ">
    36.9.1.21. .spec.exporters[].kafka.sasl.clientIDReference
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-exporters-kafka-sasl-clientsecretreference" class="j-doc-nav__link ">
    36.9.1.22. .spec.exporters[].kafka.sasl.clientSecretReference
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-exporters-kafka-tls" class="j-doc-nav__link ">
    36.9.1.23. .spec.exporters[].kafka.tls
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-exporters-kafka-tls-cacert" class="j-doc-nav__link ">
    36.9.1.24. .spec.exporters[].kafka.tls.caCert
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-exporters-kafka-tls-usercert" class="j-doc-nav__link ">
    36.9.1.25. .spec.exporters[].kafka.tls.userCert
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-kafka" class="j-doc-nav__link ">
    36.9.1.26. .spec.kafka
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-kafka-sasl" class="j-doc-nav__link ">
    36.9.1.27. .spec.kafka.sasl
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-kafka-sasl-clientidreference" class="j-doc-nav__link ">
    36.9.1.28. .spec.kafka.sasl.clientIDReference
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-kafka-sasl-clientsecretreference" class="j-doc-nav__link ">
    36.9.1.29. .spec.kafka.sasl.clientSecretReference
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-kafka-tls" class="j-doc-nav__link ">
    36.9.1.30. .spec.kafka.tls
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-kafka-tls-cacert" class="j-doc-nav__link ">
    36.9.1.31. .spec.kafka.tls.caCert
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-kafka-tls-usercert" class="j-doc-nav__link ">
    36.9.1.32. .spec.kafka.tls.userCert
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-loki" class="j-doc-nav__link ">
    36.9.1.33. .spec.loki
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-loki-statustls" class="j-doc-nav__link ">
    36.9.1.34. .spec.loki.statusTls
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-loki-statustls-cacert" class="j-doc-nav__link ">
    36.9.1.35. .spec.loki.statusTls.caCert
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-loki-statustls-usercert" class="j-doc-nav__link ">
    36.9.1.36. .spec.loki.statusTls.userCert
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-loki-tls" class="j-doc-nav__link ">
    36.9.1.37. .spec.loki.tls
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-loki-tls-cacert" class="j-doc-nav__link ">
    36.9.1.38. .spec.loki.tls.caCert
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-loki-tls-usercert" class="j-doc-nav__link ">
    36.9.1.39. .spec.loki.tls.userCert
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-processor" class="j-doc-nav__link ">
    36.9.1.40. .spec.processor
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-processor-debug" class="j-doc-nav__link ">
    36.9.1.41. .spec.processor.debug
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-processor-kafkaconsumerautoscaler" class="j-doc-nav__link ">
    36.9.1.42. .spec.processor.kafkaConsumerAutoscaler
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-processor-metrics" class="j-doc-nav__link ">
    36.9.1.43. .spec.processor.metrics
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-processor-metrics-server" class="j-doc-nav__link ">
    36.9.1.44. .spec.processor.metrics.server
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-processor-metrics-server-tls" class="j-doc-nav__link ">
    36.9.1.45. .spec.processor.metrics.server.tls
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-processor-metrics-server-tls-provided" class="j-doc-nav__link ">
    36.9.1.46. .spec.processor.metrics.server.tls.provided
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-processor-metrics-server-tls-providedcafile" class="j-doc-nav__link ">
    36.9.1.47. .spec.processor.metrics.server.tls.providedCaFile
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#spec-processor-resources" class="j-doc-nav__link ">
    36.9.1.48. .spec.processor.resources
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#json-flows-format-reference" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.10. Network flows format reference
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.10. Network flows format reference"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.10. Network flows format reference"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-flows-format_json_reference" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.10.1. Network Flows format reference
  </a>
                              <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.10.1. Network Flows format reference"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.10.1. Network Flows format reference"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#labels" class="j-doc-nav__link ">
    36.10.1.1. Labels
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#fields" class="j-doc-nav__link ">
    36.10.1.2. Fields
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#enumeration-flowdirection" class="j-doc-nav__link ">
    36.10.1.3. Enumeration: FlowDirection
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-superdoc-subnav__item">
                          

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#installing-troubleshooting" class="j-doc-nav__link j-doc-nav__link--has-children">
    36.11. Troubleshooting Network Observability
  </a>
                    <button class="j-doc-nav__children-toggle content-expander__trigger">
        <span class="visually-hidden j-doc-nav__children-toggle__expand-text">
          Expand section "36.11. Troubleshooting Network Observability"
        </span>
        <span class="visually-hidden  j-doc-nav__children-toggle__collapse-text">
          Collapse section "36.11. Troubleshooting Network Observability"
        </span>
      </button>

      <ol class="j-doc-nav__subnav j-superdoc-subnav content-expander">
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-must-gather_network-observability-troubleshooting" class="j-doc-nav__link ">
    36.11.1. Using the must-gather tool
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configure-network-traffic-console_network-observability-troubleshooting" class="j-doc-nav__link ">
    36.11.2. Configuring network traffic menu entry in the OpenShift Container Platform console
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configure-network-traffic-flowlogs-pipeline-kafka_network-observability-troubleshooting" class="j-doc-nav__link ">
    36.11.3. Flowlogs-Pipeline does not consume network flows after installing Kafka
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#configure-network-traffic-interfaces_network-observability-troubleshooting" class="j-doc-nav__link ">
    36.11.4. Failing to see network flows from both br-int and br-ex interfaces
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#controller-manager-pod-runs-out-of-memory_network-observability-troubleshooting" class="j-doc-nav__link ">
    36.11.5. Network Observability controller manager pod runs out of memory
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#resource-troubleshooting" class="j-doc-nav__link ">
    36.11.6. Resource troubleshooting
  </a>
  
          </li>
                  <li class="j-superdoc-subnav__item">
                        

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#network-observability-troubleshooting-loki-tenant-rate-limit_network-observability-troubleshooting" class="j-doc-nav__link ">
    36.11.7. LokiStack rate limit errors
  </a>
  
          </li>
              </ol>

  
          </li>
              </ol>

  
          </li>
                  <li class="j-doc-nav__list-item">
                                    

    <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking#idm140587105891184" class="j-doc-nav__link ">
    Legal Notice
  </a>
  
          </li>
              </ol>
    
  </div>
</nav>


              </div>
            </div>
            <div class="j-options-container j-options-container--mobile">
              <button class="j-sidebar__menu-trigger j-sidebar__menu-trigger--options content-expander__trigger">
                <span class="j-sidebar__menu-trigger__open-headline">
                  Settings
                </span>
                <span class="j-sidebar__menu-trigger__close-text">Close</span>
              </button>
              

  <ul class="j-doc-options__list content-expander">
    <li class="j-doc-options__item">
          <label class="j-doc-option__label j-doc-option__label--language" for="j-doc-language">
        Language:
      </label>
      <select id="j-doc-language" class="j-doc-option__select">
                  <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/networking" selected=''>
            English
          </option>
                  <option value="/documentation/zh-cn/openshift_container_platform/4.13/html-single/networking" >
            简体中文
          </option>
                  <option value="/documentation/ko-kr/openshift_container_platform/4.13/html-single/networking" >
            한국어
          </option>
                  <option value="/documentation/ja-jp/openshift_container_platform/4.13/html-single/networking" >
            日本語
          </option>
              </select>

            <noscript>
        <div class="j-doc-option__label j-doc-option__label--language" id="j-doc-option__label--language--nojs">
          Language:
        </div>
        <ul aria-labelledby="j-doc-option__label--language--nojs" class="j-doc-option__languages-list">
                      <li>
              <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking">English</a>
            </li>
                      <li>
              <a href="/documentation/zh-cn/openshift_container_platform/4.13/html-single/networking">简体中文</a>
            </li>
                      <li>
              <a href="/documentation/ko-kr/openshift_container_platform/4.13/html-single/networking">한국어</a>
            </li>
                      <li>
              <a href="/documentation/ja-jp/openshift_container_platform/4.13/html-single/networking">日本語</a>
            </li>
                  </ul>
      </noscript>

      </li>

    <li class="j-doc-options__item">
    <label for="j-doc-mode" class="j-doc-option__label j-doc-option__label--format">
      Format:
    </label>
    <select id="j-doc-mode" class="j-doc-option__select">
              <option value="/documentation/en-us/openshift_container_platform/4.13/html/networking"  class="j-doc-options__option j-doc-options__option--multi-page">
          Multi-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/networking" selected='' class="j-doc-options__option j-doc-options__option--single-page">
          Single-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/pdf/networking/OpenShift_Container_Platform-4.13-Networking-en-US.pdf"  class="j-doc-options__option j-doc-options__option--pdf">
          PDF
        </option>
          </select>

        <noscript>
      <div class="j-doc-option__label j-doc-option__label--format" id="j-doc-option__label--format--nojs">
        Format:
      </div>
      <ul class="j-doc-option__format-list" aria-labelledby="j-doc-option__label--format--nojs">
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--multi-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html/networking">Multi-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--single-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking">Single-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--pdf"><a href="/documentation/en-us/openshift_container_platform/4.13/pdf/networking/OpenShift_Container_Platform-4.13-Networking-en-US.pdf">PDF</a></li>
              </ul>
    </noscript>
  </li>
</ul>


              </div>
          </div>
                <div class="pvof-doc__tertiary-sidebar j-doc__tertiary-sidebar">
          <div class="pvof-doc__tertiary-sidebar__inner j-doc__tertiary-sidebar__inner">
            <div class="j-doc__doc-options">
              <div class="j-options-container j-options-container--desktop">
                <button class="j-sidebar__menu-trigger j-sidebar__menu-trigger--tablet content-expander__trigger">
                  <span class="j-sidebar__menu-trigger-icon"></span>
                  <h2 class="visually-hidden">Language and Page Formatting Options</h2>
                </button>
                  

  <ul class="j-doc-options__list content-expander">
    <li class="j-doc-options__item">
          <label class="j-doc-option__label j-doc-option__label--language" for="j-doc-language--2">
        Language:
      </label>
      <select id="j-doc-language--2" class="j-doc-option__select">
                  <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/networking" selected=''>
            English
          </option>
                  <option value="/documentation/zh-cn/openshift_container_platform/4.13/html-single/networking" >
            简体中文
          </option>
                  <option value="/documentation/ko-kr/openshift_container_platform/4.13/html-single/networking" >
            한국어
          </option>
                  <option value="/documentation/ja-jp/openshift_container_platform/4.13/html-single/networking" >
            日本語
          </option>
              </select>

            <noscript>
        <div class="j-doc-option__label j-doc-option__label--language" id="j-doc-option__label--language--nojs">
          Language:
        </div>
        <ul aria-labelledby="j-doc-option__label--language--nojs" class="j-doc-option__languages-list">
                      <li>
              <a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking">English</a>
            </li>
                      <li>
              <a href="/documentation/zh-cn/openshift_container_platform/4.13/html-single/networking">简体中文</a>
            </li>
                      <li>
              <a href="/documentation/ko-kr/openshift_container_platform/4.13/html-single/networking">한국어</a>
            </li>
                      <li>
              <a href="/documentation/ja-jp/openshift_container_platform/4.13/html-single/networking">日本語</a>
            </li>
                  </ul>
      </noscript>

      </li>

    <li class="j-doc-options__item">
    <label for="j-doc-mode--2" class="j-doc-option__label j-doc-option__label--format">
      Format:
    </label>
    <select id="j-doc-mode--2" class="j-doc-option__select">
              <option value="/documentation/en-us/openshift_container_platform/4.13/html/networking"  class="j-doc-options__option j-doc-options__option--multi-page">
          Multi-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/html-single/networking" selected='' class="j-doc-options__option j-doc-options__option--single-page">
          Single-page
        </option>
              <option value="/documentation/en-us/openshift_container_platform/4.13/pdf/networking/OpenShift_Container_Platform-4.13-Networking-en-US.pdf"  class="j-doc-options__option j-doc-options__option--pdf">
          PDF
        </option>
          </select>

        <noscript>
      <div class="j-doc-option__label j-doc-option__label--format" id="j-doc-option__label--format--nojs">
        Format:
      </div>
      <ul class="j-doc-option__format-list" aria-labelledby="j-doc-option__label--format--nojs">
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--multi-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html/networking">Multi-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--single-page"><a href="/documentation/en-us/openshift_container_platform/4.13/html-single/networking">Single-page</a></li>
                  <li class="j-doc-mode-no-js__link j-doc-mode-no-js__link--pdf"><a href="/documentation/en-us/openshift_container_platform/4.13/pdf/networking/OpenShift_Container_Platform-4.13-Networking-en-US.pdf">PDF</a></li>
              </ul>
    </noscript>
  </li>
</ul>


                </div>
              </div>
          </div>
        </div>

                  <div class="doc-wrapper pvof-doc__wrapper j-superdoc__content-wrapper" id="doc-wrapper">
            

  <div class="pane-page-title">
    <h1 class="title" itemprop="name">Networking</h1>
  </div>


  <div xml:lang="en-US" class="book" id="idm140587151128080"><div class="titlepage"><div><div class="producttitle"><span class="productname">OpenShift Container Platform</span> <span class="productnumber">4.13</span></div><div><h2 class="subtitle">Configuring and managing cluster networking</h2></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat OpenShift Documentation Team</span></div></div><div><a href="#idm140587105891184">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				This document provides instructions for configuring and managing your OpenShift Container Platform cluster network, including DNS, ingress, and the Pod network.
			</div></div></div></div><hr/></div><section class="chapter" id="about-networking"><div class="titlepage"><div><div><h1 class="title">Chapter 1. About networking</h1></div></div></div><p>
			Red Hat OpenShift Networking is an ecosystem of features, plugins and advanced networking capabilities that extend Kubernetes networking with the advanced networking-related features that your cluster needs to manage its network traffic for one or multiple hybrid clusters. This ecosystem of networking capabilities integrates ingress, egress, load balancing, high-performance throughput, security, inter- and intra-cluster traffic management and provides role-based observability tooling to reduce its natural complexities.
		</p><p>
			The following list highlights some of the most commonly used Red Hat OpenShift Networking features available on your cluster:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
					Primary cluster network provided by either of the following Container Network Interface (CNI) plugins:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-ovn-kubernetes">OVN-Kubernetes network plugin</a>, the default plugin
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-openshift-sdn">OpenShift SDN network plugin</a>
						</li></ul></div></li><li class="listitem">
					Certified 3rd-party alternative primary network plugins
				</li><li class="listitem">
					Cluster Network Operator for network plugin management
				</li><li class="listitem">
					Ingress Operator for TLS encrypted web traffic
				</li><li class="listitem">
					DNS Operator for name assignment
				</li><li class="listitem">
					MetalLB Operator for traffic load balancing on bare metal clusters
				</li><li class="listitem">
					IP failover support for high-availability
				</li><li class="listitem">
					Additional hardware network support through multiple CNI plugins, including for macvlan, ipvlan, and SR-IOV hardware networks
				</li><li class="listitem">
					IPv4, IPv6, and dual stack addressing
				</li><li class="listitem">
					Hybrid Linux-Windows host clusters for Windows-based workloads
				</li><li class="listitem">
					Red Hat OpenShift Service Mesh for discovery, load balancing, service-to-service authentication, failure recovery, metrics, and monitoring of services
				</li><li class="listitem">
					Single-node OpenShift
				</li><li class="listitem">
					Network Observability Operator for network debugging and insights
				</li><li class="listitem">
					<a class="link" href="https://catalog.redhat.com/software/container-stacks/detail/5f0c67b7ce85fb9e399f3a12">Submariner</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_application_interconnect/1.0/html/introduction_to_application_interconnect/index">Red Hat Application Interconnect</a> technologies for inter-cluster networking
				</li></ul></div></section><section class="chapter" id="understanding-networking"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Understanding networking</h1></div></div></div><p>
			Cluster Administrators have several options for exposing applications that run inside a cluster to external traffic and securing network connections:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Service types, such as node ports or load balancers
				</li><li class="listitem">
					API resources, such as <code class="literal">Ingress</code> and <code class="literal">Route</code>
				</li></ul></div><p>
			By default, Kubernetes allocates each pod an internal IP address for applications running within the pod. Pods and their containers can network, but clients outside the cluster do not have networking access. When you expose your application to external traffic, giving each pod its own IP address means that pods can be treated like physical hosts or virtual machines in terms of port allocation, networking, naming, service discovery, load balancing, application configuration, and migration.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				Some cloud platforms offer metadata APIs that listen on the 169.254.169.254 IP address, a link-local IP address in the IPv4 <code class="literal">169.254.0.0/16</code> CIDR block.
			</p><p>
				This CIDR block is not reachable from the pod network. Pods that need access to these IP addresses must be given host network access by setting the <code class="literal">spec.hostNetwork</code> field in the pod spec to <code class="literal">true</code>.
			</p><p>
				If you allow a pod host network access, you grant the pod privileged access to the underlying network infrastructure.
			</p></div></div><section class="section" id="nw-ne-openshift-dns_understanding-networking"><div class="titlepage"><div><div><h2 class="title">2.1. OpenShift Container Platform DNS</h2></div></div></div><p>
				If you are running multiple services, such as front-end and back-end services for use with multiple pods, environment variables are created for user names, service IPs, and more so the front-end pods can communicate with the back-end services. If the service is deleted and recreated, a new IP address can be assigned to the service, and requires the front-end pods to be recreated to pick up the updated values for the service IP environment variable. Additionally, the back-end service must be created before any of the front-end pods to ensure that the service IP is generated properly, and that it can be provided to the front-end pods as an environment variable.
			</p><p>
				For this reason, OpenShift Container Platform has a built-in DNS so that the services can be reached by the service DNS as well as the service IP/port.
			</p></section><section class="section" id="nw-ne-openshift-ingress_understanding-networking"><div class="titlepage"><div><div><h2 class="title">2.2. OpenShift Container Platform Ingress Operator</h2></div></div></div><p>
				When you create your OpenShift Container Platform cluster, pods and services running on the cluster are each allocated their own IP addresses. The IP addresses are accessible to other pods and services running nearby but are not accessible to outside clients. The Ingress Operator implements the <code class="literal">IngressController</code> API and is the component responsible for enabling external access to OpenShift Container Platform cluster services.
			</p><p>
				The Ingress Operator makes it possible for external clients to access your service by deploying and managing one or more HAProxy-based <a class="link" href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/">Ingress Controllers</a> to handle routing. You can use the Ingress Operator to route traffic by specifying OpenShift Container Platform <code class="literal">Route</code> and Kubernetes <code class="literal">Ingress</code> resources. Configurations within the Ingress Controller, such as the ability to define <code class="literal">endpointPublishingStrategy</code> type and internal load balancing, provide ways to publish Ingress Controller endpoints.
			</p><section class="section" id="nw-ne-comparing-ingress-route_understanding-networking"><div class="titlepage"><div><div><h3 class="title">2.2.1. Comparing routes and Ingress</h3></div></div></div><p>
					The Kubernetes Ingress resource in OpenShift Container Platform implements the Ingress Controller with a shared router service that runs as a pod inside the cluster. The most common way to manage Ingress traffic is with the Ingress Controller. You can scale and replicate this pod like any other regular pod. This router service is based on <a class="link" href="http://www.haproxy.org/">HAProxy</a>, which is an open source load balancer solution.
				</p><p>
					The OpenShift Container Platform route provides Ingress traffic to services in the cluster. Routes provide advanced features that might not be supported by standard Kubernetes Ingress Controllers, such as TLS re-encryption, TLS passthrough, and split traffic for blue-green deployments.
				</p><p>
					Ingress traffic accesses services in the cluster through a route. Routes and Ingress are the main resources for handling Ingress traffic. Ingress provides features similar to a route, such as accepting external requests and delegating them based on the route. However, with Ingress you can only allow certain types of connections: HTTP/2, HTTPS and server name identification (SNI), and TLS with certificate. In OpenShift Container Platform, routes are generated to meet the conditions specified by the Ingress resource.
				</p></section></section><section class="section" id="nw-networking-glossary-terms_understanding-networking"><div class="titlepage"><div><div><h2 class="title">2.3. Glossary of common terms for OpenShift Container Platform networking</h2></div></div></div><p>
				This glossary defines common terms that are used in the networking content.
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">authentication</span></dt><dd>
							To control access to an OpenShift Container Platform cluster, a cluster administrator can configure user authentication and ensure only approved users access the cluster. To interact with an OpenShift Container Platform cluster, you must authenticate to the OpenShift Container Platform API. You can authenticate by providing an OAuth access token or an X.509 client certificate in your requests to the OpenShift Container Platform API.
						</dd><dt><span class="term">AWS Load Balancer Operator</span></dt><dd>
							The AWS Load Balancer (ALB) Operator deploys and manages an instance of the <code class="literal">aws-load-balancer-controller</code>.
						</dd><dt><span class="term">Cluster Network Operator</span></dt><dd>
							The Cluster Network Operator (CNO) deploys and manages the cluster network components in an OpenShift Container Platform cluster. This includes deployment of the Container Network Interface (CNI) network plugin selected for the cluster during installation.
						</dd><dt><span class="term">config map</span></dt><dd>
							A config map provides a way to inject configuration data into pods. You can reference the data stored in a config map in a volume of type <code class="literal">ConfigMap</code>. Applications running in a pod can use this data.
						</dd><dt><span class="term">custom resource (CR)</span></dt><dd>
							A CR is extension of the Kubernetes API. You can create custom resources.
						</dd><dt><span class="term">DNS</span></dt><dd>
							Cluster DNS is a DNS server which serves DNS records for Kubernetes services. Containers started by Kubernetes automatically include this DNS server in their DNS searches.
						</dd><dt><span class="term">DNS Operator</span></dt><dd>
							The DNS Operator deploys and manages CoreDNS to provide a name resolution service to pods. This enables DNS-based Kubernetes Service discovery in OpenShift Container Platform.
						</dd><dt><span class="term">deployment</span></dt><dd>
							A Kubernetes resource object that maintains the life cycle of an application.
						</dd><dt><span class="term">domain</span></dt><dd>
							Domain is a DNS name serviced by the Ingress Controller.
						</dd><dt><span class="term">egress</span></dt><dd>
							The process of data sharing externally through a network’s outbound traffic from a pod.
						</dd><dt><span class="term">External DNS Operator</span></dt><dd>
							The External DNS Operator deploys and manages ExternalDNS to provide the name resolution for services and routes from the external DNS provider to OpenShift Container Platform.
						</dd><dt><span class="term">HTTP-based route</span></dt><dd>
							An HTTP-based route is an unsecured route that uses the basic HTTP routing protocol and exposes a service on an unsecured application port.
						</dd><dt><span class="term">Ingress</span></dt><dd>
							The Kubernetes Ingress resource in OpenShift Container Platform implements the Ingress Controller with a shared router service that runs as a pod inside the cluster.
						</dd><dt><span class="term">Ingress Controller</span></dt><dd>
							The Ingress Operator manages Ingress Controllers. Using an Ingress Controller is the most common way to allow external access to an OpenShift Container Platform cluster.
						</dd><dt><span class="term">installer-provisioned infrastructure</span></dt><dd>
							The installation program deploys and configures the infrastructure that the cluster runs on.
						</dd><dt><span class="term">kubelet</span></dt><dd>
							A primary node agent that runs on each node in the cluster to ensure that containers are running in a pod.
						</dd><dt><span class="term">Kubernetes NMState Operator</span></dt><dd>
							The Kubernetes NMState Operator provides a Kubernetes API for performing state-driven network configuration across the OpenShift Container Platform cluster’s nodes with NMState.
						</dd><dt><span class="term">kube-proxy</span></dt><dd>
							Kube-proxy is a proxy service which runs on each node and helps in making services available to the external host. It helps in forwarding the request to correct containers and is capable of performing primitive load balancing.
						</dd><dt><span class="term">load balancers</span></dt><dd>
							OpenShift Container Platform uses load balancers for communicating from outside the cluster with services running in the cluster.
						</dd><dt><span class="term">MetalLB Operator</span></dt><dd>
							As a cluster administrator, you can add the MetalLB Operator to your cluster so that when a service of type <code class="literal">LoadBalancer</code> is added to the cluster, MetalLB can add an external IP address for the service.
						</dd><dt><span class="term">multicast</span></dt><dd>
							With IP multicast, data is broadcast to many IP addresses simultaneously.
						</dd><dt><span class="term">namespaces</span></dt><dd>
							A namespace isolates specific system resources that are visible to all processes. Inside a namespace, only processes that are members of that namespace can see those resources.
						</dd><dt><span class="term">networking</span></dt><dd>
							Network information of a OpenShift Container Platform cluster.
						</dd><dt><span class="term">node</span></dt><dd>
							A worker machine in the OpenShift Container Platform cluster. A node is either a virtual machine (VM) or a physical machine.
						</dd><dt><span class="term">OpenShift Container Platform Ingress Operator</span></dt><dd>
							The Ingress Operator implements the <code class="literal">IngressController</code> API and is the component responsible for enabling external access to OpenShift Container Platform services.
						</dd><dt><span class="term">pod</span></dt><dd>
							One or more containers with shared resources, such as volume and IP addresses, running in your OpenShift Container Platform cluster. A pod is the smallest compute unit defined, deployed, and managed.
						</dd><dt><span class="term">PTP Operator</span></dt><dd>
							The PTP Operator creates and manages the <code class="literal">linuxptp</code> services.
						</dd><dt><span class="term">route</span></dt><dd>
							The OpenShift Container Platform route provides Ingress traffic to services in the cluster. Routes provide advanced features that might not be supported by standard Kubernetes Ingress Controllers, such as TLS re-encryption, TLS passthrough, and split traffic for blue-green deployments.
						</dd><dt><span class="term">scaling</span></dt><dd>
							Increasing or decreasing the resource capacity.
						</dd><dt><span class="term">service</span></dt><dd>
							Exposes a running application on a set of pods.
						</dd><dt><span class="term">Single Root I/O Virtualization (SR-IOV) Network Operator</span></dt><dd>
							The Single Root I/O Virtualization (SR-IOV) Network Operator manages the SR-IOV network devices and network attachments in your cluster.
						</dd><dt><span class="term">software-defined networking (SDN)</span></dt><dd>
							OpenShift Container Platform uses a software-defined networking (SDN) approach to provide a unified cluster network that enables communication between pods across the OpenShift Container Platform cluster.
						</dd><dt><span class="term">Stream Control Transmission Protocol (SCTP)</span></dt><dd>
							SCTP is a reliable message based protocol that runs on top of an IP network.
						</dd><dt><span class="term">taint</span></dt><dd>
							Taints and tolerations ensure that pods are scheduled onto appropriate nodes. You can apply one or more taints on a node.
						</dd><dt><span class="term">toleration</span></dt><dd>
							You can apply tolerations to pods. Tolerations allow the scheduler to schedule pods with matching taints.
						</dd><dt><span class="term">web console</span></dt><dd>
							A user interface (UI) to manage OpenShift Container Platform.
						</dd></dl></div></section></section><section class="chapter" id="accessing-hosts"><div class="titlepage"><div><div><h1 class="title">Chapter 3. Accessing hosts</h1></div></div></div><p>
			Learn how to create a bastion host to access OpenShift Container Platform instances and access the control plane nodes with secure shell (SSH) access.
		</p><section class="section" id="accessing-hosts-on-aws_accessing-hosts"><div class="titlepage"><div><div><h2 class="title">3.1. Accessing hosts on Amazon Web Services in an installer-provisioned infrastructure cluster</h2></div></div></div><p>
				The OpenShift Container Platform installer does not create any public IP addresses for any of the Amazon Elastic Compute Cloud (Amazon EC2) instances that it provisions for your OpenShift Container Platform cluster. To be able to SSH to your OpenShift Container Platform hosts, you must follow this procedure.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Create a security group that allows SSH access into the virtual private cloud (VPC) created by the <code class="literal">openshift-install</code> command.
					</li><li class="listitem">
						Create an Amazon EC2 instance on one of the public subnets the installer created.
					</li><li class="listitem"><p class="simpara">
						Associate a public IP address with the Amazon EC2 instance that you created.
					</p><p class="simpara">
						Unlike with the OpenShift Container Platform installation, you should associate the Amazon EC2 instance you created with an SSH keypair. It does not matter what operating system you choose for this instance, as it will simply serve as an SSH bastion to bridge the internet into your OpenShift Container Platform cluster’s VPC. The Amazon Machine Image (AMI) you use does matter. With Red Hat Enterprise Linux CoreOS (RHCOS), for example, you can provide keys via Ignition, like the installer does.
					</p></li><li class="listitem"><p class="simpara">
						After you provisioned your Amazon EC2 instance and can SSH into it, you must add the SSH key that you associated with your OpenShift Container Platform installation. This key can be different from the key for the bastion instance, but does not have to be.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Direct SSH access is only recommended for disaster recovery. When the Kubernetes API is responsive, run privileged pods instead.
						</p></div></div></li><li class="listitem">
						Run <code class="literal">oc get nodes</code>, inspect the output, and choose one of the nodes that is a master. The hostname looks similar to <code class="literal">ip-10-0-1-163.ec2.internal</code>.
					</li><li class="listitem"><p class="simpara">
						From the bastion SSH host you manually deployed into Amazon EC2, SSH into that control plane host. Ensure that you use the same SSH key you specified during the installation:
					</p><pre class="programlisting language-terminal">$ ssh -i &lt;ssh-key-path&gt; core@&lt;master-hostname&gt;</pre></li></ol></div></section></section><section class="chapter" id="networking-operators-overview"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Networking Operators overview</h1></div></div></div><p>
			OpenShift Container Platform supports multiple types of networking Operators. You can manage the cluster networking using these networking Operators.
		</p><section class="section" id="networking-operators-overview-cluster-network-operator"><div class="titlepage"><div><div><h2 class="title">4.1. Cluster Network Operator</h2></div></div></div><p>
				The Cluster Network Operator (CNO) deploys and manages the cluster network components in an OpenShift Container Platform cluster. This includes deployment of the Container Network Interface (CNI) network plugin selected for the cluster during installation. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#cluster-network-operator">Cluster Network Operator in OpenShift Container Platform</a>.
			</p></section><section class="section" id="networking-operators-overview-dns-operator"><div class="titlepage"><div><div><h2 class="title">4.2. DNS Operator</h2></div></div></div><p>
				The DNS Operator deploys and manages CoreDNS to provide a name resolution service to pods. This enables DNS-based Kubernetes Service discovery in OpenShift Container Platform. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#dns-operator">DNS Operator in OpenShift Container Platform</a>.
			</p></section><section class="section" id="networking-operators-overview-ingress-operator"><div class="titlepage"><div><div><h2 class="title">4.3. Ingress Operator</h2></div></div></div><p>
				When you create your OpenShift Container Platform cluster, pods and services running on the cluster are each allocated IP addresses. The IP addresses are accessible to other pods and services running nearby but are not accessible to external clients. The Ingress Operator implements the Ingress Controller API and is responsible for enabling external access to OpenShift Container Platform cluster services. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-ingress">Ingress Operator in OpenShift Container Platform</a>.
			</p></section><section class="section" id="networking-operators-overview-external-dns-operator"><div class="titlepage"><div><div><h2 class="title">4.4. External DNS Operator</h2></div></div></div><p>
				The External DNS Operator deploys and manages ExternalDNS to provide the name resolution for services and routes from the external DNS provider to OpenShift Container Platform. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#external-dns-operator">Understanding the External DNS Operator</a>.
			</p></section><section class="section" id="ingress-node-firewall-operator-1"><div class="titlepage"><div><div><h2 class="title">4.5. Ingress Node Firewall Operator</h2></div></div></div><p>
				The Ingress Node Firewall Operator uses an extended Berkley Packet Filter (eBPF) and eXpress Data Path (XDP) plugin to process node firewall rules, update statistics and generate events for dropped traffic. The operator manages ingress node firewall resources, verifies firewall configuration, does not allow incorrectly configured rules that can prevent cluster access, and loads ingress node firewall XDP programs to the selected interfaces in the rule’s object(s). For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#ingress-node-firewall-operator">Understanding the Ingress Node Firewall Operator</a>
			</p></section><section class="section" id="network-observability-operator-overview-operator"><div class="titlepage"><div><div><h2 class="title">4.6. Network Observability Operator</h2></div></div></div><p>
				The Network Observability Operator is an optional Operator that allows cluster administrators to observe the network traffic for OpenShift Container Platform clusters. The Network Observability Operator uses the eBPF technology to create network flows. The network flows are then enriched with OpenShift Container Platform information and stored in Loki. You can view and analyze the stored network flows information in the OpenShift Container Platform console for further insight and troubleshooting. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#dependency-network-observability">About Network Observability Operator</a>.
			</p></section></section><section class="chapter" id="cluster-network-operator"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Cluster Network Operator in OpenShift Container Platform</h1></div></div></div><p>
			The Cluster Network Operator (CNO) deploys and manages the cluster network components on an OpenShift Container Platform cluster, including the Container Network Interface (CNI) network plugin selected for the cluster during installation.
		</p><section class="section" id="nw-cluster-network-operator_cluster-network-operator"><div class="titlepage"><div><div><h2 class="title">5.1. Cluster Network Operator</h2></div></div></div><p>
				The Cluster Network Operator implements the <code class="literal">network</code> API from the <code class="literal">operator.openshift.io</code> API group. The Operator deploys the OVN-Kubernetes network plugin, or the network provider plugin that you selected during cluster installation, by using a daemon set.
			</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					The Cluster Network Operator is deployed during installation as a Kubernetes <code class="literal">Deployment</code>.
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Run the following command to view the Deployment status:
					</p><pre class="programlisting language-terminal">$ oc get -n openshift-network-operator deployment/network-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME               READY   UP-TO-DATE   AVAILABLE   AGE
network-operator   1/1     1            1           56m</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Run the following command to view the state of the Cluster Network Operator:
					</p><pre class="programlisting language-terminal">$ oc get clusteroperator/network</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME      VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
network   4.5.4     True        False         False      50m</pre>

						</p></div><p class="simpara">
						The following fields provide information about the status of the operator: <code class="literal">AVAILABLE</code>, <code class="literal">PROGRESSING</code>, and <code class="literal">DEGRADED</code>. The <code class="literal">AVAILABLE</code> field is <code class="literal">True</code> when the Cluster Network Operator reports an available status condition.
					</p></li></ol></div></section><section class="section" id="nw-cno-view_cluster-network-operator"><div class="titlepage"><div><div><h2 class="title">5.2. Viewing the cluster network configuration</h2></div></div></div><p>
				Every new OpenShift Container Platform installation has a <code class="literal">network.config</code> object named <code class="literal">cluster</code>.
			</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Use the <code class="literal">oc describe</code> command to view the cluster network configuration:
					</p><pre class="programlisting language-terminal">$ oc describe network.config/cluster</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">Name:         cluster
Namespace:
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  config.openshift.io/v1
Kind:         Network
Metadata:
  Self Link:           /apis/config.openshift.io/v1/networks/cluster
Spec: <span id="CO1-1"><!--Empty--></span><span class="callout">1</span>
  Cluster Network:
    Cidr:         10.128.0.0/14
    Host Prefix:  23
  Network Type:   OpenShiftSDN
  Service Network:
    172.30.0.0/16
Status: <span id="CO1-2"><!--Empty--></span><span class="callout">2</span>
  Cluster Network:
    Cidr:               10.128.0.0/14
    Host Prefix:        23
  Cluster Network MTU:  8951
  Network Type:         OpenShiftSDN
  Service Network:
    172.30.0.0/16
Events:  &lt;none&gt;</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO1-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The <code class="literal">Spec</code> field displays the configured state of the cluster network.
							</div></dd><dt><a href="#CO1-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The <code class="literal">Status</code> field displays the current state of the cluster network configuration.
							</div></dd></dl></div></li></ul></div></section><section class="section" id="nw-cno-status_cluster-network-operator"><div class="titlepage"><div><div><h2 class="title">5.3. Viewing Cluster Network Operator status</h2></div></div></div><p>
				You can inspect the status and view the details of the Cluster Network Operator using the <code class="literal">oc describe</code> command.
			</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Run the following command to view the status of the Cluster Network Operator:
					</p><pre class="programlisting language-terminal">$ oc describe clusteroperators/network</pre></li></ul></div></section><section class="section" id="nw-cno-logs_cluster-network-operator"><div class="titlepage"><div><div><h2 class="title">5.4. Viewing Cluster Network Operator logs</h2></div></div></div><p>
				You can view Cluster Network Operator logs by using the <code class="literal">oc logs</code> command.
			</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Run the following command to view the logs of the Cluster Network Operator:
					</p><pre class="programlisting language-terminal">$ oc logs --namespace=openshift-network-operator deployment/network-operator</pre></li></ul></div></section><section class="section" id="nw-operator-cr_cluster-network-operator"><div class="titlepage"><div><div><h2 class="title">5.5. Cluster Network Operator configuration</h2></div></div></div><p>
				The configuration for the cluster network is specified as part of the Cluster Network Operator (CNO) configuration and stored in a custom resource (CR) object that is named <code class="literal">cluster</code>. The CR specifies the fields for the <code class="literal">Network</code> API in the <code class="literal">operator.openshift.io</code> API group.
			</p><p>
				The CNO configuration inherits the following fields during cluster installation from the <code class="literal">Network</code> API in the <code class="literal">Network.config.openshift.io</code> API group and these fields cannot be changed:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">clusterNetwork</code></span></dt><dd>
							IP address pools from which pod IP addresses are allocated.
						</dd><dt><span class="term"><code class="literal">serviceNetwork</code></span></dt><dd>
							IP address pool for services.
						</dd><dt><span class="term"><code class="literal">defaultNetwork.type</code></span></dt><dd>
							Cluster network plugin, such as OpenShift SDN or OVN-Kubernetes.
						</dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					After cluster installation, you cannot modify the fields listed in the previous section.
				</p></div></div><p>
				You can specify the cluster network plugin configuration for your cluster by setting the fields for the <code class="literal">defaultNetwork</code> object in the CNO object named <code class="literal">cluster</code>.
			</p><section class="section" id="nw-operator-cr-cno-object_cluster-network-operator"><div class="titlepage"><div><div><h3 class="title">5.5.1. Cluster Network Operator configuration object</h3></div></div></div><p>
					The fields for the Cluster Network Operator (CNO) are described in the following table:
				</p><div class="table" id="idm140587174130144"><p class="title"><strong>Table 5.1. Cluster Network Operator configuration object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587140822256" scope="col">Field</th><th align="left" valign="middle" id="idm140587140821168" scope="col">Type</th><th align="left" valign="middle" id="idm140587140820080" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587140822256"> <p>
									<code class="literal">metadata.name</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587140821168"> <p>
									<code class="literal">string</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587140820080"> <p>
									The name of the CNO object. This name is always <code class="literal">cluster</code>.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587140822256"> <p>
									<code class="literal">spec.clusterNetwork</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587140821168"> <p>
									<code class="literal">array</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587140820080"> <p>
									A list specifying the blocks of IP addresses from which pod IP addresses are allocated and the subnet prefix length assigned to each individual node in the cluster. For example:
								</p>
								 
<pre class="programlisting language-yaml">spec:
  clusterNetwork:
  - cidr: 10.128.0.0/19
    hostPrefix: 23
  - cidr: 10.128.32.0/19
    hostPrefix: 23</pre>
								 <p>
									This value is ready-only and inherited from the <code class="literal">Network.config.openshift.io</code> object named <code class="literal">cluster</code> during cluster installation.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587140822256"> <p>
									<code class="literal">spec.serviceNetwork</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587140821168"> <p>
									<code class="literal">array</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587140820080"> <p>
									A block of IP addresses for services. The OpenShift SDN and OVN-Kubernetes network plugins support only a single IP address block for the service network. For example:
								</p>
								 
<pre class="programlisting language-yaml">spec:
  serviceNetwork:
  - 172.30.0.0/14</pre>
								 <p>
									This value is ready-only and inherited from the <code class="literal">Network.config.openshift.io</code> object named <code class="literal">cluster</code> during cluster installation.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587140822256"> <p>
									<code class="literal">spec.defaultNetwork</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587140821168"> <p>
									<code class="literal">object</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587140820080"> <p>
									Configures the network plugin for the cluster network.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587140822256"> <p>
									<code class="literal">spec.kubeProxyConfig</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587140821168"> <p>
									<code class="literal">object</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587140820080"> <p>
									The fields for this object specify the kube-proxy configuration. If you are using the OVN-Kubernetes cluster network plugin, the kube-proxy configuration has no effect.
								</p>
								 </td></tr></tbody></table></div></div><h5 id="nw-operator-cr-defaultnetwork_cluster-network-operator">defaultNetwork object configuration</h5><p>
					The values for the <code class="literal">defaultNetwork</code> object are defined in the following table:
				</p><div class="table" id="idm140587153103456"><p class="title"><strong>Table 5.2. <code class="literal">defaultNetwork</code> object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 30%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 50%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587153342720" scope="col">Field</th><th align="left" valign="middle" id="idm140587153341632" scope="col">Type</th><th align="left" valign="middle" id="idm140587153340544" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587153342720"> <p>
									<code class="literal">type</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587153341632"> <p>
									<code class="literal">string</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587153340544"> <p>
									Either <code class="literal">OpenShiftSDN</code> or <code class="literal">OVNKubernetes</code>. The Red Hat OpenShift Networking network plugin is selected during installation. This value cannot be changed after cluster installation.
								</p>
								 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
										OpenShift Container Platform uses the OVN-Kubernetes network plugin by default.
									</p></div></div>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587153342720"> <p>
									<code class="literal">openshiftSDNConfig</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587153341632"> <p>
									<code class="literal">object</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587153340544"> <p>
									This object is only valid for the OpenShift SDN network plugin.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587153342720"> <p>
									<code class="literal">ovnKubernetesConfig</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587153341632"> <p>
									<code class="literal">object</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587153340544"> <p>
									This object is only valid for the OVN-Kubernetes network plugin.
								</p>
								 </td></tr></tbody></table></div></div><h6 id="nw-operator-configuration-parameters-for-openshift-sdn_cluster-network-operator">Configuration for the OpenShift SDN network plugin</h6><p>
					The following table describes the configuration fields for the OpenShift SDN network plugin:
				</p><div class="table" id="idm140587148124176"><p class="title"><strong>Table 5.3. <code class="literal">openshiftSDNConfig</code> object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587150817424" scope="col">Field</th><th align="left" valign="middle" id="idm140587150816336" scope="col">Type</th><th align="left" valign="middle" id="idm140587150815248" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587150817424"> <p>
									<code class="literal">mode</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587150816336"> <p>
									<code class="literal">string</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587150815248"> <p>
									The network isolation mode for OpenShift SDN.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587150817424"> <p>
									<code class="literal">mtu</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587150816336"> <p>
									<code class="literal">integer</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587150815248"> <p>
									The maximum transmission unit (MTU) for the VXLAN overlay network. This value is normally configured automatically.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587150817424"> <p>
									<code class="literal">vxlanPort</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587150816336"> <p>
									<code class="literal">integer</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587150815248"> <p>
									The port to use for all VXLAN packets. The default value is <code class="literal">4789</code>.
								</p>
								 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can only change the configuration for your cluster network plugin during cluster installation.
					</p></div></div><div class="formalpara"><p class="title"><strong>Example OpenShift SDN configuration</strong></p><p>
						
<pre class="programlisting language-yaml">defaultNetwork:
  type: OpenShiftSDN
  openshiftSDNConfig:
    mode: NetworkPolicy
    mtu: 1450
    vxlanPort: 4789</pre>

					</p></div><h6 id="nw-operator-configuration-parameters-for-ovn-sdn_cluster-network-operator">Configuration for the OVN-Kubernetes network plugin</h6><p>
					The following table describes the configuration fields for the OVN-Kubernetes network plugin:
				</p><div class="table" id="idm140587159059360"><p class="title"><strong>Table 5.4. <code class="literal">ovnKubernetesConfig</code> object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587139572336" scope="col">Field</th><th align="left" valign="middle" id="idm140587139571248" scope="col">Type</th><th align="left" valign="middle" id="idm140587139570160" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587139572336"> <p>
									<code class="literal">mtu</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587139571248"> <p>
									<code class="literal">integer</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587139570160"> <p>
									The maximum transmission unit (MTU) for the Geneve (Generic Network Virtualization Encapsulation) overlay network. This value is normally configured automatically.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587139572336"> <p>
									<code class="literal">genevePort</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587139571248"> <p>
									<code class="literal">integer</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587139570160"> <p>
									The UDP port for the Geneve overlay network.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587139572336"> <p>
									<code class="literal">ipsecConfig</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587139571248"> <p>
									<code class="literal">object</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587139570160"> <p>
									If the field is present, IPsec is enabled for the cluster.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587139572336"> <p>
									<code class="literal">policyAuditConfig</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587139571248"> <p>
									<code class="literal">object</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587139570160"> <p>
									Specify a configuration object for customizing network policy audit logging. If unset, the defaults audit log settings are used.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587139572336"> <p>
									<code class="literal">gatewayConfig</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587139571248"> <p>
									<code class="literal">object</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587139570160"> <p>
									Optional: Specify a configuration object for customizing how egress traffic is sent to the node gateway.
								</p>
								 <div class="admonition note"><div class="admonition_header">Note</div><div><pre class="literallayout">While migrating egress traffic, you can expect some disruption to workloads and service traffic until the Cluster Network Operator (CNO) successfully rolls out the changes.</pre></div></div>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587139572336"> <p>
									<code class="literal">v4InternalSubnet</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587139571248"> <p>
									If your existing network infrastructure overlaps with the <code class="literal">100.64.0.0/16</code> IPv4 subnet, you can specify a different IP address range for internal use by OVN-Kubernetes. You must ensure that the IP address range does not overlap with any other subnet used by your OpenShift Container Platform installation. The IP address range must be larger than the maximum number of nodes that can be added to the cluster. For example, if the <code class="literal">clusterNetwork.cidr</code> value is <code class="literal">10.128.0.0/14</code> and the <code class="literal">clusterNetwork.hostPrefix</code> value is <code class="literal">/23</code>, then the maximum number of nodes is <code class="literal">2^(23-14)=512</code>.
								</p>
								 <p>
									This field cannot be changed after installation.
								</p>
								 </td><td align="left" valign="middle" headers="idm140587139570160"> <p>
									The default value is <code class="literal">100.64.0.0/16</code>.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587139572336"> <p>
									<code class="literal">v6InternalSubnet</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587139571248"> <p>
									If your existing network infrastructure overlaps with the <code class="literal">fd98::/48</code> IPv6 subnet, you can specify a different IP address range for internal use by OVN-Kubernetes. You must ensure that the IP address range does not overlap with any other subnet used by your OpenShift Container Platform installation. The IP address range must be larger than the maximum number of nodes that can be added to the cluster.
								</p>
								 <p>
									This field cannot be changed after installation.
								</p>
								 </td><td align="left" valign="middle" headers="idm140587139570160"> <p>
									The default value is <code class="literal">fd98::/48</code>.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140587138461552"><p class="title"><strong>Table 5.5. <code class="literal">policyAuditConfig</code> object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587150026352" scope="col">Field</th><th align="left" valign="middle" id="idm140587150025264" scope="col">Type</th><th align="left" valign="middle" id="idm140587150024176" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587150026352"> <p>
									<code class="literal">rateLimit</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587150025264"> <p>
									integer
								</p>
								 </td><td align="left" valign="middle" headers="idm140587150024176"> <p>
									The maximum number of messages to generate every second per node. The default value is <code class="literal">20</code> messages per second.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587150026352"> <p>
									<code class="literal">maxFileSize</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587150025264"> <p>
									integer
								</p>
								 </td><td align="left" valign="middle" headers="idm140587150024176"> <p>
									The maximum size for the audit log in bytes. The default value is <code class="literal">50000000</code> or 50 MB.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587150026352"> <p>
									<code class="literal">destination</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587150025264"> <p>
									string
								</p>
								 </td><td align="left" valign="middle" headers="idm140587150024176"> <p>
									One of the following additional audit log targets:
								</p>
								 <div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">libc</code></span></dt><dd>
												The libc <code class="literal">syslog()</code> function of the journald process on the host.
											</dd><dt><span class="term"><code class="literal">udp:&lt;host&gt;:&lt;port&gt;</code></span></dt><dd>
												A syslog server. Replace <code class="literal">&lt;host&gt;:&lt;port&gt;</code> with the host and port of the syslog server.
											</dd><dt><span class="term"><code class="literal">unix:&lt;file&gt;</code></span></dt><dd>
												A Unix Domain Socket file specified by <code class="literal">&lt;file&gt;</code>.
											</dd><dt><span class="term"><code class="literal">null</code></span></dt><dd>
												Do not send the audit logs to any additional target.
											</dd></dl></div>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587150026352"> <p>
									<code class="literal">syslogFacility</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587150025264"> <p>
									string
								</p>
								 </td><td align="left" valign="middle" headers="idm140587150024176"> <p>
									The syslog facility, such as <code class="literal">kern</code>, as defined by RFC5424. The default value is <code class="literal">local0</code>.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="gatewayConfig-object_cluster-network-operator"><p class="title"><strong>Table 5.6. <code class="literal">gatewayConfig</code> object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587170800128" scope="col">Field</th><th align="left" valign="middle" id="idm140587170799040" scope="col">Type</th><th align="left" valign="middle" id="idm140587170797952" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587170800128"> <p>
									<code class="literal">routingViaHost</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587170799040"> <p>
									<code class="literal">boolean</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587170797952"> <p>
									Set this field to <code class="literal">true</code> to send egress traffic from pods to the host networking stack. For highly-specialized installations and applications that rely on manually configured routes in the kernel routing table, you might want to route egress traffic to the host networking stack. By default, egress traffic is processed in OVN to exit the cluster and is not affected by specialized routes in the kernel routing table. The default value is <code class="literal">false</code>.
								</p>
								 <p>
									This field has an interaction with the Open vSwitch hardware offloading feature. If you set this field to <code class="literal">true</code>, you do not receive the performance benefits of the offloading because egress traffic is processed by the host networking stack.
								</p>
								 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can only change the configuration for your cluster network plugin during cluster installation, except for the <code class="literal">gatewayConfig</code> field that can be changed at runtime as a post-installation activity.
					</p></div></div><div class="formalpara"><p class="title"><strong>Example OVN-Kubernetes configuration with IPSec enabled</strong></p><p>
						
<pre class="programlisting language-yaml">defaultNetwork:
  type: OVNKubernetes
  ovnKubernetesConfig:
    mtu: 1400
    genevePort: 6081
    ipsecConfig: {}</pre>

					</p></div><h5 id="nw-operator-cr-kubeproxyconfig_cluster-network-operator">kubeProxyConfig object configuration</h5><p>
					The values for the <code class="literal">kubeProxyConfig</code> object are defined in the following table:
				</p><div class="table" id="idm140587123836192"><p class="title"><strong>Table 5.7. <code class="literal">kubeProxyConfig</code> object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 30%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 50%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587144430688" scope="col">Field</th><th align="left" valign="middle" id="idm140587144429600" scope="col">Type</th><th align="left" valign="middle" id="idm140587144428512" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587144430688"> <p>
									<code class="literal">iptablesSyncPeriod</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587144429600"> <p>
									<code class="literal">string</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587144428512"> <p>
									The refresh period for <code class="literal">iptables</code> rules. The default value is <code class="literal">30s</code>. Valid suffixes include <code class="literal">s</code>, <code class="literal">m</code>, and <code class="literal">h</code> and are described in the <a class="link" href="https://golang.org/pkg/time/#ParseDuration">Go <code class="literal">time</code> package</a> documentation.
								</p>
								 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Because of performance improvements introduced in OpenShift Container Platform 4.3 and greater, adjusting the <code class="literal">iptablesSyncPeriod</code> parameter is no longer necessary.
									</p></div></div>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587144430688"> <p>
									<code class="literal">proxyArguments.iptables-min-sync-period</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587144429600"> <p>
									<code class="literal">array</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587144428512"> <p>
									The minimum duration before refreshing <code class="literal">iptables</code> rules. This field ensures that the refresh does not happen too frequently. Valid suffixes include <code class="literal">s</code>, <code class="literal">m</code>, and <code class="literal">h</code> and are described in the <a class="link" href="https://golang.org/pkg/time/#ParseDuration">Go <code class="literal">time</code> package</a>. The default value is:
								</p>
								 
<pre class="programlisting language-yaml">kubeProxyConfig:
  proxyArguments:
    iptables-min-sync-period:
    - 0s</pre>
								 </td></tr></tbody></table></div></div></section><section class="section" id="nw-operator-example-cr_cluster-network-operator"><div class="titlepage"><div><div><h3 class="title">5.5.2. Cluster Network Operator example configuration</h3></div></div></div><p>
					A complete CNO configuration is specified in the following example:
				</p><div class="formalpara"><p class="title"><strong>Example Cluster Network Operator object</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  clusterNetwork: <span id="CO2-1"><!--Empty--></span><span class="callout">1</span>
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  serviceNetwork: <span id="CO2-2"><!--Empty--></span><span class="callout">2</span>
  - 172.30.0.0/16
  defaultNetwork: <span id="CO2-3"><!--Empty--></span><span class="callout">3</span>
    type: OpenShiftSDN
    openshiftSDNConfig:
      mode: NetworkPolicy
      mtu: 1450
      vxlanPort: 4789
  kubeProxyConfig:
    iptablesSyncPeriod: 30s
    proxyArguments:
      iptables-min-sync-period:
      - 0s</pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO2-1"><span class="callout">1</span></a> <a href="#CO2-2"><span class="callout">2</span></a> <a href="#CO2-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Configured only during cluster installation.
						</div></dd></dl></div></section></section><section class="section _additional-resources" id="cluster-network-operator-additional-resources"><div class="titlepage"><div><div><h2 class="title">5.6. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/api_reference/#network-operator-openshift-io-v1"><code class="literal">Network</code> API in the <code class="literal">operator.openshift.io</code> API group</a>
					</li></ul></div></section></section><section class="chapter" id="dns-operator"><div class="titlepage"><div><div><h1 class="title">Chapter 6. DNS Operator in OpenShift Container Platform</h1></div></div></div><p>
			The DNS Operator deploys and manages CoreDNS to provide a name resolution service to pods, enabling DNS-based Kubernetes Service discovery in OpenShift Container Platform.
		</p><section class="section" id="nw-dns-operator_dns-operator"><div class="titlepage"><div><div><h2 class="title">6.1. DNS Operator</h2></div></div></div><p>
				The DNS Operator implements the <code class="literal">dns</code> API from the <code class="literal">operator.openshift.io</code> API group. The Operator deploys CoreDNS using a daemon set, creates a service for the daemon set, and configures the kubelet to instruct pods to use the CoreDNS service IP address for name resolution.
			</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					The DNS Operator is deployed during installation with a <code class="literal">Deployment</code> object.
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Use the <code class="literal">oc get</code> command to view the deployment status:
					</p><pre class="programlisting language-terminal">$ oc get -n openshift-dns-operator deployment/dns-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME           READY     UP-TO-DATE   AVAILABLE   AGE
dns-operator   1/1       1            1           23h</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Use the <code class="literal">oc get</code> command to view the state of the DNS Operator:
					</p><pre class="programlisting language-terminal">$ oc get clusteroperator/dns</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME      VERSION     AVAILABLE   PROGRESSING   DEGRADED   SINCE
dns       4.1.0-0.11  True        False         False      92m</pre>

						</p></div><p class="simpara">
						<code class="literal">AVAILABLE</code>, <code class="literal">PROGRESSING</code> and <code class="literal">DEGRADED</code> provide information about the status of the operator. <code class="literal">AVAILABLE</code> is <code class="literal">True</code> when at least 1 pod from the CoreDNS daemon set reports an <code class="literal">Available</code> status condition.
					</p></li></ol></div></section><section class="section" id="nw-dns-operator-managementState_dns-operator"><div class="titlepage"><div><div><h2 class="title">6.2. Changing the DNS Operator managementState</h2></div></div></div><p>
				DNS manages the CoreDNS component to provide a name resolution service for pods and services in the cluster. The <code class="literal">managementState</code> of the DNS Operator is set to <code class="literal">Managed</code> by default, which means that the DNS Operator is actively managing its resources. You can change it to <code class="literal">Unmanaged</code>, which means the DNS Operator is not managing its resources.
			</p><p>
				The following are use cases for changing the DNS Operator <code class="literal">managementState</code>:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						You are a developer and want to test a configuration change to see if it fixes an issue in CoreDNS. You can stop the DNS Operator from overwriting the fix by setting the <code class="literal">managementState</code> to <code class="literal">Unmanaged</code>.
					</li><li class="listitem">
						You are a cluster administrator and have reported an issue with CoreDNS, but need to apply a workaround until the issue is fixed. You can set the <code class="literal">managementState</code> field of the DNS Operator to <code class="literal">Unmanaged</code> to apply the workaround.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Change <code class="literal">managementState</code> DNS Operator:
					</p><pre class="programlisting language-terminal">oc patch dns.operator.openshift.io default --type merge --patch '{"spec":{"managementState":"Unmanaged"}}'</pre></li></ul></div></section><section class="section" id="nw-controlling-dns-pod-placement_dns-operator"><div class="titlepage"><div><div><h2 class="title">6.3. Controlling DNS pod placement</h2></div></div></div><p>
				The DNS Operator has two daemon sets: one for CoreDNS and one for managing the <code class="literal">/etc/hosts</code> file. The daemon set for <code class="literal">/etc/hosts</code> must run on every node host to add an entry for the cluster image registry to support pulling images. Security policies can prohibit communication between pairs of nodes, which prevents the daemon set for CoreDNS from running on every node.
			</p><p>
				As a cluster administrator, you can use a custom node selector to configure the daemon set for CoreDNS to run or not run on certain nodes.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You installed the <code class="literal">oc</code> CLI.
					</li><li class="listitem">
						You are logged in to the cluster with a user with <code class="literal">cluster-admin</code> privileges.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						To prevent communication between certain nodes, configure the <code class="literal">spec.nodePlacement.nodeSelector</code> API field:
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Modify the DNS Operator object named <code class="literal">default</code>:
							</p><pre class="programlisting language-terminal">$ oc edit dns.operator/default</pre></li><li class="listitem"><p class="simpara">
								Specify a node selector that includes only control plane nodes in the <code class="literal">spec.nodePlacement.nodeSelector</code> API field:
							</p><pre class="programlisting language-yaml"> spec:
   nodePlacement:
     nodeSelector:
       node-role.kubernetes.io/worker: ""</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						To allow the daemon set for CoreDNS to run on nodes, configure a taint and toleration:
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Modify the DNS Operator object named <code class="literal">default</code>:
							</p><pre class="programlisting language-terminal">$ oc edit dns.operator/default</pre></li><li class="listitem"><p class="simpara">
								Specify a taint key and a toleration for the taint:
							</p><pre class="programlisting language-yaml"> spec:
   nodePlacement:
     tolerations:
     - effect: NoExecute
       key: "dns-only"
       operators: Equal
       value: abc
       tolerationSeconds: 3600 <span id="CO3-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO3-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										If the taint is <code class="literal">dns-only</code>, it can be tolerated indefinitely. You can omit <code class="literal">tolerationSeconds</code>.
									</div></dd></dl></div></li></ol></div></li></ul></div></section><section class="section" id="nw-dns-view_dns-operator"><div class="titlepage"><div><div><h2 class="title">6.4. View the default DNS</h2></div></div></div><p>
				Every new OpenShift Container Platform installation has a <code class="literal">dns.operator</code> named <code class="literal">default</code>.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Use the <code class="literal">oc describe</code> command to view the default <code class="literal">dns</code>:
					</p><pre class="programlisting language-terminal">$ oc describe dns.operator/default</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">Name:         default
Namespace:
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  operator.openshift.io/v1
Kind:         DNS
...
Status:
  Cluster Domain:  cluster.local <span id="CO4-1"><!--Empty--></span><span class="callout">1</span>
  Cluster IP:      172.30.0.10 <span id="CO4-2"><!--Empty--></span><span class="callout">2</span>
...</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO4-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The Cluster Domain field is the base DNS domain used to construct fully qualified pod and service domain names.
							</div></dd><dt><a href="#CO4-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The Cluster IP is the address pods query for name resolution. The IP is defined as the 10th address in the service CIDR range.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						To find the service CIDR of your cluster, use the <code class="literal">oc get</code> command:
					</p><pre class="programlisting language-terminal">$ oc get networks.config/cluster -o jsonpath='{$.status.serviceNetwork}'</pre></li></ol></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
					
<pre class="programlisting language-terminal">[172.30.0.0/16]</pre>

				</p></div></section><section class="section" id="nw-dns-forward_dns-operator"><div class="titlepage"><div><div><h2 class="title">6.5. Using DNS forwarding</h2></div></div></div><p>
				You can use DNS forwarding to override the default forwarding configuration in the <code class="literal">/etc/resolv.conf</code> file in the following ways:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Specify name servers for every zone. If the forwarded zone is the Ingress domain managed by OpenShift Container Platform, then the upstream name server must be authorized for the domain.
					</li><li class="listitem">
						Provide a list of upstream DNS servers.
					</li><li class="listitem">
						Change the default forwarding policy.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					A DNS forwarding configuration for the default domain can have both the default servers specified in the <code class="literal">/etc/resolv.conf</code> file and the upstream DNS servers.
				</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Modify the DNS Operator object named <code class="literal">default</code>:
					</p><pre class="programlisting language-terminal">$ oc edit dns.operator/default</pre><p class="simpara">
						After you issue the previous command, the Operator creates and updates the config map named <code class="literal">dns-default</code> with additional server configuration blocks based on <code class="literal">Server</code>. If none of the servers have a zone that matches the query, then name resolution falls back to the upstream DNS servers.
					</p><div class="formalpara"><p class="title"><strong>Configuring DNS forwarding</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: DNS
metadata:
  name: default
spec:
  servers:
  - name: example-server <span id="CO5-1"><!--Empty--></span><span class="callout">1</span>
    zones: <span id="CO5-2"><!--Empty--></span><span class="callout">2</span>
    - example.com
    forwardPlugin:
      policy: Random <span id="CO5-3"><!--Empty--></span><span class="callout">3</span>
      upstreams: <span id="CO5-4"><!--Empty--></span><span class="callout">4</span>
      - 1.1.1.1
      - 2.2.2.2:5353
  upstreamResolvers: <span id="CO5-5"><!--Empty--></span><span class="callout">5</span>
    policy: Random <span id="CO5-6"><!--Empty--></span><span class="callout">6</span>
    upstreams: <span id="CO5-7"><!--Empty--></span><span class="callout">7</span>
    - type: SystemResolvConf <span id="CO5-8"><!--Empty--></span><span class="callout">8</span>
    - type: Network
      address: 1.2.3.4 <span id="CO5-9"><!--Empty--></span><span class="callout">9</span>
      port: 53 <span id="CO5-10"><!--Empty--></span><span class="callout">10</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO5-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Must comply with the <code class="literal">rfc6335</code> service name syntax.
							</div></dd><dt><a href="#CO5-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Must conform to the definition of a subdomain in the <code class="literal">rfc1123</code> service name syntax. The cluster domain, <code class="literal">cluster.local</code>, is an invalid subdomain for the <code class="literal">zones</code> field.
							</div></dd><dt><a href="#CO5-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Defines the policy to select upstream resolvers. Default value is <code class="literal">Random</code>. You can also use the values <code class="literal">RoundRobin</code>, and <code class="literal">Sequential</code>.
							</div></dd><dt><a href="#CO5-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								A maximum of 15 <code class="literal">upstreams</code> is allowed per <code class="literal">forwardPlugin</code>.
							</div></dd><dt><a href="#CO5-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Optional. You can use it to override the default policy and forward DNS resolution to the specified DNS resolvers (upstream resolvers) for the default domain. If you do not provide any upstream resolvers, the DNS name queries go to the servers in <code class="literal">/etc/resolv.conf</code>.
							</div></dd><dt><a href="#CO5-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								Determines the order in which upstream servers are selected for querying. You can specify one of these values: <code class="literal">Random</code>, <code class="literal">RoundRobin</code>, or <code class="literal">Sequential</code>. The default value is <code class="literal">Sequential</code>.
							</div></dd><dt><a href="#CO5-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								Optional. You can use it to provide upstream resolvers.
							</div></dd><dt><a href="#CO5-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								You can specify two types of <code class="literal">upstreams</code> - <code class="literal">SystemResolvConf</code> and <code class="literal">Network</code>. <code class="literal">SystemResolvConf</code> configures the upstream to use <code class="literal">/etc/resolv.conf</code> and <code class="literal">Network</code> defines a <code class="literal">Networkresolver</code>. You can specify one or both.
							</div></dd><dt><a href="#CO5-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								If the specified type is <code class="literal">Network</code>, you must provide an IP address. The <code class="literal">address</code> field must be a valid IPv4 or IPv6 address.
							</div></dd><dt><a href="#CO5-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								If the specified type is <code class="literal">Network</code>, you can optionally provide a port. The <code class="literal">port</code> field must have a value between <code class="literal">1</code> and <code class="literal">65535</code>. If you do not specify a port for the upstream, by default port 853 is tried.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Optional: When working in a highly regulated environment, you might need the ability to secure DNS traffic when forwarding requests to upstream resolvers so that you can ensure additional DNS traffic and data privacy. Cluster administrators can configure transport layer security (TLS) for forwarded DNS queries.
					</p><div class="formalpara"><p class="title"><strong>Configuring DNS forwarding with TLS</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: DNS
metadata:
  name: default
spec:
  servers:
  - name: example-server <span id="CO6-1"><!--Empty--></span><span class="callout">1</span>
    zones: <span id="CO6-2"><!--Empty--></span><span class="callout">2</span>
    - example.com
    forwardPlugin:
      transportConfig:
        transport: TLS <span id="CO6-3"><!--Empty--></span><span class="callout">3</span>
        tls:
          caBundle:
            name: mycacert
          serverName: dnstls.example.com  <span id="CO6-4"><!--Empty--></span><span class="callout">4</span>
      policy: Random <span id="CO6-5"><!--Empty--></span><span class="callout">5</span>
      upstreams: <span id="CO6-6"><!--Empty--></span><span class="callout">6</span>
      - 1.1.1.1
      - 2.2.2.2:5353
  upstreamResolvers: <span id="CO6-7"><!--Empty--></span><span class="callout">7</span>
    transportConfig:
      transport: TLS
      tls:
        caBundle:
          name: mycacert
        serverName: dnstls.example.com
    upstreams:
    - type: Network <span id="CO6-8"><!--Empty--></span><span class="callout">8</span>
      address: 1.2.3.4 <span id="CO6-9"><!--Empty--></span><span class="callout">9</span>
      port: 53 <span id="CO6-10"><!--Empty--></span><span class="callout">10</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO6-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Must comply with the <code class="literal">rfc6335</code> service name syntax.
							</div></dd><dt><a href="#CO6-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Must conform to the definition of a subdomain in the <code class="literal">rfc1123</code> service name syntax. The cluster domain, <code class="literal">cluster.local</code>, is an invalid subdomain for the <code class="literal">zones</code> field. The cluster domain, <code class="literal">cluster.local</code>, is an invalid <code class="literal">subdomain</code> for <code class="literal">zones</code>.
							</div></dd><dt><a href="#CO6-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								When configuring TLS for forwarded DNS queries, set the <code class="literal">transport</code> field to have the value <code class="literal">TLS</code>. By default, CoreDNS caches forwarded connections for 10 seconds. CoreDNS will hold a TCP connection open for those 10 seconds if no request is issued. With large clusters, ensure that your DNS server is aware that it might get many new connections to hold open because you can initiate a connection per node. Set up your DNS hierarchy accordingly to avoid performance issues.
							</div></dd><dt><a href="#CO6-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								When configuring TLS for forwarded DNS queries, this is a mandatory server name used as part of the server name indication (SNI) to validate the upstream TLS server certificate.
							</div></dd><dt><a href="#CO6-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Defines the policy to select upstream resolvers. Default value is <code class="literal">Random</code>. You can also use the values <code class="literal">RoundRobin</code>, and <code class="literal">Sequential</code>.
							</div></dd><dt><a href="#CO6-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								Required. You can use it to provide upstream resolvers. A maximum of 15 <code class="literal">upstreams</code> entries are allowed per <code class="literal">forwardPlugin</code> entry.
							</div></dd><dt><a href="#CO6-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								Optional. You can use it to override the default policy and forward DNS resolution to the specified DNS resolvers (upstream resolvers) for the default domain. If you do not provide any upstream resolvers, the DNS name queries go to the servers in <code class="literal">/etc/resolv.conf</code>.
							</div></dd><dt><a href="#CO6-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								<code class="literal">Network</code> type indicates that this upstream resolver should handle forwarded requests separately from the upstream resolvers listed in <code class="literal">/etc/resolv.conf</code>. Only the <code class="literal">Network</code> type is allowed when using TLS and you must provide an IP address.
							</div></dd><dt><a href="#CO6-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								The <code class="literal">address</code> field must be a valid IPv4 or IPv6 address.
							</div></dd><dt><a href="#CO6-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								You can optionally provide a port. The <code class="literal">port</code> must have a value between <code class="literal">1</code> and <code class="literal">65535</code>. If you do not specify a port for the upstream, by default port 853 is tried.
							</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If <code class="literal">servers</code> is undefined or invalid, the config map only contains the default server.
						</p></div></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						View the config map:
					</p><pre class="programlisting language-terminal">$ oc get configmap/dns-default -n openshift-dns -o yaml</pre><div class="formalpara"><p class="title"><strong>Sample DNS ConfigMap based on previous sample DNS</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
data:
  Corefile: |
    example.com:5353 {
        forward . 1.1.1.1 2.2.2.2:5353
    }
    bar.com:5353 example.com:5353 {
        forward . 3.3.3.3 4.4.4.4:5454 <span id="CO7-1"><!--Empty--></span><span class="callout">1</span>
    }
    .:5353 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
            pods insecure
            upstream
            fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf 1.2.3.4:53 {
            policy Random
        }
        cache 30
        reload
    }
kind: ConfigMap
metadata:
  labels:
    dns.operator.openshift.io/owning-dns: default
  name: dns-default
  namespace: openshift-dns</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO7-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Changes to the <code class="literal">forwardPlugin</code> triggers a rolling update of the CoreDNS daemon set.
							</div></dd></dl></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						For more information on DNS forwarding, see the <a class="link" href="https://coredns.io/plugins/forward/">CoreDNS forward documentation</a>.
					</li></ul></div></section><section class="section" id="nw-dns-operator-status_dns-operator"><div class="titlepage"><div><div><h2 class="title">6.6. DNS Operator status</h2></div></div></div><p>
				You can inspect the status and view the details of the DNS Operator using the <code class="literal">oc describe</code> command.
			</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					View the status of the DNS Operator:
				</p></div><pre class="programlisting language-terminal">$ oc describe clusteroperators/dns</pre></section><section class="section" id="nw-dns-operator-logs_dns-operator"><div class="titlepage"><div><div><h2 class="title">6.7. DNS Operator logs</h2></div></div></div><p>
				You can view DNS Operator logs by using the <code class="literal">oc logs</code> command.
			</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					View the logs of the DNS Operator:
				</p></div><pre class="programlisting language-terminal">$ oc logs -n openshift-dns-operator deployment/dns-operator -c dns-operator</pre></section><section class="section" id="nw-dns-loglevel_dns-operator"><div class="titlepage"><div><div><h2 class="title">6.8. Setting the CoreDNS log level</h2></div></div></div><p>
				You can configure the CoreDNS log level to determine the amount of detail in logged error messages. The valid values for CoreDNS log level are <code class="literal">Normal</code>, <code class="literal">Debug</code>, and <code class="literal">Trace</code>. The default <code class="literal">logLevel</code> is <code class="literal">Normal</code>.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The errors plugin is always enabled. The following <code class="literal">logLevel</code> settings report different error responses:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">logLevel</code>: <code class="literal">Normal</code> enables the "errors" class: <code class="literal">log . { class error }</code>.
						</li><li class="listitem">
							<code class="literal">logLevel</code>: <code class="literal">Debug</code> enables the "denial" class: <code class="literal">log . { class denial error }</code>.
						</li><li class="listitem">
							<code class="literal">logLevel</code>: <code class="literal">Trace</code> enables the "all" class: <code class="literal">log . { class all }</code>.
						</li></ul></div></div></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						To set <code class="literal">logLevel</code> to <code class="literal">Debug</code>, enter the following command:
					</p><pre class="programlisting language-terminal">$ oc patch dnses.operator.openshift.io/default -p '{"spec":{"logLevel":"Debug"}}' --type=merge</pre></li><li class="listitem"><p class="simpara">
						To set <code class="literal">logLevel</code> to <code class="literal">Trace</code>, enter the following command:
					</p><pre class="programlisting language-terminal">$ oc patch dnses.operator.openshift.io/default -p '{"spec":{"logLevel":"Trace"}}' --type=merge</pre></li></ul></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						To ensure the desired log level was set, check the config map:
					</p><pre class="programlisting language-terminal">$ oc get configmap/dns-default -n openshift-dns -o yaml</pre></li></ul></div></section><section class="section" id="nw-dns-operatorloglevel_dns-operator"><div class="titlepage"><div><div><h2 class="title">6.9. Setting the CoreDNS Operator log level</h2></div></div></div><p>
				Cluster administrators can configure the Operator log level to more quickly track down OpenShift DNS issues. The valid values for <code class="literal">operatorLogLevel</code> are <code class="literal">Normal</code>, <code class="literal">Debug</code>, and <code class="literal">Trace</code>. <code class="literal">Trace</code> has the most detailed information. The default <code class="literal">operatorlogLevel</code> is <code class="literal">Normal</code>. There are seven logging levels for issues: Trace, Debug, Info, Warning, Error, Fatal and Panic. After the logging level is set, log entries with that severity or anything above it will be logged.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">operatorLogLevel: "Normal"</code> sets <code class="literal">logrus.SetLogLevel("Info")</code>.
					</li><li class="listitem">
						<code class="literal">operatorLogLevel: "Debug"</code> sets <code class="literal">logrus.SetLogLevel("Debug")</code>.
					</li><li class="listitem">
						<code class="literal">operatorLogLevel: "Trace"</code> sets <code class="literal">logrus.SetLogLevel("Trace")</code>.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						To set <code class="literal">operatorLogLevel</code> to <code class="literal">Debug</code>, enter the following command:
					</p><pre class="programlisting language-terminal">$ oc patch dnses.operator.openshift.io/default -p '{"spec":{"operatorLogLevel":"Debug"}}' --type=merge</pre></li><li class="listitem"><p class="simpara">
						To set <code class="literal">operatorLogLevel</code> to <code class="literal">Trace</code>, enter the following command:
					</p><pre class="programlisting language-terminal">$ oc patch dnses.operator.openshift.io/default -p '{"spec":{"operatorLogLevel":"Trace"}}' --type=merge</pre></li></ul></div></section><section class="section" id="nw-dns-cache-tuning_dns-operator"><div class="titlepage"><div><div><h2 class="title">6.10. Tuning the CoreDNS cache</h2></div></div></div><p>
				You can configure the maximum duration of both successful or unsuccessful caching, also known as positive or negative caching respectively, done by CoreDNS. Tuning the duration of caching of DNS query responses can reduce the load for any upstream DNS resolvers.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Edit the DNS Operator object named <code class="literal">default</code> by running the following command:
					</p><pre class="programlisting language-terminal">$ oc edit dns.operator.openshift.io/default</pre></li><li class="listitem"><p class="simpara">
						Modify the time-to-live (TTL) caching values:
					</p><div class="formalpara"><p class="title"><strong>Configuring DNS caching</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: DNS
metadata:
  name: default
spec:
  cache:
    positiveTTL: 1h <span id="CO8-1"><!--Empty--></span><span class="callout">1</span>
    negativeTTL: 0.5h10m <span id="CO8-2"><!--Empty--></span><span class="callout">2</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO8-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The string value <code class="literal">1h</code> is converted to its respective number of seconds by CoreDNS. If this field is omitted, the value is assumed to be <code class="literal">0s</code> and the cluster uses the internal default value of <code class="literal">900s</code> as a fallback.
							</div></dd><dt><a href="#CO8-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The string value can be a combination of units such as <code class="literal">0.5h10m</code> and is converted to its respective number of seconds by CoreDNS. If this field is omitted, the value is assumed to be <code class="literal">0s</code> and the cluster uses the internal default value of <code class="literal">30s</code> as a fallback.
							</div></dd></dl></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							Setting TTL fields to low values could lead to an increased load on the cluster, any upstream resolvers, or both.
						</p></div></div></li></ol></div></section></section><section class="chapter" id="configuring-ingress"><div class="titlepage"><div><div><h1 class="title">Chapter 7. Ingress Operator in OpenShift Container Platform</h1></div></div></div><section class="section" id="nw-ne-openshift-ingress_configuring-ingress"><div class="titlepage"><div><div><h2 class="title">7.1. OpenShift Container Platform Ingress Operator</h2></div></div></div><p>
				When you create your OpenShift Container Platform cluster, pods and services running on the cluster are each allocated their own IP addresses. The IP addresses are accessible to other pods and services running nearby but are not accessible to outside clients. The Ingress Operator implements the <code class="literal">IngressController</code> API and is the component responsible for enabling external access to OpenShift Container Platform cluster services.
			</p><p>
				The Ingress Operator makes it possible for external clients to access your service by deploying and managing one or more HAProxy-based <a class="link" href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/">Ingress Controllers</a> to handle routing. You can use the Ingress Operator to route traffic by specifying OpenShift Container Platform <code class="literal">Route</code> and Kubernetes <code class="literal">Ingress</code> resources. Configurations within the Ingress Controller, such as the ability to define <code class="literal">endpointPublishingStrategy</code> type and internal load balancing, provide ways to publish Ingress Controller endpoints.
			</p></section><section class="section" id="nw-installation-ingress-config-asset_configuring-ingress"><div class="titlepage"><div><div><h2 class="title">7.2. The Ingress configuration asset</h2></div></div></div><p>
				The installation program generates an asset with an <code class="literal">Ingress</code> resource in the <code class="literal">config.openshift.io</code> API group, <code class="literal">cluster-ingress-02-config.yml</code>.
			</p><div class="formalpara"><p class="title"><strong>YAML Definition of the <code class="literal">Ingress</code> resource</strong></p><p>
					
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Ingress
metadata:
  name: cluster
spec:
  domain: apps.openshiftdemos.com</pre>

				</p></div><p>
				The installation program stores this asset in the <code class="literal">cluster-ingress-02-config.yml</code> file in the <code class="literal">manifests/</code> directory. This <code class="literal">Ingress</code> resource defines the cluster-wide configuration for Ingress. This Ingress configuration is used as follows:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The Ingress Operator uses the domain from the cluster Ingress configuration as the domain for the default Ingress Controller.
					</li><li class="listitem">
						The OpenShift API Server Operator uses the domain from the cluster Ingress configuration. This domain is also used when generating a default host for a <code class="literal">Route</code> resource that does not specify an explicit host.
					</li></ul></div></section><section class="section" id="nw-ingress-controller-configuration-parameters_configuring-ingress"><div class="titlepage"><div><div><h2 class="title">7.3. Ingress Controller configuration parameters</h2></div></div></div><p>
				The <code class="literal">ingresscontrollers.operator.openshift.io</code> resource offers the following configuration parameters.
			</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 27%; " class="col_1"><!--Empty--></col><col style="width: 73%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587171424384" scope="col">Parameter</th><th align="left" valign="top" id="idm140587171423296" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">domain</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								<code class="literal">domain</code> is a DNS name serviced by the Ingress Controller and is used to configure multiple features:
							</p>
							 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										For the <code class="literal">LoadBalancerService</code> endpoint publishing strategy, <code class="literal">domain</code> is used to configure DNS records. See <code class="literal">endpointPublishingStrategy</code>.
									</li><li class="listitem">
										When using a generated default certificate, the certificate is valid for <code class="literal">domain</code> and its <code class="literal">subdomains</code>. See <code class="literal">defaultCertificate</code>.
									</li><li class="listitem">
										The value is published to individual Route statuses so that users know where to target external DNS records.
									</li></ul></div>
							 <p>
								The <code class="literal">domain</code> value must be unique among all Ingress Controllers and cannot be updated.
							</p>
							 <p>
								If empty, the default value is <code class="literal">ingress.config.openshift.io/cluster</code> <code class="literal">.spec.domain</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">replicas</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								<code class="literal">replicas</code> is the desired number of Ingress Controller replicas. If not set, the default value is <code class="literal">2</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">endpointPublishingStrategy</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								<code class="literal">endpointPublishingStrategy</code> is used to publish the Ingress Controller endpoints to other networks, enable load balancer integrations, and provide access to other systems.
							</p>
							 <p>
								On GCP, AWS, and Azure you can configure the following <code class="literal">endpointPublishingStrategy</code> fields:
							</p>
							 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">loadBalancer.scope</code>
									</li><li class="listitem">
										<code class="literal">loadBalancer.allowedSourceRanges</code>
									</li></ul></div>
							 <p>
								If not set, the default value is based on <code class="literal">infrastructure.config.openshift.io/cluster</code> <code class="literal">.status.platform</code>:
							</p>
							 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Amazon Web Services (AWS): <code class="literal">LoadBalancerService</code> (with External scope)
									</li><li class="listitem">
										Azure: <code class="literal">LoadBalancerService</code> (with External scope)
									</li><li class="listitem">
										Google Cloud Platform (GCP): <code class="literal">LoadBalancerService</code> (with External scope)
									</li><li class="listitem">
										Bare metal: <code class="literal">NodePortService</code>
									</li><li class="listitem"><p class="simpara">
										Other: <code class="literal">HostNetwork</code>
									</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											<code class="literal">HostNetwork</code> has a <code class="literal">hostNetwork</code> field with the following default values for the optional binding ports: <code class="literal">httpPort: 80</code>, <code class="literal">httpsPort: 443</code>, and <code class="literal">statsPort: 1936</code>. With the binding ports, you can deploy multiple Ingress Controllers on the same node for the <code class="literal">HostNetwork</code> strategy.
										</p><div class="formalpara"><p class="title"><strong>Example</strong></p><p>
												
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: internal
  namespace: openshift-ingress-operator
spec:
  domain: example.com
  endpointPublishingStrategy:
    type: HostNetwork
    hostNetwork:
      httpPort: 80
      httpsPort: 443
      statsPort: 1936</pre>

											</p></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											On Red Hat OpenStack Platform (RHOSP), the <code class="literal">LoadBalancerService</code> endpoint publishing strategy is only supported if a cloud provider is configured to create health monitors. For RHOSP 16.2, this strategy is only possible if you use the Amphora Octavia provider.
										</p><p>
											For more information, see the "Setting cloud provider options" section of the RHOSP installation documentation.
										</p></div></div></li></ul></div>
							 <p>
								For most platforms, the <code class="literal">endpointPublishingStrategy</code> value can be updated. On GCP, you can configure the following <code class="literal">endpointPublishingStrategy</code> fields:
							</p>
							 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">loadBalancer.scope</code>
									</li><li class="listitem">
										<code class="literal">loadbalancer.providerParameters.gcp.clientAccess</code>
									</li><li class="listitem">
										<code class="literal">hostNetwork.protocol</code>
									</li><li class="listitem">
										<code class="literal">nodePort.protocol</code>
									</li></ul></div>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">defaultCertificate</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								The <code class="literal">defaultCertificate</code> value is a reference to a secret that contains the default certificate that is served by the Ingress Controller. When Routes do not specify their own certificate, <code class="literal">defaultCertificate</code> is used.
							</p>
							 <p>
								The secret must contain the following keys and data: * <code class="literal">tls.crt</code>: certificate file contents * <code class="literal">tls.key</code>: key file contents
							</p>
							 <p>
								If not set, a wildcard certificate is automatically generated and used. The certificate is valid for the Ingress Controller <code class="literal">domain</code> and <code class="literal">subdomains</code>, and the generated certificate’s CA is automatically integrated with the cluster’s trust store.
							</p>
							 <p>
								The in-use certificate, whether generated or user-specified, is automatically integrated with OpenShift Container Platform built-in OAuth server.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">namespaceSelector</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								<code class="literal">namespaceSelector</code> is used to filter the set of namespaces serviced by the Ingress Controller. This is useful for implementing shards.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">routeSelector</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								<code class="literal">routeSelector</code> is used to filter the set of Routes serviced by the Ingress Controller. This is useful for implementing shards.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">nodePlacement</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								<code class="literal">nodePlacement</code> enables explicit control over the scheduling of the Ingress Controller.
							</p>
							 <p>
								If not set, the defaults values are used.
							</p>
							 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
									The <code class="literal">nodePlacement</code> parameter includes two parts, <code class="literal">nodeSelector</code> and <code class="literal">tolerations</code>. For example:
								</p><pre class="programlisting language-yaml">nodePlacement:
 nodeSelector:
   matchLabels:
     kubernetes.io/os: linux
 tolerations:
 - effect: NoSchedule
   operator: Exists</pre></div></div>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">tlsSecurityProfile</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								<code class="literal">tlsSecurityProfile</code> specifies settings for TLS connections for Ingress Controllers.
							</p>
							 <p>
								If not set, the default value is based on the <code class="literal">apiservers.config.openshift.io/cluster</code> resource.
							</p>
							 <p>
								When using the <code class="literal">Old</code>, <code class="literal">Intermediate</code>, and <code class="literal">Modern</code> profile types, the effective profile configuration is subject to change between releases. For example, given a specification to use the <code class="literal">Intermediate</code> profile deployed on release <code class="literal">X.Y.Z</code>, an upgrade to release <code class="literal">X.Y.Z+1</code> may cause a new profile configuration to be applied to the Ingress Controller, resulting in a rollout.
							</p>
							 <p>
								The minimum TLS version for Ingress Controllers is <code class="literal">1.1</code>, and the maximum TLS version is <code class="literal">1.3</code>.
							</p>
							 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Ciphers and the minimum TLS version of the configured security profile are reflected in the <code class="literal">TLSProfile</code> status.
								</p></div></div>
							 <div class="admonition important"><div class="admonition_header">Important</div><div><p>
									The Ingress Operator converts the TLS <code class="literal">1.0</code> of an <code class="literal">Old</code> or <code class="literal">Custom</code> profile to <code class="literal">1.1</code>.
								</p></div></div>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">clientTLS</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								<code class="literal">clientTLS</code> authenticates client access to the cluster and services; as a result, mutual TLS authentication is enabled. If not set, then client TLS is not enabled.
							</p>
							 <p>
								<code class="literal">clientTLS</code> has the required subfields, <code class="literal">spec.clientTLS.clientCertificatePolicy</code> and <code class="literal">spec.clientTLS.ClientCA</code>.
							</p>
							 <p>
								The <code class="literal">ClientCertificatePolicy</code> subfield accepts one of the two values: <code class="literal">Required</code> or <code class="literal">Optional</code>. The <code class="literal">ClientCA</code> subfield specifies a config map that is in the openshift-config namespace. The config map should contain a CA certificate bundle.
							</p>
							 <p>
								The <code class="literal">AllowedSubjectPatterns</code> is an optional value that specifies a list of regular expressions, which are matched against the distinguished name on a valid client certificate to filter requests. The regular expressions must use PCRE syntax. At least one pattern must match a client certificate’s distinguished name; otherwise, the Ingress Controller rejects the certificate and denies the connection. If not specified, the Ingress Controller does not reject certificates based on the distinguished name.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">routeAdmission</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								<code class="literal">routeAdmission</code> defines a policy for handling new route claims, such as allowing or denying claims across namespaces.
							</p>
							 <p>
								<code class="literal">namespaceOwnership</code> describes how hostname claims across namespaces should be handled. The default is <code class="literal">Strict</code>.
							</p>
							 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">Strict</code>: does not allow routes to claim the same hostname across namespaces.
									</li><li class="listitem">
										<code class="literal">InterNamespaceAllowed</code>: allows routes to claim different paths of the same hostname across namespaces.
									</li></ul></div>
							 <p>
								<code class="literal">wildcardPolicy</code> describes how routes with wildcard policies are handled by the Ingress Controller.
							</p>
							 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">WildcardsAllowed</code>: Indicates routes with any wildcard policy are admitted by the Ingress Controller.
									</li><li class="listitem">
										<code class="literal">WildcardsDisallowed</code>: Indicates only routes with a wildcard policy of <code class="literal">None</code> are admitted by the Ingress Controller. Updating <code class="literal">wildcardPolicy</code> from <code class="literal">WildcardsAllowed</code> to <code class="literal">WildcardsDisallowed</code> causes admitted routes with a wildcard policy of <code class="literal">Subdomain</code> to stop working. These routes must be recreated to a wildcard policy of <code class="literal">None</code> to be readmitted by the Ingress Controller. <code class="literal">WildcardsDisallowed</code> is the default setting.
									</li></ul></div>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">IngressControllerLogging</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								<code class="literal">logging</code> defines parameters for what is logged where. If this field is empty, operational logs are enabled but access logs are disabled.
							</p>
							 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
										<code class="literal">access</code> describes how client requests are logged. If this field is empty, access logging is disabled.
									</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
												<code class="literal">destination</code> describes a destination for log messages.
											</p><div class="itemizedlist"><ul class="itemizedlist" type="square"><li class="listitem"><p class="simpara">
														<code class="literal">type</code> is the type of destination for logs:
													</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
																<code class="literal">Container</code> specifies that logs should go to a sidecar container. The Ingress Operator configures the container, named <span class="strong strong"><strong>logs</strong></span>, on the Ingress Controller pod and configures the Ingress Controller to write logs to the container. The expectation is that the administrator configures a custom logging solution that reads logs from this container. Using container logs means that logs may be dropped if the rate of logs exceeds the container runtime capacity or the custom logging solution capacity.
															</li><li class="listitem">
																<code class="literal">Syslog</code> specifies that logs are sent to a Syslog endpoint. The administrator must specify an endpoint that can receive Syslog messages. The expectation is that the administrator has configured a custom Syslog instance.
															</li></ul></div></li><li class="listitem">
														<code class="literal">container</code> describes parameters for the <code class="literal">Container</code> logging destination type. Currently there are no parameters for container logging, so this field must be empty.
													</li><li class="listitem"><p class="simpara">
														<code class="literal">syslog</code> describes parameters for the <code class="literal">Syslog</code> logging destination type:
													</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
																<code class="literal">address</code> is the IP address of the syslog endpoint that receives log messages.
															</li><li class="listitem">
																<code class="literal">port</code> is the UDP port number of the syslog endpoint that receives log messages.
															</li><li class="listitem">
																<code class="literal">maxLength</code> is the maximum length of the syslog message. It must be between <code class="literal">480</code> and <code class="literal">4096</code> bytes. If this field is empty, the maximum length is set to the default value of <code class="literal">1024</code> bytes.
															</li><li class="listitem">
																<code class="literal">facility</code> specifies the syslog facility of log messages. If this field is empty, the facility is <code class="literal">local1</code>. Otherwise, it must specify a valid syslog facility: <code class="literal">kern</code>, <code class="literal">user</code>, <code class="literal">mail</code>, <code class="literal">daemon</code>, <code class="literal">auth</code>, <code class="literal">syslog</code>, <code class="literal">lpr</code>, <code class="literal">news</code>, <code class="literal">uucp</code>, <code class="literal">cron</code>, <code class="literal">auth2</code>, <code class="literal">ftp</code>, <code class="literal">ntp</code>, <code class="literal">audit</code>, <code class="literal">alert</code>, <code class="literal">cron2</code>, <code class="literal">local0</code>, <code class="literal">local1</code>, <code class="literal">local2</code>, <code class="literal">local3</code>. <code class="literal">local4</code>, <code class="literal">local5</code>, <code class="literal">local6</code>, or <code class="literal">local7</code>.
															</li></ul></div></li></ul></div></li><li class="listitem">
												<code class="literal">httpLogFormat</code> specifies the format of the log message for an HTTP request. If this field is empty, log messages use the implementation’s default HTTP log format. For HAProxy’s default HTTP log format, see <a class="link" href="http://cbonte.github.io/haproxy-dconv/2.0/configuration.html#8.2.3">the HAProxy documentation</a>.
											</li></ul></div></li></ul></div>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">httpHeaders</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								<code class="literal">httpHeaders</code> defines the policy for HTTP headers.
							</p>
							 <p>
								By setting the <code class="literal">forwardedHeaderPolicy</code> for the <code class="literal">IngressControllerHTTPHeaders</code>, you specify when and how the Ingress Controller sets the <code class="literal">Forwarded</code>, <code class="literal">X-Forwarded-For</code>, <code class="literal">X-Forwarded-Host</code>, <code class="literal">X-Forwarded-Port</code>, <code class="literal">X-Forwarded-Proto</code>, and <code class="literal">X-Forwarded-Proto-Version</code> HTTP headers.
							</p>
							 <p>
								By default, the policy is set to <code class="literal">Append</code>.
							</p>
							 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">Append</code> specifies that the Ingress Controller appends the headers, preserving any existing headers.
									</li><li class="listitem">
										<code class="literal">Replace</code> specifies that the Ingress Controller sets the headers, removing any existing headers.
									</li><li class="listitem">
										<code class="literal">IfNone</code> specifies that the Ingress Controller sets the headers if they are not already set.
									</li><li class="listitem">
										<code class="literal">Never</code> specifies that the Ingress Controller never sets the headers, preserving any existing headers.
									</li></ul></div>
							 <p>
								By setting <code class="literal">headerNameCaseAdjustments</code>, you can specify case adjustments that can be applied to HTTP header names. Each adjustment is specified as an HTTP header name with the desired capitalization. For example, specifying <code class="literal">X-Forwarded-For</code> indicates that the <code class="literal">x-forwarded-for</code> HTTP header should be adjusted to have the specified capitalization.
							</p>
							 <p>
								These adjustments are only applied to cleartext, edge-terminated, and re-encrypt routes, and only when using HTTP/1.
							</p>
							 <p>
								For request headers, these adjustments are applied only for routes that have the <code class="literal">haproxy.router.openshift.io/h1-adjust-case=true</code> annotation. For response headers, these adjustments are applied to all HTTP responses. If this field is empty, no request headers are adjusted.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">httpCompression</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								<code class="literal">httpCompression</code> defines the policy for HTTP traffic compression.
							</p>
							 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">mimeTypes</code> defines a list of MIME types to which compression should be applied. For example, <code class="literal">text/css; charset=utf-8</code>, <code class="literal">text/html</code>, <code class="literal">text/*</code>, <code class="literal">image/svg+xml</code>, <code class="literal">application/octet-stream</code>, <code class="literal">X-custom/customsub</code>, using the format pattern, <code class="literal">type/subtype; [;attribute=value]</code>. The <code class="literal">types</code> are: application, image, message, multipart, text, video, or a custom type prefaced by <code class="literal">X-</code>; e.g. To see the full notation for MIME types and subtypes, see <a class="link" href="https://datatracker.ietf.org/doc/html/rfc1341#page-7">RFC1341</a>
									</li></ul></div>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">httpErrorCodePages</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								<code class="literal">httpErrorCodePages</code> specifies custom HTTP error code response pages. By default, an IngressController uses error pages built into the IngressController image.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">httpCaptureCookies</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								<code class="literal">httpCaptureCookies</code> specifies HTTP cookies that you want to capture in access logs. If the <code class="literal">httpCaptureCookies</code> field is empty, the access logs do not capture the cookies.
							</p>
							 <p>
								For any cookie that you want to capture, the following parameters must be in your <code class="literal">IngressController</code> configuration:
							</p>
							 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">name</code> specifies the name of the cookie.
									</li><li class="listitem">
										<code class="literal">maxLength</code> specifies tha maximum length of the cookie.
									</li><li class="listitem">
										<code class="literal">matchType</code> specifies if the field <code class="literal">name</code> of the cookie exactly matches the capture cookie setting or is a prefix of the capture cookie setting. The <code class="literal">matchType</code> field uses the <code class="literal">Exact</code> and <code class="literal">Prefix</code> parameters.
									</li></ul></div>
							 <p>
								For example:
							</p>
							 
<pre class="programlisting language-yaml">  httpCaptureCookies:
  - matchType: Exact
    maxLength: 128
    name: MYCOOKIE</pre>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">httpCaptureHeaders</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								<code class="literal">httpCaptureHeaders</code> specifies the HTTP headers that you want to capture in the access logs. If the <code class="literal">httpCaptureHeaders</code> field is empty, the access logs do not capture the headers.
							</p>
							 <p>
								<code class="literal">httpCaptureHeaders</code> contains two lists of headers to capture in the access logs. The two lists of header fields are <code class="literal">request</code> and <code class="literal">response</code>. In both lists, the <code class="literal">name</code> field must specify the header name and the <code class="literal">maxlength</code> field must specify the maximum length of the header. For example:
							</p>
							 
<pre class="programlisting language-yaml">  httpCaptureHeaders:
    request:
    - maxLength: 256
      name: Connection
    - maxLength: 128
      name: User-Agent
    response:
    - maxLength: 256
      name: Content-Type
    - maxLength: 256
      name: Content-Length</pre>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">tuningOptions</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								<code class="literal">tuningOptions</code> specifies options for tuning the performance of Ingress Controller pods.
							</p>
							 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">clientFinTimeout</code> specifies how long a connection is held open while waiting for the client response to the server closing the connection. The default timeout is <code class="literal">1s</code>.
									</li><li class="listitem">
										<code class="literal">clientTimeout</code> specifies how long a connection is held open while waiting for a client response. The default timeout is <code class="literal">30s</code>.
									</li><li class="listitem">
										<code class="literal">headerBufferBytes</code> specifies how much memory is reserved, in bytes, for Ingress Controller connection sessions. This value must be at least <code class="literal">16384</code> if HTTP/2 is enabled for the Ingress Controller. If not set, the default value is <code class="literal">32768</code> bytes. Setting this field not recommended because <code class="literal">headerBufferBytes</code> values that are too small can break the Ingress Controller, and <code class="literal">headerBufferBytes</code> values that are too large could cause the Ingress Controller to use significantly more memory than necessary.
									</li><li class="listitem">
										<code class="literal">headerBufferMaxRewriteBytes</code> specifies how much memory should be reserved, in bytes, from <code class="literal">headerBufferBytes</code> for HTTP header rewriting and appending for Ingress Controller connection sessions. The minimum value for <code class="literal">headerBufferMaxRewriteBytes</code> is <code class="literal">4096</code>. <code class="literal">headerBufferBytes</code> must be greater than <code class="literal">headerBufferMaxRewriteBytes</code> for incoming HTTP requests. If not set, the default value is <code class="literal">8192</code> bytes. Setting this field not recommended because <code class="literal">headerBufferMaxRewriteBytes</code> values that are too small can break the Ingress Controller and <code class="literal">headerBufferMaxRewriteBytes</code> values that are too large could cause the Ingress Controller to use significantly more memory than necessary.
									</li><li class="listitem">
										<code class="literal">healthCheckInterval</code> specifies how long the router waits between health checks. The default is <code class="literal">5s</code>.
									</li><li class="listitem">
										<code class="literal">serverFinTimeout</code> specifies how long a connection is held open while waiting for the server response to the client that is closing the connection. The default timeout is <code class="literal">1s</code>.
									</li><li class="listitem">
										<code class="literal">serverTimeout</code> specifies how long a connection is held open while waiting for a server response. The default timeout is <code class="literal">30s</code>.
									</li><li class="listitem">
										<code class="literal">threadCount</code> specifies the number of threads to create per HAProxy process. Creating more threads allows each Ingress Controller pod to handle more connections, at the cost of more system resources being used. HAProxy supports up to <code class="literal">64</code> threads. If this field is empty, the Ingress Controller uses the default value of <code class="literal">4</code> threads. The default value can change in future releases. Setting this field is not recommended because increasing the number of HAProxy threads allows Ingress Controller pods to use more CPU time under load, and prevent other pods from receiving the CPU resources they need to perform. Reducing the number of threads can cause the Ingress Controller to perform poorly.
									</li><li class="listitem">
										<code class="literal">tlsInspectDelay</code> specifies how long the router can hold data to find a matching route. Setting this value too short can cause the router to fall back to the default certificate for edge-terminated, reencrypted, or passthrough routes, even when using a better matched certificate. The default inspect delay is <code class="literal">5s</code>.
									</li><li class="listitem">
										<code class="literal">tunnelTimeout</code> specifies how long a tunnel connection, including websockets, remains open while the tunnel is idle. The default timeout is <code class="literal">1h</code>.
									</li><li class="listitem"><p class="simpara">
										<code class="literal">maxConnections</code> specifies the maximum number of simultaneous connections that can be established per HAProxy process. Increasing this value allows each ingress controller pod to handle more connections at the cost of additional system resources. Permitted values are <code class="literal">0</code>, <code class="literal">-1</code>, any value within the range <code class="literal">2000</code> and <code class="literal">2000000</code>, or the field can be left empty.
									</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
												If this field is left empty or has the value <code class="literal">0</code>, the Ingress Controller will use the default value of <code class="literal">50000</code>. This value is subject to change in future releases.
											</li><li class="listitem">
												If the field has the value of <code class="literal">-1</code>, then HAProxy will dynamically compute a maximum value based on the available <code class="literal">ulimits</code> in the running container. This process results in a large computed value that will incur significant memory usage compared to the current default value of <code class="literal">50000</code>.
											</li><li class="listitem">
												If the field has a value that is greater than the current operating system limit, the HAProxy process will not start.
											</li><li class="listitem">
												If you choose a discrete value and the router pod is migrated to a new node, it is possible the new node does not have an identical <code class="literal">ulimit</code> configured. In such cases, the pod fails to start.
											</li><li class="listitem">
												If you have nodes with different <code class="literal">ulimits</code> configured, and you choose a discrete value, it is recommended to use the value of <code class="literal">-1</code> for this field so that the maximum number of connections is calculated at runtime.
											</li></ul></div></li></ul></div>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">logEmptyRequests</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								<code class="literal">logEmptyRequests</code> specifies connections for which no request is received and logged. These empty requests come from load balancer health probes or web browser speculative connections (preconnect) and logging these requests can be undesirable. However, these requests can be caused by network errors, in which case logging empty requests can be useful for diagnosing the errors. These requests can be caused by port scans, and logging empty requests can aid in detecting intrusion attempts. Allowed values for this field are <code class="literal">Log</code> and <code class="literal">Ignore</code>. The default value is <code class="literal">Log</code>.
							</p>
							 <p>
								The <code class="literal">LoggingPolicy</code> type accepts either one of two values:
							</p>
							 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">Log</code>: Setting this value to <code class="literal">Log</code> indicates that an event should be logged.
									</li><li class="listitem">
										<code class="literal">Ignore</code>: Setting this value to <code class="literal">Ignore</code> sets the <code class="literal">dontlognull</code> option in the HAproxy configuration.
									</li></ul></div>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587171424384"> <p>
								<code class="literal">HTTPEmptyRequestsPolicy</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587171423296"> <p>
								<code class="literal">HTTPEmptyRequestsPolicy</code> describes how HTTP connections are handled if the connection times out before a request is received. Allowed values for this field are <code class="literal">Respond</code> and <code class="literal">Ignore</code>. The default value is <code class="literal">Respond</code>.
							</p>
							 <p>
								The <code class="literal">HTTPEmptyRequestsPolicy</code> type accepts either one of two values:
							</p>
							 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">Respond</code>: If the field is set to <code class="literal">Respond</code>, the Ingress Controller sends an HTTP <code class="literal">400</code> or <code class="literal">408</code> response, logs the connection if access logging is enabled, and counts the connection in the appropriate metrics.
									</li><li class="listitem">
										<code class="literal">Ignore</code>: Setting this option to <code class="literal">Ignore</code> adds the <code class="literal">http-ignore-probes</code> parameter in the HAproxy configuration. If the field is set to <code class="literal">Ignore</code>, the Ingress Controller closes the connection without sending a response, then logs the connection, or incrementing metrics.
									</li></ul></div>
							 <p>
								These connections come from load balancer health probes or web browser speculative connections (preconnect) and can be safely ignored. However, these requests can be caused by network errors, so setting this field to <code class="literal">Ignore</code> can impede detection and diagnosis of problems. These requests can be caused by port scans, in which case logging empty requests can aid in detecting intrusion attempts.
							</p>
							 </td></tr></tbody></table></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					All parameters are optional.
				</p></div></div><section class="section" id="configuring-ingress-controller-tls"><div class="titlepage"><div><div><h3 class="title">7.3.1. Ingress Controller TLS security profiles</h3></div></div></div><p>
					TLS security profiles provide a way for servers to regulate which ciphers a connecting client can use when connecting to the server.
				</p><section class="section" id="tls-profiles-understanding_configuring-ingress"><div class="titlepage"><div><div><h4 class="title">7.3.1.1. Understanding TLS security profiles</h4></div></div></div><p>
						You can use a TLS (Transport Layer Security) security profile to define which TLS ciphers are required by various OpenShift Container Platform components. The OpenShift Container Platform TLS security profiles are based on <a class="link" href="https://wiki.mozilla.org/Security/Server_Side_TLS">Mozilla recommended configurations</a>.
					</p><p>
						You can specify one of the following TLS security profiles for each component:
					</p><div class="table" id="idm140587173617344"><p class="title"><strong>Table 7.1. TLS security profiles</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 67%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587164938576" scope="col">Profile</th><th align="left" valign="top" id="idm140587164937488" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587164938576"> <p>
										<code class="literal">Old</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587164937488"> <p>
										This profile is intended for use with legacy clients or libraries. The profile is based on the <a class="link" href="https://wiki.mozilla.org/Security/Server_Side_TLS#Old_backward_compatibility">Old backward compatibility</a> recommended configuration.
									</p>
									 <p>
										The <code class="literal">Old</code> profile requires a minimum TLS version of 1.0.
									</p>
									 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
											For the Ingress Controller, the minimum TLS version is converted from 1.0 to 1.1.
										</p></div></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587164938576"> <p>
										<code class="literal">Intermediate</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587164937488"> <p>
										This profile is the recommended configuration for the majority of clients. It is the default TLS security profile for the Ingress Controller, kubelet, and control plane. The profile is based on the <a class="link" href="https://wiki.mozilla.org/Security/Server_Side_TLS#Intermediate_compatibility_.28recommended.29">Intermediate compatibility</a> recommended configuration.
									</p>
									 <p>
										The <code class="literal">Intermediate</code> profile requires a minimum TLS version of 1.2.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587164938576"> <p>
										<code class="literal">Modern</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587164937488"> <p>
										This profile is intended for use with modern clients that have no need for backwards compatibility. This profile is based on the <a class="link" href="https://wiki.mozilla.org/Security/Server_Side_TLS#Modern_compatibility">Modern compatibility</a> recommended configuration.
									</p>
									 <p>
										The <code class="literal">Modern</code> profile requires a minimum TLS version of 1.3.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587164938576"> <p>
										<code class="literal">Custom</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587164937488"> <p>
										This profile allows you to define the TLS version and ciphers to use.
									</p>
									 <div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
											Use caution when using a <code class="literal">Custom</code> profile, because invalid configurations can cause problems.
										</p></div></div>
									 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							When using one of the predefined profile types, the effective profile configuration is subject to change between releases. For example, given a specification to use the Intermediate profile deployed on release X.Y.Z, an upgrade to release X.Y.Z+1 might cause a new profile configuration to be applied, resulting in a rollout.
						</p></div></div></section><section class="section" id="tls-profiles-ingress-configuring_configuring-ingress"><div class="titlepage"><div><div><h4 class="title">7.3.1.2. Configuring the TLS security profile for the Ingress Controller</h4></div></div></div><p>
						To configure a TLS security profile for an Ingress Controller, edit the <code class="literal">IngressController</code> custom resource (CR) to specify a predefined or custom TLS security profile. If a TLS security profile is not configured, the default value is based on the TLS security profile set for the API server.
					</p><div class="formalpara"><p class="title"><strong>Sample <code class="literal">IngressController</code> CR that configures the <code class="literal">Old</code> TLS security profile</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
 ...
spec:
  tlsSecurityProfile:
    old: {}
    type: Old
 ...</pre>

						</p></div><p>
						The TLS security profile defines the minimum TLS version and the TLS ciphers for TLS connections for Ingress Controllers.
					</p><p>
						You can see the ciphers and the minimum TLS version of the configured TLS security profile in the <code class="literal">IngressController</code> custom resource (CR) under <code class="literal">Status.Tls Profile</code> and the configured TLS security profile under <code class="literal">Spec.Tls Security Profile</code>. For the <code class="literal">Custom</code> TLS security profile, the specific ciphers and minimum TLS version are listed under both parameters.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The HAProxy Ingress Controller image supports TLS <code class="literal">1.3</code> and the <code class="literal">Modern</code> profile.
						</p><p>
							The Ingress Operator also converts the TLS <code class="literal">1.0</code> of an <code class="literal">Old</code> or <code class="literal">Custom</code> profile to <code class="literal">1.1</code>.
						</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">IngressController</code> CR in the <code class="literal">openshift-ingress-operator</code> project to configure the TLS security profile:
							</p><pre class="programlisting language-terminal">$ oc edit IngressController default -n openshift-ingress-operator</pre></li><li class="listitem"><p class="simpara">
								Add the <code class="literal">spec.tlsSecurityProfile</code> field:
							</p><div class="formalpara"><p class="title"><strong>Sample <code class="literal">IngressController</code> CR for a <code class="literal">Custom</code> profile</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
 ...
spec:
  tlsSecurityProfile:
    type: Custom <span id="CO9-1"><!--Empty--></span><span class="callout">1</span>
    custom: <span id="CO9-2"><!--Empty--></span><span class="callout">2</span>
      ciphers: <span id="CO9-3"><!--Empty--></span><span class="callout">3</span>
      - ECDHE-ECDSA-CHACHA20-POLY1305
      - ECDHE-RSA-CHACHA20-POLY1305
      - ECDHE-RSA-AES128-GCM-SHA256
      - ECDHE-ECDSA-AES128-GCM-SHA256
      minTLSVersion: VersionTLS11
 ...</pre>

								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO9-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify the TLS security profile type (<code class="literal">Old</code>, <code class="literal">Intermediate</code>, or <code class="literal">Custom</code>). The default is <code class="literal">Intermediate</code>.
									</div></dd><dt><a href="#CO9-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specify the appropriate field for the selected type:
									</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												<code class="literal">old: {}</code>
											</li><li class="listitem">
												<code class="literal">intermediate: {}</code>
											</li><li class="listitem">
												<code class="literal">custom:</code>
											</li></ul></div></dd><dt><a href="#CO9-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										For the <code class="literal">custom</code> type, specify a list of TLS ciphers and minimum accepted TLS version.
									</div></dd></dl></div></li><li class="listitem">
								Save the file to apply the changes.
							</li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Verify that the profile is set in the <code class="literal">IngressController</code> CR:
							</p><pre class="programlisting language-terminal">$ oc describe IngressController default -n openshift-ingress-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Name:         default
Namespace:    openshift-ingress-operator
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  operator.openshift.io/v1
Kind:         IngressController
 ...
Spec:
 ...
  Tls Security Profile:
    Custom:
      Ciphers:
        ECDHE-ECDSA-CHACHA20-POLY1305
        ECDHE-RSA-CHACHA20-POLY1305
        ECDHE-RSA-AES128-GCM-SHA256
        ECDHE-ECDSA-AES128-GCM-SHA256
      Min TLS Version:  VersionTLS11
    Type:               Custom
 ...</pre>

								</p></div></li></ul></div></section><section class="section" id="nw-mutual-tls-auth_configuring-ingress"><div class="titlepage"><div><div><h4 class="title">7.3.1.3. Configuring mutual TLS authentication</h4></div></div></div><p>
						You can configure the Ingress Controller to enable mutual TLS (mTLS) authentication by setting a <code class="literal">spec.clientTLS</code> value. The <code class="literal">clientTLS</code> value configures the Ingress Controller to verify client certificates. This configuration includes setting a <code class="literal">clientCA</code> value, which is a reference to a config map. The config map contains the PEM-encoded CA certificate bundle that is used to verify a client’s certificate. Optionally, you can also configure a list of certificate subject filters.
					</p><p>
						If the <code class="literal">clientCA</code> value specifies an X509v3 certificate revocation list (CRL) distribution point, the Ingress Operator downloads and manages a CRL config map based on the HTTP URI X509v3 <code class="literal">CRL Distribution Point</code> specified in each provided certificate. The Ingress Controller uses this config map during mTLS/TLS negotiation. Requests that do not provide valid certificates are rejected.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								You have a PEM-encoded CA certificate bundle.
							</li><li class="listitem"><p class="simpara">
								If your CA bundle references a CRL distribution point, you must have also included the end-entity or leaf certificate to the client CA bundle. This certificate must have included an HTTP URI under <code class="literal">CRL Distribution Points</code>, as described in RFC 5280. For example:
							</p><pre class="programlisting language-terminal"> Issuer: C=US, O=Example Inc, CN=Example Global G2 TLS RSA SHA256 2020 CA1
         Subject: SOME SIGNED CERT            X509v3 CRL Distribution Points:
                Full Name:
                  URI:http://crl.example.com/example.crl</pre></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								In the <code class="literal">openshift-config</code> namespace, create a config map from your CA bundle:
							</p><pre class="programlisting language-terminal">$ oc create configmap \
   router-ca-certs-default \
   --from-file=ca-bundle.pem=client-ca.crt \<span id="CO10-1"><!--Empty--></span><span class="callout">1</span>
   -n openshift-config</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO10-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The config map data key must be <code class="literal">ca-bundle.pem</code>, and the data value must be a CA certificate in PEM format.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Edit the <code class="literal">IngressController</code> resource in the <code class="literal">openshift-ingress-operator</code> project:
							</p><pre class="programlisting language-terminal">$ oc edit IngressController default -n openshift-ingress-operator</pre></li><li class="listitem"><p class="simpara">
								Add the <code class="literal">spec.clientTLS</code> field and subfields to configure mutual TLS:
							</p><div class="formalpara"><p class="title"><strong>Sample <code class="literal">IngressController</code> CR for a <code class="literal">clientTLS</code> profile that specifies filtering patterns</strong></p><p>
									
<pre class="programlisting language-yaml">  apiVersion: operator.openshift.io/v1
  kind: IngressController
  metadata:
    name: default
    namespace: openshift-ingress-operator
  spec:
    clientTLS:
      clientCertificatePolicy: Required
      clientCA:
        name: router-ca-certs-default
      allowedSubjectPatterns:
      - "^/CN=example.com/ST=NC/C=US/O=Security/OU=OpenShift$"</pre>

								</p></div></li></ol></div></section></section></section><section class="section" id="nw-ingress-view_configuring-ingress"><div class="titlepage"><div><div><h2 class="title">7.4. View the default Ingress Controller</h2></div></div></div><p>
				The Ingress Operator is a core feature of OpenShift Container Platform and is enabled out of the box.
			</p><p>
				Every new OpenShift Container Platform installation has an <code class="literal">ingresscontroller</code> named default. It can be supplemented with additional Ingress Controllers. If the default <code class="literal">ingresscontroller</code> is deleted, the Ingress Operator will automatically recreate it within a minute.
			</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						View the default Ingress Controller:
					</p><pre class="programlisting language-terminal">$ oc describe --namespace=openshift-ingress-operator ingresscontroller/default</pre></li></ul></div></section><section class="section" id="nw-ingress-operator-status_configuring-ingress"><div class="titlepage"><div><div><h2 class="title">7.5. View Ingress Operator status</h2></div></div></div><p>
				You can view and inspect the status of your Ingress Operator.
			</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						View your Ingress Operator status:
					</p><pre class="programlisting language-terminal">$ oc describe clusteroperators/ingress</pre></li></ul></div></section><section class="section" id="nw-ingress-operator-logs_configuring-ingress"><div class="titlepage"><div><div><h2 class="title">7.6. View Ingress Controller logs</h2></div></div></div><p>
				You can view your Ingress Controller logs.
			</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						View your Ingress Controller logs:
					</p><pre class="programlisting language-terminal">$ oc logs --namespace=openshift-ingress-operator deployments/ingress-operator -c &lt;container_name&gt;</pre></li></ul></div></section><section class="section" id="nw-ingress-controller-status_configuring-ingress"><div class="titlepage"><div><div><h2 class="title">7.7. View Ingress Controller status</h2></div></div></div><p>
				Your can view the status of a particular Ingress Controller.
			</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						View the status of an Ingress Controller:
					</p><pre class="programlisting language-terminal">$ oc describe --namespace=openshift-ingress-operator ingresscontroller/&lt;name&gt;</pre></li></ul></div></section><section class="section" id="configuring-ingress-controller"><div class="titlepage"><div><div><h2 class="title">7.8. Configuring the Ingress Controller</h2></div></div></div><section class="section" id="nw-ingress-setting-a-custom-default-certificate_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.1. Setting a custom default certificate</h3></div></div></div><p>
					As an administrator, you can configure an Ingress Controller to use a custom certificate by creating a Secret resource and editing the <code class="literal">IngressController</code> custom resource (CR).
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You must have a certificate/key pair in PEM-encoded files, where the certificate is signed by a trusted certificate authority or by a private trusted certificate authority that you configured in a custom PKI.
						</li><li class="listitem"><p class="simpara">
							Your certificate meets the following requirements:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									The certificate is valid for the ingress domain.
								</li><li class="listitem">
									The certificate uses the <code class="literal">subjectAltName</code> extension to specify a wildcard domain, such as <code class="literal">*.apps.ocp4.example.com</code>.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							You must have an <code class="literal">IngressController</code> CR. You may use the default one:
						</p><pre class="programlisting language-terminal">$ oc --namespace openshift-ingress-operator get ingresscontrollers</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      AGE
default   10m</pre>

							</p></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you have intermediate certificates, they must be included in the <code class="literal">tls.crt</code> file of the secret containing a custom default certificate. Order matters when specifying a certificate; list your intermediate certificate(s) after any server certificate(s).
					</p></div></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						The following assumes that the custom certificate and key pair are in the <code class="literal">tls.crt</code> and <code class="literal">tls.key</code> files in the current working directory. Substitute the actual path names for <code class="literal">tls.crt</code> and <code class="literal">tls.key</code>. You also may substitute another name for <code class="literal">custom-certs-default</code> when creating the Secret resource and referencing it in the IngressController CR.
					</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						This action will cause the Ingress Controller to be redeployed, using a rolling deployment strategy.
					</p></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a Secret resource containing the custom certificate in the <code class="literal">openshift-ingress</code> namespace using the <code class="literal">tls.crt</code> and <code class="literal">tls.key</code> files.
						</p><pre class="programlisting language-terminal">$ oc --namespace openshift-ingress create secret tls custom-certs-default --cert=tls.crt --key=tls.key</pre></li><li class="listitem"><p class="simpara">
							Update the IngressController CR to reference the new certificate secret:
						</p><pre class="programlisting language-terminal">$ oc patch --type=merge --namespace openshift-ingress-operator ingresscontrollers/default \
  --patch '{"spec":{"defaultCertificate":{"name":"custom-certs-default"}}}'</pre></li><li class="listitem"><p class="simpara">
							Verify the update was effective:
						</p><pre class="programlisting language-terminal">$ echo Q |\
  openssl s_client -connect console-openshift-console.apps.&lt;domain&gt;:443 -showcerts 2&gt;/dev/null |\
  openssl x509 -noout -subject -issuer -enddate</pre><p class="simpara">
							where:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;domain&gt;</code></span></dt><dd>
										Specifies the base domain name for your cluster.
									</dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">subject=C = US, ST = NC, L = Raleigh, O = RH, OU = OCP4, CN = *.apps.example.com
issuer=C = US, ST = NC, L = Raleigh, O = RH, OU = OCP4, CN = example.com
notAfter=May 10 08:32:45 2022 GM</pre>

							</p></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can alternatively apply the following YAML to set a custom default certificate:
						</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  defaultCertificate:
    name: custom-certs-default</pre></div></div><p class="simpara">
							The certificate secret name should match the value used to update the CR.
						</p></li></ol></div><p>
					Once the IngressController CR has been modified, the Ingress Operator updates the Ingress Controller’s deployment to use the custom certificate.
				</p></section><section class="section" id="nw-ingress-custom-default-certificate-remove_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.2. Removing a custom default certificate</h3></div></div></div><p>
					As an administrator, you can remove a custom certificate that you configured an Ingress Controller to use.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You previously configured a custom default certificate for the Ingress Controller.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To remove the custom certificate and restore the certificate that ships with OpenShift Container Platform, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc patch -n openshift-ingress-operator ingresscontrollers/default \
  --type json -p $'- op: remove\n  path: /spec/defaultCertificate'</pre><p class="simpara">
							There can be a delay while the cluster reconciles the new certificate configuration.
						</p></li></ul></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To confirm that the original cluster certificate is restored, enter the following command:
						</p><pre class="programlisting language-terminal">$ echo Q | \
  openssl s_client -connect console-openshift-console.apps.&lt;domain&gt;:443 -showcerts 2&gt;/dev/null | \
  openssl x509 -noout -subject -issuer -enddate</pre><p class="simpara">
							where:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;domain&gt;</code></span></dt><dd>
										Specifies the base domain name for your cluster.
									</dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">subject=CN = *.apps.&lt;domain&gt;
issuer=CN = ingress-operator@1620633373
notAfter=May 10 10:44:36 2023 GMT</pre>

							</p></div></li></ul></div></section><section class="section" id="nw-autoscaling-ingress-controller_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.3. Autoscaling an Ingress Controller</h3></div></div></div><p>
					Automatically scale an Ingress Controller to dynamically meet routing performance or availability requirements such as the requirement to increase throughput. The following procedure provides an example for scaling up the default <code class="literal">IngressController</code>.
				</p><div class="orderedlist"><p class="title"><strong>Prerequisites</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							You have the OpenShift CLI (<code class="literal">oc</code>) installed.
						</li><li class="listitem">
							You have access to an OpenShift Container Platform cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							You have the Custom Metrics Autoscaler Operator installed.
						</li></ol></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a project in the <code class="literal">openshift-ingress-operator</code> namespace by running the following command:
						</p><pre class="programlisting language-terminal">$ oc project openshift-ingress-operator</pre></li><li class="listitem"><p class="simpara">
							Enable OpenShift monitoring for user-defined projects by creating and applying a config map:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a new <code class="literal">ConfigMap</code> object, <code class="literal">cluster-monitoring-config.yaml</code>:
								</p><div class="formalpara"><p class="title"><strong>cluster-monitoring-config.yaml</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: true <span id="CO11-1"><!--Empty--></span><span class="callout">1</span></pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO11-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											When set to <code class="literal">true</code>, the <code class="literal">enableUserWorkload</code> parameter enables monitoring for user-defined projects in a cluster.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Apply the config map by running the following command:
								</p><pre class="programlisting language-terminal">$ oc apply -f cluster-monitoring-config.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a service account to authenticate with Thanos by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create serviceaccount thanos &amp;&amp; oc describe serviceaccount thanos</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Name:                thanos
Namespace:           openshift-ingress-operator
Labels:              &lt;none&gt;
Annotations:         &lt;none&gt;
Image pull secrets:  thanos-dockercfg-b4l9s
Mountable secrets:   thanos-dockercfg-b4l9s
Tokens:              thanos-token-c422q
Events:              &lt;none&gt;</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Define a <code class="literal">TriggerAuthentication</code> object within the <code class="literal">openshift-ingress-operator</code> namespace using the service account’s token.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Define the variable <code class="literal">secret</code> that contains the secret by running the following command:
								</p><pre class="programlisting language-terminal">$ secret=$(oc get secret | grep thanos-token | head -n 1 | awk '{ print $1 }')</pre></li><li class="listitem"><p class="simpara">
									Create the <code class="literal">TriggerAuthentication</code> object and pass the value of the <code class="literal">secret</code> variable to the <code class="literal">TOKEN</code> parameter:
								</p><pre class="programlisting language-terminal">$ oc process TOKEN="$secret" -f - &lt;&lt;EOF | oc apply -f -
apiVersion: template.openshift.io/v1
kind: Template
parameters:
- name: TOKEN
objects:
- apiVersion: keda.sh/v1alpha1
  kind: TriggerAuthentication
  metadata:
    name: keda-trigger-auth-prometheus
  spec:
    secretTargetRef:
    - parameter: bearerToken
      name: \${TOKEN}
      key: token
    - parameter: ca
      name: \${TOKEN}
      key: ca.crt
EOF</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create and apply a role for reading metrics from Thanos:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a new role, <code class="literal">thanos-metrics-reader.yaml</code>, that reads metrics from pods and nodes:
								</p><div class="formalpara"><p class="title"><strong>thanos-metrics-reader.yaml</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: thanos-metrics-reader
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - nodes
  verbs:
  - get
- apiGroups:
  - metrics.k8s.io
  resources:
  - pods
  - nodes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Apply the new role by running the following command:
								</p><pre class="programlisting language-terminal">$ oc apply -f thanos-metrics-reader.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Add the new role to the service account by entering the following commands:
						</p><pre class="programlisting language-terminal">$ oc adm policy add-role-to-user thanos-metrics-reader -z thanos --role=namespace=openshift-ingress-operator</pre><pre class="programlisting language-terminal">$ oc adm policy -n openshift-ingress-operator add-cluster-role-to-user cluster-monitoring-view -z thanos</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The argument <code class="literal">add-cluster-role-to-user</code> is only required if you use cross-namespace queries. The following step uses a query from the <code class="literal">kube-metrics</code> namespace which requires this argument.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Create a new <code class="literal">ScaledObject</code> YAML file, <code class="literal">ingress-autoscaler.yaml</code>, that targets the default Ingress Controller deployment:
						</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">ScaledObject</code> definition</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: ingress-scaler
spec:
  scaleTargetRef: <span id="CO12-1"><!--Empty--></span><span class="callout">1</span>
    apiVersion: operator.openshift.io/v1
    kind: IngressController
    name: default
    envSourceContainerName: ingress-operator
  minReplicaCount: 1
  maxReplicaCount: 20 <span id="CO12-2"><!--Empty--></span><span class="callout">2</span>
  cooldownPeriod: 1
  pollingInterval: 1
  triggers:
  - type: prometheus
    metricType: AverageValue
    metadata:
      serverAddress: https://&lt;example-cluster&gt;:9091 <span id="CO12-3"><!--Empty--></span><span class="callout">3</span>
      namespace: openshift-ingress-operator <span id="CO12-4"><!--Empty--></span><span class="callout">4</span>
      metricName: 'kube-node-role'
      threshold: '1'
      query: 'sum(kube_node_role{role="worker",service="kube-state-metrics"})' <span id="CO12-5"><!--Empty--></span><span class="callout">5</span>
      authModes: "bearer"
    authenticationRef:
      name: keda-trigger-auth-prometheus</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO12-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The custom resource that you are targeting. In this case, the Ingress Controller.
								</div></dd><dt><a href="#CO12-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Optional: The maximum number of replicas. If you omit this field, the default maximum is set to 100 replicas.
								</div></dd><dt><a href="#CO12-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The cluster address and port.
								</div></dd><dt><a href="#CO12-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The Ingress Operator namespace.
								</div></dd><dt><a href="#CO12-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									This expression evaluates to however many worker nodes are present in the deployed cluster.
								</div></dd></dl></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								If you are using cross-namespace queries, you must target port 9091 and not port 9092 in the <code class="literal">serverAddress</code> field. You also must have elevated privileges to read metrics from this port.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Apply the custom resource definition by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f ingress-autoscaler.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Verify that the default Ingress Controller is scaled out to match the value returned by the <code class="literal">kube-state-metrics</code> query by running the following commands:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
									Use the <code class="literal">grep</code> command to search the Ingress Controller YAML file for replicas:
								</p><pre class="programlisting language-terminal">$ oc get ingresscontroller/default -o yaml | grep replicas:</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">replicas: 3</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Get the pods in the <code class="literal">openshift-ingress</code> project:
								</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-ingress</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                             READY   STATUS    RESTARTS   AGE
router-default-7b5df44ff-l9pmm   2/2     Running   0          17h
router-default-7b5df44ff-s5sl5   2/2     Running   0          3d22h
router-default-7b5df44ff-wwsth   2/2     Running   0          66s</pre>

									</p></div></li></ul></div></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/monitoring/#enabling-monitoring-for-user-defined-projects_enabling-monitoring-for-user-defined-projects">Enabling monitoring for user-defined projects</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-cma-autoscaling-custom-install">Installing the custom metrics autoscaler</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-cma-autoscaling-custom-trigger-auth">Understanding custom metrics autoscaler trigger authentications</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-cma-autoscaling-custom-prometheus">Configuring the custom metrics autoscaler to use OpenShift Container Platform monitoring</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-cma-autoscaling-custom-adding">Understanding how to add custom metrics autoscalers</a>
						</li></ul></div></section><section class="section" id="nw-ingress-controller-configuration_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.4. Scaling an Ingress Controller</h3></div></div></div><p>
					Manually scale an Ingress Controller to meeting routing performance or availability requirements such as the requirement to increase throughput. <code class="literal">oc</code> commands are used to scale the <code class="literal">IngressController</code> resource. The following procedure provides an example for scaling up the default <code class="literal">IngressController</code>.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Scaling is not an immediate action, as it takes time to create the desired number of replicas.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the current number of available replicas for the default <code class="literal">IngressController</code>:
						</p><pre class="programlisting language-terminal">$ oc get -n openshift-ingress-operator ingresscontrollers/default -o jsonpath='{$.status.availableReplicas}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">2</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Scale the default <code class="literal">IngressController</code> to the desired number of replicas using the <code class="literal">oc patch</code> command. The following example scales the default <code class="literal">IngressController</code> to 3 replicas:
						</p><pre class="programlisting language-terminal">$ oc patch -n openshift-ingress-operator ingresscontroller/default --patch '{"spec":{"replicas": 3}}' --type=merge</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">ingresscontroller.operator.openshift.io/default patched</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify that the default <code class="literal">IngressController</code> scaled to the number of replicas that you specified:
						</p><pre class="programlisting language-terminal">$ oc get -n openshift-ingress-operator ingresscontrollers/default -o jsonpath='{$.status.availableReplicas}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">3</pre>

							</p></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can alternatively apply the following YAML to scale an Ingress Controller to three replicas:
						</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  replicas: 3               <span id="CO13-1"><!--Empty--></span><span class="callout">1</span></pre></div></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO13-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									If you need a different amount of replicas, change the <code class="literal">replicas</code> value.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="nw-configure-ingress-access-logging_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.5. Configuring Ingress access logging</h3></div></div></div><p>
					You can configure the Ingress Controller to enable access logs. If you have clusters that do not receive much traffic, then you can log to a sidecar. If you have high traffic clusters, to avoid exceeding the capacity of the logging stack or to integrate with a logging infrastructure outside of OpenShift Container Platform, you can forward logs to a custom syslog endpoint. You can also specify the format for access logs.
				</p><p>
					Container logging is useful to enable access logs on low-traffic clusters when there is no existing Syslog logging infrastructure, or for short-term use while diagnosing problems with the Ingress Controller.
				</p><p>
					Syslog is needed for high-traffic clusters where access logs could exceed the OpenShift Logging stack’s capacity, or for environments where any logging solution needs to integrate with an existing Syslog logging infrastructure. The Syslog use-cases can overlap.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						Configure Ingress access logging to a sidecar.
					</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To configure Ingress access logging, you must specify a destination using <code class="literal">spec.logging.access.destination</code>. To specify logging to a sidecar container, you must specify <code class="literal">Container</code> <code class="literal">spec.logging.access.destination.type</code>. The following example is an Ingress Controller definition that logs to a <code class="literal">Container</code> destination:
						</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  replicas: 2
  logging:
    access:
      destination:
        type: Container</pre></li><li class="listitem"><p class="simpara">
							When you configure the Ingress Controller to log to a sidecar, the operator creates a container named <code class="literal">logs</code> inside the Ingress Controller Pod:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress logs deployment.apps/router-default -c logs</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">2020-05-11T19:11:50.135710+00:00 router-default-57dfc6cd95-bpmk6 router-default-57dfc6cd95-bpmk6 haproxy[108]: 174.19.21.82:39654 [11/May/2020:19:11:50.133] public be_http:hello-openshift:hello-openshift/pod:hello-openshift:hello-openshift:10.128.2.12:8080 0/0/1/0/1 200 142 - - --NI 1/1/0/0/0 0/0 "GET / HTTP/1.1"</pre>

							</p></div></li></ul></div><p>
					Configure Ingress access logging to a Syslog endpoint.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To configure Ingress access logging, you must specify a destination using <code class="literal">spec.logging.access.destination</code>. To specify logging to a Syslog endpoint destination, you must specify <code class="literal">Syslog</code> for <code class="literal">spec.logging.access.destination.type</code>. If the destination type is <code class="literal">Syslog</code>, you must also specify a destination endpoint using <code class="literal">spec.logging.access.destination.syslog.endpoint</code> and you can specify a facility using <code class="literal">spec.logging.access.destination.syslog.facility</code>. The following example is an Ingress Controller definition that logs to a <code class="literal">Syslog</code> destination:
						</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  replicas: 2
  logging:
    access:
      destination:
        type: Syslog
        syslog:
          address: 1.2.3.4
          port: 10514</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The <code class="literal">syslog</code> destination port must be UDP.
							</p></div></div></li></ul></div><p>
					Configure Ingress access logging with a specific log format.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							You can specify <code class="literal">spec.logging.access.httpLogFormat</code> to customize the log format. The following example is an Ingress Controller definition that logs to a <code class="literal">syslog</code> endpoint with IP address 1.2.3.4 and port 10514:
						</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  replicas: 2
  logging:
    access:
      destination:
        type: Syslog
        syslog:
          address: 1.2.3.4
          port: 10514
      httpLogFormat: '%ci:%cp [%t] %ft %b/%s %B %bq %HM %HU %HV'</pre></li></ul></div><p>
					Disable Ingress access logging.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To disable Ingress access logging, leave <code class="literal">spec.logging</code> or <code class="literal">spec.logging.access</code> empty:
						</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  replicas: 2
  logging:
    access: null</pre></li></ul></div></section><section class="section" id="nw-ingress-setting-thread-count_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.6. Setting Ingress Controller thread count</h3></div></div></div><p>
					A cluster administrator can set the thread count to increase the amount of incoming connections a cluster can handle. You can patch an existing Ingress Controller to increase the amount of threads.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The following assumes that you already created an Ingress Controller.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Update the Ingress Controller to increase the number of threads:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress-operator patch ingresscontroller/default --type=merge -p '{"spec":{"tuningOptions": {"threadCount": 8}}}'</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If you have a node that is capable of running large amounts of resources, you can configure <code class="literal">spec.nodePlacement.nodeSelector</code> with labels that match the capacity of the intended node, and configure <code class="literal">spec.tuningOptions.threadCount</code> to an appropriately high value.
							</p></div></div></li></ul></div></section><section class="section" id="nw-ingress-setting-internal-lb_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.7. Configuring an Ingress Controller to use an internal load balancer</h3></div></div></div><p>
					When creating an Ingress Controller on cloud platforms, the Ingress Controller is published by a public cloud load balancer by default. As an administrator, you can create an Ingress Controller that uses an internal cloud load balancer.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						If your cloud provider is Microsoft Azure, you must have at least one public load balancer that points to your nodes. If you do not, all of your nodes will lose egress connectivity to the internet.
					</p></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						If you want to change the <code class="literal">scope</code> for an <code class="literal">IngressController</code>, you can change the <code class="literal">.spec.endpointPublishingStrategy.loadBalancer.scope</code> parameter after the custom resource (CR) is created.
					</p></div></div><div class="figure" id="idm140587167111168"><p class="title"><strong>Figure 7.1. Diagram of LoadBalancer</strong></p><div class="figure-contents"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/b4b1d1d35869ccf8ab16c88a9c99ea7a/202_OpenShift_Ingress_0222_load_balancer.png" alt="OpenShift Container Platform Ingress LoadBalancerService endpoint publishing strategy"/></div></div></div><p>
					The preceding graphic shows the following concepts pertaining to OpenShift Container Platform Ingress LoadBalancerService endpoint publishing strategy:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							You can load balance externally, using the cloud provider load balancer, or internally, using the OpenShift Ingress Controller Load Balancer.
						</li><li class="listitem">
							You can use the single IP address of the load balancer and more familiar ports, such as 8080 and 4200 as shown on the cluster depicted in the graphic.
						</li><li class="listitem">
							Traffic from the external load balancer is directed at the pods, and managed by the load balancer, as depicted in the instance of a down node. See the <a class="link" href="https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer">Kubernetes Services documentation</a> for implementation details.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an <code class="literal">IngressController</code> custom resource (CR) in a file named <code class="literal">&lt;name&gt;-ingress-controller.yaml</code>, such as in the following example:
						</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  namespace: openshift-ingress-operator
  name: &lt;name&gt; <span id="CO14-1"><!--Empty--></span><span class="callout">1</span>
spec:
  domain: &lt;domain&gt; <span id="CO14-2"><!--Empty--></span><span class="callout">2</span>
  endpointPublishingStrategy:
    type: LoadBalancerService
    loadBalancer:
      scope: Internal <span id="CO14-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO14-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal">&lt;name&gt;</code> with a name for the <code class="literal">IngressController</code> object.
								</div></dd><dt><a href="#CO14-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">domain</code> for the application published by the controller.
								</div></dd><dt><a href="#CO14-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify a value of <code class="literal">Internal</code> to use an internal load balancer.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the Ingress Controller defined in the previous step by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f &lt;name&gt;-ingress-controller.yaml <span id="CO15-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO15-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal">&lt;name&gt;</code> with the name of the <code class="literal">IngressController</code> object.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Optional: Confirm that the Ingress Controller was created by running the following command:
						</p><pre class="programlisting language-terminal">$ oc --all-namespaces=true get ingresscontrollers</pre></li></ol></div></section><section class="section" id="nw-ingress-controller-configuration-gcp-global-access_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.8. Configuring global access for an Ingress Controller on GCP</h3></div></div></div><p>
					An Ingress Controller created on GCP with an internal load balancer generates an internal IP address for the service. A cluster administrator can specify the global access option, which enables clients in any region within the same VPC network and compute region as the load balancer, to reach the workloads running on your cluster.
				</p><p>
					For more information, see the GCP documentation for <a class="link" href="https://cloud.google.com/kubernetes-engine/docs/how-to/internal-load-balancing#global_access">global access</a>.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You deployed an OpenShift Container Platform cluster on GCP infrastructure.
						</li><li class="listitem">
							You configured an Ingress Controller to use an internal load balancer.
						</li><li class="listitem">
							You installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Configure the Ingress Controller resource to allow global access.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								You can also create an Ingress Controller and specify the global access option.
							</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Configure the Ingress Controller resource:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress-operator edit ingresscontroller/default</pre></li><li class="listitem"><p class="simpara">
									Edit the YAML file:
								</p><div class="formalpara"><p class="title"><strong>Sample <code class="literal">clientAccess</code> configuration to <code class="literal">Global</code></strong></p><p>
										
<pre class="programlisting language-yaml">  spec:
    endpointPublishingStrategy:
      loadBalancer:
        providerParameters:
          gcp:
            clientAccess: Global <span id="CO16-1"><!--Empty--></span><span class="callout">1</span>
          type: GCP
        scope: Internal
      type: LoadBalancerService</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO16-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Set <code class="literal">gcp.clientAccess</code> to <code class="literal">Global</code>.
										</div></dd></dl></div></li><li class="listitem">
									Save the file to apply the changes.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Run the following command to verify that the service allows global access:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress edit svc/router-default -o yaml</pre><p class="simpara">
							The output shows that global access is enabled for GCP with the annotation, <code class="literal">networking.gke.io/internal-load-balancer-allow-global-access</code>.
						</p></li></ol></div></section><section class="section" id="nw-ingress-controller-config-tuningoptions-healthcheckinterval_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.9. Setting the Ingress Controller health check interval</h3></div></div></div><p>
					A cluster administrator can set the health check interval to define how long the router waits between two consecutive health checks. This value is applied globally as a default for all routes. The default value is 5 seconds.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The following assumes that you already created an Ingress Controller.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Update the Ingress Controller to change the interval between back end health checks:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress-operator patch ingresscontroller/default --type=merge -p '{"spec":{"tuningOptions": {"healthCheckInterval": "8s"}}}'</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								To override the <code class="literal">healthCheckInterval</code> for a single route, use the route annotation <code class="literal">router.openshift.io/haproxy.health.check.interval</code>
							</p></div></div></li></ul></div></section><section class="section" id="nw-ingress-default-internal_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.10. Configuring the default Ingress Controller for your cluster to be internal</h3></div></div></div><p>
					You can configure the <code class="literal">default</code> Ingress Controller for your cluster to be internal by deleting and recreating it.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						If your cloud provider is Microsoft Azure, you must have at least one public load balancer that points to your nodes. If you do not, all of your nodes will lose egress connectivity to the internet.
					</p></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						If you want to change the <code class="literal">scope</code> for an <code class="literal">IngressController</code>, you can change the <code class="literal">.spec.endpointPublishingStrategy.loadBalancer.scope</code> parameter after the custom resource (CR) is created.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Configure the <code class="literal">default</code> Ingress Controller for your cluster to be internal by deleting and recreating it.
						</p><pre class="programlisting language-terminal">$ oc replace --force --wait --filename - &lt;&lt;EOF
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  namespace: openshift-ingress-operator
  name: default
spec:
  endpointPublishingStrategy:
    type: LoadBalancerService
    loadBalancer:
      scope: Internal
EOF</pre></li></ol></div></section><section class="section" id="nw-route-admission-policy_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.11. Configuring the route admission policy</h3></div></div></div><p>
					Administrators and application developers can run applications in multiple namespaces with the same domain name. This is for organizations where multiple teams develop microservices that are exposed on the same hostname.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Allowing claims across namespaces should only be enabled for clusters with trust between namespaces, otherwise a malicious user could take over a hostname. For this reason, the default admission policy disallows hostname claims across namespaces.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Cluster administrator privileges.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">.spec.routeAdmission</code> field of the <code class="literal">ingresscontroller</code> resource variable using the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress-operator patch ingresscontroller/default --patch '{"spec":{"routeAdmission":{"namespaceOwnership":"InterNamespaceAllowed"}}}' --type=merge</pre><div class="formalpara"><p class="title"><strong>Sample Ingress Controller configuration</strong></p><p>
								
<pre class="programlisting language-yaml">spec:
  routeAdmission:
    namespaceOwnership: InterNamespaceAllowed
...</pre>

							</p></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can alternatively apply the following YAML to configure the route admission policy:
						</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  routeAdmission:
    namespaceOwnership: InterNamespaceAllowed</pre></div></div></li></ul></div></section><section class="section" id="using-wildcard-routes_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.12. Using wildcard routes</h3></div></div></div><p>
					The HAProxy Ingress Controller has support for wildcard routes. The Ingress Operator uses <code class="literal">wildcardPolicy</code> to configure the <code class="literal">ROUTER_ALLOW_WILDCARD_ROUTES</code> environment variable of the Ingress Controller.
				</p><p>
					The default behavior of the Ingress Controller is to admit routes with a wildcard policy of <code class="literal">None</code>, which is backwards compatible with existing <code class="literal">IngressController</code> resources.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Configure the wildcard policy.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Use the following command to edit the <code class="literal">IngressController</code> resource:
								</p><pre class="programlisting language-terminal">$ oc edit IngressController</pre></li><li class="listitem"><p class="simpara">
									Under <code class="literal">spec</code>, set the <code class="literal">wildcardPolicy</code> field to <code class="literal">WildcardsDisallowed</code> or <code class="literal">WildcardsAllowed</code>:
								</p><pre class="programlisting language-yaml">spec:
  routeAdmission:
    wildcardPolicy: WildcardsDisallowed # or WildcardsAllowed</pre></li></ol></div></li></ol></div></section><section class="section" id="nw-using-ingress-forwarded_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.13. Using X-Forwarded headers</h3></div></div></div><p>
					You configure the HAProxy Ingress Controller to specify a policy for how to handle HTTP headers including <code class="literal">Forwarded</code> and <code class="literal">X-Forwarded-For</code>. The Ingress Operator uses the <code class="literal">HTTPHeaders</code> field to configure the <code class="literal">ROUTER_SET_FORWARDED_HEADERS</code> environment variable of the Ingress Controller.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Configure the <code class="literal">HTTPHeaders</code> field for the Ingress Controller.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Use the following command to edit the <code class="literal">IngressController</code> resource:
								</p><pre class="programlisting language-terminal">$ oc edit IngressController</pre></li><li class="listitem"><p class="simpara">
									Under <code class="literal">spec</code>, set the <code class="literal">HTTPHeaders</code> policy field to <code class="literal">Append</code>, <code class="literal">Replace</code>, <code class="literal">IfNone</code>, or <code class="literal">Never</code>:
								</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  httpHeaders:
    forwardedHeaderPolicy: Append</pre></li></ol></div></li></ol></div><h5 id="example-use-cases">Example use cases</h5><p>
					<span class="strong strong"><strong>As a cluster administrator, you can:</strong></span>
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Configure an external proxy that injects the <code class="literal">X-Forwarded-For</code> header into each request before forwarding it to an Ingress Controller.
						</p><p class="simpara">
							To configure the Ingress Controller to pass the header through unmodified, you specify the <code class="literal">never</code> policy. The Ingress Controller then never sets the headers, and applications receive only the headers that the external proxy provides.
						</p></li><li class="listitem"><p class="simpara">
							Configure the Ingress Controller to pass the <code class="literal">X-Forwarded-For</code> header that your external proxy sets on external cluster requests through unmodified.
						</p><p class="simpara">
							To configure the Ingress Controller to set the <code class="literal">X-Forwarded-For</code> header on internal cluster requests, which do not go through the external proxy, specify the <code class="literal">if-none</code> policy. If an HTTP request already has the header set through the external proxy, then the Ingress Controller preserves it. If the header is absent because the request did not come through the proxy, then the Ingress Controller adds the header.
						</p></li></ul></div><p>
					<span class="strong strong"><strong>As an application developer, you can:</strong></span>
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Configure an application-specific external proxy that injects the <code class="literal">X-Forwarded-For</code> header.
						</p><p class="simpara">
							To configure an Ingress Controller to pass the header through unmodified for an application’s Route, without affecting the policy for other Routes, add an annotation <code class="literal">haproxy.router.openshift.io/set-forwarded-headers: if-none</code> or <code class="literal">haproxy.router.openshift.io/set-forwarded-headers: never</code> on the Route for the application.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								You can set the <code class="literal">haproxy.router.openshift.io/set-forwarded-headers</code> annotation on a per route basis, independent from the globally set value for the Ingress Controller.
							</p></div></div></li></ul></div></section><section class="section" id="nw-http2-haproxy_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.14. Enabling HTTP/2 Ingress connectivity</h3></div></div></div><p>
					You can enable transparent end-to-end HTTP/2 connectivity in HAProxy. It allows application owners to make use of HTTP/2 protocol capabilities, including single connection, header compression, binary streams, and more.
				</p><p>
					You can enable HTTP/2 connectivity for an individual Ingress Controller or for the entire cluster.
				</p><p>
					To enable the use of HTTP/2 for the connection from the client to HAProxy, a route must specify a custom certificate. A route that uses the default certificate cannot use HTTP/2. This restriction is necessary to avoid problems from connection coalescing, where the client re-uses a connection for different routes that use the same certificate.
				</p><p>
					The connection from HAProxy to the application pod can use HTTP/2 only for re-encrypt routes and not for edge-terminated or insecure routes. This restriction is because HAProxy uses Application-Level Protocol Negotiation (ALPN), which is a TLS extension, to negotiate the use of HTTP/2 with the back-end. The implication is that end-to-end HTTP/2 is possible with passthrough and re-encrypt and not with insecure or edge-terminated routes.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Using WebSockets with a re-encrypt route and with HTTP/2 enabled on an Ingress Controller requires WebSocket support over HTTP/2. WebSockets over HTTP/2 is a feature of HAProxy 2.4, which is unsupported in OpenShift Container Platform at this time.
					</p></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						For non-passthrough routes, the Ingress Controller negotiates its connection to the application independently of the connection from the client. This means a client may connect to the Ingress Controller and negotiate HTTP/1.1, and the Ingress Controller may then connect to the application, negotiate HTTP/2, and forward the request from the client HTTP/1.1 connection using the HTTP/2 connection to the application. This poses a problem if the client subsequently tries to upgrade its connection from HTTP/1.1 to the WebSocket protocol, because the Ingress Controller cannot forward WebSocket to HTTP/2 and cannot upgrade its HTTP/2 connection to WebSocket. Consequently, if you have an application that is intended to accept WebSocket connections, it must not allow negotiating the HTTP/2 protocol or else clients will fail to upgrade to the WebSocket protocol.
					</p></div></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						Enable HTTP/2 on a single Ingress Controller.
					</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To enable HTTP/2 on an Ingress Controller, enter the <code class="literal">oc annotate</code> command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress-operator annotate ingresscontrollers/&lt;ingresscontroller_name&gt; ingress.operator.openshift.io/default-enable-http2=true</pre><p class="simpara">
							Replace <code class="literal">&lt;ingresscontroller_name&gt;</code> with the name of the Ingress Controller to annotate.
						</p></li></ul></div><p>
					Enable HTTP/2 on the entire cluster.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To enable HTTP/2 for the entire cluster, enter the <code class="literal">oc annotate</code> command:
						</p><pre class="programlisting language-terminal">$ oc annotate ingresses.config/cluster ingress.operator.openshift.io/default-enable-http2=true</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can alternatively apply the following YAML to add the annotation:
						</p><pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Ingress
metadata:
  name: cluster
  annotations:
    ingress.operator.openshift.io/default-enable-http2: "true"</pre></div></div></li></ul></div></section><section class="section" id="nw-ingress-controller-configuration-proxy-protocol_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.15. Configuring the PROXY protocol for an Ingress Controller</h3></div></div></div><p>
					A cluster administrator can configure <a class="link" href="https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt">the PROXY protocol</a> when an Ingress Controller uses either the <code class="literal">HostNetwork</code> or <code class="literal">NodePortService</code> endpoint publishing strategy types. The PROXY protocol enables the load balancer to preserve the original client addresses for connections that the Ingress Controller receives. The original client addresses are useful for logging, filtering, and injecting HTTP headers. In the default configuration, the connections that the Ingress Controller receives only contain the source address that is associated with the load balancer.
				</p><p>
					This feature is not supported in cloud deployments. This restriction is because when OpenShift Container Platform runs in a cloud platform, and an IngressController specifies that a service load balancer should be used, the Ingress Operator configures the load balancer service and enables the PROXY protocol based on the platform requirement for preserving source addresses.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						You must configure both OpenShift Container Platform and the external load balancer to either use the PROXY protocol or to use TCP.
					</p></div></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						The PROXY protocol is unsupported for the default Ingress Controller with installer-provisioned clusters on non-cloud platforms that use a Keepalived Ingress VIP.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You created an Ingress Controller.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the Ingress Controller resource:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress-operator edit ingresscontroller/default</pre></li><li class="listitem"><p class="simpara">
							Set the PROXY configuration:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									If your Ingress Controller uses the hostNetwork endpoint publishing strategy type, set the <code class="literal">spec.endpointPublishingStrategy.hostNetwork.protocol</code> subfield to <code class="literal">PROXY</code>:
								</p><div class="formalpara"><p class="title"><strong>Sample <code class="literal">hostNetwork</code> configuration to <code class="literal">PROXY</code></strong></p><p>
										
<pre class="programlisting language-yaml">  spec:
    endpointPublishingStrategy:
      hostNetwork:
        protocol: PROXY
      type: HostNetwork</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									If your Ingress Controller uses the NodePortService endpoint publishing strategy type, set the <code class="literal">spec.endpointPublishingStrategy.nodePort.protocol</code> subfield to <code class="literal">PROXY</code>:
								</p><div class="formalpara"><p class="title"><strong>Sample <code class="literal">nodePort</code> configuration to <code class="literal">PROXY</code></strong></p><p>
										
<pre class="programlisting language-yaml">  spec:
    endpointPublishingStrategy:
      nodePort:
        protocol: PROXY
      type: NodePortService</pre>

									</p></div></li></ul></div></li></ol></div></section><section class="section" id="nw-ingress-configuring-application-domain_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.16. Specifying an alternative cluster domain using the appsDomain option</h3></div></div></div><p>
					As a cluster administrator, you can specify an alternative to the default cluster domain for user-created routes by configuring the <code class="literal">appsDomain</code> field. The <code class="literal">appsDomain</code> field is an optional domain for OpenShift Container Platform to use instead of the default, which is specified in the <code class="literal">domain</code> field. If you specify an alternative domain, it overrides the default cluster domain for the purpose of determining the default host for a new route.
				</p><p>
					For example, you can use the DNS domain for your company as the default domain for routes and ingresses for applications running on your cluster.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You deployed an OpenShift Container Platform cluster.
						</li><li class="listitem">
							You installed the <code class="literal">oc</code> command line interface.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Configure the <code class="literal">appsDomain</code> field by specifying an alternative default domain for user-created routes.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Edit the ingress <code class="literal">cluster</code> resource:
								</p><pre class="programlisting language-terminal">$ oc edit ingresses.config/cluster -o yaml</pre></li><li class="listitem"><p class="simpara">
									Edit the YAML file:
								</p><div class="formalpara"><p class="title"><strong>Sample <code class="literal">appsDomain</code> configuration to <code class="literal">test.example.com</code></strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Ingress
metadata:
  name: cluster
spec:
  domain: apps.example.com            <span id="CO17-1"><!--Empty--></span><span class="callout">1</span>
  appsDomain: &lt;test.example.com&gt;      <span id="CO17-2"><!--Empty--></span><span class="callout">2</span></pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO17-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specifies the default domain. You cannot modify the default domain after installation.
										</div></dd><dt><a href="#CO17-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Optional: Domain for OpenShift Container Platform infrastructure to use for application routes. Instead of the default prefix, <code class="literal">apps</code>, you can use an alternative prefix like <code class="literal">test</code>.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Verify that an existing route contains the domain name specified in the <code class="literal">appsDomain</code> field by exposing the route and verifying the route domain change:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Wait for the <code class="literal">openshift-apiserver</code> finish rolling updates before exposing the route.
							</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Expose the route:
								</p><pre class="programlisting language-terminal">$ oc expose service hello-openshift
route.route.openshift.io/hello-openshift exposed</pre><div class="formalpara"><p class="title"><strong>Example output:</strong></p><p>
										
<pre class="programlisting language-terminal">$ oc get routes
NAME              HOST/PORT                                   PATH   SERVICES          PORT       TERMINATION   WILDCARD
hello-openshift   hello_openshift-&lt;my_project&gt;.test.example.com
hello-openshift   8080-tcp                 None</pre>

									</p></div></li></ol></div></li></ol></div></section><section class="section" id="nw-ingress-converting-http-header-case_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.17. Converting HTTP header case</h3></div></div></div><p>
					HAProxy 2.2 lowercases HTTP header names by default, for example, changing <code class="literal">Host: xyz.com</code> to <code class="literal">host: xyz.com</code>. If legacy applications are sensitive to the capitalization of HTTP header names, use the Ingress Controller <code class="literal">spec.httpHeaders.headerNameCaseAdjustments</code> API field for a solution to accommodate legacy applications until they can be fixed.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Because OpenShift Container Platform includes HAProxy 2.2, make sure to add the necessary configuration by using <code class="literal">spec.httpHeaders.headerNameCaseAdjustments</code> before upgrading.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						As a cluster administrator, you can convert the HTTP header case by entering the <code class="literal">oc patch</code> command or by setting the <code class="literal">HeaderNameCaseAdjustments</code> field in the Ingress Controller YAML file.
					</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Specify an HTTP header to be capitalized by entering the <code class="literal">oc patch</code> command.
						</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
									Enter the <code class="literal">oc patch</code> command to change the HTTP <code class="literal">host</code> header to <code class="literal">Host</code>:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress-operator patch ingresscontrollers/default --type=merge --patch='{"spec":{"httpHeaders":{"headerNameCaseAdjustments":["Host"]}}}'</pre></li><li class="listitem"><p class="simpara">
									Annotate the route of the application:
								</p><pre class="programlisting language-terminal">$ oc annotate routes/my-application haproxy.router.openshift.io/h1-adjust-case=true</pre><p class="simpara">
									The Ingress Controller then adjusts the <code class="literal">host</code> request header as specified.
								</p></li></ol></div></li></ul></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Specify adjustments using the <code class="literal">HeaderNameCaseAdjustments</code> field by configuring the Ingress Controller YAML file.
						</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
									The following example Ingress Controller YAML adjusts the <code class="literal">host</code> header to <code class="literal">Host</code> for HTTP/1 requests to appropriately annotated routes:
								</p><div class="formalpara"><p class="title"><strong>Example Ingress Controller YAML</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  httpHeaders:
    headerNameCaseAdjustments:
    - Host</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									The following example route enables HTTP response header name case adjustments using the <code class="literal">haproxy.router.openshift.io/h1-adjust-case</code> annotation:
								</p><div class="formalpara"><p class="title"><strong>Example route YAML</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    haproxy.router.openshift.io/h1-adjust-case: true <span id="CO18-1"><!--Empty--></span><span class="callout">1</span>
  name: my-application
  namespace: my-application
spec:
  to:
    kind: Service
    name: my-application</pre>

									</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO18-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Set <code class="literal">haproxy.router.openshift.io/h1-adjust-case</code> to true.
										</div></dd></dl></div></li></ol></div></li></ul></div></section><section class="section" id="nw-configuring-router-compression_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.18. Using router compression</h3></div></div></div><p>
					You configure the HAProxy Ingress Controller to specify router compression globally for specific MIME types. You can use the <code class="literal">mimeTypes</code> variable to define the formats of MIME types to which compression is applied. The types are: application, image, message, multipart, text, video, or a custom type prefaced by "X-". To see the full notation for MIME types and subtypes, see <a class="link" href="https://datatracker.ietf.org/doc/html/rfc1341#page-7">RFC1341</a>.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Memory allocated for compression can affect the max connections. Additionally, compression of large buffers can cause latency, like heavy regex or long lists of regex.
					</p><p>
						Not all MIME types benefit from compression, but HAProxy still uses resources to try to compress if instructed to. Generally, text formats, such as html, css, and js, formats benefit from compression, but formats that are already compressed, such as image, audio, and video, benefit little in exchange for the time and resources spent on compression.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Configure the <code class="literal">httpCompression</code> field for the Ingress Controller.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Use the following command to edit the <code class="literal">IngressController</code> resource:
								</p><pre class="programlisting language-terminal">$ oc edit -n openshift-ingress-operator ingresscontrollers/default</pre></li><li class="listitem"><p class="simpara">
									Under <code class="literal">spec</code>, set the <code class="literal">httpCompression</code> policy field to <code class="literal">mimeTypes</code> and specify a list of MIME types that should have compression applied:
								</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  httpCompression:
    mimeTypes:
    - "text/html"
    - "text/css; charset=utf-8"
    - "application/json"
   ...</pre></li></ol></div></li></ol></div></section><section class="section" id="nw-exposing-router-metrics_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.19. Exposing router metrics</h3></div></div></div><p>
					You can expose the HAProxy router metrics by default in Prometheus format on the default stats port, 1936. The external metrics collection and aggregation systems such as Prometheus can access the HAProxy router metrics. You can view the HAProxy router metrics in a browser in the HTML and comma separated values (CSV) format.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You configured your firewall to access the default stats port, 1936.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Get the router pod name by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-ingress</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                              READY   STATUS    RESTARTS   AGE
router-default-76bfffb66c-46qwp   1/1     Running   0          11h</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Get the router’s username and password, which the router pod stores in the <code class="literal">/var/lib/haproxy/conf/metrics-auth/statsUsername</code> and <code class="literal">/var/lib/haproxy/conf/metrics-auth/statsPassword</code> files:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Get the username by running the following command:
								</p><pre class="programlisting language-terminal">$ oc rsh &lt;router_pod_name&gt; cat metrics-auth/statsUsername</pre></li><li class="listitem"><p class="simpara">
									Get the password by running the following command:
								</p><pre class="programlisting language-terminal">$ oc rsh &lt;router_pod_name&gt; cat metrics-auth/statsPassword</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Get the router IP and metrics certificates by running the following command:
						</p><pre class="programlisting language-terminal">$ oc describe pod &lt;router_pod&gt;</pre></li><li class="listitem"><p class="simpara">
							Get the raw statistics in Prometheus format by running the following command:
						</p><pre class="programlisting language-terminal">$ curl -u &lt;user&gt;:&lt;password&gt; http://&lt;router_IP&gt;:&lt;stats_port&gt;/metrics</pre></li><li class="listitem"><p class="simpara">
							Access the metrics securely by running the following command:
						</p><pre class="programlisting language-terminal">$ curl -u user:password https://&lt;router_IP&gt;:&lt;stats_port&gt;/metrics -k</pre></li><li class="listitem"><p class="simpara">
							Access the default stats port, 1936, by running the following command:
						</p><pre class="programlisting language-terminal">$ curl -u &lt;user&gt;:&lt;password&gt; http://&lt;router_IP&gt;:&lt;stats_port&gt;/metrics</pre><div class="example" id="idm140587172738480"><p class="title"><strong>Example 7.1. Example output</strong></p><div class="example-contents"><pre class="programlisting language-terminal">...
# HELP haproxy_backend_connections_total Total number of connections.
# TYPE haproxy_backend_connections_total gauge
haproxy_backend_connections_total{backend="http",namespace="default",route="hello-route"} 0
haproxy_backend_connections_total{backend="http",namespace="default",route="hello-route-alt"} 0
haproxy_backend_connections_total{backend="http",namespace="default",route="hello-route01"} 0
...
# HELP haproxy_exporter_server_threshold Number of servers tracked and the current threshold value.
# TYPE haproxy_exporter_server_threshold gauge
haproxy_exporter_server_threshold{type="current"} 11
haproxy_exporter_server_threshold{type="limit"} 500
...
# HELP haproxy_frontend_bytes_in_total Current total of incoming bytes.
# TYPE haproxy_frontend_bytes_in_total gauge
haproxy_frontend_bytes_in_total{frontend="fe_no_sni"} 0
haproxy_frontend_bytes_in_total{frontend="fe_sni"} 0
haproxy_frontend_bytes_in_total{frontend="public"} 119070
...
# HELP haproxy_server_bytes_in_total Current total of incoming bytes.
# TYPE haproxy_server_bytes_in_total gauge
haproxy_server_bytes_in_total{namespace="",pod="",route="",server="fe_no_sni",service=""} 0
haproxy_server_bytes_in_total{namespace="",pod="",route="",server="fe_sni",service=""} 0
haproxy_server_bytes_in_total{namespace="default",pod="docker-registry-5-nk5fz",route="docker-registry",server="10.130.0.89:5000",service="docker-registry"} 0
haproxy_server_bytes_in_total{namespace="default",pod="hello-rc-vkjqx",route="hello-route",server="10.130.0.90:8080",service="hello-svc-1"} 0
...</pre></div></div></li><li class="listitem"><p class="simpara">
							Launch the stats window by entering the following URL in a browser:
						</p><pre class="programlisting language-terminal">http://&lt;user&gt;:&lt;password&gt;@&lt;router_IP&gt;:&lt;stats_port&gt;</pre></li><li class="listitem"><p class="simpara">
							Optional: Get the stats in CSV format by entering the following URL in a browser:
						</p><pre class="programlisting language-terminal">http://&lt;user&gt;:&lt;password&gt;@&lt;router_ip&gt;:1936/metrics;csv</pre></li></ol></div></section><section class="section" id="nw-customize-ingress-error-pages_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.20. Customizing HAProxy error code response pages</h3></div></div></div><p>
					As a cluster administrator, you can specify a custom error code response page for either 503, 404, or both error pages. The HAProxy router serves a 503 error page when the application pod is not running or a 404 error page when the requested URL does not exist. For example, if you customize the 503 error code response page, then the page is served when the application pod is not running, and the default 404 error code HTTP response page is served by the HAProxy router for an incorrect route or a non-existing route.
				</p><p>
					Custom error code response pages are specified in a config map then patched to the Ingress Controller. The config map keys have two available file names as follows: <code class="literal">error-page-503.http</code> and <code class="literal">error-page-404.http</code>.
				</p><p>
					Custom HTTP error code response pages must follow the <a class="link" href="https://www.haproxy.com/documentation/hapee/latest/configuration/config-sections/http-errors/">HAProxy HTTP error page configuration guidelines</a>. Here is an example of the default OpenShift Container Platform HAProxy router <a class="link" href="https://raw.githubusercontent.com/openshift/router/master/images/router/haproxy/conf/error-page-503.http">http 503 error code response page</a>. You can use the default content as a template for creating your own custom page.
				</p><p>
					By default, the HAProxy router serves only a 503 error page when the application is not running or when the route is incorrect or non-existent. This default behavior is the same as the behavior on OpenShift Container Platform 4.8 and earlier. If a config map for the customization of an HTTP error code response is not provided, and you are using a custom HTTP error code response page, the router serves a default 404 or 503 error code response page.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you use the OpenShift Container Platform default 503 error code page as a template for your customizations, the headers in the file require an editor that can use CRLF line endings.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a config map named <code class="literal">my-custom-error-code-pages</code> in the <code class="literal">openshift-config</code> namespace:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-config create configmap my-custom-error-code-pages \
--from-file=error-page-503.http \
--from-file=error-page-404.http</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								If you do not specify the correct format for the custom error code response page, a router pod outage occurs. To resolve this outage, you must delete or correct the config map and delete the affected router pods so they can be recreated with the correct information.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Patch the Ingress Controller to reference the <code class="literal">my-custom-error-code-pages</code> config map by name:
						</p><pre class="programlisting language-terminal">$ oc patch -n openshift-ingress-operator ingresscontroller/default --patch '{"spec":{"httpErrorCodePages":{"name":"my-custom-error-code-pages"}}}' --type=merge</pre><p class="simpara">
							The Ingress Operator copies the <code class="literal">my-custom-error-code-pages</code> config map from the <code class="literal">openshift-config</code> namespace to the <code class="literal">openshift-ingress</code> namespace. The Operator names the config map according to the pattern, <code class="literal">&lt;your_ingresscontroller_name&gt;-errorpages</code>, in the <code class="literal">openshift-ingress</code> namespace.
						</p></li><li class="listitem"><p class="simpara">
							Display the copy:
						</p><pre class="programlisting language-terminal">$ oc get cm default-errorpages -n openshift-ingress</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">NAME                       DATA   AGE
default-errorpages         2      25s  <span id="CO19-1"><!--Empty--></span><span class="callout">1</span></pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO19-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The example config map name is <code class="literal">default-errorpages</code> because the <code class="literal">default</code> Ingress Controller custom resource (CR) was patched.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Confirm that the config map containing the custom error response page mounts on the router volume where the config map key is the filename that has the custom HTTP error code response:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									For 503 custom HTTP custom error code response:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress rsh &lt;router_pod&gt; cat /var/lib/haproxy/conf/error_code_pages/error-page-503.http</pre></li><li class="listitem"><p class="simpara">
									For 404 custom HTTP custom error code response:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress rsh &lt;router_pod&gt; cat /var/lib/haproxy/conf/error_code_pages/error-page-404.http</pre></li></ul></div></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						Verify your custom error code HTTP response:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a test project and application:
						</p><pre class="programlisting language-terminal"> $ oc new-project test-ingress</pre><pre class="programlisting language-terminal">$ oc new-app django-psql-example</pre></li><li class="listitem"><p class="simpara">
							For 503 custom http error code response:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Stop all the pods for the application.
								</li><li class="listitem"><p class="simpara">
									Run the following curl command or visit the route hostname in the browser:
								</p><pre class="programlisting language-terminal">$ curl -vk &lt;route_hostname&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							For 404 custom http error code response:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Visit a non-existent route or an incorrect route.
								</li><li class="listitem"><p class="simpara">
									Run the following curl command or visit the route hostname in the browser:
								</p><pre class="programlisting language-terminal">$ curl -vk &lt;route_hostname&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Check if the <code class="literal">errorfile</code> attribute is properly in the <code class="literal">haproxy.config</code> file:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress rsh &lt;router&gt; cat /var/lib/haproxy/conf/haproxy.config | grep errorfile</pre></li></ol></div></section><section class="section" id="nw-ingress-setting-max-connections_configuring-ingress"><div class="titlepage"><div><div><h3 class="title">7.8.21. Setting the Ingress Controller maximum connections</h3></div></div></div><p>
					A cluster administrator can set the maximum number of simultaneous connections for OpenShift router deployments. You can patch an existing Ingress Controller to increase the maximum number of connections.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The following assumes that you already created an Ingress Controller
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Update the Ingress Controller to change the maximum number of connections for HAProxy:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress-operator patch ingresscontroller/default --type=merge -p '{"spec":{"tuningOptions": {"maxConnections": 7500}}}'</pre><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
								If you set the <code class="literal">spec.tuningOptions.maxConnections</code> value greater than the current operating system limit, the HAProxy process will not start. See the table in the "Ingress Controller configuration parameters" section for more information about this parameter.
							</p></div></div></li></ul></div></section></section><section class="section _additional-resources" id="additional-resources-1"><div class="titlepage"><div><div><h2 class="title">7.9. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-a-custom-pki">Configuring a custom PKI</a>
					</li></ul></div></section></section><section class="chapter" id="ingress-sharding"><div class="titlepage"><div><div><h1 class="title">Chapter 8. Ingress sharding in OpenShift Container Platform</h1></div></div></div><p>
			In OpenShift Container Platform, an Ingress Controller can serve all routes, or it can serve a subset of routes. By default, the Ingress Controller serves any route created in any namespace in the cluster. You can add additional Ingress Controllers to your cluster to optimize routing by creating <span class="emphasis"><em>shards</em></span>, which are subsets of routes based on selected characteristics. To mark a route as a member of a shard, use labels in the route or namespace <code class="literal">metadata</code> field. The Ingress Controller uses <span class="emphasis"><em>selectors</em></span>, also known as a <span class="emphasis"><em>selection expression</em></span>, to select a subset of routes from the entire pool of routes to serve.
		</p><p>
			Ingress sharding is useful in cases where you want to load balance incoming traffic across multiple Ingress Controllers, when you want to isolate traffic to be routed to a specific Ingress Controller, or for a variety of other reasons described in the next section.
		</p><p>
			By default, each route uses the default domain of the cluster. However, routes can be configured to use the domain of the router instead. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-ingress-sharding-route-configuration_ingress-sharding">Creating a route for Ingress Controller Sharding</a>.
		</p><section class="section" id="nw-ingress-sharding_ingress-sharding"><div class="titlepage"><div><div><h2 class="title">8.1. Ingress Controller sharding</h2></div></div></div><p>
				You can use Ingress sharding, also known as router sharding, to distribute a set of routes across multiple routers by adding labels to routes, namespaces, or both. The Ingress Controller uses a corresponding set of selectors to admit only the routes that have a specified label. Each Ingress shard comprises the routes that are filtered using a given selection expression.
			</p><p>
				As the primary mechanism for traffic to enter the cluster, the demands on the Ingress Controller can be significant. As a cluster administrator, you can shard the routes to:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Balance Ingress Controllers, or routers, with several routes to speed up responses to changes.
					</li><li class="listitem">
						Allocate certain routes to have different reliability guarantees than other routes.
					</li><li class="listitem">
						Allow certain Ingress Controllers to have different policies defined.
					</li><li class="listitem">
						Allow only specific routes to use additional features.
					</li><li class="listitem">
						Expose different routes on different addresses so that internal and external users can see different routes, for example.
					</li><li class="listitem">
						Transfer traffic from one version of an application to another during a blue green deployment.
					</li></ul></div><p>
				When Ingress Controllers are sharded, a given route is admitted to zero or more Ingress Controllers in the group. A route’s status describes whether an Ingress Controller has admitted it or not. An Ingress Controller will only admit a route if it is unique to its shard.
			</p><p>
				An Ingress Controller can use three sharding methods:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Adding only a namespace selector to the Ingress Controller, so that all routes in a namespace with labels that match the namespace selector are in the Ingress shard.
					</li><li class="listitem">
						Adding only a route selector to the Ingress Controller, so that all routes with labels that match the route selector are in the Ingress shard.
					</li><li class="listitem">
						Adding both a namespace selector and route selector to the Ingress Controller, so that routes with labels that match the route selector in a namespace with labels that match the namespace selector are in the Ingress shard.
					</li></ul></div><p>
				With sharding, you can distribute subsets of routes over multiple Ingress Controllers. These subsets can be non-overlapping, also called <span class="emphasis"><em>traditional</em></span> sharding, or overlapping, otherwise known as <span class="emphasis"><em>overlapped</em></span> sharding.
			</p><section class="section" id="traditional-sharding-example"><div class="titlepage"><div><div><h3 class="title">8.1.1. Traditional sharding example</h3></div></div></div><p>
					An Ingress Controller <code class="literal">finops-router</code> is configured with the label selector <code class="literal">spec.namespaceSelector.matchLabels.name</code> set to <code class="literal">finance</code> and <code class="literal">ops</code>:
				</p><div class="formalpara"><p class="title"><strong>Example YAML definition for <code class="literal">finops-router</code></strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
items:
- apiVersion: operator.openshift.io/v1
  kind: IngressController
  metadata:
    name: finops-router
    namespace: openshift-ingress-operator
  spec:
    namespaceSelector:
      matchLabels:
        name:
          - finance
          - ops</pre>

					</p></div><p>
					A second Ingress Controller <code class="literal">dev-router</code> is configured with the label selector <code class="literal">spec.namespaceSelector.matchLabels.name</code> set to <code class="literal">dev</code>:
				</p><div class="formalpara"><p class="title"><strong>Example YAML definition for <code class="literal">dev-router</code></strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
items:
- apiVersion: operator.openshift.io/v1
  kind: IngressController
  metadata:
    name: dev-router
    namespace: openshift-ingress-operator
  spec:
    namespaceSelector:
      matchLabels:
        name: dev</pre>

					</p></div><p>
					If all application routes are in separate namespaces, each labeled with <code class="literal">name:finance</code>, <code class="literal">name:ops</code>, and <code class="literal">name:dev</code> respectively, this configuration effectively distributes your routes between the two Ingress Controllers. OpenShift Container Platform routes for console, authentication, and other purposes should not be handled.
				</p><p>
					In the above scenario, sharding becomes a special case of partitioning, with no overlapping subsets. Routes are divided between router shards.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						The <code class="literal">default</code> Ingress Controller continues to serve all routes unless the <code class="literal">namespaceSelector</code> or <code class="literal">routeSelector</code> fields contain routes that are meant for exclusion. See this <a class="link" href="https://access.redhat.com/solutions/5097511">Red Hat Knowledgebase solution</a> and the section "Sharding the default Ingress Controller" for more information on how to exclude routes from the default Ingress Controller.
					</p></div></div></section><section class="section" id="overlapped-sharding-example"><div class="titlepage"><div><div><h3 class="title">8.1.2. Overlapped sharding example</h3></div></div></div><p>
					In addition to <code class="literal">finops-router</code> and <code class="literal">dev-router</code> in the example above, you also have <code class="literal">devops-router</code>, which is configured with the label selector <code class="literal">spec.namespaceSelector.matchLabels.name</code> set to <code class="literal">dev</code> and <code class="literal">ops</code>:
				</p><div class="formalpara"><p class="title"><strong>Example YAML definition for <code class="literal">devops-router</code></strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
items:
- apiVersion: operator.openshift.io/v1
  kind: IngressController
  metadata:
    name: devops-router
    namespace: openshift-ingress-operator
  spec:
    namespaceSelector:
      matchLabels:
        name:
          - dev
          - ops</pre>

					</p></div><p>
					The routes in the namespaces labeled <code class="literal">name:dev</code> and <code class="literal">name:ops</code> are now serviced by two different Ingress Controllers. With this configuration, you have overlapping subsets of routes.
				</p><p>
					With overlapping subsets of routes you can create more complex routing rules. For example, you can divert higher priority traffic to the dedicated <code class="literal">finops-router</code> while sending lower priority traffic to <code class="literal">devops-router</code>.
				</p></section><section class="section" id="nw-ingress-sharding-default_ingress-sharding"><div class="titlepage"><div><div><h3 class="title">8.1.3. Sharding the default Ingress Controller</h3></div></div></div><p>
					After creating a new Ingress shard, there might be routes that are admitted to your new Ingress shard that are also admitted by the default Ingress Controller. This is because the default Ingress Controller has no selectors and admits all routes by default.
				</p><p>
					You can restrict an Ingress Controller from servicing routes with specific labels using either namespace selectors or route selectors. The following procedure restricts the default Ingress Controller from servicing your newly sharded <code class="literal">finance</code>, <code class="literal">ops</code>, and <code class="literal">dev</code>, routes using a namespace selector. This adds further isolation to Ingress shards.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						You must keep all of OpenShift Container Platform’s administration routes on the same Ingress Controller. Therefore, avoid adding additional selectors to the default Ingress Controller that exclude these essential routes.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You are logged in as a project administrator.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Modify the default Ingress Controller by running the following command:
						</p><pre class="programlisting language-terminal">$ oc edit ingresscontroller -n openshift-ingress-operator default</pre></li><li class="listitem"><p class="simpara">
							Edit the Ingress Controller to contain a <code class="literal">namespaceSelector</code> that excludes the routes with any of the <code class="literal">finance</code>, <code class="literal">ops</code>, and <code class="literal">dev</code> labels:
						</p><pre class="programlisting language-yaml">apiVersion: v1
items:
- apiVersion: operator.openshift.io/v1
  kind: IngressController
  metadata:
    name: default
    namespace: openshift-ingress-operator
  spec:
  namespaceSelector:
    matchExpressions:
    - key: type
      operator: NotIn
      values:
      - finance
      - ops
      - dev</pre></li></ol></div><p>
					The default Ingress Controller will no longer serve the namespaces labeled <code class="literal">name:finance</code>, <code class="literal">name:ops</code>, and <code class="literal">name:dev</code>.
				</p></section><section class="section" id="nw-ingress-sharding-dns_ingress-sharding"><div class="titlepage"><div><div><h3 class="title">8.1.4. Ingress sharding and DNS</h3></div></div></div><p>
					The cluster administrator is responsible for making a separate DNS entry for each router in a project. A router will not forward unknown routes to another router.
				</p><p>
					Consider the following example:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Router A lives on host 192.168.0.5 and has routes with <code class="literal">*.foo.com</code>.
						</li><li class="listitem">
							Router B lives on host 192.168.1.9 and has routes with <code class="literal">*.example.com</code>.
						</li></ul></div><p>
					Separate DNS entries must resolve <code class="literal">*.foo.com</code> to the node hosting Router A and <code class="literal">*.example.com</code> to the node hosting Router B:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">*.foo.com A IN 192.168.0.5</code>
						</li><li class="listitem">
							<code class="literal">*.example.com A IN 192.168.1.9</code>
						</li></ul></div></section><section class="section" id="nw-ingress-sharding-route-labels_ingress-sharding"><div class="titlepage"><div><div><h3 class="title">8.1.5. Configuring Ingress Controller sharding by using route labels</h3></div></div></div><p>
					Ingress Controller sharding by using route labels means that the Ingress Controller serves any route in any namespace that is selected by the route selector.
				</p><div class="figure" id="idm140587173894240"><p class="title"><strong>Figure 8.1. Ingress sharding using route labels</strong></p><div class="figure-contents"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/88e02b6d127bf165fed27d7c18366e6a/nw-sharding-route-labels.png" alt="A diagram showing multiple Ingress Controllers with different route selectors serving any route containing a label that matches a given route selector regardless of the namespace a route belongs to"/></div></div></div><p>
					Ingress Controller sharding is useful when balancing incoming traffic load among a set of Ingress Controllers and when isolating traffic to a specific Ingress Controller. For example, company A goes to one Ingress Controller and company B to another.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">router-internal.yaml</code> file:
						</p><pre class="programlisting language-terminal"># cat router-internal.yaml
apiVersion: v1
items:
- apiVersion: operator.openshift.io/v1
  kind: IngressController
  metadata:
    name: sharded
    namespace: openshift-ingress-operator
  spec:
    domain: &lt;apps-sharded.basedomain.example.net&gt; <span id="CO20-1"><!--Empty--></span><span class="callout">1</span>
    nodePlacement:
      nodeSelector:
        matchLabels:
          node-role.kubernetes.io/worker: ""
    routeSelector:
      matchLabels:
        type: sharded
  status: {}
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO20-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify a domain to be used by the Ingress Controller. This domain must be different from the default Ingress Controller domain.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Apply the Ingress Controller <code class="literal">router-internal.yaml</code> file:
						</p><pre class="programlisting language-terminal"># oc apply -f router-internal.yaml</pre><p class="simpara">
							The Ingress Controller selects routes in any namespace that have the label <code class="literal">type: sharded</code>.
						</p></li><li class="listitem"><p class="simpara">
							Create a new route using the domain configured in the <code class="literal">router-internal.yaml</code>:
						</p><pre class="programlisting language-terminal">$ oc expose svc &lt;service-name&gt; --hostname &lt;route-name&gt;.apps-sharded.basedomain.example.net</pre></li></ol></div></section><section class="section" id="nw-ingress-sharding-namespace-labels_ingress-sharding"><div class="titlepage"><div><div><h3 class="title">8.1.6. Configuring Ingress Controller sharding by using namespace labels</h3></div></div></div><p>
					Ingress Controller sharding by using namespace labels means that the Ingress Controller serves any route in any namespace that is selected by the namespace selector.
				</p><div class="figure" id="idm140587132333920"><p class="title"><strong>Figure 8.2. Ingress sharding using namespace labels</strong></p><div class="figure-contents"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/afe594e8499d327fa587b8ca4f61a2cd/nw-sharding-namespace-labels.png" alt="A diagram showing multiple Ingress Controllers with different namespace selectors serving routes that belong to the namespace containing a label that matches a given namespace selector"/></div></div></div><p>
					Ingress Controller sharding is useful when balancing incoming traffic load among a set of Ingress Controllers and when isolating traffic to a specific Ingress Controller. For example, company A goes to one Ingress Controller and company B to another.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">router-internal.yaml</code> file:
						</p><pre class="programlisting language-terminal"># cat router-internal.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
items:
- apiVersion: operator.openshift.io/v1
  kind: IngressController
  metadata:
    name: sharded
    namespace: openshift-ingress-operator
  spec:
    domain: &lt;apps-sharded.basedomain.example.net&gt; <span id="CO21-1"><!--Empty--></span><span class="callout">1</span>
    nodePlacement:
      nodeSelector:
        matchLabels:
          node-role.kubernetes.io/worker: ""
    namespaceSelector:
      matchLabels:
        type: sharded
  status: {}
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO21-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify a domain to be used by the Ingress Controller. This domain must be different from the default Ingress Controller domain.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Apply the Ingress Controller <code class="literal">router-internal.yaml</code> file:
						</p><pre class="programlisting language-terminal"># oc apply -f router-internal.yaml</pre><p class="simpara">
							The Ingress Controller selects routes in any namespace that is selected by the namespace selector that have the label <code class="literal">type: sharded</code>.
						</p></li><li class="listitem"><p class="simpara">
							Create a new route using the domain configured in the <code class="literal">router-internal.yaml</code>:
						</p><pre class="programlisting language-terminal">$ oc expose svc &lt;service-name&gt; --hostname &lt;route-name&gt;.apps-sharded.basedomain.example.net</pre></li></ol></div></section></section><section class="section" id="nw-ingress-sharding-route-configuration_ingress-sharding"><div class="titlepage"><div><div><h2 class="title">8.2. Creating a route for Ingress Controller sharding</h2></div></div></div><p>
				A route allows you to host your application at a URL. In this case, the hostname is not set and the route uses a subdomain instead. When you specify a subdomain, you automatically use the domain of the Ingress Controller that exposes the route. For situations where a route is exposed by multiple Ingress Controllers, the route is hosted at multiple URLs.
			</p><p>
				The following procedure describes how to create a route for Ingress Controller sharding, using the <code class="literal">hello-openshift</code> application as an example.
			</p><p>
				Ingress Controller sharding is useful when balancing incoming traffic load among a set of Ingress Controllers and when isolating traffic to a specific Ingress Controller. For example, company A goes to one Ingress Controller and company B to another.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You are logged in as a project administrator.
					</li><li class="listitem">
						You have a web application that exposes a port and an HTTP or TLS endpoint listening for traffic on the port.
					</li><li class="listitem">
						You have configured the Ingress Controller for sharding.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a project called <code class="literal">hello-openshift</code> by running the following command:
					</p><pre class="programlisting language-terminal">$ oc new-project hello-openshift</pre></li><li class="listitem"><p class="simpara">
						Create a pod in the project by running the following command:
					</p><pre class="programlisting language-terminal">$ oc create -f https://raw.githubusercontent.com/openshift/origin/master/examples/hello-openshift/hello-pod.json</pre></li><li class="listitem"><p class="simpara">
						Create a service called <code class="literal">hello-openshift</code> by running the following command:
					</p><pre class="programlisting language-terminal">$ oc expose pod/hello-openshift</pre></li><li class="listitem"><p class="simpara">
						Create a route definition called <code class="literal">hello-openshift-route.yaml</code>:
					</p><div class="formalpara"><p class="title"><strong>YAML definition of the created route for sharding:</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    type: sharded <span id="CO22-1"><!--Empty--></span><span class="callout">1</span>
  name: hello-openshift-edge
  namespace: hello-openshift
spec:
  subdomain: hello-openshift <span id="CO22-2"><!--Empty--></span><span class="callout">2</span>
  tls:
    termination: edge
  to:
    kind: Service
    name: hello-openshift</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO22-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Both the label key and its corresponding label value must match the ones specified in the Ingress Controller. In this example, the Ingress Controller has the label key and value <code class="literal">type: sharded</code>.
							</div></dd><dt><a href="#CO22-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The route will be exposed using the value of the <code class="literal">subdomain</code> field. When you specify the <code class="literal">subdomain</code> field, you must leave the hostname unset. If you specify both the <code class="literal">host</code> and <code class="literal">subdomain</code> fields, then the route will use the value of the <code class="literal">host</code> field, and ignore the <code class="literal">subdomain</code> field.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Use <code class="literal">hello-openshift-route.yaml</code> to create a route to the <code class="literal">hello-openshift</code> application by running the following command:
					</p><pre class="programlisting language-terminal">$ oc -n hello-openshift create -f hello-openshift-route.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Get the status of the route with the following command:
					</p><pre class="programlisting language-terminal">$ oc -n hello-openshift get routes/hello-openshift-edge -o yaml</pre><p class="simpara">
						The resulting <code class="literal">Route</code> resource should look similar to the following:
					</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    type: sharded
  name: hello-openshift-edge
  namespace: hello-openshift
spec:
  subdomain: hello-openshift
  tls:
    termination: edge
  to:
    kind: Service
    name: hello-openshift
status:
  ingress:
  - host: hello-openshift.&lt;apps-sharded.basedomain.example.net&gt; <span id="CO23-1"><!--Empty--></span><span class="callout">1</span>
    routerCanonicalHostname: router-sharded.&lt;apps-sharded.basedomain.example.net&gt; <span id="CO23-2"><!--Empty--></span><span class="callout">2</span>
    routerName: sharded <span id="CO23-3"><!--Empty--></span><span class="callout">3</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO23-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The hostname the Ingress Controller, or router, uses to expose the route. The value of the <code class="literal">host</code> field is automatically determined by the Ingress Controller, and uses its domain. In this example, the domain of the Ingress Controller is <code class="literal">&lt;apps-sharded.basedomain.example.net&gt;</code>.
							</div></dd><dt><a href="#CO23-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The hostname of the Ingress Controller.
							</div></dd><dt><a href="#CO23-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								The name of the Ingress Controller. In this example, the Ingress Controller has the name <code class="literal">sharded</code>.
							</div></dd></dl></div></li></ul></div><h3 id="additional-resources_ingress-sharding">Additional Resources</h3><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#baseline-router-performance_routing-optimization">Baseline Ingress Controller (router) performance</a>
					</li></ul></div></section></section><section class="chapter" id="ingress-node-firewall-operator"><div class="titlepage"><div><div><h1 class="title">Chapter 9. Ingress Node Firewall Operator in OpenShift Container Platform</h1></div></div></div><p>
			The Ingress Node Firewall Operator allows administrators to manage firewall configurations at the node level.
		</p><section class="section" id="installing-infw-operator_ingress-node-firewall-operator"><div class="titlepage"><div><div><h2 class="title">9.1. Installing the Ingress Node Firewall Operator</h2></div></div></div><p>
				As a cluster administrator, you can install the Ingress Node Firewall Operator by using the OpenShift Container Platform CLI or the web console.
			</p><section class="section" id="install-operator-cli_ingress-node-firewall-operator"><div class="titlepage"><div><div><h3 class="title">9.1.1. Installing the Ingress Node Firewall Operator using the CLI</h3></div></div></div><p>
					As a cluster administrator, you can install the Operator using the CLI.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have an account with administrator privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To create the <code class="literal">openshift-ingress-node-firewall</code> namespace, enter the following command:
						</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/enforce-version: v1.24
  name: openshift-ingress-node-firewall
EOF</pre></li><li class="listitem"><p class="simpara">
							To create an <code class="literal">OperatorGroup</code> CR, enter the following command:
						</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: ingress-node-firewall-operators
  namespace: openshift-ingress-node-firewall
EOF</pre></li><li class="listitem"><p class="simpara">
							Subscribe to the Ingress Node Firewall Operator.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									To create a <code class="literal">Subscription</code> CR for the Ingress Node Firewall Operator, enter the following command:
								</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: ingress-node-firewall-sub
  namespace: openshift-ingress-node-firewall
spec:
  name: ingress-node-firewall
  channel: stable
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							To verify that the Operator is installed, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc get ip -n openshift-ingress-node-firewall</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME            CSV                                         APPROVAL    APPROVED
install-5cvnz   ingress-node-firewall.4.13.0-202211122336   Automatic   true</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To verify the version of the Operator, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc get csv -n openshift-ingress-node-firewall</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                        DISPLAY                          VERSION               REPLACES                                    PHASE
ingress-node-firewall.4.13.0-202211122336   Ingress Node Firewall Operator   4.13.0-202211122336   ingress-node-firewall.4.13.0-202211102047   Succeeded</pre>

							</p></div></li></ol></div></section><section class="section" id="install-operator-web-console_ingress-node-firewall-operator"><div class="titlepage"><div><div><h3 class="title">9.1.2. Installing the Ingress Node Firewall Operator using the web console</h3></div></div></div><p>
					As a cluster administrator, you can install the Operator using the web console.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have an account with administrator privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Install the Ingress Node Firewall Operator:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									In the OpenShift Container Platform web console, click <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span>.
								</li><li class="listitem">
									Select <span class="strong strong"><strong>Ingress Node Firewall Operator</strong></span> from the list of available Operators, and then click <span class="strong strong"><strong>Install</strong></span>.
								</li><li class="listitem">
									On the <span class="strong strong"><strong>Install Operator</strong></span> page, under <span class="strong strong"><strong>Installed Namespace</strong></span>, select <span class="strong strong"><strong>Operator recommended Namespace</strong></span>.
								</li><li class="listitem">
									Click <span class="strong strong"><strong>Install</strong></span>.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Verify that the Ingress Node Firewall Operator is installed successfully:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Navigate to the <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span> page.
								</li><li class="listitem"><p class="simpara">
									Ensure that <span class="strong strong"><strong>Ingress Node Firewall Operator</strong></span> is listed in the <span class="strong strong"><strong>openshift-ingress-node-firewall</strong></span> project with a <span class="strong strong"><strong>Status</strong></span> of <span class="strong strong"><strong>InstallSucceeded</strong></span>.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										During installation an Operator might display a <span class="strong strong"><strong>Failed</strong></span> status. If the installation later succeeds with an <span class="strong strong"><strong>InstallSucceeded</strong></span> message, you can ignore the <span class="strong strong"><strong>Failed</strong></span> message.
									</p></div></div><p class="simpara">
									If the Operator does not have a <span class="strong strong"><strong>Status</strong></span> of <span class="strong strong"><strong>InstallSucceeded</strong></span>, troubleshoot using the following steps:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Inspect the <span class="strong strong"><strong>Operator Subscriptions</strong></span> and <span class="strong strong"><strong>Install Plans</strong></span> tabs for any failures or errors under <span class="strong strong"><strong>Status</strong></span>.
										</li><li class="listitem">
											Navigate to the <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Pods</strong></span> page and check the logs for pods in the <code class="literal">openshift-ingress-node-firewall</code> project.
										</li><li class="listitem"><p class="simpara">
											Check the namespace of the YAML file. If the annotation is missing, you can add the annotation <code class="literal">workload.openshift.io/allowed=management</code> to the Operator namespace with the following command:
										</p><pre class="programlisting language-terminal">$ oc annotate ns/openshift-ingress-node-firewall workload.openshift.io/allowed=management</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												For single-node OpenShift clusters, the <code class="literal">openshift-ingress-node-firewall</code> namespace requires the <code class="literal">workload.openshift.io/allowed=management</code> annotation.
											</p></div></div></li></ul></div></li></ol></div></li></ol></div></section></section><section class="section" id="nw-infw-operator-cr_ingress-node-firewall-operator"><div class="titlepage"><div><div><h2 class="title">9.2. Ingress Node Firewall Operator</h2></div></div></div><p>
				The Ingress Node Firewall Operator provides ingress firewall rules at a node level by deploying the daemon set to nodes you specify and manage in the firewall configurations. To deploy the daemon set, you create an <code class="literal">IngressNodeFirewallConfig</code> custom resource (CR). The Operator applies the <code class="literal">IngressNodeFirewallConfig</code> CR to create ingress node firewall daemon set <code class="literal">daemon</code>, which run on all nodes that match the <code class="literal">nodeSelector</code>.
			</p><p>
				You configure <code class="literal">rules</code> of the <code class="literal">IngressNodeFirewall</code> CR and apply them to clusters using the <code class="literal">nodeSelector</code> and setting values to "true".
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					The Ingress Node Firewall Operator supports only stateless firewall rules.
				</p><p>
					The maximum transmission units (MTU) parameter is 4Kb (kilobytes) in OpenShift Container Platform 4.13.
				</p><p>
					Network interface controllers (NICs) that do not support native XDP drivers will run at a lower performance.
				</p></div></div></section><section class="section" id="nw-infw-operator-deploying_ingress-node-firewall-operator"><div class="titlepage"><div><div><h2 class="title">9.3. Deploying Ingress Node Firewall Operator</h2></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisite</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						The Ingress Node Firewall Operator is installed.
					</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					To deploy the Ingress Node Firewall Operator, create a <code class="literal">IngressNodeFirewallConfig</code> custom resource that will deploy the Operator’s daemon set. You can deploy one or multiple <code class="literal">IngressNodeFirewall</code> CRDs to nodes by applying firewall rules.
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Create the <code class="literal">IngressNodeFirewallConfig</code> inside the <code class="literal">openshift-ingress-node-firewall</code> namespace named <code class="literal">ingressnodefirewallconfig</code>.
					</li><li class="listitem"><p class="simpara">
						Run the following command to deploy Ingress Node Firewall Operator rules:
					</p><pre class="programlisting language-terminal">$ oc apply -f rule.yaml</pre></li></ol></div><section class="section" id="nw-infw-operator-config-object_ingress-node-firewall-operator"><div class="titlepage"><div><div><h3 class="title">9.3.1. Ingress Node Firewall configuration object</h3></div></div></div><p>
					The fields for the Ingress Node Firewall configuration object are described in the following table:
				</p><div class="table" id="idm140587141421632"><p class="title"><strong>Table 9.1. Ingress Node Firewall Configuration object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587160789728" scope="col">Field</th><th align="left" valign="middle" id="idm140587160788640" scope="col">Type</th><th align="left" valign="middle" id="idm140587147132880" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587160789728"> <p>
									<code class="literal">metadata.name</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587160788640"> <p>
									<code class="literal">string</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587147132880"> <p>
									The name of the CR object. The name of the firewall rules object must be <code class="literal">ingressnodefirewallconfig</code>.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587160789728"> <p>
									<code class="literal">metadata.namespace</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587160788640"> <p>
									<code class="literal">string</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587147132880"> <p>
									Namespace for the Ingress Firewall Operator CR object. The <code class="literal">IngressNodeFirewallConfig</code> CR must be created inside the <code class="literal">openshift-ingress-node-firewall</code> namespace.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587160789728"> <p>
									<code class="literal">spec.nodeSelector</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587160788640"> <p>
									<code class="literal">string</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587147132880"> <p>
									A node selection constraint used to target nodes through specified node labels. For example:
								</p>
								 
<pre class="programlisting language-yaml">spec:
  nodeSelector:
    node-role.kubernetes.io/worker: ""</pre>
								 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
										One label used in <code class="literal">nodeSelector</code> must match a label on the nodes in order for the daemon set to start. For example, if the node labels <code class="literal">node-role.kubernetes.io/worker</code> and <code class="literal">node-type.kubernetes.io/vm</code> are applied to a node, then at least one label must be set using <code class="literal">nodeSelector</code> for the daemon set to start.
									</p></div></div>
								 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The Operator consumes the CR and creates an ingress node firewall daemon set on all the nodes that match the <code class="literal">nodeSelector</code>.
					</p></div></div><h4 id="nw-ingress-node-firewall-example-cr-2_ingress-node-firewall-operator">Ingress Node Firewall Operator example configuration</h4><p>
					A complete Ingress Node Firewall Configuration is specified in the following example:
				</p><div class="formalpara"><p class="title"><strong>Example Ingress Node Firewall Configuration object</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: ingressnodefirewall.openshift.io/v1alpha1
kind: IngressNodeFirewallConfig
metadata:
  name: ingressnodefirewallconfig
  namespace: openshift-ingress-node-firewall
spec:
  nodeSelector:
    node-role.kubernetes.io/worker: ""</pre>

					</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The Operator consumes the CR and creates an ingress node firewall daemon set on all the nodes that match the <code class="literal">nodeSelector</code>.
					</p></div></div></section><section class="section" id="nw-ingress-node-firewall-operator-rules-object_ingress-node-firewall-operator"><div class="titlepage"><div><div><h3 class="title">9.3.2. Ingress Node Firewall rules object</h3></div></div></div><p>
					The fields for the Ingress Node Firewall rules object are described in the following table:
				</p><div class="table" id="idm140587164491056"><p class="title"><strong>Table 9.2. Ingress Node Firewall rules object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587165726592" scope="col">Field</th><th align="left" valign="middle" id="idm140587165725504" scope="col">Type</th><th align="left" valign="middle" id="idm140587165724416" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587165726592"> <p>
									<code class="literal">metadata.name</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587165725504"> <p>
									<code class="literal">string</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587165724416"> <p>
									The name of the CR object.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587165726592"> <p>
									<code class="literal">interfaces</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587165725504"> <p>
									<code class="literal">array</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587165724416"> <p>
									The fields for this object specify the interfaces to apply the firewall rules to. For example, <code class="literal">- en0</code> and <code class="literal">- en1</code>.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587165726592"> <p>
									<code class="literal">nodeSelector</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587165725504"> <p>
									<code class="literal">array</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587165724416"> <p>
									You can use <code class="literal">nodeSelector</code> to select the nodes to apply the firewall rules to. Set the value of your named <code class="literal">nodeselector</code> labels to <code class="literal">true</code> to apply the rule.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587165726592"> <p>
									<code class="literal">ingress</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587165725504"> <p>
									<code class="literal">object</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587165724416"> <p>
									<code class="literal">ingress</code> allows you to configure the rules that allow outside access to the services on your cluster.
								</p>
								 </td></tr></tbody></table></div></div><h6 id="nw-infw-ingress-rules-object_ingress-node-firewall-operator">Ingress object configuration</h6><p>
					The values for the <code class="literal">ingress</code> object are defined in the following table:
				</p><div class="table" id="idm140587149492896"><p class="title"><strong>Table 9.3. <code class="literal">ingress</code> object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 30%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 50%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587139849968" scope="col">Field</th><th align="left" valign="middle" id="idm140587139848880" scope="col">Type</th><th align="left" valign="middle" id="idm140587139847792" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587139849968"> <p>
									<code class="literal">sourceCIDRs</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587139848880"> <p>
									<code class="literal">array</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587139847792"> <p>
									Allows you to set the CIDR block. You can configure multiple CIDRs from different address families.
								</p>
								 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Different CIDRs allow you to use the same order rule. In the case that there are multiple <code class="literal">IngressNodeFirewall</code> objects for the same nodes and interfaces with overlapping CIDRs, the <code class="literal">order</code> field will specify which rule is applied first. Rules are applied in ascending order.
									</p></div></div>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587139849968"> <p>
									<code class="literal">rules</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587139848880"> <p>
									<code class="literal">array</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587139847792"> <p>
									Ingress firewall <code class="literal">rules.order</code> objects are ordered starting at <code class="literal">1</code> for each <code class="literal">source.CIDR</code> with up to 100 rules per CIDR. Lower order rules are executed first.
								</p>
								 <p>
									<code class="literal">rules.protocolConfig.protocol</code> supports the following protocols: TCP, UDP, SCTP, ICMP and ICMPv6. ICMP and ICMPv6 rules can match against ICMP and ICMPv6 types or codes. TCP, UDP, and SCTP rules can match against a single destination port or a range of ports using <code class="literal">&lt;start : end-1&gt;</code> format.
								</p>
								 <p>
									Set <code class="literal">rules.action</code> to <code class="literal">allow</code> to apply the rule or <code class="literal">deny</code> to disallow the rule.
								</p>
								 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Ingress firewall rules are verified using a verification webhook that blocks any invalid configuration. The verification webhook prevents you from blocking any critical cluster services such as the API server or SSH.
									</p></div></div>
								 </td></tr></tbody></table></div></div><h5 id="nw-ingress-node-firewall-example-cr_ingress-node-firewall-operator">Ingress Node Firewall rules object example</h5><p>
					A complete Ingress Node Firewall configuration is specified in the following example:
				</p><div class="formalpara"><p class="title"><strong>Example Ingress Node Firewall configuration</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: ingressnodefirewall.openshift.io/v1alpha1
kind: IngressNodeFirewall
metadata:
  name: ingressnodefirewall
spec:
  interfaces:
  - eth0
  nodeSelector:
    matchLabels:
      &lt;do_node_ingress_firewall&gt;: 'true'
  ingress:
  - sourceCIDRs:
       - 172.16.0.0/12
    rules:
    - order: 10
      protocolConfig:
        protocol: ICMP
        icmp:
          icmpType: 8 #ICMP Echo request
      action: Deny
    - order: 20
      protocolConfig:
        protocol: TCP
        tcp:
          ports: "8000-9000"
      action: Deny
  - sourceCIDRs:
       - fc00:f853:ccd:e793::0/64
    rules:
    - order: 10
      protocolConfig:
        protocol: ICMPv6
        icmpv6:
          icmpType: 128 #ICMPV6 Echo request
      action: Deny</pre>

					</p></div><h5 id="nw-ingress-node-firewall-zero-trust-example-cr_ingress-node-firewall-operator">Zero trust Ingress Node Firewall rules object example</h5><p>
					Zero trust Ingress Node Firewall rules can provide additional security to multi-interface clusters. For example, you can use zero trust Ingress Node Firewall rules to drop all traffic on a specific interface except for SSH.
				</p><p>
					A complete configuration of a zero trust Ingress Node Firewall rule set is specified in the following example:
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Users need to add all ports their application will use to their allowlist in the following case to ensure proper functionality.
					</p></div></div><div class="formalpara"><p class="title"><strong>Example zero trust Ingress Node Firewall rules</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: ingressnodefirewall.openshift.io/v1alpha1
kind: IngressNodeFirewall
metadata:
 name: ingressnodefirewall-zero-trust
spec:
 interfaces:
 - eth1 <span id="CO24-1"><!--Empty--></span><span class="callout">1</span>
 nodeSelector:
   matchLabels:
     &lt;do_node_ingress_firewall&gt;: 'true'
 ingress:
 - sourceCIDRs:
      - 0.0.0.0/0 <span id="CO24-2"><!--Empty--></span><span class="callout">2</span>
   rules:
   - order: 10
     protocolConfig:
       protocol: TCP
       tcp:
         ports: 22
     action: Allow
   - order: 20
     action: Deny <span id="CO24-3"><!--Empty--></span><span class="callout">3</span></pre>

					</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO24-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Multi-interface cluster
						</div></dd><dt><a href="#CO24-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							<code class="literal">0.0.0.0/0</code> set to match any CIDR
						</div></dd><dt><a href="#CO24-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							<code class="literal">action</code> set to <code class="literal">deny</code>
						</div></dd></dl></div></section></section><section class="section" id="nw-infw-operator-viewing_ingress-node-firewall-operator"><div class="titlepage"><div><div><h2 class="title">9.4. Viewing Ingress Node Firewall Operator rules</h2></div></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Run the following command to view all current rules :
					</p><pre class="programlisting language-terminal">$ oc get ingressnodefirewall</pre></li><li class="listitem"><p class="simpara">
						Choose one of the returned <code class="literal">&lt;resource&gt;</code> names and run the following command to view the rules or configs:
					</p><pre class="programlisting language-terminal">$ oc get &lt;resource&gt; &lt;name&gt; -o yaml</pre></li></ol></div></section><section class="section" id="nw-infw-operator-troubleshooting_ingress-node-firewall-operator"><div class="titlepage"><div><div><h2 class="title">9.5. Troubleshooting the Ingress Node Firewall Operator</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Run the following command to list installed Ingress Node Firewall custom resource definitions (CRD):
					</p><pre class="programlisting language-terminal">$ oc get crds | grep ingressnodefirewall</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME               READY   UP-TO-DATE   AVAILABLE   AGE
ingressnodefirewallconfigs.ingressnodefirewall.openshift.io       2022-08-25T10:03:01Z
ingressnodefirewallnodestates.ingressnodefirewall.openshift.io    2022-08-25T10:03:00Z
ingressnodefirewalls.ingressnodefirewall.openshift.io             2022-08-25T10:03:00Z</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Run the following command to view the state of the Ingress Node Firewall Operator:
					</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-ingress-node-firewall</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                                       READY  STATUS         RESTARTS  AGE
ingress-node-firewall-controller-manager   2/2    Running        0         5d21h
ingress-node-firewall-daemon-pqx56         3/3    Running        0         5d21h</pre>

						</p></div><p class="simpara">
						The following fields provide information about the status of the Operator: <code class="literal">READY</code>, <code class="literal">STATUS</code>, <code class="literal">AGE</code>, and <code class="literal">RESTARTS</code>. The <code class="literal">STATUS</code> field is <code class="literal">Running</code> when the Ingress Node Firewall Operator is deploying a daemon set to the assigned nodes.
					</p></li><li class="listitem"><p class="simpara">
						Run the following command to collect all ingress firewall node pods' logs:
					</p><pre class="programlisting language-terminal">$ oc adm must-gather – gather_ingress_node_firewall</pre><p class="simpara">
						The logs are available in the sos node’s report containing eBPF <code class="literal">bpftool</code> outputs at <code class="literal">/sos_commands/ebpf</code>. These reports include lookup tables used or updated as the ingress firewall XDP handles packet processing, updates statistics, and emits events.
					</p></li></ul></div></section></section><section class="chapter" id="ingress-controller-dnsmgt"><div class="titlepage"><div><div><h1 class="title">Chapter 10. Configuring an Ingress Controller for manual DNS Management</h1></div></div></div><p>
			As a cluster administrator, when you create an Ingress Controller, the Operator manages the DNS records automatically. This has some limitations when the required DNS zone is different from the cluster DNS zone or when the DNS zone is hosted outside the cloud provider.
		</p><p>
			As a cluster administrator, you can configure an Ingress Controller to stop automatic DNS management and start manual DNS management. Set <code class="literal">dnsManagementPolicy</code> to specify when it should be automatically or manually managed.
		</p><p>
			When you change an Ingress Controller from <code class="literal">Managed</code> to <code class="literal">Unmanaged</code> DNS management policy, the Operator does not clean up the previous wildcard DNS record provisioned on the cloud. When you change an Ingress Controller from <code class="literal">Unmanaged</code> to <code class="literal">Managed</code> DNS management policy, the Operator attempts to create the DNS record on the cloud provider if it does not exist or updates the DNS record if it already exists.
		</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				When you set <code class="literal">dnsManagementPolicy</code> to <code class="literal">unmanaged</code>, you have to manually manage the lifecycle of the wildcard DNS record on the cloud provider.
			</p></div></div><section class="section" id="literal-managed-literal-dns-management-policy"><div class="titlepage"><div><div><h2 class="title">10.1. <code class="literal">Managed</code> DNS management policy</h2></div></div></div><p>
				The <code class="literal">Managed</code> DNS management policy for Ingress Controllers ensures that the lifecycle of the wildcard DNS record on the cloud provider is automatically managed by the Operator.
			</p></section><section class="section" id="literal-unmanaged-literal-dns-management-policy"><div class="titlepage"><div><div><h2 class="title">10.2. <code class="literal">Unmanaged</code> DNS management policy</h2></div></div></div><p>
				The <code class="literal">Unmanaged</code> DNS management policy for Ingress Controllers ensures that the lifecycle of the wildcard DNS record on the cloud provider is not automatically managed, instead it becomes the responsibility of the cluster administrator.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					On the AWS cloud platform, if the domain on the Ingress Controller does not match with <code class="literal">dnsConfig.Spec.BaseDomain</code> then the DNS management policy is automatically set to <code class="literal">Unmanaged</code>.
				</p></div></div></section><section class="section" id="creating-a-custom-ingress-controller_ingress-controller-dnsmgt"><div class="titlepage"><div><div><h2 class="title">10.3. Creating a custom Ingress Controller with the <code class="literal">Unmanaged</code> DNS management policy</h2></div></div></div><p>
				As a cluster administrator, you can create a new custom Ingress Controller with the <code class="literal">Unmanaged</code> DNS management policy.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						Log in as a user with <code class="literal">cluster-admin</code> privileges.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a custom resource (CR) file named <code class="literal">sample-ingress.yaml</code> containing the following:
					</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  namespace: openshift-ingress-operator
  name: &lt;name&gt; <span id="CO25-1"><!--Empty--></span><span class="callout">1</span>
spec:
  domain: &lt;domain&gt; <span id="CO25-2"><!--Empty--></span><span class="callout">2</span>
  endpointPublishingStrategy:
    type: LoadBalancerService
    loadBalancer:
      scope: External <span id="CO25-3"><!--Empty--></span><span class="callout">3</span>
      dnsManagementPolicy: Unmanaged <span id="CO25-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO25-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specify the <code class="literal">&lt;name&gt;</code> with a name for the <code class="literal">IngressController</code> object.
							</div></dd><dt><a href="#CO25-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specify the <code class="literal">domain</code> based on the DNS record that was created as a prerequisite.
							</div></dd><dt><a href="#CO25-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specify the <code class="literal">scope</code> as <code class="literal">External</code> to expose the load balancer externally.
							</div></dd><dt><a href="#CO25-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								<code class="literal">dnsManagementPolicy</code> indicates if the Ingress Controller is managing the lifecycle of the wildcard DNS record associated with the load balancer. The valid values are <code class="literal">Managed</code> and <code class="literal">Unmanaged</code>. The default value is <code class="literal">Managed</code>.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Save the file to apply the changes.
					</p><pre class="programlisting language-terminal">oc apply -f &lt;name&gt;.yaml <span id="CO26-1"><!--Empty--></span><span class="callout">1</span></pre></li></ol></div></section><section class="section" id="modifying-an-existing-ingress-controller_ingress-controller-dnsmgt"><div class="titlepage"><div><div><h2 class="title">10.4. Modifying an existing Ingress Controller</h2></div></div></div><p>
				As a cluster administrator, you can modify an existing Ingress Controller to manually manage the DNS record lifecycle.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						Log in as a user with <code class="literal">cluster-admin</code> privileges.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Modify the chosen <code class="literal">IngressController</code> to set <code class="literal">dnsManagementPolicy</code>:
					</p><pre class="programlisting language-terminal">SCOPE=$(oc -n openshift-ingress-operator get ingresscontroller &lt;name&gt; -o=jsonpath="{.status.endpointPublishingStrategy.loadBalancer.scope}")

oc -n openshift-ingress-operator patch ingresscontrollers/&lt;name&gt; --type=merge --patch='{"spec":{"endpointPublishingStrategy":{"type":"LoadBalancerService","loadBalancer":{"dnsManagementPolicy":"Unmanaged", "scope":"${SCOPE}"}}}}'</pre></li><li class="listitem">
						Optional: You can delete the associated DNS record in the cloud provider.
					</li></ol></div></section><section class="section _additional-resources" id="configuring-ingress-controller-dns-management-additional-resources"><div class="titlepage"><div><div><h2 class="title">10.5. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-ingress-controller-configuration-parameters_configuring-ingress">Ingress Controller configuration parameters</a>
					</li></ul></div></section></section><section class="chapter" id="nw-ingress-controller-endpoint-publishing-strategies"><div class="titlepage"><div><div><h1 class="title">Chapter 11. Configuring the Ingress Controller endpoint publishing strategy</h1></div></div></div><section class="section" id="nw-ingress-controller-endpoint-publishing-strategies_nw-ingress-controller-endpoint-publishing-strategies"><div class="titlepage"><div><div><h2 class="title">11.1. Ingress Controller endpoint publishing strategy</h2></div></div></div><p>
				<span class="strong strong"><strong><code class="literal">NodePortService</code> endpoint publishing strategy</strong></span>
			</p><p>
				The <code class="literal">NodePortService</code> endpoint publishing strategy publishes the Ingress Controller using a Kubernetes NodePort service.
			</p><p>
				In this configuration, the Ingress Controller deployment uses container networking. A <code class="literal">NodePortService</code> is created to publish the deployment. The specific node ports are dynamically allocated by OpenShift Container Platform; however, to support static port allocations, your changes to the node port field of the managed <code class="literal">NodePortService</code> are preserved.
			</p><div class="figure" id="idm140587165029152"><p class="title"><strong>Figure 11.1. Diagram of NodePortService</strong></p><div class="figure-contents"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/5b5c971559bcbb827169ef0452f4c9b9/202_OpenShift_Ingress_0222_node_port.png" alt="OpenShift Container Platform Ingress NodePort endpoint publishing strategy"/></div></div></div><p>
				The preceding graphic shows the following concepts pertaining to OpenShift Container Platform Ingress NodePort endpoint publishing strategy:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						All the available nodes in the cluster have their own, externally accessible IP addresses. The service running in the cluster is bound to the unique NodePort for all the nodes.
					</li><li class="listitem">
						When the client connects to a node that is down, for example, by connecting the <code class="literal">10.0.128.4</code> IP address in the graphic, the node port directly connects the client to an available node that is running the service. In this scenario, no load balancing is required. As the image shows, the <code class="literal">10.0.128.4</code> address is down and another IP address must be used instead.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The Ingress Operator ignores any updates to <code class="literal">.spec.ports[].nodePort</code> fields of the service.
				</p><p>
					By default, ports are allocated automatically and you can access the port allocations for integrations. However, sometimes static port allocations are necessary to integrate with existing infrastructure which may not be easily reconfigured in response to dynamic ports. To achieve integrations with static node ports, you can update the managed service resource directly.
				</p></div></div><p>
				For more information, see the <a class="link" href="https://kubernetes.io/docs/concepts/services-networking/service/#nodeport">Kubernetes Services documentation on <code class="literal">NodePort</code></a>.
			</p><p>
				<span class="strong strong"><strong><code class="literal">HostNetwork</code> endpoint publishing strategy</strong></span>
			</p><p>
				The <code class="literal">HostNetwork</code> endpoint publishing strategy publishes the Ingress Controller on node ports where the Ingress Controller is deployed.
			</p><p>
				An Ingress Controller with the <code class="literal">HostNetwork</code> endpoint publishing strategy can have only one pod replica per node. If you want <span class="emphasis"><em>n</em></span> replicas, you must use at least <span class="emphasis"><em>n</em></span> nodes where those replicas can be scheduled. Because each pod replica requests ports <code class="literal">80</code> and <code class="literal">443</code> on the node host where it is scheduled, a replica cannot be scheduled to a node if another pod on the same node is using those ports.
			</p><section class="section" id="nw-ingresscontroller-change-internal_nw-ingress-controller-endpoint-publishing-strategies"><div class="titlepage"><div><div><h3 class="title">11.1.1. Configuring the Ingress Controller endpoint publishing scope to Internal</h3></div></div></div><p>
					When a cluster administrator installs a new cluster without specifying that the cluster is private, the default Ingress Controller is created with a <code class="literal">scope</code> set to <code class="literal">External</code>. Cluster administrators can change an <code class="literal">External</code> scoped Ingress Controller to <code class="literal">Internal</code>.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You installed the <code class="literal">oc</code> CLI.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To change an <code class="literal">External</code> scoped Ingress Controller to <code class="literal">Internal</code>, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress-operator patch ingresscontrollers/default --type=merge --patch='{"spec":{"endpointPublishingStrategy":{"type":"LoadBalancerService","loadBalancer":{"scope":"Internal"}}}}'</pre></li><li class="listitem"><p class="simpara">
							To check the status of the Ingress Controller, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress-operator get ingresscontrollers/default -o yaml</pre><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
									The <code class="literal">Progressing</code> status condition indicates whether you must take further action. For example, the status condition can indicate that you need to delete the service by entering the following command:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress delete services/router-default</pre><p class="simpara">
									If you delete the service, the Ingress Operator recreates it as <code class="literal">Internal</code>.
								</p></li></ul></div></li></ul></div></section><section class="section" id="nw-ingresscontroller-change-external_nw-ingress-controller-endpoint-publishing-strategies"><div class="titlepage"><div><div><h3 class="title">11.1.2. Configuring the Ingress Controller endpoint publishing scope to External</h3></div></div></div><p>
					When a cluster administrator installs a new cluster without specifying that the cluster is private, the default Ingress Controller is created with a <code class="literal">scope</code> set to <code class="literal">External</code>.
				</p><p>
					The Ingress Controller’s scope can be configured to be <code class="literal">Internal</code> during installation or after, and cluster administrators can change an <code class="literal">Internal</code> Ingress Controller to <code class="literal">External</code>.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						On some platforms, it is necessary to delete and recreate the service.
					</p><p>
						Changing the scope can cause disruption to Ingress traffic, potentially for several minutes. This applies to platforms where it is necessary to delete and recreate the service, because the procedure can cause OpenShift Container Platform to deprovision the existing service load balancer, provision a new one, and update DNS.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You installed the <code class="literal">oc</code> CLI.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To change an <code class="literal">Internal</code> scoped Ingress Controller to <code class="literal">External</code>, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress-operator patch ingresscontrollers/private --type=merge --patch='{"spec":{"endpointPublishingStrategy":{"type":"LoadBalancerService","loadBalancer":{"scope":"External"}}}}'</pre></li><li class="listitem"><p class="simpara">
							To check the status of the Ingress Controller, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress-operator get ingresscontrollers/default -o yaml</pre><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
									The <code class="literal">Progressing</code> status condition indicates whether you must take further action. For example, the status condition can indicate that you need to delete the service by entering the following command:
								</p><pre class="programlisting language-terminal">$ oc -n openshift-ingress delete services/router-default</pre><p class="simpara">
									If you delete the service, the Ingress Operator recreates it as <code class="literal">External</code>.
								</p></li></ul></div></li></ul></div></section></section><section class="section _additional-resources" id="additional-resources_nw-ingress-controller-endpoint-publishing-strategies"><div class="titlepage"><div><div><h2 class="title">11.2. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-ingress-controller-configuration-parameters_configuring-ingress">Ingress Controller configuration parameters</a>.
					</li></ul></div></section></section><section class="chapter" id="verifying-connectivity-endpoint"><div class="titlepage"><div><div><h1 class="title">Chapter 12. Verifying connectivity to an endpoint</h1></div></div></div><p>
			The Cluster Network Operator (CNO) runs a controller, the connectivity check controller, that performs a connection health check between resources within your cluster. By reviewing the results of the health checks, you can diagnose connection problems or eliminate network connectivity as the cause of an issue that you are investigating.
		</p><section class="section" id="nw-pod-network-connectivity-checks_verifying-connectivity-endpoint"><div class="titlepage"><div><div><h2 class="title">12.1. Connection health checks performed</h2></div></div></div><p>
				To verify that cluster resources are reachable, a TCP connection is made to each of the following cluster API services:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Kubernetes API server service
					</li><li class="listitem">
						Kubernetes API server endpoints
					</li><li class="listitem">
						OpenShift API server service
					</li><li class="listitem">
						OpenShift API server endpoints
					</li><li class="listitem">
						Load balancers
					</li></ul></div><p>
				To verify that services and service endpoints are reachable on every node in the cluster, a TCP connection is made to each of the following targets:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Health check target service
					</li><li class="listitem">
						Health check target endpoints
					</li></ul></div></section><section class="section" id="nw-pod-network-connectivity-implementation_verifying-connectivity-endpoint"><div class="titlepage"><div><div><h2 class="title">12.2. Implementation of connection health checks</h2></div></div></div><p>
				The connectivity check controller orchestrates connection verification checks in your cluster. The results for the connection tests are stored in <code class="literal">PodNetworkConnectivity</code> objects in the <code class="literal">openshift-network-diagnostics</code> namespace. Connection tests are performed every minute in parallel.
			</p><p>
				The Cluster Network Operator (CNO) deploys several resources to the cluster to send and receive connectivity health checks:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Health check source</span></dt><dd>
							This program deploys in a single pod replica set managed by a <code class="literal">Deployment</code> object. The program consumes <code class="literal">PodNetworkConnectivity</code> objects and connects to the <code class="literal">spec.targetEndpoint</code> specified in each object.
						</dd><dt><span class="term">Health check target</span></dt><dd>
							A pod deployed as part of a daemon set on every node in the cluster. The pod listens for inbound health checks. The presence of this pod on every node allows for the testing of connectivity to each node.
						</dd></dl></div></section><section class="section" id="nw-pod-network-connectivity-check-object_verifying-connectivity-endpoint"><div class="titlepage"><div><div><h2 class="title">12.3. PodNetworkConnectivityCheck object fields</h2></div></div></div><p>
				The <code class="literal">PodNetworkConnectivityCheck</code> object fields are described in the following tables.
			</p><div class="table" id="idm140587129878928"><p class="title"><strong>Table 12.1. PodNetworkConnectivityCheck object fields</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 50%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587144029152" scope="col">Field</th><th align="left" valign="top" id="idm140587144028064" scope="col">Type</th><th align="left" valign="top" id="idm140587144026976" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587144029152"> <p>
								<code class="literal">metadata.name</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144028064"> <p>
								<code class="literal">string</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144026976"> <p>
								The name of the object in the following format: <code class="literal">&lt;source&gt;-to-&lt;target&gt;</code>. The destination described by <code class="literal">&lt;target&gt;</code> includes one of following strings:
							</p>
							 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">load-balancer-api-external</code>
									</li><li class="listitem">
										<code class="literal">load-balancer-api-internal</code>
									</li><li class="listitem">
										<code class="literal">kubernetes-apiserver-endpoint</code>
									</li><li class="listitem">
										<code class="literal">kubernetes-apiserver-service-cluster</code>
									</li><li class="listitem">
										<code class="literal">network-check-target</code>
									</li><li class="listitem">
										<code class="literal">openshift-apiserver-endpoint</code>
									</li><li class="listitem">
										<code class="literal">openshift-apiserver-service-cluster</code>
									</li></ul></div>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587144029152"> <p>
								<code class="literal">metadata.namespace</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144028064"> <p>
								<code class="literal">string</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144026976"> <p>
								The namespace that the object is associated with. This value is always <code class="literal">openshift-network-diagnostics</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587144029152"> <p>
								<code class="literal">spec.sourcePod</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144028064"> <p>
								<code class="literal">string</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144026976"> <p>
								The name of the pod where the connection check originates, such as <code class="literal">network-check-source-596b4c6566-rgh92</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587144029152"> <p>
								<code class="literal">spec.targetEndpoint</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144028064"> <p>
								<code class="literal">string</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144026976"> <p>
								The target of the connection check, such as <code class="literal">api.devcluster.example.com:6443</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587144029152"> <p>
								<code class="literal">spec.tlsClientCert</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144028064"> <p>
								<code class="literal">object</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144026976"> <p>
								Configuration for the TLS certificate to use.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587144029152"> <p>
								<code class="literal">spec.tlsClientCert.name</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144028064"> <p>
								<code class="literal">string</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144026976"> <p>
								The name of the TLS certificate used, if any. The default value is an empty string.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587144029152"> <p>
								<code class="literal">status</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144028064"> <p>
								<code class="literal">object</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144026976"> <p>
								An object representing the condition of the connection test and logs of recent connection successes and failures.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587144029152"> <p>
								<code class="literal">status.conditions</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144028064"> <p>
								<code class="literal">array</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144026976"> <p>
								The latest status of the connection check and any previous statuses.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587144029152"> <p>
								<code class="literal">status.failures</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144028064"> <p>
								<code class="literal">array</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144026976"> <p>
								Connection test logs from unsuccessful attempts.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587144029152"> <p>
								<code class="literal">status.outages</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144028064"> <p>
								<code class="literal">array</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144026976"> <p>
								Connect test logs covering the time periods of any outages.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587144029152"> <p>
								<code class="literal">status.successes</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144028064"> <p>
								<code class="literal">array</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587144026976"> <p>
								Connection test logs from successful attempts.
							</p>
							 </td></tr></tbody></table></div></div><p>
				The following table describes the fields for objects in the <code class="literal">status.conditions</code> array:
			</p><div class="table" id="idm140587158918832"><p class="title"><strong>Table 12.2. status.conditions</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 50%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587141921040" scope="col">Field</th><th align="left" valign="top" id="idm140587141919952" scope="col">Type</th><th align="left" valign="top" id="idm140587156714896" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587141921040"> <p>
								<code class="literal">lastTransitionTime</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587141919952"> <p>
								<code class="literal">string</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587156714896"> <p>
								The time that the condition of the connection transitioned from one status to another.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587141921040"> <p>
								<code class="literal">message</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587141919952"> <p>
								<code class="literal">string</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587156714896"> <p>
								The details about last transition in a human readable format.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587141921040"> <p>
								<code class="literal">reason</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587141919952"> <p>
								<code class="literal">string</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587156714896"> <p>
								The last status of the transition in a machine readable format.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587141921040"> <p>
								<code class="literal">status</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587141919952"> <p>
								<code class="literal">string</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587156714896"> <p>
								The status of the condition.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587141921040"> <p>
								<code class="literal">type</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587141919952"> <p>
								<code class="literal">string</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587156714896"> <p>
								The type of the condition.
							</p>
							 </td></tr></tbody></table></div></div><p>
				The following table describes the fields for objects in the <code class="literal">status.conditions</code> array:
			</p><div class="table" id="idm140587154428272"><p class="title"><strong>Table 12.3. status.outages</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 50%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587152901344" scope="col">Field</th><th align="left" valign="top" id="idm140587166160464" scope="col">Type</th><th align="left" valign="top" id="idm140587166159376" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587152901344"> <p>
								<code class="literal">end</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587166160464"> <p>
								<code class="literal">string</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587166159376"> <p>
								The timestamp from when the connection failure is resolved.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587152901344"> <p>
								<code class="literal">endLogs</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587166160464"> <p>
								<code class="literal">array</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587166159376"> <p>
								Connection log entries, including the log entry related to the successful end of the outage.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587152901344"> <p>
								<code class="literal">message</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587166160464"> <p>
								<code class="literal">string</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587166159376"> <p>
								A summary of outage details in a human readable format.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587152901344"> <p>
								<code class="literal">start</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587166160464"> <p>
								<code class="literal">string</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587166159376"> <p>
								The timestamp from when the connection failure is first detected.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587152901344"> <p>
								<code class="literal">startLogs</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587166160464"> <p>
								<code class="literal">array</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587166159376"> <p>
								Connection log entries, including the original failure.
							</p>
							 </td></tr></tbody></table></div></div><h4 id="connection-log-fields">Connection log fields</h4><p>
				The fields for a connection log entry are described in the following table. The object is used in the following fields:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">status.failures[]</code>
					</li><li class="listitem">
						<code class="literal">status.successes[]</code>
					</li><li class="listitem">
						<code class="literal">status.outages[].startLogs[]</code>
					</li><li class="listitem">
						<code class="literal">status.outages[].endLogs[]</code>
					</li></ul></div><div class="table" id="idm140587129633728"><p class="title"><strong>Table 12.4. Connection log object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 50%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587176320832" scope="col">Field</th><th align="left" valign="top" id="idm140587176319744" scope="col">Type</th><th align="left" valign="top" id="idm140587176318656" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587176320832"> <p>
								<code class="literal">latency</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587176319744"> <p>
								<code class="literal">string</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587176318656"> <p>
								Records the duration of the action.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587176320832"> <p>
								<code class="literal">message</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587176319744"> <p>
								<code class="literal">string</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587176318656"> <p>
								Provides the status in a human readable format.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587176320832"> <p>
								<code class="literal">reason</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587176319744"> <p>
								<code class="literal">string</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587176318656"> <p>
								Provides the reason for status in a machine readable format. The value is one of <code class="literal">TCPConnect</code>, <code class="literal">TCPConnectError</code>, <code class="literal">DNSResolve</code>, <code class="literal">DNSError</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587176320832"> <p>
								<code class="literal">success</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587176319744"> <p>
								<code class="literal">boolean</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587176318656"> <p>
								Indicates if the log entry is a success or failure.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587176320832"> <p>
								<code class="literal">time</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587176319744"> <p>
								<code class="literal">string</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587176318656"> <p>
								The start time of connection check.
							</p>
							 </td></tr></tbody></table></div></div></section><section class="section" id="nw-pod-network-connectivity-verify_verifying-connectivity-endpoint"><div class="titlepage"><div><div><h2 class="title">12.4. Verifying network connectivity for an endpoint</h2></div></div></div><p>
				As a cluster administrator, you can verify the connectivity of an endpoint, such as an API server, load balancer, service, or pod.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To list the current <code class="literal">PodNetworkConnectivityCheck</code> objects, enter the following command:
					</p><pre class="programlisting language-terminal">$ oc get podnetworkconnectivitycheck -n openshift-network-diagnostics</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                                                                                                                                AGE
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0   75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-1   73m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-2   75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-kubernetes-apiserver-service-cluster                               75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-kubernetes-default-service-cluster                                 75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-load-balancer-api-external                                         75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-load-balancer-api-internal                                         75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-network-check-target-ci-ln-x5sv9rb-f76d1-4rzrp-master-0            75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-network-check-target-ci-ln-x5sv9rb-f76d1-4rzrp-master-1            75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-network-check-target-ci-ln-x5sv9rb-f76d1-4rzrp-master-2            75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-network-check-target-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh      74m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-network-check-target-ci-ln-x5sv9rb-f76d1-4rzrp-worker-c-n8mbf      74m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-network-check-target-ci-ln-x5sv9rb-f76d1-4rzrp-worker-d-4hnrz      74m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-network-check-target-service-cluster                               75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-openshift-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0    75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-openshift-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-1    75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-openshift-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-2    74m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-openshift-apiserver-service-cluster                                75m</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						View the connection test logs:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								From the output of the previous command, identify the endpoint that you want to review the connectivity logs for.
							</li><li class="listitem"><p class="simpara">
								To view the object, enter the following command:
							</p><pre class="programlisting language-terminal">$ oc get podnetworkconnectivitycheck &lt;name&gt; \
  -n openshift-network-diagnostics -o yaml</pre><p class="simpara">
								where <code class="literal">&lt;name&gt;</code> specifies the name of the <code class="literal">PodNetworkConnectivityCheck</code> object.
							</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">apiVersion: controlplane.operator.openshift.io/v1alpha1
kind: PodNetworkConnectivityCheck
metadata:
  name: network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0
  namespace: openshift-network-diagnostics
  ...
spec:
  sourcePod: network-check-source-7c88f6d9f-hmg2f
  targetEndpoint: 10.0.0.4:6443
  tlsClientCert:
    name: ""
status:
  conditions:
  - lastTransitionTime: "2021-01-13T20:11:34Z"
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnectSuccess
    status: "True"
    type: Reachable
  failures:
  - latency: 2.241775ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: failed
      to establish a TCP connection to 10.0.0.4:6443: dial tcp 10.0.0.4:6443: connect:
      connection refused'
    reason: TCPConnectError
    success: false
    time: "2021-01-13T20:10:34Z"
  - latency: 2.582129ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: failed
      to establish a TCP connection to 10.0.0.4:6443: dial tcp 10.0.0.4:6443: connect:
      connection refused'
    reason: TCPConnectError
    success: false
    time: "2021-01-13T20:09:34Z"
  - latency: 3.483578ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: failed
      to establish a TCP connection to 10.0.0.4:6443: dial tcp 10.0.0.4:6443: connect:
      connection refused'
    reason: TCPConnectError
    success: false
    time: "2021-01-13T20:08:34Z"
  outages:
  - end: "2021-01-13T20:11:34Z"
    endLogs:
    - latency: 2.032018ms
      message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0:
        tcp connection to 10.0.0.4:6443 succeeded'
      reason: TCPConnect
      success: true
      time: "2021-01-13T20:11:34Z"
    - latency: 2.241775ms
      message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0:
        failed to establish a TCP connection to 10.0.0.4:6443: dial tcp 10.0.0.4:6443:
        connect: connection refused'
      reason: TCPConnectError
      success: false
      time: "2021-01-13T20:10:34Z"
    - latency: 2.582129ms
      message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0:
        failed to establish a TCP connection to 10.0.0.4:6443: dial tcp 10.0.0.4:6443:
        connect: connection refused'
      reason: TCPConnectError
      success: false
      time: "2021-01-13T20:09:34Z"
    - latency: 3.483578ms
      message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0:
        failed to establish a TCP connection to 10.0.0.4:6443: dial tcp 10.0.0.4:6443:
        connect: connection refused'
      reason: TCPConnectError
      success: false
      time: "2021-01-13T20:08:34Z"
    message: Connectivity restored after 2m59.999789186s
    start: "2021-01-13T20:08:34Z"
    startLogs:
    - latency: 3.483578ms
      message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0:
        failed to establish a TCP connection to 10.0.0.4:6443: dial tcp 10.0.0.4:6443:
        connect: connection refused'
      reason: TCPConnectError
      success: false
      time: "2021-01-13T20:08:34Z"
  successes:
  - latency: 2.845865ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:14:34Z"
  - latency: 2.926345ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:13:34Z"
  - latency: 2.895796ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:12:34Z"
  - latency: 2.696844ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:11:34Z"
  - latency: 1.502064ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:10:34Z"
  - latency: 1.388857ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:09:34Z"
  - latency: 1.906383ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:08:34Z"
  - latency: 2.089073ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:07:34Z"
  - latency: 2.156994ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:06:34Z"
  - latency: 1.777043ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:05:34Z"</pre>

								</p></div></li></ol></div></li></ol></div></section></section><section class="chapter" id="changing-cluster-network-mtu"><div class="titlepage"><div><div><h1 class="title">Chapter 13. Changing the MTU for the cluster network</h1></div></div></div><p class="_abstract _abstract">
			As a cluster administrator, you can change the MTU for the cluster network after cluster installation. This change is disruptive as cluster nodes must be rebooted to finalize the MTU change. You can change the MTU only for clusters using the OVN-Kubernetes or OpenShift SDN network plugins.
		</p><section class="section" id="nw-cluster-mtu-change-about_changing-cluster-network-mtu"><div class="titlepage"><div><div><h2 class="title">13.1. About the cluster MTU</h2></div></div></div><p>
				During installation the maximum transmission unit (MTU) for the cluster network is detected automatically based on the MTU of the primary network interface of nodes in the cluster. You do not normally need to override the detected MTU.
			</p><p>
				You might want to change the MTU of the cluster network for several reasons:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The MTU detected during cluster installation is not correct for your infrastructure
					</li><li class="listitem">
						Your cluster infrastructure now requires a different MTU, such as from the addition of nodes that need a different MTU for optimal performance
					</li></ul></div><p>
				You can change the cluster MTU for only the OVN-Kubernetes and OpenShift SDN cluster network plugins.
			</p><section class="section" id="service-interruption-considerations_changing-cluster-network-mtu"><div class="titlepage"><div><div><h3 class="title">13.1.1. Service interruption considerations</h3></div></div></div><p>
					When you initiate an MTU change on your cluster the following effects might impact service availability:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							At least two rolling reboots are required to complete the migration to a new MTU. During this time, some nodes are not available as they restart.
						</li><li class="listitem">
							Specific applications deployed to the cluster with shorter timeout intervals than the absolute TCP timeout interval might experience disruption during the MTU change.
						</li></ul></div></section><section class="section" id="mtu-value-selection_changing-cluster-network-mtu"><div class="titlepage"><div><div><h3 class="title">13.1.2. MTU value selection</h3></div></div></div><p>
					When planning your MTU migration there are two related but distinct MTU values to consider.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<span class="strong strong"><strong>Hardware MTU</strong></span>: This MTU value is set based on the specifics of your network infrastructure.
						</li><li class="listitem"><p class="simpara">
							<span class="strong strong"><strong>Cluster network MTU</strong></span>: This MTU value is always less than your hardware MTU to account for the cluster network overlay overhead. The specific overhead is determined by your network plugin:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<span class="strong strong"><strong>OVN-Kubernetes</strong></span>: <code class="literal">100</code> bytes
								</li><li class="listitem">
									<span class="strong strong"><strong>OpenShift SDN</strong></span>: <code class="literal">50</code> bytes
								</li></ul></div></li></ul></div><p>
					If your cluster requires different MTU values for different nodes, you must subtract the overhead value for your network plugin from the lowest MTU value that is used by any node in your cluster. For example, if some nodes in your cluster have an MTU of <code class="literal">9001</code>, and some have an MTU of <code class="literal">1500</code>, you must set this value to <code class="literal">1400</code>.
				</p></section><section class="section" id="how-the-migration-process-works_changing-cluster-network-mtu"><div class="titlepage"><div><div><h3 class="title">13.1.3. How the migration process works</h3></div></div></div><p>
					The following table summarizes the migration process by segmenting between the user-initiated steps in the process and the actions that the migration performs in response.
				</p><div class="table" id="idm140587167430800"><p class="title"><strong>Table 13.1. Live migration of the cluster MTU</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587170089616" scope="col">User-initiated steps</th><th align="left" valign="top" id="idm140587170088528" scope="col">OpenShift Container Platform activity</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587170089616"> <p>
									Set the following values in the Cluster Network Operator configuration:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">spec.migration.mtu.machine.to</code>
										</li><li class="listitem">
											<code class="literal">spec.migration.mtu.network.from</code>
										</li><li class="listitem">
											<code class="literal">spec.migration.mtu.network.to</code>
										</li></ul></div>
								 </td><td align="left" valign="top" headers="idm140587170088528"> <p>
									<span class="strong strong"><strong>Cluster Network Operator (CNO)</strong></span>: Confirms that each field is set to a valid value.
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											The <code class="literal">mtu.machine.to</code> must be set to either the new hardware MTU or to the current hardware MTU if the MTU for the hardware is not changing. This value is transient and is used as part of the migration process. Separately, if you specify a hardware MTU that is different from your existing hardware MTU value, you must manually configure the MTU to persist by other means, such as with a machine config, DHCP setting, or a Linux kernel command line.
										</li><li class="listitem">
											The <code class="literal">mtu.network.from</code> field must equal the <code class="literal">network.status.clusterNetworkMTU</code> field, which is the current MTU of the cluster network.
										</li><li class="listitem">
											The <code class="literal">mtu.network.to</code> field must be set to the target cluster network MTU and must be lower than the hardware MTU to allow for the overlay overhead of the network plugin. For OVN-Kubernetes, the overhead is <code class="literal">100</code> bytes and for OpenShift SDN the overhead is <code class="literal">50</code> bytes.
										</li></ul></div>
								 <p>
									If the values provided are valid, the CNO writes out a new temporary configuration with the MTU for the cluster network set to the value of the <code class="literal">mtu.network.to</code> field.
								</p>
								 <p>
									<span class="strong strong"><strong>Machine Config Operator (MCO)</strong></span>: Performs a rolling reboot of each node in the cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587170089616"> <p>
									Reconfigure the MTU of the primary network interface for the nodes on the cluster. You can use a variety of methods to accomplish this, including:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Deploying a new NetworkManager connection profile with the MTU change
										</li><li class="listitem">
											Changing the MTU through a DHCP server setting
										</li><li class="listitem">
											Changing the MTU through boot parameters
										</li></ul></div>
								 </td><td align="left" valign="top" headers="idm140587170088528"> <p>
									N/A
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587170089616"> <p>
									Set the <code class="literal">mtu</code> value in the CNO configuration for the network plugin and set <code class="literal">spec.migration</code> to <code class="literal">null</code>.
								</p>
								 </td><td align="left" valign="top" headers="idm140587170088528"> <p>
									<span class="strong strong"><strong>Machine Config Operator (MCO)</strong></span>: Performs a rolling reboot of each node in the cluster with the new MTU configuration.
								</p>
								 </td></tr></tbody></table></div></div></section></section><section class="section" id="nw-cluster-mtu-change_changing-cluster-network-mtu"><div class="titlepage"><div><div><h2 class="title">13.2. Changing the cluster MTU</h2></div></div></div><p>
				As a cluster administrator, you can change the maximum transmission unit (MTU) for your cluster. The migration is disruptive and nodes in your cluster might be temporarily unavailable as the MTU update rolls out.
			</p><p>
				The following procedure describes how to change the cluster MTU by using either machine configs, DHCP, or an ISO. If you use the DHCP or ISO approach, you must refer to configuration artifacts that you kept after installing your cluster to complete the procedure.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You are logged in to the cluster with a user with <code class="literal">cluster-admin</code> privileges.
					</li><li class="listitem"><p class="simpara">
						You identified the target MTU for your cluster. The correct MTU varies depending on the network plugin that your cluster uses:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								<span class="strong strong"><strong>OVN-Kubernetes</strong></span>: The cluster MTU must be set to <code class="literal">100</code> less than the lowest hardware MTU value in your cluster.
							</li><li class="listitem">
								<span class="strong strong"><strong>OpenShift SDN</strong></span>: The cluster MTU must be set to <code class="literal">50</code> less than the lowest hardware MTU value in your cluster.
							</li></ul></div></li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					To increase or decrease the MTU for the cluster network complete the following procedure.
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To obtain the current MTU for the cluster network, enter the following command:
					</p><pre class="programlisting language-terminal">$ oc describe network.config cluster</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-text">...
Status:
  Cluster Network:
    Cidr:               10.217.0.0/22
    Host Prefix:        23
  Cluster Network MTU:  1400
  Network Type:         OpenShiftSDN
  Service Network:
    10.217.4.0/23
...</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Prepare your configuration for the hardware MTU:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								If your hardware MTU is specified with DHCP, update your DHCP configuration such as with the following dnsmasq configuration:
							</p><pre class="programlisting language-text">dhcp-option-force=26,&lt;mtu&gt;</pre><p class="simpara">
								where:
							</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;mtu&gt;</code></span></dt><dd>
											Specifies the hardware MTU for the DHCP server to advertise.
										</dd></dl></div></li><li class="listitem">
								If your hardware MTU is specified with a kernel command line with PXE, update that configuration accordingly.
							</li><li class="listitem"><p class="simpara">
								If your hardware MTU is specified in a NetworkManager connection configuration, complete the following steps. This approach is the default for OpenShift Container Platform if you do not explicitly specify your network configuration with DHCP, a kernel command line, or some other method. Your cluster nodes must all use the same underlying network configuration for the following procedure to work unmodified.
							</p><div class="orderedlist"><ol class="orderedlist" type="i"><li class="listitem"><p class="simpara">
										Find the primary network interface:
									</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
												If you are using the OpenShift SDN network plugin, enter the following command:
											</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt; -- chroot /host ip route list match 0.0.0.0/0 | awk '{print $5 }'</pre><p class="simpara">
												where:
											</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;node_name&gt;</code></span></dt><dd>
															Specifies the name of a node in your cluster.
														</dd></dl></div></li><li class="listitem"><p class="simpara">
												If you are using the OVN-Kubernetes network plugin, enter the following command:
											</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt; -- chroot /host nmcli -g connection.interface-name c show ovs-if-phys0</pre><p class="simpara">
												where:
											</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;node_name&gt;</code></span></dt><dd>
															Specifies the name of a node in your cluster.
														</dd></dl></div></li></ul></div></li><li class="listitem"><p class="simpara">
										Create the following NetworkManager configuration in the <code class="literal">&lt;interface&gt;-mtu.conf</code> file:
									</p><div class="formalpara"><p class="title"><strong>Example NetworkManager connection configuration</strong></p><p>
											
<pre class="programlisting language-ini">[connection-&lt;interface&gt;-mtu]
match-device=interface-name:&lt;interface&gt;
ethernet.mtu=&lt;mtu&gt;</pre>

										</p></div><p class="simpara">
										where:
									</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;mtu&gt;</code></span></dt><dd>
													Specifies the new hardware MTU value.
												</dd><dt><span class="term"><code class="literal">&lt;interface&gt;</code></span></dt><dd>
													Specifies the primary network interface name.
												</dd></dl></div></li><li class="listitem"><p class="simpara">
										Create two <code class="literal">MachineConfig</code> objects, one for the control plane nodes and another for the worker nodes in your cluster:
									</p><div class="orderedlist"><ol class="orderedlist" type="A"><li class="listitem"><p class="simpara">
												Create the following Butane config in the <code class="literal">control-plane-interface.bu</code> file:
											</p><pre class="programlisting language-yaml">variant: openshift
version: 4.13.0
metadata:
  name: 01-control-plane-interface
  labels:
    machineconfiguration.openshift.io/role: master
storage:
  files:
    - path: /etc/NetworkManager/conf.d/99-&lt;interface&gt;-mtu.conf <span id="CO26-2"><!--Empty--></span><span class="callout">1</span>
      contents:
        local: &lt;interface&gt;-mtu.conf <span id="CO26-3"><!--Empty--></span><span class="callout">2</span>
      mode: 0600</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO26-1"><span class="callout">1</span></a> <a href="#CO26-2"><span class="callout">1</span></a> </dt><dd><div class="para">
														Specify the NetworkManager connection name for the primary network interface.
													</div></dd><dt><a href="#CO26-3"><span class="callout">2</span></a> </dt><dd><div class="para">
														Specify the local filename for the updated NetworkManager configuration file from the previous step.
													</div></dd></dl></div></li><li class="listitem"><p class="simpara">
												Create the following Butane config in the <code class="literal">worker-interface.bu</code> file:
											</p><pre class="programlisting language-yaml">variant: openshift
version: 4.13.0
metadata:
  name: 01-worker-interface
  labels:
    machineconfiguration.openshift.io/role: worker
storage:
  files:
    - path: /etc/NetworkManager/conf.d/99-&lt;interface&gt;-mtu.conf <span id="CO27-1"><!--Empty--></span><span class="callout">1</span>
      contents:
        local: &lt;interface&gt;-mtu.conf <span id="CO27-2"><!--Empty--></span><span class="callout">2</span>
      mode: 0600</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO27-1"><span class="callout">1</span></a> </dt><dd><div class="para">
														Specify the NetworkManager connection name for the primary network interface.
													</div></dd><dt><a href="#CO27-2"><span class="callout">2</span></a> </dt><dd><div class="para">
														Specify the local filename for the updated NetworkManager configuration file from the previous step.
													</div></dd></dl></div></li><li class="listitem"><p class="simpara">
												Create <code class="literal">MachineConfig</code> objects from the Butane configs by running the following command:
											</p><pre class="programlisting language-terminal">$ for manifest in control-plane-interface worker-interface; do
    butane --files-dir . $manifest.bu &gt; $manifest.yaml
  done</pre></li></ol></div></li></ol></div></li></ul></div></li><li class="listitem"><p class="simpara">
						To begin the MTU migration, specify the migration configuration by entering the following command. The Machine Config Operator performs a rolling reboot of the nodes in the cluster in preparation for the MTU change.
					</p><pre class="programlisting language-terminal">$ oc patch Network.operator.openshift.io cluster --type=merge --patch \
  '{"spec": { "migration": { "mtu": { "network": { "from": &lt;overlay_from&gt;, "to": &lt;overlay_to&gt; } , "machine": { "to" : &lt;machine_to&gt; } } } } }'</pre><p class="simpara">
						where:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;overlay_from&gt;</code></span></dt><dd>
									Specifies the current cluster network MTU value.
								</dd><dt><span class="term"><code class="literal">&lt;overlay_to&gt;</code></span></dt><dd>
									Specifies the target MTU for the cluster network. This value is set relative to the value for <code class="literal">&lt;machine_to&gt;</code> and for OVN-Kubernetes must be <code class="literal">100</code> less and for OpenShift SDN must be <code class="literal">50</code> less.
								</dd><dt><span class="term"><code class="literal">&lt;machine_to&gt;</code></span></dt><dd>
									Specifies the MTU for the primary network interface on the underlying host network.
								</dd></dl></div><div class="formalpara"><p class="title"><strong>Example that increases the cluster MTU</strong></p><p>
							
<pre class="programlisting language-terminal">$ oc patch Network.operator.openshift.io cluster --type=merge --patch \
  '{"spec": { "migration": { "mtu": { "network": { "from": 1400, "to": 9000 } , "machine": { "to" : 9100} } } } }'</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						As the MCO updates machines in each machine config pool, it reboots each node one by one. You must wait until all the nodes are updated. Check the machine config pool status by entering the following command:
					</p><pre class="programlisting language-terminal">$ oc get mcp</pre><p class="simpara">
						A successfully updated node has the following status: <code class="literal">UPDATED=true</code>, <code class="literal">UPDATING=false</code>, <code class="literal">DEGRADED=false</code>.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							By default, the MCO updates one machine per pool at a time, causing the total time the migration takes to increase with the size of the cluster.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Confirm the status of the new machine configuration on the hosts:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								To list the machine configuration state and the name of the applied machine configuration, enter the following command:
							</p><pre class="programlisting language-terminal">$ oc describe node | egrep "hostname|machineconfig"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-text">kubernetes.io/hostname=master-0
machineconfiguration.openshift.io/currentConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/desiredConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done</pre>

								</p></div><p class="simpara">
								Verify that the following statements are true:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										The value of <code class="literal">machineconfiguration.openshift.io/state</code> field is <code class="literal">Done</code>.
									</li><li class="listitem">
										The value of the <code class="literal">machineconfiguration.openshift.io/currentConfig</code> field is equal to the value of the <code class="literal">machineconfiguration.openshift.io/desiredConfig</code> field.
									</li></ul></div></li><li class="listitem"><p class="simpara">
								To confirm that the machine config is correct, enter the following command:
							</p><pre class="programlisting language-terminal">$ oc get machineconfig &lt;config_name&gt; -o yaml | grep ExecStart</pre><p class="simpara">
								where <code class="literal">&lt;config_name&gt;</code> is the name of the machine config from the <code class="literal">machineconfiguration.openshift.io/currentConfig</code> field.
							</p><p class="simpara">
								The machine config must include the following update to the systemd configuration:
							</p><pre class="programlisting language-plain">ExecStart=/usr/local/bin/mtu-migration.sh</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Update the underlying network interface MTU value:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								If you are specifying the new MTU with a NetworkManager connection configuration, enter the following command. The MachineConfig Operator automatically performs a rolling reboot of the nodes in your cluster.
							</p><pre class="programlisting language-terminal">$ for manifest in control-plane-interface worker-interface; do
    oc create -f $manifest.yaml
  done</pre></li><li class="listitem">
								If you are specifying the new MTU with a DHCP server option or a kernel command line and PXE, make the necessary changes for your infrastructure.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						As the MCO updates machines in each machine config pool, it reboots each node one by one. You must wait until all the nodes are updated. Check the machine config pool status by entering the following command:
					</p><pre class="programlisting language-terminal">$ oc get mcp</pre><p class="simpara">
						A successfully updated node has the following status: <code class="literal">UPDATED=true</code>, <code class="literal">UPDATING=false</code>, <code class="literal">DEGRADED=false</code>.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							By default, the MCO updates one machine per pool at a time, causing the total time the migration takes to increase with the size of the cluster.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Confirm the status of the new machine configuration on the hosts:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								To list the machine configuration state and the name of the applied machine configuration, enter the following command:
							</p><pre class="programlisting language-terminal">$ oc describe node | egrep "hostname|machineconfig"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-text">kubernetes.io/hostname=master-0
machineconfiguration.openshift.io/currentConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/desiredConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done</pre>

								</p></div><p class="simpara">
								Verify that the following statements are true:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										The value of <code class="literal">machineconfiguration.openshift.io/state</code> field is <code class="literal">Done</code>.
									</li><li class="listitem">
										The value of the <code class="literal">machineconfiguration.openshift.io/currentConfig</code> field is equal to the value of the <code class="literal">machineconfiguration.openshift.io/desiredConfig</code> field.
									</li></ul></div></li><li class="listitem"><p class="simpara">
								To confirm that the machine config is correct, enter the following command:
							</p><pre class="programlisting language-terminal">$ oc get machineconfig &lt;config_name&gt; -o yaml | grep path:</pre><p class="simpara">
								where <code class="literal">&lt;config_name&gt;</code> is the name of the machine config from the <code class="literal">machineconfiguration.openshift.io/currentConfig</code> field.
							</p><p class="simpara">
								If the machine config is successfully deployed, the previous output contains the <code class="literal">/etc/NetworkManager/system-connections/&lt;connection_name&gt;</code> file path.
							</p><p class="simpara">
								The machine config must not contain the <code class="literal">ExecStart=/usr/local/bin/mtu-migration.sh</code> line.
							</p></li></ol></div></li><li class="listitem"><p class="simpara">
						To finalize the MTU migration, enter one of the following commands:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								If you are using the OVN-Kubernetes network plugin:
							</p><pre class="programlisting language-terminal">$ oc patch Network.operator.openshift.io cluster --type=merge --patch \
  '{"spec": { "migration": null, "defaultNetwork":{ "ovnKubernetesConfig": { "mtu": &lt;mtu&gt; }}}}'</pre><p class="simpara">
								where:
							</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;mtu&gt;</code></span></dt><dd>
											Specifies the new cluster network MTU that you specified with <code class="literal">&lt;overlay_to&gt;</code>.
										</dd></dl></div></li><li class="listitem"><p class="simpara">
								If you are using the OpenShift SDN network plugin:
							</p><pre class="programlisting language-terminal">$ oc patch Network.operator.openshift.io cluster --type=merge --patch \
  '{"spec": { "migration": null, "defaultNetwork":{ "openshiftSDNConfig": { "mtu": &lt;mtu&gt; }}}}'</pre><p class="simpara">
								where:
							</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;mtu&gt;</code></span></dt><dd>
											Specifies the new cluster network MTU that you specified with <code class="literal">&lt;overlay_to&gt;</code>.
										</dd></dl></div></li></ul></div></li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
					You can verify that a node in your cluster uses an MTU that you specified in the previous procedure.
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To get the current MTU for the cluster network, enter the following command:
					</p><pre class="programlisting language-terminal">$ oc describe network.config cluster</pre></li><li class="listitem"><p class="simpara">
						Get the current MTU for the primary network interface of a node.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								To list the nodes in your cluster, enter the following command:
							</p><pre class="programlisting language-terminal">$ oc get nodes</pre></li><li class="listitem"><p class="simpara">
								To obtain the current MTU setting for the primary network interface on a node, enter the following command:
							</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node&gt; -- chroot /host ip address show &lt;interface&gt;</pre><p class="simpara">
								where:
							</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;node&gt;</code></span></dt><dd>
											Specifies a node from the output from the previous step.
										</dd><dt><span class="term"><code class="literal">&lt;interface&gt;</code></span></dt><dd>
											Specifies the primary network interface name for the node.
										</dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-text">ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 8051</pre>

								</p></div></li></ol></div></li></ol></div></section><section class="section _additional-resources" id="changing-cluster-network-mtu-additional-resources"><div class="titlepage"><div><div><h2 class="title">13.3. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-user-infra-machines-advanced_network_installing-bare-metal">Using advanced networking options for PXE and ISO installations</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_networking/assembly_manually-creating-networkmanager-profiles-in-key-file-format_configuring-and-managing-networking">Manually creating NetworkManager profiles in key file format</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_and_managing_networking/index#configuring-a-dynamic-ethernet-connection-using-nmcli_configuring-an-ethernet-connection">Configuring a dynamic Ethernet connection using nmcli</a>
					</li></ul></div></section></section><section class="chapter" id="configuring-node-port-service-range"><div class="titlepage"><div><div><h1 class="title">Chapter 14. Configuring the node port service range</h1></div></div></div><p>
			As a cluster administrator, you can expand the available node port range. If your cluster uses of a large number of node ports, you might need to increase the number of available ports.
		</p><p>
			The default port range is <code class="literal">30000-32767</code>. You can never reduce the port range, even if you first expand it beyond the default range.
		</p><section class="section" id="configuring-node-port-service-range-prerequisites"><div class="titlepage"><div><div><h2 class="title">14.1. Prerequisites</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Your cluster infrastructure must allow access to the ports that you specify within the expanded range. For example, if you expand the node port range to <code class="literal">30000-32900</code>, the inclusive port range of <code class="literal">32768-32900</code> must be allowed by your firewall or packet filtering configuration.
					</li></ul></div></section><section class="section" id="nw-nodeport-service-range-edit_configuring-node-port-service-range"><div class="titlepage"><div><div><h2 class="title">14.2. Expanding the node port range</h2></div></div></div><p>
				You can expand the node port range for the cluster.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						Log in to the cluster with a user with <code class="literal">cluster-admin</code> privileges.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To expand the node port range, enter the following command. Replace <code class="literal">&lt;port&gt;</code> with the largest port number in the new range.
					</p><pre class="programlisting language-terminal">$ oc patch network.config.openshift.io cluster --type=merge -p \
  '{
    "spec":
      { "serviceNodePortRange": "30000-&lt;port&gt;" }
  }'</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
						You can alternatively apply the following YAML to update the node port range:
					</p><pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  serviceNodePortRange: "30000-&lt;port&gt;"</pre></div></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">network.config.openshift.io/cluster patched</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						To confirm that the configuration is active, enter the following command. It can take several minutes for the update to apply.
					</p><pre class="programlisting language-terminal">$ oc get configmaps -n openshift-kube-apiserver config \
  -o jsonpath="{.data['config\.yaml']}" | \
  grep -Eo '"service-node-port-range":["[[:digit:]]+-[[:digit:]]+"]'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">"service-node-port-range":["30000-33000"]</pre>

						</p></div></li></ol></div></section><section class="section _additional-resources" id="configuring-node-port-service-range-additional-resources"><div class="titlepage"><div><div><h2 class="title">14.3. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-ingress-cluster-traffic-nodeport">Configuring ingress cluster traffic using a NodePort</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/api_reference/#network-config-openshift-io-v1">Network [config.openshift.io/v1</a>]
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/api_reference/#service-v1">Service [core/v1</a>]
					</li></ul></div></section></section><section class="chapter" id="configuring-cluster-network-range"><div class="titlepage"><div><div><h1 class="title">Chapter 15. Configuring the cluster network range</h1></div></div></div><p>
			As a cluster administrator, you can expand the cluster network range after cluster installation. You might want to expand the cluster network range if you need more IP addresses for additional nodes.
		</p><p>
			For example, if you deployed a cluster and specified <code class="literal">10.128.0.0/19</code> as the cluster network range and a host prefix of <code class="literal">23</code>, you are limited to 16 nodes. You can expand that to 510 nodes by changing the CIDR mask on a cluster to <code class="literal">/14</code>.
		</p><p>
			When expanding the cluster network address range, your cluster must use the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-ovn-kubernetes">OVN-Kubernetes network plugin</a>. Other network plugins are not supported.
		</p><p>
			The following limitations apply when modifying the cluster network IP address range:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					The CIDR mask size specified must always be smaller than the currently configured CIDR mask size, because you can only increase IP space by adding more nodes to an installed cluster
				</li><li class="listitem">
					The host prefix cannot be modified
				</li><li class="listitem">
					Pods that are configured with an overridden default gateway must be recreated after the cluster network expands
				</li></ul></div><section class="section" id="nw-cluster-network-range-edit_configuring-cluster-network-range"><div class="titlepage"><div><div><h2 class="title">15.1. Expanding the cluster network IP address range</h2></div></div></div><p>
				You can expand the IP address range for the cluster network. Because this change requires rolling out a new Operator configuration across the cluster, it can take up to 30 minutes to take effect.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						Log in to the cluster with a user with <code class="literal">cluster-admin</code> privileges.
					</li><li class="listitem">
						Ensure that the cluster uses the OVN-Kubernetes network plugin.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To obtain the cluster network range and host prefix for your cluster, enter the following command:
					</p><pre class="programlisting language-terminal">$ oc get network.operator.openshift.io \
  -o jsonpath="{.items[0].spec.clusterNetwork}"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-text">[{"cidr":"10.217.0.0/22","hostPrefix":23}]</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						To expand the cluster network IP address range, enter the following command. Use the CIDR IP address range and host prefix returned from the output of the previous command.
					</p><pre class="programlisting language-terminal">$ oc patch Network.config.openshift.io cluster --type='merge' --patch \
  '{
    "spec":{
      "clusterNetwork": [ {"cidr":"&lt;network&gt;/&lt;cidr&gt;","hostPrefix":&lt;prefix&gt;} ],
      "networkType": "OVNKubernetes"
    }
  }'</pre><p class="simpara">
						where:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;network&gt;</code></span></dt><dd>
									Specifies the network part of the <code class="literal">cidr</code> field that you obtained from the previous step. You cannot change this value.
								</dd><dt><span class="term"><code class="literal">&lt;cidr&gt;</code></span></dt><dd>
									Specifies the network prefix length. For example, <code class="literal">14</code>. Change this value to a smaller number than the value from the output in the previous step to expand the cluster network range.
								</dd><dt><span class="term"><code class="literal">&lt;prefix&gt;</code></span></dt><dd>
									Specifies the current host prefix for your cluster. This value must be the same value for the <code class="literal">hostPrefix</code> field that you obtained from the previous step.
								</dd></dl></div><div class="formalpara"><p class="title"><strong>Example command</strong></p><p>
							
<pre class="programlisting language-terminal">$ oc patch Network.config.openshift.io cluster --type='merge' --patch \
  '{
    "spec":{
      "clusterNetwork": [ {"cidr":"10.217.0.0/14","hostPrefix": 23} ],
      "networkType": "OVNKubernetes"
    }
  }'</pre>

						</p></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-text">network.config.openshift.io/cluster patched</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						To confirm that the configuration is active, enter the following command. It can take up to 30 minutes for this change to take effect.
					</p><pre class="programlisting language-terminal">$ oc get network.operator.openshift.io \
  -o jsonpath="{.items[0].spec.clusterNetwork}"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-text">[{"cidr":"10.217.0.0/14","hostPrefix":23}]</pre>

						</p></div></li></ol></div></section><section class="section _additional-resources" id="configuring-cluster-network-range-additional-resources"><div class="titlepage"><div><div><h2 class="title">15.2. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-ovn-kubernetes">About the OVN-Kubernetes network plugin</a>
					</li></ul></div></section></section><section class="chapter" id="configuring-ipfailover"><div class="titlepage"><div><div><h1 class="title">Chapter 16. Configuring IP failover</h1></div></div></div><p>
			This topic describes configuring IP failover for pods and services on your OpenShift Container Platform cluster.
		</p><p>
			IP failover manages a pool of Virtual IP (VIP) addresses on a set of nodes. Every VIP in the set is serviced by a node selected from the set. As long a single node is available, the VIPs are served. There is no way to explicitly distribute the VIPs over the nodes, so there can be nodes with no VIPs and other nodes with many VIPs. If there is only one node, all VIPs are on it.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				The VIPs must be routable from outside the cluster.
			</p></div></div><p>
			IP failover monitors a port on each VIP to determine whether the port is reachable on the node. If the port is not reachable, the VIP is not assigned to the node. If the port is set to <code class="literal">0</code>, this check is suppressed. The check script does the needed testing.
		</p><p>
			IP failover uses <a class="link" href="http://www.keepalived.org/">Keepalived</a> to host a set of externally accessible VIP addresses on a set of hosts. Each VIP is only serviced by a single host at a time. Keepalived uses the Virtual Router Redundancy Protocol (VRRP) to determine which host, from the set of hosts, services which VIP. If a host becomes unavailable, or if the service that Keepalived is watching does not respond, the VIP is switched to another host from the set. This means a VIP is always serviced as long as a host is available.
		</p><p>
			When a node running Keepalived passes the check script, the VIP on that node can enter the <code class="literal">master</code> state based on its priority and the priority of the current master and as determined by the preemption strategy.
		</p><p>
			A cluster administrator can provide a script through the <code class="literal">OPENSHIFT_HA_NOTIFY_SCRIPT</code> variable, and this script is called whenever the state of the VIP on the node changes. Keepalived uses the <code class="literal">master</code> state when it is servicing the VIP, the <code class="literal">backup</code> state when another node is servicing the VIP, or in the <code class="literal">fault</code> state when the check script fails. The notify script is called with the new state whenever the state changes.
		</p><p>
			You can create an IP failover deployment configuration on OpenShift Container Platform. The IP failover deployment configuration specifies the set of VIP addresses, and the set of nodes on which to service them. A cluster can have multiple IP failover deployment configurations, with each managing its own set of unique VIP addresses. Each node in the IP failover configuration runs an IP failover pod, and this pod runs Keepalived.
		</p><p>
			When using VIPs to access a pod with host networking, the application pod runs on all nodes that are running the IP failover pods. This enables any of the IP failover nodes to become the master and service the VIPs when needed. If application pods are not running on all nodes with IP failover, either some IP failover nodes never service the VIPs or some application pods never receive any traffic. Use the same selector and replication count, for both IP failover and the application pods, to avoid this mismatch.
		</p><p>
			While using VIPs to access a service, any of the nodes can be in the IP failover set of nodes, since the service is reachable on all nodes, no matter where the application pod is running. Any of the IP failover nodes can become master at any time. The service can either use external IPs and a service port or it can use a <code class="literal">NodePort</code>.
		</p><p>
			When using external IPs in the service definition, the VIPs are set to the external IPs, and the IP failover monitoring port is set to the service port. When using a node port, the port is open on every node in the cluster, and the service load-balances traffic from whatever node currently services the VIP. In this case, the IP failover monitoring port is set to the <code class="literal">NodePort</code> in the service definition.
		</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				Setting up a <code class="literal">NodePort</code> is a privileged operation.
			</p></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				Even though a service VIP is highly available, performance can still be affected. Keepalived makes sure that each of the VIPs is serviced by some node in the configuration, and several VIPs can end up on the same node even when other nodes have none. Strategies that externally load-balance across a set of VIPs can be thwarted when IP failover puts multiple VIPs on the same node.
			</p></div></div><p>
			When you use <code class="literal">ingressIP</code>, you can set up IP failover to have the same VIP range as the <code class="literal">ingressIP</code> range. You can also disable the monitoring port. In this case, all the VIPs appear on same node in the cluster. Any user can set up a service with an <code class="literal">ingressIP</code> and have it highly available.
		</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				There are a maximum of 254 VIPs in the cluster.
			</p></div></div><section class="section" id="nw-ipfailover-environment-variables_configuring-ipfailover"><div class="titlepage"><div><div><h2 class="title">16.1. IP failover environment variables</h2></div></div></div><p>
				The following table contains the variables used to configure IP failover.
			</p><div class="table" id="idm140587118984992"><p class="title"><strong>Table 16.1. IP failover environment variables</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 38%; " class="col_1"><!--Empty--></col><col style="width: 13%; " class="col_2"><!--Empty--></col><col style="width: 50%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587172335696" scope="col">Variable Name</th><th align="left" valign="top" id="idm140587172334608" scope="col">Default</th><th align="left" valign="top" id="idm140587172333520" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587172335696"> <p>
								<code class="literal">OPENSHIFT_HA_MONITOR_PORT</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587172334608"> <p>
								<code class="literal">80</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587172333520"> <p>
								The IP failover pod tries to open a TCP connection to this port on each Virtual IP (VIP). If connection is established, the service is considered to be running. If this port is set to <code class="literal">0</code>, the test always passes.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587172335696"> <p>
								<code class="literal">OPENSHIFT_HA_NETWORK_INTERFACE</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587172334608"> </td><td align="left" valign="top" headers="idm140587172333520"> <p>
								The interface name that IP failover uses to send Virtual Router Redundancy Protocol (VRRP) traffic. The default value is <code class="literal">eth0</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587172335696"> <p>
								<code class="literal">OPENSHIFT_HA_REPLICA_COUNT</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587172334608"> <p>
								<code class="literal">2</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587172333520"> <p>
								The number of replicas to create. This must match <code class="literal">spec.replicas</code> value in IP failover deployment configuration.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587172335696"> <p>
								<code class="literal">OPENSHIFT_HA_VIRTUAL_IPS</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587172334608"> </td><td align="left" valign="top" headers="idm140587172333520"> <p>
								The list of IP address ranges to replicate. This must be provided. For example, <code class="literal">1.2.3.4-6,1.2.3.9</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587172335696"> <p>
								<code class="literal">OPENSHIFT_HA_VRRP_ID_OFFSET</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587172334608"> <p>
								<code class="literal">0</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587172333520"> <p>
								The offset value used to set the virtual router IDs. Using different offset values allows multiple IP failover configurations to exist within the same cluster. The default offset is <code class="literal">0</code>, and the allowed range is <code class="literal">0</code> through <code class="literal">255</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587172335696"> <p>
								<code class="literal">OPENSHIFT_HA_VIP_GROUPS</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587172334608"> </td><td align="left" valign="top" headers="idm140587172333520"> <p>
								The number of groups to create for VRRP. If not set, a group is created for each virtual IP range specified with the <code class="literal">OPENSHIFT_HA_VIP_GROUPS</code> variable.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587172335696"> <p>
								<code class="literal">OPENSHIFT_HA_IPTABLES_CHAIN</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587172334608"> <p>
								INPUT
							</p>
							 </td><td align="left" valign="top" headers="idm140587172333520"> <p>
								The name of the iptables chain, to automatically add an <code class="literal">iptables</code> rule to allow the VRRP traffic on. If the value is not set, an <code class="literal">iptables</code> rule is not added. If the chain does not exist, it is not created.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587172335696"> <p>
								<code class="literal">OPENSHIFT_HA_CHECK_SCRIPT</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587172334608"> </td><td align="left" valign="top" headers="idm140587172333520"> <p>
								The full path name in the pod file system of a script that is periodically run to verify the application is operating.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587172335696"> <p>
								<code class="literal">OPENSHIFT_HA_CHECK_INTERVAL</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587172334608"> <p>
								<code class="literal">2</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587172333520"> <p>
								The period, in seconds, that the check script is run.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587172335696"> <p>
								<code class="literal">OPENSHIFT_HA_NOTIFY_SCRIPT</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587172334608"> </td><td align="left" valign="top" headers="idm140587172333520"> <p>
								The full path name in the pod file system of a script that is run whenever the state changes.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587172335696"> <p>
								<code class="literal">OPENSHIFT_HA_PREEMPTION</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587172334608"> <p>
								<code class="literal">preempt_nodelay 300</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587172333520"> <p>
								The strategy for handling a new higher priority host. The <code class="literal">nopreempt</code> strategy does not move master from the lower priority host to the higher priority host.
							</p>
							 </td></tr></tbody></table></div></div></section><section class="section" id="nw-ipfailover-configuration_configuring-ipfailover"><div class="titlepage"><div><div><h2 class="title">16.2. Configuring IP failover</h2></div></div></div><p>
				As a cluster administrator, you can configure IP failover on an entire cluster, or on a subset of nodes, as defined by the label selector. You can also configure multiple IP failover deployment configurations in your cluster, where each one is independent of the others.
			</p><p>
				The IP failover deployment configuration ensures that a failover pod runs on each of the nodes matching the constraints or the label used.
			</p><p>
				This pod runs Keepalived, which can monitor an endpoint and use Virtual Router Redundancy Protocol (VRRP) to fail over the virtual IP (VIP) from one node to another if the first node cannot reach the service or endpoint.
			</p><p>
				For production use, set a <code class="literal">selector</code> that selects at least two nodes, and set <code class="literal">replicas</code> equal to the number of selected nodes.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You are logged in to the cluster with a user with <code class="literal">cluster-admin</code> privileges.
					</li><li class="listitem">
						You created a pull secret.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create an IP failover service account:
					</p><pre class="programlisting language-terminal">$ oc create sa ipfailover</pre></li><li class="listitem"><p class="simpara">
						Update security context constraints (SCC) for <code class="literal">hostNetwork</code>:
					</p><pre class="programlisting language-terminal">$ oc adm policy add-scc-to-user privileged -z ipfailover
$ oc adm policy add-scc-to-user hostnetwork -z ipfailover</pre></li><li class="listitem"><p class="simpara">
						Create a deployment YAML file to configure IP failover:
					</p><div class="formalpara"><p class="title"><strong>Example deployment YAML for IP failover configuration</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: ipfailover-keepalived <span id="CO28-1"><!--Empty--></span><span class="callout">1</span>
  labels:
    ipfailover: hello-openshift
spec:
  strategy:
    type: Recreate
  replicas: 2
  selector:
    matchLabels:
      ipfailover: hello-openshift
  template:
    metadata:
      labels:
        ipfailover: hello-openshift
    spec:
      serviceAccountName: ipfailover
      privileged: true
      hostNetwork: true
      nodeSelector:
        node-role.kubernetes.io/worker: ""
      containers:
      - name: openshift-ipfailover
        image: quay.io/openshift/origin-keepalived-ipfailover
        ports:
        - containerPort: 63000
          hostPort: 63000
        imagePullPolicy: IfNotPresent
        securityContext:
          privileged: true
        volumeMounts:
        - name: lib-modules
          mountPath: /lib/modules
          readOnly: true
        - name: host-slash
          mountPath: /host
          readOnly: true
          mountPropagation: HostToContainer
        - name: etc-sysconfig
          mountPath: /etc/sysconfig
          readOnly: true
        - name: config-volume
          mountPath: /etc/keepalive
        env:
        - name: OPENSHIFT_HA_CONFIG_NAME
          value: "ipfailover"
        - name: OPENSHIFT_HA_VIRTUAL_IPS <span id="CO28-2"><!--Empty--></span><span class="callout">2</span>
          value: "1.1.1.1-2"
        - name: OPENSHIFT_HA_VIP_GROUPS <span id="CO28-3"><!--Empty--></span><span class="callout">3</span>
          value: "10"
        - name: OPENSHIFT_HA_NETWORK_INTERFACE <span id="CO28-4"><!--Empty--></span><span class="callout">4</span>
          value: "ens3" #The host interface to assign the VIPs
        - name: OPENSHIFT_HA_MONITOR_PORT <span id="CO28-5"><!--Empty--></span><span class="callout">5</span>
          value: "30060"
        - name: OPENSHIFT_HA_VRRP_ID_OFFSET <span id="CO28-6"><!--Empty--></span><span class="callout">6</span>
          value: "0"
        - name: OPENSHIFT_HA_REPLICA_COUNT <span id="CO28-7"><!--Empty--></span><span class="callout">7</span>
          value: "2" #Must match the number of replicas in the deployment
        - name: OPENSHIFT_HA_USE_UNICAST
          value: "false"
        #- name: OPENSHIFT_HA_UNICAST_PEERS
          #value: "10.0.148.40,10.0.160.234,10.0.199.110"
        - name: OPENSHIFT_HA_IPTABLES_CHAIN <span id="CO28-8"><!--Empty--></span><span class="callout">8</span>
          value: "INPUT"
        #- name: OPENSHIFT_HA_NOTIFY_SCRIPT <span id="CO28-9"><!--Empty--></span><span class="callout">9</span>
        #  value: /etc/keepalive/mynotifyscript.sh
        - name: OPENSHIFT_HA_CHECK_SCRIPT <span id="CO28-10"><!--Empty--></span><span class="callout">10</span>
          value: "/etc/keepalive/mycheckscript.sh"
        - name: OPENSHIFT_HA_PREEMPTION <span id="CO28-11"><!--Empty--></span><span class="callout">11</span>
          value: "preempt_delay 300"
        - name: OPENSHIFT_HA_CHECK_INTERVAL <span id="CO28-12"><!--Empty--></span><span class="callout">12</span>
          value: "2"
        livenessProbe:
          initialDelaySeconds: 10
          exec:
            command:
            - pgrep
            - keepalived
      volumes:
      - name: lib-modules
        hostPath:
          path: /lib/modules
      - name: host-slash
        hostPath:
          path: /
      - name: etc-sysconfig
        hostPath:
          path: /etc/sysconfig
      # config-volume contains the check script
      # created with `oc create configmap keepalived-checkscript --from-file=mycheckscript.sh`
      - configMap:
          defaultMode: 0755
          name: keepalived-checkscript
        name: config-volume
      imagePullSecrets:
        - name: openshift-pull-secret <span id="CO28-13"><!--Empty--></span><span class="callout">13</span></pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO28-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The name of the IP failover deployment.
							</div></dd><dt><a href="#CO28-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The list of IP address ranges to replicate. This must be provided. For example, <code class="literal">1.2.3.4-6,1.2.3.9</code>.
							</div></dd><dt><a href="#CO28-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								The number of groups to create for VRRP. If not set, a group is created for each virtual IP range specified with the <code class="literal">OPENSHIFT_HA_VIP_GROUPS</code> variable.
							</div></dd><dt><a href="#CO28-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								The interface name that IP failover uses to send VRRP traffic. By default, <code class="literal">eth0</code> is used.
							</div></dd><dt><a href="#CO28-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								The IP failover pod tries to open a TCP connection to this port on each VIP. If connection is established, the service is considered to be running. If this port is set to <code class="literal">0</code>, the test always passes. The default value is <code class="literal">80</code>.
							</div></dd><dt><a href="#CO28-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								The offset value used to set the virtual router IDs. Using different offset values allows multiple IP failover configurations to exist within the same cluster. The default offset is <code class="literal">0</code>, and the allowed range is <code class="literal">0</code> through <code class="literal">255</code>.
							</div></dd><dt><a href="#CO28-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								The number of replicas to create. This must match <code class="literal">spec.replicas</code> value in IP failover deployment configuration. The default value is <code class="literal">2</code>.
							</div></dd><dt><a href="#CO28-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								The name of the <code class="literal">iptables</code> chain to automatically add an <code class="literal">iptables</code> rule to allow the VRRP traffic on. If the value is not set, an <code class="literal">iptables</code> rule is not added. If the chain does not exist, it is not created, and Keepalived operates in unicast mode. The default is <code class="literal">INPUT</code>.
							</div></dd><dt><a href="#CO28-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								The full path name in the pod file system of a script that is run whenever the state changes.
							</div></dd><dt><a href="#CO28-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								The full path name in the pod file system of a script that is periodically run to verify the application is operating.
							</div></dd><dt><a href="#CO28-11"><span class="callout">11</span></a> </dt><dd><div class="para">
								The strategy for handling a new higher priority host. The default value is <code class="literal">preempt_delay 300</code>, which causes a Keepalived instance to take over a VIP after 5 minutes if a lower-priority master is holding the VIP.
							</div></dd><dt><a href="#CO28-12"><span class="callout">12</span></a> </dt><dd><div class="para">
								The period, in seconds, that the check script is run. The default value is <code class="literal">2</code>.
							</div></dd><dt><a href="#CO28-13"><span class="callout">13</span></a> </dt><dd><div class="para">
								Create the pull secret before creating the deployment, otherwise you will get an error when creating the deployment.
							</div></dd></dl></div></li></ol></div></section><section class="section" id="nw-ipfailover-virtual-ip-addresses-concept_configuring-ipfailover"><div class="titlepage"><div><div><h2 class="title">16.3. About virtual IP addresses</h2></div></div></div><p>
				Keepalived manages a set of virtual IP addresses (VIP). The administrator must make sure that all of these addresses:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Are accessible on the configured hosts from outside the cluster.
					</li><li class="listitem">
						Are not used for any other purpose within the cluster.
					</li></ul></div><p>
				Keepalived on each node determines whether the needed service is running. If it is, VIPs are supported and Keepalived participates in the negotiation to determine which node serves the VIP. For a node to participate, the service must be listening on the watch port on a VIP or the check must be disabled.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Each VIP in the set may end up being served by a different node.
				</p></div></div></section><section class="section" id="nw-ipfailover-configuring-check-notify-scripts_configuring-ipfailover"><div class="titlepage"><div><div><h2 class="title">16.4. Configuring check and notify scripts</h2></div></div></div><p>
				Keepalived monitors the health of the application by periodically running an optional user supplied check script. For example, the script can test a web server by issuing a request and verifying the response.
			</p><p>
				When a check script is not provided, a simple default script is run that tests the TCP connection. This default test is suppressed when the monitor port is <code class="literal">0</code>.
			</p><p>
				Each IP failover pod manages a Keepalived daemon that manages one or more virtual IPs (VIP) on the node where the pod is running. The Keepalived daemon keeps the state of each VIP for that node. A particular VIP on a particular node may be in <code class="literal">master</code>, <code class="literal">backup</code>, or <code class="literal">fault</code> state.
			</p><p>
				When the check script for that VIP on the node that is in <code class="literal">master</code> state fails, the VIP on that node enters the <code class="literal">fault</code> state, which triggers a renegotiation. During renegotiation, all VIPs on a node that are not in the <code class="literal">fault</code> state participate in deciding which node takes over the VIP. Ultimately, the VIP enters the <code class="literal">master</code> state on some node, and the VIP stays in the <code class="literal">backup</code> state on the other nodes.
			</p><p>
				When a node with a VIP in <code class="literal">backup</code> state fails, the VIP on that node enters the <code class="literal">fault</code> state. When the check script passes again for a VIP on a node in the <code class="literal">fault</code> state, the VIP on that node exits the <code class="literal">fault</code> state and negotiates to enter the <code class="literal">master</code> state. The VIP on that node may then enter either the <code class="literal">master</code> or the <code class="literal">backup</code> state.
			</p><p>
				As cluster administrator, you can provide an optional notify script, which is called whenever the state changes. Keepalived passes the following three parameters to the script:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">$1</code> - <code class="literal">group</code> or <code class="literal">instance</code>
					</li><li class="listitem">
						<code class="literal">$2</code> - Name of the <code class="literal">group</code> or <code class="literal">instance</code>
					</li><li class="listitem">
						<code class="literal">$3</code> - The new state: <code class="literal">master</code>, <code class="literal">backup</code>, or <code class="literal">fault</code>
					</li></ul></div><p>
				The check and notify scripts run in the IP failover pod and use the pod file system, not the host file system. However, the IP failover pod makes the host file system available under the <code class="literal">/hosts</code> mount path. When configuring a check or notify script, you must provide the full path to the script. The recommended approach for providing the scripts is to use a config map.
			</p><p>
				The full path names of the check and notify scripts are added to the Keepalived configuration file, <code class="literal">_/etc/keepalived/keepalived.conf</code>, which is loaded every time Keepalived starts. The scripts can be added to the pod with a config map as follows.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You are logged in to the cluster with a user with <code class="literal">cluster-admin</code> privileges.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create the desired script and create a config map to hold it. The script has no input arguments and must return <code class="literal">0</code> for <code class="literal">OK</code> and <code class="literal">1</code> for <code class="literal">fail</code>.
					</p><p class="simpara">
						The check script, <code class="literal"><span class="emphasis"><em>mycheckscript.sh</em></span></code>:
					</p><pre class="programlisting language-bash">#!/bin/bash
    # Whatever tests are needed
    # E.g., send request and verify response
exit 0</pre></li><li class="listitem"><p class="simpara">
						Create the config map:
					</p><pre class="programlisting language-terminal">$ oc create configmap mycustomcheck --from-file=mycheckscript.sh</pre></li><li class="listitem"><p class="simpara">
						Add the script to the pod. The <code class="literal">defaultMode</code> for the mounted config map files must able to run by using <code class="literal">oc</code> commands or by editing the deployment configuration. A value of <code class="literal">0755</code>, <code class="literal">493</code> decimal, is typical:
					</p><pre class="programlisting language-terminal">$ oc set env deploy/ipfailover-keepalived \
    OPENSHIFT_HA_CHECK_SCRIPT=/etc/keepalive/mycheckscript.sh</pre><pre class="programlisting language-terminal">$ oc set volume deploy/ipfailover-keepalived --add --overwrite \
    --name=config-volume \
    --mount-path=/etc/keepalive \
    --source='{"configMap": { "name": "mycustomcheck", "defaultMode": 493}}'</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The <code class="literal">oc set env</code> command is whitespace sensitive. There must be no whitespace on either side of the <code class="literal">=</code> sign.
						</p></div></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
						You can alternatively edit the <code class="literal">ipfailover-keepalived</code> deployment configuration:
					</p><pre class="programlisting language-terminal">$ oc edit deploy ipfailover-keepalived</pre><pre class="programlisting language-yaml">    spec:
      containers:
      - env:
        - name: OPENSHIFT_HA_CHECK_SCRIPT  <span id="CO29-1"><!--Empty--></span><span class="callout">1</span>
          value: /etc/keepalive/mycheckscript.sh
...
        volumeMounts: <span id="CO29-2"><!--Empty--></span><span class="callout">2</span>
        - mountPath: /etc/keepalive
          name: config-volume
      dnsPolicy: ClusterFirst
...
      volumes: <span id="CO29-3"><!--Empty--></span><span class="callout">3</span>
      - configMap:
          defaultMode: 0755 <span id="CO29-4"><!--Empty--></span><span class="callout">4</span>
          name: customrouter
        name: config-volume
...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO29-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								In the <code class="literal">spec.container.env</code> field, add the <code class="literal">OPENSHIFT_HA_CHECK_SCRIPT</code> environment variable to point to the mounted script file.
							</div></dd><dt><a href="#CO29-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Add the <code class="literal">spec.container.volumeMounts</code> field to create the mount point.
							</div></dd><dt><a href="#CO29-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Add a new <code class="literal">spec.volumes</code> field to mention the config map.
							</div></dd><dt><a href="#CO29-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								This sets run permission on the files. When read back, it is displayed in decimal, <code class="literal">493</code>.
							</div></dd></dl></div><p>
						Save the changes and exit the editor. This restarts <code class="literal">ipfailover-keepalived</code>.
					</p></div></div></li></ol></div></section><section class="section" id="nw-ipfailover-configuring-vrrp-preemption_configuring-ipfailover"><div class="titlepage"><div><div><h2 class="title">16.5. Configuring VRRP preemption</h2></div></div></div><p>
				When a Virtual IP (VIP) on a node leaves the <code class="literal">fault</code> state by passing the check script, the VIP on the node enters the <code class="literal">backup</code> state if it has lower priority than the VIP on the node that is currently in the <code class="literal">master</code> state. However, if the VIP on the node that is leaving <code class="literal">fault</code> state has a higher priority, the preemption strategy determines its role in the cluster.
			</p><p>
				The <code class="literal">nopreempt</code> strategy does not move <code class="literal">master</code> from the lower priority VIP on the host to the higher priority VIP on the host. With <code class="literal">preempt_delay 300</code>, the default, Keepalived waits the specified 300 seconds and moves <code class="literal">master</code> to the higher priority VIP on the host.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You installed the OpenShift CLI (<code class="literal">oc</code>).
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						To specify preemption enter <code class="literal">oc edit deploy ipfailover-keepalived</code> to edit the router deployment configuration:
					</p><pre class="programlisting language-terminal">$ oc edit deploy ipfailover-keepalived</pre><pre class="programlisting language-yaml">...
    spec:
      containers:
      - env:
        - name: OPENSHIFT_HA_PREEMPTION  <span id="CO30-1"><!--Empty--></span><span class="callout">1</span>
          value: preempt_delay 300
...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO30-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Set the <code class="literal">OPENSHIFT_HA_PREEMPTION</code> value:
							</div><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										<code class="literal">preempt_delay 300</code>: Keepalived waits the specified 300 seconds and moves <code class="literal">master</code> to the higher priority VIP on the host. This is the default value.
									</li><li class="listitem">
										<code class="literal">nopreempt</code>: does not move <code class="literal">master</code> from the lower priority VIP on the host to the higher priority VIP on the host.
									</li></ul></div></dd></dl></div></li></ul></div></section><section class="section" id="nw-ipfailover-vrrp-ip-offset_configuring-ipfailover"><div class="titlepage"><div><div><h2 class="title">16.6. About VRRP ID offset</h2></div></div></div><p>
				Each IP failover pod managed by the IP failover deployment configuration, <code class="literal">1</code> pod per node or replica, runs a Keepalived daemon. As more IP failover deployment configurations are configured, more pods are created and more daemons join into the common Virtual Router Redundancy Protocol (VRRP) negotiation. This negotiation is done by all the Keepalived daemons and it determines which nodes service which virtual IPs (VIP).
			</p><p>
				Internally, Keepalived assigns a unique <code class="literal">vrrp-id</code> to each VIP. The negotiation uses this set of <code class="literal">vrrp-ids</code>, when a decision is made, the VIP corresponding to the winning <code class="literal">vrrp-id</code> is serviced on the winning node.
			</p><p>
				Therefore, for every VIP defined in the IP failover deployment configuration, the IP failover pod must assign a corresponding <code class="literal">vrrp-id</code>. This is done by starting at <code class="literal">OPENSHIFT_HA_VRRP_ID_OFFSET</code> and sequentially assigning the <code class="literal">vrrp-ids</code> to the list of VIPs. The <code class="literal">vrrp-ids</code> can have values in the range <code class="literal">1..255</code>.
			</p><p>
				When there are multiple IP failover deployment configurations, you must specify <code class="literal">OPENSHIFT_HA_VRRP_ID_OFFSET</code> so that there is room to increase the number of VIPs in the deployment configuration and none of the <code class="literal">vrrp-id</code> ranges overlap.
			</p></section><section class="section" id="nw-ipfailover-configuring-more-than-254_configuring-ipfailover"><div class="titlepage"><div><div><h2 class="title">16.7. Configuring IP failover for more than 254 addresses</h2></div></div></div><p>
				IP failover management is limited to 254 groups of Virtual IP (VIP) addresses. By default OpenShift Container Platform assigns one IP address to each group. You can use the <code class="literal">OPENSHIFT_HA_VIP_GROUPS</code> variable to change this so multiple IP addresses are in each group and define the number of VIP groups available for each Virtual Router Redundancy Protocol (VRRP) instance when configuring IP failover.
			</p><p>
				Grouping VIPs creates a wider range of allocation of VIPs per VRRP in the case of VRRP failover events, and is useful when all hosts in the cluster have access to a service locally. For example, when a service is being exposed with an <code class="literal">ExternalIP</code>.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					As a rule for failover, do not limit services, such as the router, to one specific host. Instead, services should be replicated to each host so that in the case of IP failover, the services do not have to be recreated on the new host.
				</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					If you are using OpenShift Container Platform health checks, the nature of IP failover and groups means that all instances in the group are not checked. For that reason, <a class="link" href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">the Kubernetes health checks</a> must be used to ensure that services are live.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You are logged in to the cluster with a user with <code class="literal">cluster-admin</code> privileges.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						To change the number of IP addresses assigned to each group, change the value for the <code class="literal">OPENSHIFT_HA_VIP_GROUPS</code> variable, for example:
					</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">Deployment</code> YAML for IP failover configuration</strong></p><p>
							
<pre class="programlisting language-yaml">...
    spec:
        env:
        - name: OPENSHIFT_HA_VIP_GROUPS <span id="CO31-1"><!--Empty--></span><span class="callout">1</span>
          value: "3"
...</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO31-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								If <code class="literal">OPENSHIFT_HA_VIP_GROUPS</code> is set to <code class="literal">3</code> in an environment with seven VIPs, it creates three groups, assigning three VIPs to the first group, and two VIPs to the two remaining groups.
							</div></dd></dl></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					If the number of groups set by <code class="literal">OPENSHIFT_HA_VIP_GROUPS</code> is fewer than the number of IP addresses set to fail over, the group contains more than one IP address, and all of the addresses move as a single unit.
				</p></div></div></section><section class="section" id="nw-ipfailover-cluster-ha-ingress_configuring-ipfailover"><div class="titlepage"><div><div><h2 class="title">16.8. High availability For ingressIP</h2></div></div></div><p>
				In non-cloud clusters, IP failover and <code class="literal">ingressIP</code> to a service can be combined. The result is high availability services for users that create services using <code class="literal">ingressIP</code>.
			</p><p>
				The approach is to specify an <code class="literal">ingressIPNetworkCIDR</code> range and then use the same range in creating the ipfailover configuration.
			</p><p>
				Because IP failover can support up to a maximum of 255 VIPs for the entire cluster, the <code class="literal">ingressIPNetworkCIDR</code> needs to be <code class="literal">/24</code> or smaller.
			</p></section><section class="section" id="nw-ipfailover-remove_configuring-ipfailover"><div class="titlepage"><div><div><h2 class="title">16.9. Removing IP failover</h2></div></div></div><p>
				When IP failover is initially configured, the worker nodes in the cluster are modified with an <code class="literal">iptables</code> rule that explicitly allows multicast packets on <code class="literal">224.0.0.18</code> for Keepalived. Because of the change to the nodes, removing IP failover requires running a job to remove the <code class="literal">iptables</code> rule and removing the virtual IP addresses used by Keepalived.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Optional: Identify and delete any check and notify scripts that are stored as config maps:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Identify whether any pods for IP failover use a config map as a volume:
							</p><pre class="programlisting language-terminal">$ oc get pod -l ipfailover \
  -o jsonpath="\
{range .items[?(@.spec.volumes[*].configMap)]}
{'Namespace: '}{.metadata.namespace}
{'Pod:       '}{.metadata.name}
{'Volumes that use config maps:'}
{range .spec.volumes[?(@.configMap)]}  {'volume:    '}{.name}
  {'configMap: '}{.configMap.name}{'\n'}{end}
{end}"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="screen">Namespace: default
Pod:       keepalived-worker-59df45db9c-2x9mn
Volumes that use config maps:
  volume:    config-volume
  configMap: mycustomcheck</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								If the preceding step provided the names of config maps that are used as volumes, delete the config maps:
							</p><pre class="programlisting language-terminal">$ oc delete configmap &lt;configmap_name&gt;</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Identify an existing deployment for IP failover:
					</p><pre class="programlisting language-terminal">$ oc get deployment -l ipfailover</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAMESPACE   NAME         READY   UP-TO-DATE   AVAILABLE   AGE
default     ipfailover   2/2     2            2           105d</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Delete the deployment:
					</p><pre class="programlisting language-terminal">$ oc delete deployment &lt;ipfailover_deployment_name&gt;</pre></li><li class="listitem"><p class="simpara">
						Remove the <code class="literal">ipfailover</code> service account:
					</p><pre class="programlisting language-terminal">$ oc delete sa ipfailover</pre></li><li class="listitem"><p class="simpara">
						Run a job that removes the IP tables rule that was added when IP failover was initially configured:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create a file such as <code class="literal">remove-ipfailover-job.yaml</code> with contents that are similar to the following example:
							</p><pre class="programlisting language-yaml">apiVersion: batch/v1
kind: Job
metadata:
  generateName: remove-ipfailover-
  labels:
    app: remove-ipfailover
spec:
  template:
    metadata:
      name: remove-ipfailover
    spec:
      containers:
      - name: remove-ipfailover
        image: quay.io/openshift/origin-keepalived-ipfailover:4.13
        command: ["/var/lib/ipfailover/keepalived/remove-failover.sh"]
      nodeSelector:
        kubernetes.io/hostname: &lt;host_name&gt;  &lt;.&gt;
      restartPolicy: Never</pre><p class="simpara">
								&lt;.&gt; Run the job for each node in your cluster that was configured for IP failover and replace the hostname each time.
							</p></li><li class="listitem"><p class="simpara">
								Run the job:
							</p><pre class="programlisting language-terminal">$ oc create -f remove-ipfailover-job.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="screen">job.batch/remove-ipfailover-2h8dm created</pre>

								</p></div></li></ol></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Confirm that the job removed the initial configuration for IP failover.
					</p><pre class="programlisting language-terminal">$ oc logs job/remove-ipfailover-2h8dm</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">remove-failover.sh: OpenShift IP Failover service terminating.
  - Removing ip_vs module ...
  - Cleaning up ...
  - Releasing VIPs  (interface eth0) ...</pre>

						</p></div></li></ul></div></section></section><section class="chapter" id="nodes-setting-interface-level-network-sysctls"><div class="titlepage"><div><div><h1 class="title">Chapter 17. Configuring interface-level network sysctls</h1></div></div></div><p>
			In Linux, sysctl allows an administrator to modify kernel parameters at runtime. You can modify interface-level network sysctls using the tuning Container Network Interface (CNI) meta plugin. The tuning CNI meta plugin operates in a chain with a main CNI plugin as illustrated.
		</p><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/af56fdca7a66f114406dd89fc4fb9ca9/264_OpenShift_CNI_plugin_chain_0722.png" alt="CNI plugin"/></div></div><p>
			The main CNI plugin assigns the interface and passes this to the tuning CNI meta plugin at runtime. You can change some sysctls and several interface attributes (promiscuous mode, all-multicast mode, MTU, and MAC address) in the network namespace by using the tuning CNI meta plugin. In the tuning CNI meta plugin configuration, the interface name is represented by the <code class="literal">IFNAME</code> token, and is replaced with the actual name of the interface at runtime.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				In OpenShift Container Platform, the tuning CNI meta plugin only supports changing interface-level network sysctls.
			</p></div></div><section class="section" id="nw-configuring-tuning-cni_set-networkinterface-sysctls"><div class="titlepage"><div><div><h2 class="title">17.1. Configuring the tuning CNI</h2></div></div></div><p>
				The following procedure configures the tuning CNI to change the interface-level network <code class="literal">net.ipv4.conf.IFNAME.accept_redirects</code> sysctl. This example enables accepting and sending ICMP-redirected packets.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a network attachment definition, such as <code class="literal">tuning-example.yaml</code>, with the following content:
					</p><pre class="programlisting language-yaml">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: &lt;name&gt; <span id="CO32-1"><!--Empty--></span><span class="callout">1</span>
  namespace: default <span id="CO32-2"><!--Empty--></span><span class="callout">2</span>
spec:
  config: '{
    "cniVersion": "0.4.0", <span id="CO32-3"><!--Empty--></span><span class="callout">3</span>
    "name": "&lt;name&gt;", <span id="CO32-4"><!--Empty--></span><span class="callout">4</span>
    "plugins": [{
       "type": "&lt;main_CNI_plugin&gt;" <span id="CO32-5"><!--Empty--></span><span class="callout">5</span>
      },
      {
       "type": "tuning", <span id="CO32-6"><!--Empty--></span><span class="callout">6</span>
       "sysctl": {
            "net.ipv4.conf.IFNAME.accept_redirects": "1" <span id="CO32-7"><!--Empty--></span><span class="callout">7</span>
        }
      }
     ]
}</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO32-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specifies the name for the additional network attachment to create. The name must be unique within the specified namespace.
							</div></dd><dt><a href="#CO32-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specifies the namespace that the object is associated with.
							</div></dd><dt><a href="#CO32-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specifies the CNI specification version.
							</div></dd><dt><a href="#CO32-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Specifies the name for the configuration. It is recommended to match the configuration name to the name value of the network attachment definition.
							</div></dd><dt><a href="#CO32-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Specifies the name of the main CNI plugin to configure.
							</div></dd><dt><a href="#CO32-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								Specifies the name of the CNI meta plugin.
							</div></dd><dt><a href="#CO32-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								Specifies the sysctl to set.
							</div></dd></dl></div><p class="simpara">
						An example yaml file is shown here:
					</p><pre class="programlisting language-yaml">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: tuningnad
  namespace: default
spec:
  config: '{
    "cniVersion": "0.4.0",
    "name": "tuningnad",
    "plugins": [{
      "type": "bridge"
      },
      {
      "type": "tuning",
      "sysctl": {
         "net.ipv4.conf.IFNAME.accept_redirects": "1"
        }
    }
  ]
}'</pre></li><li class="listitem"><p class="simpara">
						Apply the yaml by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f tuning-example.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">networkattachmentdefinition.k8.cni.cncf.io/tuningnad created</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Create a pod such as <code class="literal">examplepod.yaml</code> with the network attachment definition similar to the following:
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: tunepod
  namespace: default
  annotations:
    k8s.v1.cni.cncf.io/networks: tuningnad <span id="CO33-1"><!--Empty--></span><span class="callout">1</span>
spec:
  containers:
  - name: podexample
    image: centos
    command: ["/bin/bash", "-c", "sleep INF"]
    securityContext:
      runAsUser: 2000 <span id="CO33-2"><!--Empty--></span><span class="callout">2</span>
      runAsGroup: 3000 <span id="CO33-3"><!--Empty--></span><span class="callout">3</span>
      allowPrivilegeEscalation: false <span id="CO33-4"><!--Empty--></span><span class="callout">4</span>
      capabilities: <span id="CO33-5"><!--Empty--></span><span class="callout">5</span>
        drop: ["ALL"]
  securityContext:
    runAsNonRoot: true <span id="CO33-6"><!--Empty--></span><span class="callout">6</span>
    seccompProfile: <span id="CO33-7"><!--Empty--></span><span class="callout">7</span>
      type: RuntimeDefault</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO33-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specify the name of the configured <code class="literal">NetworkAttachmentDefinition</code>.
							</div></dd><dt><a href="#CO33-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								<code class="literal">runAsUser</code> controls which user ID the container is run with.
							</div></dd><dt><a href="#CO33-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								<code class="literal">runAsGroup</code> controls which primary group ID the containers is run with.
							</div></dd><dt><a href="#CO33-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								<code class="literal">allowPrivilegeEscalation</code> determines if a pod can request to allow privilege escalation. If unspecified, it defaults to true. This boolean directly controls whether the <code class="literal">no_new_privs</code> flag gets set on the container process.
							</div></dd><dt><a href="#CO33-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								<code class="literal">capabilities</code> permit privileged actions without giving full root access. This policy ensures all capabilities are dropped from the pod.
							</div></dd><dt><a href="#CO33-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								<code class="literal">runAsNonRoot: true</code> requires that the container will run with a user with any UID other than 0.
							</div></dd><dt><a href="#CO33-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								<code class="literal">RuntimeDefault</code> enables the default seccomp profile for a pod or container workload.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Apply the yaml by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f examplepod.yaml</pre></li><li class="listitem"><p class="simpara">
						Verify that the pod is created by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get pod</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME      READY   STATUS    RESTARTS   AGE
tunepod   1/1     Running   0          47s</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Log in to the pod by running the following command:
					</p><pre class="programlisting language-terminal">$ oc rsh tunepod</pre></li><li class="listitem"><p class="simpara">
						Verify the values of the configured sysctl flags. For example, find the value <code class="literal">net.ipv4.conf.net1.accept_redirects</code> by running the following command:
					</p><pre class="programlisting language-terminal">sh-4.4# sysctl net.ipv4.conf.net1.accept_redirects</pre><div class="formalpara"><p class="title"><strong>Expected output</strong></p><p>
							
<pre class="programlisting language-terminal">net.ipv4.conf.net1.accept_redirects = 1</pre>

						</p></div></li></ol></div></section><section class="section _additional-resources" id="additional-resources_nodes-setting-interface-level-network-sysctls"><div class="titlepage"><div><div><h2 class="title">17.2. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-containers-sysctls">Using sysctls in containers</a>
					</li></ul></div></section></section><section class="chapter" id="using-sctp"><div class="titlepage"><div><div><h1 class="title">Chapter 18. Using the Stream Control Transmission Protocol (SCTP) on a bare metal cluster</h1></div></div></div><p>
			As a cluster administrator, you can use the Stream Control Transmission Protocol (SCTP) on a cluster.
		</p><section class="section" id="nw-sctp-about_using-sctp"><div class="titlepage"><div><div><h2 class="title">18.1. Support for Stream Control Transmission Protocol (SCTP) on OpenShift Container Platform</h2></div></div></div><p>
				As a cluster administrator, you can enable SCTP on the hosts in the cluster. On Red Hat Enterprise Linux CoreOS (RHCOS), the SCTP module is disabled by default.
			</p><p>
				SCTP is a reliable message based protocol that runs on top of an IP network.
			</p><p>
				When enabled, you can use SCTP as a protocol with pods, services, and network policy. A <code class="literal">Service</code> object must be defined with the <code class="literal">type</code> parameter set to either the <code class="literal">ClusterIP</code> or <code class="literal">NodePort</code> value.
			</p><section class="section" id="example_configurations_using-sctp"><div class="titlepage"><div><div><h3 class="title">18.1.1. Example configurations using SCTP protocol</h3></div></div></div><p>
					You can configure a pod or service to use SCTP by setting the <code class="literal">protocol</code> parameter to the <code class="literal">SCTP</code> value in the pod or service object.
				</p><p>
					In the following example, a pod is configured to use SCTP:
				</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  namespace: project1
  name: example-pod
spec:
  containers:
    - name: example-pod
...
      ports:
        - containerPort: 30100
          name: sctpserver
          protocol: SCTP</pre><p>
					In the following example, a service is configured to use SCTP:
				</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Service
metadata:
  namespace: project1
  name: sctpserver
spec:
...
  ports:
    - name: sctpserver
      protocol: SCTP
      port: 30100
      targetPort: 30100
  type: ClusterIP</pre><p>
					In the following example, a <code class="literal">NetworkPolicy</code> object is configured to apply to SCTP network traffic on port <code class="literal">80</code> from any pods with a specific label:
				</p><pre class="programlisting language-yaml">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-sctp-on-http
spec:
  podSelector:
    matchLabels:
      role: web
  ingress:
  - ports:
    - protocol: SCTP
      port: 80</pre></section></section><section class="section" id="nw-sctp-enabling_using-sctp"><div class="titlepage"><div><div><h2 class="title">18.2. Enabling Stream Control Transmission Protocol (SCTP)</h2></div></div></div><p>
				As a cluster administrator, you can load and enable the blacklisted SCTP kernel module on worker nodes in your cluster.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a file named <code class="literal">load-sctp-module.yaml</code> that contains the following YAML definition:
					</p><pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  name: load-sctp-module
  labels:
    machineconfiguration.openshift.io/role: worker
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - path: /etc/modprobe.d/sctp-blacklist.conf
          mode: 0644
          overwrite: true
          contents:
            source: data:,
        - path: /etc/modules-load.d/sctp-load.conf
          mode: 0644
          overwrite: true
          contents:
            source: data:,sctp</pre></li><li class="listitem"><p class="simpara">
						To create the <code class="literal">MachineConfig</code> object, enter the following command:
					</p><pre class="programlisting language-terminal">$ oc create -f load-sctp-module.yaml</pre></li><li class="listitem"><p class="simpara">
						Optional: To watch the status of the nodes while the MachineConfig Operator applies the configuration change, enter the following command. When the status of a node transitions to <code class="literal">Ready</code>, the configuration update is applied.
					</p><pre class="programlisting language-terminal">$ oc get nodes</pre></li></ol></div></section><section class="section" id="nw-sctp-verifying_using-sctp"><div class="titlepage"><div><div><h2 class="title">18.3. Verifying Stream Control Transmission Protocol (SCTP) is enabled</h2></div></div></div><p>
				You can verify that SCTP is working on a cluster by creating a pod with an application that listens for SCTP traffic, associating it with a service, and then connecting to the exposed service.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Access to the internet from the cluster to install the <code class="literal">nc</code> package.
					</li><li class="listitem">
						Install the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						Access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a pod starts an SCTP listener:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create a file named <code class="literal">sctp-server.yaml</code> that defines a pod with the following YAML:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: sctpserver
  labels:
    app: sctpserver
spec:
  containers:
    - name: sctpserver
      image: registry.access.redhat.com/ubi9/ubi
      command: ["/bin/sh", "-c"]
      args:
        ["dnf install -y nc &amp;&amp; sleep inf"]
      ports:
        - containerPort: 30102
          name: sctpserver
          protocol: SCTP</pre></li><li class="listitem"><p class="simpara">
								Create the pod by entering the following command:
							</p><pre class="programlisting language-terminal">$ oc create -f sctp-server.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Create a service for the SCTP listener pod.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create a file named <code class="literal">sctp-service.yaml</code> that defines a service with the following YAML:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Service
metadata:
  name: sctpservice
  labels:
    app: sctpserver
spec:
  type: NodePort
  selector:
    app: sctpserver
  ports:
    - name: sctpserver
      protocol: SCTP
      port: 30102
      targetPort: 30102</pre></li><li class="listitem"><p class="simpara">
								To create the service, enter the following command:
							</p><pre class="programlisting language-terminal">$ oc create -f sctp-service.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Create a pod for the SCTP client.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create a file named <code class="literal">sctp-client.yaml</code> with the following YAML:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: sctpclient
  labels:
    app: sctpclient
spec:
  containers:
    - name: sctpclient
      image: registry.access.redhat.com/ubi9/ubi
      command: ["/bin/sh", "-c"]
      args:
        ["dnf install -y nc &amp;&amp; sleep inf"]</pre></li><li class="listitem"><p class="simpara">
								To create the <code class="literal">Pod</code> object, enter the following command:
							</p><pre class="programlisting language-terminal">$ oc apply -f sctp-client.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Run an SCTP listener on the server.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								To connect to the server pod, enter the following command:
							</p><pre class="programlisting language-terminal">$ oc rsh sctpserver</pre></li><li class="listitem"><p class="simpara">
								To start the SCTP listener, enter the following command:
							</p><pre class="programlisting language-terminal">$ nc -l 30102 --sctp</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Connect to the SCTP listener on the server.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Open a new terminal window or tab in your terminal program.
							</li><li class="listitem"><p class="simpara">
								Obtain the IP address of the <code class="literal">sctpservice</code> service. Enter the following command:
							</p><pre class="programlisting language-terminal">$ oc get services sctpservice -o go-template='{{.spec.clusterIP}}{{"\n"}}'</pre></li><li class="listitem"><p class="simpara">
								To connect to the client pod, enter the following command:
							</p><pre class="programlisting language-terminal">$ oc rsh sctpclient</pre></li><li class="listitem"><p class="simpara">
								To start the SCTP client, enter the following command. Replace <code class="literal">&lt;cluster_IP&gt;</code> with the cluster IP address of the <code class="literal">sctpservice</code> service.
							</p><pre class="programlisting language-terminal"># nc &lt;cluster_IP&gt; 30102 --sctp</pre></li></ol></div></li></ol></div></section></section><section class="chapter" id="using-ptp"><div class="titlepage"><div><div><h1 class="title">Chapter 19. Using PTP hardware</h1></div></div></div><p>
			You can configure <code class="literal">linuxptp</code> services and use PTP-capable hardware in OpenShift Container Platform cluster nodes.
		</p><section class="section" id="about-using-ptp-hardware"><div class="titlepage"><div><div><h2 class="title">19.1. About PTP hardware</h2></div></div></div><p>
				You can use the OpenShift Container Platform console or OpenShift CLI (<code class="literal">oc</code>) to install PTP by deploying the PTP Operator. The PTP Operator creates and manages the <code class="literal">linuxptp</code> services and provides the following features:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Discovery of the PTP-capable devices in the cluster.
					</li><li class="listitem">
						Management of the configuration of <code class="literal">linuxptp</code> services.
					</li><li class="listitem">
						Notification of PTP clock events that negatively affect the performance and reliability of your application with the PTP Operator <code class="literal">cloud-event-proxy</code> sidecar.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The PTP Operator works with PTP-capable devices on clusters provisioned only on bare-metal infrastructure.
				</p></div></div></section><section class="section" id="ptp-introduction_using-ptp"><div class="titlepage"><div><div><h2 class="title">19.2. About PTP</h2></div></div></div><p>
				Precision Time Protocol (PTP) is used to synchronize clocks in a network. When used in conjunction with hardware support, PTP is capable of sub-microsecond accuracy, and is more accurate than Network Time Protocol (NTP).
			</p><section class="section" id="ptp-elements_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.2.1. Elements of a PTP domain</h3></div></div></div><p>
					PTP is used to synchronize multiple nodes connected in a network, with clocks for each node. The clocks synchronized by PTP are organized in a source-destination hierarchy. The hierarchy is created and updated automatically by the best master clock (BMC) algorithm, which runs on every clock. Destination clocks are synchronized to source clocks, and destination clocks can themselves be the source for other downstream clocks. The three primary types of PTP clocks are described below.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Grandmaster clock</span></dt><dd>
								The grandmaster clock provides standard time information to other clocks across the network and ensures accurate and stable synchronisation. It writes time stamps and responds to time requests from other clocks. Grandmaster clocks synchronize to a Global Navigation Satellite System (GNSS) time source. The Grandmaster clock is the authoritative source of time in the network and is responsible for providing time synchronization to all other devices.
							</dd><dt><span class="term">Ordinary clock</span></dt><dd>
								The ordinary clock has a single port connection that can play the role of source or destination clock, depending on its position in the network. The ordinary clock can read and write time stamps.
							</dd><dt><span class="term">Boundary clock</span></dt><dd>
								The boundary clock has ports in two or more communication paths and can be a source and a destination to other destination clocks at the same time. The boundary clock works as a destination clock upstream. The destination clock receives the timing message, adjusts for delay, and then creates a new source time signal to pass down the network. The boundary clock produces a new timing packet that is still correctly synced with the source clock and can reduce the number of connected devices reporting directly to the source clock.
							</dd></dl></div></section><section class="section" id="ptp-advantages-over-ntp_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.2.2. Advantages of PTP over NTP</h3></div></div></div><p>
					One of the main advantages that PTP has over NTP is the hardware support present in various network interface controllers (NIC) and network switches. The specialized hardware allows PTP to account for delays in message transfer and improves the accuracy of time synchronization. To achieve the best possible accuracy, it is recommended that all networking components between PTP clocks are PTP hardware enabled.
				</p><p>
					Hardware-based PTP provides optimal accuracy, since the NIC can time stamp the PTP packets at the exact moment they are sent and received. Compare this to software-based PTP, which requires additional processing of the PTP packets by the operating system.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Before enabling PTP, ensure that NTP is disabled for the required nodes. You can disable the chrony time service (<code class="literal">chronyd</code>) using a <code class="literal">MachineConfig</code> custom resource. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/post-installation_configuration/#cnf-disable-chronyd_post-install-machine-configuration-tasks">Disabling chrony time service</a>.
					</p></div></div></section><section class="section" id="ptp-dual-nics_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.2.3. Using PTP with dual NIC hardware</h3></div></div></div><p>
					OpenShift Container Platform supports single and dual NIC hardware for precision PTP timing in the cluster.
				</p><p>
					For 5G telco networks that deliver mid-band spectrum coverage, each virtual distributed unit (vDU) requires connections to 6 radio units (RUs). To make these connections, each vDU host requires 2 NICs configured as boundary clocks.
				</p><p>
					Dual NIC hardware allows you to connect each NIC to the same upstream leader clock with separate <code class="literal">ptp4l</code> instances for each NIC feeding the downstream clocks.
				</p></section></section><section class="section" id="ptp-linuxptp-introduction_using-ptp"><div class="titlepage"><div><div><h2 class="title">19.3. Overview of linuxptp in OpenShift Container Platform nodes</h2></div></div></div><p>
				OpenShift Container Platform uses PTP and <code class="literal">linuxptp</code> for high precision system timing in bare-metal infrastructure. The <code class="literal">linuxptp</code> package includes the <code class="literal">ts2phc</code>, <code class="literal">pmc</code>, <code class="literal">ptp4l</code>, and <code class="literal">phc2sys</code> programs for system clock synchronization.
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">ts2phc</span></dt><dd><p class="simpara">
							<code class="literal">ts2phc</code> synchronizes the PTP hardware clock (PHC) across PTP devices with a high degree of precision. <code class="literal">ts2phc</code> is used in grandmaster clock configurations. It receives the precision timing signal a high precision clock source such as Global Navigation Satellite System (GNSS). GNSS provides an accurate and reliable source of synchronized time for use in large distributed networks. GNSS clocks typically provide time information with a precision of a few nanoseconds.
						</p><p class="simpara">
							The <code class="literal">ts2phc</code> system daemon sends timing information from the grandmaster clock to other PTP devices in the network by reading time information from the grandmaster clock and converting it to PHC format. PHC time is used by other devices in the network to synchronize their clocks with the grandmaster clock.
						</p></dd><dt><span class="term">pmc</span></dt><dd>
							<code class="literal">pmc</code> implements a PTP management client (<code class="literal">pmc</code>) according to IEEE standard 1588.1588. <code class="literal">pmc</code> provides basic management access for the <code class="literal">ptp4l</code> system daemon. <code class="literal">pmc</code> reads from standard input and sends the output over the selected transport, printing any replies it receives.
						</dd><dt><span class="term">ptp4l</span></dt><dd><p class="simpara">
							<code class="literal">ptp4l</code> implements the PTP boundary clock and ordinary clock and runs as a system daemon. <code class="literal">ptp4l</code> does the following:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Synchronizes the PHC to the source clock with hardware time stamping
								</li><li class="listitem">
									Synchronizes the system clock to the source clock with software time stamping
								</li></ul></div></dd><dt><span class="term">phc2sys</span></dt><dd>
							<code class="literal">phc2sys</code> synchronizes the system clock to the PHC on the network interface controller (NIC). The <code class="literal">phc2sys</code> system daemon continuously monitors the PHC for timing information. When it detects a timing error, the PHC corrects the system clock.
						</dd></dl></div></section><section class="section" id="install-ptp-operator-cli_using-ptp"><div class="titlepage"><div><div><h2 class="title">19.4. Installing the PTP Operator using the CLI</h2></div></div></div><p>
				As a cluster administrator, you can install the Operator by using the CLI.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						A cluster installed on bare-metal hardware with nodes that have hardware that supports PTP.
					</li><li class="listitem">
						Install the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						Log in as a user with <code class="literal">cluster-admin</code> privileges.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a namespace for the PTP Operator.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Save the following YAML in the <code class="literal">ptp-namespace.yaml</code> file:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: openshift-ptp
  annotations:
    workload.openshift.io/allowed: management
  labels:
    name: openshift-ptp
    openshift.io/cluster-monitoring: "true"</pre></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">Namespace</code> CR:
							</p><pre class="programlisting language-terminal">$ oc create -f ptp-namespace.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Create an Operator group for the PTP Operator.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Save the following YAML in the <code class="literal">ptp-operatorgroup.yaml</code> file:
							</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: ptp-operators
  namespace: openshift-ptp
spec:
  targetNamespaces:
  - openshift-ptp</pre></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">OperatorGroup</code> CR:
							</p><pre class="programlisting language-terminal">$ oc create -f ptp-operatorgroup.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Subscribe to the PTP Operator.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Save the following YAML in the <code class="literal">ptp-sub.yaml</code> file:
							</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: ptp-operator-subscription
  namespace: openshift-ptp
spec:
  channel: "stable"
  name: ptp-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace</pre></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">Subscription</code> CR:
							</p><pre class="programlisting language-terminal">$ oc create -f ptp-sub.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						To verify that the Operator is installed, enter the following command:
					</p><pre class="programlisting language-terminal">$ oc get csv -n openshift-ptp -o custom-columns=Name:.metadata.name,Phase:.status.phase</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">Name                         Phase
4.13.0-202301261535          Succeeded</pre>

						</p></div></li></ol></div></section><section class="section" id="install-ptp-operator-web-console_using-ptp"><div class="titlepage"><div><div><h2 class="title">19.5. Installing the PTP Operator using the web console</h2></div></div></div><p>
				As a cluster administrator, you can install the PTP Operator using the web console.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					You have to create the namespace and Operator group as mentioned in the previous section.
				</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Install the PTP Operator using the OpenShift Container Platform web console:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								In the OpenShift Container Platform web console, click <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span>.
							</li><li class="listitem">
								Choose <span class="strong strong"><strong>PTP Operator</strong></span> from the list of available Operators, and then click <span class="strong strong"><strong>Install</strong></span>.
							</li><li class="listitem">
								On the <span class="strong strong"><strong>Install Operator</strong></span> page, under <span class="strong strong"><strong>A specific namespace on the cluster</strong></span> select <span class="strong strong"><strong>openshift-ptp</strong></span>. Then, click <span class="strong strong"><strong>Install</strong></span>.
							</li></ol></div></li><li class="listitem"><p class="simpara">
						Optional: Verify that the PTP Operator installed successfully:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Switch to the <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span> page.
							</li><li class="listitem"><p class="simpara">
								Ensure that <span class="strong strong"><strong>PTP Operator</strong></span> is listed in the <span class="strong strong"><strong>openshift-ptp</strong></span> project with a <span class="strong strong"><strong>Status</strong></span> of <span class="strong strong"><strong>InstallSucceeded</strong></span>.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									During installation an Operator might display a <span class="strong strong"><strong>Failed</strong></span> status. If the installation later succeeds with an <span class="strong strong"><strong>InstallSucceeded</strong></span> message, you can ignore the <span class="strong strong"><strong>Failed</strong></span> message.
								</p></div></div><p class="simpara">
								If the Operator does not appear as installed, to troubleshoot further:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Go to the <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span> page and inspect the <span class="strong strong"><strong>Operator Subscriptions</strong></span> and <span class="strong strong"><strong>Install Plans</strong></span> tabs for any failure or errors under <span class="strong strong"><strong>Status</strong></span>.
									</li><li class="listitem">
										Go to the <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Pods</strong></span> page and check the logs for pods in the <code class="literal">openshift-ptp</code> project.
									</li></ul></div></li></ol></div></li></ol></div></section><section class="section" id="configuring-ptp-devices"><div class="titlepage"><div><div><h2 class="title">19.6. Configuring PTP devices</h2></div></div></div><p>
				The PTP Operator adds the <code class="literal">NodePtpDevice.ptp.openshift.io</code> custom resource definition (CRD) to OpenShift Container Platform.
			</p><p>
				When installed, the PTP Operator searches your cluster for PTP-capable network devices on each node. It creates and updates a <code class="literal">NodePtpDevice</code> custom resource (CR) object for each node that provides a compatible PTP-capable network device.
			</p><section class="section" id="discover-ptp-devices_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.6.1. Discovering PTP capable network devices in your cluster</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To return a complete list of PTP capable network devices in your cluster, run the following command:
						</p><pre class="programlisting language-terminal">$ oc get NodePtpDevice -n openshift-ptp -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">apiVersion: v1
items:
- apiVersion: ptp.openshift.io/v1
  kind: NodePtpDevice
  metadata:
    creationTimestamp: "2022-01-27T15:16:28Z"
    generation: 1
    name: dev-worker-0 <span id="CO34-1"><!--Empty--></span><span class="callout">1</span>
    namespace: openshift-ptp
    resourceVersion: "6538103"
    uid: d42fc9ad-bcbf-4590-b6d8-b676c642781a
  spec: {}
  status:
    devices: <span id="CO34-2"><!--Empty--></span><span class="callout">2</span>
    - name: eno1
    - name: eno2
    - name: eno3
    - name: eno4
    - name: enp5s0f0
    - name: enp5s0f1
...</pre>

							</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO34-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The value for the <code class="literal">name</code> parameter is the same as the name of the parent node.
								</div></dd><dt><a href="#CO34-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The <code class="literal">devices</code> collection includes a list of the PTP capable devices that the PTP Operator discovers for the node.
								</div></dd></dl></div></li></ul></div></section><section class="section" id="configuring-linuxptp-services-as-grandmaster-clock_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.6.2. Configuring linuxptp services as a grandmaster clock</h3></div></div></div><p>
					You can configure the <code class="literal">linuxptp</code> services (<code class="literal">ptp4l</code>, <code class="literal">phc2sys</code>, <code class="literal">ts2phc</code>) as grandmaster clock by creating a <code class="literal">PtpConfig</code> custom resource (CR) that configures the host NIC.
				</p><p>
					The <code class="literal">ts2phc</code> utility allows you to synchronize the system clock with the PTP grandmaster clock so that the node can stream precision clock signal to downstream PTP ordinary clocks and boundary clocks.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Use the following example <code class="literal">PtpConfig</code> CR as the basis to configure <code class="literal">linuxptp</code> services as the grandmaster clock for your particular hardware and environment. This example CR does not configure PTP fast events. To configure PTP fast events, set appropriate values for <code class="literal">ptp4lOpts</code>, <code class="literal">ptp4lConf</code>, and <code class="literal">ptpClockThreshold</code>. <code class="literal">ptpClockThreshold</code> is used only when events are enabled. See "Configuring the PTP fast event notifications publisher" for more information.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install an Intel Westport Channel network interface in the bare-metal cluster host.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							Install the PTP Operator.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the <code class="literal">PtpConfig</code> resource. For example:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Save the following YAML in the <code class="literal">grandmaster-clock-ptp-config.yaml</code> file:
								</p><div class="formalpara"><p class="title"><strong>Example PTP grandmaster clock configuration</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: ptp.openshift.io/v1
kind: PtpConfig
metadata:
  name: grandmaster-clock
  namespace: openshift-ptp
  annotations: {}
spec:
  profile:
    - name: grandmaster-clock
      # The interface name is hardware-specific
      interface: $interface
      ptp4lOpts: "-2"
      phc2sysOpts: "-a -r -r -n 24"
      ptpSchedulingPolicy: SCHED_FIFO
      ptpSchedulingPriority: 10
      ptpSettings:
        logReduce: "true"
      ptp4lConf: |
        [global]
        #
        # Default Data Set
        #
        twoStepFlag 1
        slaveOnly 0
        priority1 128
        priority2 128
        domainNumber 24
        #utc_offset 37
        clockClass 255
        clockAccuracy 0xFE
        offsetScaledLogVariance 0xFFFF
        free_running 0
        freq_est_interval 1
        dscp_event 0
        dscp_general 0
        dataset_comparison G.8275.x
        G.8275.defaultDS.localPriority 128
        #
        # Port Data Set
        #
        logAnnounceInterval -3
        logSyncInterval -4
        logMinDelayReqInterval -4
        logMinPdelayReqInterval -4
        announceReceiptTimeout 3
        syncReceiptTimeout 0
        delayAsymmetry 0
        fault_reset_interval -4
        neighborPropDelayThresh 20000000
        masterOnly 0
        G.8275.portDS.localPriority 128
        #
        # Run time options
        #
        assume_two_step 0
        logging_level 6
        path_trace_enabled 0
        follow_up_info 0
        hybrid_e2e 0
        inhibit_multicast_service 0
        net_sync_monitor 0
        tc_spanning_tree 0
        tx_timestamp_timeout 50
        unicast_listen 0
        unicast_master_table 0
        unicast_req_duration 3600
        use_syslog 1
        verbose 0
        summary_interval 0
        kernel_leap 1
        check_fup_sync 0
        clock_class_threshold 7
        #
        # Servo Options
        #
        pi_proportional_const 0.0
        pi_integral_const 0.0
        pi_proportional_scale 0.0
        pi_proportional_exponent -0.3
        pi_proportional_norm_max 0.7
        pi_integral_scale 0.0
        pi_integral_exponent 0.4
        pi_integral_norm_max 0.3
        step_threshold 2.0
        first_step_threshold 0.00002
        max_frequency 900000000
        clock_servo pi
        sanity_freq_limit 200000000
        ntpshm_segment 0
        #
        # Transport options
        #
        transportSpecific 0x0
        ptp_dst_mac 01:1B:19:00:00:00
        p2p_dst_mac 01:80:C2:00:00:0E
        udp_ttl 1
        udp6_scope 0x0E
        uds_address /var/run/ptp4l
        #
        # Default interface options
        #
        clock_type OC
        network_transport L2
        delay_mechanism E2E
        time_stamping hardware
        tsproc_mode filter
        delay_filter moving_median
        delay_filter_length 10
        egressLatency 0
        ingressLatency 0
        boundary_clock_jbod 0
        #
        # Clock description
        #
        productDescription ;;
        revisionData ;;
        manufacturerIdentity 00:00:00
        userDescription ;
        timeSource 0xA0
  recommend:
    - profile: grandmaster-clock
      priority: 4
      match:
        - nodeLabel: "node-role.kubernetes.io/$mcp"</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Create the CR by running the following command:
								</p><pre class="programlisting language-terminal">$ oc create -f grandmaster-clock-ptp-config.yaml</pre></li></ol></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check that the <code class="literal">PtpConfig</code> profile is applied to the node.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Get the list of pods in the <code class="literal">openshift-ptp</code> namespace by running the following command:
								</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-ptp -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                          READY   STATUS    RESTARTS   AGE     IP             NODE
linuxptp-daemon-74m2g         3/3     Running   3          4d15h   10.16.230.7    compute-1.example.com
ptp-operator-5f4f48d7c-x7zkf  1/1     Running   1          4d15h   10.128.1.145   compute-1.example.com</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Check that the profile is correct. Examine the logs of the <code class="literal">linuxptp</code> daemon that corresponds to the node you specified in the <code class="literal">PtpConfig</code> profile. Run the following command:
								</p><pre class="programlisting language-terminal">$ oc logs linuxptp-daemon-74m2g -n openshift-ptp -c linuxptp-daemon-container</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">ts2phc[94980.334]: [ts2phc.0.config] nmea delay: 98690975 ns
ts2phc[94980.334]: [ts2phc.0.config] ens3f0 extts index 0 at 1676577329.999999999 corr 0 src 1676577330.901342528 diff -1
ts2phc[94980.334]: [ts2phc.0.config] ens3f0 master offset         -1 s2 freq      -1
ts2phc[94980.441]: [ts2phc.0.config] nmea sentence: GNRMC,195453.00,A,4233.24427,N,07126.64420,W,0.008,,160223,,,A,V
phc2sys[94980.450]: [ptp4l.0.config] CLOCK_REALTIME phc offset       943 s2 freq  -89604 delay    504
phc2sys[94980.512]: [ptp4l.0.config] CLOCK_REALTIME phc offset      1000 s2 freq  -89264 delay    474</pre>

									</p></div></li></ol></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#cnf-configuring-the-ptp-fast-event-publisher_using-ptp">Configuring the PTP fast event notifications publisher</a>
						</li></ul></div><section class="section" id="nw-ptp-grandmaster-clock-configuration-reference_using-ptp"><div class="titlepage"><div><div><h4 class="title">19.6.2.1. Grandmaster clock PtpConfig configuration reference</h4></div></div></div><p>
						The following reference information describes the configuration options for the <code class="literal">PtpConfig</code> custom resource (CR) that configures the <code class="literal">linuxptp</code> services (<code class="literal">ptp4l</code>, <code class="literal">phc2sys</code>, <code class="literal">ts2phc</code>) as grandmaster clock.
					</p><div class="table" id="idm140587129839488"><p class="title"><strong>Table 19.1. PtpConfig configuration options for PTP Grandmaster clock</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 75%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587175682304" scope="col">PtpConfig CR field</th><th align="left" valign="top" id="idm140587175681216" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587175682304"> <p>
										<code class="literal">plugins</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587175681216"> <p>
										Specify an array of <code class="literal">.exec.cmdline</code> options that configure the NIC for grandmaster clock operation. Grandmaster clock configuration requires certain PTP pins to be disabled.
									</p>
									 <p>
										The plugin mechanism allows the PTP Operator to do automated hardware configuration. For the Intel Westport Channel NIC, when <code class="literal">enableDefaultConfig</code> is true, The PTP Operator runs a hard-coded script to do the required configuration for the NIC.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587175682304"> <p>
										<code class="literal">ptp4lOpts</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587175681216"> <p>
										Specify system configuration options for the <code class="literal">ptp4l</code> service. The options should not include the network interface name <code class="literal">-i &lt;interface&gt;</code> and service config file <code class="literal">-f /etc/ptp4l.conf</code> because the network interface name and the service config file are automatically appended.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587175682304"> <p>
										<code class="literal">ptp4lConf</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587175681216"> <p>
										Specify the required configuration to start <code class="literal">ptp4l</code> as grandmaster clock. For example, the <code class="literal">ens2f1</code> interface synchronizes downstream connected devices. For grandmaster clocks, set <code class="literal">clockClass</code> to <code class="literal">6</code> and set <code class="literal">clockAccuracy</code> to <code class="literal">0x27</code>. Set <code class="literal">timeSource</code> to <code class="literal">0x20</code> for when receiving the timing signal from a Global navigation satellite system (GNSS).
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587175682304"> <p>
										<code class="literal">tx_timestamp_timeout</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587175681216"> <p>
										Specify the maximum amount of time to wait for the transmit (TX) timestamp from the sender before discarding the data.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587175682304"> <p>
										<code class="literal">boundary_clock_jbod</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587175681216"> <p>
										Specify the JBOD boundary clock time delay value. This value is used to correct the time values that are passed between the network time devices.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587175682304"> <p>
										<code class="literal">phc2sysOpts</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587175681216"> <p>
										Specify system config options for the <code class="literal">phc2sys</code> service. If this field is empty the PTP Operator does not start the <code class="literal">phc2sys</code> service.
									</p>
									 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
											Ensure that the network interface listed here is configured as grandmaster and is referenced as required in the <code class="literal">ts2phcConf</code> and <code class="literal">ptp4lConf</code> fields.
										</p></div></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587175682304"> <p>
										<code class="literal">ptpSchedulingPolicy</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587175681216"> <p>
										Configure the scheduling policy for <code class="literal">ptp4l</code> and <code class="literal">phc2sys</code> processes. Default value is <code class="literal">SCHED_OTHER</code>. Use <code class="literal">SCHED_FIFO</code> on systems that support FIFO scheduling.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587175682304"> <p>
										<code class="literal">ptpSchedulingPriority</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587175681216"> <p>
										Set an integer value from 1-65 to configure FIFO priority for <code class="literal">ptp4l</code> and <code class="literal">phc2sys</code> processes when <code class="literal">ptpSchedulingPolicy</code> is set to <code class="literal">SCHED_FIFO</code>. The <code class="literal">ptpSchedulingPriority</code> field is not used when <code class="literal">ptpSchedulingPolicy</code> is set to <code class="literal">SCHED_OTHER</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587175682304"> <p>
										<code class="literal">ptpClockThreshold</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587175681216"> <p>
										Optional. If <code class="literal">ptpClockThreshold</code> stanza is not present, default values are used for <code class="literal">ptpClockThreshold</code> fields. Stanza shows default <code class="literal">ptpClockThreshold</code> values. <code class="literal">ptpClockThreshold</code> values configure how long after the PTP master clock is disconnected before PTP events are triggered. <code class="literal">holdOverTimeout</code> is the time value in seconds before the PTP clock event state changes to <code class="literal">FREERUN</code> when the PTP master clock is disconnected. The <code class="literal">maxOffsetThreshold</code> and <code class="literal">minOffsetThreshold</code> settings configure offset values in nanoseconds that compare against the values for <code class="literal">CLOCK_REALTIME</code> (<code class="literal">phc2sys</code>) or master offset (<code class="literal">ptp4l</code>). When the <code class="literal">ptp4l</code> or <code class="literal">phc2sys</code> offset value is outside this range, the PTP clock state is set to <code class="literal">FREERUN</code>. When the offset value is within this range, the PTP clock state is set to <code class="literal">LOCKED</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587175682304"> <p>
										<code class="literal">ts2phcConf</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587175681216"> <p>
										Sets the configuration for the <code class="literal">ts2phc</code> command.
									</p>
									 <p>
										<code class="literal">leapfile</code> is the default path to the current leap seconds definition file in the PTP Operator container image.
									</p>
									 <p>
										<code class="literal">ts2phc.nmea_serialport</code> is the serial port device that is connected to the NMEA GPS clock source. When configured, the GNSS receiver is accessible on <code class="literal">/dev/gnss&lt;id&gt;</code>. If the host has multiple GNSS receivers, you can find the correct device by enumerating either of the following devices:
									</p>
									 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												<code class="literal">/sys/class/net/&lt;eth_port&gt;/device/gnss/</code>
											</li><li class="listitem">
												<code class="literal">/sys/class/gnss/gnss&lt;id&gt;/device/</code>
											</li></ul></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587175682304"> <p>
										<code class="literal">ts2phcOpts</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587175681216"> <p>
										Set options for the <code class="literal">ts2phc</code> command.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587175682304"> <p>
										<code class="literal">recommend</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587175681216"> <p>
										Specify an array of one or more <code class="literal">recommend</code> objects that define rules on how the <code class="literal">profile</code> should be applied to nodes.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587175682304"> <p>
										<code class="literal">.recommend.profile</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587175681216"> <p>
										Specify the <code class="literal">.recommend.profile</code> object name that is defined in the <code class="literal">profile</code> section.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587175682304"> <p>
										<code class="literal">.recommend.priority</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587175681216"> <p>
										Specify the <code class="literal">priority</code> with an integer value between <code class="literal">0</code> and <code class="literal">99</code>. A larger number gets lower priority, so a priority of <code class="literal">99</code> is lower than a priority of <code class="literal">10</code>. If a node can be matched with multiple profiles according to rules defined in the <code class="literal">match</code> field, the profile with the higher priority is applied to that node.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587175682304"> <p>
										<code class="literal">.recommend.match</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587175681216"> <p>
										Specify <code class="literal">.recommend.match</code> rules with <code class="literal">nodeLabel</code> or <code class="literal">nodeName</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587175682304"> <p>
										<code class="literal">.recommend.match.nodeLabel</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587175681216"> <p>
										Set <code class="literal">nodeLabel</code> with the <code class="literal">key</code> of <code class="literal">node.Labels</code> from the node object by using the <code class="literal">oc get nodes --show-labels</code> command. For example: <code class="literal">node-role.kubernetes.io/worker</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587175682304"> <p>
										<code class="literal">.recommend.match.nodeName</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587175681216"> <p>
										Set <code class="literal">nodeName</code> with value of <code class="literal">node.Name</code> from the node object by using the <code class="literal">oc get nodes</code> command. For example: <code class="literal">compute-1.example.com</code>.
									</p>
									 </td></tr></tbody></table></div></div></section></section><section class="section" id="configuring-linuxptp-services-as-ordinary-clock_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.6.3. Configuring linuxptp services as an ordinary clock</h3></div></div></div><p>
					You can configure <code class="literal">linuxptp</code> services (<code class="literal">ptp4l</code>, <code class="literal">phc2sys</code>) as ordinary clock by creating a <code class="literal">PtpConfig</code> custom resource (CR) object.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Use the following example <code class="literal">PtpConfig</code> CR as the basis to configure <code class="literal">linuxptp</code> services as an ordinary clock for your particular hardware and environment. This example CR does not configure PTP fast events. To configure PTP fast events, set appropriate values for <code class="literal">ptp4lOpts</code>, <code class="literal">ptp4lConf</code>, and <code class="literal">ptpClockThreshold</code>. <code class="literal">ptpClockThreshold</code> is required only when events are enabled. See "Configuring the PTP fast event notifications publisher" for more information.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							Install the PTP Operator.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the following <code class="literal">PtpConfig</code> CR, and then save the YAML in the <code class="literal">ordinary-clock-ptp-config.yaml</code> file.
						</p><div id="ptp-ordinary-clock" class="formalpara"><p class="title"><strong>Example PTP ordinary clock configuration</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: ptp.openshift.io/v1
kind: PtpConfig
metadata:
  name: ordinary-clock
  namespace: openshift-ptp
  annotations: {}
spec:
  profile:
    - name: ordinary-clock
      # The interface name is hardware-specific
      interface: $interface
      ptp4lOpts: "-2 -s"
      phc2sysOpts: "-a -r -n 24"
      ptpSchedulingPolicy: SCHED_FIFO
      ptpSchedulingPriority: 10
      ptpSettings:
        logReduce: "true"
      ptp4lConf: |
        [global]
        #
        # Default Data Set
        #
        twoStepFlag 1
        slaveOnly 1
        priority1 128
        priority2 128
        domainNumber 24
        #utc_offset 37
        clockClass 255
        clockAccuracy 0xFE
        offsetScaledLogVariance 0xFFFF
        free_running 0
        freq_est_interval 1
        dscp_event 0
        dscp_general 0
        dataset_comparison G.8275.x
        G.8275.defaultDS.localPriority 128
        #
        # Port Data Set
        #
        logAnnounceInterval -3
        logSyncInterval -4
        logMinDelayReqInterval -4
        logMinPdelayReqInterval -4
        announceReceiptTimeout 3
        syncReceiptTimeout 0
        delayAsymmetry 0
        fault_reset_interval -4
        neighborPropDelayThresh 20000000
        masterOnly 0
        G.8275.portDS.localPriority 128
        #
        # Run time options
        #
        assume_two_step 0
        logging_level 6
        path_trace_enabled 0
        follow_up_info 0
        hybrid_e2e 0
        inhibit_multicast_service 0
        net_sync_monitor 0
        tc_spanning_tree 0
        tx_timestamp_timeout 50
        unicast_listen 0
        unicast_master_table 0
        unicast_req_duration 3600
        use_syslog 1
        verbose 0
        summary_interval 0
        kernel_leap 1
        check_fup_sync 0
        clock_class_threshold 7
        #
        # Servo Options
        #
        pi_proportional_const 0.0
        pi_integral_const 0.0
        pi_proportional_scale 0.0
        pi_proportional_exponent -0.3
        pi_proportional_norm_max 0.7
        pi_integral_scale 0.0
        pi_integral_exponent 0.4
        pi_integral_norm_max 0.3
        step_threshold 2.0
        first_step_threshold 0.00002
        max_frequency 900000000
        clock_servo pi
        sanity_freq_limit 200000000
        ntpshm_segment 0
        #
        # Transport options
        #
        transportSpecific 0x0
        ptp_dst_mac 01:1B:19:00:00:00
        p2p_dst_mac 01:80:C2:00:00:0E
        udp_ttl 1
        udp6_scope 0x0E
        uds_address /var/run/ptp4l
        #
        # Default interface options
        #
        clock_type OC
        network_transport L2
        delay_mechanism E2E
        time_stamping hardware
        tsproc_mode filter
        delay_filter moving_median
        delay_filter_length 10
        egressLatency 0
        ingressLatency 0
        boundary_clock_jbod 0
        #
        # Clock description
        #
        productDescription ;;
        revisionData ;;
        manufacturerIdentity 00:00:00
        userDescription ;
        timeSource 0xA0
  recommend:
    - profile: ordinary-clock
      priority: 4
      match:
        - nodeLabel: "node-role.kubernetes.io/$mcp"</pre>

							</p></div><div class="table" id="idm140587170906224"><p class="title"><strong>Table 19.2. PTP ordinary clock CR configuration options</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 75%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587135242192" scope="col">Custom resource field</th><th align="left" valign="top" id="idm140587135241104" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587135242192"> <p>
											<code class="literal">name</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587135241104"> <p>
											The name of the <code class="literal">PtpConfig</code> CR.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587135242192"> <p>
											<code class="literal">profile</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587135241104"> <p>
											Specify an array of one or more <code class="literal">profile</code> objects. Each profile must be uniquely named.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587135242192"> <p>
											<code class="literal">interface</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587135241104"> <p>
											Specify the network interface to be used by the <code class="literal">ptp4l</code> service, for example <code class="literal">ens787f1</code>.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587135242192"> <p>
											<code class="literal">ptp4lOpts</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587135241104"> <p>
											Specify system config options for the <code class="literal">ptp4l</code> service, for example <code class="literal">-2</code> to select the IEEE 802.3 network transport. The options should not include the network interface name <code class="literal">-i &lt;interface&gt;</code> and service config file <code class="literal">-f /etc/ptp4l.conf</code> because the network interface name and the service config file are automatically appended. Append <code class="literal">--summary_interval -4</code> to use PTP fast events with this interface.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587135242192"> <p>
											<code class="literal">phc2sysOpts</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587135241104"> <p>
											Specify system config options for the <code class="literal">phc2sys</code> service. If this field is empty, the PTP Operator does not start the <code class="literal">phc2sys</code> service. For Intel Columbiaville 800 Series NICs, set <code class="literal">phc2sysOpts</code> options to <code class="literal">-a -r -m -n 24 -N 8 -R 16</code>. <code class="literal">-m</code> prints messages to <code class="literal">stdout</code>. The <code class="literal">linuxptp-daemon</code> <code class="literal">DaemonSet</code> parses the logs and generates Prometheus metrics.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587135242192"> <p>
											<code class="literal">ptp4lConf</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587135241104"> <p>
											Specify a string that contains the configuration to replace the default <code class="literal">/etc/ptp4l.conf</code> file. To use the default configuration, leave the field empty.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587135242192"> <p>
											<code class="literal">tx_timestamp_timeout</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587135241104"> <p>
											For Intel Columbiaville 800 Series NICs, set <code class="literal">tx_timestamp_timeout</code> to <code class="literal">50</code>.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587135242192"> <p>
											<code class="literal">boundary_clock_jbod</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587135241104"> <p>
											For Intel Columbiaville 800 Series NICs, set <code class="literal">boundary_clock_jbod</code> to <code class="literal">0</code>.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587135242192"> <p>
											<code class="literal">ptpSchedulingPolicy</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587135241104"> <p>
											Scheduling policy for <code class="literal">ptp4l</code> and <code class="literal">phc2sys</code> processes. Default value is <code class="literal">SCHED_OTHER</code>. Use <code class="literal">SCHED_FIFO</code> on systems that support FIFO scheduling.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587135242192"> <p>
											<code class="literal">ptpSchedulingPriority</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587135241104"> <p>
											Integer value from 1-65 used to set FIFO priority for <code class="literal">ptp4l</code> and <code class="literal">phc2sys</code> processes when <code class="literal">ptpSchedulingPolicy</code> is set to <code class="literal">SCHED_FIFO</code>. The <code class="literal">ptpSchedulingPriority</code> field is not used when <code class="literal">ptpSchedulingPolicy</code> is set to <code class="literal">SCHED_OTHER</code>.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587135242192"> <p>
											<code class="literal">ptpClockThreshold</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587135241104"> <p>
											Optional. If <code class="literal">ptpClockThreshold</code> is not present, default values are used for the <code class="literal">ptpClockThreshold</code> fields. <code class="literal">ptpClockThreshold</code> configures how long after the PTP master clock is disconnected before PTP events are triggered. <code class="literal">holdOverTimeout</code> is the time value in seconds before the PTP clock event state changes to <code class="literal">FREERUN</code> when the PTP master clock is disconnected. The <code class="literal">maxOffsetThreshold</code> and <code class="literal">minOffsetThreshold</code> settings configure offset values in nanoseconds that compare against the values for <code class="literal">CLOCK_REALTIME</code> (<code class="literal">phc2sys</code>) or master offset (<code class="literal">ptp4l</code>). When the <code class="literal">ptp4l</code> or <code class="literal">phc2sys</code> offset value is outside this range, the PTP clock state is set to <code class="literal">FREERUN</code>. When the offset value is within this range, the PTP clock state is set to <code class="literal">LOCKED</code>.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587135242192"> <p>
											<code class="literal">recommend</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587135241104"> <p>
											Specify an array of one or more <code class="literal">recommend</code> objects that define rules on how the <code class="literal">profile</code> should be applied to nodes.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587135242192"> <p>
											<code class="literal">.recommend.profile</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587135241104"> <p>
											Specify the <code class="literal">.recommend.profile</code> object name defined in the <code class="literal">profile</code> section.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587135242192"> <p>
											<code class="literal">.recommend.priority</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587135241104"> <p>
											Set <code class="literal">.recommend.priority</code> to <code class="literal">0</code> for ordinary clock.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587135242192"> <p>
											<code class="literal">.recommend.match</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587135241104"> <p>
											Specify <code class="literal">.recommend.match</code> rules with <code class="literal">nodeLabel</code> or <code class="literal">nodeName</code>.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587135242192"> <p>
											<code class="literal">.recommend.match.nodeLabel</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587135241104"> <p>
											Update <code class="literal">nodeLabel</code> with the <code class="literal">key</code> of <code class="literal">node.Labels</code> from the node object by using the <code class="literal">oc get nodes --show-labels</code> command. For example: <code class="literal">node-role.kubernetes.io/worker</code>.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587135242192"> <p>
											<code class="literal">.recommend.match.nodeLabel</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587135241104"> <p>
											Update <code class="literal">nodeName</code> with value of <code class="literal">node.Name</code> from the node object by using the <code class="literal">oc get nodes</code> command. For example: <code class="literal">compute-0.example.com</code>.
										</p>
										 </td></tr></tbody></table></div></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">PtpConfig</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f ordinary-clock-ptp-config.yaml</pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check that the <code class="literal">PtpConfig</code> profile is applied to the node.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Get the list of pods in the <code class="literal">openshift-ptp</code> namespace by running the following command:
								</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-ptp -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                            READY   STATUS    RESTARTS   AGE   IP               NODE
linuxptp-daemon-4xkbb           1/1     Running   0          43m   10.1.196.24      compute-0.example.com
linuxptp-daemon-tdspf           1/1     Running   0          43m   10.1.196.25      compute-1.example.com
ptp-operator-657bbb64c8-2f8sj   1/1     Running   0          43m   10.129.0.61      control-plane-1.example.com</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Check that the profile is correct. Examine the logs of the <code class="literal">linuxptp</code> daemon that corresponds to the node you specified in the <code class="literal">PtpConfig</code> profile. Run the following command:
								</p><pre class="programlisting language-terminal">$ oc logs linuxptp-daemon-4xkbb -n openshift-ptp -c linuxptp-daemon-container</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">I1115 09:41:17.117596 4143292 daemon.go:107] in applyNodePTPProfile
I1115 09:41:17.117604 4143292 daemon.go:109] updating NodePTPProfile to:
I1115 09:41:17.117607 4143292 daemon.go:110] ------------------------------------
I1115 09:41:17.117612 4143292 daemon.go:102] Profile Name: profile1
I1115 09:41:17.117616 4143292 daemon.go:102] Interface: ens787f1
I1115 09:41:17.117620 4143292 daemon.go:102] Ptp4lOpts: -2 -s
I1115 09:41:17.117623 4143292 daemon.go:102] Phc2sysOpts: -a -r -n 24
I1115 09:41:17.117626 4143292 daemon.go:116] ------------------------------------</pre>

									</p></div></li></ol></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For more information about FIFO priority scheduling on PTP hardware, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#cnf-configuring-fifo-priority-scheduling-for-ptp_using-ptp">Configuring FIFO priority scheduling for PTP hardware</a>.
						</li><li class="listitem">
							For more information about configuring PTP fast events, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#cnf-configuring-the-ptp-fast-event-publisher_using-ptp">Configuring the PTP fast event notifications publisher</a>.
						</li></ul></div></section><section class="section" id="configuring-linuxptp-services-as-boundary-clock_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.6.4. Configuring linuxptp services as a boundary clock</h3></div></div></div><p>
					You can configure the <code class="literal">linuxptp</code> services (<code class="literal">ptp4l</code>, <code class="literal">phc2sys</code>) as boundary clock by creating a <code class="literal">PtpConfig</code> custom resource (CR) object.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Use the following example <code class="literal">PtpConfig</code> CR as the basis to configure <code class="literal">linuxptp</code> services as the boundary clock for your particular hardware and environment. This example CR does not configure PTP fast events. To configure PTP fast events, set appropriate values for <code class="literal">ptp4lOpts</code>, <code class="literal">ptp4lConf</code>, and <code class="literal">ptpClockThreshold</code>. <code class="literal">ptpClockThreshold</code> is used only when events are enabled. See "Configuring the PTP fast event notifications publisher" for more information.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							Install the PTP Operator.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the following <code class="literal">PtpConfig</code> CR, and then save the YAML in the <code class="literal">boundary-clock-ptp-config.yaml</code> file.
						</p><div class="formalpara"><p class="title"><strong>Example PTP boundary clock configuration</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: ptp.openshift.io/v1
kind: PtpConfig
metadata:
  name: boundary-clock
  namespace: openshift-ptp
  annotations: {}
spec:
  profile:
    - name: boundary-clock
      ptp4lOpts: "-2"
      phc2sysOpts: "-a -r -n 24"
      ptpSchedulingPolicy: SCHED_FIFO
      ptpSchedulingPriority: 10
      ptpSettings:
        logReduce: "true"
      ptp4lConf: |
        # The interface name is hardware-specific
        [$iface_slave]
        masterOnly 0
        [$iface_master_1]
        masterOnly 1
        [$iface_master_2]
        masterOnly 1
        [$iface_master_3]
        masterOnly 1
        [global]
        #
        # Default Data Set
        #
        twoStepFlag 1
        slaveOnly 0
        priority1 128
        priority2 128
        domainNumber 24
        #utc_offset 37
        clockClass 248
        clockAccuracy 0xFE
        offsetScaledLogVariance 0xFFFF
        free_running 0
        freq_est_interval 1
        dscp_event 0
        dscp_general 0
        dataset_comparison G.8275.x
        G.8275.defaultDS.localPriority 128
        #
        # Port Data Set
        #
        logAnnounceInterval -3
        logSyncInterval -4
        logMinDelayReqInterval -4
        logMinPdelayReqInterval -4
        announceReceiptTimeout 3
        syncReceiptTimeout 0
        delayAsymmetry 0
        fault_reset_interval -4
        neighborPropDelayThresh 20000000
        masterOnly 0
        G.8275.portDS.localPriority 128
        #
        # Run time options
        #
        assume_two_step 0
        logging_level 6
        path_trace_enabled 0
        follow_up_info 0
        hybrid_e2e 0
        inhibit_multicast_service 0
        net_sync_monitor 0
        tc_spanning_tree 0
        tx_timestamp_timeout 50
        unicast_listen 0
        unicast_master_table 0
        unicast_req_duration 3600
        use_syslog 1
        verbose 0
        summary_interval 0
        kernel_leap 1
        check_fup_sync 0
        clock_class_threshold 7
        #
        # Servo Options
        #
        pi_proportional_const 0.0
        pi_integral_const 0.0
        pi_proportional_scale 0.0
        pi_proportional_exponent -0.3
        pi_proportional_norm_max 0.7
        pi_integral_scale 0.0
        pi_integral_exponent 0.4
        pi_integral_norm_max 0.3
        step_threshold 2.0
        first_step_threshold 0.00002
        max_frequency 900000000
        clock_servo pi
        sanity_freq_limit 200000000
        ntpshm_segment 0
        #
        # Transport options
        #
        transportSpecific 0x0
        ptp_dst_mac 01:1B:19:00:00:00
        p2p_dst_mac 01:80:C2:00:00:0E
        udp_ttl 1
        udp6_scope 0x0E
        uds_address /var/run/ptp4l
        #
        # Default interface options
        #
        clock_type BC
        network_transport L2
        delay_mechanism E2E
        time_stamping hardware
        tsproc_mode filter
        delay_filter moving_median
        delay_filter_length 10
        egressLatency 0
        ingressLatency 0
        boundary_clock_jbod 0
        #
        # Clock description
        #
        productDescription ;;
        revisionData ;;
        manufacturerIdentity 00:00:00
        userDescription ;
        timeSource 0xA0
  recommend:
    - profile: boundary-clock
      priority: 4
      match:
        - nodeLabel: "node-role.kubernetes.io/$mcp"</pre>

							</p></div><div class="table" id="idm140587175326864"><p class="title"><strong>Table 19.3. PTP boundary clock CR configuration options</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 75%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587140727216" scope="col">Custom resource field</th><th align="left" valign="top" id="idm140587140726128" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">name</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											The name of the <code class="literal">PtpConfig</code> CR.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">profile</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											Specify an array of one or more <code class="literal">profile</code> objects.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">name</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											Specify the name of a profile object which uniquely identifies a profile object.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">ptp4lOpts</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											Specify system config options for the <code class="literal">ptp4l</code> service. The options should not include the network interface name <code class="literal">-i &lt;interface&gt;</code> and service config file <code class="literal">-f /etc/ptp4l.conf</code> because the network interface name and the service config file are automatically appended.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">ptp4lConf</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											Specify the required configuration to start <code class="literal">ptp4l</code> as boundary clock. For example, <code class="literal">ens1f0</code> synchronizes from a grandmaster clock and <code class="literal">ens1f3</code> synchronizes connected devices.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">&lt;interface_1&gt;</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											The interface that receives the synchronization clock.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">&lt;interface_2&gt;</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											The interface that sends the synchronization clock.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">tx_timestamp_timeout</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											For Intel Columbiaville 800 Series NICs, set <code class="literal">tx_timestamp_timeout</code> to <code class="literal">50</code>.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">boundary_clock_jbod</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											For Intel Columbiaville 800 Series NICs, ensure <code class="literal">boundary_clock_jbod</code> is set to <code class="literal">0</code>. For Intel Fortville X710 Series NICs, ensure <code class="literal">boundary_clock_jbod</code> is set to <code class="literal">1</code>.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">phc2sysOpts</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											Specify system config options for the <code class="literal">phc2sys</code> service. If this field is empty, the PTP Operator does not start the <code class="literal">phc2sys</code> service.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">ptpSchedulingPolicy</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											Scheduling policy for ptp4l and phc2sys processes. Default value is <code class="literal">SCHED_OTHER</code>. Use <code class="literal">SCHED_FIFO</code> on systems that support FIFO scheduling.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">ptpSchedulingPriority</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											Integer value from 1-65 used to set FIFO priority for <code class="literal">ptp4l</code> and <code class="literal">phc2sys</code> processes when <code class="literal">ptpSchedulingPolicy</code> is set to <code class="literal">SCHED_FIFO</code>. The <code class="literal">ptpSchedulingPriority</code> field is not used when <code class="literal">ptpSchedulingPolicy</code> is set to <code class="literal">SCHED_OTHER</code>.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">ptpClockThreshold</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											Optional. If <code class="literal">ptpClockThreshold</code> is not present, default values are used for the <code class="literal">ptpClockThreshold</code> fields. <code class="literal">ptpClockThreshold</code> configures how long after the PTP master clock is disconnected before PTP events are triggered. <code class="literal">holdOverTimeout</code> is the time value in seconds before the PTP clock event state changes to <code class="literal">FREERUN</code> when the PTP master clock is disconnected. The <code class="literal">maxOffsetThreshold</code> and <code class="literal">minOffsetThreshold</code> settings configure offset values in nanoseconds that compare against the values for <code class="literal">CLOCK_REALTIME</code> (<code class="literal">phc2sys</code>) or master offset (<code class="literal">ptp4l</code>). When the <code class="literal">ptp4l</code> or <code class="literal">phc2sys</code> offset value is outside this range, the PTP clock state is set to <code class="literal">FREERUN</code>. When the offset value is within this range, the PTP clock state is set to <code class="literal">LOCKED</code>.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">recommend</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											Specify an array of one or more <code class="literal">recommend</code> objects that define rules on how the <code class="literal">profile</code> should be applied to nodes.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">.recommend.profile</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											Specify the <code class="literal">.recommend.profile</code> object name defined in the <code class="literal">profile</code> section.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">.recommend.priority</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											Specify the <code class="literal">priority</code> with an integer value between <code class="literal">0</code> and <code class="literal">99</code>. A larger number gets lower priority, so a priority of <code class="literal">99</code> is lower than a priority of <code class="literal">10</code>. If a node can be matched with multiple profiles according to rules defined in the <code class="literal">match</code> field, the profile with the higher priority is applied to that node.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">.recommend.match</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											Specify <code class="literal">.recommend.match</code> rules with <code class="literal">nodeLabel</code> or <code class="literal">nodeName</code>.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">.recommend.match.nodeLabel</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											Update <code class="literal">nodeLabel</code> with the <code class="literal">key</code> of <code class="literal">node.Labels</code> from the node object by using the <code class="literal">oc get nodes --show-labels</code> command. For example: <code class="literal">node-role.kubernetes.io/worker</code>.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587140727216"> <p>
											<code class="literal">.recommend.match.nodeLabel</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140587140726128"> <p>
											Update <code class="literal">nodeName</code> with value of <code class="literal">node.Name</code> from the node object by using the <code class="literal">oc get nodes</code> command. For example: <code class="literal">compute-0.example.com</code>.
										</p>
										 </td></tr></tbody></table></div></div></li><li class="listitem"><p class="simpara">
							Create the CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f boundary-clock-ptp-config.yaml</pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check that the <code class="literal">PtpConfig</code> profile is applied to the node.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Get the list of pods in the <code class="literal">openshift-ptp</code> namespace by running the following command:
								</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-ptp -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                            READY   STATUS    RESTARTS   AGE   IP               NODE
linuxptp-daemon-4xkbb           1/1     Running   0          43m   10.1.196.24      compute-0.example.com
linuxptp-daemon-tdspf           1/1     Running   0          43m   10.1.196.25      compute-1.example.com
ptp-operator-657bbb64c8-2f8sj   1/1     Running   0          43m   10.129.0.61      control-plane-1.example.com</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Check that the profile is correct. Examine the logs of the <code class="literal">linuxptp</code> daemon that corresponds to the node you specified in the <code class="literal">PtpConfig</code> profile. Run the following command:
								</p><pre class="programlisting language-terminal">$ oc logs linuxptp-daemon-4xkbb -n openshift-ptp -c linuxptp-daemon-container</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">I1115 09:41:17.117596 4143292 daemon.go:107] in applyNodePTPProfile
I1115 09:41:17.117604 4143292 daemon.go:109] updating NodePTPProfile to:
I1115 09:41:17.117607 4143292 daemon.go:110] ------------------------------------
I1115 09:41:17.117612 4143292 daemon.go:102] Profile Name: profile1
I1115 09:41:17.117616 4143292 daemon.go:102] Interface:
I1115 09:41:17.117620 4143292 daemon.go:102] Ptp4lOpts: -2
I1115 09:41:17.117623 4143292 daemon.go:102] Phc2sysOpts: -a -r -n 24
I1115 09:41:17.117626 4143292 daemon.go:116] ------------------------------------</pre>

									</p></div></li></ol></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For more information about FIFO priority scheduling on PTP hardware, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#cnf-configuring-fifo-priority-scheduling-for-ptp_using-ptp">Configuring FIFO priority scheduling for PTP hardware</a>.
						</li><li class="listitem">
							For more information about configuring PTP fast events, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#cnf-configuring-the-ptp-fast-event-publisher_using-ptp">Configuring the PTP fast event notifications publisher</a>.
						</li></ul></div></section><section class="section" id="ptp-configuring-linuxptp-services-as-bc-for-dual-nic_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.6.5. Configuring linuxptp services as boundary clocks for dual NIC hardware</h3></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Precision Time Protocol (PTP) hardware with dual NIC configured as boundary clocks is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
					</p><p>
						For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
					</p></div></div><p>
					You can configure the <code class="literal">linuxptp</code> services (<code class="literal">ptp4l</code>, <code class="literal">phc2sys</code>) as boundary clocks for dual NIC hardware by creating a <code class="literal">PtpConfig</code> custom resource (CR) object for each NIC.
				</p><p>
					Dual NIC hardware allows you to connect each NIC to the same upstream leader clock with separate <code class="literal">ptp4l</code> instances for each NIC feeding the downstream clocks.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							Install the PTP Operator.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create two separate <code class="literal">PtpConfig</code> CRs, one for each NIC, using the reference CR in "Configuring linuxptp services as a boundary clock" as the basis for each CR. For example:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create <code class="literal">boundary-clock-ptp-config-nic1.yaml</code>, specifying values for <code class="literal">phc2sysOpts</code>:
								</p><pre class="programlisting language-yaml">apiVersion: ptp.openshift.io/v1
kind: PtpConfig
metadata:
  name: boundary-clock-ptp-config-nic1
  namespace: openshift-ptp
spec:
  profile:
  - name: "profile1"
    ptp4lOpts: "-2 --summary_interval -4"
    ptp4lConf: | <span id="CO35-1"><!--Empty--></span><span class="callout">1</span>
      [ens5f1]
      masterOnly 1
      [ens5f0]
      masterOnly 0
    ...
    phc2sysOpts: "-a -r -m -n 24 -N 8 -R 16" <span id="CO35-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO35-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the required interfaces to start <code class="literal">ptp4l</code> as a boundary clock. For example, <code class="literal">ens5f0</code> synchronizes from a grandmaster clock and <code class="literal">ens5f1</code> synchronizes connected devices.
										</div></dd><dt><a href="#CO35-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Required <code class="literal">phc2sysOpts</code> values. <code class="literal">-m</code> prints messages to <code class="literal">stdout</code>. The <code class="literal">linuxptp-daemon</code> <code class="literal">DaemonSet</code> parses the logs and generates Prometheus metrics.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Create <code class="literal">boundary-clock-ptp-config-nic2.yaml</code>, removing the <code class="literal">phc2sysOpts</code> field altogether to disable the <code class="literal">phc2sys</code> service for the second NIC:
								</p><pre class="programlisting language-yaml">apiVersion: ptp.openshift.io/v1
kind: PtpConfig
metadata:
  name: boundary-clock-ptp-config-nic2
  namespace: openshift-ptp
spec:
  profile:
  - name: "profile2"
    ptp4lOpts: "-2 --summary_interval -4"
    ptp4lConf: | <span id="CO36-1"><!--Empty--></span><span class="callout">1</span>
      [ens7f1]
      masterOnly 1
      [ens7f0]
      masterOnly 0
...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO36-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the required interfaces to start <code class="literal">ptp4l</code> as a boundary clock on the second NIC.
										</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										You must completely remove the <code class="literal">phc2sysOpts</code> field from the second <code class="literal">PtpConfig</code> CR to disable the <code class="literal">phc2sys</code> service on the second NIC.
									</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Create the dual NIC <code class="literal">PtpConfig</code> CRs by running the following commands:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create the CR that configures PTP for the first NIC:
								</p><pre class="programlisting language-terminal">$ oc create -f boundary-clock-ptp-config-nic1.yaml</pre></li><li class="listitem"><p class="simpara">
									Create the CR that configures PTP for the second NIC:
								</p><pre class="programlisting language-terminal">$ oc create -f boundary-clock-ptp-config-nic2.yaml</pre></li></ol></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Check that the PTP Operator has applied the <code class="literal">PtpConfig</code> CRs for both NICs. Examine the logs for the <code class="literal">linuxptp</code> daemon corresponding to the node that has the dual NIC hardware installed. For example, run the following command:
						</p><pre class="programlisting language-terminal">$ oc logs linuxptp-daemon-cvgr6 -n openshift-ptp -c linuxptp-daemon-container</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">ptp4l[80828.335]: [ptp4l.1.config] master offset          5 s2 freq   -5727 path delay       519
ptp4l[80828.343]: [ptp4l.0.config] master offset         -5 s2 freq  -10607 path delay       533
phc2sys[80828.390]: [ptp4l.0.config] CLOCK_REALTIME phc offset         1 s2 freq  -87239 delay    539</pre>

							</p></div></li></ul></div></section><section class="section" id="nw-columbiaville-ptp-config-refererence_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.6.6. Intel Columbiaville E800 series NIC as PTP ordinary clock reference</h3></div></div></div><p>
					The following table describes the changes that you must make to the reference PTP configuration in order to use Intel Columbiaville E800 series NICs as ordinary clocks. Make the changes in a <code class="literal">PtpConfig</code> custom resource (CR) that you apply to the cluster.
				</p><div class="table" id="idm140587122377840"><p class="title"><strong>Table 19.4. Recommended PTP settings for Intel Columbiaville NIC</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587172370768" scope="col">PTP configuration</th><th align="left" valign="top" id="idm140587172369680" scope="col">Recommended setting</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587172370768"> <p>
									<code class="literal">phc2sysOpts</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587172369680"> <p>
									<code class="literal">-a -r -m -n 24 -N 8 -R 16</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587172370768"> <p>
									<code class="literal">tx_timestamp_timeout</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587172369680"> <p>
									<code class="literal">50</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587172370768"> <p>
									<code class="literal">boundary_clock_jbod</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587172369680"> <p>
									<code class="literal">0</code>
								</p>
								 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						For <code class="literal">phc2sysOpts</code>, <code class="literal">-m</code> prints messages to <code class="literal">stdout</code>. The <code class="literal">linuxptp-daemon</code> <code class="literal">DaemonSet</code> parses the logs and generates Prometheus metrics.
					</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For a complete example CR that configures <code class="literal">linuxptp</code> services as an ordinary clock with PTP fast events, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-linuxptp-services-as-ordinary-clock_using-ptp">Configuring linuxptp services as ordinary clock</a>.
						</li></ul></div></section><section class="section" id="cnf-configuring-fifo-priority-scheduling-for-ptp_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.6.7. Configuring FIFO priority scheduling for PTP hardware</h3></div></div></div><p>
					In telco or other deployment configurations that require low latency performance, PTP daemon threads run in a constrained CPU footprint alongside the rest of the infrastructure components. By default, PTP threads run with the <code class="literal">SCHED_OTHER</code> policy. Under high load, these threads might not get the scheduling latency they require for error-free operation.
				</p><p>
					To mitigate against potential scheduling latency errors, you can configure the PTP Operator <code class="literal">linuxptp</code> services to allow threads to run with a <code class="literal">SCHED_FIFO</code> policy. If <code class="literal">SCHED_FIFO</code> is set for a <code class="literal">PtpConfig</code> CR, then <code class="literal">ptp4l</code> and <code class="literal">phc2sys</code> will run in the parent container under <code class="literal">chrt</code> with a priority set by the <code class="literal">ptpSchedulingPriority</code> field of the <code class="literal">PtpConfig</code> CR.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Setting <code class="literal">ptpSchedulingPolicy</code> is optional, and is only required if you are experiencing latency errors.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">PtpConfig</code> CR profile:
						</p><pre class="programlisting language-terminal">$ oc edit PtpConfig -n openshift-ptp</pre></li><li class="listitem"><p class="simpara">
							Change the <code class="literal">ptpSchedulingPolicy</code> and <code class="literal">ptpSchedulingPriority</code> fields:
						</p><pre class="programlisting language-yaml">apiVersion: ptp.openshift.io/v1
kind: PtpConfig
metadata:
  name: &lt;ptp_config_name&gt;
  namespace: openshift-ptp
...
spec:
  profile:
  - name: "profile1"
...
    ptpSchedulingPolicy: SCHED_FIFO <span id="CO37-1"><!--Empty--></span><span class="callout">1</span>
    ptpSchedulingPriority: 10 <span id="CO37-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO37-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Scheduling policy for <code class="literal">ptp4l</code> and <code class="literal">phc2sys</code> processes. Use <code class="literal">SCHED_FIFO</code> on systems that support FIFO scheduling.
								</div></dd><dt><a href="#CO37-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Required. Sets the integer value 1-65 used to configure FIFO priority for <code class="literal">ptp4l</code> and <code class="literal">phc2sys</code> processes.
								</div></dd></dl></div></li><li class="listitem">
							Save and exit to apply the changes to the <code class="literal">PtpConfig</code> CR.
						</li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Get the name of the <code class="literal">linuxptp-daemon</code> pod and corresponding node where the <code class="literal">PtpConfig</code> CR has been applied:
						</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-ptp -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                            READY   STATUS    RESTARTS   AGE     IP            NODE
linuxptp-daemon-gmv2n           3/3     Running   0          1d17h   10.1.196.24   compute-0.example.com
linuxptp-daemon-lgm55           3/3     Running   0          1d17h   10.1.196.25   compute-1.example.com
ptp-operator-3r4dcvf7f4-zndk7   1/1     Running   0          1d7h    10.129.0.61   control-plane-1.example.com</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check that the <code class="literal">ptp4l</code> process is running with the updated <code class="literal">chrt</code> FIFO priority:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ptp logs linuxptp-daemon-lgm55 -c linuxptp-daemon-container|grep chrt</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">I1216 19:24:57.091872 1600715 daemon.go:285] /bin/chrt -f 65 /usr/sbin/ptp4l -f /var/run/ptp4l.0.config -2  --summary_interval -4 -m</pre>

							</p></div></li></ol></div></section><section class="section" id="cnf-configuring-log-filtering-for-linuxptp_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.6.8. Configuring log filtering for linuxptp services</h3></div></div></div><p>
					The <code class="literal">linuxptp</code> daemon generates logs that you can use for debugging purposes. In telco or other deployment configurations that feature a limited storage capacity, these logs can add to the storage demand.
				</p><p>
					To reduce the number log messages, you can configure the <code class="literal">PtpConfig</code> custom resource (CR) to exclude log messages that report the <code class="literal">master offset</code> value. The <code class="literal">master offset</code> log message reports the difference between the current node’s clock and the master clock in nanoseconds.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							Install the PTP Operator.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">PtpConfig</code> CR:
						</p><pre class="programlisting language-terminal">$ oc edit PtpConfig -n openshift-ptp</pre></li><li class="listitem"><p class="simpara">
							In <code class="literal">spec.profile</code>, add the <code class="literal">ptpSettings.logReduce</code> specification and set the value to <code class="literal">true</code>:
						</p><pre class="programlisting language-yaml">apiVersion: ptp.openshift.io/v1
kind: PtpConfig
metadata:
  name: &lt;ptp_config_name&gt;
  namespace: openshift-ptp
...
spec:
  profile:
  - name: "profile1"
...
    ptpSettings:
      logReduce: "true"</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								For debugging purposes, you can revert this specification to <code class="literal">False</code> to include the master offset messages.
							</p></div></div></li><li class="listitem">
							Save and exit to apply the changes to the <code class="literal">PtpConfig</code> CR.
						</li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Get the name of the <code class="literal">linuxptp-daemon</code> pod and corresponding node where the <code class="literal">PtpConfig</code> CR has been applied:
						</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-ptp -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                            READY   STATUS    RESTARTS   AGE     IP            NODE
linuxptp-daemon-gmv2n           3/3     Running   0          1d17h   10.1.196.24   compute-0.example.com
linuxptp-daemon-lgm55           3/3     Running   0          1d17h   10.1.196.25   compute-1.example.com
ptp-operator-3r4dcvf7f4-zndk7   1/1     Running   0          1d7h    10.129.0.61   control-plane-1.example.com</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify that master offset messages are excluded from the logs by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-ptp logs &lt;linux_daemon_container&gt; -c linuxptp-daemon-container | grep "master offset" <span id="CO38-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO38-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									&lt;linux_daemon_container&gt; is the name of the <code class="literal">linuxptp-daemon</code> pod, for example <code class="literal">linuxptp-daemon-gmv2n</code>.
								</div></dd></dl></div><p class="simpara">
							When you configure the <code class="literal">logReduce</code> specification, this command does not report any instances of <code class="literal">master offset</code> in the logs of the <code class="literal">linuxptp</code> daemon.
						</p></li></ol></div></section></section><section class="section" id="cnf-troubleshooting-common-ptp-operator-issues_using-ptp"><div class="titlepage"><div><div><h2 class="title">19.7. Troubleshooting common PTP Operator issues</h2></div></div></div><p>
				Troubleshoot common problems with the PTP Operator by performing the following steps.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install the OpenShift Container Platform CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						Log in as a user with <code class="literal">cluster-admin</code> privileges.
					</li><li class="listitem">
						Install the PTP Operator on a bare-metal cluster with hosts that support PTP.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Check the Operator and operands are successfully deployed in the cluster for the configured nodes.
					</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-ptp -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                            READY   STATUS    RESTARTS   AGE     IP            NODE
linuxptp-daemon-lmvgn           3/3     Running   0          4d17h   10.1.196.24   compute-0.example.com
linuxptp-daemon-qhfg7           3/3     Running   0          4d17h   10.1.196.25   compute-1.example.com
ptp-operator-6b8dcbf7f4-zndk7   1/1     Running   0          5d7h    10.129.0.61   control-plane-1.example.com</pre>

						</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							When the PTP fast event bus is enabled, the number of ready <code class="literal">linuxptp-daemon</code> pods is <code class="literal">3/3</code>. If the PTP fast event bus is not enabled, <code class="literal">2/2</code> is displayed.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Check that supported hardware is found in the cluster.
					</p><pre class="programlisting language-terminal">$ oc -n openshift-ptp get nodeptpdevices.ptp.openshift.io</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                                  AGE
control-plane-0.example.com           10d
control-plane-1.example.com           10d
compute-0.example.com                 10d
compute-1.example.com                 10d
compute-2.example.com                 10d</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Check the available PTP network interfaces for a node:
					</p><pre class="programlisting language-terminal">$ oc -n openshift-ptp get nodeptpdevices.ptp.openshift.io &lt;node_name&gt; -o yaml</pre><p class="simpara">
						where:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;node_name&gt;</span></dt><dd><p class="simpara">
									Specifies the node you want to query, for example, <code class="literal">compute-0.example.com</code>.
								</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: ptp.openshift.io/v1
kind: NodePtpDevice
metadata:
  creationTimestamp: "2021-09-14T16:52:33Z"
  generation: 1
  name: compute-0.example.com
  namespace: openshift-ptp
  resourceVersion: "177400"
  uid: 30413db0-4d8d-46da-9bef-737bacd548fd
spec: {}
status:
  devices:
  - name: eno1
  - name: eno2
  - name: eno3
  - name: eno4
  - name: enp5s0f0
  - name: enp5s0f1</pre>

									</p></div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Check that the PTP interface is successfully synchronized to the primary clock by accessing the <code class="literal">linuxptp-daemon</code> pod for the corresponding node.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Get the name of the <code class="literal">linuxptp-daemon</code> pod and corresponding node you want to troubleshoot by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-ptp -o wide</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                            READY   STATUS    RESTARTS   AGE     IP            NODE
linuxptp-daemon-lmvgn           3/3     Running   0          4d17h   10.1.196.24   compute-0.example.com
linuxptp-daemon-qhfg7           3/3     Running   0          4d17h   10.1.196.25   compute-1.example.com
ptp-operator-6b8dcbf7f4-zndk7   1/1     Running   0          5d7h    10.129.0.61   control-plane-1.example.com</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Remote shell into the required <code class="literal">linuxptp-daemon</code> container:
							</p><pre class="programlisting language-terminal">$ oc rsh -n openshift-ptp -c linuxptp-daemon-container &lt;linux_daemon_container&gt;</pre><p class="simpara">
								where:
							</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;linux_daemon_container&gt;</span></dt><dd>
											is the container you want to diagnose, for example <code class="literal">linuxptp-daemon-lmvgn</code>.
										</dd></dl></div></li><li class="listitem"><p class="simpara">
								In the remote shell connection to the <code class="literal">linuxptp-daemon</code> container, use the PTP Management Client (<code class="literal">pmc</code>) tool to diagnose the network interface. Run the following <code class="literal">pmc</code> command to check the sync status of the PTP device, for example <code class="literal">ptp4l</code>.
							</p><pre class="programlisting language-terminal"># pmc -u -f /var/run/ptp4l.0.config -b 0 'GET PORT_DATA_SET'</pre><div class="formalpara"><p class="title"><strong>Example output when the node is successfully synced to the primary clock</strong></p><p>
									
<pre class="programlisting language-terminal">sending: GET PORT_DATA_SET
    40a6b7.fffe.166ef0-1 seq 0 RESPONSE MANAGEMENT PORT_DATA_SET
        portIdentity            40a6b7.fffe.166ef0-1
        portState               SLAVE
        logMinDelayReqInterval  -4
        peerMeanPathDelay       0
        logAnnounceInterval     -3
        announceReceiptTimeout  3
        logSyncInterval         -4
        delayMechanism          1
        logMinPdelayReqInterval -4
        versionNumber           2</pre>

								</p></div></li></ol></div></li></ol></div><section class="section" id="cnf-about-collecting-nro-data_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.7.1. Collecting Precision Time Protocol (PTP) Operator data</h3></div></div></div><p>
					You can use the <code class="literal">oc adm must-gather</code> CLI command to collect information about your cluster, including features and objects associated with Precision Time Protocol (PTP) Operator.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have installed the PTP Operator.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To collect PTP Operator data with <code class="literal">must-gather</code>, you must specify the PTP Operator <code class="literal">must-gather</code> image.
						</p><pre class="programlisting language-terminal">$ oc adm must-gather --image=registry.redhat.io/openshift4/ptp-must-gather-rhel8:v4.13</pre></li></ul></div></section></section><section class="section" id="ptp-hardware-fast-event-notifications-framework"><div class="titlepage"><div><div><h2 class="title">19.8. PTP hardware fast event notifications framework</h2></div></div></div><p>
				Cloud native applications such as virtual RAN (vRAN) require access to notifications about hardware timing events that are critical to the functioning of the overall network. PTP clock synchronization errors can negatively affect the performance and reliability of your low-latency application, for example, a vRAN application running in a distributed unit (DU).
			</p><section class="section" id="cnf-about-ptp-and-clock-synchronization_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.8.1. About PTP and clock synchronization error events</h3></div></div></div><p>
					Loss of PTP synchronization is a critical error for a RAN network. If synchronization is lost on a node, the radio might be shut down and the network Over the Air (OTA) traffic might be shifted to another node in the wireless network. Fast event notifications mitigate against workload errors by allowing cluster nodes to communicate PTP clock sync status to the vRAN application running in the DU.
				</p><p>
					Event notifications are available to vRAN applications running on the same DU node. A publish-subscribe REST API passes events notifications to the messaging bus. Publish-subscribe messaging, or pub-sub messaging, is an asynchronous service-to-service communication architecture where any message published to a topic is immediately received by all of the subscribers to the topic.
				</p><p>
					The PTP Operator generates fast event notifications for every PTP-capable network interface. You can access the events by using a <code class="literal">cloud-event-proxy</code> sidecar container over an HTTP or Advanced Message Queuing Protocol (AMQP) message bus.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						PTP fast event notifications are available for network interfaces configured to use PTP ordinary clocks or PTP boundary clocks.
					</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Use HTTP transport instead of AMQP for PTP and bare-metal events where possible. AMQ Interconnect is EOL from 30 June 2024. Extended life cycle support (ELS) for AMQ Interconnect ends 29 November 2029. For more information see, <a class="link" href="https://access.redhat.com/support/policy/updates/jboss_notes#p_Interconnect">Red Hat AMQ Interconnect support status</a>.
					</p></div></div></section><section class="section" id="cnf-about-ptp-fast-event-notifications-framework_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.8.2. About the PTP fast event notifications framework</h3></div></div></div><p>
					Use the Precision Time Protocol (PTP) fast event notifications framework to subscribe cluster applications to PTP events that the bare-metal cluster node generates.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The fast events notifications framework uses a REST API for communication. The REST API is based on the <span class="emphasis"><em>O-RAN O-Cloud Notification API Specification for Event Consumers 3.0</em></span> that is available from <a class="link" href="https://orandownloadsweb.azurewebsites.net/specifications">O-RAN ALLIANCE Specifications</a>.
					</p></div></div><p>
					The framework consists of a publisher, subscriber, and an AMQ or HTTP messaging protocol to handle communications between the publisher and subscriber applications. Applications run the <code class="literal">cloud-event-proxy</code> container in a sidecar pattern to subscribe to PTP events. The <code class="literal">cloud-event-proxy</code> sidecar container can access the same resources as the primary application container without using any of the resources of the primary application and with no significant latency.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Use HTTP transport instead of AMQP for PTP and bare-metal events where possible. AMQ Interconnect is EOL from 30 June 2024. Extended life cycle support (ELS) for AMQ Interconnect ends 29 November 2029. For more information see, <a class="link" href="https://access.redhat.com/support/policy/updates/jboss_notes#p_Interconnect">Red Hat AMQ Interconnect support status</a>.
					</p></div></div><div class="figure" id="idm140587168451424"><p class="title"><strong>Figure 19.1. Overview of PTP fast events</strong></p><div class="figure-contents"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/5c30eafec0067ac28094ae6c8922d221/319_OpenShift_PTP_bare-metal_OCP_nodes_0323_4.13.png" alt="Overview of PTP fast events"/></div></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term"> <span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/066d390945eb233d66d1b10310fec9e7/darkcircle-1.png" width="20" alt="20"/></span>
						 Event is generated on the cluster host</span></dt><dd>
								<code class="literal">linuxptp-daemon</code> in the PTP Operator-managed pod runs as a Kubernetes <code class="literal">DaemonSet</code> and manages the various <code class="literal">linuxptp</code> processes (<code class="literal">ptp4l</code>, <code class="literal">phc2sys</code>, and optionally for grandmaster clocks, <code class="literal">ts2phc</code>). The <code class="literal">linuxptp-daemon</code> passes the event to the UNIX domain socket.
							</dd><dt><span class="term"> <span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/dffcf5a2ed44061a19f38d92a8a3d1bb/darkcircle-2.png" width="20" alt="20"/></span>
						 Event is passed to the cloud-event-proxy sidecar</span></dt><dd>
								The PTP plugin reads the event from the UNIX domain socket and passes it to the <code class="literal">cloud-event-proxy</code> sidecar in the PTP Operator-managed pod. <code class="literal">cloud-event-proxy</code> delivers the event from the Kubernetes infrastructure to Cloud-Native Network Functions (CNFs) with low latency.
							</dd><dt><span class="term"> <span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/e4171dd0f615beb50be5dc701259f444/darkcircle-3.png" width="20" alt="20"/></span>
						 Event is persisted</span></dt><dd>
								The <code class="literal">cloud-event-proxy</code> sidecar in the PTP Operator-managed pod processes the event and publishes the cloud-native event by using a REST API.
							</dd><dt><span class="term"> <span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/dd170f3f40e17838a75352ca1962f9bb/darkcircle-4.png" width="20" alt="20"/></span>
						 Message is transported</span></dt><dd>
								The message transporter transports the event to the <code class="literal">cloud-event-proxy</code> sidecar in the application pod over HTTP or AMQP 1.0 QPID.
							</dd><dt><span class="term"> <span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/b86066dfcdf40a6c7ce19b770a9d3868/darkcircle-5.png" width="20" alt="20"/></span>
						 Event is available from the REST API</span></dt><dd>
								The <code class="literal">cloud-event-proxy</code> sidecar in the Application pod processes the event and makes it available by using the REST API.
							</dd><dt><span class="term"> <span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/3355b6bb1a911bd888724ef147bddf81/darkcircle-6.png" width="20" alt="20"/></span>
						 Consumer application requests a subscription and receives the subscribed event</span></dt><dd>
								The consumer application sends an API request to the <code class="literal">cloud-event-proxy</code> sidecar in the application pod to create a PTP events subscription. The <code class="literal">cloud-event-proxy</code> sidecar creates an AMQ or HTTP messaging listener protocol for the resource specified in the subscription.
							</dd></dl></div><p>
					The <code class="literal">cloud-event-proxy</code> sidecar in the application pod receives the event from the PTP Operator-managed pod, unwraps the cloud events object to retrieve the data, and posts the event to the consumer application. The consumer application listens to the address specified in the resource qualifier and receives and processes the PTP event.
				</p></section><section class="section" id="cnf-configuring-the-ptp-fast-event-publisher_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.8.3. Configuring the PTP fast event notifications publisher</h3></div></div></div><p>
					To start using PTP fast event notifications for a network interface in your cluster, you must enable the fast event publisher in the PTP Operator <code class="literal">PtpOperatorConfig</code> custom resource (CR) and configure <code class="literal">ptpClockThreshold</code> values in a <code class="literal">PtpConfig</code> CR that you create.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift Container Platform CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							You have installed the PTP Operator.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Modify the default PTP Operator config to enable PTP fast events.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Save the following YAML in the <code class="literal">ptp-operatorconfig.yaml</code> file:
								</p><pre class="programlisting language-yaml">apiVersion: ptp.openshift.io/v1
kind: PtpOperatorConfig
metadata:
  name: default
  namespace: openshift-ptp
spec:
  daemonNodeSelector:
    node-role.kubernetes.io/worker: ""
  ptpEventConfig:
    enableEventPublisher: true <span id="CO39-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO39-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Set <code class="literal">enableEventPublisher</code> to <code class="literal">true</code> to enable PTP fast event notifications.
										</div></dd></dl></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								In OpenShift Container Platform 4.13 or later, you do not need to set the <code class="literal">spec.ptpEventConfig.transportHost</code> field in the <code class="literal">PtpOperatorConfig</code> resource when you use HTTP transport for PTP events. Set <code class="literal">transportHost</code> only when you use AMQP transport for PTP events.
							</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Update the <code class="literal">PtpOperatorConfig</code> CR:
								</p><pre class="programlisting language-terminal">$ oc apply -f ptp-operatorconfig.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">PtpConfig</code> custom resource (CR) for the PTP enabled interface, and set the required values for <code class="literal">ptpClockThreshold</code> and <code class="literal">ptp4lOpts</code>. The following YAML illustrates the required values that you must set in the <code class="literal">PtpConfig</code> CR:
						</p><pre class="programlisting language-yaml">spec:
  profile:
  - name: "profile1"
    interface: "enp5s0f0"
    ptp4lOpts: "-2 -s --summary_interval -4" <span id="CO40-1"><!--Empty--></span><span class="callout">1</span>
    phc2sysOpts: "-a -r -m -n 24 -N 8 -R 16" <span id="CO40-2"><!--Empty--></span><span class="callout">2</span>
    ptp4lConf: "" <span id="CO40-3"><!--Empty--></span><span class="callout">3</span>
    ptpClockThreshold: <span id="CO40-4"><!--Empty--></span><span class="callout">4</span>
      holdOverTimeout: 5
      maxOffsetThreshold: 100
      minOffsetThreshold: -100</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO40-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Append <code class="literal">--summary_interval -4</code> to use PTP fast events.
								</div></dd><dt><a href="#CO40-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Required <code class="literal">phc2sysOpts</code> values. <code class="literal">-m</code> prints messages to <code class="literal">stdout</code>. The <code class="literal">linuxptp-daemon</code> <code class="literal">DaemonSet</code> parses the logs and generates Prometheus metrics.
								</div></dd><dt><a href="#CO40-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify a string that contains the configuration to replace the default <code class="literal">/etc/ptp4l.conf</code> file. To use the default configuration, leave the field empty.
								</div></dd><dt><a href="#CO40-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Optional. If the <code class="literal">ptpClockThreshold</code> stanza is not present, default values are used for the <code class="literal">ptpClockThreshold</code> fields. The stanza shows default <code class="literal">ptpClockThreshold</code> values. The <code class="literal">ptpClockThreshold</code> values configure how long after the PTP master clock is disconnected before PTP events are triggered. <code class="literal">holdOverTimeout</code> is the time value in seconds before the PTP clock event state changes to <code class="literal">FREERUN</code> when the PTP master clock is disconnected. The <code class="literal">maxOffsetThreshold</code> and <code class="literal">minOffsetThreshold</code> settings configure offset values in nanoseconds that compare against the values for <code class="literal">CLOCK_REALTIME</code> (<code class="literal">phc2sys</code>) or master offset (<code class="literal">ptp4l</code>). When the <code class="literal">ptp4l</code> or <code class="literal">phc2sys</code> offset value is outside this range, the PTP clock state is set to <code class="literal">FREERUN</code>. When the offset value is within this range, the PTP clock state is set to <code class="literal">LOCKED</code>.
								</div></dd></dl></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For a complete example CR that configures <code class="literal">linuxptp</code> services as an ordinary clock with PTP fast events, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-linuxptp-services-as-ordinary-clock_using-ptp">Configuring linuxptp services as ordinary clock</a>.
						</li></ul></div></section><section class="section" id="cnf-migrating-from-amqp-to-http-transport_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.8.4. Migrating consumer applications to use HTTP transport for PTP or bare-metal events</h3></div></div></div><p>
					If you have previously deployed PTP or bare-metal events consumer applications, you need to update the applications to use HTTP message transport.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have logged in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							You have updated the PTP Operator or Bare Metal Event Relay to version 4.13+ which uses HTTP transport by default.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Update your events consumer application to use HTTP transport. Set the <code class="literal">http-event-publishers</code> variable for the cloud event sidecar deployment.
						</p><p class="simpara">
							For example, in a cluster with PTP events configured, the following YAML snippet illustrates a cloud event sidecar deployment:
						</p><pre class="programlisting language-yaml">containers:
  - name: cloud-event-sidecar
    image: cloud-event-sidecar
    args:
      - "--metrics-addr=127.0.0.1:9091"
      - "--store-path=/store"
      - "--transport-host=consumer-events-subscription-service.cloud-events.svc.cluster.local:9043"
      - "--http-event-publishers=ptp-event-publisher-service-NODE_NAME.openshift-ptp.svc.cluster.local:9043" <span id="CO41-1"><!--Empty--></span><span class="callout">1</span>
      - "--api-port=8089"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO41-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The PTP Operator automatically resolves <code class="literal">NODE_NAME</code> to the host that is generating the PTP events. For example, <code class="literal">compute-1.example.com</code>.
								</div></dd></dl></div><p class="simpara">
							In a cluster with bare-metal events configured, set the <code class="literal">http-event-publishers</code> field to <code class="literal">hw-event-publisher-service.openshift-bare-metal-events.svc.cluster.local:9043</code> in the cloud event sidecar deployment CR.
						</p></li><li class="listitem"><p class="simpara">
							Deploy the <code class="literal">consumer-events-subscription-service</code> service alongside the events consumer application. For example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/scrape: "true"
    service.alpha.openshift.io/serving-cert-secret-name: sidecar-consumer-secret
  name: consumer-events-subscription-service
  namespace: cloud-events
  labels:
    app: consumer-service
spec:
  ports:
    - name: sub-port
      port: 9043
  selector:
    app: consumer
  clusterIP: None
  sessionAffinity: None
  type: ClusterIP</pre></li></ol></div></section><section class="section" id="cnf-installing-amq-interconnect-messaging-bus_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.8.5. Installing the AMQ messaging bus</h3></div></div></div><p>
					To pass PTP fast event notifications between publisher and subscriber on a node, you can install and configure an AMQ messaging bus to run locally on the node. To use AMQ messaging, you must install the AMQ Interconnect Operator.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Use HTTP transport instead of AMQP for PTP and bare-metal events where possible. AMQ Interconnect is EOL from 30 June 2024. Extended life cycle support (ELS) for AMQ Interconnect ends 29 November 2029. For more information see, <a class="link" href="https://access.redhat.com/support/policy/updates/jboss_notes#p_Interconnect">Red Hat AMQ Interconnect support status</a>.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift Container Platform CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the AMQ Interconnect Operator to its own <code class="literal">amq-interconnect</code> namespace. See <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_amq/2021.q1/html/deploying_amq_interconnect_on_openshift/adding-operator-router-ocp">Adding the Red Hat Integration - AMQ Interconnect Operator</a>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check that the AMQ Interconnect Operator is available and the required pods are running:
						</p><pre class="programlisting language-terminal">$ oc get pods -n amq-interconnect</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                    READY   STATUS    RESTARTS   AGE
amq-interconnect-645db76c76-k8ghs       1/1     Running   0          23h
interconnect-operator-5cb5fc7cc-4v7qm   1/1     Running   0          23h</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check that the required <code class="literal">linuxptp-daemon</code> PTP event producer pods are running in the <code class="literal">openshift-ptp</code> namespace.
						</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-ptp</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                     READY   STATUS    RESTARTS       AGE
linuxptp-daemon-2t78p    3/3     Running   0              12h
linuxptp-daemon-k8n88    3/3     Running   0              12h</pre>

							</p></div></li></ol></div></section><section class="section" id="cnf-fast-event-notifications-api-refererence_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.8.6. Subscribing DU applications to PTP events REST API reference</h3></div></div></div><p>
					Use the PTP event notifications REST API to subscribe a distributed unit (DU) application to the PTP events that are generated on the parent node.
				</p><p>
					Subscribe applications to PTP events by using the resource address <code class="literal">/cluster/node/&lt;node_name&gt;/ptp</code>, where <code class="literal">&lt;node_name&gt;</code> is the cluster node running the DU application.
				</p><p>
					Deploy your <code class="literal">cloud-event-consumer</code> DU application container and <code class="literal">cloud-event-proxy</code> sidecar container in a separate DU application pod. The <code class="literal">cloud-event-consumer</code> DU application subscribes to the <code class="literal">cloud-event-proxy</code> container in the application pod.
				</p><p>
					Use the following API endpoints to subscribe the <code class="literal">cloud-event-consumer</code> DU application to PTP events posted by the <code class="literal">cloud-event-proxy</code> container at <code class="literal">http://localhost:8089/api/ocloudNotifications/v1/</code> in the DU application pod:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							<code class="literal">/api/ocloudNotifications/v1/subscriptions</code>
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<code class="literal">POST</code>: Creates a new subscription
								</li><li class="listitem">
									<code class="literal">GET</code>: Retrieves a list of subscriptions
								</li></ul></div></li><li class="listitem"><p class="simpara">
							<code class="literal">/api/ocloudNotifications/v1/subscriptions/&lt;subscription_id&gt;</code>
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<code class="literal">GET</code>: Returns details for the specified subscription ID
								</li></ul></div></li><li class="listitem"><p class="simpara">
							<code class="literal">api/ocloudNotifications/v1/subscriptions/status/&lt;subscription_id&gt;</code>
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<code class="literal">PUT</code>: Creates a new status ping request for the specified subscription ID
								</li></ul></div></li><li class="listitem"><p class="simpara">
							<code class="literal">/api/ocloudNotifications/v1/health</code>
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<code class="literal">GET</code>: Returns the health status of <code class="literal">ocloudNotifications</code> API
								</li></ul></div></li><li class="listitem"><p class="simpara">
							<code class="literal">api/ocloudNotifications/v1/publishers</code>
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<code class="literal">GET</code>: Returns an array of <code class="literal">os-clock-sync-state</code>, <code class="literal">ptp-clock-class-change</code>, and <code class="literal">lock-state</code> messages for the cluster node
								</li></ul></div></li><li class="listitem"><p class="simpara">
							<code class="literal">/api/ocloudnotifications/v1/&lt;resource_address&gt;/CurrentState</code>
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<code class="literal">GET</code>: Returns the current state of one the following event types: <code class="literal">os-clock-sync-state</code>, <code class="literal">ptp-clock-class-change</code>, or <code class="literal">lock-state</code> events
								</li></ul></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						<code class="literal">9089</code> is the default port for the <code class="literal">cloud-event-consumer</code> container deployed in the application pod. You can configure a different port for your DU application as required.
					</p></div></div><section class="section" id="api-ocloudnotifications-v1-subscriptions"><div class="titlepage"><div><div><h4 class="title">19.8.6.1. api/ocloudNotifications/v1/subscriptions</h4></div></div></div><h6 id="http-method">HTTP method</h6><p>
						<code class="literal">GET api/ocloudNotifications/v1/subscriptions</code>
					</p><h5 id="description">Description</h5><p>
						Returns a list of subscriptions. If subscriptions exist, a <code class="literal">200 OK</code> status code is returned along with the list of subscriptions.
					</p><div class="formalpara"><p class="title"><strong>Example API response</strong></p><p>
							
<pre class="programlisting language-json">[
 {
  "id": "75b1ad8f-c807-4c23-acf5-56f4b7ee3826",
  "endpointUri": "http://localhost:9089/event",
  "uriLocation": "http://localhost:8089/api/ocloudNotifications/v1/subscriptions/75b1ad8f-c807-4c23-acf5-56f4b7ee3826",
  "resource": "/cluster/node/compute-1.example.com/ptp"
 }
]</pre>

						</p></div><h6 id="http-method-2">HTTP method</h6><p>
						<code class="literal">POST api/ocloudNotifications/v1/subscriptions</code>
					</p><h5 id="description-2">Description</h5><p>
						Creates a new subscription. If a subscription is successfully created, or if it already exists, a <code class="literal">201 Created</code> status code is returned.
					</p><div class="table" id="idm140587155620784"><p class="title"><strong>Table 19.5. Query parameters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587155615952" scope="col">Parameter</th><th align="left" valign="top" id="idm140587144594400" scope="col">Type</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587155615952"> <p>
										subscription
									</p>
									 </td><td align="left" valign="top" headers="idm140587144594400"> <p>
										data
									</p>
									 </td></tr></tbody></table></div></div><div class="formalpara"><p class="title"><strong>Example payload</strong></p><p>
							
<pre class="programlisting language-json">{
  "uriLocation": "http://localhost:8089/api/ocloudNotifications/v1/subscriptions",
  "resource": "/cluster/node/compute-1.example.com/ptp"
}</pre>

						</p></div></section><section class="section" id="api-ocloudnotifications-v1-subscriptions-subscription_id"><div class="titlepage"><div><div><h4 class="title">19.8.6.2. api/ocloudNotifications/v1/subscriptions/&lt;subscription_id&gt;</h4></div></div></div><h6 id="http-method-3">HTTP method</h6><p>
						<code class="literal">GET api/ocloudNotifications/v1/subscriptions/&lt;subscription_id&gt;</code>
					</p><h5 id="description-3">Description</h5><p>
						Returns details for the subscription with ID <code class="literal">&lt;subscription_id&gt;</code>
					</p><div class="table" id="idm140587121540704"><p class="title"><strong>Table 19.6. Query parameters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587123943344" scope="col">Parameter</th><th align="left" valign="top" id="idm140587123942256" scope="col">Type</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587123943344"> <p>
										<code class="literal">&lt;subscription_id&gt;</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587123942256"> <p>
										string
									</p>
									 </td></tr></tbody></table></div></div><div class="formalpara"><p class="title"><strong>Example API response</strong></p><p>
							
<pre class="programlisting language-json">{
  "id":"48210fb3-45be-4ce0-aa9b-41a0e58730ab",
  "endpointUri": "http://localhost:9089/event",
  "uriLocation":"http://localhost:8089/api/ocloudNotifications/v1/subscriptions/48210fb3-45be-4ce0-aa9b-41a0e58730ab",
  "resource":"/cluster/node/compute-1.example.com/ptp"
}</pre>

						</p></div></section><section class="section" id="api-ocloudnotifications-v1-subscriptions-status-subscription_id"><div class="titlepage"><div><div><h4 class="title">19.8.6.3. api/ocloudNotifications/v1/subscriptions/status/&lt;subscription_id&gt;</h4></div></div></div><h6 id="http-method-4">HTTP method</h6><p>
						<code class="literal">PUT api/ocloudNotifications/v1/subscriptions/status/&lt;subscription_id&gt;</code>
					</p><h5 id="description-4">Description</h5><p>
						Creates a new status ping request for subscription with ID <code class="literal">&lt;subscription_id&gt;</code>. If a subscription is present, the status request is successful and a <code class="literal">202 Accepted</code> status code is returned.
					</p><div class="table" id="idm140587161873568"><p class="title"><strong>Table 19.7. Query parameters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587161868848" scope="col">Parameter</th><th align="left" valign="top" id="idm140587161867760" scope="col">Type</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587161868848"> <p>
										<code class="literal">&lt;subscription_id&gt;</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587161867760"> <p>
										string
									</p>
									 </td></tr></tbody></table></div></div><div class="formalpara"><p class="title"><strong>Example API response</strong></p><p>
							
<pre class="programlisting language-json">{"status":"ping sent"}</pre>

						</p></div></section><section class="section" id="api-ocloudnotifications-v1-health"><div class="titlepage"><div><div><h4 class="title">19.8.6.4. api/ocloudNotifications/v1/health/</h4></div></div></div><h6 id="http-method-5">HTTP method</h6><p>
						<code class="literal">GET api/ocloudNotifications/v1/health/</code>
					</p><h5 id="description-5">Description</h5><p>
						Returns the health status for the <code class="literal">ocloudNotifications</code> REST API.
					</p><div class="formalpara"><p class="title"><strong>Example API response</strong></p><p>
							
<pre class="programlisting language-terminal">OK</pre>

						</p></div></section><section class="section" id="api-ocloudnotifications-v1-publishers"><div class="titlepage"><div><div><h4 class="title">19.8.6.5. api/ocloudNotifications/v1/publishers</h4></div></div></div><h6 id="http-method-6">HTTP method</h6><p>
						<code class="literal">GET api/ocloudNotifications/v1/publishers</code>
					</p><h5 id="description-6">Description</h5><p>
						Returns an array of <code class="literal">os-clock-sync-state</code>, <code class="literal">ptp-clock-class-change</code>, and <code class="literal">lock-state</code> details for the cluster node. The system generates notifications when the relevant equipment state changes.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<code class="literal">os-clock-sync-state</code> notifications describe the host operating system clock synchronization state. Can be in <code class="literal">LOCKED</code> or <code class="literal">FREERUN</code> state.
							</li><li class="listitem">
								<code class="literal">ptp-clock-class-change</code> notifications describe the current state of the PTP clock class.
							</li><li class="listitem">
								<code class="literal">lock-state</code> notifications describe the current status of the PTP equipment lock state. Can be in <code class="literal">LOCKED</code>, <code class="literal">HOLDOVER</code> or <code class="literal">FREERUN</code> state.
							</li></ul></div><div class="formalpara"><p class="title"><strong>Example API response</strong></p><p>
							
<pre class="programlisting language-json">[
  {
    "id": "0fa415ae-a3cf-4299-876a-589438bacf75",
    "endpointUri": "http://localhost:9085/api/ocloudNotifications/v1/dummy",
    "uriLocation": "http://localhost:9085/api/ocloudNotifications/v1/publishers/0fa415ae-a3cf-4299-876a-589438bacf75",
    "resource": "/cluster/node/compute-1.example.com/sync/sync-status/os-clock-sync-state"
  },
  {
    "id": "28cd82df-8436-4f50-bbd9-7a9742828a71",
    "endpointUri": "http://localhost:9085/api/ocloudNotifications/v1/dummy",
    "uriLocation": "http://localhost:9085/api/ocloudNotifications/v1/publishers/28cd82df-8436-4f50-bbd9-7a9742828a71",
    "resource": "/cluster/node/compute-1.example.com/sync/ptp-status/ptp-clock-class-change"
  },
  {
    "id": "44aa480d-7347-48b0-a5b0-e0af01fa9677",
    "endpointUri": "http://localhost:9085/api/ocloudNotifications/v1/dummy",
    "uriLocation": "http://localhost:9085/api/ocloudNotifications/v1/publishers/44aa480d-7347-48b0-a5b0-e0af01fa9677",
    "resource": "/cluster/node/compute-1.example.com/sync/ptp-status/lock-state"
  }
]</pre>

						</p></div><p>
						You can find <code class="literal">os-clock-sync-state</code>, <code class="literal">ptp-clock-class-change</code> and <code class="literal">lock-state</code> events in the logs for the <code class="literal">cloud-event-proxy</code> container. For example:
					</p><pre class="programlisting language-terminal">$ oc logs -f linuxptp-daemon-cvgr6 -n openshift-ptp -c cloud-event-proxy</pre><div class="formalpara"><p class="title"><strong>Example os-clock-sync-state event</strong></p><p>
							
<pre class="programlisting language-json">{
   "id":"c8a784d1-5f4a-4c16-9a81-a3b4313affe5",
   "type":"event.sync.sync-status.os-clock-sync-state-change",
   "source":"/cluster/compute-1.example.com/ptp/CLOCK_REALTIME",
   "dataContentType":"application/json",
   "time":"2022-05-06T15:31:23.906277159Z",
   "data":{
      "version":"v1",
      "values":[
         {
            "resource":"/sync/sync-status/os-clock-sync-state",
            "dataType":"notification",
            "valueType":"enumeration",
            "value":"LOCKED"
         },
         {
            "resource":"/sync/sync-status/os-clock-sync-state",
            "dataType":"metric",
            "valueType":"decimal64.3",
            "value":"-53"
         }
      ]
   }
}</pre>

						</p></div><div class="formalpara"><p class="title"><strong>Example ptp-clock-class-change event</strong></p><p>
							
<pre class="programlisting language-json">{
   "id":"69eddb52-1650-4e56-b325-86d44688d02b",
   "type":"event.sync.ptp-status.ptp-clock-class-change",
   "source":"/cluster/compute-1.example.com/ptp/ens2fx/master",
   "dataContentType":"application/json",
   "time":"2022-05-06T15:31:23.147100033Z",
   "data":{
      "version":"v1",
      "values":[
         {
            "resource":"/sync/ptp-status/ptp-clock-class-change",
            "dataType":"metric",
            "valueType":"decimal64.3",
            "value":"135"
         }
      ]
   }
}</pre>

						</p></div><div class="formalpara"><p class="title"><strong>Example lock-state event</strong></p><p>
							
<pre class="programlisting language-json">{
   "id":"305ec18b-1472-47b3-aadd-8f37933249a9",
   "type":"event.sync.ptp-status.ptp-state-change",
   "source":"/cluster/compute-1.example.com/ptp/ens2fx/master",
   "dataContentType":"application/json",
   "time":"2022-05-06T15:31:23.467684081Z",
   "data":{
      "version":"v1",
      "values":[
         {
            "resource":"/sync/ptp-status/lock-state",
            "dataType":"notification",
            "valueType":"enumeration",
            "value":"LOCKED"
         },
         {
            "resource":"/sync/ptp-status/lock-state",
            "dataType":"metric",
            "valueType":"decimal64.3",
            "value":"62"
         }
      ]
   }
}</pre>

						</p></div></section><section class="section" id="api-ocloudnotifications-v1-resource_address-currentstate"><div class="titlepage"><div><div><h4 class="title">19.8.6.6. /api/ocloudnotifications/v1/&lt;resource_address&gt;/CurrentState</h4></div></div></div><h6 id="http-method-7">HTTP method</h6><p>
						<code class="literal">GET api/ocloudNotifications/v1/cluster/node/&lt;node_name&gt;/sync/ptp-status/lock-state/CurrentState</code>
					</p><p>
						<code class="literal">GET api/ocloudNotifications/v1/cluster/node/&lt;node_name&gt;/sync/sync-status/os-clock-sync-state/CurrentState</code>
					</p><p>
						<code class="literal">GET api/ocloudNotifications/v1/cluster/node/&lt;node_name&gt;/sync/ptp-status/ptp-clock-class-change/CurrentState</code>
					</p><h5 id="description-7">Description</h5><p>
						Configure the <code class="literal">CurrentState</code> API endpoint to return the current state of the <code class="literal">os-clock-sync-state</code>, <code class="literal">ptp-clock-class-change</code>, or <code class="literal">lock-state</code> events for the cluster node.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<code class="literal">os-clock-sync-state</code> notifications describe the host operating system clock synchronization state. Can be in <code class="literal">LOCKED</code> or <code class="literal">FREERUN</code> state.
							</li><li class="listitem">
								<code class="literal">ptp-clock-class-change</code> notifications describe the current state of the PTP clock class.
							</li><li class="listitem">
								<code class="literal">lock-state</code> notifications describe the current status of the PTP equipment lock state. Can be in <code class="literal">LOCKED</code>, <code class="literal">HOLDOVER</code> or <code class="literal">FREERUN</code> state.
							</li></ul></div><div class="table" id="idm140587175092000"><p class="title"><strong>Table 19.8. Query parameters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587168376704" scope="col">Parameter</th><th align="left" valign="top" id="idm140587168375616" scope="col">Type</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587168376704"> <p>
										<code class="literal">&lt;resource_address&gt;</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587168375616"> <p>
										string
									</p>
									 </td></tr></tbody></table></div></div><div class="formalpara"><p class="title"><strong>Example lock-state API response</strong></p><p>
							
<pre class="programlisting language-json">{
  "id": "c1ac3aa5-1195-4786-84f8-da0ea4462921",
  "type": "event.sync.ptp-status.ptp-state-change",
  "source": "/cluster/node/compute-1.example.com/sync/ptp-status/lock-state",
  "dataContentType": "application/json",
  "time": "2023-01-10T02:41:57.094981478Z",
  "data": {
    "version": "v1",
    "values": [
      {
        "resource": "/cluster/node/compute-1.example.com/ens5fx/master",
        "dataType": "notification",
        "valueType": "enumeration",
        "value": "LOCKED"
      },
      {
        "resource": "/cluster/node/compute-1.example.com/ens5fx/master",
        "dataType": "metric",
        "valueType": "decimal64.3",
        "value": "29"
      }
    ]
  }
}</pre>

						</p></div><div class="formalpara"><p class="title"><strong>Example os-clock-sync-state API response</strong></p><p>
							
<pre class="programlisting language-json">{
  "specversion": "0.3",
  "id": "4f51fe99-feaa-4e66-9112-66c5c9b9afcb",
  "source": "/cluster/node/compute-1.example.com/sync/sync-status/os-clock-sync-state",
  "type": "event.sync.sync-status.os-clock-sync-state-change",
  "subject": "/cluster/node/compute-1.example.com/sync/sync-status/os-clock-sync-state",
  "datacontenttype": "application/json",
  "time": "2022-11-29T17:44:22.202Z",
  "data": {
    "version": "v1",
    "values": [
      {
        "resource": "/cluster/node/compute-1.example.com/CLOCK_REALTIME",
        "dataType": "notification",
        "valueType": "enumeration",
        "value": "LOCKED"
      },
      {
        "resource": "/cluster/node/compute-1.example.com/CLOCK_REALTIME",
        "dataType": "metric",
        "valueType": "decimal64.3",
        "value": "27"
      }
    ]
  }
}</pre>

						</p></div><div class="formalpara"><p class="title"><strong>Example ptp-clock-class-change API response</strong></p><p>
							
<pre class="programlisting language-json">{
  "id": "064c9e67-5ad4-4afb-98ff-189c6aa9c205",
  "type": "event.sync.ptp-status.ptp-clock-class-change",
  "source": "/cluster/node/compute-1.example.com/sync/ptp-status/ptp-clock-class-change",
  "dataContentType": "application/json",
  "time": "2023-01-10T02:41:56.785673989Z",
  "data": {
    "version": "v1",
    "values": [
      {
        "resource": "/cluster/node/compute-1.example.com/ens5fx/master",
        "dataType": "metric",
        "valueType": "decimal64.3",
        "value": "165"
      }
    ]
  }
}</pre>

						</p></div></section></section><section class="section" id="cnf-monitoring-fast-events-metrics_using-ptp"><div class="titlepage"><div><div><h3 class="title">19.8.7. Monitoring PTP fast event metrics</h3></div></div></div><p>
					You can monitor PTP fast events metrics from cluster nodes where the <code class="literal">linuxptp-daemon</code> is running. You can also monitor PTP fast event metrics in the OpenShift Container Platform web console by using the pre-configured and self-updating Prometheus monitoring stack.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift Container Platform CLI <code class="literal">oc</code>.
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							Install and configure the PTP Operator on a node with PTP-capable hardware.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check for exposed PTP metrics on any node where the <code class="literal">linuxptp-daemon</code> is running. For example, run the following command:
						</p><pre class="programlisting language-terminal">$ curl http://&lt;node_name&gt;:9091/metrics</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen"># HELP openshift_ptp_clock_state 0 = FREERUN, 1 = LOCKED, 2 = HOLDOVER
# TYPE openshift_ptp_clock_state gauge
openshift_ptp_clock_state{iface="ens1fx",node="compute-1.example.com",process="ptp4l"} 1
openshift_ptp_clock_state{iface="ens3fx",node="compute-1.example.com",process="ptp4l"} 1
openshift_ptp_clock_state{iface="ens5fx",node="compute-1.example.com",process="ptp4l"} 1
openshift_ptp_clock_state{iface="ens7fx",node="compute-1.example.com",process="ptp4l"} 1
# HELP openshift_ptp_delay_ns
# TYPE openshift_ptp_delay_ns gauge
openshift_ptp_delay_ns{from="master",iface="ens1fx",node="compute-1.example.com",process="ptp4l"} 842
openshift_ptp_delay_ns{from="master",iface="ens3fx",node="compute-1.example.com",process="ptp4l"} 480
openshift_ptp_delay_ns{from="master",iface="ens5fx",node="compute-1.example.com",process="ptp4l"} 584
openshift_ptp_delay_ns{from="master",iface="ens7fx",node="compute-1.example.com",process="ptp4l"} 482
openshift_ptp_delay_ns{from="phc",iface="CLOCK_REALTIME",node="compute-1.example.com",process="phc2sys"} 547
# HELP openshift_ptp_offset_ns
# TYPE openshift_ptp_offset_ns gauge
openshift_ptp_offset_ns{from="master",iface="ens1fx",node="compute-1.example.com",process="ptp4l"} -2
openshift_ptp_offset_ns{from="master",iface="ens3fx",node="compute-1.example.com",process="ptp4l"} -44
openshift_ptp_offset_ns{from="master",iface="ens5fx",node="compute-1.example.com",process="ptp4l"} -8
openshift_ptp_offset_ns{from="master",iface="ens7fx",node="compute-1.example.com",process="ptp4l"} 3
openshift_ptp_offset_ns{from="phc",iface="CLOCK_REALTIME",node="compute-1.example.com",process="phc2sys"} 12</pre>

							</p></div></li><li class="listitem">
							To view the PTP event in the OpenShift Container Platform web console, copy the name of the PTP metric you want to query, for example, <code class="literal">openshift_ptp_offset_ns</code>.
						</li><li class="listitem">
							In the OpenShift Container Platform web console, click <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Metrics</strong></span>.
						</li><li class="listitem">
							Paste the PTP metric name into the <span class="strong strong"><strong>Expression</strong></span> field, and click <span class="strong strong"><strong>Run queries</strong></span>.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/monitoring/#managing-metrics">Managing metrics</a>
						</li></ul></div></section></section></section><section class="chapter" id="ptp-cloud-events-consumer-dev-reference"><div class="titlepage"><div><div><h1 class="title">Chapter 20. Developing PTP events consumer applications</h1></div></div></div><p>
			When developing consumer applications that make use of Precision Time Protocol (PTP) events on a bare-metal cluster node, you need to deploy your consumer application and a <code class="literal">cloud-event-proxy</code> container in a separate application pod. The <code class="literal">cloud-event-proxy</code> container receives the events from the PTP Operator pod and passes it to the consumer application. The consumer application subscribes to the events posted in the <code class="literal">cloud-event-proxy</code> container by using a REST API.
		</p><p>
			For more information about deploying PTP events applications, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#cnf-about-ptp-fast-event-notifications-framework_using-ptp">About the PTP fast event notifications framework</a>.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				The following information provides general guidance for developing consumer applications that use PTP events. A complete events consumer application example is outside the scope of this information.
			</p></div></div><section class="section" id="ptp-events-consumer-application_ptp-consumer"><div class="titlepage"><div><div><h2 class="title">20.1. PTP events consumer application reference</h2></div></div></div><p>
				PTP event consumer applications require the following features:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						A web service running with a <code class="literal">POST</code> handler to receive the cloud native PTP events JSON payload
					</li><li class="listitem">
						A <code class="literal">createSubscription</code> function to subscribe to the PTP events producer
					</li><li class="listitem">
						A <code class="literal">getCurrentState</code> function to poll the current state of the PTP events producer
					</li></ol></div><p>
				The following example Go snippets illustrate these requirements:
			</p><div class="formalpara"><p class="title"><strong>Example PTP events consumer server function in Go</strong></p><p>
					
<pre class="programlisting language-go">func server() {
  http.HandleFunc("/event", getEvent)
  http.ListenAndServe("localhost:8989", nil)
}

func getEvent(w http.ResponseWriter, req *http.Request) {
  defer req.Body.Close()
  bodyBytes, err := io.ReadAll(req.Body)
  if err != nil {
    log.Errorf("error reading event %v", err)
  }
  e := string(bodyBytes)
  if e != "" {
    processEvent(bodyBytes)
    log.Infof("received event %s", string(bodyBytes))
  } else {
    w.WriteHeader(http.StatusNoContent)
  }
}</pre>

				</p></div><div class="formalpara"><p class="title"><strong>Example PTP events createSubscription function in Go</strong></p><p>
					
<pre class="programlisting language-go">import (
"github.com/redhat-cne/sdk-go/pkg/pubsub"
"github.com/redhat-cne/sdk-go/pkg/types"
v1pubsub "github.com/redhat-cne/sdk-go/v1/pubsub"
)

// Subscribe to PTP events using REST API
s1,_:=createsubscription("/cluster/node/&lt;node_name&gt;/sync/sync-status/os-clock-sync-state") <span id="CO42-1"><!--Empty--></span><span class="callout">1</span>
s2,_:=createsubscription("/cluster/node/&lt;node_name&gt;/sync/ptp-status/ptp-clock-class-change")
s3,_:=createsubscription("/cluster/node/&lt;node_name&gt;/sync/ptp-status/lock-state")

// Create PTP event subscriptions POST
func createSubscription(resourceAddress string) (sub pubsub.PubSub, err error) {
  var status int
      apiPath:= "/api/ocloudNotifications/v1/"
      localAPIAddr:=localhost:8989 // vDU service API address
      apiAddr:= "localhost:8089" // event framework API address

  subURL := &amp;types.URI{URL: url.URL{Scheme: "http",
    Host: apiAddr
    Path: fmt.Sprintf("%s%s", apiPath, "subscriptions")}}
  endpointURL := &amp;types.URI{URL: url.URL{Scheme: "http",
    Host: localAPIAddr,
    Path: "event"}}

  sub = v1pubsub.NewPubSub(endpointURL, resourceAddress)
  var subB []byte

  if subB, err = json.Marshal(&amp;sub); err == nil {
    rc := restclient.New()
    if status, subB = rc.PostWithReturn(subURL, subB); status != http.StatusCreated {
      err = fmt.Errorf("error in subscription creation api at %s, returned status %d", subURL, status)
    } else {
      err = json.Unmarshal(subB, &amp;sub)
    }
  } else {
    err = fmt.Errorf("failed to marshal subscription for %s", resourceAddress)
  }
  return
}</pre>

				</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO42-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						Replace <code class="literal">&lt;node_name&gt;</code> with the FQDN of the node that is generating the PTP events. For example, <code class="literal">compute-1.example.com</code>.
					</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example PTP events consumer getCurrentState function in Go</strong></p><p>
					
<pre class="programlisting language-go">//Get PTP event state for the resource
func getCurrentState(resource string) {
  //Create publisher
  url := &amp;types.URI{URL: url.URL{Scheme: "http",
    Host: localhost:8989,
    Path: fmt.SPrintf("/api/ocloudNotifications/v1/%s/CurrentState",resource}}
  rc := restclient.New()
  status, event := rc.Get(url)
  if status != http.StatusOK {
    log.Errorf("CurrentState:error %d from url %s, %s", status, url.String(), event)
  } else {
    log.Debugf("Got CurrentState: %s ", event)
  }
}</pre>

				</p></div></section><section class="section" id="ptp-reference-deployment-and-service-crs_ptp-consumer"><div class="titlepage"><div><div><h2 class="title">20.2. Reference cloud-event-proxy deployment and service CRs</h2></div></div></div><p>
				Use the following example <code class="literal">cloud-event-proxy</code> deployment and subscriber service CRs as a reference when deploying your PTP events consumer application.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Use HTTP transport instead of AMQP for PTP and bare-metal events where possible. AMQ Interconnect is EOL from 30 June 2024. Extended life cycle support (ELS) for AMQ Interconnect ends 30 November 2030. For more information see, <a class="link" href="https://access.redhat.com/support/policy/updates/jboss_notes#p_Interconnect">Red Hat AMQ Interconnect support status</a>.
				</p></div></div><div class="formalpara"><p class="title"><strong>Reference cloud-event-proxy deployment with HTTP transport</strong></p><p>
					
<pre class="programlisting language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: event-consumer-deployment
  namespace: &lt;namespace&gt;
  labels:
    app: consumer
spec:
  replicas: 1
  selector:
    matchLabels:
      app: consumer
  template:
    metadata:
      labels:
        app: consumer
    spec:
      serviceAccountName: sidecar-consumer-sa
      containers:
        - name: event-subscriber
          image: event-subscriber-app
        - name: cloud-event-proxy-as-sidecar
          image: openshift4/ose-cloud-event-proxy
          args:
            - "--metrics-addr=127.0.0.1:9091"
            - "--store-path=/store"
            - "--transport-host=consumer-events-subscription-service.cloud-events.svc.cluster.local:9043"
            - "--http-event-publishers=ptp-event-publisher-service-NODE_NAME.openshift-ptp.svc.cluster.local:9043"
            - "--api-port=8089"
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: NODE_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
              volumeMounts:
                - name: pubsubstore
                  mountPath: /store
          ports:
            - name: metrics-port
              containerPort: 9091
            - name: sub-port
              containerPort: 9043
          volumes:
            - name: pubsubstore
              emptyDir: {}</pre>

				</p></div><div class="formalpara"><p class="title"><strong>Reference cloud-event-proxy deployment with AMQ transport</strong></p><p>
					
<pre class="programlisting language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: cloud-event-proxy-sidecar
  namespace: cloud-events
  labels:
    app: cloud-event-proxy
spec:
  selector:
    matchLabels:
      app: cloud-event-proxy
  template:
    metadata:
      labels:
        app: cloud-event-proxy
    spec:
      nodeSelector:
        node-role.kubernetes.io/worker: ""
      containers:
        - name: cloud-event-sidecar
          image: openshift4/ose-cloud-event-proxy
          args:
            - "--metrics-addr=127.0.0.1:9091"
            - "--store-path=/store"
            - "--transport-host=amqp://router.router.svc.cluster.local"
            - "--api-port=8089"
          env:
            - name: &lt;node_name&gt;
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: &lt;node_ip&gt;
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
          volumeMounts:
            - name: pubsubstore
              mountPath: /store
          ports:
            - name: metrics-port
              containerPort: 9091
            - name: sub-port
              containerPort: 9043
          volumes:
            - name: pubsubstore
              emptyDir: {}</pre>

				</p></div><div class="formalpara"><p class="title"><strong>Reference cloud-event-proxy subscriber service</strong></p><p>
					
<pre class="programlisting language-yaml">apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/scrape: "true"
    service.alpha.openshift.io/serving-cert-secret-name: sidecar-consumer-secret
  name: consumer-events-subscription-service
  namespace: cloud-events
  labels:
    app: consumer-service
spec:
  ports:
    - name: sub-port
      port: 9043
  selector:
    app: consumer
  clusterIP: None
  sessionAffinity: None
  type: ClusterIP</pre>

				</p></div></section><section class="section" id="ptp-cloud-event-proxy-sidecar-api_ptp-consumer"><div class="titlepage"><div><div><h2 class="title">20.3. PTP events available from the cloud-event-proxy sidecar REST API</h2></div></div></div><p>
				PTP events consumer applications can poll the PTP events producer for the following PTP timing events.
			</p><div class="table" id="idm140587158074848"><p class="title"><strong>Table 20.1. PTP events available from the cloud-event-proxy sidecar</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587159075488" scope="col">Resource URI</th><th align="left" valign="top" id="idm140587159074400" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587159075488"> <p>
								<code class="literal">/cluster/node/&lt;node_name&gt;/sync/ptp-status/lock-state</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587159074400"> <p>
								Describes the current status of the PTP equipment lock state. Can be in <code class="literal">LOCKED</code>, <code class="literal">HOLDOVER</code>, or <code class="literal">FREERUN</code> state.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587159075488"> <p>
								<code class="literal">/cluster/node/&lt;node_name&gt;/sync/sync-status/os-clock-sync-state</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587159074400"> <p>
								Describes the host operating system clock synchronization state. Can be in <code class="literal">LOCKED</code> or <code class="literal">FREERUN</code> state.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587159075488"> <p>
								<code class="literal">/cluster/node/&lt;node_name&gt;/sync/ptp-status/ptp-clock-class-change</code>
							</p>
							 </td><td align="left" valign="top" headers="idm140587159074400"> <p>
								Describes the current state of the PTP clock class.
							</p>
							 </td></tr></tbody></table></div></div></section><section class="section" id="ptp-subscribing-consumer-app-to-events_ptp-consumer"><div class="titlepage"><div><div><h2 class="title">20.4. Subscribing the consumer application to PTP events</h2></div></div></div><p>
				Before the PTP events consumer application can poll for events, you need to subscribe the application to the event producer.
			</p><section class="section" id="ptp-sub-lock-state-events_ptp-consumer"><div class="titlepage"><div><div><h3 class="title">20.4.1. Subscribing to PTP lock-state events</h3></div></div></div><p>
					To create a subscription for PTP <code class="literal">lock-state</code> events, send a <code class="literal">POST</code> action to the cloud event API at <code class="literal">http://localhost:8081/api/ocloudNotifications/v1/subscriptions</code> with the following payload:
				</p><pre class="programlisting language-json">{
"endpointUri": "http://localhost:8989/event",
"resource": "/cluster/node/&lt;node_name&gt;/sync/ptp-status/lock-state",
}</pre><div class="formalpara"><p class="title"><strong>Example response</strong></p><p>
						
<pre class="programlisting language-json">{
"id": "e23473d9-ba18-4f78-946e-401a0caeff90",
"endpointUri": "http://localhost:8989/event",
"uriLocation": "http://localhost:8089/api/ocloudNotifications/v1/subscriptions/e23473d9-ba18-4f78-946e-401a0caeff90",
"resource": "/cluster/node/&lt;node_name&gt;/sync/ptp-status/lock-state",
}</pre>

					</p></div></section><section class="section" id="ptp-sub-os-clock-sync-state_ptp-consumer"><div class="titlepage"><div><div><h3 class="title">20.4.2. Subscribing to PTP os-clock-sync-state events</h3></div></div></div><p>
					To create a subscription for PTP <code class="literal">os-clock-sync-state</code> events, send a <code class="literal">POST</code> action to the cloud event API at <code class="literal">http://localhost:8081/api/ocloudNotifications/v1/subscriptions</code> with the following payload:
				</p><pre class="programlisting language-json">{
"endpointUri": "http://localhost:8989/event",
"resource": "/cluster/node/&lt;node_name&gt;/sync/sync-status/os-clock-sync-state",
}</pre><div class="formalpara"><p class="title"><strong>Example response</strong></p><p>
						
<pre class="programlisting language-json">{
"id": "e23473d9-ba18-4f78-946e-401a0caeff90",
"endpointUri": "http://localhost:8989/event",
"uriLocation": "http://localhost:8089/api/ocloudNotifications/v1/subscriptions/e23473d9-ba18-4f78-946e-401a0caeff90",
"resource": "/cluster/node/&lt;node_name&gt;/sync/sync-status/os-clock-sync-state",
}</pre>

					</p></div></section><section class="section" id="ptp-sub-ptp-clock-class-change_ptp-consumer"><div class="titlepage"><div><div><h3 class="title">20.4.3. Subscribing to PTP ptp-clock-class-change events</h3></div></div></div><p>
					To create a subscription for PTP <code class="literal">ptp-clock-class-change</code> events, send a <code class="literal">POST</code> action to the cloud event API at <code class="literal">http://localhost:8081/api/ocloudNotifications/v1/subscriptions</code> with the following payload:
				</p><pre class="programlisting language-json">{
"endpointUri": "http://localhost:8989/event",
"resource": "/cluster/node/&lt;node_name&gt;/sync/ptp-status/ptp-clock-class-change",
}</pre><div class="formalpara"><p class="title"><strong>Example response</strong></p><p>
						
<pre class="programlisting language-json">{
"id": "e23473d9-ba18-4f78-946e-401a0caeff90",
"endpointUri": "http://localhost:8989/event",
"uriLocation": "http://localhost:8089/api/ocloudNotifications/v1/subscriptions/e23473d9-ba18-4f78-946e-401a0caeff90",
"resource": "/cluster/node/&lt;node_name&gt;/sync/ptp-status/ptp-clock-class-change",
}</pre>

					</p></div></section></section><section class="section" id="ptp-getting-the-current-ptp-clock-status_ptp-consumer"><div class="titlepage"><div><div><h2 class="title">20.5. Getting the current PTP clock status</h2></div></div></div><p>
				To get the current PTP status for the node, send a <code class="literal">GET</code> action to one of the following event REST APIs:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">http://localhost:8081/api/ocloudNotifications/v1/cluster/node/&lt;node_name&gt;/sync/ptp-status/lock-state/CurrentState</code>
					</li><li class="listitem">
						<code class="literal">http://localhost:8081/api/ocloudNotifications/v1/cluster/node/&lt;node_name&gt;/sync/sync-status/os-clock-sync-state/CurrentState</code>
					</li><li class="listitem">
						<code class="literal">http://localhost:8081/api/ocloudNotifications/v1/cluster/node/&lt;node_name&gt;/sync/ptp-status/ptp-clock-class-change/CurrentState</code>
					</li></ul></div><p>
				The response is a cloud native event JSON object. For example:
			</p><div class="formalpara"><p class="title"><strong>Example lock-state API response</strong></p><p>
					
<pre class="programlisting language-json">{
  "id": "c1ac3aa5-1195-4786-84f8-da0ea4462921",
  "type": "event.sync.ptp-status.ptp-state-change",
  "source": "/cluster/node/compute-1.example.com/sync/ptp-status/lock-state",
  "dataContentType": "application/json",
  "time": "2023-01-10T02:41:57.094981478Z",
  "data": {
    "version": "v1",
    "values": [
      {
        "resource": "/cluster/node/compute-1.example.com/ens5fx/master",
        "dataType": "notification",
        "valueType": "enumeration",
        "value": "LOCKED"
      },
      {
        "resource": "/cluster/node/compute-1.example.com/ens5fx/master",
        "dataType": "metric",
        "valueType": "decimal64.3",
        "value": "29"
      }
    ]
  }
}</pre>

				</p></div></section><section class="section" id="ptp-verifying-events-consumer-app-is-receiving-events_ptp-consumer"><div class="titlepage"><div><div><h2 class="title">20.6. Verifying that the PTP events consumer application is receiving events</h2></div></div></div><p>
				Verify that the <code class="literal">cloud-event-proxy</code> container in the application pod is receiving PTP events.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You have logged in as a user with <code class="literal">cluster-admin</code> privileges.
					</li><li class="listitem">
						You have installed and configured the PTP Operator.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Get the list of active <code class="literal">linuxptp-daemon</code> pods. Run the following command:
					</p><pre class="programlisting language-terminal">$ oc get pods -n openshift-ptp</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                    READY   STATUS    RESTARTS   AGE
linuxptp-daemon-2t78p   3/3     Running   0          8h
linuxptp-daemon-k8n88   3/3     Running   0          8h</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Access the metrics for the required consumer-side <code class="literal">cloud-event-proxy</code> container by running the following command:
					</p><pre class="programlisting language-terminal">$ oc exec -it &lt;linuxptp-daemon&gt; -n openshift-ptp -c cloud-event-proxy -- curl 127.0.0.1:9091/metrics</pre><p class="simpara">
						where:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;linuxptp-daemon&gt;</span></dt><dd><p class="simpara">
									Specifies the pod you want to query, for example, <code class="literal">linuxptp-daemon-2t78p</code>.
								</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal"># HELP cne_transport_connections_resets Metric to get number of connection resets
# TYPE cne_transport_connections_resets gauge
cne_transport_connection_reset 1
# HELP cne_transport_receiver Metric to get number of receiver created
# TYPE cne_transport_receiver gauge
cne_transport_receiver{address="/cluster/node/compute-1.example.com/ptp",status="active"} 2
cne_transport_receiver{address="/cluster/node/compute-1.example.com/redfish/event",status="active"} 2
# HELP cne_transport_sender Metric to get number of sender created
# TYPE cne_transport_sender gauge
cne_transport_sender{address="/cluster/node/compute-1.example.com/ptp",status="active"} 1
cne_transport_sender{address="/cluster/node/compute-1.example.com/redfish/event",status="active"} 1
# HELP cne_events_ack Metric to get number of events produced
# TYPE cne_events_ack gauge
cne_events_ack{status="success",type="/cluster/node/compute-1.example.com/ptp"} 18
cne_events_ack{status="success",type="/cluster/node/compute-1.example.com/redfish/event"} 18
# HELP cne_events_transport_published Metric to get number of events published by the transport
# TYPE cne_events_transport_published gauge
cne_events_transport_published{address="/cluster/node/compute-1.example.com/ptp",status="failed"} 1
cne_events_transport_published{address="/cluster/node/compute-1.example.com/ptp",status="success"} 18
cne_events_transport_published{address="/cluster/node/compute-1.example.com/redfish/event",status="failed"} 1
cne_events_transport_published{address="/cluster/node/compute-1.example.com/redfish/event",status="success"} 18
# HELP cne_events_transport_received Metric to get number of events received  by the transport
# TYPE cne_events_transport_received gauge
cne_events_transport_received{address="/cluster/node/compute-1.example.com/ptp",status="success"} 18
cne_events_transport_received{address="/cluster/node/compute-1.example.com/redfish/event",status="success"} 18
# HELP cne_events_api_published Metric to get number of events published by the rest api
# TYPE cne_events_api_published gauge
cne_events_api_published{address="/cluster/node/compute-1.example.com/ptp",status="success"} 19
cne_events_api_published{address="/cluster/node/compute-1.example.com/redfish/event",status="success"} 19
# HELP cne_events_received Metric to get number of events received
# TYPE cne_events_received gauge
cne_events_received{status="success",type="/cluster/node/compute-1.example.com/ptp"} 18
cne_events_received{status="success",type="/cluster/node/compute-1.example.com/redfish/event"} 18
# HELP promhttp_metric_handler_requests_in_flight Current number of scrapes being served.
# TYPE promhttp_metric_handler_requests_in_flight gauge
promhttp_metric_handler_requests_in_flight 1
# HELP promhttp_metric_handler_requests_total Total number of scrapes by HTTP status code.
# TYPE promhttp_metric_handler_requests_total counter
promhttp_metric_handler_requests_total{code="200"} 4
promhttp_metric_handler_requests_total{code="500"} 0
promhttp_metric_handler_requests_total{code="503"} 0</pre>

									</p></div></dd></dl></div></li></ol></div></section></section><section class="chapter" id="external-dns-operator-1"><div class="titlepage"><div><div><h1 class="title">Chapter 21. External DNS Operator</h1></div></div></div><section class="section" id="external-dns-operator"><div class="titlepage"><div><div><h2 class="title">21.1. External DNS Operator in OpenShift Container Platform</h2></div></div></div><p>
				The External DNS Operator deploys and manages <code class="literal">ExternalDNS</code> to provide the name resolution for services and routes from the external DNS provider to OpenShift Container Platform.
			</p><section class="section" id="nw-external-dns-operator_external-dns-operator"><div class="titlepage"><div><div><h3 class="title">21.1.1. External DNS Operator</h3></div></div></div><p>
					The External DNS Operator implements the External DNS API from the <code class="literal">olm.openshift.io</code> API group. The External DNS Operator deploys the <code class="literal">ExternalDNS</code> using a deployment resource. The ExternalDNS deployment watches the resources such as services and routes in the cluster and updates the external DNS providers.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						You can deploy the ExternalDNS Operator on demand from the OperatorHub, this creates a <code class="literal">Subscription</code> object.
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check the name of an install plan:
						</p><pre class="programlisting language-terminal">$ oc -n external-dns-operator get sub external-dns-operator -o yaml | yq '.status.installplan.name'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">install-zcvlr</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check the status of an install plan, the status of an install plan must be <code class="literal">Complete</code>:
						</p><pre class="programlisting language-terminal">$ oc -n external-dns-operator get ip &lt;install_plan_name&gt; -o yaml | yq .status.phase'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Complete</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Use the <code class="literal">oc get</code> command to view the <code class="literal">Deployment</code> status:
						</p><pre class="programlisting language-terminal">$ oc get -n external-dns-operator deployment/external-dns-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                    READY     UP-TO-DATE   AVAILABLE   AGE
external-dns-operator   1/1       1            1           23h</pre>

							</p></div></li></ol></div></section><section class="section" id="nw-external-dns-operator-logs_external-dns-operator"><div class="titlepage"><div><div><h3 class="title">21.1.2. External DNS Operator logs</h3></div></div></div><p>
					You can view External DNS Operator logs by using the <code class="literal">oc logs</code> command.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the logs of the External DNS Operator:
						</p><pre class="programlisting language-terminal">$ oc logs -n external-dns-operator deployment/external-dns-operator -c external-dns-operator</pre></li></ol></div><section class="section" id="external-dns-operator-domain-name-limitations"><div class="titlepage"><div><div><h4 class="title">21.1.2.1. External DNS Operator domain name limitations</h4></div></div></div><p>
						External DNS Operator uses the TXT registry, which follows the new format and adds the prefix for the TXT records. This reduces the maximum length of the domain name for the TXT records. A DNS record cannot be present without a corresponding TXT record, so the domain name of the DNS record must follow the same limit as the TXT records. For example, DNS record is <code class="literal">&lt;domain-name-from-source&gt;</code> and the TXT record is <code class="literal">external-dns-&lt;record-type&gt;-&lt;domain-name-from-source&gt;</code>.
					</p><p>
						The domain name of the DNS records generated by External DNS Operator has the following limitations:
					</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 27%; " class="col_1"><!--Empty--></col><col style="width: 73%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587149242960" scope="col">Record type</th><th align="left" valign="top" id="idm140587144904176" scope="col">Number of characters</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587149242960"> <p>
										CNAME
									</p>
									 </td><td align="left" valign="top" headers="idm140587144904176"> <p>
										44
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587149242960"> <p>
										Wildcard CNAME records on AzureDNS
									</p>
									 </td><td align="left" valign="top" headers="idm140587144904176"> <p>
										42
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587149242960"> <p>
										A
									</p>
									 </td><td align="left" valign="top" headers="idm140587144904176"> <p>
										48
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587149242960"> <p>
										Wildcard A records on AzureDNS
									</p>
									 </td><td align="left" valign="top" headers="idm140587144904176"> <p>
										46
									</p>
									 </td></tr></tbody></table></div><p>
						If the domain name generated by External DNS exceeds the domain name limitation, the External DNS instance gives the following error:
					</p><pre class="programlisting language-terminal">$ oc -n external-dns-operator logs external-dns-aws-7ddbd9c7f8-2jqjh <span id="CO43-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO43-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The <code class="literal">external-dns-aws-7ddbd9c7f8-2jqjh</code> parameter specifies the name of the External DNS pod.
							</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">time="2022-09-02T08:53:57Z" level=info msg="Desired change: CREATE external-dns-cname-hello-openshift-aaaaaaaaaa-bbbbbbbbbb-ccccccc.test.example.io TXT [Id: /hostedzone/Z06988883Q0H0RL6UMXXX]"
time="2022-09-02T08:53:57Z" level=info msg="Desired change: CREATE external-dns-hello-openshift-aaaaaaaaaa-bbbbbbbbbb-ccccccc.test.example.io TXT [Id: /hostedzone/Z06988883Q0H0RL6UMXXX]"
time="2022-09-02T08:53:57Z" level=info msg="Desired change: CREATE hello-openshift-aaaaaaaaaa-bbbbbbbbbb-ccccccc.test.example.io A [Id: /hostedzone/Z06988883Q0H0RL6UMXXX]"
time="2022-09-02T08:53:57Z" level=error msg="Failure in zone test.example.io. [Id: /hostedzone/Z06988883Q0H0RL6UMXXX]"
time="2022-09-02T08:53:57Z" level=error msg="InvalidChangeBatch: [FATAL problem: DomainLabelTooLong (Domain label is too long) encountered with 'external-dns-a-hello-openshift-aaaaaaaaaa-bbbbbbbbbb-ccccccc']\n\tstatus code: 400, request id: e54dfd5a-06c6-47b0-bcb9-a4f7c3a4e0c6"</pre>

						</p></div></section></section></section><section class="section" id="installing-external-dns-on-cloud-providers"><div class="titlepage"><div><div><h2 class="title">21.2. Installing External DNS Operator on cloud providers</h2></div></div></div><p>
				You can install External DNS Operator on cloud providers such as AWS, Azure and GCP.
			</p><section class="section" id="nw-installing-external-dns-operator_installing-external-dns-on-cloud-providers"><div class="titlepage"><div><div><h3 class="title">21.2.1. Installing the External DNS Operator</h3></div></div></div><p>
					You can install the External DNS Operator using the OpenShift Container Platform OperatorHub.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Click <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span> in the OpenShift Container Platform Web Console.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>External DNS Operator</strong></span>. You can use the <span class="strong strong"><strong>Filter by keyword</strong></span> text box or the filter list to search for External DNS Operator from the list of Operators.
						</li><li class="listitem">
							Select the <code class="literal">external-dns-operator</code> namespace.
						</li><li class="listitem">
							On the External DNS Operator page, click <span class="strong strong"><strong>Install</strong></span>.
						</li><li class="listitem"><p class="simpara">
							On the <span class="strong strong"><strong>Install Operator</strong></span> page, ensure that you selected the following options:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Update the channel as <span class="strong strong"><strong>stable-v1</strong></span>.
								</li><li class="listitem">
									Installation mode as <span class="strong strong"><strong>A specific name on the cluster</strong></span>.
								</li><li class="listitem">
									Installed namespace as <code class="literal">external-dns-operator</code>. If namespace <code class="literal">external-dns-operator</code> does not exist, it gets created during the Operator installation.
								</li><li class="listitem">
									Select <span class="strong strong"><strong>Approval Strategy</strong></span> as <span class="strong strong"><strong>Automatic</strong></span> or <span class="strong strong"><strong>Manual</strong></span>. Approval Strategy is set to <span class="strong strong"><strong>Automatic</strong></span> by default.
								</li><li class="listitem">
									Click <span class="strong strong"><strong>Install</strong></span>.
								</li></ol></div></li></ol></div><p>
					If you select <span class="strong strong"><strong>Automatic</strong></span> updates, the Operator Lifecycle Manager (OLM) automatically upgrades the running instance of your Operator without any intervention.
				</p><p>
					If you select <span class="strong strong"><strong>Manual</strong></span> updates, the OLM creates an update request. As a cluster administrator, you must then manually approve that update request to have the Operator updated to the new version.
				</p><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
						Verify that External DNS Operator shows the <span class="strong strong"><strong>Status</strong></span> as <span class="strong strong"><strong>Succeeded</strong></span> on the Installed Operators dashboard.
					</p></div></section></section><section class="section" id="external-dns-operator-configuration-parameters"><div class="titlepage"><div><div><h2 class="title">21.3. External DNS Operator configuration parameters</h2></div></div></div><p>
				The External DNS Operators includes the following configuration parameters:
			</p><section class="section" id="nw-external-dns-operator-configuration-parameters_external-dns-operator-configuration-parameters"><div class="titlepage"><div><div><h3 class="title">21.3.1. External DNS Operator configuration parameters</h3></div></div></div><p>
					The External DNS Operator includes the following configuration parameters:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 27%; " class="col_1"><!--Empty--></col><col style="width: 73%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587131028352" scope="col">Parameter</th><th align="left" valign="top" id="idm140587131027264" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587131028352"> <p>
									<code class="literal">spec</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587131027264"> <p>
									Enables the type of a cloud provider.
								</p>
								 
<pre class="programlisting language-yaml">spec:
  provider:
    type: AWS <span id="CO44-1"><!--Empty--></span><span class="callout">1</span>
    aws:
      credentials:
        name: aws-access-key <span id="CO44-2"><!--Empty--></span><span class="callout">2</span></pre>
								 <div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO44-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Defines available options such as AWS, GCP and Azure.
										</div></dd><dt><a href="#CO44-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Defines a name of the <code class="literal">secret</code> which contains credentials for your cloud provider.
										</div></dd></dl></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587131028352"> <p>
									<code class="literal">zones</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587131027264"> <p>
									Enables you to specify DNS zones by their domains. If you do not specify zones, <code class="literal">ExternalDNS</code> discovers all the zones present in your cloud provider account.
								</p>
								 
<pre class="programlisting language-yaml">zones:
- "myzoneid" <span id="CO45-1"><!--Empty--></span><span class="callout">1</span></pre>
								 <div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO45-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specifies the IDs of DNS zones.
										</div></dd></dl></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587131028352"> <p>
									<code class="literal">domains</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587131027264"> <p>
									Enables you to specify AWS zones by their domains. If you do not specify domains, <code class="literal">ExternalDNS</code> discovers all the zones present in your cloud provider account.
								</p>
								 
<pre class="programlisting language-yaml">domains:
- filterType: Include <span id="CO46-1"><!--Empty--></span><span class="callout">1</span>
  matchType: Exact <span id="CO46-2"><!--Empty--></span><span class="callout">2</span>
  name: "myzonedomain1.com" <span id="CO46-3"><!--Empty--></span><span class="callout">3</span>
- filterType: Include
  matchType: Pattern <span id="CO46-4"><!--Empty--></span><span class="callout">4</span>
  pattern: ".*\\.otherzonedomain\\.com" <span id="CO46-5"><!--Empty--></span><span class="callout">5</span></pre>
								 <div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO46-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Instructs <code class="literal">ExternalDNS</code> to include the domain specified.
										</div></dd><dt><a href="#CO46-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Instructs <code class="literal">ExtrnalDNS</code> that the domain matching has to be exact as opposed to regular expression match.
										</div></dd><dt><a href="#CO46-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Defines the exact domain name by which <code class="literal">ExternalDNS</code> filters.
										</div></dd><dt><a href="#CO46-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Sets <code class="literal">regex-domain-filter</code> flag in <code class="literal">ExternalDNS</code>. You can limit possible domains by using a Regex filter.
										</div></dd><dt><a href="#CO46-5"><span class="callout">5</span></a> </dt><dd><div class="para">
											Defines the regex pattern to be used by <code class="literal">ExternalDNS</code> to filter the domains of the target zones.
										</div></dd></dl></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587131028352"> <p>
									<code class="literal">source</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587131027264"> <p>
									Enables you to specify the source for the DNS records, <code class="literal">Service</code> or <code class="literal">Route</code>.
								</p>
								 
<pre class="programlisting language-yaml">source: <span id="CO47-1"><!--Empty--></span><span class="callout">1</span>
  type: Service <span id="CO47-2"><!--Empty--></span><span class="callout">2</span>
  service:
    serviceType:<span id="CO47-3"><!--Empty--></span><span class="callout">3</span>
      - LoadBalancer
      - ClusterIP
  labelFilter: <span id="CO47-4"><!--Empty--></span><span class="callout">4</span>
    matchLabels:
      external-dns.mydomain.org/publish: "yes"
  hostnameAnnotation: "Allow" <span id="CO47-5"><!--Empty--></span><span class="callout">5</span>
  fqdnTemplate:
  - "{{.Name}}.myzonedomain.com" <span id="CO47-6"><!--Empty--></span><span class="callout">6</span></pre>
								 <div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO47-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Defines the settings for the source of DNS records.
										</div></dd><dt><a href="#CO47-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											The <code class="literal">ExternalDNS</code> uses <code class="literal">Service</code> type as source for creating dns records.
										</div></dd><dt><a href="#CO47-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Sets <code class="literal">service-type-filter</code> flag in <code class="literal">ExternalDNS</code>. The <code class="literal">serviceType</code> contains the following fields:
										</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
													<code class="literal">default</code>: <code class="literal">LoadBalancer</code>
												</li><li class="listitem">
													<code class="literal">expected</code>: <code class="literal">ClusterIP</code>
												</li><li class="listitem">
													<code class="literal">NodePort</code>
												</li><li class="listitem">
													<code class="literal">LoadBalancer</code>
												</li><li class="listitem">
													<code class="literal">ExternalName</code>
												</li></ul></div></dd><dt><a href="#CO47-4"><span class="callout">4</span></a> </dt><dd><div class="para">
											Ensures that the controller considers only those resources which matches with label filter.
										</div></dd><dt><a href="#CO47-5"><span class="callout">5</span></a> </dt><dd><div class="para">
											The default value for <code class="literal">hostnameAnnotation</code> is <code class="literal">Ignore</code> which instructs <code class="literal">ExternalDNS</code> to generate DNS records using the templates specified in the field <code class="literal">fqdnTemplates</code>. When the value is <code class="literal">Allow</code> the DNS records get generated based on the value specified in the <code class="literal">external-dns.alpha.kubernetes.io/hostname</code> annotation.
										</div></dd><dt><a href="#CO47-6"><span class="callout">6</span></a> </dt><dd><div class="para">
											External DNS Operator uses a string to generate DNS names from sources that don’t define a hostname, or to add a hostname suffix when paired with the fake source.
										</div></dd></dl></div>
								 
<pre class="programlisting language-yaml">source:
  type: OpenShiftRoute <span id="CO48-1"><!--Empty--></span><span class="callout">1</span>
  openshiftRouteOptions:
    routerName: default <span id="CO48-2"><!--Empty--></span><span class="callout">2</span>
    labelFilter:
      matchLabels:
        external-dns.mydomain.org/publish: "yes"</pre>
								 <div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO48-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											ExternalDNS` uses type <code class="literal">route</code> as source for creating dns records.
										</div></dd><dt><a href="#CO48-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											If the source is <code class="literal">OpenShiftRoute</code>, then you can pass the Ingress Controller name. The <code class="literal">ExternalDNS</code> uses canonical name of Ingress Controller as the target for CNAME record.
										</div></dd></dl></div>
								 </td></tr></tbody></table></div></section></section><section class="section" id="creating-dns-records-on-aws"><div class="titlepage"><div><div><h2 class="title">21.4. Creating DNS records on AWS</h2></div></div></div><p>
				You can create DNS records on AWS and AWS GovCloud by using External DNS Operator.
			</p><section class="section" id="nw-control-dns-records-public-hosted-zone-aws_creating-dns-records-on-aws"><div class="titlepage"><div><div><h3 class="title">21.4.1. Creating DNS records on an public hosted zone for AWS by using Red Hat External DNS Operator</h3></div></div></div><p>
					You can create DNS records on a public hosted zone for AWS by using the Red Hat External DNS Operator. You can use the same instructions to create DNS records on a hosted zone for AWS GovCloud.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check the user. The user must have access to the <code class="literal">kube-system</code> namespace. If you don’t have the credentials, as you can fetch the credentials from the <code class="literal">kube-system</code> namespace to use the cloud provider client:
						</p><pre class="programlisting language-terminal">$ oc whoami</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">system:admin</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Fetch the values from aws-creds secret present in <code class="literal">kube-system</code> namespace.
						</p><pre class="programlisting language-terminal">$ export AWS_ACCESS_KEY_ID=$(oc get secrets aws-creds -n kube-system  --template={{.data.aws_access_key_id}} | base64 -d)
$ export AWS_SECRET_ACCESS_KEY=$(oc get secrets aws-creds -n kube-system  --template={{.data.aws_secret_access_key}} | base64 -d)</pre></li><li class="listitem"><p class="simpara">
							Get the routes to check the domain:
						</p><pre class="programlisting language-terminal">$ oc get routes --all-namespaces | grep console</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">openshift-console          console             console-openshift-console.apps.testextdnsoperator.apacshift.support                       console             https   reencrypt/Redirect     None
openshift-console          downloads           downloads-openshift-console.apps.testextdnsoperator.apacshift.support                     downloads           http    edge/Redirect          None</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Get the list of dns zones to find the one which corresponds to the previously found route’s domain:
						</p><pre class="programlisting language-terminal">$ aws route53 list-hosted-zones | grep testextdnsoperator.apacshift.support</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">HOSTEDZONES	terraform	/hostedzone/Z02355203TNN1XXXX1J6O	testextdnsoperator.apacshift.support.	5</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create <code class="literal">ExternalDNS</code> resource for <code class="literal">route</code> source:
						</p><pre class="programlisting language-yaml">$ cat &lt;&lt;EOF | oc create -f -
apiVersion: externaldns.olm.openshift.io/v1beta1
kind: ExternalDNS
metadata:
  name: sample-aws <span id="CO49-1"><!--Empty--></span><span class="callout">1</span>
spec:
  domains:
  - filterType: Include   <span id="CO49-2"><!--Empty--></span><span class="callout">2</span>
    matchType: Exact   <span id="CO49-3"><!--Empty--></span><span class="callout">3</span>
    name: testextdnsoperator.apacshift.support <span id="CO49-4"><!--Empty--></span><span class="callout">4</span>
  provider:
    type: AWS <span id="CO49-5"><!--Empty--></span><span class="callout">5</span>
  source:  <span id="CO49-6"><!--Empty--></span><span class="callout">6</span>
    type: OpenShiftRoute <span id="CO49-7"><!--Empty--></span><span class="callout">7</span>
    openshiftRouteOptions:
      routerName: default <span id="CO49-8"><!--Empty--></span><span class="callout">8</span>
EOF</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO49-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Defines the name of external DNS resource.
								</div></dd><dt><a href="#CO49-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									By default all hosted zones are selected as potential targets. You can include a hosted zone that you need.
								</div></dd><dt><a href="#CO49-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The matching of the target zone’s domain has to be exact (as opposed to regular expression match).
								</div></dd><dt><a href="#CO49-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specify the exact domain of the zone you want to update. The hostname of the routes must be subdomains of the specified domain.
								</div></dd><dt><a href="#CO49-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Defines the <code class="literal">AWS Route53</code> DNS provider.
								</div></dd><dt><a href="#CO49-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Defines options for the source of DNS records.
								</div></dd><dt><a href="#CO49-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									Defines OpenShift <code class="literal">route</code> resource as the source for the DNS records which gets created in the previously specified DNS provider.
								</div></dd><dt><a href="#CO49-8"><span class="callout">8</span></a> </dt><dd><div class="para">
									If the source is <code class="literal">OpenShiftRoute</code>, then you can pass the OpenShift Ingress Controller name. External DNS Operator selects the canonical hostname of that router as the target while creating CNAME record.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Check the records created for OCP routes using the following command:
						</p><pre class="programlisting language-terminal">$ aws route53 list-resource-record-sets --hosted-zone-id Z02355203TNN1XXXX1J6O --query "ResourceRecordSets[?Type == 'CNAME']" | grep console</pre></li></ol></div></section></section><section class="section" id="creating-dns-records-on-azure"><div class="titlepage"><div><div><h2 class="title">21.5. Creating DNS records on Azure</h2></div></div></div><p>
				You can create DNS records on Azure using External DNS Operator.
			</p><section class="section" id="nw-control-dns-records-public-hosted-zone-azure_creating-dns-records-on-azure"><div class="titlepage"><div><div><h3 class="title">21.5.1. Creating DNS records on an public DNS zone for Azure by using Red Hat External DNS Operator</h3></div></div></div><p>
					You can create DNS records on a public DNS zone for Azure by using Red Hat External DNS Operator.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check the user. The user must have access to the <code class="literal">kube-system</code> namespace. If you don’t have the credentials, as you can fetch the credentials from the <code class="literal">kube-system</code> namespace to use the cloud provider client:
						</p><pre class="programlisting language-terminal">$ oc whoami</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">system:admin</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Fetch the values from azure-credentials secret present in <code class="literal">kube-system</code> namespace.
						</p><pre class="programlisting language-terminal">$ CLIENT_ID=$(oc get secrets azure-credentials  -n kube-system  --template={{.data.azure_client_id}} | base64 -d)
$ CLIENT_SECRET=$(oc get secrets azure-credentials  -n kube-system  --template={{.data.azure_client_secret}} | base64 -d)
$ RESOURCE_GROUP=$(oc get secrets azure-credentials  -n kube-system  --template={{.data.azure_resourcegroup}} | base64 -d)
$ SUBSCRIPTION_ID=$(oc get secrets azure-credentials  -n kube-system  --template={{.data.azure_subscription_id}} | base64 -d)
$ TENANT_ID=$(oc get secrets azure-credentials  -n kube-system  --template={{.data.azure_tenant_id}} | base64 -d)</pre></li><li class="listitem"><p class="simpara">
							Login to azure with base64 decoded values:
						</p><pre class="programlisting language-terminal">$ az login --service-principal -u "${CLIENT_ID}" -p "${CLIENT_SECRET}" --tenant "${TENANT_ID}"</pre></li><li class="listitem"><p class="simpara">
							Get the routes to check the domain:
						</p><pre class="programlisting language-terminal">$ oc get routes --all-namespaces | grep console</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">openshift-console          console             console-openshift-console.apps.test.azure.example.com                       console             https   reencrypt/Redirect     None
openshift-console          downloads           downloads-openshift-console.apps.test.azure.example.com                     downloads           http    edge/Redirect          None</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Get the list of dns zones to find the one which corresponds to the previously found route’s domain:
						</p><pre class="programlisting language-terminal">$ az network dns zone list --resource-group "${RESOURCE_GROUP}"</pre></li><li class="listitem"><p class="simpara">
							Create <code class="literal">ExternalDNS</code> resource for <code class="literal">route</code> source:
						</p><pre class="programlisting language-yaml">apiVersion: externaldns.olm.openshift.io/v1beta1
kind: ExternalDNS
metadata:
  name: sample-azure <span id="CO50-1"><!--Empty--></span><span class="callout">1</span>
spec:
  zones:
  - "/subscriptions/1234567890/resourceGroups/test-azure-xxxxx-rg/providers/Microsoft.Network/dnszones/test.azure.example.com" <span id="CO50-2"><!--Empty--></span><span class="callout">2</span>
  provider:
    type: Azure <span id="CO50-3"><!--Empty--></span><span class="callout">3</span>
  source:
    openshiftRouteOptions: <span id="CO50-4"><!--Empty--></span><span class="callout">4</span>
      routerName: default <span id="CO50-5"><!--Empty--></span><span class="callout">5</span>
    type: OpenShiftRoute <span id="CO50-6"><!--Empty--></span><span class="callout">6</span>
EOF</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO50-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specifies the name of External DNS CR.
								</div></dd><dt><a href="#CO50-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Define the zone ID.
								</div></dd><dt><a href="#CO50-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Defines the Azure DNS provider.
								</div></dd><dt><a href="#CO50-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									You can define options for the source of DNS records.
								</div></dd><dt><a href="#CO50-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									If the source is <code class="literal">OpenShiftRoute</code> then you can pass the OpenShift Ingress Controller name. External DNS selects the canonical hostname of that router as the target while creating CNAME record.
								</div></dd><dt><a href="#CO50-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Defines OpenShift <code class="literal">route</code> resource as the source for the DNS records which gets created in the previously specified DNS provider.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Check the records created for OCP routes using the following command:
						</p><pre class="programlisting language-terminal">$ az network dns record-set list -g "${RESOURCE_GROUP}"  -z test.azure.example.com | grep console</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								To create records on private hosted zones on private Azure dns, you need to specify the private zone under <code class="literal">zones</code> which populates the provider type to <code class="literal">azure-private-dns</code> in the <code class="literal">ExternalDNS</code> container args.
							</p></div></div></li></ol></div></section></section><section class="section" id="creating-dns-records-on-gcp"><div class="titlepage"><div><div><h2 class="title">21.6. Creating DNS records on GCP</h2></div></div></div><p>
				You can create DNS records on GCP using External DNS Operator.
			</p><section class="section" id="nw-control-dns-records-public-managed-zone-gcp_creating-dns-records-on-gcp"><div class="titlepage"><div><div><h3 class="title">21.6.1. Creating DNS records on an public managed zone for GCP by using Red Hat External DNS Operator</h3></div></div></div><p>
					You can create DNS records on a public managed zone for GCP by using Red Hat External DNS Operator.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check the user. The user must have access to the <code class="literal">kube-system</code> namespace. If you don’t have the credentials, as you can fetch the credentials from the <code class="literal">kube-system</code> namespace to use the cloud provider client:
						</p><pre class="programlisting language-terminal">$ oc whoami</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">system:admin</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Copy the value of service_account.json in gcp-credentials secret in a file encoded-gcloud.json by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get secret gcp-credentials -n kube-system --template='{{$v := index .data "service_account.json"}}{{$v}}' | base64 -d - &gt; decoded-gcloud.json</pre></li><li class="listitem"><p class="simpara">
							Export Google credentials:
						</p><pre class="programlisting language-terminal">$ export GOOGLE_CREDENTIALS=decoded-gcloud.json</pre></li><li class="listitem"><p class="simpara">
							Activate your account by using the following command:
						</p><pre class="programlisting language-terminal">$ gcloud auth activate-service-account  &lt;client_email as per decoded-gcloud.json&gt; --key-file=decoded-gcloud.json</pre></li><li class="listitem"><p class="simpara">
							Set your project:
						</p><pre class="programlisting language-terminal">$ gcloud config set project &lt;project_id as per decoded-gcloud.json&gt;</pre></li><li class="listitem"><p class="simpara">
							Get the routes to check the domain:
						</p><pre class="programlisting language-terminal">$ oc get routes --all-namespaces | grep console</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">openshift-console          console             console-openshift-console.apps.test.gcp.example.com                       console             https   reencrypt/Redirect     None
openshift-console          downloads           downloads-openshift-console.apps.test.gcp.example.com                     downloads           http    edge/Redirect          None</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Get the list of managed zones to find the zone which corresponds to the previously found route’s domain:
						</p><pre class="programlisting language-terminal">$ gcloud dns managed-zones list | grep test.gcp.example.com
qe-cvs4g-private-zone test.gcp.example.com</pre></li><li class="listitem"><p class="simpara">
							Create <code class="literal">ExternalDNS</code> resource for <code class="literal">route</code> source:
						</p><pre class="programlisting language-yaml">apiVersion: externaldns.olm.openshift.io/v1beta1
kind: ExternalDNS
metadata:
  name: sample-gcp <span id="CO51-1"><!--Empty--></span><span class="callout">1</span>
spec:
  domains:
    - filterType: Include <span id="CO51-2"><!--Empty--></span><span class="callout">2</span>
      matchType: Exact <span id="CO51-3"><!--Empty--></span><span class="callout">3</span>
      name: test.gcp.example.com <span id="CO51-4"><!--Empty--></span><span class="callout">4</span>
  provider:
    type: GCP <span id="CO51-5"><!--Empty--></span><span class="callout">5</span>
  source:
    openshiftRouteOptions: <span id="CO51-6"><!--Empty--></span><span class="callout">6</span>
      routerName: default <span id="CO51-7"><!--Empty--></span><span class="callout">7</span>
    type: OpenShiftRoute <span id="CO51-8"><!--Empty--></span><span class="callout">8</span>
EOF</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO51-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specifies the name of External DNS CR.
								</div></dd><dt><a href="#CO51-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									By default all hosted zones are selected as potential targets. You can include a hosted zone that you need.
								</div></dd><dt><a href="#CO51-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The matching of the target zone’s domain has to be exact (as opposed to regular expression match).
								</div></dd><dt><a href="#CO51-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specify the exact domain of the zone you want to update. The hostname of the routes must be subdomains of the specified domain.
								</div></dd><dt><a href="#CO51-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Defines Google Cloud DNS provider.
								</div></dd><dt><a href="#CO51-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									You can define options for the source of DNS records.
								</div></dd><dt><a href="#CO51-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									If the source is <code class="literal">OpenShiftRoute</code> then you can pass the OpenShift Ingress Controller name. External DNS selects the canonical hostname of that router as the target while creating CNAME record.
								</div></dd><dt><a href="#CO51-8"><span class="callout">8</span></a> </dt><dd><div class="para">
									Defines OpenShift <code class="literal">route</code> resource as the source for the DNS records which gets created in the previously specified DNS provider.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Check the records created for OCP routes using the following command:
						</p><pre class="programlisting language-terminal">$ gcloud dns record-sets list --zone=qe-cvs4g-private-zone | grep console</pre></li></ol></div></section></section><section class="section" id="creating-dns-records-on-infoblox"><div class="titlepage"><div><div><h2 class="title">21.7. Creating DNS records on Infoblox</h2></div></div></div><p>
				You can create DNS records on Infoblox using the Red Hat External DNS Operator.
			</p><section class="section" id="nw-control-dns-records-public-dns-zone-infoblox_creating-dns-records-on-infoblox"><div class="titlepage"><div><div><h3 class="title">21.7.1. Creating DNS records on a public DNS zone on Infoblox</h3></div></div></div><p>
					You can create DNS records on a public DNS zone on Infoblox by using the Red Hat External DNS Operator.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have access to the Infoblox UI.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">secret</code> object with Infoblox credentials by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n external-dns-operator create secret generic infoblox-credentials --from-literal=EXTERNAL_DNS_INFOBLOX_WAPI_USERNAME=&lt;infoblox_username&gt; --from-literal=EXTERNAL_DNS_INFOBLOX_WAPI_PASSWORD=&lt;infoblox_password&gt;</pre></li><li class="listitem"><p class="simpara">
							Get the routes objects to check your cluster domain by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get routes --all-namespaces | grep console</pre><div class="formalpara"><p class="title"><strong>Example Output</strong></p><p>
								
<pre class="programlisting language-terminal">openshift-console          console             console-openshift-console.apps.test.example.com                       console             https   reencrypt/Redirect     None
openshift-console          downloads           downloads-openshift-console.apps.test.example.com                     downloads           http    edge/Redirect          None</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">ExternalDNS</code> resource YAML file, for example, sample-infoblox.yaml, as follows:
						</p><pre class="programlisting language-yaml">apiVersion: externaldns.olm.openshift.io/v1beta1
kind: ExternalDNS
metadata:
  name: sample-infoblox
spec:
  provider:
    type: Infoblox
    infoblox:
      credentials:
        name: infoblox-credentials
      gridHost: ${INFOBLOX_GRID_PUBLIC_IP}
      wapiPort: 443
      wapiVersion: "2.3.1"
  domains:
  - filterType: Include
    matchType: Exact
    name: test.example.com
  source:
    type: OpenShiftRoute
    openshiftRouteOptions:
      routerName: default</pre></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">ExternalDNS</code> resource on Infoblox by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f sample-infoblox.yaml</pre></li><li class="listitem"><p class="simpara">
							From the Infoblox UI, check the DNS records created for <code class="literal">console</code> routes:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Click <span class="strong strong"><strong>Data Management</strong></span> → <span class="strong strong"><strong>DNS</strong></span> → <span class="strong strong"><strong>Zones</strong></span>.
								</li><li class="listitem">
									Select the zone name.
								</li></ol></div></li></ol></div></section></section><section class="section" id="external-dns-operator-cluster-wide-proxy"><div class="titlepage"><div><div><h2 class="title">21.8. Configuring the cluster-wide proxy on the External DNS Operator</h2></div></div></div><p>
				You can configure the cluster-wide proxy in the External DNS Operator. After configuring the cluster-wide proxy in the External DNS Operator, Operator Lifecycle Manager (OLM) automatically updates all the deployments of the Operators with the environment variables such as <code class="literal">HTTP_PROXY</code>, <code class="literal">HTTPS_PROXY</code>, and <code class="literal">NO_PROXY</code>.
			</p><section class="section" id="nw-configuring-cluster-wide-proxy_external-dns-operator-cluster-wide-proxy"><div class="titlepage"><div><div><h3 class="title">21.8.1. Configuring the External DNS Operator to trust the certificate authority of the cluster-wide proxy</h3></div></div></div><p>
					You can configure the External DNS Operator to trust the certificate authority of the cluster-wide proxy.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the config map to contain the CA bundle in the <code class="literal">external-dns-operator</code> namespace by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n external-dns-operator create configmap trusted-ca</pre></li><li class="listitem"><p class="simpara">
							To inject the trusted CA bundle into the config map, add the <code class="literal">config.openshift.io/inject-trusted-cabundle=true</code> label to the config map by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n external-dns-operator label cm trusted-ca config.openshift.io/inject-trusted-cabundle=true</pre></li><li class="listitem"><p class="simpara">
							Update the subscription of the External DNS Operator by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n external-dns-operator patch subscription external-dns-operator --type='json' -p='[{"op": "add", "path": "/spec/config", "value":{"env":[{"name":"TRUSTED_CA_CONFIGMAP_NAME","value":"trusted-ca"}]}}]'</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							After the deployment of the External DNS Operator is completed, verify that the trusted CA environment variable is added to the <code class="literal">external-dns-operator</code> deployment by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n external-dns-operator exec deploy/external-dns-operator -c external-dns-operator -- printenv TRUSTED_CA_CONFIGMAP_NAME</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">trusted-ca</pre>

							</p></div></li></ul></div></section></section></section><section class="chapter" id="network-policy"><div class="titlepage"><div><div><h1 class="title">Chapter 22. Network policy</h1></div></div></div><section class="section" id="about-network-policy"><div class="titlepage"><div><div><h2 class="title">22.1. About network policy</h2></div></div></div><p>
				As a cluster administrator, you can define network policies that restrict traffic to pods in your cluster.
			</p><section class="section" id="nw-networkpolicy-about_about-network-policy"><div class="titlepage"><div><div><h3 class="title">22.1.1. About network policy</h3></div></div></div><p>
					In a cluster using a network plugin that supports Kubernetes network policy, network isolation is controlled entirely by <code class="literal">NetworkPolicy</code> objects. In OpenShift Container Platform 4.13, OpenShift SDN supports using network policy in its default network isolation mode.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Network policy does not apply to the host network namespace. Pods with host networking enabled are unaffected by network policy rules. However, pods connecting to the host-networked pods might be affected by the network policy rules.
					</p><p>
						Network policies cannot block traffic from localhost or from their resident nodes.
					</p></div></div><p>
					By default, all pods in a project are accessible from other pods and network endpoints. To isolate one or more pods in a project, you can create <code class="literal">NetworkPolicy</code> objects in that project to indicate the allowed incoming connections. Project administrators can create and delete <code class="literal">NetworkPolicy</code> objects within their own project.
				</p><p>
					If a pod is matched by selectors in one or more <code class="literal">NetworkPolicy</code> objects, then the pod will accept only connections that are allowed by at least one of those <code class="literal">NetworkPolicy</code> objects. A pod that is not selected by any <code class="literal">NetworkPolicy</code> objects is fully accessible.
				</p><p>
					A network policy applies to only the TCP, UDP, and SCTP protocols. Other protocols are not affected.
				</p><p>
					The following example <code class="literal">NetworkPolicy</code> objects demonstrate supporting different scenarios:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Deny all traffic:
						</p><p class="simpara">
							To make a project deny by default, add a <code class="literal">NetworkPolicy</code> object that matches all pods but accepts no traffic:
						</p><pre class="programlisting language-yaml">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-by-default
spec:
  podSelector: {}
  ingress: []</pre></li><li class="listitem"><p class="simpara">
							Only allow connections from the OpenShift Container Platform Ingress Controller:
						</p><p class="simpara">
							To make a project allow only connections from the OpenShift Container Platform Ingress Controller, add the following <code class="literal">NetworkPolicy</code> object.
						</p><pre class="programlisting language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-ingress
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: ingress
  podSelector: {}
  policyTypes:
  - Ingress</pre></li><li class="listitem"><p class="simpara">
							Only accept connections from pods within a project:
						</p><p class="simpara">
							To make pods accept connections from other pods in the same project, but reject all other connections from pods in other projects, add the following <code class="literal">NetworkPolicy</code> object:
						</p><pre class="programlisting language-yaml">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-same-namespace
spec:
  podSelector: {}
  ingress:
  - from:
    - podSelector: {}</pre></li><li class="listitem"><p class="simpara">
							Only allow HTTP and HTTPS traffic based on pod labels:
						</p><p class="simpara">
							To enable only HTTP and HTTPS access to the pods with a specific label (<code class="literal">role=frontend</code> in following example), add a <code class="literal">NetworkPolicy</code> object similar to the following:
						</p><pre class="programlisting language-yaml">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-http-and-https
spec:
  podSelector:
    matchLabels:
      role: frontend
  ingress:
  - ports:
    - protocol: TCP
      port: 80
    - protocol: TCP
      port: 443</pre></li><li class="listitem"><p class="simpara">
							Accept connections by using both namespace and pod selectors:
						</p><p class="simpara">
							To match network traffic by combining namespace and pod selectors, you can use a <code class="literal">NetworkPolicy</code> object similar to the following:
						</p><pre class="programlisting language-yaml">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-pod-and-namespace-both
spec:
  podSelector:
    matchLabels:
      name: test-pods
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            project: project_name
        podSelector:
          matchLabels:
            name: test-pods</pre></li></ul></div><p>
					<code class="literal">NetworkPolicy</code> objects are additive, which means you can combine multiple <code class="literal">NetworkPolicy</code> objects together to satisfy complex network requirements.
				</p><p>
					For example, for the <code class="literal">NetworkPolicy</code> objects defined in previous samples, you can define both <code class="literal">allow-same-namespace</code> and <code class="literal">allow-http-and-https</code> policies within the same project. Thus allowing the pods with the label <code class="literal">role=frontend</code>, to accept any connection allowed by each policy. That is, connections on any port from pods in the same namespace, and connections on ports <code class="literal">80</code> and <code class="literal">443</code> from pods in any namespace.
				</p><section class="section" id="nw-networkpolicy-allow-from-router_about-network-policy"><div class="titlepage"><div><div><h4 class="title">22.1.1.1. Using the allow-from-router network policy</h4></div></div></div><p>
						Use the following <code class="literal">NetworkPolicy</code> to allow external traffic regardless of the router configuration:
					</p><pre class="programlisting language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-router
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          policy-group.network.openshift.io/ingress: ""<span id="CO52-1"><!--Empty--></span><span class="callout">1</span>
  podSelector: {}
  policyTypes:
  - Ingress</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO52-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								<code class="literal">policy-group.network.openshift.io/ingress:""</code> label supports both OpenShift-SDN and OVN-Kubernetes.
							</div></dd></dl></div></section><section class="section" id="nw-networkpolicy-allow-from-hostnetwork_about-network-policy"><div class="titlepage"><div><div><h4 class="title">22.1.1.2. Using the allow-from-hostnetwork network policy</h4></div></div></div><p>
						Add the following <code class="literal">allow-from-hostnetwork</code> <code class="literal">NetworkPolicy</code> object to direct traffic from the host network pods:
					</p><pre class="programlisting language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-hostnetwork
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          policy-group.network.openshift.io/host-network: ""
  podSelector: {}
  policyTypes:
  - Ingress</pre></section></section><section class="section" id="nw-networkpolicy-optimize-sdn_about-network-policy"><div class="titlepage"><div><div><h3 class="title">22.1.2. Optimizations for network policy with OpenShift SDN</h3></div></div></div><p>
					Use a network policy to isolate pods that are differentiated from one another by labels within a namespace.
				</p><p>
					It is inefficient to apply <code class="literal">NetworkPolicy</code> objects to large numbers of individual pods in a single namespace. Pod labels do not exist at the IP address level, so a network policy generates a separate Open vSwitch (OVS) flow rule for every possible link between every pod selected with a <code class="literal">podSelector</code>.
				</p><p>
					For example, if the spec <code class="literal">podSelector</code> and the ingress <code class="literal">podSelector</code> within a <code class="literal">NetworkPolicy</code> object each match 200 pods, then 40,000 (200*200) OVS flow rules are generated. This might slow down a node.
				</p><p>
					When designing your network policy, refer to the following guidelines:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Reduce the number of OVS flow rules by using namespaces to contain groups of pods that need to be isolated.
						</p><p class="simpara">
							<code class="literal">NetworkPolicy</code> objects that select a whole namespace, by using the <code class="literal">namespaceSelector</code> or an empty <code class="literal">podSelector</code>, generate only a single OVS flow rule that matches the VXLAN virtual network ID (VNID) of the namespace.
						</p></li><li class="listitem">
							Keep the pods that do not need to be isolated in their original namespace, and move the pods that require isolation into one or more different namespaces.
						</li><li class="listitem">
							Create additional targeted cross-namespace network policies to allow the specific traffic that you do want to allow from the isolated pods.
						</li></ul></div></section><section class="section" id="nw-networkpolicy-optimize-ovn_about-network-policy"><div class="titlepage"><div><div><h3 class="title">22.1.3. Optimizations for network policy with OVN-Kubernetes network plugin</h3></div></div></div><p>
					When designing your network policy, refer to the following guidelines:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							For network policies with the same <code class="literal">spec.podSelector</code> spec, it is more efficient to use one network policy with multiple <code class="literal">ingress</code> or <code class="literal">egress</code> rules, than multiple network policies with subsets of <code class="literal">ingress</code> or <code class="literal">egress</code> rules.
						</li><li class="listitem"><p class="simpara">
							Every <code class="literal">ingress</code> or <code class="literal">egress</code> rule based on the <code class="literal">podSelector</code> or <code class="literal">namespaceSelector</code> spec generates the number of OVS flows proportional to <code class="literal">number of pods selected by network policy + number of pods selected by ingress or egress rule</code>. Therefore, it is preferable to use the <code class="literal">podSelector</code> or <code class="literal">namespaceSelector</code> spec that can select as many pods as you need in one rule, instead of creating individual rules for every pod.
						</p><p class="simpara">
							For example, the following policy contains two rules:
						</p><pre class="programlisting language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
spec:
  podSelector: {}
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: frontend
  - from:
    - podSelector:
        matchLabels:
          role: backend</pre><p class="simpara">
							The following policy expresses those same two rules as one:
						</p><pre class="programlisting language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
spec:
  podSelector: {}
  ingress:
  - from:
    - podSelector:
        matchExpressions:
        - {key: role, operator: In, values: [frontend, backend]}</pre><p class="simpara">
							The same guideline applies to the <code class="literal">spec.podSelector</code> spec. If you have the same <code class="literal">ingress</code> or <code class="literal">egress</code> rules for different network policies, it might be more efficient to create one network policy with a common <code class="literal">spec.podSelector</code> spec. For example, the following two policies have different rules:
						</p><pre class="programlisting language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: policy1
spec:
  podSelector:
    matchLabels:
      role: db
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: frontend
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: policy2
spec:
  podSelector:
    matchLabels:
      role: client
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: frontend</pre><p class="simpara">
							The following network policy expresses those same two rules as one:
						</p><pre class="programlisting language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: policy3
spec:
  podSelector:
    matchExpressions:
    - {key: role, operator: In, values: [db, client]}
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: frontend</pre><p class="simpara">
							You can apply this optimization when only multiple selectors are expressed as one. In cases where selectors are based on different labels, it may not be possible to apply this optimization. In those cases, consider applying some new labels for network policy optimization specifically.
						</p></li></ul></div></section><section class="section" id="about-network-policy-next-steps"><div class="titlepage"><div><div><h3 class="title">22.1.4. Next steps</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#creating-network-policy">Creating a network policy</a>
						</li><li class="listitem">
							Optional: <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#default-network-policy">Defining a default network policy</a>
						</li></ul></div></section><section class="section _additional-resources" id="about-network-policy-additional-resources"><div class="titlepage"><div><div><h3 class="title">22.1.5. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#rbac-projects-namespaces_using-rbac">Projects and namespaces</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#multitenant-network-policy">Configuring multitenant network policy</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/api_reference/#networkpolicy-networking-k8s-io-v1">NetworkPolicy API</a>
						</li></ul></div></section></section><section class="section" id="creating-network-policy"><div class="titlepage"><div><div><h2 class="title">22.2. Creating a network policy</h2></div></div></div><p>
				As a user with the <code class="literal">admin</code> role, you can create a network policy for a namespace.
			</p><section class="section" id="nw-networkpolicy-object_creating-network-policy"><div class="titlepage"><div><div><h3 class="title">22.2.1. Example NetworkPolicy object</h3></div></div></div><p>
					The following annotates an example NetworkPolicy object:
				</p><pre class="programlisting language-yaml">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-27107 <span id="CO53-1"><!--Empty--></span><span class="callout">1</span>
spec:
  podSelector: <span id="CO53-2"><!--Empty--></span><span class="callout">2</span>
    matchLabels:
      app: mongodb
  ingress:
  - from:
    - podSelector: <span id="CO53-3"><!--Empty--></span><span class="callout">3</span>
        matchLabels:
          app: app
    ports: <span id="CO53-4"><!--Empty--></span><span class="callout">4</span>
    - protocol: TCP
      port: 27017</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO53-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The name of the NetworkPolicy object.
						</div></dd><dt><a href="#CO53-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							A selector that describes the pods to which the policy applies. The policy object can only select pods in the project that defines the NetworkPolicy object.
						</div></dd><dt><a href="#CO53-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							A selector that matches the pods from which the policy object allows ingress traffic. The selector matches pods in the same namespace as the NetworkPolicy.
						</div></dd><dt><a href="#CO53-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							A list of one or more destination ports on which to accept traffic.
						</div></dd></dl></div></section><section class="section admin" id="nw-networkpolicy-create-cli_creating-network-policy"><div class="titlepage"><div><div><h3 class="title">22.2.2. Creating a network policy using the CLI</h3></div></div></div><p class="admin admin">
					To define granular rules describing ingress or egress network traffic allowed for namespaces in your cluster, you can create a network policy.
				</p><div class="admonition note admin"><div class="admonition_header">Note</div><div><p class="admin admin">
						If you log in with a user with the <code class="literal admin">cluster-admin</code> role, then you can create a network policy in any namespace in the cluster.
					</p></div></div><div class="itemizedlist admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist admin" type="disc"><li class="listitem">
							Your cluster uses a network plugin that supports <code class="literal admin">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal admin">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
						</li><li class="listitem">
							You installed the OpenShift CLI (<code class="literal admin">oc</code>).
						</li><li class="listitem">
							You are logged in to the cluster with a user with <code class="literal admin">admin</code> privileges.
						</li><li class="listitem">
							You are working in the namespace that the network policy applies to.
						</li></ul></div><div class="orderedlist admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist admin" type="1"><li class="listitem"><p class="simpara">
							Create a policy rule:
						</p><div class="orderedlist admin"><ol class="orderedlist admin" type="a"><li class="listitem"><p class="simpara">
									Create a <code class="literal admin">&lt;policy_name&gt;.yaml</code> file:
								</p><pre class="programlisting language-terminal admin admin">$ touch &lt;policy_name&gt;.yaml</pre><p class="admin admin">
									where:
								</p><div class="variablelist admin"><dl class="variablelist admin"><dt><span class="term"><code class="literal admin">&lt;policy_name&gt;</code></span></dt><dd>
												Specifies the network policy file name.
											</dd></dl></div></li><li class="listitem"><p class="simpara">
									Define a network policy in the file that you just created, such as in the following examples:
								</p><div class="admin admin"><p class="title"><strong>Deny ingress from all pods in all namespaces</strong></p><p>
										This is a fundamental policy, blocking all cross-pod networking other than cross-pod traffic allowed by the configuration of other Network Policies.
									</p></div><pre class="programlisting language-yaml admin admin">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-by-default
spec:
  podSelector:
  ingress: []</pre><div class="admin admin"><p class="title"><strong>Allow ingress from all pods in the same namespace</strong></p><p>
										
<pre class="programlisting language-yaml">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-same-namespace
spec:
  podSelector:
  ingress:
  - from:
    - podSelector: {}</pre>

									</p></div><div class="admin admin"><p class="title"><strong>Allow ingress traffic to one pod from a particular namespace</strong></p><p>
										This policy allows traffic to pods labelled <code class="literal admin">pod-a</code> from pods running in <code class="literal admin">namespace-y</code>.
									</p></div><pre class="programlisting language-yaml admin admin">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-traffic-pod
spec:
  podSelector:
   matchLabels:
      pod: pod-a
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
           kubernetes.io/metadata.name: namespace-y</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							To create the network policy object, enter the following command:
						</p><pre class="programlisting language-terminal admin admin">$ oc apply -f &lt;policy_name&gt;.yaml -n &lt;namespace&gt;</pre><p class="admin admin">
							where:
						</p><div class="variablelist admin"><dl class="variablelist admin"><dt><span class="term"><code class="literal admin">&lt;policy_name&gt;</code></span></dt><dd>
										Specifies the network policy file name.
									</dd><dt><span class="term"><code class="literal admin">&lt;namespace&gt;</code></span></dt><dd>
										Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.
									</dd></dl></div><div class="admin admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">networkpolicy.networking.k8s.io/deny-by-default created</pre>

							</p></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you log in to the web console with <code class="literal">cluster-admin</code> privileges, you have a choice of creating a network policy in any namespace in the cluster directly in YAML or from a form in the web console.
					</p></div></div></section><section class="section admin" id="nw-networkpolicy-deny-all-multi-network-policy_creating-network-policy"><div class="titlepage"><div><div><h3 class="title">22.2.3. Creating a default deny all network policy</h3></div></div></div><p class="admin admin">
					This is a fundamental policy, blocking all cross-pod networking other than network traffic allowed by the configuration of other deployed network policies. This procedure enforces a default <code class="literal admin">deny-by-default</code> policy.
				</p><div class="admonition note admin"><div class="admonition_header">Note</div><div><p class="admin admin">
						If you log in with a user with the <code class="literal admin">cluster-admin</code> role, then you can create a network policy in any namespace in the cluster.
					</p></div></div><div class="itemizedlist admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist admin" type="disc"><li class="listitem">
							Your cluster uses a network plugin that supports <code class="literal admin">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal admin">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
						</li><li class="listitem">
							You installed the OpenShift CLI (<code class="literal admin">oc</code>).
						</li><li class="listitem">
							You are logged in to the cluster with a user with <code class="literal admin">admin</code> privileges.
						</li><li class="listitem">
							You are working in the namespace that the network policy applies to.
						</li></ul></div><div class="orderedlist admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist admin" type="1"><li class="listitem"><p class="simpara">
							Create the following YAML that defines a <code class="literal admin">deny-by-default</code> policy to deny ingress from all pods in all namespaces. Save the YAML in the <code class="literal admin">deny-by-default.yaml</code> file:
						</p><pre class="programlisting language-yaml admin admin">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-by-default
  namespace: default <span id="CO54-1"><!--Empty--></span><span class="callout">1</span>
spec:
  podSelector: {} <span id="CO54-2"><!--Empty--></span><span class="callout">2</span>
  ingress: [] <span id="CO54-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist admin"><dl class="calloutlist admin"><dt><a href="#CO54-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal admin">namespace: default</code> deploys this policy to the <code class="literal admin">default</code> namespace.
								</div></dd><dt><a href="#CO54-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									<code class="literal admin">podSelector:</code> is empty, this means it matches all the pods. Therefore, the policy applies to all pods in the default namespace.
								</div></dd><dt><a href="#CO54-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									There are no <code class="literal admin">ingress</code> rules specified. This causes incoming traffic to be dropped to all pods.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Apply the policy by entering the following command:
						</p><pre class="programlisting language-terminal admin admin">$ oc apply -f deny-by-default.yaml</pre><div class="admin admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">networkpolicy.networking.k8s.io/deny-by-default created</pre>

							</p></div></li></ol></div></section><section class="section admin" id="nw-networkpolicy-allow-external-clients_creating-network-policy"><div class="titlepage"><div><div><h3 class="title">22.2.4. Creating a network policy to allow traffic from external clients</h3></div></div></div><p class="admin admin">
					With the <code class="literal admin">deny-by-default</code> policy in place you can proceed to configure a policy that allows traffic from external clients to a pod with the label <code class="literal admin">app=web</code>.
				</p><div class="admonition note admin"><div class="admonition_header">Note</div><div><p class="admin admin">
						If you log in with a user with the <code class="literal admin">cluster-admin</code> role, then you can create a network policy in any namespace in the cluster.
					</p></div></div><p class="admin admin">
					Follow this procedure to configure a policy that allows external service from the public Internet directly or by using a Load Balancer to access the pod. Traffic is only allowed to a pod with the label <code class="literal admin">app=web</code>.
				</p><div class="itemizedlist admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist admin" type="disc"><li class="listitem">
							Your cluster uses a network plugin that supports <code class="literal admin">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal admin">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
						</li><li class="listitem">
							You installed the OpenShift CLI (<code class="literal admin">oc</code>).
						</li><li class="listitem">
							You are logged in to the cluster with a user with <code class="literal admin">admin</code> privileges.
						</li><li class="listitem">
							You are working in the namespace that the network policy applies to.
						</li></ul></div><div class="orderedlist admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist admin" type="1"><li class="listitem"><p class="simpara">
							Create a policy that allows traffic from the public Internet directly or by using a load balancer to access the pod. Save the YAML in the <code class="literal admin">web-allow-external.yaml</code> file:
						</p><pre class="programlisting language-yaml admin admin">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: web-allow-external
  namespace: default
spec:
  policyTypes:
  - Ingress
  podSelector:
    matchLabels:
      app: web
  ingress:
    - {}</pre></li><li class="listitem"><p class="simpara">
							Apply the policy by entering the following command:
						</p><pre class="programlisting language-terminal admin admin">$ oc apply -f web-allow-external.yaml</pre><div class="admin admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">networkpolicy.networking.k8s.io/web-allow-external created</pre>

							</p></div></li></ol></div><p class="admin admin">
					This policy allows traffic from all resources, including external traffic as illustrated in the following diagram:
				</p><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/418cc0d92d9424c51592a0cef00fa005/292_OpenShift_Configuring_multi-network_policy_1122.png" alt="Allow traffic from external clients"/></div></div></section><section class="section admin" id="nw-networkpolicy-allow-traffic-from-all-applications_creating-network-policy"><div class="titlepage"><div><div><h3 class="title">22.2.5. Creating a network policy allowing traffic to an application from all namespaces</h3></div></div></div><div class="admonition note admin"><div class="admonition_header">Note</div><div><p class="admin admin">
						If you log in with a user with the <code class="literal admin">cluster-admin</code> role, then you can create a network policy in any namespace in the cluster.
					</p></div></div><p class="admin admin">
					Follow this procedure to configure a policy that allows traffic from all pods in all namespaces to a particular application.
				</p><div class="itemizedlist admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist admin" type="disc"><li class="listitem">
							Your cluster uses a network plugin that supports <code class="literal admin">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal admin">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
						</li><li class="listitem">
							You installed the OpenShift CLI (<code class="literal admin">oc</code>).
						</li><li class="listitem">
							You are logged in to the cluster with a user with <code class="literal admin">admin</code> privileges.
						</li><li class="listitem">
							You are working in the namespace that the network policy applies to.
						</li></ul></div><div class="orderedlist admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist admin" type="1"><li class="listitem"><p class="simpara">
							Create a policy that allows traffic from all pods in all namespaces to a particular application. Save the YAML in the <code class="literal admin">web-allow-all-namespaces.yaml</code> file:
						</p><pre class="programlisting language-yaml admin admin">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: web-allow-all-namespaces
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: web <span id="CO55-1"><!--Empty--></span><span class="callout">1</span>
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector: {} <span id="CO55-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist admin"><dl class="calloutlist admin"><dt><a href="#CO55-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Applies the policy only to <code class="literal admin">app:web</code> pods in default namespace.
								</div></dd><dt><a href="#CO55-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Selects all pods in all namespaces.
								</div></dd></dl></div><div class="admonition note admin"><div class="admonition_header">Note</div><div><p class="admin admin">
								By default, if you omit specifying a <code class="literal admin">namespaceSelector</code> it does not select any namespaces, which means the policy allows traffic only from the namespace the network policy is deployed to.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Apply the policy by entering the following command:
						</p><pre class="programlisting language-terminal admin admin">$ oc apply -f web-allow-all-namespaces.yaml</pre><div class="admin admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">networkpolicy.networking.k8s.io/web-allow-all-namespaces created</pre>

							</p></div></li></ol></div><div class="orderedlist admin"><p class="title"><strong>Verification</strong></p><ol class="orderedlist admin" type="1"><li class="listitem"><p class="simpara">
							Start a web service in the <code class="literal admin">default</code> namespace by entering the following command:
						</p><pre class="programlisting language-terminal admin admin">$ oc run web --namespace=default --image=nginx --labels="app=web" --expose --port=80</pre></li><li class="listitem"><p class="simpara">
							Run the following command to deploy an <code class="literal admin">alpine</code> image in the <code class="literal admin">secondary</code> namespace and to start a shell:
						</p><pre class="programlisting language-terminal admin admin">$ oc run test-$RANDOM --namespace=secondary --rm -i -t --image=alpine -- sh</pre></li><li class="listitem"><p class="simpara">
							Run the following command in the shell and observe that the request is allowed:
						</p><pre class="programlisting language-terminal admin admin"># wget -qO- --timeout=2 http://web.default</pre><div class="admin admin"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</pre>

							</p></div></li></ol></div></section><section class="section admin" id="nw-networkpolicy-allow-traffic-from-a-namespace_creating-network-policy"><div class="titlepage"><div><div><h3 class="title">22.2.6. Creating a network policy allowing traffic to an application from a namespace</h3></div></div></div><div class="admonition note admin"><div class="admonition_header">Note</div><div><p class="admin admin">
						If you log in with a user with the <code class="literal admin">cluster-admin</code> role, then you can create a network policy in any namespace in the cluster.
					</p></div></div><p class="admin admin">
					Follow this procedure to configure a policy that allows traffic to a pod with the label <code class="literal admin">app=web</code> from a particular namespace. You might want to do this to:
				</p><div class="itemizedlist admin"><ul class="itemizedlist admin" type="disc"><li class="listitem">
							Restrict traffic to a production database only to namespaces where production workloads are deployed.
						</li><li class="listitem">
							Enable monitoring tools deployed to a particular namespace to scrape metrics from the current namespace.
						</li></ul></div><div class="itemizedlist admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist admin" type="disc"><li class="listitem">
							Your cluster uses a network plugin that supports <code class="literal admin">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal admin">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
						</li><li class="listitem">
							You installed the OpenShift CLI (<code class="literal admin">oc</code>).
						</li><li class="listitem">
							You are logged in to the cluster with a user with <code class="literal admin">admin</code> privileges.
						</li><li class="listitem">
							You are working in the namespace that the network policy applies to.
						</li></ul></div><div class="orderedlist admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist admin" type="1"><li class="listitem"><p class="simpara">
							Create a policy that allows traffic from all pods in a particular namespaces with a label <code class="literal admin">purpose=production</code>. Save the YAML in the <code class="literal admin">web-allow-prod.yaml</code> file:
						</p><pre class="programlisting language-yaml admin admin">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: web-allow-prod
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: web <span id="CO56-1"><!--Empty--></span><span class="callout">1</span>
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          purpose: production <span id="CO56-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist admin"><dl class="calloutlist admin"><dt><a href="#CO56-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Applies the policy only to <code class="literal admin">app:web</code> pods in the default namespace.
								</div></dd><dt><a href="#CO56-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Restricts traffic to only pods in namespaces that have the label <code class="literal admin">purpose=production</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Apply the policy by entering the following command:
						</p><pre class="programlisting language-terminal admin admin">$ oc apply -f web-allow-prod.yaml</pre><div class="admin admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">networkpolicy.networking.k8s.io/web-allow-prod created</pre>

							</p></div></li></ol></div><div class="orderedlist admin"><p class="title"><strong>Verification</strong></p><ol class="orderedlist admin" type="1"><li class="listitem"><p class="simpara">
							Start a web service in the <code class="literal admin">default</code> namespace by entering the following command:
						</p><pre class="programlisting language-terminal admin admin">$ oc run web --namespace=default --image=nginx --labels="app=web" --expose --port=80</pre></li><li class="listitem"><p class="simpara">
							Run the following command to create the <code class="literal admin">prod</code> namespace:
						</p><pre class="programlisting language-terminal admin admin">$ oc create namespace prod</pre></li><li class="listitem"><p class="simpara">
							Run the following command to label the <code class="literal admin">prod</code> namespace:
						</p><pre class="programlisting language-terminal admin admin">$ oc label namespace/prod purpose=production</pre></li><li class="listitem"><p class="simpara">
							Run the following command to create the <code class="literal admin">dev</code> namespace:
						</p><pre class="programlisting language-terminal admin admin">$ oc create namespace dev</pre></li><li class="listitem"><p class="simpara">
							Run the following command to label the <code class="literal admin">dev</code> namespace:
						</p><pre class="programlisting language-terminal admin admin">$ oc label namespace/dev purpose=testing</pre></li><li class="listitem"><p class="simpara">
							Run the following command to deploy an <code class="literal admin">alpine</code> image in the <code class="literal admin">dev</code> namespace and to start a shell:
						</p><pre class="programlisting language-terminal admin admin">$ oc run test-$RANDOM --namespace=dev --rm -i -t --image=alpine -- sh</pre></li><li class="listitem"><p class="simpara">
							Run the following command in the shell and observe that the request is blocked:
						</p><pre class="programlisting language-terminal admin admin"># wget -qO- --timeout=2 http://web.default</pre><div class="admin admin"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">wget: download timed out</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Run the following command to deploy an <code class="literal admin">alpine</code> image in the <code class="literal admin">prod</code> namespace and start a shell:
						</p><pre class="programlisting language-terminal admin admin">$ oc run test-$RANDOM --namespace=prod --rm -i -t --image=alpine -- sh</pre></li><li class="listitem"><p class="simpara">
							Run the following command in the shell and observe that the request is allowed:
						</p><pre class="programlisting language-terminal admin admin"># wget -qO- --timeout=2 http://web.default</pre><div class="admin admin"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</pre>

							</p></div></li></ol></div></section><section class="section _additional-resources" id="additional-resources-2"><div class="titlepage"><div><div><h3 class="title">22.2.7. Additional resources</h3></div></div></div><div class="itemizedlist admin"><ul class="itemizedlist admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/web_console/#web-console">Accessing the web console</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#logging-network-policy">Logging for egress firewall and network policy rules</a>
						</li></ul></div></section></section><section class="section admin" id="viewing-network-policy"><div class="titlepage"><div><div><h2 class="title">22.3. Viewing a network policy</h2></div></div></div><p class="admin admin">
				As a user with the <code class="literal admin">admin</code> role, you can view a network policy for a namespace.
			</p><section class="section admin" id="nw-networkpolicy-object_viewing-network-policy"><div class="titlepage"><div><div><h3 class="title">22.3.1. Example NetworkPolicy object</h3></div></div></div><p class="admin admin">
					The following annotates an example NetworkPolicy object:
				</p><pre class="programlisting language-yaml admin admin">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-27107 <span id="CO57-1"><!--Empty--></span><span class="callout">1</span>
spec:
  podSelector: <span id="CO57-2"><!--Empty--></span><span class="callout">2</span>
    matchLabels:
      app: mongodb
  ingress:
  - from:
    - podSelector: <span id="CO57-3"><!--Empty--></span><span class="callout">3</span>
        matchLabels:
          app: app
    ports: <span id="CO57-4"><!--Empty--></span><span class="callout">4</span>
    - protocol: TCP
      port: 27017</pre><div class="calloutlist admin"><dl class="calloutlist admin"><dt><a href="#CO57-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The name of the NetworkPolicy object.
						</div></dd><dt><a href="#CO57-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							A selector that describes the pods to which the policy applies. The policy object can only select pods in the project that defines the NetworkPolicy object.
						</div></dd><dt><a href="#CO57-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							A selector that matches the pods from which the policy object allows ingress traffic. The selector matches pods in the same namespace as the NetworkPolicy.
						</div></dd><dt><a href="#CO57-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							A list of one or more destination ports on which to accept traffic.
						</div></dd></dl></div></section><section class="section admin" id="nw-networkpolicy-view-cli_viewing-network-policy"><div class="titlepage"><div><div><h3 class="title">22.3.2. Viewing network policies using the CLI</h3></div></div></div><p class="admin admin">
					You can examine the network policies in a namespace.
				</p><div class="admonition note admin"><div class="admonition_header">Note</div><div><p class="admin admin">
						If you log in with a user with the <code class="literal admin">cluster-admin</code> role, then you can view any network policy in the cluster.
					</p></div></div><div class="itemizedlist admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist admin" type="disc"><li class="listitem">
							You installed the OpenShift CLI (<code class="literal admin">oc</code>).
						</li><li class="listitem">
							You are logged in to the cluster with a user with <code class="literal admin">admin</code> privileges.
						</li><li class="listitem">
							You are working in the namespace where the network policy exists.
						</li></ul></div><div class="itemizedlist admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist admin" type="disc"><li class="listitem"><p class="simpara">
							List network policies in a namespace:
						</p><div class="itemizedlist admin"><ul class="itemizedlist admin" type="circle"><li class="listitem"><p class="simpara">
									To view network policy objects defined in a namespace, enter the following command:
								</p><pre class="programlisting language-terminal admin admin">$ oc get networkpolicy</pre></li><li class="listitem"><p class="simpara">
									Optional: To examine a specific network policy, enter the following command:
								</p><pre class="programlisting language-terminal admin admin">$ oc describe networkpolicy &lt;policy_name&gt; -n &lt;namespace&gt;</pre><p class="admin admin">
									where:
								</p><div class="variablelist admin"><dl class="variablelist admin"><dt><span class="term"><code class="literal admin">&lt;policy_name&gt;</code></span></dt><dd>
												Specifies the name of the network policy to inspect.
											</dd><dt><span class="term"><code class="literal admin">&lt;namespace&gt;</code></span></dt><dd>
												Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.
											</dd></dl></div><p class="admin admin">
									For example:
								</p><pre class="programlisting language-terminal admin admin">$ oc describe networkpolicy allow-same-namespace</pre><div class="admin admin"><p class="title"><strong>Output for <code class="literal admin">oc describe</code> command</strong></p><p>
										
<pre class="programlisting language-text">Name:         allow-same-namespace
Namespace:    ns1
Created on:   2021-05-24 22:28:56 -0400 EDT
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Spec:
  PodSelector:     &lt;none&gt; (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    To Port: &lt;any&gt; (traffic allowed to all ports)
    From:
      PodSelector: &lt;none&gt;
  Not affecting egress traffic
  Policy Types: Ingress</pre>

									</p></div></li></ul></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you log in to the web console with <code class="literal">cluster-admin</code> privileges, you have a choice of viewing a network policy in any namespace in the cluster directly in YAML or from a form in the web console.
					</p></div></div></section></section><section class="section" id="editing-network-policy"><div class="titlepage"><div><div><h2 class="title">22.4. Editing a network policy</h2></div></div></div><p>
				As a user with the <code class="literal">admin</code> role, you can edit an existing network policy for a namespace.
			</p><section class="section admin" id="nw-networkpolicy-edit_editing-network-policy"><div class="titlepage"><div><div><h3 class="title">22.4.1. Editing a network policy</h3></div></div></div><p class="admin admin">
					You can edit a network policy in a namespace.
				</p><div class="admonition note admin"><div class="admonition_header">Note</div><div><p class="admin admin">
						If you log in with a user with the <code class="literal admin">cluster-admin</code> role, then you can edit a network policy in any namespace in the cluster.
					</p></div></div><div class="itemizedlist admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist admin" type="disc"><li class="listitem">
							Your cluster uses a network plugin that supports <code class="literal admin">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal admin">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
						</li><li class="listitem">
							You installed the OpenShift CLI (<code class="literal admin">oc</code>).
						</li><li class="listitem">
							You are logged in to the cluster with a user with <code class="literal admin">admin</code> privileges.
						</li><li class="listitem">
							You are working in the namespace where the network policy exists.
						</li></ul></div><div class="orderedlist admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist admin" type="1"><li class="listitem"><p class="simpara">
							Optional: To list the network policy objects in a namespace, enter the following command:
						</p><pre class="programlisting language-terminal admin admin">$ oc get networkpolicy</pre><p class="admin admin">
							where:
						</p><div class="variablelist admin"><dl class="variablelist admin"><dt><span class="term"><code class="literal admin">&lt;namespace&gt;</code></span></dt><dd>
										Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.
									</dd></dl></div></li><li class="listitem"><p class="simpara">
							Edit the network policy object.
						</p><div class="itemizedlist admin"><ul class="itemizedlist admin" type="disc"><li class="listitem"><p class="simpara">
									If you saved the network policy definition in a file, edit the file and make any necessary changes, and then enter the following command.
								</p><pre class="programlisting language-terminal admin admin">$ oc apply -n &lt;namespace&gt; -f &lt;policy_file&gt;.yaml</pre><p class="admin admin">
									where:
								</p><div class="variablelist admin"><dl class="variablelist admin"><dt><span class="term"><code class="literal admin">&lt;namespace&gt;</code></span></dt><dd>
												Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.
											</dd><dt><span class="term"><code class="literal admin">&lt;policy_file&gt;</code></span></dt><dd>
												Specifies the name of the file containing the network policy.
											</dd></dl></div></li><li class="listitem"><p class="simpara">
									If you need to update the network policy object directly, enter the following command:
								</p><pre class="programlisting language-terminal admin admin">$ oc edit networkpolicy &lt;policy_name&gt; -n &lt;namespace&gt;</pre><p class="admin admin">
									where:
								</p><div class="variablelist admin"><dl class="variablelist admin"><dt><span class="term"><code class="literal admin">&lt;policy_name&gt;</code></span></dt><dd>
												Specifies the name of the network policy.
											</dd><dt><span class="term"><code class="literal admin">&lt;namespace&gt;</code></span></dt><dd>
												Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.
											</dd></dl></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Confirm that the network policy object is updated.
						</p><pre class="programlisting language-terminal admin admin">$ oc describe networkpolicy &lt;policy_name&gt; -n &lt;namespace&gt;</pre><p class="admin admin">
							where:
						</p><div class="variablelist admin"><dl class="variablelist admin"><dt><span class="term"><code class="literal admin">&lt;policy_name&gt;</code></span></dt><dd>
										Specifies the name of the network policy.
									</dd><dt><span class="term"><code class="literal admin">&lt;namespace&gt;</code></span></dt><dd>
										Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.
									</dd></dl></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you log in to the web console with <code class="literal">cluster-admin</code> privileges, you have a choice of editing a network policy in any namespace in the cluster directly in YAML or from the policy in the web console through the <span class="strong strong"><strong>Actions</strong></span> menu.
					</p></div></div></section><section class="section" id="nw-networkpolicy-object_editing-network-policy"><div class="titlepage"><div><div><h3 class="title">22.4.2. Example NetworkPolicy object</h3></div></div></div><p>
					The following annotates an example NetworkPolicy object:
				</p><pre class="programlisting language-yaml">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-27107 <span id="CO58-1"><!--Empty--></span><span class="callout">1</span>
spec:
  podSelector: <span id="CO58-2"><!--Empty--></span><span class="callout">2</span>
    matchLabels:
      app: mongodb
  ingress:
  - from:
    - podSelector: <span id="CO58-3"><!--Empty--></span><span class="callout">3</span>
        matchLabels:
          app: app
    ports: <span id="CO58-4"><!--Empty--></span><span class="callout">4</span>
    - protocol: TCP
      port: 27017</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO58-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The name of the NetworkPolicy object.
						</div></dd><dt><a href="#CO58-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							A selector that describes the pods to which the policy applies. The policy object can only select pods in the project that defines the NetworkPolicy object.
						</div></dd><dt><a href="#CO58-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							A selector that matches the pods from which the policy object allows ingress traffic. The selector matches pods in the same namespace as the NetworkPolicy.
						</div></dd><dt><a href="#CO58-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							A list of one or more destination ports on which to accept traffic.
						</div></dd></dl></div></section><section class="section _additional-resources" id="editing-network-policy-additional-resources"><div class="titlepage"><div><div><h3 class="title">22.4.3. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#creating-network-policy">Creating a network policy</a>
						</li></ul></div></section></section><section class="section" id="deleting-network-policy"><div class="titlepage"><div><div><h2 class="title">22.5. Deleting a network policy</h2></div></div></div><p>
				As a user with the <code class="literal">admin</code> role, you can delete a network policy from a namespace.
			</p><section class="section admin" id="nw-networkpolicy-delete-cli_deleting-network-policy"><div class="titlepage"><div><div><h3 class="title">22.5.1. Deleting a network policy using the CLI</h3></div></div></div><p class="admin admin">
					You can delete a network policy in a namespace.
				</p><div class="admonition note admin"><div class="admonition_header">Note</div><div><p class="admin admin">
						If you log in with a user with the <code class="literal admin">cluster-admin</code> role, then you can delete any network policy in the cluster.
					</p></div></div><div class="itemizedlist admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist admin" type="disc"><li class="listitem">
							Your cluster uses a network plugin that supports <code class="literal admin">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal admin">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
						</li><li class="listitem">
							You installed the OpenShift CLI (<code class="literal admin">oc</code>).
						</li><li class="listitem">
							You are logged in to the cluster with a user with <code class="literal admin">admin</code> privileges.
						</li><li class="listitem">
							You are working in the namespace where the network policy exists.
						</li></ul></div><div class="itemizedlist admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist admin" type="disc"><li class="listitem"><p class="simpara">
							To delete a network policy object, enter the following command:
						</p><pre class="programlisting language-terminal admin admin">$ oc delete networkpolicy &lt;policy_name&gt; -n &lt;namespace&gt;</pre><p class="admin admin">
							where:
						</p><div class="variablelist admin"><dl class="variablelist admin"><dt><span class="term"><code class="literal admin">&lt;policy_name&gt;</code></span></dt><dd>
										Specifies the name of the network policy.
									</dd><dt><span class="term"><code class="literal admin">&lt;namespace&gt;</code></span></dt><dd>
										Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.
									</dd></dl></div><div class="admin admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">networkpolicy.networking.k8s.io/default-deny deleted</pre>

							</p></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you log in to the web console with <code class="literal">cluster-admin</code> privileges, you have a choice of deleting a network policy in any namespace in the cluster directly in YAML or from the policy in the web console through the <span class="strong strong"><strong>Actions</strong></span> menu.
					</p></div></div></section></section><section class="section" id="default-network-policy"><div class="titlepage"><div><div><h2 class="title">22.6. Defining a default network policy for projects</h2></div></div></div><p>
				As a cluster administrator, you can modify the new project template to automatically include network policies when you create a new project. If you do not yet have a customized template for new projects, you must first create one.
			</p><section class="section" id="modifying-template-for-new-projects_default-network-policy"><div class="titlepage"><div><div><h3 class="title">22.6.1. Modifying the template for new projects</h3></div></div></div><p>
					As a cluster administrator, you can modify the default project template so that new projects are created using your custom requirements.
				</p><p>
					To create your own custom project template:
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem"><p class="simpara">
							Generate the default project template:
						</p><pre class="programlisting language-terminal">$ oc adm create-bootstrap-project-template -o yaml &gt; template.yaml</pre></li><li class="listitem">
							Use a text editor to modify the generated <code class="literal">template.yaml</code> file by adding objects or modifying existing objects.
						</li><li class="listitem"><p class="simpara">
							The project template must be created in the <code class="literal">openshift-config</code> namespace. Load your modified template:
						</p><pre class="programlisting language-terminal">$ oc create -f template.yaml -n openshift-config</pre></li><li class="listitem"><p class="simpara">
							Edit the project configuration resource using the web console or CLI.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Using the web console:
								</p><div class="orderedlist"><ol class="orderedlist" type="i"><li class="listitem">
											Navigate to the <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Cluster Settings</strong></span> page.
										</li><li class="listitem">
											Click <span class="strong strong"><strong>Configuration</strong></span> to view all configuration resources.
										</li><li class="listitem">
											Find the entry for <span class="strong strong"><strong>Project</strong></span> and click <span class="strong strong"><strong>Edit YAML</strong></span>.
										</li></ol></div></li><li class="listitem"><p class="simpara">
									Using the CLI:
								</p><div class="orderedlist"><ol class="orderedlist" type="i"><li class="listitem"><p class="simpara">
											Edit the <code class="literal">project.config.openshift.io/cluster</code> resource:
										</p><pre class="programlisting language-terminal">$ oc edit project.config.openshift.io/cluster</pre></li></ol></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Update the <code class="literal">spec</code> section to include the <code class="literal">projectRequestTemplate</code> and <code class="literal">name</code> parameters, and set the name of your uploaded project template. The default name is <code class="literal">project-request</code>.
						</p><div class="formalpara"><p class="title"><strong>Project configuration resource with custom project template</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Project
metadata:
  ...
spec:
  projectRequestTemplate:
    name: &lt;template_name&gt;</pre>

							</p></div></li><li class="listitem">
							After you save your changes, create a new project to verify that your changes were successfully applied.
						</li></ol></div></section><section class="section" id="nw-networkpolicy-project-defaults_default-network-policy"><div class="titlepage"><div><div><h3 class="title">22.6.2. Adding network policies to the new project template</h3></div></div></div><p>
					As a cluster administrator, you can add network policies to the default template for new projects. OpenShift Container Platform will automatically create all the <code class="literal">NetworkPolicy</code> objects specified in the template in the project.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Your cluster uses a default CNI network provider that supports <code class="literal">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
						</li><li class="listitem">
							You installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You must log in to the cluster with a user with <code class="literal">cluster-admin</code> privileges.
						</li><li class="listitem">
							You must have created a custom default project template for new projects.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the default template for a new project by running the following command:
						</p><pre class="programlisting language-terminal">$ oc edit template &lt;project_template&gt; -n openshift-config</pre><p class="simpara">
							Replace <code class="literal">&lt;project_template&gt;</code> with the name of the default template that you configured for your cluster. The default template name is <code class="literal">project-request</code>.
						</p></li><li class="listitem"><p class="simpara">
							In the template, add each <code class="literal">NetworkPolicy</code> object as an element to the <code class="literal">objects</code> parameter. The <code class="literal">objects</code> parameter accepts a collection of one or more objects.
						</p><p class="simpara">
							In the following example, the <code class="literal">objects</code> parameter collection includes several <code class="literal">NetworkPolicy</code> objects.
						</p><pre class="programlisting language-yaml">objects:
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allow-from-same-namespace
  spec:
    podSelector: {}
    ingress:
    - from:
      - podSelector: {}
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allow-from-openshift-ingress
  spec:
    ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            network.openshift.io/policy-group: ingress
    podSelector: {}
    policyTypes:
    - Ingress
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allow-from-kube-apiserver-operator
  spec:
    ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            kubernetes.io/metadata.name: openshift-kube-apiserver-operator
        podSelector:
          matchLabels:
            app: kube-apiserver-operator
    policyTypes:
    - Ingress
...</pre></li><li class="listitem"><p class="simpara">
							Optional: Create a new project to confirm that your network policy objects are created successfully by running the following commands:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create a new project:
								</p><pre class="programlisting language-terminal">$ oc new-project &lt;project&gt; <span id="CO59-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO59-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Replace <code class="literal">&lt;project&gt;</code> with the name for the project you are creating.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Confirm that the network policy objects in the new project template exist in the new project:
								</p><pre class="programlisting language-terminal">$ oc get networkpolicy
NAME                           POD-SELECTOR   AGE
allow-from-openshift-ingress   &lt;none&gt;         7s
allow-from-same-namespace      &lt;none&gt;         7s</pre></li></ol></div></li></ol></div></section></section><section class="section" id="multitenant-network-policy"><div class="titlepage"><div><div><h2 class="title">22.7. Configuring multitenant isolation with network policy</h2></div></div></div><p>
				As a cluster administrator, you can configure your network policies to provide multitenant network isolation.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					If you are using the OpenShift SDN network plugin, configuring network policies as described in this section provides network isolation similar to multitenant mode but with network policy mode set.
				</p></div></div><section class="section" id="nw-networkpolicy-multitenant-isolation_multitenant-network-policy"><div class="titlepage"><div><div><h3 class="title">22.7.1. Configuring multitenant isolation by using network policy</h3></div></div></div><p>
					You can configure your project to isolate it from pods and services in other project namespaces.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Your cluster uses a network plugin that supports <code class="literal">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
						</li><li class="listitem">
							You installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You are logged in to the cluster with a user with <code class="literal">admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the following <code class="literal">NetworkPolicy</code> objects:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									A policy named <code class="literal">allow-from-openshift-ingress</code>.
								</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-ingress
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          policy-group.network.openshift.io/ingress: ""
  podSelector: {}
  policyTypes:
  - Ingress
EOF</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										<code class="literal">policy-group.network.openshift.io/ingress: ""</code> is the preferred namespace selector label for OpenShift SDN. You can use the <code class="literal">network.openshift.io/policy-group: ingress</code> namespace selector label, but this is a legacy label.
									</p></div></div></li><li class="listitem"><p class="simpara">
									A policy named <code class="literal">allow-from-openshift-monitoring</code>:
								</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-monitoring
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: monitoring
  podSelector: {}
  policyTypes:
  - Ingress
EOF</pre></li><li class="listitem"><p class="simpara">
									A policy named <code class="literal">allow-same-namespace</code>:
								</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF| oc create -f -
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-same-namespace
spec:
  podSelector:
  ingress:
  - from:
    - podSelector: {}
EOF</pre></li><li class="listitem"><p class="simpara">
									A policy named <code class="literal">allow-from-kube-apiserver-operator</code>:
								</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-kube-apiserver-operator
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: openshift-kube-apiserver-operator
      podSelector:
        matchLabels:
          app: kube-apiserver-operator
  policyTypes:
  - Ingress
EOF</pre><p class="simpara">
									For more details, see <a class="link" href="https://access.redhat.com/solutions/6964520">New <code class="literal">kube-apiserver-operator</code> webhook controller validating health of webhook</a>.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Optional: To confirm that the network policies exist in your current project, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc describe networkpolicy</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">Name:         allow-from-openshift-ingress
Namespace:    example1
Created on:   2020-06-09 00:28:17 -0400 EDT
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Spec:
  PodSelector:     &lt;none&gt; (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    To Port: &lt;any&gt; (traffic allowed to all ports)
    From:
      NamespaceSelector: network.openshift.io/policy-group: ingress
  Not affecting egress traffic
  Policy Types: Ingress


Name:         allow-from-openshift-monitoring
Namespace:    example1
Created on:   2020-06-09 00:29:57 -0400 EDT
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Spec:
  PodSelector:     &lt;none&gt; (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    To Port: &lt;any&gt; (traffic allowed to all ports)
    From:
      NamespaceSelector: network.openshift.io/policy-group: monitoring
  Not affecting egress traffic
  Policy Types: Ingress</pre>

							</p></div></li></ol></div></section><section class="section" id="multitenant-network-policy-next-steps"><div class="titlepage"><div><div><h3 class="title">22.7.2. Next steps</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#default-network-policy">Defining a default network policy</a>
						</li></ul></div></section><section class="section _additional-resources" id="multitenant-network-policy-additional-resources"><div class="titlepage"><div><div><h3 class="title">22.7.3. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-openshift-sdn-modes_about-openshift-sdn">OpenShift SDN network isolation modes</a>
						</li></ul></div></section></section></section><section class="chapter" id="aws-load-balancer-operator-1"><div class="titlepage"><div><div><h1 class="title">Chapter 23. AWS Load Balancer Operator</h1></div></div></div><section class="section" id="aws-load-balancer-operator-release-notes"><div class="titlepage"><div><div><h2 class="title">23.1. AWS Load Balancer Operator release notes</h2></div></div></div><p>
				The AWS Load Balancer (ALB) Operator deploys and manages an instance of the <code class="literal">AWSLoadBalancerController</code> resource.
			</p><p>
				These release notes track the development of the AWS Load Balancer Operator in OpenShift Container Platform.
			</p><p>
				For an overview of the AWS Load Balancer Operator, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#aws-load-balancer-operator">AWS Load Balancer Operator in OpenShift Container Platform</a>.
			</p><section class="section" id="aws-load-balancer-operator-release-notes-1.0.0"><div class="titlepage"><div><div><h3 class="title">23.1.1. AWS Load Balancer Operator 1.0.0</h3></div></div></div><p>
					The AWS Load Balancer Operator is now generally available with this release. The AWS Load Balancer Operator version 1.0.0 supports the AWS Load Balancer Controller version 2.4.4.
				</p><p>
					The following advisory is available for the AWS Load Balancer Operator version 1.0.0:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/errata/RHEA-2023:1954">RHEA-2023:1954 Release of AWS Load Balancer Operator on OperatorHub Enhancement Advisory Update</a>
						</li></ul></div><section class="section" id="aws-load-balancer-operator-1.0.0-notable-changes"><div class="titlepage"><div><div><h4 class="title">23.1.1.1. Notable changes</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								This release uses the new <code class="literal">v1</code> API version.
							</li></ul></div></section><section class="section" id="aws-load-balancer-operator-1.0.0-bug-fixes"><div class="titlepage"><div><div><h4 class="title">23.1.1.2. Bug fixes</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Previously, the controller provisioned by the AWS Load Balancer Operator did not properly use the configuration for the cluster-wide proxy. These settings are now applied appropriately to the controller. (<a class="link" href="https://issues.redhat.com/browse/OCPBUGS-4052"><span class="strong strong"><strong>OCPBUGS-4052</strong></span></a>, <a class="link" href="https://issues.redhat.com/browse/OCPBUGS-5295"><span class="strong strong"><strong>OCPBUGS-5295</strong></span></a>)
							</li></ul></div></section></section><section class="section" id="aws-load-balancer-operator-release-notes-earlier-versions"><div class="titlepage"><div><div><h3 class="title">23.1.2. Earlier versions</h3></div></div></div><p>
					The two earliest versions of the AWS Load Balancer Operator are available as a Technology Preview. These versions should not be used in a production cluster. For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
				</p><p>
					The following advisory is available for the AWS Load Balancer Operator version 0.2.0:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/errata/RHEA-2022:9084">RHEA-2022:9084 Release of AWS Load Balancer Operator on OperatorHub Enhancement Advisory Update</a>
						</li></ul></div><p>
					The following advisory is available for the AWS Load Balancer Operator version 0.0.1:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/errata/RHEA-2022:5780">RHEA-2022:5780 Release of AWS Load Balancer Operator on OperatorHub Enhancement Advisory Update</a>
						</li></ul></div></section></section><section class="section" id="aws-load-balancer-operator"><div class="titlepage"><div><div><h2 class="title">23.2. AWS Load Balancer Operator in OpenShift Container Platform</h2></div></div></div><p>
				The AWS Load Balancer (ALB) Operator deploys and manages an instance of the <code class="literal">aws-load-balancer-controller</code>. You can install the ALB Operator from the OperatorHub by using OpenShift Container Platform web console or CLI.
			</p><section class="section" id="nw-aws-load-balancer-operator-considerations_aws-load-balancer-operator"><div class="titlepage"><div><div><h3 class="title">23.2.1. AWS Load Balancer Operator considerations</h3></div></div></div><p>
					Review the following limitations before installing and using the AWS Load Balancer Operator.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The IP traffic mode only works on AWS Elastic Kubernetes Service (EKS). The AWS Load Balancer Operator disables the IP traffic mode for the AWS Load Balancer Controller. As a result of disabling the IP traffic mode, the AWS Load Balancer Controller cannot use the pod readiness gate.
						</li><li class="listitem">
							The AWS Load Balancer Operator adds command-line flags such as <code class="literal">--disable-ingress-class-annotation</code> and <code class="literal">--disable-ingress-group-name-annotation</code> to the AWS Load Balancer Controller. Therefore, the AWS Load Balancer Operator does not allow using the <code class="literal">kubernetes.io/ingress.class</code> and <code class="literal">alb.ingress.kubernetes.io/group.name</code> annotations in the <code class="literal">Ingress</code> resource.
						</li></ul></div></section><section class="section" id="nw-aws-load-balancer-operator_aws-load-balancer-operator"><div class="titlepage"><div><div><h3 class="title">23.2.2. AWS Load Balancer Operator</h3></div></div></div><p>
					The AWS Load Balancer Operator can tag the public subnets if the <code class="literal">kubernetes.io/role/elb</code> tag is missing. Also, the AWS Load Balancer Operator detects the following from the underlying AWS cloud:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The ID of the virtual private cloud (VPC) on which the cluster hosting the Operator is deployed in.
						</li><li class="listitem">
							Public and private subnets of the discovered VPC.
						</li></ul></div><p>
					The AWS Load Balancer Operator supports the Kubernetes service resource of type <code class="literal">LoadBalancer</code> by using Network Load Balancer (NLB) with the <code class="literal">instance</code> target type only.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You must have the AWS credentials secret. The credentials are used to provide subnet tagging and VPC discovery.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							You can deploy the AWS Load Balancer Operator on demand from the OperatorHub, by creating a <code class="literal">Subscription</code> object:
						</p><pre class="programlisting language-terminal">$ oc -n aws-load-balancer-operator get sub aws-load-balancer-operator --template='{{.status.installplan.name}}{{"\n"}}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">install-zlfbt</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check the status of an install plan. The status of an install plan must be <code class="literal">Complete</code>:
						</p><pre class="programlisting language-terminal">$ oc -n aws-load-balancer-operator get ip &lt;install_plan_name&gt; --template='{{.status.phase}}{{"\n"}}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Complete</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Use the <code class="literal">oc get</code> command to view the <code class="literal">Deployment</code> status:
						</p><pre class="programlisting language-terminal">$ oc get -n aws-load-balancer-operator deployment/aws-load-balancer-operator-controller-manager</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                           READY     UP-TO-DATE   AVAILABLE   AGE
aws-load-balancer-operator-controller-manager  1/1       1            1           23h</pre>

							</p></div></li></ol></div></section><section class="section" id="nw-aws-load-balancer-operator-logs_aws-load-balancer-operator"><div class="titlepage"><div><div><h3 class="title">23.2.3. AWS Load Balancer Operator logs</h3></div></div></div><p>
					Use the <code class="literal">oc logs</code> command to view the AWS Load Balancer Operator logs.
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							View the logs of the AWS Load Balancer Operator:
						</p><pre class="programlisting language-terminal">$ oc logs -n aws-load-balancer-operator deployment/aws-load-balancer-operator-controller-manager -c manager</pre></li></ul></div></section></section><section class="section" id="nw-aws-load-balancer-operator"><div class="titlepage"><div><div><h2 class="title">23.3. Understanding AWS Load Balancer Operator</h2></div></div></div><p>
				The AWS Load Balancer (ALB) Operator deploys and manages an instance of the <code class="literal">aws-load-balancer-controller</code> resource. You can install the AWS Load Balancer Operator from the OperatorHub by using OpenShift Container Platform web console or CLI.
			</p><section class="section" id="nw-installing-aws-load-balancer-operator_aws-load-balancer-operator"><div class="titlepage"><div><div><h3 class="title">23.3.1. Installing the AWS Load Balancer Operator</h3></div></div></div><p>
					You can install the AWS Load Balancer Operator from the OperatorHub by using the OpenShift Container Platform web console.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have logged in to the OpenShift Container Platform web console as a user with <code class="literal">cluster-admin</code> permissions.
						</li><li class="listitem">
							Your cluster is configured with AWS as the platform type and cloud provider.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span> in the OpenShift Container Platform web console.
						</li><li class="listitem">
							Select the <span class="strong strong"><strong>AWS Load Balancer Operator</strong></span>. You can use the <span class="strong strong"><strong>Filter by keyword</strong></span> text box or use the filter list to search for the AWS Load Balancer Operator from the list of Operators.
						</li><li class="listitem">
							Select the <code class="literal">aws-load-balancer-operator</code> namespace.
						</li><li class="listitem">
							Follow the instructions to prepare the Operator for installation.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>AWS Load Balancer Operator</strong></span> page, click <span class="strong strong"><strong>Install</strong></span>.
						</li><li class="listitem"><p class="simpara">
							On the <span class="strong strong"><strong>Install Operator</strong></span> page, select the following options:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									<span class="strong strong"><strong>Update the channel</strong></span> as <span class="strong strong"><strong>stable-v1</strong></span>.
								</li><li class="listitem">
									<span class="strong strong"><strong>Installation mode</strong></span> as <span class="strong strong"><strong>A specific namespace on the cluster</strong></span>.
								</li><li class="listitem">
									<span class="strong strong"><strong>Installed Namespace</strong></span> as <code class="literal">aws-load-balancer-operator</code>. If the <code class="literal">aws-load-balancer-operator</code> namespace does not exist, it gets created during the Operator installation.
								</li><li class="listitem">
									Select <span class="strong strong"><strong>Update approval</strong></span> as <span class="strong strong"><strong>Automatic</strong></span> or <span class="strong strong"><strong>Manual</strong></span>. By default, the <span class="strong strong"><strong>Update approval</strong></span> is set to <span class="strong strong"><strong>Automatic</strong></span>. If you select automatic updates, the Operator Lifecycle Manager (OLM) automatically upgrades the running instance of your Operator without any intervention. If you select manual updates, the OLM creates an update request. As a cluster administrator, you must then manually approve that update request to update the Operator updated to the new version.
								</li><li class="listitem">
									Click <span class="strong strong"><strong>Install</strong></span>.
								</li></ol></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Verify that the AWS Load Balancer Operator shows the <span class="strong strong"><strong>Status</strong></span> as <span class="strong strong"><strong>Succeeded</strong></span> on the Installed Operators dashboard.
						</li></ul></div></section></section><section class="section" id="albo-sts-cluster"><div class="titlepage"><div><div><h2 class="title">23.4. Installing AWS Load Balancer Operator on a Security Token Service cluster</h2></div></div></div><p>
				You can install the AWS Load Balancer Operator on a Security Token Service (STS) cluster.
			</p><p>
				The AWS Load Balancer Operator relies on <code class="literal">CredentialsRequest</code> to bootstrap the Operator and for each <code class="literal">AWSLoadBalancerController</code> instance. The AWS Load Balancer Operator waits until the required secrets are created and available. The Cloud Credential Operator does not provision the secrets automatically in the STS cluster. You must set the credentials secrets manually by using the <code class="literal">ccoctl</code> binary.
			</p><p>
				If you do not want to provision credential secret by using the Cloud Credential Operator, you can configure the <code class="literal">AWSLoadBalancerController</code> instance on the STS cluster by specifying the credential secret in the AWS load Balancer Controller custom resource (CR).
			</p><section class="section" id="nw-bootstra-albo-on-sts-cluster_albo-sts-cluster"><div class="titlepage"><div><div><h3 class="title">23.4.1. Bootstrapping AWS Load Balancer Operator on Security Token Service cluster</h3></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You must extract and prepare the <code class="literal">ccoctl</code> binary.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the <code class="literal">aws-load-balancer-operator</code> namespace by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create namespace aws-load-balancer-operator</pre></li><li class="listitem"><p class="simpara">
							Download the <code class="literal">CredentialsRequest</code> custom resource (CR) of the AWS Load Balancer Operator, and create a directory to store it by running the following command:
						</p><pre class="programlisting language-terminal">$ curl --create-dirs -o &lt;path-to-credrequests-dir&gt;/cr.yaml https://raw.githubusercontent.com/openshift/aws-load-balancer-operator/main/hack/operator-credentials-request.yaml</pre></li><li class="listitem"><p class="simpara">
							Use the <code class="literal">ccoctl</code> tool to process <code class="literal">CredentialsRequest</code> objects of the AWS Load Balancer Operator, by running the following command:
						</p><pre class="programlisting language-terminal">$ ccoctl aws create-iam-roles \
    --name &lt;name&gt; --region=&lt;aws_region&gt; \
    --credentials-requests-dir=&lt;path-to-credrequests-dir&gt; \
    --identity-provider-arn &lt;oidc-arn&gt;</pre></li><li class="listitem"><p class="simpara">
							Apply the secrets generated in the manifests directory of your cluster by running the following command:
						</p><pre class="programlisting language-terminal">$ ls manifests/*-credentials.yaml | xargs -I{} oc apply -f {}</pre></li><li class="listitem"><p class="simpara">
							Verify that the credentials secret of the AWS Load Balancer Operator is created by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n aws-load-balancer-operator get secret aws-load-balancer-operator --template='{{index .data "credentials"}}' | base64 -d</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">[default]
sts_regional_endpoints = regional
role_arn = arn:aws:iam::999999999999:role/aws-load-balancer-operator-aws-load-balancer-operator
web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token</pre>

							</p></div></li></ol></div></section><section class="section" id="nw-installing-albo-on-sts-cluster_albo-sts-cluster"><div class="titlepage"><div><div><h3 class="title">23.4.2. Configuring AWS Load Balancer Operator on Security Token Service cluster by using managed <code class="literal">CredentialsRequest</code> objects</h3></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You must extract and prepare the <code class="literal">ccoctl</code> binary.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							The AWS Load Balancer Operator creates the <code class="literal">CredentialsRequest</code> object in the <code class="literal">openshift-cloud-credential-operator</code> namespace for each <code class="literal">AWSLoadBalancerController</code> custom resource (CR). You can extract and save the created <code class="literal">CredentialsRequest</code> object in a directory by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get credentialsrequest -n openshift-cloud-credential-operator  \
    aws-load-balancer-controller-&lt;cr-name&gt; -o yaml &gt; &lt;path-to-credrequests-dir&gt;/cr.yaml <span id="CO60-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO60-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal">aws-load-balancer-controller-&lt;cr-name&gt;</code> parameter specifies the credential request name created by the AWS Load Balancer Operator. The <code class="literal">cr-name</code> specifies the name of the AWS Load Balancer Controller instance.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Use the <code class="literal">ccoctl</code> tool to process all <code class="literal">CredentialsRequest</code> objects in the <code class="literal">credrequests</code> directory by running the following command:
						</p><pre class="programlisting language-terminal">$ ccoctl aws create-iam-roles \
    --name &lt;name&gt; --region=&lt;aws_region&gt; \
    --credentials-requests-dir=&lt;path-to-credrequests-dir&gt; \
    --identity-provider-arn &lt;oidc-arn&gt;</pre></li><li class="listitem"><p class="simpara">
							Apply the secrets generated in manifests directory to your cluster, by running the following command:
						</p><pre class="programlisting language-terminal">$ ls manifests/*-credentials.yaml | xargs -I{} oc apply -f {}</pre></li><li class="listitem"><p class="simpara">
							Verify that the <code class="literal">aws-load-balancer-controller</code> pod is created:
						</p><pre class="programlisting language-terminal">$ oc -n aws-load-balancer-operator get pods
NAME                                                            READY   STATUS    RESTARTS   AGE
aws-load-balancer-controller-cluster-9b766d6-gg82c              1/1     Running   0          137m
aws-load-balancer-operator-controller-manager-b55ff68cc-85jzg   2/2     Running   0          3h26m</pre></li></ol></div></section><section class="section" id="nw-installing-albo-on-sts-cluster-predefined-credentials_albo-sts-cluster"><div class="titlepage"><div><div><h3 class="title">23.4.3. Configuring the AWS Load Balancer Operator on Security Token Service cluster by using specific credentials</h3></div></div></div><p>
					You can specify the credential secret by using the <code class="literal">spec.credentials</code> field in the AWS Load Balancer Controller custom resource (CR). You can use the predefined <code class="literal">CredentialsRequest</code> object of the controller to know which roles are required.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You must extract and prepare the <code class="literal">ccoctl</code> binary.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Download the CredentialsRequest custom resource (CR) of the AWS Load Balancer Controller, and create a directory to store it by running the following command:
						</p><pre class="programlisting language-terminal">$ curl --create-dirs -o &lt;path-to-credrequests-dir&gt;/cr.yaml https://raw.githubusercontent.com/openshift/aws-load-balancer-operator/main/hack/controller/controller-credentials-request.yaml</pre></li><li class="listitem"><p class="simpara">
							Use the <code class="literal">ccoctl</code> tool to process the <code class="literal">CredentialsRequest</code> object of the controller:
						</p><pre class="programlisting language-terminal">$ ccoctl aws create-iam-roles \
        --name &lt;name&gt; --region=&lt;aws_region&gt; \
        --credentials-requests-dir=&lt;path-to-credrequests-dir&gt; \
        --identity-provider-arn &lt;oidc-arn&gt;</pre></li><li class="listitem"><p class="simpara">
							Apply the secrets to your cluster:
						</p><pre class="programlisting language-terminal">$ ls manifests/*-credentials.yaml | xargs -I{} oc apply -f {}</pre></li><li class="listitem"><p class="simpara">
							Verify the credentials secret has been created for use by the controller:
						</p><pre class="programlisting language-terminal">$ oc -n aws-load-balancer-operator get secret aws-load-balancer-controller-manual-cluster --template='{{index .data "credentials"}}' | base64 -d</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">[default]
    sts_regional_endpoints = regional
    role_arn = arn:aws:iam::999999999999:role/aws-load-balancer-operator-aws-load-balancer-controller
    web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">AWSLoadBalancerController</code> resource YAML file, for example, <code class="literal">sample-aws-lb-manual-creds.yaml</code>, as follows:
						</p><pre class="programlisting language-yaml">apiVersion: networking.olm.openshift.io/v1
kind: AWSLoadBalancerController <span id="CO61-1"><!--Empty--></span><span class="callout">1</span>
metadata:
  name: cluster <span id="CO61-2"><!--Empty--></span><span class="callout">2</span>
spec:
  credentials:
    name: &lt;secret-name&gt; <span id="CO61-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO61-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Defines the <code class="literal">AWSLoadBalancerController</code> resource.
								</div></dd><dt><a href="#CO61-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Defines the AWS Load Balancer Controller instance name. This instance name gets added as a suffix to all related resources.
								</div></dd><dt><a href="#CO61-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specifies the secret name containing AWS credentials that the controller uses.
								</div></dd></dl></div></li></ol></div></section><section class="section _additional-resources" id="additional-resources-3"><div class="titlepage"><div><div><h3 class="title">23.4.4. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/authentication_and_authorization/#cco-ccoctl-configuring_cco-mode-sts">Configuring the Cloud Credential Operator utility</a>
						</li></ul></div></section></section><section class="section" id="nw-create-instance-aws-load-balancer"><div class="titlepage"><div><div><h2 class="title">23.5. Creating an instance of AWS Load Balancer Controller</h2></div></div></div><p>
				After installing the Operator, you can create an instance of the AWS Load Balancer Controller.
			</p><section class="section" id="nw-creating-instance-aws-load-balancer-controller_create-instance-aws-load-balancer"><div class="titlepage"><div><div><h3 class="title">23.5.1. Creating an instance of the AWS Load Balancer Controller using AWS Load Balancer Operator</h3></div></div></div><p>
					You can install only a single instance of the <code class="literal">aws-load-balancer-controller</code> in a cluster. You can create the AWS Load Balancer Controller by using CLI. The AWS Load Balancer(ALB) Operator reconciles only the resource with the name <code class="literal">cluster</code>.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have created the <code class="literal">echoserver</code> namespace.
						</li><li class="listitem">
							You have access to the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an <code class="literal">aws-load-balancer-controller</code> resource YAML file, for example, <code class="literal">sample-aws-lb.yaml</code>, as follows:
						</p><pre class="programlisting language-yaml">apiVersion: networking.olm.openshift.io/v1
kind: AWSLoadBalancerController <span id="CO62-1"><!--Empty--></span><span class="callout">1</span>
metadata:
  name: cluster <span id="CO62-2"><!--Empty--></span><span class="callout">2</span>
spec:
  subnetTagging: Auto <span id="CO62-3"><!--Empty--></span><span class="callout">3</span>
  additionalResourceTags: <span id="CO62-4"><!--Empty--></span><span class="callout">4</span>
  - key: example.org/security-scope
    value: staging
  ingressClass: alb <span id="CO62-5"><!--Empty--></span><span class="callout">5</span>
  config:
    replicas: 2 <span id="CO62-6"><!--Empty--></span><span class="callout">6</span>
  enabledAddons: <span id="CO62-7"><!--Empty--></span><span class="callout">7</span>
    - AWSWAFv2 <span id="CO62-8"><!--Empty--></span><span class="callout">8</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO62-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Defines the <code class="literal">aws-load-balancer-controller</code> resource.
								</div></dd><dt><a href="#CO62-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Defines the AWS Load Balancer Controller instance name. This instance name gets added as a suffix to all related resources.
								</div></dd><dt><a href="#CO62-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Valid options are <code class="literal">Auto</code> and <code class="literal">Manual</code>. When the value is set to <code class="literal">Auto</code>, the Operator attempts to determine the subnets that belong to the cluster and tags them appropriately. The Operator cannot determine the role correctly if the internal subnet tags are not present on internal subnet. If you installed your cluster on user-provided infrastructure, you can manually tag the subnets with the appropriate role tags and set the subnet tagging policy to <code class="literal">Manual</code>.
								</div></dd><dt><a href="#CO62-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Defines the tags used by the controller when it provisions AWS resources.
								</div></dd><dt><a href="#CO62-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									The default value for this field is <code class="literal">alb</code>. The Operator provisions an <code class="literal">IngressClass</code> resource with the same name if it does not exist.
								</div></dd><dt><a href="#CO62-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Specifies the number of replicas of the controller.
								</div></dd><dt><a href="#CO62-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									Specifies add-ons for AWS load balancers, which get specified through annotations.
								</div></dd><dt><a href="#CO62-8"><span class="callout">8</span></a> </dt><dd><div class="para">
									Enables the <code class="literal">alb.ingress.kubernetes.io/wafv2-acl-arn</code> annotation.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">aws-load-balancer-controller</code> resource by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f sample-aws-lb.yaml</pre></li><li class="listitem"><p class="simpara">
							After the AWS Load Balancer Controller is running, create a <code class="literal">deployment</code> resource:
						</p><pre class="programlisting language-yaml">apiVersion: apps/v1
kind: Deployment <span id="CO63-1"><!--Empty--></span><span class="callout">1</span>
metadata:
  name: &lt;echoserver&gt; <span id="CO63-2"><!--Empty--></span><span class="callout">2</span>
  namespace: echoserver
spec:
  selector:
    matchLabels:
      app: echoserver
  replicas: 3 <span id="CO63-3"><!--Empty--></span><span class="callout">3</span>
  template:
    metadata:
      labels:
        app: echoserver
    spec:
      containers:
        - image: openshift/origin-node
          command:
           - "/bin/socat"
          args:
            - TCP4-LISTEN:8080,reuseaddr,fork
            - EXEC:'/bin/bash -c \"printf \\\"HTTP/1.0 200 OK\r\n\r\n\\\"; sed -e \\\"/^\r/q\\\"\"'
          imagePullPolicy: Always
          name: echoserver
          ports:
            - containerPort: 8080</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO63-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Defines the deployment resource.
								</div></dd><dt><a href="#CO63-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specifies the deployment name.
								</div></dd><dt><a href="#CO63-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specifies the number of replicas of the deployment.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">service</code> resource:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Service <span id="CO64-1"><!--Empty--></span><span class="callout">1</span>
metadata:
  name: &lt;echoserver&gt; <span id="CO64-2"><!--Empty--></span><span class="callout">2</span>
  namespace: echoserver
spec:
  ports:
    - port: 80
      targetPort: 8080
      protocol: TCP
  type: NodePort
  selector:
    app: echoserver</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO64-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Defines the service resource.
								</div></dd><dt><a href="#CO64-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specifies the name of the service.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Deploy an ALB-backed <code class="literal">Ingress</code> resource:
						</p><pre class="programlisting language-yaml">apiVersion: networking.k8s.io/v1
kind: Ingress <span id="CO65-1"><!--Empty--></span><span class="callout">1</span>
metadata:
  name: &lt;echoserver&gt; <span id="CO65-2"><!--Empty--></span><span class="callout">2</span>
  namespace: echoserver
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: instance
spec:
  ingressClassName: alb
  rules:
    - http:
        paths:
          - path: /
            pathType: Exact
            backend:
              service:
                name: &lt;echoserver&gt; <span id="CO65-3"><!--Empty--></span><span class="callout">3</span>
                port:
                  number: 80</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO65-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Defines the ingress resource.
								</div></dd><dt><a href="#CO65-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specifies the name of the ingress resource.
								</div></dd><dt><a href="#CO65-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specifies the name of the service resource.
								</div></dd></dl></div></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Verify the status of the <code class="literal">Ingress</code> resource to show the host of the provisioned AWS Load Balancer (ALB) by running the following command:
						</p><pre class="programlisting language-terminal">$ HOST=$(oc get ingress -n echoserver echoserver --template='{{(index .status.loadBalancer.ingress 0).hostname}}')</pre></li><li class="listitem"><p class="simpara">
							Verify the status of the provisioned AWS Load Balancer (ALB) host by running the following command:
						</p><pre class="programlisting language-terminal">$ curl $HOST</pre></li></ul></div></section></section><section class="section" id="nw-multiple-ingress-through-single-alb"><div class="titlepage"><div><div><h2 class="title">23.6. Creating multiple ingresses</h2></div></div></div><p>
				You can route the traffic to different services that are part of a single domain through a single AWS Load Balancer (ALB). Each Ingress resource provides different endpoints of the domain.
			</p><section class="section" id="nw-creating-multiple-ingress-through-single-alb_multiple-ingress-through-single-alb"><div class="titlepage"><div><div><h3 class="title">23.6.1. Creating multiple ingresses through a single AWS Load Balancer</h3></div></div></div><p>
					You can route the traffic to multiple Ingresses through a single AWS Load Balancer (ALB) by using the CLI.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have an access to the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an <code class="literal">IngressClassParams</code> resource YAML file, for example, <code class="literal">sample-single-lb-params.yaml</code>, as follows:
						</p><pre class="programlisting language-yaml">apiVersion: elbv2.k8s.aws/v1beta1 <span id="CO66-1"><!--Empty--></span><span class="callout">1</span>
kind: IngressClassParams
metadata:
  name: single-lb-params <span id="CO66-2"><!--Empty--></span><span class="callout">2</span>
spec:
  group:
    name: single-lb <span id="CO66-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO66-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Defines the API group and version of the <code class="literal">IngressClassParams</code> resource.
								</div></dd><dt><a href="#CO66-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specifies the name of the <code class="literal">IngressClassParams</code> resource.
								</div></dd><dt><a href="#CO66-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specifies the name of the <code class="literal">IngressGroup</code>. All Ingresses of this class belong to this <code class="literal">IngressGroup</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">IngressClassParams</code> resource by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f sample-single-lb-params.yaml</pre></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">IngressClass</code> resource YAML file, for example, <code class="literal">sample-single-lb-class.yaml</code>, as follows:
						</p><pre class="programlisting language-yaml">apiVersion: networking.k8s.io/v1 <span id="CO67-1"><!--Empty--></span><span class="callout">1</span>
kind: IngressClass
metadata:
  name: single-lb <span id="CO67-2"><!--Empty--></span><span class="callout">2</span>
spec:
  controller: ingress.k8s.aws/alb <span id="CO67-3"><!--Empty--></span><span class="callout">3</span>
  parameters:
    apiGroup: elbv2.k8s.aws <span id="CO67-4"><!--Empty--></span><span class="callout">4</span>
    kind: IngressClassParams <span id="CO67-5"><!--Empty--></span><span class="callout">5</span>
    name: single-lb-params <span id="CO67-6"><!--Empty--></span><span class="callout">6</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO67-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Defines the API group and version of the <code class="literal">IngressClass</code> resource.
								</div></dd><dt><a href="#CO67-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specifies the name of the <code class="literal">IngressClass</code>.
								</div></dd><dt><a href="#CO67-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Defines the controller name. <code class="literal">ingress.k8s.aws/alb</code> denotes that all Ingresses of this class should be managed by the <code class="literal">aws-load-balancer-controller</code>.
								</div></dd><dt><a href="#CO67-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Defines the API group of the <code class="literal">IngressClassParams</code> resource.
								</div></dd><dt><a href="#CO67-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Defines the resource type of the <code class="literal">IngressClassParams</code> resource.
								</div></dd><dt><a href="#CO67-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Defines the name of the <code class="literal">IngressClassParams</code> resource.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">IngressClass</code> resource by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f sample-single-lb-class.yaml</pre></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">AWSLoadBalancerController</code> resource YAML file, for example, <code class="literal">sample-single-lb.yaml</code>, as follows:
						</p><pre class="programlisting language-yaml">apiVersion: networking.olm.openshift.io/v1
kind: AWSLoadBalancerController
metadata:
  name: cluster
spec:
  subnetTagging: Auto
  ingressClass: single-lb <span id="CO68-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO68-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Defines the name of the <code class="literal">IngressClass</code> resource.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">AWSLoadBalancerController</code> resource by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f sample-single-lb.yaml</pre></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">Ingress</code> resource YAML file, for example, <code class="literal">sample-multiple-ingress.yaml</code>, as follows:
						</p><pre class="programlisting language-yaml">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-1 <span id="CO69-1"><!--Empty--></span><span class="callout">1</span>
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing <span id="CO69-2"><!--Empty--></span><span class="callout">2</span>
    alb.ingress.kubernetes.io/group.order: "1" <span id="CO69-3"><!--Empty--></span><span class="callout">3</span>
    alb.ingress.kubernetes.io/target-type: instance <span id="CO69-4"><!--Empty--></span><span class="callout">4</span>
spec:
  ingressClassName: single-lb <span id="CO69-5"><!--Empty--></span><span class="callout">5</span>
  rules:
  - host: example.com <span id="CO69-6"><!--Empty--></span><span class="callout">6</span>
    http:
        paths:
        - path: /blog <span id="CO69-7"><!--Empty--></span><span class="callout">7</span>
          pathType: Prefix
          backend:
            service:
              name: example-1 <span id="CO69-8"><!--Empty--></span><span class="callout">8</span>
              port:
                number: 80 <span id="CO69-9"><!--Empty--></span><span class="callout">9</span>
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-2
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/group.order: "2"
    alb.ingress.kubernetes.io/target-type: instance
spec:
  ingressClassName: single-lb
  rules:
  - host: example.com
    http:
        paths:
        - path: /store
          pathType: Prefix
          backend:
            service:
              name: example-2
              port:
                number: 80
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-3
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/group.order: "3"
    alb.ingress.kubernetes.io/target-type: instance
spec:
  ingressClassName: single-lb
  rules:
  - host: example.com
    http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: example-3
              port:
                number: 80</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO69-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specifies the name of an ingress.
								</div></dd><dt><a href="#CO69-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Indicates the load balancer to provision in the public subnet and makes it accessible over the internet.
								</div></dd><dt><a href="#CO69-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specifies the order in which the rules from the Ingresses are matched when the request is received at the load balancer.
								</div></dd><dt><a href="#CO69-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Indicates the load balancer will target OpenShift nodes to reach the service.
								</div></dd><dt><a href="#CO69-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Specifies the Ingress Class that belongs to this ingress.
								</div></dd><dt><a href="#CO69-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Defines the name of a domain used for request routing.
								</div></dd><dt><a href="#CO69-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									Defines the path that must route to the service.
								</div></dd><dt><a href="#CO69-8"><span class="callout">8</span></a> </dt><dd><div class="para">
									Defines the name of the service that serves the endpoint configured in the ingress.
								</div></dd><dt><a href="#CO69-9"><span class="callout">9</span></a> </dt><dd><div class="para">
									Defines the port on the service that serves the endpoint.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">Ingress</code> resources by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create -f sample-multiple-ingress.yaml</pre></li></ol></div></section></section><section class="section" id="nw-adding-tls-termination"><div class="titlepage"><div><div><h2 class="title">23.7. Adding TLS termination</h2></div></div></div><p>
				You can add TLS termination on the AWS Load Balancer.
			</p><section class="section" id="nw-adding-tls-termination_adding-tls-termination"><div class="titlepage"><div><div><h3 class="title">23.7.1. Adding TLS termination on the AWS Load Balancer</h3></div></div></div><p>
					You can route the traffic for the domain to pods of a service and add TLS termination on the AWS Load Balancer.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have an access to the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Install the Operator and create an instance of the <code class="literal">aws-load-balancer-controller</code> resource:
						</p><pre class="programlisting language-yaml">apiVersion: networking.olm.openshift.io/v1
kind: AWSLoadBalancerController
metadata:
  name: cluster
spec:
  subnetTagging: Auto
  ingressClass: tls-termination <span id="CO70-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO70-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Defines the name of an <code class="literal">ingressClass</code> resource reconciled by the AWS Load Balancer Controller. This <code class="literal">ingressClass</code> resource gets created if it is not present. You can add additional <code class="literal">ingressClass</code> values. The controller reconciles the <code class="literal">ingressClass</code> values if the <code class="literal">spec.controller</code> is set to <code class="literal">ingress.k8s.aws/alb</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">Ingress</code> resource:
						</p><pre class="programlisting language-yaml">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: &lt;example&gt; <span id="CO71-1"><!--Empty--></span><span class="callout">1</span>
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing <span id="CO71-2"><!--Empty--></span><span class="callout">2</span>
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-west-2:xxxxx <span id="CO71-3"><!--Empty--></span><span class="callout">3</span>
spec:
  ingressClassName: tls-termination <span id="CO71-4"><!--Empty--></span><span class="callout">4</span>
  rules:
  - host: &lt;example.com&gt; <span id="CO71-5"><!--Empty--></span><span class="callout">5</span>
    http:
        paths:
          - path: /
            pathType: Exact
            backend:
              service:
                name: &lt;example-service&gt; <span id="CO71-6"><!--Empty--></span><span class="callout">6</span>
                port:
                  number: 80</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO71-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specifies the name of an ingress.
								</div></dd><dt><a href="#CO71-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The controller provisions the load balancer for this <code class="literal">Ingress</code> resource in a public subnet so that the load balancer is reachable over the internet.
								</div></dd><dt><a href="#CO71-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The Amazon Resource Name of the certificate that you attach to the load balancer.
								</div></dd><dt><a href="#CO71-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Defines the ingress class name.
								</div></dd><dt><a href="#CO71-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Defines the domain for traffic routing.
								</div></dd><dt><a href="#CO71-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Defines the service for traffic routing.
								</div></dd></dl></div></li></ol></div></section></section><section class="section" id="nw-aws-load-balancer-operator-cluster-wide-proxy"><div class="titlepage"><div><div><h2 class="title">23.8. Configuring cluster-wide proxy</h2></div></div></div><p>
				You can configure the cluster-wide proxy in the AWS Load Balancer Operator. After configuring the cluster-wide proxy in the AWS Load Balancer Operator, Operator Lifecycle Manager (OLM) automatically updates all the deployments of the Operators with the environment variables such as <code class="literal">HTTP_PROXY</code>, <code class="literal">HTTPS_PROXY</code>, and <code class="literal">NO_PROXY</code>. These variables are populated to the managed controller by the AWS Load Balancer Operator.
			</p><section class="section" id="nw-configuring-cluster-wide-proxy_aws-load-balancer-operator"><div class="titlepage"><div><div><h3 class="title">23.8.1. Configuring the AWS Load Balancer Operator to trust the certificate authority of the cluster-wide proxy</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the config map to contain the certificate authority (CA) bundle in the <code class="literal">aws-load-balancer-operator</code> namespace and inject a CA bundle that is trusted by OpenShift Container Platform into a config map by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n aws-load-balancer-operator create configmap trusted-ca</pre></li><li class="listitem"><p class="simpara">
							To inject the trusted CA bundle into the config map, add the <code class="literal">config.openshift.io/inject-trusted-cabundle=true</code> label to the config map by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n aws-load-balancer-operator label cm trusted-ca config.openshift.io/inject-trusted-cabundle=true</pre></li><li class="listitem"><p class="simpara">
							Update the subscription of the AWS Load Balancer Operator to access the config map in the deployment of the AWS Load Balancer Operator by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n aws-load-balancer-operator patch subscription aws-load-balancer-operator --type='merge' -p '{"spec":{"config":{"env":[{"name":"TRUSTED_CA_CONFIGMAP_NAME","value":"trusted-ca"}],"volumes":[{"name":"trusted-ca","configMap":{"name":"trusted-ca"}}],"volumeMounts":[{"name":"trusted-ca","mountPath":"/etc/pki/tls/certs/albo-tls-ca-bundle.crt","subPath":"ca-bundle.crt"}]}}}'</pre></li><li class="listitem"><p class="simpara">
							After the deployment of the AWS Load Balancer Operator is completed, verify that the CA bundle is added to the <code class="literal">aws-load-balancer-operator-controller-manager</code> deployment by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n aws-load-balancer-operator exec deploy/aws-load-balancer-operator-controller-manager -c manager -- bash -c "ls -l /etc/pki/tls/certs/albo-tls-ca-bundle.crt; printenv TRUSTED_CA_CONFIGMAP_NAME"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">-rw-r--r--. 1 root 1000690000 5875 Jan 11 12:25 /etc/pki/tls/certs/albo-tls-ca-bundle.crt
trusted-ca</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Optional: Restart deployment of the AWS Load Balancer Operator every time the config map changes by running the following command:
						</p><pre class="programlisting language-terminal">$ oc -n aws-load-balancer-operator rollout restart deployment/aws-load-balancer-operator-controller-manager</pre></li></ol></div></section><section class="section _additional-resources" id="additional-resources-4"><div class="titlepage"><div><div><h3 class="title">23.8.2. Additional resources</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#certificate-injection-using-operators_configuring-a-custom-pki">Certificate injection using Operators</a>
						</li></ul></div></section></section></section><section class="chapter" id="multiple-networks"><div class="titlepage"><div><div><h1 class="title">Chapter 24. Multiple networks</h1></div></div></div><section class="section" id="understanding-multiple-networks"><div class="titlepage"><div><div><h2 class="title">24.1. Understanding multiple networks</h2></div></div></div><p>
				In Kubernetes, container networking is delegated to networking plugins that implement the Container Network Interface (CNI).
			</p><p>
				OpenShift Container Platform uses the Multus CNI plugin to allow chaining of CNI plugins. During cluster installation, you configure your <span class="emphasis"><em>default</em></span> pod network. The default network handles all ordinary network traffic for the cluster. You can define an <span class="emphasis"><em>additional network</em></span> based on the available CNI plugins and attach one or more of these networks to your pods. You can define more than one additional network for your cluster, depending on your needs. This gives you flexibility when you configure pods that deliver network functionality, such as switching or routing.
			</p><section class="section" id="additional-network-considerations"><div class="titlepage"><div><div><h3 class="title">24.1.1. Usage scenarios for an additional network</h3></div></div></div><p>
					You can use an additional network in situations where network isolation is needed, including data plane and control plane separation. Isolating network traffic is useful for the following performance and security reasons:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Performance</span></dt><dd>
								You can send traffic on two different planes to manage how much traffic is along each plane.
							</dd><dt><span class="term">Security</span></dt><dd>
								You can send sensitive traffic onto a network plane that is managed specifically for security considerations, and you can separate private data that must not be shared between tenants or customers.
							</dd></dl></div><p>
					All of the pods in the cluster still use the cluster-wide default network to maintain connectivity across the cluster. Every pod has an <code class="literal">eth0</code> interface that is attached to the cluster-wide pod network. You can view the interfaces for a pod by using the <code class="literal">oc exec -it &lt;pod_name&gt; -- ip a</code> command. If you add additional network interfaces that use Multus CNI, they are named <code class="literal">net1</code>, <code class="literal">net2</code>, …​, <code class="literal">netN</code>.
				</p><p>
					To attach additional network interfaces to a pod, you must create configurations that define how the interfaces are attached. You specify each interface by using a <code class="literal">NetworkAttachmentDefinition</code> custom resource (CR). A CNI configuration inside each of these CRs defines how that interface is created.
				</p></section><section class="section" id="additional-networks-provided"><div class="titlepage"><div><div><h3 class="title">24.1.2. Additional networks in OpenShift Container Platform</h3></div></div></div><p>
					OpenShift Container Platform provides the following CNI plugins for creating additional networks in your cluster:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<span class="strong strong"><strong>bridge</strong></span>: <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-multus-bridge-object_configuring-additional-network">Configure a bridge-based additional network</a> to allow pods on the same host to communicate with each other and the host.
						</li><li class="listitem">
							<span class="strong strong"><strong>host-device</strong></span>: <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-multus-host-device-object_configuring-additional-network">Configure a host-device additional network</a> to allow pods access to a physical Ethernet network device on the host system.
						</li><li class="listitem">
							<span class="strong strong"><strong>ipvlan</strong></span>: <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-multus-ipvlan-object_configuring-additional-network">Configure an ipvlan-based additional network</a> to allow pods on a host to communicate with other hosts and pods on those hosts, similar to a macvlan-based additional network. Unlike a macvlan-based additional network, each pod shares the same MAC address as the parent physical network interface.
						</li><li class="listitem">
							<span class="strong strong"><strong>vlan</strong></span>: <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-multus-vlan-object_configuring-additional-network">Configure a vlan-based additional network</a> to allow VLAN-based network isolation and connectivity for pods.
						</li><li class="listitem">
							<span class="strong strong"><strong>macvlan</strong></span>: <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-multus-macvlan-object_configuring-additional-network">Configure a macvlan-based additional network</a> to allow pods on a host to communicate with other hosts and pods on those hosts by using a physical network interface. Each pod that is attached to a macvlan-based additional network is provided a unique MAC address.
						</li><li class="listitem">
							<span class="strong strong"><strong>SR-IOV</strong></span>: <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-sriov">Configure an SR-IOV based additional network</a> to allow pods to attach to a virtual function (VF) interface on SR-IOV capable hardware on the host system.
						</li></ul></div></section></section><section class="section" id="configuring-additional-network"><div class="titlepage"><div><div><h2 class="title">24.2. Configuring an additional network</h2></div></div></div><p>
				As a cluster administrator, you can configure an additional network for your cluster. The following network types are supported:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-multus-bridge-object_configuring-additional-network">Bridge</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-multus-host-device-object_configuring-additional-network">Host device</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-multus-vlan-object_configuring-additional-network">VLAN</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-multus-ipvlan-object_configuring-additional-network">IPVLAN</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-multus-macvlan-object_configuring-additional-network">MACVLAN</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuration-ovnk-additional-networks_configuring-additional-network">OVN-Kubernetes</a>
					</li></ul></div><section class="section" id="configuring-additional-network_approaches-managing-additional-network"><div class="titlepage"><div><div><h3 class="title">24.2.1. Approaches to managing an additional network</h3></div></div></div><p>
					You can manage the life cycle of an additional network by two approaches. Each approach is mutually exclusive and you can only use one approach for managing an additional network at a time. For either approach, the additional network is managed by a Container Network Interface (CNI) plugin that you configure.
				</p><p>
					For an additional network, IP addresses are provisioned through an IP Address Management (IPAM) CNI plugin that you configure as part of the additional network. The IPAM plugin supports a variety of IP address assignment approaches including DHCP and static assignment.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Modify the Cluster Network Operator (CNO) configuration: The CNO automatically creates and manages the <code class="literal">NetworkAttachmentDefinition</code> object. In addition to managing the object lifecycle the CNO ensures a DHCP is available for an additional network that uses a DHCP assigned IP address.
						</li><li class="listitem">
							Applying a YAML manifest: You can manage the additional network directly by creating an <code class="literal">NetworkAttachmentDefinition</code> object. This approach allows for the chaining of CNI plugins.
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						When deploying OpenShift Container Platform nodes with multiple network interfaces on Red Hat OpenStack Platform (RHOSP) with OVN SDN, DNS configuration of the secondary interface might take precedence over the DNS configuration of the primary interface. In this case, remove the DNS nameservers for the subnet id that is attached to the secondary interface:
					</p><pre class="programlisting language-terminal">$ openstack subnet set --dns-nameserver 0.0.0.0 &lt;subnet_id&gt;</pre></div></div></section><section class="section" id="configuring-additional-network_configuration-additional-network-attachment"><div class="titlepage"><div><div><h3 class="title">24.2.2. Configuration for an additional network attachment</h3></div></div></div><p>
					An additional network is configured by using the <code class="literal">NetworkAttachmentDefinition</code> API in the <code class="literal">k8s.cni.cncf.io</code> API group.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Do not store any sensitive information or a secret in the <code class="literal">NetworkAttachmentDefinition</code> object because this information is accessible by the project administration user.
					</p></div></div><p>
					The configuration for the API is described in the following table:
				</p><div class="table" id="idm140587134446192"><p class="title"><strong>Table 24.1. <code class="literal">NetworkAttachmentDefinition</code> API fields</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 30%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 50%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587107334400" scope="col">Field</th><th align="left" valign="middle" id="idm140587107333312" scope="col">Type</th><th align="left" valign="middle" id="idm140587107332224" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587107334400"> <p>
									<code class="literal">metadata.name</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587107333312"> <p>
									<code class="literal">string</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587107332224"> <p>
									The name for the additional network.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587107334400"> <p>
									<code class="literal">metadata.namespace</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587107333312"> <p>
									<code class="literal">string</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587107332224"> <p>
									The namespace that the object is associated with.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587107334400"> <p>
									<code class="literal">spec.config</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587107333312"> <p>
									<code class="literal">string</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587107332224"> <p>
									The CNI plugin configuration in JSON format.
								</p>
								 </td></tr></tbody></table></div></div><section class="section" id="configuring-additional-network_configuration-additional-network-cno"><div class="titlepage"><div><div><h4 class="title">24.2.2.1. Configuration of an additional network through the Cluster Network Operator</h4></div></div></div><p>
						The configuration for an additional network attachment is specified as part of the Cluster Network Operator (CNO) configuration.
					</p><p>
						The following YAML describes the configuration parameters for managing an additional network with the CNO:
					</p><div class="formalpara"><p class="title"><strong>Cluster Network Operator configuration</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  # ...
  additionalNetworks: <span id="CO72-1"><!--Empty--></span><span class="callout">1</span>
  - name: &lt;name&gt; <span id="CO72-2"><!--Empty--></span><span class="callout">2</span>
    namespace: &lt;namespace&gt; <span id="CO72-3"><!--Empty--></span><span class="callout">3</span>
    rawCNIConfig: |- <span id="CO72-4"><!--Empty--></span><span class="callout">4</span>
      {
        ...
      }
    type: Raw</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO72-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								An array of one or more additional network configurations.
							</div></dd><dt><a href="#CO72-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The name for the additional network attachment that you are creating. The name must be unique within the specified <code class="literal">namespace</code>.
							</div></dd><dt><a href="#CO72-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								The namespace to create the network attachment in. If you do not specify a value, then the <code class="literal">default</code> namespace is used.
							</div></dd><dt><a href="#CO72-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								A CNI plugin configuration in JSON format.
							</div></dd></dl></div></section><section class="section" id="configuring-additional-network_configuration-additional-network-yaml"><div class="titlepage"><div><div><h4 class="title">24.2.2.2. Configuration of an additional network from a YAML manifest</h4></div></div></div><p>
						The configuration for an additional network is specified from a YAML configuration file, such as in the following example:
					</p><pre class="programlisting language-yaml">apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: &lt;name&gt; <span id="CO73-1"><!--Empty--></span><span class="callout">1</span>
spec:
  config: |- <span id="CO73-2"><!--Empty--></span><span class="callout">2</span>
    {
      ...
    }</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO73-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The name for the additional network attachment that you are creating.
							</div></dd><dt><a href="#CO73-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								A CNI plugin configuration in JSON format.
							</div></dd></dl></div></section></section><section class="section" id="configuring-additional-network_configuration-additional-network-types"><div class="titlepage"><div><div><h3 class="title">24.2.3. Configurations for additional network types</h3></div></div></div><p>
					The specific configuration fields for additional networks is described in the following sections.
				</p><section class="section" id="nw-multus-bridge-object_configuring-additional-network"><div class="titlepage"><div><div><h4 class="title">24.2.3.1. Configuration for a bridge additional network</h4></div></div></div><p>
						The following object describes the configuration parameters for the bridge CNI plugin:
					</p><div class="table" id="idm140587123683200"><p class="title"><strong>Table 24.2. Bridge CNI plugin JSON configuration object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587130318576" scope="col">Field</th><th align="left" valign="middle" id="idm140587130317488" scope="col">Type</th><th align="left" valign="middle" id="idm140587130316400" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587130318576"> <p>
										<code class="literal">cniVersion</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130317488"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130316400"> <p>
										The CNI specification version. The <code class="literal">0.3.1</code> value is required.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130318576"> <p>
										<code class="literal">name</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130317488"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130316400"> <p>
										The value for the <code class="literal">name</code> parameter you provided previously for the CNO configuration.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130318576"> <p>
										<code class="literal">type</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130317488"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130316400"> <p>
										The name of the CNI plugin to configure: <code class="literal">bridge</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130318576"> <p>
										<code class="literal">ipam</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130317488"> <p>
										<code class="literal">object</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130316400"> <p>
										The configuration object for the IPAM CNI plugin. The plugin manages IP address assignment for the attachment definition.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130318576"> <p>
										<code class="literal">bridge</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130317488"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130316400"> <p>
										Optional: Specify the name of the virtual bridge to use. If the bridge interface does not exist on the host, it is created. The default value is <code class="literal">cni0</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130318576"> <p>
										<code class="literal">ipMasq</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130317488"> <p>
										<code class="literal">boolean</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130316400"> <p>
										Optional: Set to <code class="literal">true</code> to enable IP masquerading for traffic that leaves the virtual network. The source IP address for all traffic is rewritten to the bridge’s IP address. If the bridge does not have an IP address, this setting has no effect. The default value is <code class="literal">false</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130318576"> <p>
										<code class="literal">isGateway</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130317488"> <p>
										<code class="literal">boolean</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130316400"> <p>
										Optional: Set to <code class="literal">true</code> to assign an IP address to the bridge. The default value is <code class="literal">false</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130318576"> <p>
										<code class="literal">isDefaultGateway</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130317488"> <p>
										<code class="literal">boolean</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130316400"> <p>
										Optional: Set to <code class="literal">true</code> to configure the bridge as the default gateway for the virtual network. The default value is <code class="literal">false</code>. If <code class="literal">isDefaultGateway</code> is set to <code class="literal">true</code>, then <code class="literal">isGateway</code> is also set to <code class="literal">true</code> automatically.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130318576"> <p>
										<code class="literal">forceAddress</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130317488"> <p>
										<code class="literal">boolean</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130316400"> <p>
										Optional: Set to <code class="literal">true</code> to allow assignment of a previously assigned IP address to the virtual bridge. When set to <code class="literal">false</code>, if an IPv4 address or an IPv6 address from overlapping subsets is assigned to the virtual bridge, an error occurs. The default value is <code class="literal">false</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130318576"> <p>
										<code class="literal">hairpinMode</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130317488"> <p>
										<code class="literal">boolean</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130316400"> <p>
										Optional: Set to <code class="literal">true</code> to allow the virtual bridge to send an Ethernet frame back through the virtual port it was received on. This mode is also known as <span class="emphasis"><em>reflective relay</em></span>. The default value is <code class="literal">false</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130318576"> <p>
										<code class="literal">promiscMode</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130317488"> <p>
										<code class="literal">boolean</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130316400"> <p>
										Optional: Set to <code class="literal">true</code> to enable promiscuous mode on the bridge. The default value is <code class="literal">false</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130318576"> <p>
										<code class="literal">vlan</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130317488"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130316400"> <p>
										Optional: Specify a virtual LAN (VLAN) tag as an integer value. By default, no VLAN tag is assigned.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130318576"> <p>
										<code class="literal">preserveDefaultVlan</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130317488"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130316400"> <p>
										Optional: Indicates whether the default vlan must be preserved on the <code class="literal">veth</code> end connected to the bridge. Defaults to true.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130318576"> <p>
										<code class="literal">vlanTrunk</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130317488"> <p>
										<code class="literal">list</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130316400"> <p>
										Optional: Assign a VLAN trunk tag. The default value is <code class="literal">none</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130318576"> <p>
										<code class="literal">mtu</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130317488"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130316400"> <p>
										Optional: Set the maximum transmission unit (MTU) to the specified value. The default value is automatically set by the kernel.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130318576"> <p>
										<code class="literal">enabledad</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130317488"> <p>
										<code class="literal">boolean</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130316400"> <p>
										Optional: Enables duplicate address detection for the container side <code class="literal">veth</code>. The default value is <code class="literal">false</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130318576"> <p>
										<code class="literal">macspoofchk</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130317488"> <p>
										<code class="literal">boolean</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130316400"> <p>
										Optional: Enables mac spoof check, limiting the traffic originating from the container to the mac address of the interface. The default value is <code class="literal">false</code>.
									</p>
									 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The VLAN parameter configures the VLAN tag on the host end of the <code class="literal">veth</code> and also enables the <code class="literal">vlan_filtering</code> feature on the bridge interface.
						</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							To configure uplink for a L2 network you need to allow the vlan on the uplink interface by using the following command:
						</p><pre class="programlisting language-terminal">$  bridge vlan add vid VLAN_ID dev DEV</pre></div></div><section class="section" id="nw-multus-bridge-config-example_configuring-additional-network"><div class="titlepage"><div><div><h5 class="title">24.2.3.1.1. bridge configuration example</h5></div></div></div><p>
							The following example configures an additional network named <code class="literal">bridge-net</code>:
						</p><pre class="programlisting language-json">{
  "cniVersion": "0.3.1",
  "name": "bridge-net",
  "type": "bridge",
  "isGateway": true,
  "vlan": 2,
  "ipam": {
    "type": "dhcp"
    }
}</pre></section></section><section class="section" id="nw-multus-host-device-object_configuring-additional-network"><div class="titlepage"><div><div><h4 class="title">24.2.3.2. Configuration for a host device additional network</h4></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Specify your network device by setting only one of the following parameters: <code class="literal">device</code>,<code class="literal">hwaddr</code>, <code class="literal">kernelpath</code>, or <code class="literal">pciBusID</code>.
						</p></div></div><p>
						The following object describes the configuration parameters for the host-device CNI plugin:
					</p><div class="table" id="idm140587123637760"><p class="title"><strong>Table 24.3. Host device CNI plugin JSON configuration object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587123467456" scope="col">Field</th><th align="left" valign="middle" id="idm140587123466368" scope="col">Type</th><th align="left" valign="middle" id="idm140587123465280" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587123467456"> <p>
										<code class="literal">cniVersion</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123466368"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123465280"> <p>
										The CNI specification version. The <code class="literal">0.3.1</code> value is required.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587123467456"> <p>
										<code class="literal">name</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123466368"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123465280"> <p>
										The value for the <code class="literal">name</code> parameter you provided previously for the CNO configuration.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587123467456"> <p>
										<code class="literal">type</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123466368"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123465280"> <p>
										The name of the CNI plugin to configure: <code class="literal">host-device</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587123467456"> <p>
										<code class="literal">device</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123466368"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123465280"> <p>
										Optional: The name of the device, such as <code class="literal">eth0</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587123467456"> <p>
										<code class="literal">hwaddr</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123466368"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123465280"> <p>
										Optional: The device hardware MAC address.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587123467456"> <p>
										<code class="literal">kernelpath</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123466368"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123465280"> <p>
										Optional: The Linux kernel device path, such as <code class="literal">/sys/devices/pci0000:00/0000:00:1f.6</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587123467456"> <p>
										<code class="literal">pciBusID</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123466368"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123465280"> <p>
										Optional: The PCI address of the network device, such as <code class="literal">0000:00:1f.6</code>.
									</p>
									 </td></tr></tbody></table></div></div><section class="section" id="nw-multus-hostdev-config-example_configuring-additional-network"><div class="titlepage"><div><div><h5 class="title">24.2.3.2.1. host-device configuration example</h5></div></div></div><p>
							The following example configures an additional network named <code class="literal">hostdev-net</code>:
						</p><pre class="programlisting language-json">{
  "cniVersion": "0.3.1",
  "name": "hostdev-net",
  "type": "host-device",
  "device": "eth1"
}</pre></section></section><section class="section" id="nw-multus-vlan-object_configuring-additional-network"><div class="titlepage"><div><div><h4 class="title">24.2.3.3. Configuration for an VLAN additional network</h4></div></div></div><p>
						The following object describes the configuration parameters for the VLAN CNI plugin:
					</p><div class="table" id="idm140587174643008"><p class="title"><strong>Table 24.4. VLAN CNI plugin JSON configuration object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587130453136" scope="col">Field</th><th align="left" valign="middle" id="idm140587130452160" scope="col">Type</th><th align="left" valign="middle" id="idm140587130451072" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587130453136"> <p>
										<code class="literal">cniVersion</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130452160"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130451072"> <p>
										The CNI specification version. The <code class="literal">0.3.1</code> value is required.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130453136"> <p>
										<code class="literal">name</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130452160"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130451072"> <p>
										The value for the <code class="literal">name</code> parameter you provided previously for the CNO configuration.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130453136"> <p>
										<code class="literal">type</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130452160"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130451072"> <p>
										The name of the CNI plugin to configure: <code class="literal">vlan</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130453136"> <p>
										<code class="literal">master</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130452160"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130451072"> <p>
										The Ethernet interface to associate with the network attachment. If a <code class="literal">master</code> is not specified, the interface for the default network route is used.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130453136"> <p>
										<code class="literal">vlanId</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130452160"> <p>
										<code class="literal">integer</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130451072"> <p>
										Set the id of the vlan.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130453136"> <p>
										<code class="literal">ipam</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130452160"> <p>
										<code class="literal">object</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130451072"> <p>
										The configuration object for the IPAM CNI plugin. The plugin manages IP address assignment for the attachment definition.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130453136"> <p>
										<code class="literal">mtu</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130452160"> <p>
										<code class="literal">integer</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130451072"> <p>
										Optional: Set the maximum transmission unit (MTU) to the specified value. The default value is automatically set by the kernel.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130453136"> <p>
										<code class="literal">dns</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130452160"> <p>
										<code class="literal">integer</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130451072"> <p>
										Optional: DNS information to return, for example, a priority-ordered list of DNS nameservers.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587130453136"> <p>
										<code class="literal">linkInContainer</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130452160"> <p>
										<code class="literal">boolean</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587130451072"> <p>
										Optional: Specifies if the master interface is in the container network namespace or the main network namespace.
									</p>
									 </td></tr></tbody></table></div></div><section class="section" id="nw-multus-vlan-config-example_configuring-additional-network"><div class="titlepage"><div><div><h5 class="title">24.2.3.3.1. vlan configuration example</h5></div></div></div><p>
							The following example configures an additional network named <code class="literal">vlan-net</code>:
						</p><pre class="programlisting language-json">{
  "name": "vlan-net",
  "cniVersion": "0.3.1",
  "type": "vlan",
  "master": "eth0",
  "mtu": 1500,
  "vlanId": 5,
  "linkInContainer": false,
  "ipam": {
      "type": "host-local",
      "subnet": "10.1.1.0/24"
  },
  "dns": {
      "nameservers": [ "10.1.1.1", "8.8.8.8" ]
  }
}</pre></section></section><section class="section" id="nw-multus-ipvlan-object_configuring-additional-network"><div class="titlepage"><div><div><h4 class="title">24.2.3.4. Configuration for an IPVLAN additional network</h4></div></div></div><p>
						The following object describes the configuration parameters for the IPVLAN CNI plugin:
					</p><div class="table" id="idm140587157006224"><p class="title"><strong>Table 24.5. IPVLAN CNI plugin JSON configuration object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587141352480" scope="col">Field</th><th align="left" valign="middle" id="idm140587141351392" scope="col">Type</th><th align="left" valign="middle" id="idm140587142132624" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587141352480"> <p>
										<code class="literal">cniVersion</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587141351392"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587142132624"> <p>
										The CNI specification version. The <code class="literal">0.3.1</code> value is required.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587141352480"> <p>
										<code class="literal">name</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587141351392"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587142132624"> <p>
										The value for the <code class="literal">name</code> parameter you provided previously for the CNO configuration.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587141352480"> <p>
										<code class="literal">type</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587141351392"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587142132624"> <p>
										The name of the CNI plugin to configure: <code class="literal">ipvlan</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587141352480"> <p>
										<code class="literal">ipam</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587141351392"> <p>
										<code class="literal">object</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587142132624"> <p>
										The configuration object for the IPAM CNI plugin. The plugin manages IP address assignment for the attachment definition. This is required unless the plugin is chained.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587141352480"> <p>
										<code class="literal">mode</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587141351392"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587142132624"> <p>
										Optional: The operating mode for the virtual network. The value must be <code class="literal">l2</code>, <code class="literal">l3</code>, or <code class="literal">l3s</code>. The default value is <code class="literal">l2</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587141352480"> <p>
										<code class="literal">master</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587141351392"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587142132624"> <p>
										Optional: The Ethernet interface to associate with the network attachment. If a <code class="literal">master</code> is not specified, the interface for the default network route is used.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587141352480"> <p>
										<code class="literal">mtu</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587141351392"> <p>
										<code class="literal">integer</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587142132624"> <p>
										Optional: Set the maximum transmission unit (MTU) to the specified value. The default value is automatically set by the kernel.
									</p>
									 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The <code class="literal">ipvlan</code> object does not allow virtual interfaces to communicate with the <code class="literal">master</code> interface. Therefore the container will not be able to reach the host by using the <code class="literal">ipvlan</code> interface. Be sure that the container joins a network that provides connectivity to the host, such as a network supporting the Precision Time Protocol (<code class="literal">PTP</code>).
								</li><li class="listitem">
									A single <code class="literal">master</code> interface cannot simultaneously be configured to use both <code class="literal">macvlan</code> and <code class="literal">ipvlan</code>.
								</li><li class="listitem">
									For IP allocation schemes that cannot be interface agnostic, the <code class="literal">ipvlan</code> plugin can be chained with an earlier plugin that handles this logic. If the <code class="literal">master</code> is omitted, then the previous result must contain a single interface name for the <code class="literal">ipvlan</code> plugin to enslave. If <code class="literal">ipam</code> is omitted, then the previous result is used to configure the <code class="literal">ipvlan</code> interface.
								</li></ul></div></div></div><section class="section" id="nw-multus-ipvlan-config-example_configuring-additional-network"><div class="titlepage"><div><div><h5 class="title">24.2.3.4.1. ipvlan configuration example</h5></div></div></div><p>
							The following example configures an additional network named <code class="literal">ipvlan-net</code>:
						</p><pre class="programlisting language-json">{
  "cniVersion": "0.3.1",
  "name": "ipvlan-net",
  "type": "ipvlan",
  "master": "eth1",
  "mode": "l3",
  "ipam": {
    "type": "static",
    "addresses": [
       {
         "address": "192.168.10.10/24"
       }
    ]
  }
}</pre></section></section><section class="section" id="nw-multus-macvlan-object_configuring-additional-network"><div class="titlepage"><div><div><h4 class="title">24.2.3.5. Configuration for a MACVLAN additional network</h4></div></div></div><p>
						The following object describes the configuration parameters for the macvlan CNI plugin:
					</p><div class="table" id="idm140587157121936"><p class="title"><strong>Table 24.6. MACVLAN CNI plugin JSON configuration object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587165222656" scope="col">Field</th><th align="left" valign="middle" id="idm140587165221568" scope="col">Type</th><th align="left" valign="middle" id="idm140587165220480" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587165222656"> <p>
										<code class="literal">cniVersion</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587165221568"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587165220480"> <p>
										The CNI specification version. The <code class="literal">0.3.1</code> value is required.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587165222656"> <p>
										<code class="literal">name</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587165221568"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587165220480"> <p>
										The value for the <code class="literal">name</code> parameter you provided previously for the CNO configuration.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587165222656"> <p>
										<code class="literal">type</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587165221568"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587165220480"> <p>
										The name of the CNI plugin to configure: <code class="literal">macvlan</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587165222656"> <p>
										<code class="literal">ipam</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587165221568"> <p>
										<code class="literal">object</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587165220480"> <p>
										The configuration object for the IPAM CNI plugin. The plugin manages IP address assignment for the attachment definition.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587165222656"> <p>
										<code class="literal">mode</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587165221568"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587165220480"> <p>
										Optional: Configures traffic visibility on the virtual network. Must be either <code class="literal">bridge</code>, <code class="literal">passthru</code>, <code class="literal">private</code>, or <code class="literal">vepa</code>. If a value is not provided, the default value is <code class="literal">bridge</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587165222656"> <p>
										<code class="literal">master</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587165221568"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587165220480"> <p>
										Optional: The host network interface to associate with the newly created macvlan interface. If a value is not specified, then the default route interface is used.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587165222656"> <p>
										<code class="literal">mtu</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587165221568"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587165220480"> <p>
										Optional: The maximum transmission unit (MTU) to the specified value. The default value is automatically set by the kernel.
									</p>
									 </td></tr></tbody></table></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you specify the <code class="literal">master</code> key for the plugin configuration, use a different physical network interface than the one that is associated with your primary network plugin to avoid possible conflicts.
						</p></div></div><section class="section" id="nw-multus-macvlan-config-example_configuring-additional-network"><div class="titlepage"><div><div><h5 class="title">24.2.3.5.1. macvlan configuration example</h5></div></div></div><p>
							The following example configures an additional network named <code class="literal">macvlan-net</code>:
						</p><pre class="programlisting language-json">{
  "cniVersion": "0.3.1",
  "name": "macvlan-net",
  "type": "macvlan",
  "master": "eth1",
  "mode": "bridge",
  "ipam": {
    "type": "dhcp"
    }
}</pre></section></section><section class="section" id="configuration-ovnk-additional-networks_configuring-additional-network"><div class="titlepage"><div><div><h4 class="title">24.2.3.6. Configuration for an OVN-Kubernetes additional network</h4></div></div></div><p>
						The Red Hat OpenShift Networking OVN-Kubernetes network plugin allows the configuration of secondary network interfaces for pods. To configure secondary network interfaces, you must define the configurations in the <code class="literal">NetworkAttachmentDefinition</code> custom resource definition (CRD).
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Configuration for an OVN-Kubernetes additional network is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
						</p><p>
							For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
						</p></div></div><p>
						The following sections provide example configurations for each of the topologies that OVN-Kubernetes currently allows for secondary networks.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Networks names must be unique. For example, creating multiple <code class="literal">NetworkAttachmentDefinition</code> CRDs with different configurations that reference the same network is unsupported.
						</p></div></div><section class="section" id="configuration-ovnk-network-plugin-json-object_configuring-additional-network"><div class="titlepage"><div><div><h5 class="title">24.2.3.6.1. OVN-Kubernetes network plugin JSON configuration table</h5></div></div></div><p>
							The following table describes the configuration parameters for the OVN-Kubernetes CNI network plugin:
						</p><div class="table" id="idm140587121568272"><p class="title"><strong>Table 24.7. OVN-Kubernetes network plugin JSON configuration table</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587121562480" scope="col">Field</th><th align="left" valign="middle" id="idm140587122766640" scope="col">Type</th><th align="left" valign="middle" id="idm140587122765552" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587121562480"> <p>
											<code class="literal">cniVersion</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587122766640"> <p>
											<code class="literal">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587122765552"> <p>
											The CNI specification version. The required value is <code class="literal">0.3.1</code>.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587121562480"> <p>
											<code class="literal">name</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587122766640"> <p>
											<code class="literal">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587122765552"> <p>
											The name of the network. These networks are not namespaced. For example, you can have a network named <code class="literal">l2-network</code> referenced from two different <code class="literal">NetworkAttachmentDefinitions</code> that exist on two different namespaces. This ensures that pods making use of the <code class="literal">NetworkAttachmentDefinition</code> on their own different namespaces can communicate over the same secondary network. However, those two different <code class="literal">NetworkAttachmentDefinitions</code> must also share the same network specific parameters such as <code class="literal">topology</code>, <code class="literal">subnets</code>, <code class="literal">mtu</code>, and <code class="literal">excludeSubnets</code>.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587121562480"> <p>
											<code class="literal">type</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587122766640"> <p>
											<code class="literal">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587122765552"> <p>
											The name of the CNI plugin to configure. The required value is <code class="literal">ovn-k8s-cni-overlay</code>.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587121562480"> <p>
											<code class="literal">topology</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587122766640"> <p>
											<code class="literal">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587122765552"> <p>
											The topological configuration for the network. The required value is <code class="literal">layer2</code>.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587121562480"> <p>
											<code class="literal">subnets</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587122766640"> <p>
											<code class="literal">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587122765552"> <p>
											The subnet to use for the network across the cluster. When specifying <code class="literal">layer2</code> for the <code class="literal">topology</code>, only include the CIDR for the node. For example, <code class="literal">10.100.200.0/24</code>.
										</p>
										 <p>
											For <code class="literal">"topology":"layer2"</code> deployments, IPv6 (<code class="literal">2001:DBB::/64</code>) and dual-stack (<code class="literal">192.168.100.0/24,2001:DBB::/64</code>) subnets are supported.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587121562480"> <p>
											<code class="literal">mtu</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587122766640"> <p>
											<code class="literal">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587122765552"> <p>
											The maximum transmission unit (MTU) to the specified value. The default value, <code class="literal">1300</code>, is automatically set by the kernel.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587121562480"> <p>
											<code class="literal">netAttachDefName</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587122766640"> <p>
											<code class="literal">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587122765552"> <p>
											The metadata <code class="literal">namespace</code> and <code class="literal">name</code> of the network attachment definition object where this configuration is included. For example, if this configuration is defined in a <code class="literal">NetworkAttachmentDefinition</code> in namespace <code class="literal">ns1</code> named <code class="literal">l2-network</code>, this should be set to <code class="literal">ns1/l2-network</code>.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587121562480"> <p>
											<code class="literal">excludeSubnets</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587122766640"> <p>
											<code class="literal">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587122765552"> <p>
											A comma-separated list of CIDRs and IPs. IPs are removed from the assignable IP pool, and are never passed to the pods. When omitted, the logical switch implementing the network only provides layer 2 communication, and users must configure IPs for the pods. Port security only prevents MAC spoofing.
										</p>
										 </td></tr></tbody></table></div></div></section><section class="section" id="configuration-layer-two-switched-topology_configuring-additional-network"><div class="titlepage"><div><div><h5 class="title">24.2.3.6.2. Configuration for a switched topology</h5></div></div></div><p>
							The switched (layer 2) topology networks interconnect the workloads through a cluster-wide logical switch. This configuration can be used for IPv6 and dual-stack deployments.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Layer 2 switched topology networks only allow for the transfer of data packets between pods within a cluster.
							</p></div></div><p>
							The following <code class="literal">NetworkAttachmentDefinition</code> custom resource definition (CRD) YAML describes the fields needed to configure a switched secondary network.
						</p><pre class="programlisting language-yaml">    {
            "cniVersion": "0.3.1",
            "name": "l2-network",
            "type": "ovn-k8s-cni-overlay",
            "topology":"layer2",
            "subnets": "10.100.200.0/24",
            "mtu": 1300,
            "netAttachDefName": "ns1/l2-network",
            "excludeSubnets": "10.100.200.0/29"
    }</pre></section><section class="section" id="configuring-pods-secondary-network_configuring-additional-network"><div class="titlepage"><div><div><h5 class="title">24.2.3.6.3. Configuring pods for additional networks</h5></div></div></div><p>
							You must specify the secondary network attachments through the <code class="literal">k8s.v1.cni.cncf.io/networks</code> annotation.
						</p><p>
							The following example provisions a pod with two secondary attachments, one for each of the attachment configurations presented in this guide.
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: l2-network
  name: tinypod
  namespace: ns1
spec:
  containers:
  - args:
    - pause
    image: k8s.gcr.io/e2e-test-images/agnhost:2.36
    imagePullPolicy: IfNotPresent
    name: agnhost-container</pre></section><section class="section" id="configuring-pods-static-ip_configuring-additional-network"><div class="titlepage"><div><div><h5 class="title">24.2.3.6.4. Configuring pods with a static IP address</h5></div></div></div><p>
							The following example provisions a pod with a static IP address.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										You can only specify the IP address for a pod’s secondary network attachment for layer 2 attachments.
									</li><li class="listitem">
										Specifying a static IP address for the pod is only possible when the attachment configuration does not feature subnets.
									</li></ul></div></div></div><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
      {
        "name": "l2-network", <span id="CO74-1"><!--Empty--></span><span class="callout">1</span>
        "mac": "02:03:04:05:06:07", <span id="CO74-2"><!--Empty--></span><span class="callout">2</span>
        "interface": "myiface1", <span id="CO74-3"><!--Empty--></span><span class="callout">3</span>
        "ips": [
          "192.0.2.20/24"
          ] <span id="CO74-4"><!--Empty--></span><span class="callout">4</span>
      }
    ]'
  name: tinypod
  namespace: ns1
spec:
  containers:
  - args:
    - pause
    image: k8s.gcr.io/e2e-test-images/agnhost:2.36
    imagePullPolicy: IfNotPresent
    name: agnhost-container</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO74-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the network. This value must be unique across all <code class="literal">NetworkAttachmentDefinitions</code>.
								</div></dd><dt><a href="#CO74-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The MAC address to be assigned for the interface.
								</div></dd><dt><a href="#CO74-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The name of the network interface to be created for the pod.
								</div></dd><dt><a href="#CO74-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The IP addresses to be assigned to the network interface.
								</div></dd></dl></div></section></section></section><section class="section" id="nw-multus-ipam-object_configuring-additional-network"><div class="titlepage"><div><div><h3 class="title">24.2.4. Configuration of IP address assignment for an additional network</h3></div></div></div><p>
					The IP address management (IPAM) Container Network Interface (CNI) plugin provides IP addresses for other CNI plugins.
				</p><p>
					You can use the following IP address assignment types:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Static assignment.
						</li><li class="listitem">
							Dynamic assignment through a DHCP server. The DHCP server you specify must be reachable from the additional network.
						</li><li class="listitem">
							Dynamic assignment through the Whereabouts IPAM CNI plugin.
						</li></ul></div><section class="section" id="nw-multus-static_configuring-additional-network"><div class="titlepage"><div><div><h4 class="title">24.2.4.1. Static IP address assignment configuration</h4></div></div></div><p>
						The following table describes the configuration for static IP address assignment:
					</p><div class="table" id="idm140587133947520"><p class="title"><strong>Table 24.8. <code class="literal">ipam</code> static configuration object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587139099088" scope="col">Field</th><th align="left" valign="middle" id="idm140587139098000" scope="col">Type</th><th align="left" valign="middle" id="idm140587139096912" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587139099088"> <p>
										<code class="literal">type</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587139098000"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587139096912"> <p>
										The IPAM address type. The value <code class="literal">static</code> is required.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587139099088"> <p>
										<code class="literal">addresses</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587139098000"> <p>
										<code class="literal">array</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587139096912"> <p>
										An array of objects specifying IP addresses to assign to the virtual interface. Both IPv4 and IPv6 IP addresses are supported.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587139099088"> <p>
										<code class="literal">routes</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587139098000"> <p>
										<code class="literal">array</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587139096912"> <p>
										An array of objects specifying routes to configure inside the pod.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587139099088"> <p>
										<code class="literal">dns</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587139098000"> <p>
										<code class="literal">array</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587139096912"> <p>
										Optional: An array of objects specifying the DNS configuration.
									</p>
									 </td></tr></tbody></table></div></div><p>
						The <code class="literal">addresses</code> array requires objects with the following fields:
					</p><div class="table" id="idm140587131992512"><p class="title"><strong>Table 24.9. <code class="literal">ipam.addresses[]</code> array</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587133237664" scope="col">Field</th><th align="left" valign="middle" id="idm140587133236576" scope="col">Type</th><th align="left" valign="middle" id="idm140587133235488" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587133237664"> <p>
										<code class="literal">address</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587133236576"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587133235488"> <p>
										An IP address and network prefix that you specify. For example, if you specify <code class="literal">10.10.21.10/24</code>, then the additional network is assigned an IP address of <code class="literal">10.10.21.10</code> and the netmask is <code class="literal">255.255.255.0</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587133237664"> <p>
										<code class="literal">gateway</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587133236576"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587133235488"> <p>
										The default gateway to route egress network traffic to.
									</p>
									 </td></tr></tbody></table></div></div><div class="table" id="idm140587149669232"><p class="title"><strong>Table 24.10. <code class="literal">ipam.routes[]</code> array</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587149663152" scope="col">Field</th><th align="left" valign="middle" id="idm140587140659632" scope="col">Type</th><th align="left" valign="middle" id="idm140587140658544" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587149663152"> <p>
										<code class="literal">dst</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587140659632"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587140658544"> <p>
										The IP address range in CIDR format, such as <code class="literal">192.168.17.0/24</code> or <code class="literal">0.0.0.0/0</code> for the default route.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587149663152"> <p>
										<code class="literal">gw</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587140659632"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587140658544"> <p>
										The gateway where network traffic is routed.
									</p>
									 </td></tr></tbody></table></div></div><div class="table" id="idm140587123510416"><p class="title"><strong>Table 24.11. <code class="literal">ipam.dns</code> object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587123661712" scope="col">Field</th><th align="left" valign="middle" id="idm140587123660624" scope="col">Type</th><th align="left" valign="middle" id="idm140587123659536" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587123661712"> <p>
										<code class="literal">nameservers</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123660624"> <p>
										<code class="literal">array</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123659536"> <p>
										An array of one or more IP addresses for to send DNS queries to.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587123661712"> <p>
										<code class="literal">domain</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123660624"> <p>
										<code class="literal">array</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123659536"> <p>
										The default domain to append to a hostname. For example, if the domain is set to <code class="literal">example.com</code>, a DNS lookup query for <code class="literal">example-host</code> is rewritten as <code class="literal">example-host.example.com</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587123661712"> <p>
										<code class="literal">search</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123660624"> <p>
										<code class="literal">array</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587123659536"> <p>
										An array of domain names to append to an unqualified hostname, such as <code class="literal">example-host</code>, during a DNS lookup query.
									</p>
									 </td></tr></tbody></table></div></div><div class="formalpara"><p class="title"><strong>Static IP address assignment configuration example</strong></p><p>
							
<pre class="programlisting language-json">{
  "ipam": {
    "type": "static",
      "addresses": [
        {
          "address": "191.168.1.7/24"
        }
      ]
  }
}</pre>

						</p></div></section><section class="section" id="nw-multus-dhcp_configuring-additional-network"><div class="titlepage"><div><div><h4 class="title">24.2.4.2. Dynamic IP address (DHCP) assignment configuration</h4></div></div></div><p>
						The following JSON describes the configuration for dynamic IP address address assignment with DHCP.
					</p><div class="admonition important"><div class="admonition_header">Renewal of DHCP leases</div><div><p>
							A pod obtains its original DHCP lease when it is created. The lease must be periodically renewed by a minimal DHCP server deployment running on the cluster.
						</p><p>
							To trigger the deployment of the DHCP server, you must create a shim network attachment by editing the Cluster Network Operator configuration, as in the following example:
						</p><div class="formalpara"><p class="title"><strong>Example shim network attachment definition</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  additionalNetworks:
  - name: dhcp-shim
    namespace: default
    type: Raw
    rawCNIConfig: |-
      {
        "name": "dhcp-shim",
        "cniVersion": "0.3.1",
        "type": "bridge",
        "ipam": {
          "type": "dhcp"
        }
      }
  # ...</pre>

							</p></div></div></div><div class="table" id="idm140587121407024"><p class="title"><strong>Table 24.12. <code class="literal">ipam</code> DHCP configuration object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587122074880" scope="col">Field</th><th align="left" valign="middle" id="idm140587122073904" scope="col">Type</th><th align="left" valign="middle" id="idm140587122072816" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587122074880"> <p>
										<code class="literal">type</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587122073904"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587122072816"> <p>
										The IPAM address type. The value <code class="literal">dhcp</code> is required.
									</p>
									 </td></tr></tbody></table></div></div><div class="formalpara"><p class="title"><strong>Dynamic IP address (DHCP) assignment configuration example</strong></p><p>
							
<pre class="programlisting language-json">{
  "ipam": {
    "type": "dhcp"
  }
}</pre>

						</p></div></section><section class="section" id="nw-multus-whereabouts_configuring-additional-network"><div class="titlepage"><div><div><h4 class="title">24.2.4.3. Dynamic IP address assignment configuration with Whereabouts</h4></div></div></div><p>
						The Whereabouts CNI plugin allows the dynamic assignment of an IP address to an additional network without the use of a DHCP server.
					</p><p>
						The following table describes the configuration for dynamic IP address assignment with Whereabouts:
					</p><div class="table" id="idm140587123967776"><p class="title"><strong>Table 24.13. <code class="literal">ipam</code> whereabouts configuration object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587140096240" scope="col">Field</th><th align="left" valign="middle" id="idm140587140095152" scope="col">Type</th><th align="left" valign="middle" id="idm140587140094064" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587140096240"> <p>
										<code class="literal">type</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587140095152"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587140094064"> <p>
										The IPAM address type. The value <code class="literal">whereabouts</code> is required.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587140096240"> <p>
										<code class="literal">range</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587140095152"> <p>
										<code class="literal">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587140094064"> <p>
										An IP address and range in CIDR notation. IP addresses are assigned from within this range of addresses.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587140096240"> <p>
										<code class="literal">exclude</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587140095152"> <p>
										<code class="literal">array</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587140094064"> <p>
										Optional: A list of zero or more IP addresses and ranges in CIDR notation. IP addresses within an excluded address range are not assigned.
									</p>
									 </td></tr></tbody></table></div></div><div class="formalpara"><p class="title"><strong>Dynamic IP address assignment configuration example that uses Whereabouts</strong></p><p>
							
<pre class="programlisting language-json">{
  "ipam": {
    "type": "whereabouts",
    "range": "192.0.2.192/27",
    "exclude": [
       "192.0.2.192/30",
       "192.0.2.196/32"
    ]
  }
}</pre>

						</p></div></section><section class="section" id="nw-multus-creating-whereabouts-reconciler-daemon-set_configuring-additional-network"><div class="titlepage"><div><div><h4 class="title">24.2.4.4. Creating a Whereabouts reconciler daemon set</h4></div></div></div><p>
						The Whereabouts reconciler is responsible for managing dynamic IP address assignments for the pods within a cluster using the Whereabouts IP Address Management (IPAM) solution. It ensures that each pods gets a unique IP address from the specified IP address range. It also handles IP address releases when pods are deleted or scaled down.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You can also use a <code class="literal">NetworkAttachmentDefinition</code> custom resource for dynamic IP address assignment.
						</p></div></div><p>
						The Whereabouts reconciler daemon set is automatically created when you configure an additional network through the Cluster Network Operator. It is not automatically created when you configure an additional network from a YAML manifest.
					</p><p>
						To trigger the deployment of the Whereabouts reconciler daemonset, you must manually create a <code class="literal">whereabouts-shim</code> network attachment by editing the Cluster Network Operator custom resource file.
					</p><p>
						Use the following procedure to deploy the Whereabouts reconciler daemonset.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">Network.operator.openshift.io</code> custom resource (CR) by running the following command:
							</p><pre class="programlisting language-terminal">$ oc edit network.operator.openshift.io cluster</pre></li><li class="listitem"><p class="simpara">
								Modify the <code class="literal">additionalNetworks</code> parameter in the CR to add the <code class="literal">whereabouts-shim</code> network attachment definition. For example:
							</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  additionalNetworks:
  - name: whereabouts-shim
    namespace: default
    rawCNIConfig: |-
      {
       "name": "whereabouts-shim",
       "cniVersion": "0.3.1",
       "type": "bridge",
       "ipam": {
         "type": "whereabouts"
       }
      }
    type: Raw</pre></li><li class="listitem">
								Save the file and exit the text editor.
							</li><li class="listitem"><p class="simpara">
								Verify that the <code class="literal">whereabouts-reconciler</code> daemon set deployed successfully by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get all -n openshift-multus | grep whereabouts-reconciler</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">pod/whereabouts-reconciler-jnp6g 1/1 Running 0 6s
pod/whereabouts-reconciler-k76gg 1/1 Running 0 6s
pod/whereabouts-reconciler-k86t9 1/1 Running 0 6s
pod/whereabouts-reconciler-p4sxw 1/1 Running 0 6s
pod/whereabouts-reconciler-rvfdv 1/1 Running 0 6s
pod/whereabouts-reconciler-svzw9 1/1 Running 0 6s
daemonset.apps/whereabouts-reconciler 6 6 6 6 6 kubernetes.io/os=linux 6s</pre>

								</p></div></li></ol></div></section></section><section class="section" id="nw-multus-create-network_configuring-additional-network"><div class="titlepage"><div><div><h3 class="title">24.2.5. Creating an additional network attachment with the Cluster Network Operator</h3></div></div></div><p>
					The Cluster Network Operator (CNO) manages additional network definitions. When you specify an additional network to create, the CNO creates the <code class="literal">NetworkAttachmentDefinition</code> object automatically.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Do not edit the <code class="literal">NetworkAttachmentDefinition</code> objects that the Cluster Network Operator manages. Doing so might disrupt network traffic on your additional network.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Optional: Create the namespace for the additional networks:
						</p><pre class="programlisting language-terminal">$ oc create namespace &lt;namespace_name&gt;</pre></li><li class="listitem"><p class="simpara">
							To edit the CNO configuration, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc edit networks.operator.openshift.io cluster</pre></li><li class="listitem"><p class="simpara">
							Modify the CR that you are creating by adding the configuration for the additional network that you are creating, as in the following example CR.
						</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  # ...
  additionalNetworks:
  - name: tertiary-net
    namespace: namespace2
    type: Raw
    rawCNIConfig: |-
      {
        "cniVersion": "0.3.1",
        "name": "tertiary-net",
        "type": "ipvlan",
        "master": "eth1",
        "mode": "l2",
        "ipam": {
          "type": "static",
          "addresses": [
            {
              "address": "192.168.1.23/24"
            }
          ]
        }
      }</pre></li><li class="listitem">
							Save your changes and quit the text editor to commit your changes.
						</li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Confirm that the CNO created the <code class="literal">NetworkAttachmentDefinition</code> object by running the following command. There might be a delay before the CNO creates the object.
						</p><pre class="programlisting language-terminal">$ oc get network-attachment-definitions -n &lt;namespace&gt;</pre><p class="simpara">
							where:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;namespace&gt;</code></span></dt><dd>
										Specifies the namespace for the network attachment that you added to the CNO configuration.
									</dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                 AGE
test-network-1       14m</pre>

							</p></div></li></ul></div></section><section class="section" id="nw-multus-create-network-apply_configuring-additional-network"><div class="titlepage"><div><div><h3 class="title">24.2.6. Creating an additional network attachment by applying a YAML manifest</h3></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a YAML file with your additional network configuration, such as in the following example:
						</p><pre class="programlisting language-yaml">apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: next-net
spec:
  config: |-
    {
      "cniVersion": "0.3.1",
      "name": "work-network",
      "type": "host-device",
      "device": "eth1",
      "ipam": {
        "type": "dhcp"
      }
    }</pre></li><li class="listitem"><p class="simpara">
							To create the additional network, enter the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f &lt;file&gt;.yaml</pre><p class="simpara">
							where:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;file&gt;</code></span></dt><dd>
										Specifies the name of the file contained the YAML manifest.
									</dd></dl></div></li></ol></div></section></section><section class="section" id="about-virtual-routing-and-forwarding"><div class="titlepage"><div><div><h2 class="title">24.3. About virtual routing and forwarding</h2></div></div></div><section class="section" id="cnf-about-virtual-routing-and-forwarding_about-virtual-routing-and-forwarding"><div class="titlepage"><div><div><h3 class="title">24.3.1. About virtual routing and forwarding</h3></div></div></div><p>
					Virtual routing and forwarding (VRF) devices combined with IP rules provide the ability to create virtual routing and forwarding domains. VRF reduces the number of permissions needed by CNF, and provides increased visibility of the network topology of secondary networks. VRF is used to provide multi-tenancy functionality, for example, where each tenant has its own unique routing tables and requires different default gateways.
				</p><p>
					Processes can bind a socket to the VRF device. Packets through the binded socket use the routing table associated with the VRF device. An important feature of VRF is that it impacts only OSI model layer 3 traffic and above so L2 tools, such as LLDP, are not affected. This allows higher priority IP rules such as policy based routing to take precedence over the VRF device rules directing specific traffic.
				</p><section class="section" id="cnf-benefits-secondary-networks-telecommunications-operators_about-virtual-routing-and-forwarding"><div class="titlepage"><div><div><h4 class="title">24.3.1.1. Benefits of secondary networks for pods for telecommunications operators</h4></div></div></div><p>
						In telecommunications use cases, each CNF can potentially be connected to multiple different networks sharing the same address space. These secondary networks can potentially conflict with the cluster’s main network CIDR. Using the CNI VRF plugin, network functions can be connected to different customers' infrastructure using the same IP address, keeping different customers isolated. IP addresses are overlapped with OpenShift Container Platform IP space. The CNI VRF plugin also reduces the number of permissions needed by CNF and increases the visibility of network topologies of secondary networks.
					</p></section></section></section><section class="section" id="configuring-multi-network-policy"><div class="titlepage"><div><div><h2 class="title">24.4. Configuring multi-network policy</h2></div></div></div><p>
				As a cluster administrator, you can configure multi-network for additional networks. You can specify multi-network policy for SR-IOV and macvlan additional networks. Macvlan additional networks are fully supported. Other types of additional networks, such as ipvlan, are not supported.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Support for configuring multi-network policies for SR-IOV additional networks is a Technology Preview feature and is only supported with kernel network interface cards (NICs). SR-IOV is not supported for Data Plane Development Kit (DPDK) applications.
				</p><p>
					For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
				</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Configured network policies are ignored in IPv6 networks.
				</p></div></div><section class="section" id="nw-multi-network-policy-differences_configuring-multi-network-policy"><div class="titlepage"><div><div><h3 class="title">24.4.1. Differences between multi-network policy and network policy</h3></div></div></div><p>
					Although the <code class="literal">MultiNetworkPolicy</code> API implements the <code class="literal">NetworkPolicy</code> API, there are several important differences:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							You must use the <code class="literal">MultiNetworkPolicy</code> API:
						</p><pre class="programlisting language-yaml">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy</pre></li><li class="listitem">
							You must use the <code class="literal">multi-networkpolicy</code> resource name when using the CLI to interact with multi-network policies. For example, you can view a multi-network policy object with the <code class="literal">oc get multi-networkpolicy &lt;name&gt;</code> command where <code class="literal">&lt;name&gt;</code> is the name of a multi-network policy.
						</li><li class="listitem"><p class="simpara">
							You must specify an annotation with the name of the network attachment definition that defines the macvlan or SR-IOV additional network:
						</p><pre class="programlisting language-yaml">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  annotations:
    k8s.v1.cni.cncf.io/policy-for: &lt;network_name&gt;</pre><p class="simpara">
							where:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">&lt;network_name&gt;</code></span></dt><dd>
										Specifies the name of a network attachment definition.
									</dd></dl></div></li></ul></div></section><section class="section" id="nw-multi-network-policy-enable_configuring-multi-network-policy"><div class="titlepage"><div><div><h3 class="title">24.4.2. Enabling multi-network policy for the cluster</h3></div></div></div><p>
					As a cluster administrator, you can enable multi-network policy support on your cluster.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							Log in to the cluster with a user with <code class="literal">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create the <code class="literal">multinetwork-enable-patch.yaml</code> file with the following YAML:
						</p><pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  useMultiNetworkPolicy: true</pre></li><li class="listitem"><p class="simpara">
							Configure the cluster to enable multi-network policy:
						</p><pre class="programlisting language-terminal">$ oc patch network.operator.openshift.io cluster --type=merge --patch-file=multinetwork-enable-patch.yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">network.operator.openshift.io/cluster patched</pre>

							</p></div></li></ol></div></section><section class="section" id="configuring-multi-network-policy_working-with-multi-network-policy"><div class="titlepage"><div><div><h3 class="title">24.4.3. Working with multi-network policy</h3></div></div></div><p>
					As a cluster administrator, you can create, edit, view, and delete multi-network policies.
				</p><section class="section" id="configuring-multi-network-policy_prerequisites"><div class="titlepage"><div><div><h4 class="title">24.4.3.1. Prerequisites</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								You have enabled multi-network policy support for your cluster.
							</li></ul></div></section><section class="section cluster-admin" id="nw-networkpolicy-create-cli_configuring-multi-network-policy"><div class="titlepage"><div><div><h4 class="title">24.4.3.2. Creating a multi-network policy using the CLI</h4></div></div></div><p class="cluster-admin cluster-admin">
						To define granular rules describing ingress or egress network traffic allowed for namespaces in your cluster, you can create a multi-network policy.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Your cluster uses a network plugin that supports <code class="literal cluster-admin">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal cluster-admin">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
							</li><li class="listitem">
								You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								You are logged in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li><li class="listitem">
								You are working in the namespace that the multi-network policy applies to.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create a policy rule:
							</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
										Create a <code class="literal cluster-admin">&lt;policy_name&gt;.yaml</code> file:
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ touch &lt;policy_name&gt;.yaml</pre><p class="cluster-admin cluster-admin">
										where:
									</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;policy_name&gt;</code></span></dt><dd>
													Specifies the multi-network policy file name.
												</dd></dl></div></li><li class="listitem"><p class="simpara">
										Define a multi-network policy in the file that you just created, such as in the following examples:
									</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Deny ingress from all pods in all namespaces</strong></p><p>
											This is a fundamental policy, blocking all cross-pod networking other than cross-pod traffic allowed by the configuration of other Network Policies.
										</p></div><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name: deny-by-default
  annotations:
    k8s.v1.cni.cncf.io/policy-for: &lt;network_name&gt;
spec:
  podSelector:
  ingress: []</pre><p class="cluster-admin cluster-admin">
										where:
									</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;network_name&gt;</code></span></dt><dd>
													Specifies the name of a network attachment definition.
												</dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Allow ingress from all pods in the same namespace</strong></p><p>
											
<pre class="programlisting language-yaml">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name: allow-same-namespace
  annotations:
    k8s.v1.cni.cncf.io/policy-for: &lt;network_name&gt;
spec:
  podSelector:
  ingress:
  - from:
    - podSelector: {}</pre>

										</p></div><p class="cluster-admin cluster-admin">
										where:
									</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;network_name&gt;</code></span></dt><dd>
													Specifies the name of a network attachment definition.
												</dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Allow ingress traffic to one pod from a particular namespace</strong></p><p>
											This policy allows traffic to pods labelled <code class="literal cluster-admin">pod-a</code> from pods running in <code class="literal cluster-admin">namespace-y</code>.
										</p></div><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name: allow-traffic-pod
  annotations:
    k8s.v1.cni.cncf.io/policy-for: &lt;network_name&gt;
spec:
  podSelector:
   matchLabels:
      pod: pod-a
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
           kubernetes.io/metadata.name: namespace-y</pre><p class="cluster-admin cluster-admin">
										where:
									</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;network_name&gt;</code></span></dt><dd>
													Specifies the name of a network attachment definition.
												</dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Restrict traffic to a service</strong></p><p>
											This policy when applied ensures every pod with both labels <code class="literal cluster-admin">app=bookstore</code> and <code class="literal cluster-admin">role=api</code> can only be accessed by pods with label <code class="literal cluster-admin">app=bookstore</code>. In this example the application could be a REST API server, marked with labels <code class="literal cluster-admin">app=bookstore</code> and <code class="literal cluster-admin">role=api</code>.
										</p></div><p class="cluster-admin cluster-admin">
										This example addresses the following use cases:
									</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
												Restricting the traffic to a service to only the other microservices that need to use it.
											</li><li class="listitem"><p class="simpara">
												Restricting the connections to a database to only permit the application using it.
											</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name: api-allow
  annotations:
    k8s.v1.cni.cncf.io/policy-for: &lt;network_name&gt;
spec:
  podSelector:
    matchLabels:
      app: bookstore
      role: api
  ingress:
  - from:
      - podSelector:
          matchLabels:
            app: bookstore</pre><p class="cluster-admin cluster-admin">
												where:
											</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;network_name&gt;</code></span></dt><dd>
															Specifies the name of a network attachment definition.
														</dd></dl></div></li></ul></div></li></ol></div></li><li class="listitem"><p class="simpara">
								To create the multi-network policy object, enter the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f &lt;policy_name&gt;.yaml -n &lt;namespace&gt;</pre><p class="cluster-admin cluster-admin">
								where:
							</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;policy_name&gt;</code></span></dt><dd>
											Specifies the multi-network policy file name.
										</dd><dt><span class="term"><code class="literal cluster-admin">&lt;namespace&gt;</code></span></dt><dd>
											Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.
										</dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">multinetworkpolicy.k8s.cni.cncf.io/deny-by-default created</pre>

								</p></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you log in to the web console with <code class="literal">cluster-admin</code> privileges, you have a choice of creating a network policy in any namespace in the cluster directly in YAML or from a form in the web console.
						</p></div></div></section><section class="section cluster-admin" id="nw-networkpolicy-edit_configuring-multi-network-policy"><div class="titlepage"><div><div><h4 class="title">24.4.3.3. Editing a multi-network policy</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can edit a multi-network policy in a namespace.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Your cluster uses a network plugin that supports <code class="literal cluster-admin">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal cluster-admin">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
							</li><li class="listitem">
								You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								You are logged in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li><li class="listitem">
								You are working in the namespace where the multi-network policy exists.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Optional: To list the multi-network policy objects in a namespace, enter the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get multi-networkpolicy</pre><p class="cluster-admin cluster-admin">
								where:
							</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;namespace&gt;</code></span></dt><dd>
											Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.
										</dd></dl></div></li><li class="listitem"><p class="simpara">
								Edit the multi-network policy object.
							</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
										If you saved the multi-network policy definition in a file, edit the file and make any necessary changes, and then enter the following command.
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -n &lt;namespace&gt; -f &lt;policy_file&gt;.yaml</pre><p class="cluster-admin cluster-admin">
										where:
									</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;namespace&gt;</code></span></dt><dd>
													Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.
												</dd><dt><span class="term"><code class="literal cluster-admin">&lt;policy_file&gt;</code></span></dt><dd>
													Specifies the name of the file containing the network policy.
												</dd></dl></div></li><li class="listitem"><p class="simpara">
										If you need to update the multi-network policy object directly, enter the following command:
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit multi-networkpolicy &lt;policy_name&gt; -n &lt;namespace&gt;</pre><p class="cluster-admin cluster-admin">
										where:
									</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;policy_name&gt;</code></span></dt><dd>
													Specifies the name of the network policy.
												</dd><dt><span class="term"><code class="literal cluster-admin">&lt;namespace&gt;</code></span></dt><dd>
													Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.
												</dd></dl></div></li></ul></div></li><li class="listitem"><p class="simpara">
								Confirm that the multi-network policy object is updated.
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc describe multi-networkpolicy &lt;policy_name&gt; -n &lt;namespace&gt;</pre><p class="cluster-admin cluster-admin">
								where:
							</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;policy_name&gt;</code></span></dt><dd>
											Specifies the name of the multi-network policy.
										</dd><dt><span class="term"><code class="literal cluster-admin">&lt;namespace&gt;</code></span></dt><dd>
											Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.
										</dd></dl></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you log in to the web console with <code class="literal">cluster-admin</code> privileges, you have a choice of editing a network policy in any namespace in the cluster directly in YAML or from the policy in the web console through the <span class="strong strong"><strong>Actions</strong></span> menu.
						</p></div></div></section><section class="section cluster-admin" id="nw-networkpolicy-view-cli_configuring-multi-network-policy"><div class="titlepage"><div><div><h4 class="title">24.4.3.4. Viewing multi-network policies using the CLI</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can examine the multi-network policies in a namespace.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								You are logged in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li><li class="listitem">
								You are working in the namespace where the multi-network policy exists.
							</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								List multi-network policies in a namespace:
							</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem"><p class="simpara">
										To view multi-network policy objects defined in a namespace, enter the following command:
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get multi-networkpolicy</pre></li><li class="listitem"><p class="simpara">
										Optional: To examine a specific multi-network policy, enter the following command:
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc describe multi-networkpolicy &lt;policy_name&gt; -n &lt;namespace&gt;</pre><p class="cluster-admin cluster-admin">
										where:
									</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;policy_name&gt;</code></span></dt><dd>
													Specifies the name of the multi-network policy to inspect.
												</dd><dt><span class="term"><code class="literal cluster-admin">&lt;namespace&gt;</code></span></dt><dd>
													Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.
												</dd></dl></div></li></ul></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you log in to the web console with <code class="literal">cluster-admin</code> privileges, you have a choice of viewing a network policy in any namespace in the cluster directly in YAML or from a form in the web console.
						</p></div></div></section><section class="section cluster-admin" id="nw-networkpolicy-delete-cli_configuring-multi-network-policy"><div class="titlepage"><div><div><h4 class="title">24.4.3.5. Deleting a multi-network policy using the CLI</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can delete a multi-network policy in a namespace.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Your cluster uses a network plugin that supports <code class="literal cluster-admin">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal cluster-admin">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
							</li><li class="listitem">
								You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								You are logged in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li><li class="listitem">
								You are working in the namespace where the multi-network policy exists.
							</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								To delete a multi-network policy object, enter the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete multi-networkpolicy &lt;policy_name&gt; -n &lt;namespace&gt;</pre><p class="cluster-admin cluster-admin">
								where:
							</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;policy_name&gt;</code></span></dt><dd>
											Specifies the name of the multi-network policy.
										</dd><dt><span class="term"><code class="literal cluster-admin">&lt;namespace&gt;</code></span></dt><dd>
											Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.
										</dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-text">multinetworkpolicy.k8s.cni.cncf.io/default-deny deleted</pre>

								</p></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you log in to the web console with <code class="literal">cluster-admin</code> privileges, you have a choice of deleting a network policy in any namespace in the cluster directly in YAML or from the policy in the web console through the <span class="strong strong"><strong>Actions</strong></span> menu.
						</p></div></div></section><section class="section cluster-admin" id="nw-networkpolicy-deny-all-multi-network-policy_configuring-multi-network-policy"><div class="titlepage"><div><div><h4 class="title">24.4.3.6. Creating a default deny all multi-network policy</h4></div></div></div><p class="cluster-admin cluster-admin">
						This is a fundamental policy, blocking all cross-pod networking other than network traffic allowed by the configuration of other deployed network policies. This procedure enforces a default <code class="literal cluster-admin">deny-by-default</code> policy.
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							If you log in with a user with the <code class="literal cluster-admin">cluster-admin</code> role, then you can create a network policy in any namespace in the cluster.
						</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Your cluster uses a network plugin that supports <code class="literal cluster-admin">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal cluster-admin">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
							</li><li class="listitem">
								You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								You are logged in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li><li class="listitem">
								You are working in the namespace that the multi-network policy applies to.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create the following YAML that defines a <code class="literal cluster-admin">deny-by-default</code> policy to deny ingress from all pods in all namespaces. Save the YAML in the <code class="literal cluster-admin">deny-by-default.yaml</code> file:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name: deny-by-default
  namespace: default <span id="CO75-1"><!--Empty--></span><span class="callout">1</span>
  annotations:
    k8s.v1.cni.cncf.io/policy-for: &lt;network_name&gt; <span id="CO75-2"><!--Empty--></span><span class="callout">2</span>
spec:
  podSelector: {} <span id="CO75-3"><!--Empty--></span><span class="callout">3</span>
  ingress: [] <span id="CO75-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO75-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										<code class="literal cluster-admin">namespace: default</code> deploys this policy to the <code class="literal cluster-admin">default</code> namespace.
									</div></dd><dt><a href="#CO75-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										<code class="literal cluster-admin">network_name</code>: specifies the name of a network attachment definition.
									</div></dd><dt><a href="#CO75-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										<code class="literal cluster-admin">podSelector:</code> is empty, this means it matches all the pods. Therefore, the policy applies to all pods in the default namespace.
									</div></dd><dt><a href="#CO75-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										There are no <code class="literal cluster-admin">ingress</code> rules specified. This causes incoming traffic to be dropped to all pods.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Apply the policy by entering the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f deny-by-default.yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">multinetworkpolicy.k8s.cni.cncf.io/deny-by-default created</pre>

								</p></div></li></ol></div></section><section class="section cluster-admin" id="nw-networkpolicy-allow-external-clients_configuring-multi-network-policy"><div class="titlepage"><div><div><h4 class="title">24.4.3.7. Creating a multi-network policy to allow traffic from external clients</h4></div></div></div><p class="cluster-admin cluster-admin">
						With the <code class="literal cluster-admin">deny-by-default</code> policy in place you can proceed to configure a policy that allows traffic from external clients to a pod with the label <code class="literal cluster-admin">app=web</code>.
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							If you log in with a user with the <code class="literal cluster-admin">cluster-admin</code> role, then you can create a network policy in any namespace in the cluster.
						</p></div></div><p class="cluster-admin cluster-admin">
						Follow this procedure to configure a policy that allows external service from the public Internet directly or by using a Load Balancer to access the pod. Traffic is only allowed to a pod with the label <code class="literal cluster-admin">app=web</code>.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Your cluster uses a network plugin that supports <code class="literal cluster-admin">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal cluster-admin">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
							</li><li class="listitem">
								You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								You are logged in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li><li class="listitem">
								You are working in the namespace that the multi-network policy applies to.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create a policy that allows traffic from the public Internet directly or by using a load balancer to access the pod. Save the YAML in the <code class="literal cluster-admin">web-allow-external.yaml</code> file:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name: web-allow-external
  namespace: default
  annotations:
    k8s.v1.cni.cncf.io/policy-for: &lt;network_name&gt;
spec:
  policyTypes:
  - Ingress
  podSelector:
    matchLabels:
      app: web
  ingress:
    - {}</pre></li><li class="listitem"><p class="simpara">
								Apply the policy by entering the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f web-allow-external.yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">multinetworkpolicy.k8s.cni.cncf.io/web-allow-external created</pre>

								</p></div></li></ol></div><p class="cluster-admin cluster-admin">
						This policy allows traffic from all resources, including external traffic as illustrated in the following diagram:
					</p><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/418cc0d92d9424c51592a0cef00fa005/292_OpenShift_Configuring_multi-network_policy_1122.png" alt="Allow traffic from external clients"/></div></div></section><section class="section cluster-admin" id="nw-networkpolicy-allow-traffic-from-all-applications_configuring-multi-network-policy"><div class="titlepage"><div><div><h4 class="title">24.4.3.8. Creating a multi-network policy allowing traffic to an application from all namespaces</h4></div></div></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							If you log in with a user with the <code class="literal cluster-admin">cluster-admin</code> role, then you can create a network policy in any namespace in the cluster.
						</p></div></div><p class="cluster-admin cluster-admin">
						Follow this procedure to configure a policy that allows traffic from all pods in all namespaces to a particular application.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Your cluster uses a network plugin that supports <code class="literal cluster-admin">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal cluster-admin">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
							</li><li class="listitem">
								You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								You are logged in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li><li class="listitem">
								You are working in the namespace that the multi-network policy applies to.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create a policy that allows traffic from all pods in all namespaces to a particular application. Save the YAML in the <code class="literal cluster-admin">web-allow-all-namespaces.yaml</code> file:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name: web-allow-all-namespaces
  namespace: default
  annotations:
    k8s.v1.cni.cncf.io/policy-for: &lt;network_name&gt;
spec:
  podSelector:
    matchLabels:
      app: web <span id="CO76-1"><!--Empty--></span><span class="callout">1</span>
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector: {} <span id="CO76-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO76-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Applies the policy only to <code class="literal cluster-admin">app:web</code> pods in default namespace.
									</div></dd><dt><a href="#CO76-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Selects all pods in all namespaces.
									</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
									By default, if you omit specifying a <code class="literal cluster-admin">namespaceSelector</code> it does not select any namespaces, which means the policy allows traffic only from the namespace the network policy is deployed to.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Apply the policy by entering the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f web-allow-all-namespaces.yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">multinetworkpolicy.k8s.cni.cncf.io/web-allow-all-namespaces created</pre>

								</p></div></li></ol></div><div class="orderedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Start a web service in the <code class="literal cluster-admin">default</code> namespace by entering the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc run web --namespace=default --image=nginx --labels="app=web" --expose --port=80</pre></li><li class="listitem"><p class="simpara">
								Run the following command to deploy an <code class="literal cluster-admin">alpine</code> image in the <code class="literal cluster-admin">secondary</code> namespace and to start a shell:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc run test-$RANDOM --namespace=secondary --rm -i -t --image=alpine -- sh</pre></li><li class="listitem"><p class="simpara">
								Run the following command in the shell and observe that the request is allowed:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin"># wget -qO- --timeout=2 http://web.default</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Expected output</strong></p><p>
									
<pre class="programlisting language-terminal">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</pre>

								</p></div></li></ol></div></section><section class="section cluster-admin" id="nw-networkpolicy-allow-traffic-from-a-namespace_configuring-multi-network-policy"><div class="titlepage"><div><div><h4 class="title">24.4.3.9. Creating a multi-network policy allowing traffic to an application from a namespace</h4></div></div></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							If you log in with a user with the <code class="literal cluster-admin">cluster-admin</code> role, then you can create a network policy in any namespace in the cluster.
						</p></div></div><p class="cluster-admin cluster-admin">
						Follow this procedure to configure a policy that allows traffic to a pod with the label <code class="literal cluster-admin">app=web</code> from a particular namespace. You might want to do this to:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Restrict traffic to a production database only to namespaces where production workloads are deployed.
							</li><li class="listitem">
								Enable monitoring tools deployed to a particular namespace to scrape metrics from the current namespace.
							</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Your cluster uses a network plugin that supports <code class="literal cluster-admin">NetworkPolicy</code> objects, such as the OpenShift SDN network provider with <code class="literal cluster-admin">mode: NetworkPolicy</code> set. This mode is the default for OpenShift SDN.
							</li><li class="listitem">
								You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								You are logged in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li><li class="listitem">
								You are working in the namespace that the multi-network policy applies to.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create a policy that allows traffic from all pods in a particular namespaces with a label <code class="literal cluster-admin">purpose=production</code>. Save the YAML in the <code class="literal cluster-admin">web-allow-prod.yaml</code> file:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name: web-allow-prod
  namespace: default
  annotations:
    k8s.v1.cni.cncf.io/policy-for: &lt;network_name&gt;
spec:
  podSelector:
    matchLabels:
      app: web <span id="CO77-1"><!--Empty--></span><span class="callout">1</span>
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          purpose: production <span id="CO77-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO77-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Applies the policy only to <code class="literal cluster-admin">app:web</code> pods in the default namespace.
									</div></dd><dt><a href="#CO77-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Restricts traffic to only pods in namespaces that have the label <code class="literal cluster-admin">purpose=production</code>.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Apply the policy by entering the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f web-allow-prod.yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">multinetworkpolicy.k8s.cni.cncf.io/web-allow-prod created</pre>

								</p></div></li></ol></div><div class="orderedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Start a web service in the <code class="literal cluster-admin">default</code> namespace by entering the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc run web --namespace=default --image=nginx --labels="app=web" --expose --port=80</pre></li><li class="listitem"><p class="simpara">
								Run the following command to create the <code class="literal cluster-admin">prod</code> namespace:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create namespace prod</pre></li><li class="listitem"><p class="simpara">
								Run the following command to label the <code class="literal cluster-admin">prod</code> namespace:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc label namespace/prod purpose=production</pre></li><li class="listitem"><p class="simpara">
								Run the following command to create the <code class="literal cluster-admin">dev</code> namespace:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create namespace dev</pre></li><li class="listitem"><p class="simpara">
								Run the following command to label the <code class="literal cluster-admin">dev</code> namespace:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc label namespace/dev purpose=testing</pre></li><li class="listitem"><p class="simpara">
								Run the following command to deploy an <code class="literal cluster-admin">alpine</code> image in the <code class="literal cluster-admin">dev</code> namespace and to start a shell:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc run test-$RANDOM --namespace=dev --rm -i -t --image=alpine -- sh</pre></li><li class="listitem"><p class="simpara">
								Run the following command in the shell and observe that the request is blocked:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin"># wget -qO- --timeout=2 http://web.default</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Expected output</strong></p><p>
									
<pre class="programlisting language-terminal">wget: download timed out</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Run the following command to deploy an <code class="literal cluster-admin">alpine</code> image in the <code class="literal cluster-admin">prod</code> namespace and start a shell:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc run test-$RANDOM --namespace=prod --rm -i -t --image=alpine -- sh</pre></li><li class="listitem"><p class="simpara">
								Run the following command in the shell and observe that the request is allowed:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin"># wget -qO- --timeout=2 http://web.default</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Expected output</strong></p><p>
									
<pre class="programlisting language-terminal">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</pre>

								</p></div></li></ol></div></section></section><section class="section _additional-resources" id="configuring-multi-network-policy_additional-resources"><div class="titlepage"><div><div><h3 class="title">24.4.4. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-network-policy">About network policy</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#understanding-multiple-networks">Understanding multiple networks</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-multus-macvlan-object_configuring-additional-network">Configuring a macvlan network</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-sriov-device">Configuring an SR-IOV network device</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="attaching-pod"><div class="titlepage"><div><div><h2 class="title">24.5. Attaching a pod to an additional network</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster user you can attach a pod to an additional network.
			</p><section class="section cluster-admin" id="nw-multus-add-pod_attaching-pod"><div class="titlepage"><div><div><h3 class="title">24.5.1. Adding a pod to an additional network</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can add a pod to an additional network. The pod continues to send normal cluster-related network traffic over the default network.
				</p><p class="cluster-admin cluster-admin">
					When a pod is created additional networks are attached to it. However, if a pod already exists, you cannot attach additional networks to it.
				</p><p class="cluster-admin cluster-admin">
					The pod must be in the same namespace as the additional network.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in to the cluster.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Add an annotation to the <code class="literal cluster-admin">Pod</code> object. Only one of the following annotation formats can be used:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									To attach an additional network without any customization, add an annotation with the following format. Replace <code class="literal cluster-admin">&lt;network&gt;</code> with the name of the additional network to associate with the pod:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: &lt;network&gt;[,&lt;network&gt;,...] <span id="CO78-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO78-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											To specify more than one additional network, separate each network with a comma. Do not include whitespace between the comma. If you specify the same additional network multiple times, that pod will have multiple network interfaces attached to that network.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To attach an additional network with customizations, add an annotation with the following format:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: |-
      [
        {
          "name": "&lt;network&gt;", <span id="CO79-1"><!--Empty--></span><span class="callout">1</span>
          "namespace": "&lt;namespace&gt;", <span id="CO79-2"><!--Empty--></span><span class="callout">2</span>
          "default-route": ["&lt;default-route&gt;"] <span id="CO79-3"><!--Empty--></span><span class="callout">3</span>
        }
      ]</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO79-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the name of the additional network defined by a <code class="literal cluster-admin">NetworkAttachmentDefinition</code> object.
										</div></dd><dt><a href="#CO79-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Specify the namespace where the <code class="literal cluster-admin">NetworkAttachmentDefinition</code> object is defined.
										</div></dd><dt><a href="#CO79-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Optional: Specify an override for the default route, such as <code class="literal cluster-admin">192.168.17.1</code>.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							To create the pod, enter the following command. Replace <code class="literal cluster-admin">&lt;name&gt;</code> with the name of the pod.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f &lt;name&gt;.yaml</pre></li><li class="listitem"><p class="simpara">
							Optional: To Confirm that the annotation exists in the <code class="literal cluster-admin">Pod</code> CR, enter the following command, replacing <code class="literal cluster-admin">&lt;name&gt;</code> with the name of the pod.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pod &lt;name&gt; -o yaml</pre><p class="cluster-admin cluster-admin">
							In the following example, the <code class="literal cluster-admin">example-pod</code> pod is attached to the <code class="literal cluster-admin">net1</code> additional network:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pod example-pod -o yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: macvlan-bridge
    k8s.v1.cni.cncf.io/network-status: |- <span id="CO80-1"><!--Empty--></span><span class="callout">1</span>
      [{
          "name": "openshift-sdn",
          "interface": "eth0",
          "ips": [
              "10.128.2.14"
          ],
          "default": true,
          "dns": {}
      },{
          "name": "macvlan-bridge",
          "interface": "net1",
          "ips": [
              "20.2.2.100"
          ],
          "mac": "22:2f:60:a5:f8:00",
          "dns": {}
      }]
  name: example-pod
  namespace: default
spec:
  ...
status:
  ...</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO80-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal cluster-admin">k8s.v1.cni.cncf.io/network-status</code> parameter is a JSON array of objects. Each object describes the status of an additional network attached to the pod. The annotation value is stored as a plain text value.
								</div></dd></dl></div></li></ol></div><section class="section cluster-admin" id="nw-multus-advanced-annotations_attaching-pod"><div class="titlepage"><div><div><h4 class="title">24.5.1.1. Specifying pod-specific addressing and routing options</h4></div></div></div><p class="cluster-admin cluster-admin">
						When attaching a pod to an additional network, you may want to specify further properties about that network in a particular pod. This allows you to change some aspects of routing, as well as specify static IP addresses and MAC addresses. To accomplish this, you can use the JSON formatted annotations.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								The pod must be in the same namespace as the additional network.
							</li><li class="listitem">
								Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								You must log in to the cluster.
							</li></ul></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Procedure</strong></p><p>
							To add a pod to an additional network while specifying addressing and/or routing options, complete the following steps:
						</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal cluster-admin">Pod</code> resource definition. If you are editing an existing <code class="literal cluster-admin">Pod</code> resource, run the following command to edit its definition in the default editor. Replace <code class="literal cluster-admin">&lt;name&gt;</code> with the name of the <code class="literal cluster-admin">Pod</code> resource to edit.
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit pod &lt;name&gt;</pre></li><li class="listitem"><p class="simpara">
								In the <code class="literal cluster-admin">Pod</code> resource definition, add the <code class="literal cluster-admin">k8s.v1.cni.cncf.io/networks</code> parameter to the pod <code class="literal cluster-admin">metadata</code> mapping. The <code class="literal cluster-admin">k8s.v1.cni.cncf.io/networks</code> accepts a JSON string of a list of objects that reference the name of <code class="literal cluster-admin">NetworkAttachmentDefinition</code> custom resource (CR) names in addition to specifying additional properties.
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: '[&lt;network&gt;[,&lt;network&gt;,...]]' <span id="CO81-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO81-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal cluster-admin">&lt;network&gt;</code> with a JSON object as shown in the following examples. The single quotes are required.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								In the following example the annotation specifies which network attachment will have the default route, using the <code class="literal cluster-admin">default-route</code> parameter.
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Pod
metadata:
  name: example-pod
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
    {
      "name": "net1"
    },
    {
      "name": "net2", <span id="CO82-1"><!--Empty--></span><span class="callout">1</span>
      "default-route": ["192.0.2.1"] <span id="CO82-2"><!--Empty--></span><span class="callout">2</span>
    }]'
spec:
  containers:
  - name: example-pod
    command: ["/bin/bash", "-c", "sleep 2000000000000"]
    image: centos/tools</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO82-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The <code class="literal cluster-admin">name</code> key is the name of the additional network to associate with the pod.
									</div></dd><dt><a href="#CO82-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The <code class="literal cluster-admin">default-route</code> key specifies a value of a gateway for traffic to be routed over if no other routing entry is present in the routing table. If more than one <code class="literal cluster-admin">default-route</code> key is specified, this will cause the pod to fail to become active.
									</div></dd></dl></div></li></ol></div><p class="cluster-admin cluster-admin">
						The default route will cause any traffic that is not specified in other routes to be routed to the gateway.
					</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
							Setting the default route to an interface other than the default network interface for OpenShift Container Platform may cause traffic that is anticipated for pod-to-pod traffic to be routed over another interface.
						</p></div></div><p class="cluster-admin cluster-admin">
						To verify the routing properties of a pod, the <code class="literal cluster-admin">oc</code> command may be used to execute the <code class="literal cluster-admin">ip</code> command within a pod.
					</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec -it &lt;pod_name&gt; -- ip route</pre><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							You may also reference the pod’s <code class="literal cluster-admin">k8s.v1.cni.cncf.io/network-status</code> to see which additional network has been assigned the default route, by the presence of the <code class="literal cluster-admin">default-route</code> key in the JSON-formatted list of objects.
						</p></div></div><p class="cluster-admin cluster-admin">
						To set a static IP address or MAC address for a pod you can use the JSON formatted annotations. This requires you create networks that specifically allow for this functionality. This can be specified in a rawCNIConfig for the CNO.
					</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Edit the CNO CR by running the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit networks.operator.openshift.io cluster</pre></li></ol></div><p class="cluster-admin cluster-admin">
						The following YAML describes the configuration parameters for the CNO:
					</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Cluster Network Operator YAML configuration</strong></p><p>
							
<pre class="programlisting language-yaml">name: &lt;name&gt; <span id="CO83-1"><!--Empty--></span><span class="callout">1</span>
namespace: &lt;namespace&gt; <span id="CO83-2"><!--Empty--></span><span class="callout">2</span>
rawCNIConfig: '{ <span id="CO83-3"><!--Empty--></span><span class="callout">3</span>
  ...
}'
type: Raw</pre>

						</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO83-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specify a name for the additional network attachment that you are creating. The name must be unique within the specified <code class="literal cluster-admin">namespace</code>.
							</div></dd><dt><a href="#CO83-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specify the namespace to create the network attachment in. If you do not specify a value, then the <code class="literal cluster-admin">default</code> namespace is used.
							</div></dd><dt><a href="#CO83-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specify the CNI plugin configuration in JSON format, which is based on the following template.
							</div></dd></dl></div><p class="cluster-admin cluster-admin">
						The following object describes the configuration parameters for utilizing static MAC address and IP address using the macvlan CNI plugin:
					</p><div class="cluster-admin cluster-admin"><p class="title"><strong>macvlan CNI plugin JSON configuration object using static IP and MAC address</strong></p><p>
							
<pre class="programlisting language-json">{
  "cniVersion": "0.3.1",
  "name": "&lt;name&gt;", <span id="CO84-1"><!--Empty--></span><span class="callout">1</span>
  "plugins": [{ <span id="CO84-2"><!--Empty--></span><span class="callout">2</span>
      "type": "macvlan",
      "capabilities": { "ips": true }, <span id="CO84-3"><!--Empty--></span><span class="callout">3</span>
      "master": "eth0", <span id="CO84-4"><!--Empty--></span><span class="callout">4</span>
      "mode": "bridge",
      "ipam": {
        "type": "static"
      }
    }, {
      "capabilities": { "mac": true }, <span id="CO84-5"><!--Empty--></span><span class="callout">5</span>
      "type": "tuning"
    }]
}</pre>

						</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO84-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specifies the name for the additional network attachment to create. The name must be unique within the specified <code class="literal cluster-admin">namespace</code>.
							</div></dd><dt><a href="#CO84-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specifies an array of CNI plugin configurations. The first object specifies a macvlan plugin configuration and the second object specifies a tuning plugin configuration.
							</div></dd><dt><a href="#CO84-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specifies that a request is made to enable the static IP address functionality of the CNI plugin runtime configuration capabilities.
							</div></dd><dt><a href="#CO84-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Specifies the interface that the macvlan plugin uses.
							</div></dd><dt><a href="#CO84-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Specifies that a request is made to enable the static MAC address functionality of a CNI plugin.
							</div></dd></dl></div><p class="cluster-admin cluster-admin">
						The above network attachment can be referenced in a JSON formatted annotation, along with keys to specify which static IP and MAC address will be assigned to a given pod.
					</p><p class="cluster-admin cluster-admin">
						Edit the pod with:
					</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit pod &lt;name&gt;</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>macvlan CNI plugin JSON configuration object using static IP and MAC address</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: example-pod
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
      {
        "name": "&lt;name&gt;", <span id="CO85-1"><!--Empty--></span><span class="callout">1</span>
        "ips": [ "192.0.2.205/24" ], <span id="CO85-2"><!--Empty--></span><span class="callout">2</span>
        "mac": "CA:FE:C0:FF:EE:00" <span id="CO85-3"><!--Empty--></span><span class="callout">3</span>
      }
    ]'</pre>

						</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO85-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Use the <code class="literal cluster-admin">&lt;name&gt;</code> as provided when creating the <code class="literal cluster-admin">rawCNIConfig</code> above.
							</div></dd><dt><a href="#CO85-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Provide an IP address including the subnet mask.
							</div></dd><dt><a href="#CO85-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Provide the MAC address.
							</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							Static IP addresses and MAC addresses do not have to be used at the same time, you may use them individually, or together.
						</p></div></div><p class="cluster-admin cluster-admin">
						To verify the IP address and MAC properties of a pod with additional networks, use the <code class="literal cluster-admin">oc</code> command to execute the ip command within a pod.
					</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec -it &lt;pod_name&gt; -- ip a</pre></section></section></section><section class="section cluster-admin" id="removing-pod"><div class="titlepage"><div><div><h2 class="title">24.6. Removing a pod from an additional network</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster user you can remove a pod from an additional network.
			</p><section class="section cluster-admin" id="nw-multus-remove-pod_removing-pod"><div class="titlepage"><div><div><h3 class="title">24.6.1. Removing a pod from an additional network</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can remove a pod from an additional network only by deleting the pod.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							An additional network is attached to the pod.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in to the cluster.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							To delete the pod, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete pod &lt;name&gt; -n &lt;namespace&gt;</pre><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem">
									<code class="literal cluster-admin">&lt;name&gt;</code> is the name of the pod.
								</li><li class="listitem">
									<code class="literal cluster-admin">&lt;namespace&gt;</code> is the namespace that contains the pod.
								</li></ul></div></li></ul></div></section></section><section class="section cluster-admin" id="edit-additional-network"><div class="titlepage"><div><div><h2 class="title">24.7. Editing an additional network</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator you can modify the configuration for an existing additional network.
			</p><section class="section cluster-admin" id="nw-multus-edit-network_edit-additional-network"><div class="titlepage"><div><div><h3 class="title">24.7.1. Modifying an additional network attachment definition</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can make changes to an existing additional network. Any existing pods attached to the additional network will not be updated.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have configured an additional network for your cluster.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Procedure</strong></p><p>
						To edit an additional network for your cluster, complete the following steps:
					</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Run the following command to edit the Cluster Network Operator (CNO) CR in your default text editor:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit networks.operator.openshift.io cluster</pre></li><li class="listitem">
							In the <code class="literal cluster-admin">additionalNetworks</code> collection, update the additional network with your changes.
						</li><li class="listitem">
							Save your changes and quit the text editor to commit your changes.
						</li><li class="listitem"><p class="simpara">
							Optional: Confirm that the CNO updated the <code class="literal cluster-admin">NetworkAttachmentDefinition</code> object by running the following command. Replace <code class="literal cluster-admin">&lt;network-name&gt;</code> with the name of the additional network to display. There might be a delay before the CNO updates the <code class="literal cluster-admin">NetworkAttachmentDefinition</code> object to reflect your changes.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get network-attachment-definitions &lt;network-name&gt; -o yaml</pre><p class="cluster-admin cluster-admin">
							For example, the following console output displays a <code class="literal cluster-admin">NetworkAttachmentDefinition</code> object that is named <code class="literal cluster-admin">net1</code>:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get network-attachment-definitions net1 -o go-template='{{printf "%s\n" .spec.config}}'
{ "cniVersion": "0.3.1", "type": "macvlan",
"master": "ens5",
"mode": "bridge",
"ipam":       {"type":"static","routes":[{"dst":"0.0.0.0/0","gw":"10.128.2.1"}],"addresses":[{"address":"10.128.2.100/23","gateway":"10.128.2.1"}],"dns":{"nameservers":["172.30.0.10"],"domain":"us-west-2.compute.internal","search":["us-west-2.compute.internal"]}} }</pre></li></ol></div></section></section><section class="section cluster-admin" id="remove-additional-network"><div class="titlepage"><div><div><h2 class="title">24.8. Removing an additional network</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator you can remove an additional network attachment.
			</p><section class="section cluster-admin" id="nw-multus-delete-network_remove-additional-network"><div class="titlepage"><div><div><h3 class="title">24.8.1. Removing an additional network attachment definition</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can remove an additional network from your OpenShift Container Platform cluster. The additional network is not removed from any pods it is attached to.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Procedure</strong></p><p>
						To remove an additional network from your cluster, complete the following steps:
					</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Edit the Cluster Network Operator (CNO) in your default text editor by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit networks.operator.openshift.io cluster</pre></li><li class="listitem"><p class="simpara">
							Modify the CR by removing the configuration from the <code class="literal cluster-admin">additionalNetworks</code> collection for the network attachment definition you are removing.
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  additionalNetworks: [] <span id="CO86-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO86-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									If you are removing the configuration mapping for the only additional network attachment definition in the <code class="literal cluster-admin">additionalNetworks</code> collection, you must specify an empty collection.
								</div></dd></dl></div></li><li class="listitem">
							Save your changes and quit the text editor to commit your changes.
						</li><li class="listitem"><p class="simpara">
							Optional: Confirm that the additional network CR was deleted by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get network-attachment-definition --all-namespaces</pre></li></ol></div></section></section><section class="section cluster-admin" id="assigning-a-secondary-network-to-a-vrf"><div class="titlepage"><div><div><h2 class="title">24.9. Assigning a secondary network to a VRF</h2></div></div></div><section class="section cluster-admin" id="cnf-assigning-a-secondary-network-to-a-vrf_assigning-a-secondary-network-to-a-vrf"><div class="titlepage"><div><div><h3 class="title">24.9.1. Assigning a secondary network to a VRF</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can configure an additional network for your VRF domain by using the CNI VRF plugin. The virtual network created by this plugin is associated with a physical interface that you specify.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						Applications that use VRFs need to bind to a specific device. The common usage is to use the <code class="literal cluster-admin">SO_BINDTODEVICE</code> option for a socket. <code class="literal cluster-admin">SO_BINDTODEVICE</code> binds the socket to a device that is specified in the passed interface name, for example, <code class="literal cluster-admin">eth1</code>. To use <code class="literal cluster-admin">SO_BINDTODEVICE</code>, the application must have <code class="literal cluster-admin">CAP_NET_RAW</code> capabilities.
					</p><p class="cluster-admin cluster-admin">
						Using a VRF through the <code class="literal cluster-admin">ip vrf exec</code> command is not supported in OpenShift Container Platform pods. To use VRF, bind applications directly to the VRF interface.
					</p></div></div><section class="section cluster-admin" id="cnf-creating-an-additional-network-attachment-with-the-cni-vrf-plug-in_assigning-a-secondary-network-to-a-vrf"><div class="titlepage"><div><div><h4 class="title">24.9.1.1. Creating an additional network attachment with the CNI VRF plugin</h4></div></div></div><p class="cluster-admin cluster-admin">
						The Cluster Network Operator (CNO) manages additional network definitions. When you specify an additional network to create, the CNO creates the <code class="literal cluster-admin">NetworkAttachmentDefinition</code> custom resource (CR) automatically.
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							Do not edit the <code class="literal cluster-admin">NetworkAttachmentDefinition</code> CRs that the Cluster Network Operator manages. Doing so might disrupt network traffic on your additional network.
						</p></div></div><p class="cluster-admin cluster-admin">
						To create an additional network attachment with the CNI VRF plugin, perform the following procedure.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Install the OpenShift Container Platform CLI (oc).
							</li><li class="listitem">
								Log in to the OpenShift cluster as a user with cluster-admin privileges.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create the <code class="literal cluster-admin">Network</code> custom resource (CR) for the additional network attachment and insert the <code class="literal cluster-admin">rawCNIConfig</code> configuration for the additional network, as in the following example CR. Save the YAML as the file <code class="literal cluster-admin">additional-network-attachment.yaml</code>.
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
  spec:
  additionalNetworks:
  - name: test-network-1
    namespace: additional-network-1
    type: Raw
    rawCNIConfig: '{
      "cniVersion": "0.3.1",
      "name": "macvlan-vrf",
      "plugins": [  <span id="CO87-1"><!--Empty--></span><span class="callout">1</span>
      {
        "type": "macvlan",  <span id="CO87-2"><!--Empty--></span><span class="callout">2</span>
        "master": "eth1",
        "ipam": {
            "type": "static",
            "addresses": [
            {
                "address": "191.168.1.23/24"
            }
            ]
        }
      },
      {
        "type": "vrf",
        "vrfname": "example-vrf-name",  <span id="CO87-3"><!--Empty--></span><span class="callout">3</span>
        "table": 1001   <span id="CO87-4"><!--Empty--></span><span class="callout">4</span>
      }]
    }'</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO87-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										<code class="literal cluster-admin">plugins</code> must be a list. The first item in the list must be the secondary network underpinning the VRF network. The second item in the list is the VRF plugin configuration.
									</div></dd><dt><a href="#CO87-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										<code class="literal cluster-admin">type</code> must be set to <code class="literal cluster-admin">vrf</code>.
									</div></dd><dt><a href="#CO87-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										<code class="literal cluster-admin">vrfname</code> is the name of the VRF that the interface is assigned to. If it does not exist in the pod, it is created.
									</div></dd><dt><a href="#CO87-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										Optional. <code class="literal cluster-admin">table</code> is the routing table ID. By default, the <code class="literal cluster-admin">tableid</code> parameter is used. If it is not specified, the CNI assigns a free routing table ID to the VRF.
									</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
									VRF functions correctly only when the resource is of type <code class="literal cluster-admin">netdevice</code>.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Create the <code class="literal cluster-admin">Network</code> resource:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f additional-network-attachment.yaml</pre></li><li class="listitem"><p class="simpara">
								Confirm that the CNO created the <code class="literal cluster-admin">NetworkAttachmentDefinition</code> CR by running the following command. Replace <code class="literal cluster-admin">&lt;namespace&gt;</code> with the namespace that you specified when configuring the network attachment, for example, <code class="literal cluster-admin">additional-network-1</code>.
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get network-attachment-definitions -n &lt;namespace&gt;</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                       AGE
additional-network-1       14m</pre>

								</p></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
									There might be a delay before the CNO creates the CR.
								</p></div></div></li></ol></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Verifying that the additional VRF network attachment is successful</strong></p><p>
							To verify that the VRF CNI is correctly configured and the additional network attachment is attached, do the following:
						</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
								Create a network that uses the VRF CNI.
							</li><li class="listitem">
								Assign the network to a pod.
							</li><li class="listitem"><p class="simpara">
								Verify that the pod network attachment is connected to the VRF additional network. Remote shell into the pod and run the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ip vrf show</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Name              Table
-----------------------
red                 10</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Confirm the VRF interface is master of the secondary interface:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ip link</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">5: net1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master red state UP mode</pre>

								</p></div></li></ol></div></section></section></section></section><section class="chapter cluster-admin" id="hardware-networks"><div class="titlepage"><div><div><h1 class="title">Chapter 25. Hardware networks</h1></div></div></div><section class="section cluster-admin" id="about-sriov"><div class="titlepage"><div><div><h2 class="title">25.1. About Single Root I/O Virtualization (SR-IOV) hardware networks</h2></div></div></div><p class="cluster-admin cluster-admin">
				The Single Root I/O Virtualization (SR-IOV) specification is a standard for a type of PCI device assignment that can share a single device with multiple pods.
			</p><p class="cluster-admin cluster-admin">
				SR-IOV can segment a compliant network device, recognized on the host node as a physical function (PF), into multiple virtual functions (VFs). The VF is used like any other network device. The SR-IOV network device driver for the device determines how the VF is exposed in the container:
			</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
						<code class="literal cluster-admin">netdevice</code> driver: A regular kernel network device in the <code class="literal cluster-admin">netns</code> of the container
					</li><li class="listitem">
						<code class="literal cluster-admin">vfio-pci</code> driver: A character device mounted in the container
					</li></ul></div><p class="cluster-admin cluster-admin">
				You can use SR-IOV network devices with additional networks on your OpenShift Container Platform cluster installed on bare metal or Red Hat OpenStack Platform (RHOSP) infrastructure for applications that require high bandwidth or low latency.
			</p><p class="cluster-admin cluster-admin">
				You can configure multi-network policies for SR-IOV networks. The support for this is technology preview and SR-IOV additional networks are only supported with kernel NICs. They are not supported for Data Plane Development Kit (DPDK) applications.
			</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
					Creating multi-network policies on SR-IOV networks might not deliver the same performance to applications compared to SR-IOV networks without a multi-network policy configured.
				</p></div></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
					Multi-network policies for SR-IOV network is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
				</p><p class="cluster-admin cluster-admin">
					For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
				</p></div></div><p class="cluster-admin cluster-admin">
				You can enable SR-IOV on a node by using the following command:
			</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc label node &lt;node_name&gt; feature.node.kubernetes.io/network-sriov.capable="true"</pre><section class="section cluster-admin" id="components-sr-iov-network-devices"><div class="titlepage"><div><div><h3 class="title">25.1.1. Components that manage SR-IOV network devices</h3></div></div></div><p class="cluster-admin cluster-admin">
					The SR-IOV Network Operator creates and manages the components of the SR-IOV stack. It performs the following functions:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Orchestrates discovery and management of SR-IOV network devices
						</li><li class="listitem">
							Generates <code class="literal cluster-admin">NetworkAttachmentDefinition</code> custom resources for the SR-IOV Container Network Interface (CNI)
						</li><li class="listitem">
							Creates and updates the configuration of the SR-IOV network device plugin
						</li><li class="listitem">
							Creates node specific <code class="literal cluster-admin">SriovNetworkNodeState</code> custom resources
						</li><li class="listitem">
							Updates the <code class="literal cluster-admin">spec.interfaces</code> field in each <code class="literal cluster-admin">SriovNetworkNodeState</code> custom resource
						</li></ul></div><p class="cluster-admin cluster-admin">
					The Operator provisions the following components:
				</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">SR-IOV network configuration daemon</span></dt><dd>
								A daemon set that is deployed on worker nodes when the SR-IOV Network Operator starts. The daemon is responsible for discovering and initializing SR-IOV network devices in the cluster.
							</dd><dt><span class="term">SR-IOV Network Operator webhook</span></dt><dd>
								A dynamic admission controller webhook that validates the Operator custom resource and sets appropriate default values for unset fields.
							</dd><dt><span class="term">SR-IOV Network resources injector</span></dt><dd>
								A dynamic admission controller webhook that provides functionality for patching Kubernetes pod specifications with requests and limits for custom network resources such as SR-IOV VFs. The SR-IOV network resources injector adds the <code class="literal cluster-admin">resource</code> field to only the first container in a pod automatically.
							</dd><dt><span class="term">SR-IOV network device plugin</span></dt><dd>
								A device plugin that discovers, advertises, and allocates SR-IOV network virtual function (VF) resources. Device plugins are used in Kubernetes to enable the use of limited resources, typically in physical devices. Device plugins give the Kubernetes scheduler awareness of resource availability, so that the scheduler can schedule pods on nodes with sufficient resources.
							</dd><dt><span class="term">SR-IOV CNI plugin</span></dt><dd>
								A CNI plugin that attaches VF interfaces allocated from the SR-IOV network device plugin directly into a pod.
							</dd><dt><span class="term">SR-IOV InfiniBand CNI plugin</span></dt><dd>
								A CNI plugin that attaches InfiniBand (IB) VF interfaces allocated from the SR-IOV network device plugin directly into a pod.
							</dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						The SR-IOV Network resources injector and SR-IOV Network Operator webhook are enabled by default and can be disabled by editing the <code class="literal cluster-admin">default</code> <code class="literal cluster-admin">SriovOperatorConfig</code> CR. Use caution when disabling the SR-IOV Network Operator Admission Controller webhook. You can disable the webhook under specific circumstances, such as troubleshooting, or if you want to use unsupported devices.
					</p></div></div><section class="section cluster-admin" id="nw-sriov-supported-platforms_about-sriov"><div class="titlepage"><div><div><h4 class="title">25.1.1.1. Supported platforms</h4></div></div></div><p class="cluster-admin cluster-admin">
						The SR-IOV Network Operator is supported on the following platforms:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Bare metal
							</li><li class="listitem">
								Red Hat OpenStack Platform (RHOSP)
							</li></ul></div></section><section class="section cluster-admin" id="supported-devices_about-sriov"><div class="titlepage"><div><div><h4 class="title">25.1.1.2. Supported devices</h4></div></div></div><p class="cluster-admin cluster-admin">
						OpenShift Container Platform supports the following network interface controllers:
					</p><div class="table" id="idm140587137687824"><p class="title"><strong>Table 25.1. Supported network interface controllers</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 40%; " class="col_2"><!--Empty--></col><col style="width: 20%; " class="col_3"><!--Empty--></col><col style="width: 20%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587143397168" scope="col">Manufacturer</th><th align="left" valign="top" id="idm140587143396080" scope="col">Model</th><th align="left" valign="top" id="idm140587143394992" scope="col">Vendor ID</th><th align="left" valign="top" id="idm140587140009168" scope="col">Device ID</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Broadcom
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										BCM57414
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										14e4
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										16d7
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Broadcom
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										BCM57508
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										14e4
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										1750
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Broadcom
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										BCM57504
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										14e4
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										1751
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Intel
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										X710
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										8086
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										1572
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Intel
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										XL710
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										8086
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										1583
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Intel
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										XXV710
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										8086
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										158b
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Intel
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										E810-CQDA2
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										8086
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										1592
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Intel
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										E810-2CQDA2
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										8086
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										1592
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Intel
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										E810-XXVDA2
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										8086
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										159b
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Intel
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										E810-XXVDA4
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										8086
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										1593
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Intel
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										E810-XXVDA4T
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										8086
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										1593
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Mellanox
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										MT27700 Family [ConnectX‑4]
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										15b3
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										1013
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Mellanox
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										MT27710 Family [ConnectX‑4 Lx]
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										15b3
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										1015
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Mellanox
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										MT27800 Family [ConnectX‑5]
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										15b3
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										1017
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Mellanox
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										MT28880 Family [ConnectX‑5 Ex]
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										15b3
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										1019
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Mellanox
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										MT28908 Family [ConnectX‑6]
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										15b3
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										101b
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Mellanox
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										MT2892 Family [ConnectX‑6 Dx]
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										15b3
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										101d
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Mellanox
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										MT2894 Family [ConnectX‑6 Lx]
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										15b3
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										101f
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Mellanox
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										MT42822 BlueField‑2 in ConnectX‑6 NIC mode
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										15b3
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										a2d6
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Pensando <sup>[1]</sup>
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										DSC-25 dual-port 25G distributed services card for ionic driver
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										0x1dd8
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										0x1002
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Pensando <sup>[1]</sup>
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										DSC-100 dual-port 100G distributed services card for ionic driver
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										0x1dd8
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										0x1003
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587143397168"> <p>
										Silicom
									</p>
									 </td><td align="left" valign="top" headers="idm140587143396080"> <p>
										STS Family
									</p>
									 </td><td align="left" valign="top" headers="idm140587143394992"> <p>
										8086
									</p>
									 </td><td align="left" valign="top" headers="idm140587140009168"> <p>
										1591
									</p>
									 </td></tr></tbody></table></div></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
								OpenShift SR-IOV is supported, but you must set a static, Virtual Function (VF) media access control (MAC) address using the SR-IOV CNI config file when using SR-IOV.
							</li></ol></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							For the most up-to-date list of supported cards and compatible OpenShift Container Platform versions available, see <a class="link" href="https://access.redhat.com/articles/6954499">Openshift Single Root I/O Virtualization (SR-IOV) and PTP hardware networks Support Matrix</a>.
						</p></div></div></section><section class="section cluster-admin" id="discover-sr-iov-devices_about-sriov"><div class="titlepage"><div><div><h4 class="title">25.1.1.3. Automated discovery of SR-IOV network devices</h4></div></div></div><p class="cluster-admin cluster-admin">
						The SR-IOV Network Operator searches your cluster for SR-IOV capable network devices on worker nodes. The Operator creates and updates a SriovNetworkNodeState custom resource (CR) for each worker node that provides a compatible SR-IOV network device.
					</p><p class="cluster-admin cluster-admin">
						The CR is assigned the same name as the worker node. The <code class="literal cluster-admin">status.interfaces</code> list provides information about the network devices on a node.
					</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
							Do not modify a <code class="literal cluster-admin">SriovNetworkNodeState</code> object. The Operator creates and manages these resources automatically.
						</p></div></div><section class="section cluster-admin" id="example-sriovnetworknodestate_about-sriov"><div class="titlepage"><div><div><h5 class="title">25.1.1.3.1. Example SriovNetworkNodeState object</h5></div></div></div><p class="cluster-admin cluster-admin">
							The following YAML is an example of a <code class="literal cluster-admin">SriovNetworkNodeState</code> object created by the SR-IOV Network Operator:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>An SriovNetworkNodeState object</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodeState
metadata:
  name: node-25 <span id="CO88-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-sriov-network-operator
  ownerReferences:
  - apiVersion: sriovnetwork.openshift.io/v1
    blockOwnerDeletion: true
    controller: true
    kind: SriovNetworkNodePolicy
    name: default
spec:
  dpConfigVersion: "39824"
status:
  interfaces: <span id="CO88-2"><!--Empty--></span><span class="callout">2</span>
  - deviceID: "1017"
    driver: mlx5_core
    mtu: 1500
    name: ens785f0
    pciAddress: "0000:18:00.0"
    totalvfs: 8
    vendor: 15b3
  - deviceID: "1017"
    driver: mlx5_core
    mtu: 1500
    name: ens785f1
    pciAddress: "0000:18:00.1"
    totalvfs: 8
    vendor: 15b3
  - deviceID: 158b
    driver: i40e
    mtu: 1500
    name: ens817f0
    pciAddress: 0000:81:00.0
    totalvfs: 64
    vendor: "8086"
  - deviceID: 158b
    driver: i40e
    mtu: 1500
    name: ens817f1
    pciAddress: 0000:81:00.1
    totalvfs: 64
    vendor: "8086"
  - deviceID: 158b
    driver: i40e
    mtu: 1500
    name: ens803f0
    pciAddress: 0000:86:00.0
    totalvfs: 64
    vendor: "8086"
  syncStatus: Succeeded</pre>

							</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO88-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The value of the <code class="literal cluster-admin">name</code> field is the same as the name of the worker node.
								</div></dd><dt><a href="#CO88-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The <code class="literal cluster-admin">interfaces</code> stanza includes a list of all of the SR-IOV devices discovered by the Operator on the worker node.
								</div></dd></dl></div></section></section><section class="section cluster-admin" id="example-vf-use-in-pod_about-sriov"><div class="titlepage"><div><div><h4 class="title">25.1.1.4. Example use of a virtual function in a pod</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can run a remote direct memory access (RDMA) or a Data Plane Development Kit (DPDK) application in a pod with SR-IOV VF attached.
					</p><p class="cluster-admin cluster-admin">
						This example shows a pod using a virtual function (VF) in RDMA mode:
					</p><div class="cluster-admin cluster-admin"><p class="title"><strong><code class="literal cluster-admin">Pod</code> spec that uses RDMA mode</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: rdma-app
  annotations:
    k8s.v1.cni.cncf.io/networks: sriov-rdma-mlnx
spec:
  containers:
  - name: testpmd
    image: &lt;RDMA_image&gt;
    imagePullPolicy: IfNotPresent
    securityContext:
      runAsUser: 0
      capabilities:
        add: ["IPC_LOCK","SYS_RESOURCE","NET_RAW"]
    command: ["sleep", "infinity"]</pre>

						</p></div><p class="cluster-admin cluster-admin">
						The following example shows a pod with a VF in DPDK mode:
					</p><div class="cluster-admin cluster-admin"><p class="title"><strong><code class="literal cluster-admin">Pod</code> spec that uses DPDK mode</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: dpdk-app
  annotations:
    k8s.v1.cni.cncf.io/networks: sriov-dpdk-net
spec:
  containers:
  - name: testpmd
    image: &lt;DPDK_image&gt;
    securityContext:
      runAsUser: 0
      capabilities:
        add: ["IPC_LOCK","SYS_RESOURCE","NET_RAW"]
    volumeMounts:
    - mountPath: /dev/hugepages
      name: hugepage
    resources:
      limits:
        memory: "1Gi"
        cpu: "2"
        hugepages-1Gi: "4Gi"
      requests:
        memory: "1Gi"
        cpu: "2"
        hugepages-1Gi: "4Gi"
    command: ["sleep", "infinity"]
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages</pre>

						</p></div></section><section class="section cluster-admin" id="nw-sriov-app-netutil_about-sriov"><div class="titlepage"><div><div><h4 class="title">25.1.1.5. DPDK library for use with container applications</h4></div></div></div><p class="cluster-admin cluster-admin">
						An <a class="link" href="https://github.com/openshift/app-netutil">optional library</a>, <code class="literal cluster-admin">app-netutil</code>, provides several API methods for gathering network information about a pod from within a container running within that pod.
					</p><p class="cluster-admin cluster-admin">
						This library can assist with integrating SR-IOV virtual functions (VFs) in Data Plane Development Kit (DPDK) mode into the container. The library provides both a Golang API and a C API.
					</p><p class="cluster-admin cluster-admin">
						Currently there are three API methods implemented:
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">GetCPUInfo()</code></span></dt><dd>
									This function determines which CPUs are available to the container and returns the list.
								</dd><dt><span class="term"><code class="literal cluster-admin">GetHugepages()</code></span></dt><dd>
									This function determines the amount of huge page memory requested in the <code class="literal cluster-admin">Pod</code> spec for each container and returns the values.
								</dd><dt><span class="term"><code class="literal cluster-admin">GetInterfaces()</code></span></dt><dd>
									This function determines the set of interfaces in the container and returns the list. The return value includes the interface type and type-specific data for each interface.
								</dd></dl></div><p class="cluster-admin cluster-admin">
						The repository for the library includes a sample Dockerfile to build a container image, <code class="literal cluster-admin">dpdk-app-centos</code>. The container image can run one of the following DPDK sample applications, depending on an environment variable in the pod specification: <code class="literal cluster-admin">l2fwd</code>, <code class="literal cluster-admin">l3wd</code> or <code class="literal cluster-admin">testpmd</code>. The container image provides an example of integrating the <code class="literal cluster-admin">app-netutil</code> library into the container image itself. The library can also integrate into an init container. The init container can collect the required data and pass the data to an existing DPDK workload.
					</p></section><section class="section cluster-admin" id="nw-sriov-hugepages_about-sriov"><div class="titlepage"><div><div><h4 class="title">25.1.1.6. Huge pages resource injection for Downward API</h4></div></div></div><p class="cluster-admin cluster-admin">
						When a pod specification includes a resource request or limit for huge pages, the Network Resources Injector automatically adds Downward API fields to the pod specification to provide the huge pages information to the container.
					</p><p class="cluster-admin cluster-admin">
						The Network Resources Injector adds a volume that is named <code class="literal cluster-admin">podnetinfo</code> and is mounted at <code class="literal cluster-admin">/etc/podnetinfo</code> for each container in the pod. The volume uses the Downward API and includes a file for huge pages requests and limits. The file naming convention is as follows:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								<code class="literal cluster-admin">/etc/podnetinfo/hugepages_1G_request_&lt;container-name&gt;</code>
							</li><li class="listitem">
								<code class="literal cluster-admin">/etc/podnetinfo/hugepages_1G_limit_&lt;container-name&gt;</code>
							</li><li class="listitem">
								<code class="literal cluster-admin">/etc/podnetinfo/hugepages_2M_request_&lt;container-name&gt;</code>
							</li><li class="listitem">
								<code class="literal cluster-admin">/etc/podnetinfo/hugepages_2M_limit_&lt;container-name&gt;</code>
							</li></ul></div><p class="cluster-admin cluster-admin">
						The paths specified in the previous list are compatible with the <code class="literal cluster-admin">app-netutil</code> library. By default, the library is configured to search for resource information in the <code class="literal cluster-admin">/etc/podnetinfo</code> directory. If you choose to specify the Downward API path items yourself manually, the <code class="literal cluster-admin">app-netutil</code> library searches for the following paths in addition to the paths in the previous list.
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								<code class="literal cluster-admin">/etc/podnetinfo/hugepages_request</code>
							</li><li class="listitem">
								<code class="literal cluster-admin">/etc/podnetinfo/hugepages_limit</code>
							</li><li class="listitem">
								<code class="literal cluster-admin">/etc/podnetinfo/hugepages_1G_request</code>
							</li><li class="listitem">
								<code class="literal cluster-admin">/etc/podnetinfo/hugepages_1G_limit</code>
							</li><li class="listitem">
								<code class="literal cluster-admin">/etc/podnetinfo/hugepages_2M_request</code>
							</li><li class="listitem">
								<code class="literal cluster-admin">/etc/podnetinfo/hugepages_2M_limit</code>
							</li></ul></div><p class="cluster-admin cluster-admin">
						As with the paths that the Network Resources Injector can create, the paths in the preceding list can optionally end with a <code class="literal cluster-admin">_&lt;container-name&gt;</code> suffix.
					</p></section></section><section class="section _additional-resources" id="configure-multi-networks-additional-resources"><div class="titlepage"><div><div><h3 class="title">25.1.2. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-multi-network-policy">Configuring multi-network policy</a>
						</li></ul></div></section><section class="section cluster-admin" id="about-sriov-next-steps"><div class="titlepage"><div><div><h3 class="title">25.1.3. Next steps</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#installing-sriov-operator">Installing the SR-IOV Network Operator</a>
						</li><li class="listitem">
							Optional: <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-sriov-operator">Configuring the SR-IOV Network Operator</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-sriov-device">Configuring an SR-IOV network device</a>
						</li><li class="listitem">
							If you use OpenShift Virtualization: <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/virtualization/#virt-attaching-vm-to-sriov-network">Connecting a virtual machine to an SR-IOV network</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-sriov-net-attach">Configuring an SR-IOV network attachment</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#add-pod">Adding a pod to an SR-IOV additional network</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="installing-sriov-operator"><div class="titlepage"><div><div><h2 class="title">25.2. Installing the SR-IOV Network Operator</h2></div></div></div><p class="cluster-admin cluster-admin">
				You can install the Single Root I/O Virtualization (SR-IOV) Network Operator on your cluster to manage SR-IOV network devices and network attachments.
			</p><section class="section cluster-admin" id="installing-sr-iov-operator_installing-sriov-operator"><div class="titlepage"><div><div><h3 class="title">25.2.1. Installing SR-IOV Network Operator</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can install the SR-IOV Network Operator by using the OpenShift Container Platform CLI or the web console.
				</p><section class="section cluster-admin" id="install-operator-cli_installing-sriov-operator"><div class="titlepage"><div><div><h4 class="title">25.2.1.1. CLI: Installing the SR-IOV Network Operator</h4></div></div></div><p class="cluster-admin cluster-admin">
						As a cluster administrator, you can install the Operator using the CLI.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								A cluster installed on bare-metal hardware with nodes that have hardware that supports SR-IOV.
							</li><li class="listitem">
								Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								An account with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								To create the <code class="literal cluster-admin">openshift-sriov-network-operator</code> namespace, enter the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-sriov-network-operator
  annotations:
    workload.openshift.io/allowed: management
EOF</pre></li><li class="listitem"><p class="simpara">
								To create an OperatorGroup CR, enter the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: sriov-network-operators
  namespace: openshift-sriov-network-operator
spec:
  targetNamespaces:
  - openshift-sriov-network-operator
EOF</pre></li><li class="listitem"><p class="simpara">
								Subscribe to the SR-IOV Network Operator.
							</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
										Run the following command to get the OpenShift Container Platform major and minor version. It is required for the <code class="literal cluster-admin">channel</code> value in the next step.
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ OC_VERSION=$(oc version -o yaml | grep openshiftVersion | \
    grep -o '[0-9]*[.][0-9]*' | head -1)</pre></li><li class="listitem"><p class="simpara">
										To create a Subscription CR for the SR-IOV Network Operator, enter the following command:
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: sriov-network-operator-subscription
  namespace: openshift-sriov-network-operator
spec:
  channel: "${OC_VERSION}"
  name: sriov-network-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								To verify that the Operator is installed, enter the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get csv -n openshift-sriov-network-operator \
  -o custom-columns=Name:.metadata.name,Phase:.status.phase</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Name                                         Phase
sriov-network-operator.4.13.0-202310121402   Succeeded</pre>

								</p></div></li></ol></div></section><section class="section cluster-admin" id="install-operator-web-console_installing-sriov-operator"><div class="titlepage"><div><div><h4 class="title">25.2.1.2. Web console: Installing the SR-IOV Network Operator</h4></div></div></div><p class="cluster-admin cluster-admin">
						As a cluster administrator, you can install the Operator using the web console.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								A cluster installed on bare-metal hardware with nodes that have hardware that supports SR-IOV.
							</li><li class="listitem">
								Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								An account with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Install the SR-IOV Network Operator:
							</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem">
										In the OpenShift Container Platform web console, click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">OperatorHub</span></strong></span>.
									</li><li class="listitem">
										Select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">SR-IOV Network Operator</span></strong></span> from the list of available Operators, and then click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Install</span></strong></span>.
									</li><li class="listitem">
										On the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Install Operator</span></strong></span> page, under <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Namespace</span></strong></span>, select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operator recommended Namespace</span></strong></span>.
									</li><li class="listitem">
										Click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Install</span></strong></span>.
									</li></ol></div></li><li class="listitem"><p class="simpara">
								Verify that the SR-IOV Network Operator is installed successfully:
							</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem">
										Navigate to the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Operators</span></strong></span> page.
									</li><li class="listitem"><p class="simpara">
										Ensure that <span class="strong strong"><strong><span class="cluster-admin cluster-admin">SR-IOV Network Operator</span></strong></span> is listed in the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">openshift-sriov-network-operator</span></strong></span> project with a <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Status</span></strong></span> of <span class="strong strong"><strong><span class="cluster-admin cluster-admin">InstallSucceeded</span></strong></span>.
									</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
											During installation an Operator might display a <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Failed</span></strong></span> status. If the installation later succeeds with an <span class="strong strong"><strong><span class="cluster-admin cluster-admin">InstallSucceeded</span></strong></span> message, you can ignore the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Failed</span></strong></span> message.
										</p></div></div><p class="cluster-admin cluster-admin">
										If the Operator does not appear as installed, to troubleshoot further:
									</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
												Inspect the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operator Subscriptions</span></strong></span> and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Install Plans</span></strong></span> tabs for any failure or errors under <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Status</span></strong></span>.
											</li><li class="listitem">
												Navigate to the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Workloads</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Pods</span></strong></span> page and check the logs for pods in the <code class="literal cluster-admin">openshift-sriov-network-operator</code> project.
											</li><li class="listitem"><p class="simpara">
												Check the namespace of the YAML file. If the annotation is missing, you can add the annotation <code class="literal cluster-admin">workload.openshift.io/allowed=management</code> to the Operator namespace with the following command:
											</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate ns/openshift-sriov-network-operator workload.openshift.io/allowed=management</pre><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
													For single-node OpenShift clusters, the annotation <code class="literal cluster-admin">workload.openshift.io/allowed=management</code> is required for the namespace.
												</p></div></div></li></ul></div></li></ol></div></li></ol></div></section></section><section class="section cluster-admin" id="installing-sriov-operator-next-steps"><div class="titlepage"><div><div><h3 class="title">25.2.2. Next steps</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Optional: <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-sriov-operator">Configuring the SR-IOV Network Operator</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="configuring-sriov-operator"><div class="titlepage"><div><div><h2 class="title">25.3. Configuring the SR-IOV Network Operator</h2></div></div></div><p class="cluster-admin cluster-admin">
				The Single Root I/O Virtualization (SR-IOV) Network Operator manages the SR-IOV network devices and network attachments in your cluster.
			</p><section class="section cluster-admin" id="nw-sriov-configuring-operator_configuring-sriov-operator"><div class="titlepage"><div><div><h3 class="title">25.3.1. Configuring the SR-IOV Network Operator</h3></div></div></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						Modifying the SR-IOV Network Operator configuration is not normally necessary. The default configuration is recommended for most use cases. Complete the steps to modify the relevant configuration only if the default behavior of the Operator is not compatible with your use case.
					</p></div></div><p class="cluster-admin cluster-admin">
					The SR-IOV Network Operator adds the <code class="literal cluster-admin">SriovOperatorConfig.sriovnetwork.openshift.io</code> CustomResourceDefinition resource. The Operator automatically creates a SriovOperatorConfig custom resource (CR) named <code class="literal cluster-admin">default</code> in the <code class="literal cluster-admin">openshift-sriov-network-operator</code> namespace.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						The <code class="literal cluster-admin">default</code> CR contains the SR-IOV Network Operator configuration for your cluster. To change the Operator configuration, you must modify this CR.
					</p></div></div><section class="section cluster-admin" id="nw-sriov-operator-cr_configuring-sriov-operator"><div class="titlepage"><div><div><h4 class="title">25.3.1.1. SR-IOV Network Operator config custom resource</h4></div></div></div><p class="cluster-admin cluster-admin">
						The fields for the <code class="literal cluster-admin">sriovoperatorconfig</code> custom resource are described in the following table:
					</p><div class="table" id="idm140587124119216"><p class="title"><strong>Table 25.2. SR-IOV Network Operator config custom resource</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587124113120" scope="col">Field</th><th align="left" valign="middle" id="idm140587134699632" scope="col">Type</th><th align="left" valign="middle" id="idm140587134698544" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587124113120"> <p>
										<code class="literal cluster-admin">metadata.name</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587134699632"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587134698544"> <p class="cluster-admin cluster-admin">
										Specifies the name of the SR-IOV Network Operator instance. The default value is <code class="literal cluster-admin">default</code>. Do not set a different value.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587124113120"> <p>
										<code class="literal cluster-admin">metadata.namespace</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587134699632"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587134698544"> <p class="cluster-admin cluster-admin">
										Specifies the namespace of the SR-IOV Network Operator instance. The default value is <code class="literal cluster-admin">openshift-sriov-network-operator</code>. Do not set a different value.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587124113120"> <p>
										<code class="literal cluster-admin">spec.configDaemonNodeSelector</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587134699632"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587134698544"> <p class="cluster-admin cluster-admin">
										Specifies the node selection to control scheduling the SR-IOV Network Config Daemon on selected nodes. By default, this field is not set and the Operator deploys the SR-IOV Network Config daemon set on worker nodes.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587124113120"> <p>
										<code class="literal cluster-admin">spec.disableDrain</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587134699632"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587134698544"> <p class="cluster-admin cluster-admin">
										Specifies whether to disable the node draining process or enable the node draining process when you apply a new policy to configure the NIC on a node. Setting this field to <code class="literal cluster-admin">true</code> facilitates software development and installing OpenShift Container Platform on a single node. By default, this field is not set.
									</p>
									 <p class="cluster-admin cluster-admin">
										For single-node clusters, set this field to <code class="literal cluster-admin">true</code> after installing the Operator. This field must remain set to <code class="literal cluster-admin">true</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587124113120"> <p>
										<code class="literal cluster-admin">spec.enableInjector</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587134699632"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587134698544"> <p class="cluster-admin cluster-admin">
										Specifies whether to enable or disable the Network Resources Injector daemon set. By default, this field is set to <code class="literal cluster-admin">true</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587124113120"> <p>
										<code class="literal cluster-admin">spec.enableOperatorWebhook</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587134699632"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587134698544"> <p class="cluster-admin cluster-admin">
										Specifies whether to enable or disable the Operator Admission Controller webhook daemon set. By default, this field is set to <code class="literal cluster-admin">true</code>.
									</p>
									 </td></tr><tr><td align="left" valign="middle" headers="idm140587124113120"> <p>
										<code class="literal cluster-admin">spec.logLevel</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587134699632"> <p>
										<code class="literal cluster-admin">integer</code>
									</p>
									 </td><td align="left" valign="middle" headers="idm140587134698544"> <p class="cluster-admin cluster-admin">
										Specifies the log verbosity level of the Operator. Set to <code class="literal cluster-admin">0</code> to show only the basic logs. Set to <code class="literal cluster-admin">2</code> to show all the available logs. By default, this field is set to <code class="literal cluster-admin">2</code>.
									</p>
									 </td></tr></tbody></table></div></div></section><section class="section cluster-admin" id="about-network-resource-injector_configuring-sriov-operator"><div class="titlepage"><div><div><h4 class="title">25.3.1.2. About the Network Resources Injector</h4></div></div></div><p class="cluster-admin cluster-admin">
						The Network Resources Injector is a Kubernetes Dynamic Admission Controller application. It provides the following capabilities:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Mutation of resource requests and limits in a pod specification to add an SR-IOV resource name according to an SR-IOV network attachment definition annotation.
							</li><li class="listitem">
								Mutation of a pod specification with a Downward API volume to expose pod annotations, labels, and huge pages requests and limits. Containers that run in the pod can access the exposed information as files under the <code class="literal cluster-admin">/etc/podnetinfo</code> path.
							</li></ul></div><p class="cluster-admin cluster-admin">
						By default, the Network Resources Injector is enabled by the SR-IOV Network Operator and runs as a daemon set on all control plane nodes. The following is an example of Network Resources Injector pods running in a cluster with three control plane nodes:
					</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods -n openshift-sriov-network-operator</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                                      READY   STATUS    RESTARTS   AGE
network-resources-injector-5cz5p          1/1     Running   0          10m
network-resources-injector-dwqpx          1/1     Running   0          10m
network-resources-injector-lktz5          1/1     Running   0          10m</pre>

						</p></div></section><section class="section cluster-admin" id="about-sr-iov-operator-admission-control-webhook_configuring-sriov-operator"><div class="titlepage"><div><div><h4 class="title">25.3.1.3. About the SR-IOV Network Operator admission controller webhook</h4></div></div></div><p class="cluster-admin cluster-admin">
						The SR-IOV Network Operator Admission Controller webhook is a Kubernetes Dynamic Admission Controller application. It provides the following capabilities:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Validation of the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> CR when it is created or updated.
							</li><li class="listitem">
								Mutation of the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> CR by setting the default value for the <code class="literal cluster-admin">priority</code> and <code class="literal cluster-admin">deviceType</code> fields when the CR is created or updated.
							</li></ul></div><p class="cluster-admin cluster-admin">
						By default the SR-IOV Network Operator Admission Controller webhook is enabled by the Operator and runs as a daemon set on all control plane nodes.
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							Use caution when disabling the SR-IOV Network Operator Admission Controller webhook. You can disable the webhook under specific circumstances, such as troubleshooting, or if you want to use unsupported devices. For information about configuring unsupported devices, see <a class="link" href="https://access.redhat.com/articles/7010183">Configuring the SR-IOV Network Operator to use an unsupported NIC</a>.
						</p></div></div><p class="cluster-admin cluster-admin">
						The following is an example of the Operator Admission Controller webhook pods running in a cluster with three control plane nodes:
					</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods -n openshift-sriov-network-operator</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                                      READY   STATUS    RESTARTS   AGE
operator-webhook-9jkw6                    1/1     Running   0          16m
operator-webhook-kbr5p                    1/1     Running   0          16m
operator-webhook-rpfrl                    1/1     Running   0          16m</pre>

						</p></div></section><section class="section cluster-admin" id="about-custom-node-selectors_configuring-sriov-operator"><div class="titlepage"><div><div><h4 class="title">25.3.1.4. About custom node selectors</h4></div></div></div><p class="cluster-admin cluster-admin">
						The SR-IOV Network Config daemon discovers and configures the SR-IOV network devices on cluster nodes. By default, it is deployed to all the <code class="literal cluster-admin">worker</code> nodes in the cluster. You can use node labels to specify on which nodes the SR-IOV Network Config daemon runs.
					</p></section><section class="section cluster-admin" id="disable-enable-network-resource-injector_configuring-sriov-operator"><div class="titlepage"><div><div><h4 class="title">25.3.1.5. Disabling or enabling the Network Resources Injector</h4></div></div></div><p class="cluster-admin cluster-admin">
						To disable or enable the Network Resources Injector, which is enabled by default, complete the following procedure.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li><li class="listitem">
								You must have installed the SR-IOV Network Operator.
							</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								Set the <code class="literal cluster-admin">enableInjector</code> field. Replace <code class="literal cluster-admin">&lt;value&gt;</code> with <code class="literal cluster-admin">false</code> to disable the feature or <code class="literal cluster-admin">true</code> to enable the feature.
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch sriovoperatorconfig default \
  --type=merge -n openshift-sriov-network-operator \
  --patch '{ "spec": { "enableInjector": &lt;value&gt; } }'</pre><div class="admonition tip cluster-admin"><div class="admonition_header">Tip</div><div><p class="cluster-admin cluster-admin">
								You can alternatively apply the following YAML to update the Operator:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovOperatorConfig
metadata:
  name: default
  namespace: openshift-sriov-network-operator
spec:
  enableInjector: &lt;value&gt;</pre></div></div></li></ul></div></section><section class="section cluster-admin" id="disable-enable-sr-iov-operator-admission-control-webhook_configuring-sriov-operator"><div class="titlepage"><div><div><h4 class="title">25.3.1.6. Disabling or enabling the SR-IOV Network Operator admission controller webhook</h4></div></div></div><p class="cluster-admin cluster-admin">
						To disable or enable the admission controller webhook, which is enabled by default, complete the following procedure.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li><li class="listitem">
								You must have installed the SR-IOV Network Operator.
							</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								Set the <code class="literal cluster-admin">enableOperatorWebhook</code> field. Replace <code class="literal cluster-admin">&lt;value&gt;</code> with <code class="literal cluster-admin">false</code> to disable the feature or <code class="literal cluster-admin">true</code> to enable it:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch sriovoperatorconfig default --type=merge \
  -n openshift-sriov-network-operator \
  --patch '{ "spec": { "enableOperatorWebhook": &lt;value&gt; } }'</pre><div class="admonition tip cluster-admin"><div class="admonition_header">Tip</div><div><p class="cluster-admin cluster-admin">
								You can alternatively apply the following YAML to update the Operator:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovOperatorConfig
metadata:
  name: default
  namespace: openshift-sriov-network-operator
spec:
  enableOperatorWebhook: &lt;value&gt;</pre></div></div></li></ul></div></section><section class="section cluster-admin" id="configuring-custom-nodeselector_configuring-sriov-operator"><div class="titlepage"><div><div><h4 class="title">25.3.1.7. Configuring a custom NodeSelector for the SR-IOV Network Config daemon</h4></div></div></div><p class="cluster-admin cluster-admin">
						The SR-IOV Network Config daemon discovers and configures the SR-IOV network devices on cluster nodes. By default, it is deployed to all the <code class="literal cluster-admin">worker</code> nodes in the cluster. You can use node labels to specify on which nodes the SR-IOV Network Config daemon runs.
					</p><p class="cluster-admin cluster-admin">
						To specify the nodes where the SR-IOV Network Config daemon is deployed, complete the following procedure.
					</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
							When you update the <code class="literal cluster-admin">configDaemonNodeSelector</code> field, the SR-IOV Network Config daemon is recreated on each selected node. While the daemon is recreated, cluster users are unable to apply any new SR-IOV Network node policy or create new SR-IOV pods.
						</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								To update the node selector for the operator, enter the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch sriovoperatorconfig default --type=json \
  -n openshift-sriov-network-operator \
  --patch '[{
      "op": "replace",
      "path": "/spec/configDaemonNodeSelector",
      "value": {&lt;node_label&gt;}
    }]'</pre><p class="cluster-admin cluster-admin">
								Replace <code class="literal cluster-admin">&lt;node_label&gt;</code> with a label to apply as in the following example: <code class="literal cluster-admin">"node-role.kubernetes.io/worker": ""</code>.
							</p><div class="admonition tip cluster-admin"><div class="admonition_header">Tip</div><div><p class="cluster-admin cluster-admin">
								You can alternatively apply the following YAML to update the Operator:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovOperatorConfig
metadata:
  name: default
  namespace: openshift-sriov-network-operator
spec:
  configDaemonNodeSelector:
    &lt;node_label&gt;</pre></div></div></li></ul></div></section><section class="section cluster-admin" id="configure-sr-iov-operator-single-node_configuring-sriov-operator"><div class="titlepage"><div><div><h4 class="title">25.3.1.8. Configuring the SR-IOV Network Operator for single node installations</h4></div></div></div><p class="cluster-admin cluster-admin">
						By default, the SR-IOV Network Operator drains workloads from a node before every policy change. The Operator performs this action to ensure that there no workloads using the virtual functions before the reconfiguration.
					</p><p class="cluster-admin cluster-admin">
						For installations on a single node, there are no other nodes to receive the workloads. As a result, the Operator must be configured not to drain the workloads from the single node.
					</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
							After performing the following procedure to disable draining workloads, you must remove any workload that uses an SR-IOV network interface before you change any SR-IOV network node policy.
						</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li><li class="listitem">
								You must have installed the SR-IOV Network Operator.
							</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								To set the <code class="literal cluster-admin">disableDrain</code> field to <code class="literal cluster-admin">true</code>, enter the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch sriovoperatorconfig default --type=merge \
  -n openshift-sriov-network-operator \
  --patch '{ "spec": { "disableDrain": true } }'</pre><div class="admonition tip cluster-admin"><div class="admonition_header">Tip</div><div><p class="cluster-admin cluster-admin">
								You can alternatively apply the following YAML to update the Operator:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovOperatorConfig
metadata:
  name: default
  namespace: openshift-sriov-network-operator
spec:
  disableDrain: true</pre></div></div></li></ul></div></section><section class="section cluster-admin" id="sriov-operator-hosted-control-planes_configuring-sriov-operator"><div class="titlepage"><div><div><h4 class="title">25.3.1.9. Deploying the SR-IOV Operator for hosted control planes</h4></div></div></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
							Hosted control planes is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
						</p><p class="cluster-admin cluster-admin">
							For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
						</p></div></div><p class="_abstract _abstract">
						After you configure and deploy your hosting service cluster, you can create a subscription to the SR-IOV Operator on a hosted cluster. The SR-IOV pod runs on worker machines rather than the control plane.
					</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Prerequisites</strong></p><p>
							You must configure and deploy the hosted cluster on AWS. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.8/html/clusters/cluster_mce_overview#hosting-service-cluster-configure-aws">Configuring the hosting cluster on AWS (Technology Preview)</a>.
						</p></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create a namespace and an Operator group:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Namespace
metadata:
  name: openshift-sriov-network-operator
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: sriov-network-operators
  namespace: openshift-sriov-network-operator
spec:
  targetNamespaces:
  - openshift-sriov-network-operator</pre></li><li class="listitem"><p class="simpara">
								Create a subscription to the SR-IOV Operator:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: sriov-network-operator-subsription
  namespace: openshift-sriov-network-operator
spec:
  channel: "4.13"
  name: sriov-network-operator
  config:
    nodeSelector:
      node-role.kubernetes.io/worker: ""
  source: s/qe-app-registry/redhat-operators
  sourceNamespace: openshift-marketplace</pre></li></ol></div><div class="orderedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								To verify that the SR-IOV Operator is ready, run the following command and view the resulting output:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get csv -n openshift-sriov-network-operator</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                                         DISPLAY                   VERSION               REPLACES                                     PHASE
sriov-network-operator.4.13.0-202211021237   SR-IOV Network Operator   4.13.0-202211021237   sriov-network-operator.4.13.0-202210290517   Succeeded</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								To verify that the SR-IOV pods are deployed, run the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods -n openshift-sriov-network-operator</pre></li></ol></div></section></section><section class="section cluster-admin" id="configuring-sriov-operator-next-steps"><div class="titlepage"><div><div><h3 class="title">25.3.2. Next steps</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-sriov-device">Configuring an SR-IOV network device</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="configuring-sriov-device"><div class="titlepage"><div><div><h2 class="title">25.4. Configuring an SR-IOV network device</h2></div></div></div><p class="cluster-admin cluster-admin">
				You can configure a Single Root I/O Virtualization (SR-IOV) device in your cluster.
			</p><section class="section cluster-admin" id="nw-sriov-networknodepolicy-object_configuring-sriov-device"><div class="titlepage"><div><div><h3 class="title">25.4.1. SR-IOV network node configuration object</h3></div></div></div><p class="cluster-admin cluster-admin">
					You specify the SR-IOV network device configuration for a node by creating an SR-IOV network node policy. The API object for the policy is part of the <code class="literal cluster-admin">sriovnetwork.openshift.io</code> API group.
				</p><p class="cluster-admin cluster-admin">
					The following YAML describes an SR-IOV network node policy:
				</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: &lt;name&gt; <span id="CO89-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-sriov-network-operator <span id="CO89-2"><!--Empty--></span><span class="callout">2</span>
spec:
  resourceName: &lt;sriov_resource_name&gt; <span id="CO89-3"><!--Empty--></span><span class="callout">3</span>
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true" <span id="CO89-4"><!--Empty--></span><span class="callout">4</span>
  priority: &lt;priority&gt; <span id="CO89-5"><!--Empty--></span><span class="callout">5</span>
  mtu: &lt;mtu&gt; <span id="CO89-6"><!--Empty--></span><span class="callout">6</span>
  needVhostNet: false <span id="CO89-7"><!--Empty--></span><span class="callout">7</span>
  numVfs: &lt;num&gt; <span id="CO89-8"><!--Empty--></span><span class="callout">8</span>
  nicSelector: <span id="CO89-9"><!--Empty--></span><span class="callout">9</span>
    vendor: "&lt;vendor_code&gt;" <span id="CO89-10"><!--Empty--></span><span class="callout">10</span>
    deviceID: "&lt;device_id&gt;" <span id="CO89-11"><!--Empty--></span><span class="callout">11</span>
    pfNames: ["&lt;pf_name&gt;", ...] <span id="CO89-12"><!--Empty--></span><span class="callout">12</span>
    rootDevices: ["&lt;pci_bus_id&gt;", ...] <span id="CO89-13"><!--Empty--></span><span class="callout">13</span>
    netFilter: "&lt;filter_string&gt;" <span id="CO89-14"><!--Empty--></span><span class="callout">14</span>
  deviceType: &lt;device_type&gt; <span id="CO89-15"><!--Empty--></span><span class="callout">15</span>
  isRdma: false <span id="CO89-16"><!--Empty--></span><span class="callout">16</span>
  linkType: &lt;link_type&gt; <span id="CO89-17"><!--Empty--></span><span class="callout">17</span>
  eSwitchMode: "switchdev" <span id="CO89-18"><!--Empty--></span><span class="callout">18</span>
  excludeTopology: false <span id="CO89-19"><!--Empty--></span><span class="callout">19</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO89-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The name for the custom resource object.
						</div></dd><dt><a href="#CO89-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							The namespace where the SR-IOV Network Operator is installed.
						</div></dd><dt><a href="#CO89-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							The resource name of the SR-IOV network device plugin. You can create multiple SR-IOV network node policies for a resource name.
						</div><p class="cluster-admin cluster-admin">
							When specifying a name, be sure to use the accepted syntax expression <code class="literal cluster-admin">^[a-zA-Z0-9_]+$</code> in the <code class="literal cluster-admin">resourceName</code>.
						</p></dd><dt><a href="#CO89-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							The node selector specifies the nodes to configure. Only SR-IOV network devices on the selected nodes are configured. The SR-IOV Container Network Interface (CNI) plugin and device plugin are deployed on selected nodes only.
						</div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
								The SR-IOV Network Operator applies node network configuration policies to nodes in sequence. Before applying node network configuration policies, the SR-IOV Network Operator checks if the machine config pool (MCP) for a node is in an unhealthy state such as <code class="literal cluster-admin">Degraded</code> or <code class="literal cluster-admin">Updating</code>. If a node is in an unhealthy MCP, the process of applying node network configuration policies to all targeted nodes in the cluster pauses until the MCP returns to a healthy state.
							</p><p class="cluster-admin cluster-admin">
								To avoid a node in an unhealthy MCP from blocking the application of node network configuration policies to other nodes, including nodes in other MCPs, you must create a separate node network configuration policy for each MCP.
							</p></div></div></dd><dt><a href="#CO89-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Optional: The priority is an integer value between <code class="literal cluster-admin">0</code> and <code class="literal cluster-admin">99</code>. A smaller value receives higher priority. For example, a priority of <code class="literal cluster-admin">10</code> is a higher priority than <code class="literal cluster-admin">99</code>. The default value is <code class="literal cluster-admin">99</code>.
						</div></dd><dt><a href="#CO89-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							Optional: The maximum transmission unit (MTU) of the virtual function. The maximum MTU value can vary for different network interface controller (NIC) models.
						</div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
								If you want to create virtual function on the default network interface, ensure that the MTU is set to a value that matches the cluster MTU.
							</p></div></div></dd><dt><a href="#CO89-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							Optional: Set <code class="literal cluster-admin">needVhostNet</code> to <code class="literal cluster-admin">true</code> to mount the <code class="literal cluster-admin">/dev/vhost-net</code> device in the pod. Use the mounted <code class="literal cluster-admin">/dev/vhost-net</code> device with Data Plane Development Kit (DPDK) to forward traffic to the kernel network stack.
						</div></dd><dt><a href="#CO89-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							The number of the virtual functions (VF) to create for the SR-IOV physical network device. For an Intel network interface controller (NIC), the number of VFs cannot be larger than the total VFs supported by the device. For a Mellanox NIC, the number of VFs cannot be larger than <code class="literal cluster-admin">128</code>.
						</div></dd><dt><a href="#CO89-9"><span class="callout">9</span></a> </dt><dd><div class="para">
							The NIC selector identifies the device for the Operator to configure. You do not have to specify values for all the parameters. It is recommended to identify the network device with enough precision to avoid selecting a device unintentionally.
						</div><p class="cluster-admin cluster-admin">
							If you specify <code class="literal cluster-admin">rootDevices</code>, you must also specify a value for <code class="literal cluster-admin">vendor</code>, <code class="literal cluster-admin">deviceID</code>, or <code class="literal cluster-admin">pfNames</code>. If you specify both <code class="literal cluster-admin">pfNames</code> and <code class="literal cluster-admin">rootDevices</code> at the same time, ensure that they refer to the same device. If you specify a value for <code class="literal cluster-admin">netFilter</code>, then you do not need to specify any other parameter because a network ID is unique.
						</p></dd><dt><a href="#CO89-10"><span class="callout">10</span></a> </dt><dd><div class="para">
							Optional: The vendor hexadecimal code of the SR-IOV network device. The only allowed values are <code class="literal cluster-admin">8086</code> and <code class="literal cluster-admin">15b3</code>.
						</div></dd><dt><a href="#CO89-11"><span class="callout">11</span></a> </dt><dd><div class="para">
							Optional: The device hexadecimal code of the SR-IOV network device. For example, <code class="literal cluster-admin">101b</code> is the device ID for a Mellanox ConnectX-6 device.
						</div></dd><dt><a href="#CO89-12"><span class="callout">12</span></a> </dt><dd><div class="para">
							Optional: An array of one or more physical function (PF) names for the device.
						</div></dd><dt><a href="#CO89-13"><span class="callout">13</span></a> </dt><dd><div class="para">
							Optional: An array of one or more PCI bus addresses for the PF of the device. Provide the address in the following format: <code class="literal cluster-admin">0000:02:00.1</code>.
						</div></dd><dt><a href="#CO89-14"><span class="callout">14</span></a> </dt><dd><div class="para">
							Optional: The platform-specific network filter. The only supported platform is Red Hat OpenStack Platform (RHOSP). Acceptable values use the following format: <code class="literal cluster-admin">openstack/NetworkID:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</code>. Replace <code class="literal cluster-admin">xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</code> with the value from the <code class="literal cluster-admin">/var/config/openstack/latest/network_data.json</code> metadata file.
						</div></dd><dt><a href="#CO89-15"><span class="callout">15</span></a> </dt><dd><div class="para">
							Optional: The driver type for the virtual functions. The only allowed values are <code class="literal cluster-admin">netdevice</code> and <code class="literal cluster-admin">vfio-pci</code>. The default value is <code class="literal cluster-admin">netdevice</code>.
						</div><p class="cluster-admin cluster-admin">
							For a Mellanox NIC to work in DPDK mode on bare metal nodes, use the <code class="literal cluster-admin">netdevice</code> driver type and set <code class="literal cluster-admin">isRdma</code> to <code class="literal cluster-admin">true</code>.
						</p></dd><dt><a href="#CO89-16"><span class="callout">16</span></a> </dt><dd><div class="para">
							Optional: Configures whether to enable remote direct memory access (RDMA) mode. The default value is <code class="literal cluster-admin">false</code>.
						</div><p class="cluster-admin cluster-admin">
							If the <code class="literal cluster-admin">isRdma</code> parameter is set to <code class="literal cluster-admin">true</code>, you can continue to use the RDMA-enabled VF as a normal network device. A device can be used in either mode.
						</p><p class="cluster-admin cluster-admin">
							Set <code class="literal cluster-admin">isRdma</code> to <code class="literal cluster-admin">true</code> and additionally set <code class="literal cluster-admin">needVhostNet</code> to <code class="literal cluster-admin">true</code> to configure a Mellanox NIC for use with Fast Datapath DPDK applications.
						</p></dd><dt><a href="#CO89-17"><span class="callout">17</span></a> </dt><dd><div class="para">
							Optional: The link type for the VFs. The default value is <code class="literal cluster-admin">eth</code> for Ethernet. Change this value to 'ib' for InfiniBand.
						</div><p class="cluster-admin cluster-admin">
							When <code class="literal cluster-admin">linkType</code> is set to <code class="literal cluster-admin">ib</code>, <code class="literal cluster-admin">isRdma</code> is automatically set to <code class="literal cluster-admin">true</code> by the SR-IOV Network Operator webhook. When <code class="literal cluster-admin">linkType</code> is set to <code class="literal cluster-admin">ib</code>, <code class="literal cluster-admin">deviceType</code> should not be set to <code class="literal cluster-admin">vfio-pci</code>.
						</p><p class="cluster-admin cluster-admin">
							Do not set linkType to 'eth' for SriovNetworkNodePolicy, because this can lead to an incorrect number of available devices reported by the device plugin.
						</p></dd><dt><a href="#CO89-18"><span class="callout">18</span></a> </dt><dd><div class="para">
							Optional: To enable hardware offloading, the 'eSwitchMode' field must be set to <code class="literal cluster-admin">"switchdev"</code>.
						</div></dd><dt><a href="#CO89-19"><span class="callout">19</span></a> </dt><dd><div class="para">
							Optional: To exclude advertising an SR-IOV network resource’s NUMA node to the Topology Manager, set the value to <code class="literal cluster-admin">true</code>. The default value is <code class="literal cluster-admin">false</code>.
						</div></dd></dl></div><section class="section cluster-admin" id="sr-iov-network-node-configuration-examples_configuring-sriov-device"><div class="titlepage"><div><div><h4 class="title">25.4.1.1. SR-IOV network node configuration examples</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following example describes the configuration for an InfiniBand device:
					</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example configuration for an InfiniBand device</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-ib-net-1
  namespace: openshift-sriov-network-operator
spec:
  resourceName: ibnic1
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  numVfs: 4
  nicSelector:
    vendor: "15b3"
    deviceID: "101b"
    rootDevices:
      - "0000:19:00.0"
  linkType: ib
  isRdma: true</pre>

						</p></div><p class="cluster-admin cluster-admin">
						The following example describes the configuration for an SR-IOV network device in a RHOSP virtual machine:
					</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example configuration for an SR-IOV device in a virtual machine</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-sriov-net-openstack-1
  namespace: openshift-sriov-network-operator
spec:
  resourceName: sriovnic1
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  numVfs: 1 <span id="CO90-1"><!--Empty--></span><span class="callout">1</span>
  nicSelector:
    vendor: "15b3"
    deviceID: "101b"
    netFilter: "openstack/NetworkID:ea24bd04-8674-4f69-b0ee-fa0b3bd20509" <span id="CO90-2"><!--Empty--></span><span class="callout">2</span></pre>

						</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO90-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The <code class="literal cluster-admin">numVfs</code> field is always set to <code class="literal cluster-admin">1</code> when configuring the node network policy for a virtual machine.
							</div></dd><dt><a href="#CO90-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The <code class="literal cluster-admin">netFilter</code> field must refer to a network ID when the virtual machine is deployed on RHOSP. Valid values for <code class="literal cluster-admin">netFilter</code> are available from an <code class="literal cluster-admin">SriovNetworkNodeState</code> object.
							</div></dd></dl></div></section><section class="section cluster-admin" id="nw-sriov-nic-partitioning_configuring-sriov-device"><div class="titlepage"><div><div><h4 class="title">25.4.1.2. Virtual function (VF) partitioning for SR-IOV devices</h4></div></div></div><p class="cluster-admin cluster-admin">
						In some cases, you might want to split virtual functions (VFs) from the same physical function (PF) into multiple resource pools. For example, you might want some of the VFs to load with the default driver and the remaining VFs load with the <code class="literal cluster-admin">vfio-pci</code> driver. In such a deployment, the <code class="literal cluster-admin">pfNames</code> selector in your SriovNetworkNodePolicy custom resource (CR) can be used to specify a range of VFs for a pool using the following format: <code class="literal cluster-admin">&lt;pfname&gt;#&lt;first_vf&gt;-&lt;last_vf&gt;</code>.
					</p><p class="cluster-admin cluster-admin">
						For example, the following YAML shows the selector for an interface named <code class="literal cluster-admin">netpf0</code> with VF <code class="literal cluster-admin">2</code> through <code class="literal cluster-admin">7</code>:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">pfNames: ["netpf0#2-7"]</pre><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								<code class="literal cluster-admin">netpf0</code> is the PF interface name.
							</li><li class="listitem">
								<code class="literal cluster-admin">2</code> is the first VF index (0-based) that is included in the range.
							</li><li class="listitem">
								<code class="literal cluster-admin">7</code> is the last VF index (0-based) that is included in the range.
							</li></ul></div><p class="cluster-admin cluster-admin">
						You can select VFs from the same PF by using different policy CRs if the following requirements are met:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								The <code class="literal cluster-admin">numVfs</code> value must be identical for policies that select the same PF.
							</li><li class="listitem">
								The VF index must be in the range of <code class="literal cluster-admin">0</code> to <code class="literal cluster-admin">&lt;numVfs&gt;-1</code>. For example, if you have a policy with <code class="literal cluster-admin">numVfs</code> set to <code class="literal cluster-admin">8</code>, then the <code class="literal cluster-admin">&lt;first_vf&gt;</code> value must not be smaller than <code class="literal cluster-admin">0</code>, and the <code class="literal cluster-admin">&lt;last_vf&gt;</code> must not be larger than <code class="literal cluster-admin">7</code>.
							</li><li class="listitem">
								The VFs ranges in different policies must not overlap.
							</li><li class="listitem">
								The <code class="literal cluster-admin">&lt;first_vf&gt;</code> must not be larger than the <code class="literal cluster-admin">&lt;last_vf&gt;</code>.
							</li></ul></div><p class="cluster-admin cluster-admin">
						The following example illustrates NIC partitioning for an SR-IOV device.
					</p><p class="cluster-admin cluster-admin">
						The policy <code class="literal cluster-admin">policy-net-1</code> defines a resource pool <code class="literal cluster-admin">net-1</code> that contains the VF <code class="literal cluster-admin">0</code> of PF <code class="literal cluster-admin">netpf0</code> with the default VF driver. The policy <code class="literal cluster-admin">policy-net-1-dpdk</code> defines a resource pool <code class="literal cluster-admin">net-1-dpdk</code> that contains the VF <code class="literal cluster-admin">8</code> to <code class="literal cluster-admin">15</code> of PF <code class="literal cluster-admin">netpf0</code> with the <code class="literal cluster-admin">vfio</code> VF driver.
					</p><p class="cluster-admin cluster-admin">
						Policy <code class="literal cluster-admin">policy-net-1</code>:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-net-1
  namespace: openshift-sriov-network-operator
spec:
  resourceName: net1
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  numVfs: 16
  nicSelector:
    pfNames: ["netpf0#0-0"]
  deviceType: netdevice</pre><p class="cluster-admin cluster-admin">
						Policy <code class="literal cluster-admin">policy-net-1-dpdk</code>:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-net-1-dpdk
  namespace: openshift-sriov-network-operator
spec:
  resourceName: net1dpdk
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  numVfs: 16
  nicSelector:
    pfNames: ["netpf0#8-15"]
  deviceType: vfio-pci</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Verifying that the interface is successfully partitioned</strong></p><p>
							Confirm that the interface partitioned to virtual functions (VFs) for the SR-IOV device by running the following command.
						</p></div><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ip link show &lt;interface&gt; <span id="CO91-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO91-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Replace <code class="literal cluster-admin">&lt;interface&gt;</code> with the interface that you specified when partitioning to VFs for the SR-IOV device, for example, <code class="literal cluster-admin">ens3f1</code>.
							</div></dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">5: ens3f1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
link/ether 3c:fd:fe:d1:bc:01 brd ff:ff:ff:ff:ff:ff

vf 0     link/ether 5a:e7:88:25:ea:a0 brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off
vf 1     link/ether 3e:1d:36:d7:3d:49 brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off
vf 2     link/ether ce:09:56:97:df:f9 brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off
vf 3     link/ether 5e:91:cf:88:d1:38 brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off
vf 4     link/ether e6:06:a1:96:2f:de brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off</pre>

						</p></div></section></section><section class="section cluster-admin" id="nw-sriov-configuring-device_configuring-sriov-device"><div class="titlepage"><div><div><h3 class="title">25.4.2. Configuring SR-IOV network devices</h3></div></div></div><p class="cluster-admin cluster-admin">
					The SR-IOV Network Operator adds the <code class="literal cluster-admin">SriovNetworkNodePolicy.sriovnetwork.openshift.io</code> CustomResourceDefinition to OpenShift Container Platform. You can configure an SR-IOV network device by creating a SriovNetworkNodePolicy custom resource (CR).
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						When applying the configuration specified in a <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object, the SR-IOV Operator might drain the nodes, and in some cases, reboot nodes.
					</p><p class="cluster-admin cluster-admin">
						It might take several minutes for a configuration change to apply.
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the SR-IOV Network Operator.
						</li><li class="listitem">
							You have enough available nodes in your cluster to handle the evicted workload from drained nodes.
						</li><li class="listitem">
							You have not selected any control plane nodes for SR-IOV network device configuration.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Create an <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object, and then save the YAML in the <code class="literal cluster-admin">&lt;name&gt;-sriov-node-network.yaml</code> file. Replace <code class="literal cluster-admin">&lt;name&gt;</code> with the name for this configuration.
						</li><li class="listitem">
							Optional: Label the SR-IOV capable cluster nodes with <code class="literal cluster-admin">SriovNetworkNodePolicy.Spec.NodeSelector</code> if they are not already labeled. For more information about labeling nodes, see "Understanding how to update labels on nodes".
						</li><li class="listitem"><p class="simpara">
							Create the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f &lt;name&gt;-sriov-node-network.yaml</pre><p class="cluster-admin cluster-admin">
							where <code class="literal cluster-admin">&lt;name&gt;</code> specifies the name for this configuration.
						</p><p class="cluster-admin cluster-admin">
							After applying the configuration update, all the pods in <code class="literal cluster-admin">sriov-network-operator</code> namespace transition to the <code class="literal cluster-admin">Running</code> status.
						</p></li><li class="listitem"><p class="simpara">
							To verify that the SR-IOV network device is configured, enter the following command. Replace <code class="literal cluster-admin">&lt;node_name&gt;</code> with the name of a node with the SR-IOV network device that you just configured.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get sriovnetworknodestates -n openshift-sriov-network-operator &lt;node_name&gt; -o jsonpath='{.status.syncStatus}'</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-nodes-working-updating_nodes-nodes-working">Understanding how to update labels on nodes</a>.
						</li></ul></div></section><section class="section cluster-admin" id="nw-sriov-troubleshooting_configuring-sriov-device"><div class="titlepage"><div><div><h3 class="title">25.4.3. Troubleshooting SR-IOV configuration</h3></div></div></div><p class="cluster-admin cluster-admin">
					After following the procedure to configure an SR-IOV network device, the following sections address some error conditions.
				</p><p class="cluster-admin cluster-admin">
					To display the state of nodes, run the following command:
				</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get sriovnetworknodestates -n openshift-sriov-network-operator &lt;node_name&gt;</pre><p class="cluster-admin cluster-admin">
					where: <code class="literal cluster-admin">&lt;node_name&gt;</code> specifies the name of a node with an SR-IOV network device.
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Error output: Cannot allocate memory</strong></p><p>
						
<pre class="programlisting language-terminal">"lastSyncError": "write /sys/bus/pci/devices/0000:3b:00.1/sriov_numvfs: cannot allocate memory"</pre>

					</p></div><p class="cluster-admin cluster-admin">
					When a node indicates that it cannot allocate memory, check the following items:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Confirm that global SR-IOV settings are enabled in the BIOS for the node.
						</li><li class="listitem">
							Confirm that VT-d is enabled in the BIOS for the node.
						</li></ul></div></section><section class="section cluster-admin" id="cnf-assigning-a-sriov-network-to-a-vrf_configuring-sriov-device"><div class="titlepage"><div><div><h3 class="title">25.4.4. Assigning an SR-IOV network to a VRF</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can assign an SR-IOV network interface to your VRF domain by using the CNI VRF plugin.
				</p><p class="cluster-admin cluster-admin">
					To do this, add the VRF configuration to the optional <code class="literal cluster-admin">metaPlugins</code> parameter of the <code class="literal cluster-admin">SriovNetwork</code> resource.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						Applications that use VRFs need to bind to a specific device. The common usage is to use the <code class="literal cluster-admin">SO_BINDTODEVICE</code> option for a socket. <code class="literal cluster-admin">SO_BINDTODEVICE</code> binds the socket to a device that is specified in the passed interface name, for example, <code class="literal cluster-admin">eth1</code>. To use <code class="literal cluster-admin">SO_BINDTODEVICE</code>, the application must have <code class="literal cluster-admin">CAP_NET_RAW</code> capabilities.
					</p><p class="cluster-admin cluster-admin">
						Using a VRF through the <code class="literal cluster-admin">ip vrf exec</code> command is not supported in OpenShift Container Platform pods. To use VRF, bind applications directly to the VRF interface.
					</p></div></div><section class="section cluster-admin" id="cnf-creating-an-additional-sriov-network-with-vrf-plug-in_configuring-sriov-device"><div class="titlepage"><div><div><h4 class="title">25.4.4.1. Creating an additional SR-IOV network attachment with the CNI VRF plugin</h4></div></div></div><p class="cluster-admin cluster-admin">
						The SR-IOV Network Operator manages additional network definitions. When you specify an additional SR-IOV network to create, the SR-IOV Network Operator creates the <code class="literal cluster-admin">NetworkAttachmentDefinition</code> custom resource (CR) automatically.
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							Do not edit <code class="literal cluster-admin">NetworkAttachmentDefinition</code> custom resources that the SR-IOV Network Operator manages. Doing so might disrupt network traffic on your additional network.
						</p></div></div><p class="cluster-admin cluster-admin">
						To create an additional SR-IOV network attachment with the CNI VRF plugin, perform the following procedure.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Install the OpenShift Container Platform CLI (oc).
							</li><li class="listitem">
								Log in to the OpenShift Container Platform cluster as a user with cluster-admin privileges.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create the <code class="literal cluster-admin">SriovNetwork</code> custom resource (CR) for the additional SR-IOV network attachment and insert the <code class="literal cluster-admin">metaPlugins</code> configuration, as in the following example CR. Save the YAML as the file <code class="literal cluster-admin">sriov-network-attachment.yaml</code>.
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: example-network
  namespace: additional-sriov-network-1
spec:
  ipam: |
    {
      "type": "host-local",
      "subnet": "10.56.217.0/24",
      "rangeStart": "10.56.217.171",
      "rangeEnd": "10.56.217.181",
      "routes": [{
        "dst": "0.0.0.0/0"
      }],
      "gateway": "10.56.217.1"
    }
  vlan: 0
  resourceName: intelnics
  metaPlugins : |
    {
      "type": "vrf", <span id="CO92-1"><!--Empty--></span><span class="callout">1</span>
      "vrfname": "example-vrf-name" <span id="CO92-2"><!--Empty--></span><span class="callout">2</span>
    }</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO92-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										<code class="literal cluster-admin">type</code> must be set to <code class="literal cluster-admin">vrf</code>.
									</div></dd><dt><a href="#CO92-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										<code class="literal cluster-admin">vrfname</code> is the name of the VRF that the interface is assigned to. If it does not exist in the pod, it is created.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create the <code class="literal cluster-admin">SriovNetwork</code> resource:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f sriov-network-attachment.yaml</pre></li></ol></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Verifying that the <code class="literal cluster-admin">NetworkAttachmentDefinition</code> CR is successfully created</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								Confirm that the SR-IOV Network Operator created the <code class="literal cluster-admin">NetworkAttachmentDefinition</code> CR by running the following command.
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get network-attachment-definitions -n &lt;namespace&gt; <span id="CO93-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO93-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal cluster-admin">&lt;namespace&gt;</code> with the namespace that you specified when configuring the network attachment, for example, <code class="literal cluster-admin">additional-sriov-network-1</code>.
									</div></dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                            AGE
additional-sriov-network-1      14m</pre>

								</p></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
									There might be a delay before the SR-IOV Network Operator creates the CR.
								</p></div></div></li></ul></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Verifying that the additional SR-IOV network attachment is successful</strong></p><p>
							To verify that the VRF CNI is correctly configured and the additional SR-IOV network attachment is attached, do the following:
						</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
								Create an SR-IOV network that uses the VRF CNI.
							</li><li class="listitem">
								Assign the network to a pod.
							</li><li class="listitem"><p class="simpara">
								Verify that the pod network attachment is connected to the SR-IOV additional network. Remote shell into the pod and run the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ip vrf show</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Name              Table
-----------------------
red                 10</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Confirm the VRF interface is master of the secondary interface:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ip link</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">...
5: net1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master red state UP mode
...</pre>

								</p></div></li></ol></div></section></section><section class="section cluster-admin" id="nw-sriov-exclude-topology-manager_configuring-sriov-device"><div class="titlepage"><div><div><h3 class="title">25.4.5. Exclude the SR-IOV network topology for NUMA-aware scheduling</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can exclude advertising the Non-Uniform Memory Access (NUMA) node for the SR-IOV network to the Topology Manager for more flexible SR-IOV network deployments during NUMA-aware pod scheduling.
				</p><p class="cluster-admin cluster-admin">
					In some scenarios, it is a priority to maximize CPU and memory resources for a pod on a single NUMA node. By not providing a hint to the Topology Manager about the NUMA node for the pod’s SR-IOV network resource, the Topology Manager can deploy the SR-IOV network resource and the pod CPU and memory resources to different NUMA nodes. This can add to network latency because of the data transfer between NUMA nodes. However, it is acceptable in scenarios when workloads require optimal CPU and memory performance.
				</p><p class="cluster-admin cluster-admin">
					For example, consider a compute node, <code class="literal cluster-admin">compute-1</code>, that features two NUMA nodes: <code class="literal cluster-admin">numa0</code> and <code class="literal cluster-admin">numa1</code>. The SR-IOV-enabled NIC is present on <code class="literal cluster-admin">numa0</code>. The CPUs available for pod scheduling are present on <code class="literal cluster-admin">numa1</code> only. By setting the <code class="literal cluster-admin">excludeTopology</code> specification to <code class="literal cluster-admin">true</code>, the Topology Manager can assign CPU and memory resources for the pod to <code class="literal cluster-admin">numa1</code> and can assign the SR-IOV network resource for the same pod to <code class="literal cluster-admin">numa0</code>. This is only possible when you set the <code class="literal cluster-admin">excludeTopology</code> specification to <code class="literal cluster-admin">true</code>. Otherwise, the Topology Manager attempts to place all resources on the same NUMA node.
				</p><section class="section cluster-admin" id="nw-sriov-configure-exclude-topology-manager_configuring-sriov-device"><div class="titlepage"><div><div><h4 class="title">25.4.5.1. Excluding the SR-IOV network topology for NUMA-aware scheduling</h4></div></div></div><p class="cluster-admin cluster-admin">
						To exclude advertising the SR-IOV network resource’s Non-Uniform Memory Access (NUMA) node to the Topology Manager, you can configure the <code class="literal cluster-admin">excludeTopology</code> specification in the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> custom resource. Use this configuration for more flexible SR-IOV network deployments during NUMA-aware pod scheduling.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								You have configured the CPU Manager policy to <code class="literal cluster-admin">static</code>. For more information about CPU Manager, see the <span class="emphasis"><em><span class="cluster-admin cluster-admin">Additional resources</span></em></span> section.
							</li><li class="listitem">
								You have configured the Topology Manager policy to <code class="literal cluster-admin">single-numa-node</code>.
							</li><li class="listitem">
								You have installed the SR-IOV Network Operator.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> CR:
							</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
										Save the following YAML in the <code class="literal cluster-admin">sriov-network-node-policy.yaml</code> file, replacing values in the YAML to match your environment:
									</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: &lt;policy_name&gt;
  namespace: openshift-sriov-network-operator
spec:
  resourceName: sriovnuma0 <span id="CO94-1"><!--Empty--></span><span class="callout">1</span>
  nodeSelector:
    kubernetes.io/hostname: &lt;node_name&gt;
  numVfs: &lt;number_of_Vfs&gt;
  nicSelector: <span id="CO94-2"><!--Empty--></span><span class="callout">2</span>
    vendor: "&lt;vendor_ID&gt;"
    deviceID: "&lt;device_ID&gt;"
  deviceType: netdevice
  excludeTopology: true <span id="CO94-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO94-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												The resource name of the SR-IOV network device plugin. This YAML uses a sample <code class="literal cluster-admin">resourceName</code> value.
											</div></dd><dt><a href="#CO94-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Identify the device for the Operator to configure by using the NIC selector.
											</div></dd><dt><a href="#CO94-3"><span class="callout">3</span></a> </dt><dd><div class="para">
												To exclude advertising the NUMA node for the SR-IOV network resource to the Topology Manager, set the value to <code class="literal cluster-admin">true</code>. The default value is <code class="literal cluster-admin">false</code>.
											</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
											If multiple <code class="literal cluster-admin">SriovNetworkNodePolicy</code> resources target the same SR-IOV network resource, the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> resources must have the same value as the <code class="literal cluster-admin">excludeTopology</code> specification. Otherwise, the conflicting policy is rejected.
										</p></div></div></li><li class="listitem"><p class="simpara">
										Create the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> resource by running the following command:
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f sriov-network-node-policy.yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">sriovnetworknodepolicy.sriovnetwork.openshift.io/policy-for-numa-0 created</pre>

										</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
								Create the <code class="literal cluster-admin">SriovNetwork</code> CR:
							</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
										Save the following YAML in the <code class="literal cluster-admin">sriov-network.yaml</code> file, replacing values in the YAML to match your environment:
									</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: sriov-numa-0-network <span id="CO95-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-sriov-network-operator
spec:
  resourceName: sriovnuma0 <span id="CO95-2"><!--Empty--></span><span class="callout">2</span>
  networkNamespace: &lt;namespace&gt; <span id="CO95-3"><!--Empty--></span><span class="callout">3</span>
  ipam: |- <span id="CO95-4"><!--Empty--></span><span class="callout">4</span>
    {
      "type": "&lt;ipam_type&gt;",
    }</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO95-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												Replace <code class="literal cluster-admin">sriov-numa-0-network</code> with the name for the SR-IOV network resource.
											</div></dd><dt><a href="#CO95-2"><span class="callout">2</span></a> </dt><dd><div class="para">
												Specify the resource name for the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> CR from the previous step. This YAML uses a sample <code class="literal cluster-admin">resourceName</code> value.
											</div></dd><dt><a href="#CO95-3"><span class="callout">3</span></a> </dt><dd><div class="para">
												Enter the namespace for your SR-IOV network resource.
											</div></dd><dt><a href="#CO95-4"><span class="callout">4</span></a> </dt><dd><div class="para">
												Enter the IP address management configuration for the SR-IOV network.
											</div></dd></dl></div></li><li class="listitem"><p class="simpara">
										Create the <code class="literal cluster-admin">SriovNetwork</code> resource by running the following command:
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f sriov-network.yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">sriovnetwork.sriovnetwork.openshift.io/sriov-numa-0-network created</pre>

										</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
								Create a pod and assign the SR-IOV network resource from the previous step:
							</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
										Save the following YAML in the <code class="literal cluster-admin">sriov-network-pod.yaml</code> file, replacing values in the YAML to match your environment:
									</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Pod
metadata:
  name: &lt;pod_name&gt;
  annotations:
    k8s.v1.cni.cncf.io/networks: |-
      [
        {
          "name": "sriov-numa-0-network", <span id="CO96-1"><!--Empty--></span><span class="callout">1</span>
        }
      ]
spec:
  containers:
  - name: &lt;container_name&gt;
    image: &lt;image&gt;
    imagePullPolicy: IfNotPresent
    command: ["sleep", "infinity"]</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO96-1"><span class="callout">1</span></a> </dt><dd><div class="para">
												This is the name of the <code class="literal cluster-admin">SriovNetwork</code> resource that uses the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> resource.
											</div></dd></dl></div></li><li class="listitem"><p class="simpara">
										Create the <code class="literal cluster-admin">Pod</code> resource by running the following command:
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f sriov-network-pod.yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">pod/example-pod created</pre>

										</p></div></li></ol></div></li></ol></div><div class="orderedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Verify the status of the pod by running the following command, replacing <code class="literal cluster-admin">&lt;pod_name&gt;</code> with the name of the pod:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pod &lt;pod_name&gt;</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                                     READY   STATUS    RESTARTS   AGE
test-deployment-sriov-76cbbf4756-k9v72   1/1     Running   0          45h</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Open a debug session with the target pod to verify that the SR-IOV network resources are deployed to a different node than the memory and CPU resources.
							</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
										Open a debug session with the pod by running the following command, replacing &lt;pod_name&gt; with the target pod name.
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc debug pod/&lt;pod_name&gt;</pre></li><li class="listitem"><p class="simpara">
										Set <code class="literal cluster-admin">/host</code> as the root directory within the debug shell. The debug pod mounts the root file system from the host in <code class="literal cluster-admin">/host</code> within the pod. By changing the root directory to <code class="literal cluster-admin">/host</code>, you can run binaries from the host file system:
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ chroot /host</pre></li><li class="listitem"><p class="simpara">
										View information about the CPU allocation by running the following commands:
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ lscpu | grep NUMA</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">NUMA node(s):                    2
NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,...
NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,...</pre>

										</p></div><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat /proc/self/status | grep Cpus</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">Cpus_allowed:	aa
Cpus_allowed_list:	1,3,5,7</pre>

										</p></div><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat  /sys/class/net/net1/device/numa_node</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
											
<pre class="programlisting language-terminal">0</pre>

										</p></div><p class="cluster-admin cluster-admin">
										In this example, CPUs 1,3,5, and 7 are allocated to <code class="literal cluster-admin">NUMA node1</code> but the SR-IOV network resource can use the NIC in <code class="literal cluster-admin">NUMA node0</code>.
									</p></li></ol></div></li></ol></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							If the <code class="literal cluster-admin">excludeTopology</code> specification is set to <code class="literal cluster-admin">True</code>, it is possible that the required resources exist in the same NUMA node.
						</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#using-cpu-manager">Using CPU Manager</a>
							</li></ul></div></section></section><section class="section cluster-admin" id="configuring-sriov-device-next-steps"><div class="titlepage"><div><div><h3 class="title">25.4.6. Next steps</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-sriov-net-attach">Configuring an SR-IOV network attachment</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="configuring-sriov-net-attach"><div class="titlepage"><div><div><h2 class="title">25.5. Configuring an SR-IOV Ethernet network attachment</h2></div></div></div><p class="cluster-admin cluster-admin">
				You can configure an Ethernet network attachment for an Single Root I/O Virtualization (SR-IOV) device in the cluster.
			</p><section class="section cluster-admin" id="nw-sriov-network-object_configuring-sriov-net-attach"><div class="titlepage"><div><div><h3 class="title">25.5.1. Ethernet device configuration object</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can configure an Ethernet network device by defining an <code class="literal cluster-admin">SriovNetwork</code> object.
				</p><p class="cluster-admin cluster-admin">
					The following YAML describes an <code class="literal cluster-admin">SriovNetwork</code> object:
				</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: &lt;name&gt; <span id="CO97-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-sriov-network-operator <span id="CO97-2"><!--Empty--></span><span class="callout">2</span>
spec:
  resourceName: &lt;sriov_resource_name&gt; <span id="CO97-3"><!--Empty--></span><span class="callout">3</span>
  networkNamespace: &lt;target_namespace&gt; <span id="CO97-4"><!--Empty--></span><span class="callout">4</span>
  vlan: &lt;vlan&gt; <span id="CO97-5"><!--Empty--></span><span class="callout">5</span>
  spoofChk: "&lt;spoof_check&gt;" <span id="CO97-6"><!--Empty--></span><span class="callout">6</span>
  ipam: |- <span id="CO97-7"><!--Empty--></span><span class="callout">7</span>
    {}
  linkState: &lt;link_state&gt; <span id="CO97-8"><!--Empty--></span><span class="callout">8</span>
  maxTxRate: &lt;max_tx_rate&gt; <span id="CO97-9"><!--Empty--></span><span class="callout">9</span>
  minTxRate: &lt;min_tx_rate&gt; <span id="CO97-10"><!--Empty--></span><span class="callout">10</span>
  vlanQoS: &lt;vlan_qos&gt; <span id="CO97-11"><!--Empty--></span><span class="callout">11</span>
  trust: "&lt;trust_vf&gt;" <span id="CO97-12"><!--Empty--></span><span class="callout">12</span>
  capabilities: &lt;capabilities&gt; <span id="CO97-13"><!--Empty--></span><span class="callout">13</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO97-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							A name for the object. The SR-IOV Network Operator creates a <code class="literal cluster-admin">NetworkAttachmentDefinition</code> object with same name.
						</div></dd><dt><a href="#CO97-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							The namespace where the SR-IOV Network Operator is installed.
						</div></dd><dt><a href="#CO97-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							The value for the <code class="literal cluster-admin">spec.resourceName</code> parameter from the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object that defines the SR-IOV hardware for this additional network.
						</div></dd><dt><a href="#CO97-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							The target namespace for the <code class="literal cluster-admin">SriovNetwork</code> object. Only pods in the target namespace can attach to the additional network.
						</div></dd><dt><a href="#CO97-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Optional: A Virtual LAN (VLAN) ID for the additional network. The integer value must be from <code class="literal cluster-admin">0</code> to <code class="literal cluster-admin">4095</code>. The default value is <code class="literal cluster-admin">0</code>.
						</div></dd><dt><a href="#CO97-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							Optional: The spoof check mode of the VF. The allowed values are the strings <code class="literal cluster-admin">"on"</code> and <code class="literal cluster-admin">"off"</code>.
						</div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
								You must enclose the value you specify in quotes or the object is rejected by the SR-IOV Network Operator.
							</p></div></div></dd><dt><a href="#CO97-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							A configuration object for the IPAM CNI plugin as a YAML block scalar. The plugin manages IP address assignment for the attachment definition.
						</div></dd><dt><a href="#CO97-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							Optional: The link state of virtual function (VF). Allowed value are <code class="literal cluster-admin">enable</code>, <code class="literal cluster-admin">disable</code> and <code class="literal cluster-admin">auto</code>.
						</div></dd><dt><a href="#CO97-9"><span class="callout">9</span></a> </dt><dd><div class="para">
							Optional: A maximum transmission rate, in Mbps, for the VF.
						</div></dd><dt><a href="#CO97-10"><span class="callout">10</span></a> </dt><dd><div class="para">
							Optional: A minimum transmission rate, in Mbps, for the VF. This value must be less than or equal to the maximum transmission rate.
						</div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								Intel NICs do not support the <code class="literal cluster-admin">minTxRate</code> parameter. For more information, see <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1772847">BZ#1772847</a>.
							</p></div></div></dd><dt><a href="#CO97-11"><span class="callout">11</span></a> </dt><dd><div class="para">
							Optional: An IEEE 802.1p priority level for the VF. The default value is <code class="literal cluster-admin">0</code>.
						</div></dd><dt><a href="#CO97-12"><span class="callout">12</span></a> </dt><dd><div class="para">
							Optional: The trust mode of the VF. The allowed values are the strings <code class="literal cluster-admin">"on"</code> and <code class="literal cluster-admin">"off"</code>.
						</div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
								You must enclose the value that you specify in quotes, or the SR-IOV Network Operator rejects the object.
							</p></div></div></dd><dt><a href="#CO97-13"><span class="callout">13</span></a> </dt><dd><div class="para">
							Optional: The capabilities to configure for this additional network. You can specify <code class="literal cluster-admin">"{ "ips": true }"</code> to enable IP address support or <code class="literal cluster-admin">"{ "mac": true }"</code> to enable MAC address support.
						</div></dd></dl></div><section class="section cluster-admin" id="nw-multus-ipam-object_configuring-sriov-net-attach"><div class="titlepage"><div><div><h4 class="title">25.5.1.1. Configuration of IP address assignment for an additional network</h4></div></div></div><p class="cluster-admin cluster-admin">
						The IP address management (IPAM) Container Network Interface (CNI) plugin provides IP addresses for other CNI plugins.
					</p><p class="cluster-admin cluster-admin">
						You can use the following IP address assignment types:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Static assignment.
							</li><li class="listitem">
								Dynamic assignment through a DHCP server. The DHCP server you specify must be reachable from the additional network.
							</li><li class="listitem">
								Dynamic assignment through the Whereabouts IPAM CNI plugin.
							</li></ul></div><section class="section cluster-admin" id="nw-multus-static_configuring-sriov-net-attach"><div class="titlepage"><div><div><h5 class="title">25.5.1.1.1. Static IP address assignment configuration</h5></div></div></div><p class="cluster-admin cluster-admin">
							The following table describes the configuration for static IP address assignment:
						</p><div class="table" id="idm140587145235216"><p class="title"><strong>Table 25.3. <code class="literal cluster-admin">ipam</code> static configuration object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587145228512" scope="col">Field</th><th align="left" valign="middle" id="idm140587142492496" scope="col">Type</th><th align="left" valign="middle" id="idm140587142491408" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587145228512"> <p>
											<code class="literal cluster-admin">type</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587142492496"> <p>
											<code class="literal cluster-admin">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587142491408"> <p>
											The IPAM address type. The value <code class="literal cluster-admin">static</code> is required.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587145228512"> <p>
											<code class="literal cluster-admin">addresses</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587142492496"> <p>
											<code class="literal cluster-admin">array</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587142491408"> <p>
											An array of objects specifying IP addresses to assign to the virtual interface. Both IPv4 and IPv6 IP addresses are supported.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587145228512"> <p>
											<code class="literal cluster-admin">routes</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587142492496"> <p>
											<code class="literal cluster-admin">array</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587142491408"> <p>
											An array of objects specifying routes to configure inside the pod.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587145228512"> <p>
											<code class="literal cluster-admin">dns</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587142492496"> <p>
											<code class="literal cluster-admin">array</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587142491408"> <p>
											Optional: An array of objects specifying the DNS configuration.
										</p>
										 </td></tr></tbody></table></div></div><p class="cluster-admin cluster-admin">
							The <code class="literal cluster-admin">addresses</code> array requires objects with the following fields:
						</p><div class="table" id="idm140587145242240"><p class="title"><strong>Table 25.4. <code class="literal cluster-admin">ipam.addresses[]</code> array</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587104611088" scope="col">Field</th><th align="left" valign="middle" id="idm140587104610000" scope="col">Type</th><th align="left" valign="middle" id="idm140587104608912" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587104611088"> <p>
											<code class="literal cluster-admin">address</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587104610000"> <p>
											<code class="literal cluster-admin">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587104608912"> <p>
											An IP address and network prefix that you specify. For example, if you specify <code class="literal cluster-admin">10.10.21.10/24</code>, then the additional network is assigned an IP address of <code class="literal cluster-admin">10.10.21.10</code> and the netmask is <code class="literal cluster-admin">255.255.255.0</code>.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587104611088"> <p>
											<code class="literal cluster-admin">gateway</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587104610000"> <p>
											<code class="literal cluster-admin">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587104608912"> <p>
											The default gateway to route egress network traffic to.
										</p>
										 </td></tr></tbody></table></div></div><div class="table" id="idm140587144406096"><p class="title"><strong>Table 25.5. <code class="literal cluster-admin">ipam.routes[]</code> array</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587139790352" scope="col">Field</th><th align="left" valign="middle" id="idm140587139789264" scope="col">Type</th><th align="left" valign="middle" id="idm140587139788176" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587139790352"> <p>
											<code class="literal cluster-admin">dst</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587139789264"> <p>
											<code class="literal cluster-admin">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587139788176"> <p>
											The IP address range in CIDR format, such as <code class="literal cluster-admin">192.168.17.0/24</code> or <code class="literal cluster-admin">0.0.0.0/0</code> for the default route.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587139790352"> <p>
											<code class="literal cluster-admin">gw</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587139789264"> <p>
											<code class="literal cluster-admin">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587139788176"> <p>
											The gateway where network traffic is routed.
										</p>
										 </td></tr></tbody></table></div></div><div class="table" id="idm140587142941376"><p class="title"><strong>Table 25.6. <code class="literal cluster-admin">ipam.dns</code> object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587133701040" scope="col">Field</th><th align="left" valign="middle" id="idm140587133699952" scope="col">Type</th><th align="left" valign="middle" id="idm140587133698864" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587133701040"> <p>
											<code class="literal cluster-admin">nameservers</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587133699952"> <p>
											<code class="literal cluster-admin">array</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587133698864"> <p>
											An array of one or more IP addresses for to send DNS queries to.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587133701040"> <p>
											<code class="literal cluster-admin">domain</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587133699952"> <p>
											<code class="literal cluster-admin">array</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587133698864"> <p>
											The default domain to append to a hostname. For example, if the domain is set to <code class="literal cluster-admin">example.com</code>, a DNS lookup query for <code class="literal cluster-admin">example-host</code> is rewritten as <code class="literal cluster-admin">example-host.example.com</code>.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587133701040"> <p>
											<code class="literal cluster-admin">search</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587133699952"> <p>
											<code class="literal cluster-admin">array</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587133698864"> <p>
											An array of domain names to append to an unqualified hostname, such as <code class="literal cluster-admin">example-host</code>, during a DNS lookup query.
										</p>
										 </td></tr></tbody></table></div></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Static IP address assignment configuration example</strong></p><p>
								
<pre class="programlisting language-json">{
  "ipam": {
    "type": "static",
      "addresses": [
        {
          "address": "191.168.1.7/24"
        }
      ]
  }
}</pre>

							</p></div></section><section class="section cluster-admin" id="nw-multus-dhcp_configuring-sriov-net-attach"><div class="titlepage"><div><div><h5 class="title">25.5.1.1.2. Dynamic IP address (DHCP) assignment configuration</h5></div></div></div><p class="cluster-admin cluster-admin">
							The following JSON describes the configuration for dynamic IP address address assignment with DHCP.
						</p><div class="admonition important cluster-admin"><div class="admonition_header">Renewal of DHCP leases</div><div><p class="cluster-admin cluster-admin">
								A pod obtains its original DHCP lease when it is created. The lease must be periodically renewed by a minimal DHCP server deployment running on the cluster.
							</p><p class="cluster-admin cluster-admin">
								The SR-IOV Network Operator does not create a DHCP server deployment; The Cluster Network Operator is responsible for creating the minimal DHCP server deployment.
							</p><p class="cluster-admin cluster-admin">
								To trigger the deployment of the DHCP server, you must create a shim network attachment by editing the Cluster Network Operator configuration, as in the following example:
							</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example shim network attachment definition</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  additionalNetworks:
  - name: dhcp-shim
    namespace: default
    type: Raw
    rawCNIConfig: |-
      {
        "name": "dhcp-shim",
        "cniVersion": "0.3.1",
        "type": "bridge",
        "ipam": {
          "type": "dhcp"
        }
      }
  # ...</pre>

								</p></div></div></div><div class="table" id="idm140587141900176"><p class="title"><strong>Table 25.7. <code class="literal cluster-admin">ipam</code> DHCP configuration object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587141893472" scope="col">Field</th><th align="left" valign="middle" id="idm140587141892384" scope="col">Type</th><th align="left" valign="middle" id="idm140587123996016" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587141893472"> <p>
											<code class="literal cluster-admin">type</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587141892384"> <p>
											<code class="literal cluster-admin">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587123996016"> <p>
											The IPAM address type. The value <code class="literal cluster-admin">dhcp</code> is required.
										</p>
										 </td></tr></tbody></table></div></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Dynamic IP address (DHCP) assignment configuration example</strong></p><p>
								
<pre class="programlisting language-json">{
  "ipam": {
    "type": "dhcp"
  }
}</pre>

							</p></div></section><section class="section cluster-admin" id="nw-multus-whereabouts_configuring-sriov-net-attach"><div class="titlepage"><div><div><h5 class="title">25.5.1.1.3. Dynamic IP address assignment configuration with Whereabouts</h5></div></div></div><p class="cluster-admin cluster-admin">
							The Whereabouts CNI plugin allows the dynamic assignment of an IP address to an additional network without the use of a DHCP server.
						</p><p class="cluster-admin cluster-admin">
							The following table describes the configuration for dynamic IP address assignment with Whereabouts:
						</p><div class="table" id="idm140587139435792"><p class="title"><strong>Table 25.8. <code class="literal cluster-admin">ipam</code> whereabouts configuration object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587106181472" scope="col">Field</th><th align="left" valign="middle" id="idm140587106180384" scope="col">Type</th><th align="left" valign="middle" id="idm140587106179296" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587106181472"> <p>
											<code class="literal cluster-admin">type</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587106180384"> <p>
											<code class="literal cluster-admin">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587106179296"> <p>
											The IPAM address type. The value <code class="literal cluster-admin">whereabouts</code> is required.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587106181472"> <p>
											<code class="literal cluster-admin">range</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587106180384"> <p>
											<code class="literal cluster-admin">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587106179296"> <p>
											An IP address and range in CIDR notation. IP addresses are assigned from within this range of addresses.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587106181472"> <p>
											<code class="literal cluster-admin">exclude</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587106180384"> <p>
											<code class="literal cluster-admin">array</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587106179296"> <p>
											Optional: A list of zero or more IP addresses and ranges in CIDR notation. IP addresses within an excluded address range are not assigned.
										</p>
										 </td></tr></tbody></table></div></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Dynamic IP address assignment configuration example that uses Whereabouts</strong></p><p>
								
<pre class="programlisting language-json">{
  "ipam": {
    "type": "whereabouts",
    "range": "192.0.2.192/27",
    "exclude": [
       "192.0.2.192/30",
       "192.0.2.196/32"
    ]
  }
}</pre>

							</p></div></section><section class="section cluster-admin" id="nw-multus-creating-whereabouts-reconciler-daemon-set_configuring-sriov-net-attach"><div class="titlepage"><div><div><h5 class="title">25.5.1.1.4. Creating a Whereabouts reconciler daemon set</h5></div></div></div><p class="cluster-admin cluster-admin">
							The Whereabouts reconciler is responsible for managing dynamic IP address assignments for the pods within a cluster using the Whereabouts IP Address Management (IPAM) solution. It ensures that each pods gets a unique IP address from the specified IP address range. It also handles IP address releases when pods are deleted or scaled down.
						</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								You can also use a <code class="literal cluster-admin">NetworkAttachmentDefinition</code> custom resource for dynamic IP address assignment.
							</p></div></div><p class="cluster-admin cluster-admin">
							The Whereabouts reconciler daemon set is automatically created when you configure an additional network through the Cluster Network Operator. It is not automatically created when you configure an additional network from a YAML manifest.
						</p><p class="cluster-admin cluster-admin">
							To trigger the deployment of the Whereabouts reconciler daemonset, you must manually create a <code class="literal cluster-admin">whereabouts-shim</code> network attachment by editing the Cluster Network Operator custom resource file.
						</p><p class="cluster-admin cluster-admin">
							Use the following procedure to deploy the Whereabouts reconciler daemonset.
						</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
									Edit the <code class="literal cluster-admin">Network.operator.openshift.io</code> custom resource (CR) by running the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit network.operator.openshift.io cluster</pre></li><li class="listitem"><p class="simpara">
									Modify the <code class="literal cluster-admin">additionalNetworks</code> parameter in the CR to add the <code class="literal cluster-admin">whereabouts-shim</code> network attachment definition. For example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  additionalNetworks:
  - name: whereabouts-shim
    namespace: default
    rawCNIConfig: |-
      {
       "name": "whereabouts-shim",
       "cniVersion": "0.3.1",
       "type": "bridge",
       "ipam": {
         "type": "whereabouts"
       }
      }
    type: Raw</pre></li><li class="listitem">
									Save the file and exit the text editor.
								</li><li class="listitem"><p class="simpara">
									Verify that the <code class="literal cluster-admin">whereabouts-reconciler</code> daemon set deployed successfully by running the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get all -n openshift-multus | grep whereabouts-reconciler</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">pod/whereabouts-reconciler-jnp6g 1/1 Running 0 6s
pod/whereabouts-reconciler-k76gg 1/1 Running 0 6s
pod/whereabouts-reconciler-k86t9 1/1 Running 0 6s
pod/whereabouts-reconciler-p4sxw 1/1 Running 0 6s
pod/whereabouts-reconciler-rvfdv 1/1 Running 0 6s
pod/whereabouts-reconciler-svzw9 1/1 Running 0 6s
daemonset.apps/whereabouts-reconciler 6 6 6 6 6 kubernetes.io/os=linux 6s</pre>

									</p></div></li></ol></div></section></section></section><section class="section cluster-admin" id="nw-sriov-network-attachment_configuring-sriov-net-attach"><div class="titlepage"><div><div><h3 class="title">25.5.2. Configuring SR-IOV additional network</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can configure an additional network that uses SR-IOV hardware by creating an <code class="literal cluster-admin">SriovNetwork</code> object. When you create an <code class="literal cluster-admin">SriovNetwork</code> object, the SR-IOV Network Operator automatically creates a <code class="literal cluster-admin">NetworkAttachmentDefinition</code> object.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						Do not modify or delete an <code class="literal cluster-admin">SriovNetwork</code> object if it is attached to any pods in a <code class="literal cluster-admin">running</code> state.
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal cluster-admin">SriovNetwork</code> object, and then save the YAML in the <code class="literal cluster-admin">&lt;name&gt;.yaml</code> file, where <code class="literal cluster-admin">&lt;name&gt;</code> is a name for this additional network. The object specification might resemble the following example:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: attach1
  namespace: openshift-sriov-network-operator
spec:
  resourceName: net1
  networkNamespace: project2
  ipam: |-
    {
      "type": "host-local",
      "subnet": "10.56.217.0/24",
      "rangeStart": "10.56.217.171",
      "rangeEnd": "10.56.217.181",
      "gateway": "10.56.217.1"
    }</pre></li><li class="listitem"><p class="simpara">
							To create the object, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f &lt;name&gt;.yaml</pre><p class="cluster-admin cluster-admin">
							where <code class="literal cluster-admin">&lt;name&gt;</code> specifies the name of the additional network.
						</p></li><li class="listitem"><p class="simpara">
							Optional: To confirm that the <code class="literal cluster-admin">NetworkAttachmentDefinition</code> object that is associated with the <code class="literal cluster-admin">SriovNetwork</code> object that you created in the previous step exists, enter the following command. Replace <code class="literal cluster-admin">&lt;namespace&gt;</code> with the networkNamespace you specified in the <code class="literal cluster-admin">SriovNetwork</code> object.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get net-attach-def -n &lt;namespace&gt;</pre></li></ol></div></section><section class="section cluster-admin" id="configuring-sriov-net-attach-next-steps"><div class="titlepage"><div><div><h3 class="title">25.5.3. Next steps</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#add-pod">Adding a pod to an SR-IOV additional network</a>
						</li></ul></div></section><section class="section _additional-resources" id="configuring-sriov-net-attach-additional-resources"><div class="titlepage"><div><div><h3 class="title">25.5.4. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-sriov-device">Configuring an SR-IOV network device</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="configuring-sriov-ib-attach"><div class="titlepage"><div><div><h2 class="title">25.6. Configuring an SR-IOV InfiniBand network attachment</h2></div></div></div><p class="cluster-admin cluster-admin">
				You can configure an InfiniBand (IB) network attachment for an Single Root I/O Virtualization (SR-IOV) device in the cluster.
			</p><section class="section cluster-admin" id="nw-sriov-ibnetwork-object_configuring-sriov-ib-attach"><div class="titlepage"><div><div><h3 class="title">25.6.1. InfiniBand device configuration object</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can configure an InfiniBand (IB) network device by defining an <code class="literal cluster-admin">SriovIBNetwork</code> object.
				</p><p class="cluster-admin cluster-admin">
					The following YAML describes an <code class="literal cluster-admin">SriovIBNetwork</code> object:
				</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovIBNetwork
metadata:
  name: &lt;name&gt; <span id="CO98-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-sriov-network-operator <span id="CO98-2"><!--Empty--></span><span class="callout">2</span>
spec:
  resourceName: &lt;sriov_resource_name&gt; <span id="CO98-3"><!--Empty--></span><span class="callout">3</span>
  networkNamespace: &lt;target_namespace&gt; <span id="CO98-4"><!--Empty--></span><span class="callout">4</span>
  ipam: |- <span id="CO98-5"><!--Empty--></span><span class="callout">5</span>
    {}
  linkState: &lt;link_state&gt; <span id="CO98-6"><!--Empty--></span><span class="callout">6</span>
  capabilities: &lt;capabilities&gt; <span id="CO98-7"><!--Empty--></span><span class="callout">7</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO98-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							A name for the object. The SR-IOV Network Operator creates a <code class="literal cluster-admin">NetworkAttachmentDefinition</code> object with same name.
						</div></dd><dt><a href="#CO98-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							The namespace where the SR-IOV Operator is installed.
						</div></dd><dt><a href="#CO98-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							The value for the <code class="literal cluster-admin">spec.resourceName</code> parameter from the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object that defines the SR-IOV hardware for this additional network.
						</div></dd><dt><a href="#CO98-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							The target namespace for the <code class="literal cluster-admin">SriovIBNetwork</code> object. Only pods in the target namespace can attach to the network device.
						</div></dd><dt><a href="#CO98-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Optional: A configuration object for the IPAM CNI plugin as a YAML block scalar. The plugin manages IP address assignment for the attachment definition.
						</div></dd><dt><a href="#CO98-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							Optional: The link state of virtual function (VF). Allowed values are <code class="literal cluster-admin">enable</code>, <code class="literal cluster-admin">disable</code> and <code class="literal cluster-admin">auto</code>.
						</div></dd><dt><a href="#CO98-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							Optional: The capabilities to configure for this network. You can specify <code class="literal cluster-admin">"{ "ips": true }"</code> to enable IP address support or <code class="literal cluster-admin">"{ "infinibandGUID": true }"</code> to enable IB Global Unique Identifier (GUID) support.
						</div></dd></dl></div><section class="section cluster-admin" id="nw-multus-ipam-object_configuring-sriov-ib-attach"><div class="titlepage"><div><div><h4 class="title">25.6.1.1. Configuration of IP address assignment for an additional network</h4></div></div></div><p class="cluster-admin cluster-admin">
						The IP address management (IPAM) Container Network Interface (CNI) plugin provides IP addresses for other CNI plugins.
					</p><p class="cluster-admin cluster-admin">
						You can use the following IP address assignment types:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Static assignment.
							</li><li class="listitem">
								Dynamic assignment through a DHCP server. The DHCP server you specify must be reachable from the additional network.
							</li><li class="listitem">
								Dynamic assignment through the Whereabouts IPAM CNI plugin.
							</li></ul></div><section class="section cluster-admin" id="nw-multus-static_configuring-sriov-ib-attach"><div class="titlepage"><div><div><h5 class="title">25.6.1.1.1. Static IP address assignment configuration</h5></div></div></div><p class="cluster-admin cluster-admin">
							The following table describes the configuration for static IP address assignment:
						</p><div class="table" id="idm140587108520032"><p class="title"><strong>Table 25.9. <code class="literal cluster-admin">ipam</code> static configuration object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587108324320" scope="col">Field</th><th align="left" valign="middle" id="idm140587108323232" scope="col">Type</th><th align="left" valign="middle" id="idm140587108322144" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587108324320"> <p>
											<code class="literal cluster-admin">type</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587108323232"> <p>
											<code class="literal cluster-admin">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587108322144"> <p>
											The IPAM address type. The value <code class="literal cluster-admin">static</code> is required.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587108324320"> <p>
											<code class="literal cluster-admin">addresses</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587108323232"> <p>
											<code class="literal cluster-admin">array</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587108322144"> <p>
											An array of objects specifying IP addresses to assign to the virtual interface. Both IPv4 and IPv6 IP addresses are supported.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587108324320"> <p>
											<code class="literal cluster-admin">routes</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587108323232"> <p>
											<code class="literal cluster-admin">array</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587108322144"> <p>
											An array of objects specifying routes to configure inside the pod.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587108324320"> <p>
											<code class="literal cluster-admin">dns</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587108323232"> <p>
											<code class="literal cluster-admin">array</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587108322144"> <p>
											Optional: An array of objects specifying the DNS configuration.
										</p>
										 </td></tr></tbody></table></div></div><p class="cluster-admin cluster-admin">
							The <code class="literal cluster-admin">addresses</code> array requires objects with the following fields:
						</p><div class="table" id="idm140587132769248"><p class="title"><strong>Table 25.10. <code class="literal cluster-admin">ipam.addresses[]</code> array</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587127340896" scope="col">Field</th><th align="left" valign="middle" id="idm140587127339808" scope="col">Type</th><th align="left" valign="middle" id="idm140587127338720" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587127340896"> <p>
											<code class="literal cluster-admin">address</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587127339808"> <p>
											<code class="literal cluster-admin">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587127338720"> <p>
											An IP address and network prefix that you specify. For example, if you specify <code class="literal cluster-admin">10.10.21.10/24</code>, then the additional network is assigned an IP address of <code class="literal cluster-admin">10.10.21.10</code> and the netmask is <code class="literal cluster-admin">255.255.255.0</code>.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587127340896"> <p>
											<code class="literal cluster-admin">gateway</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587127339808"> <p>
											<code class="literal cluster-admin">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587127338720"> <p>
											The default gateway to route egress network traffic to.
										</p>
										 </td></tr></tbody></table></div></div><div class="table" id="idm140587108824224"><p class="title"><strong>Table 25.11. <code class="literal cluster-admin">ipam.routes[]</code> array</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587108817648" scope="col">Field</th><th align="left" valign="middle" id="idm140587108816560" scope="col">Type</th><th align="left" valign="middle" id="idm140587108815472" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587108817648"> <p>
											<code class="literal cluster-admin">dst</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587108816560"> <p>
											<code class="literal cluster-admin">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587108815472"> <p>
											The IP address range in CIDR format, such as <code class="literal cluster-admin">192.168.17.0/24</code> or <code class="literal cluster-admin">0.0.0.0/0</code> for the default route.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587108817648"> <p>
											<code class="literal cluster-admin">gw</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587108816560"> <p>
											<code class="literal cluster-admin">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587108815472"> <p>
											The gateway where network traffic is routed.
										</p>
										 </td></tr></tbody></table></div></div><div class="table" id="idm140587174247392"><p class="title"><strong>Table 25.12. <code class="literal cluster-admin">ipam.dns</code> object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587107386400" scope="col">Field</th><th align="left" valign="middle" id="idm140587107385312" scope="col">Type</th><th align="left" valign="middle" id="idm140587107384224" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587107386400"> <p>
											<code class="literal cluster-admin">nameservers</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587107385312"> <p>
											<code class="literal cluster-admin">array</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587107384224"> <p>
											An array of one or more IP addresses for to send DNS queries to.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587107386400"> <p>
											<code class="literal cluster-admin">domain</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587107385312"> <p>
											<code class="literal cluster-admin">array</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587107384224"> <p>
											The default domain to append to a hostname. For example, if the domain is set to <code class="literal cluster-admin">example.com</code>, a DNS lookup query for <code class="literal cluster-admin">example-host</code> is rewritten as <code class="literal cluster-admin">example-host.example.com</code>.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587107386400"> <p>
											<code class="literal cluster-admin">search</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587107385312"> <p>
											<code class="literal cluster-admin">array</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587107384224"> <p>
											An array of domain names to append to an unqualified hostname, such as <code class="literal cluster-admin">example-host</code>, during a DNS lookup query.
										</p>
										 </td></tr></tbody></table></div></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Static IP address assignment configuration example</strong></p><p>
								
<pre class="programlisting language-json">{
  "ipam": {
    "type": "static",
      "addresses": [
        {
          "address": "191.168.1.7/24"
        }
      ]
  }
}</pre>

							</p></div></section><section class="section cluster-admin" id="nw-multus-dhcp_configuring-sriov-ib-attach"><div class="titlepage"><div><div><h5 class="title">25.6.1.1.2. Dynamic IP address (DHCP) assignment configuration</h5></div></div></div><p class="cluster-admin cluster-admin">
							The following JSON describes the configuration for dynamic IP address address assignment with DHCP.
						</p><div class="admonition important cluster-admin"><div class="admonition_header">Renewal of DHCP leases</div><div><p class="cluster-admin cluster-admin">
								A pod obtains its original DHCP lease when it is created. The lease must be periodically renewed by a minimal DHCP server deployment running on the cluster.
							</p><p class="cluster-admin cluster-admin">
								To trigger the deployment of the DHCP server, you must create a shim network attachment by editing the Cluster Network Operator configuration, as in the following example:
							</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example shim network attachment definition</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  additionalNetworks:
  - name: dhcp-shim
    namespace: default
    type: Raw
    rawCNIConfig: |-
      {
        "name": "dhcp-shim",
        "cniVersion": "0.3.1",
        "type": "bridge",
        "ipam": {
          "type": "dhcp"
        }
      }
  # ...</pre>

								</p></div></div></div><div class="table" id="idm140587104393520"><p class="title"><strong>Table 25.13. <code class="literal cluster-admin">ipam</code> DHCP configuration object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587124294416" scope="col">Field</th><th align="left" valign="middle" id="idm140587124293328" scope="col">Type</th><th align="left" valign="middle" id="idm140587124292240" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587124294416"> <p>
											<code class="literal cluster-admin">type</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587124293328"> <p>
											<code class="literal cluster-admin">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587124292240"> <p>
											The IPAM address type. The value <code class="literal cluster-admin">dhcp</code> is required.
										</p>
										 </td></tr></tbody></table></div></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Dynamic IP address (DHCP) assignment configuration example</strong></p><p>
								
<pre class="programlisting language-json">{
  "ipam": {
    "type": "dhcp"
  }
}</pre>

							</p></div></section><section class="section cluster-admin" id="nw-multus-whereabouts_configuring-sriov-ib-attach"><div class="titlepage"><div><div><h5 class="title">25.6.1.1.3. Dynamic IP address assignment configuration with Whereabouts</h5></div></div></div><p class="cluster-admin cluster-admin">
							The Whereabouts CNI plugin allows the dynamic assignment of an IP address to an additional network without the use of a DHCP server.
						</p><p class="cluster-admin cluster-admin">
							The following table describes the configuration for dynamic IP address assignment with Whereabouts:
						</p><div class="table" id="idm140587105226672"><p class="title"><strong>Table 25.14. <code class="literal cluster-admin">ipam</code> whereabouts configuration object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587132798976" scope="col">Field</th><th align="left" valign="middle" id="idm140587132797888" scope="col">Type</th><th align="left" valign="middle" id="idm140587132796800" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587132798976"> <p>
											<code class="literal cluster-admin">type</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587132797888"> <p>
											<code class="literal cluster-admin">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587132796800"> <p>
											The IPAM address type. The value <code class="literal cluster-admin">whereabouts</code> is required.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587132798976"> <p>
											<code class="literal cluster-admin">range</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587132797888"> <p>
											<code class="literal cluster-admin">string</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587132796800"> <p>
											An IP address and range in CIDR notation. IP addresses are assigned from within this range of addresses.
										</p>
										 </td></tr><tr><td align="left" valign="middle" headers="idm140587132798976"> <p>
											<code class="literal cluster-admin">exclude</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587132797888"> <p>
											<code class="literal cluster-admin">array</code>
										</p>
										 </td><td align="left" valign="middle" headers="idm140587132796800"> <p>
											Optional: A list of zero or more IP addresses and ranges in CIDR notation. IP addresses within an excluded address range are not assigned.
										</p>
										 </td></tr></tbody></table></div></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Dynamic IP address assignment configuration example that uses Whereabouts</strong></p><p>
								
<pre class="programlisting language-json">{
  "ipam": {
    "type": "whereabouts",
    "range": "192.0.2.192/27",
    "exclude": [
       "192.0.2.192/30",
       "192.0.2.196/32"
    ]
  }
}</pre>

							</p></div></section><section class="section cluster-admin" id="nw-multus-creating-whereabouts-reconciler-daemon-set_configuring-sriov-ib-attach"><div class="titlepage"><div><div><h5 class="title">25.6.1.1.4. Creating a Whereabouts reconciler daemon set</h5></div></div></div><p class="cluster-admin cluster-admin">
							The Whereabouts reconciler is responsible for managing dynamic IP address assignments for the pods within a cluster using the Whereabouts IP Address Management (IPAM) solution. It ensures that each pods gets a unique IP address from the specified IP address range. It also handles IP address releases when pods are deleted or scaled down.
						</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								You can also use a <code class="literal cluster-admin">NetworkAttachmentDefinition</code> custom resource for dynamic IP address assignment.
							</p></div></div><p class="cluster-admin cluster-admin">
							The Whereabouts reconciler daemon set is automatically created when you configure an additional network through the Cluster Network Operator. It is not automatically created when you configure an additional network from a YAML manifest.
						</p><p class="cluster-admin cluster-admin">
							To trigger the deployment of the Whereabouts reconciler daemonset, you must manually create a <code class="literal cluster-admin">whereabouts-shim</code> network attachment by editing the Cluster Network Operator custom resource file.
						</p><p class="cluster-admin cluster-admin">
							Use the following procedure to deploy the Whereabouts reconciler daemonset.
						</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
									Edit the <code class="literal cluster-admin">Network.operator.openshift.io</code> custom resource (CR) by running the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit network.operator.openshift.io cluster</pre></li><li class="listitem"><p class="simpara">
									Modify the <code class="literal cluster-admin">additionalNetworks</code> parameter in the CR to add the <code class="literal cluster-admin">whereabouts-shim</code> network attachment definition. For example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  additionalNetworks:
  - name: whereabouts-shim
    namespace: default
    rawCNIConfig: |-
      {
       "name": "whereabouts-shim",
       "cniVersion": "0.3.1",
       "type": "bridge",
       "ipam": {
         "type": "whereabouts"
       }
      }
    type: Raw</pre></li><li class="listitem">
									Save the file and exit the text editor.
								</li><li class="listitem"><p class="simpara">
									Verify that the <code class="literal cluster-admin">whereabouts-reconciler</code> daemon set deployed successfully by running the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get all -n openshift-multus | grep whereabouts-reconciler</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">pod/whereabouts-reconciler-jnp6g 1/1 Running 0 6s
pod/whereabouts-reconciler-k76gg 1/1 Running 0 6s
pod/whereabouts-reconciler-k86t9 1/1 Running 0 6s
pod/whereabouts-reconciler-p4sxw 1/1 Running 0 6s
pod/whereabouts-reconciler-rvfdv 1/1 Running 0 6s
pod/whereabouts-reconciler-svzw9 1/1 Running 0 6s
daemonset.apps/whereabouts-reconciler 6 6 6 6 6 kubernetes.io/os=linux 6s</pre>

									</p></div></li></ol></div></section></section></section><section class="section cluster-admin" id="nw-sriov-network-attachment_configuring-sriov-ib-attach"><div class="titlepage"><div><div><h3 class="title">25.6.2. Configuring SR-IOV additional network</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can configure an additional network that uses SR-IOV hardware by creating an <code class="literal cluster-admin">SriovIBNetwork</code> object. When you create an <code class="literal cluster-admin">SriovIBNetwork</code> object, the SR-IOV Network Operator automatically creates a <code class="literal cluster-admin">NetworkAttachmentDefinition</code> object.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						Do not modify or delete an <code class="literal cluster-admin">SriovIBNetwork</code> object if it is attached to any pods in a <code class="literal cluster-admin">running</code> state.
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal cluster-admin">SriovIBNetwork</code> object, and then save the YAML in the <code class="literal cluster-admin">&lt;name&gt;.yaml</code> file, where <code class="literal cluster-admin">&lt;name&gt;</code> is a name for this additional network. The object specification might resemble the following example:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovIBNetwork
metadata:
  name: attach1
  namespace: openshift-sriov-network-operator
spec:
  resourceName: net1
  networkNamespace: project2
  ipam: |-
    {
      "type": "host-local",
      "subnet": "10.56.217.0/24",
      "rangeStart": "10.56.217.171",
      "rangeEnd": "10.56.217.181",
      "gateway": "10.56.217.1"
    }</pre></li><li class="listitem"><p class="simpara">
							To create the object, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f &lt;name&gt;.yaml</pre><p class="cluster-admin cluster-admin">
							where <code class="literal cluster-admin">&lt;name&gt;</code> specifies the name of the additional network.
						</p></li><li class="listitem"><p class="simpara">
							Optional: To confirm that the <code class="literal cluster-admin">NetworkAttachmentDefinition</code> object that is associated with the <code class="literal cluster-admin">SriovIBNetwork</code> object that you created in the previous step exists, enter the following command. Replace <code class="literal cluster-admin">&lt;namespace&gt;</code> with the networkNamespace you specified in the <code class="literal cluster-admin">SriovIBNetwork</code> object.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get net-attach-def -n &lt;namespace&gt;</pre></li></ol></div></section><section class="section cluster-admin" id="configuring-sriov-ib-attach-next-steps"><div class="titlepage"><div><div><h3 class="title">25.6.3. Next steps</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#add-pod">Adding a pod to an SR-IOV additional network</a>
						</li></ul></div></section><section class="section _additional-resources" id="configuring-sriov-ib-attach-additional-resources"><div class="titlepage"><div><div><h3 class="title">25.6.4. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-sriov-device">Configuring an SR-IOV network device</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="add-pod"><div class="titlepage"><div><div><h2 class="title">25.7. Adding a pod to an SR-IOV additional network</h2></div></div></div><p class="cluster-admin cluster-admin">
				You can add a pod to an existing Single Root I/O Virtualization (SR-IOV) network.
			</p><section class="section cluster-admin" id="nw-sriov-runtime-config_configuring-sr-iov"><div class="titlepage"><div><div><h3 class="title">25.7.1. Runtime configuration for a network attachment</h3></div></div></div><p class="cluster-admin cluster-admin">
					When attaching a pod to an additional network, you can specify a runtime configuration to make specific customizations for the pod. For example, you can request a specific MAC hardware address.
				</p><p class="cluster-admin cluster-admin">
					You specify the runtime configuration by setting an annotation in the pod specification. The annotation key is <code class="literal cluster-admin">k8s.v1.cni.cncf.io/networks</code>, and it accepts a JSON object that describes the runtime configuration.
				</p><section class="section cluster-admin" id="runtime-config-ethernet_configuring-sr-iov"><div class="titlepage"><div><div><h4 class="title">25.7.1.1. Runtime configuration for an Ethernet-based SR-IOV attachment</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following JSON describes the runtime configuration options for an Ethernet-based SR-IOV network attachment.
					</p><pre class="programlisting language-json cluster-admin cluster-admin">[
  {
    "name": "&lt;name&gt;", <span id="CO99-1"><!--Empty--></span><span class="callout">1</span>
    "mac": "&lt;mac_address&gt;", <span id="CO99-2"><!--Empty--></span><span class="callout">2</span>
    "ips": ["&lt;cidr_range&gt;"] <span id="CO99-3"><!--Empty--></span><span class="callout">3</span>
  }
]</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO99-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The name of the SR-IOV network attachment definition CR.
							</div></dd><dt><a href="#CO99-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Optional: The MAC address for the SR-IOV device that is allocated from the resource type defined in the SR-IOV network attachment definition CR. To use this feature, you also must specify <code class="literal cluster-admin">{ "mac": true }</code> in the <code class="literal cluster-admin">SriovNetwork</code> object.
							</div></dd><dt><a href="#CO99-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Optional: IP addresses for the SR-IOV device that is allocated from the resource type defined in the SR-IOV network attachment definition CR. Both IPv4 and IPv6 addresses are supported. To use this feature, you also must specify <code class="literal cluster-admin">{ "ips": true }</code> in the <code class="literal cluster-admin">SriovNetwork</code> object.
							</div></dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example runtime configuration</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: sample-pod
  annotations:
    k8s.v1.cni.cncf.io/networks: |-
      [
        {
          "name": "net1",
          "mac": "20:04:0f:f1:88:01",
          "ips": ["192.168.10.1/24", "2001::1/64"]
        }
      ]
spec:
  containers:
  - name: sample-container
    image: &lt;image&gt;
    imagePullPolicy: IfNotPresent
    command: ["sleep", "infinity"]</pre>

						</p></div></section><section class="section cluster-admin" id="runtime-config-infiniband_configuring-sr-iov"><div class="titlepage"><div><div><h4 class="title">25.7.1.2. Runtime configuration for an InfiniBand-based SR-IOV attachment</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following JSON describes the runtime configuration options for an InfiniBand-based SR-IOV network attachment.
					</p><pre class="programlisting language-json cluster-admin cluster-admin">[
  {
    "name": "&lt;network_attachment&gt;", <span id="CO100-1"><!--Empty--></span><span class="callout">1</span>
    "infiniband-guid": "&lt;guid&gt;", <span id="CO100-2"><!--Empty--></span><span class="callout">2</span>
    "ips": ["&lt;cidr_range&gt;"] <span id="CO100-3"><!--Empty--></span><span class="callout">3</span>
  }
]</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO100-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The name of the SR-IOV network attachment definition CR.
							</div></dd><dt><a href="#CO100-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The InfiniBand GUID for the SR-IOV device. To use this feature, you also must specify <code class="literal cluster-admin">{ "infinibandGUID": true }</code> in the <code class="literal cluster-admin">SriovIBNetwork</code> object.
							</div></dd><dt><a href="#CO100-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								The IP addresses for the SR-IOV device that is allocated from the resource type defined in the SR-IOV network attachment definition CR. Both IPv4 and IPv6 addresses are supported. To use this feature, you also must specify <code class="literal cluster-admin">{ "ips": true }</code> in the <code class="literal cluster-admin">SriovIBNetwork</code> object.
							</div></dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example runtime configuration</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: sample-pod
  annotations:
    k8s.v1.cni.cncf.io/networks: |-
      [
        {
          "name": "ib1",
          "infiniband-guid": "c2:11:22:33:44:55:66:77",
          "ips": ["192.168.10.1/24", "2001::1/64"]
        }
      ]
spec:
  containers:
  - name: sample-container
    image: &lt;image&gt;
    imagePullPolicy: IfNotPresent
    command: ["sleep", "infinity"]</pre>

						</p></div></section></section><section class="section cluster-admin" id="nw-multus-add-pod_configuring-sr-iov"><div class="titlepage"><div><div><h3 class="title">25.7.2. Adding a pod to an additional network</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can add a pod to an additional network. The pod continues to send normal cluster-related network traffic over the default network.
				</p><p class="cluster-admin cluster-admin">
					When a pod is created additional networks are attached to it. However, if a pod already exists, you cannot attach additional networks to it.
				</p><p class="cluster-admin cluster-admin">
					The pod must be in the same namespace as the additional network.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						The SR-IOV Network Resource Injector adds the <code class="literal cluster-admin">resource</code> field to the first container in a pod automatically.
					</p><p class="cluster-admin cluster-admin">
						If you are using an Intel network interface controller (NIC) in Data Plane Development Kit (DPDK) mode, only the first container in your pod is configured to access the NIC. Your SR-IOV additional network is configured for DPDK mode if the <code class="literal cluster-admin">deviceType</code> is set to <code class="literal cluster-admin">vfio-pci</code> in the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object.
					</p><p class="cluster-admin cluster-admin">
						You can work around this issue by either ensuring that the container that needs access to the NIC is the first container defined in the <code class="literal cluster-admin">Pod</code> object or by disabling the Network Resource Injector. For more information, see <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1990953">BZ#1990953</a>.
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in to the cluster.
						</li><li class="listitem">
							Install the SR-IOV Operator.
						</li><li class="listitem">
							Create either an <code class="literal cluster-admin">SriovNetwork</code> object or an <code class="literal cluster-admin">SriovIBNetwork</code> object to attach the pod to.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Add an annotation to the <code class="literal cluster-admin">Pod</code> object. Only one of the following annotation formats can be used:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									To attach an additional network without any customization, add an annotation with the following format. Replace <code class="literal cluster-admin">&lt;network&gt;</code> with the name of the additional network to associate with the pod:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: &lt;network&gt;[,&lt;network&gt;,...] <span id="CO101-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO101-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											To specify more than one additional network, separate each network with a comma. Do not include whitespace between the comma. If you specify the same additional network multiple times, that pod will have multiple network interfaces attached to that network.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To attach an additional network with customizations, add an annotation with the following format:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: |-
      [
        {
          "name": "&lt;network&gt;", <span id="CO102-1"><!--Empty--></span><span class="callout">1</span>
          "namespace": "&lt;namespace&gt;", <span id="CO102-2"><!--Empty--></span><span class="callout">2</span>
          "default-route": ["&lt;default-route&gt;"] <span id="CO102-3"><!--Empty--></span><span class="callout">3</span>
        }
      ]</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO102-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the name of the additional network defined by a <code class="literal cluster-admin">NetworkAttachmentDefinition</code> object.
										</div></dd><dt><a href="#CO102-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Specify the namespace where the <code class="literal cluster-admin">NetworkAttachmentDefinition</code> object is defined.
										</div></dd><dt><a href="#CO102-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Optional: Specify an override for the default route, such as <code class="literal cluster-admin">192.168.17.1</code>.
										</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
							To create the pod, enter the following command. Replace <code class="literal cluster-admin">&lt;name&gt;</code> with the name of the pod.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f &lt;name&gt;.yaml</pre></li><li class="listitem"><p class="simpara">
							Optional: To Confirm that the annotation exists in the <code class="literal cluster-admin">Pod</code> CR, enter the following command, replacing <code class="literal cluster-admin">&lt;name&gt;</code> with the name of the pod.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pod &lt;name&gt; -o yaml</pre><p class="cluster-admin cluster-admin">
							In the following example, the <code class="literal cluster-admin">example-pod</code> pod is attached to the <code class="literal cluster-admin">net1</code> additional network:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pod example-pod -o yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: macvlan-bridge
    k8s.v1.cni.cncf.io/network-status: |- <span id="CO103-1"><!--Empty--></span><span class="callout">1</span>
      [{
          "name": "openshift-sdn",
          "interface": "eth0",
          "ips": [
              "10.128.2.14"
          ],
          "default": true,
          "dns": {}
      },{
          "name": "macvlan-bridge",
          "interface": "net1",
          "ips": [
              "20.2.2.100"
          ],
          "mac": "22:2f:60:a5:f8:00",
          "dns": {}
      }]
  name: example-pod
  namespace: default
spec:
  ...
status:
  ...</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO103-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal cluster-admin">k8s.v1.cni.cncf.io/network-status</code> parameter is a JSON array of objects. Each object describes the status of an additional network attached to the pod. The annotation value is stored as a plain text value.
								</div></dd></dl></div></li></ol></div></section><section class="section cluster-admin" id="nw-sriov-topology-manager_configuring-sr-iov"><div class="titlepage"><div><div><h3 class="title">25.7.3. Creating a non-uniform memory access (NUMA) aligned SR-IOV pod</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can create a NUMA aligned SR-IOV pod by restricting SR-IOV and the CPU resources allocated from the same NUMA node with <code class="literal cluster-admin">restricted</code> or <code class="literal cluster-admin">single-numa-node</code> Topology Manager polices.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You have configured the CPU Manager policy to <code class="literal cluster-admin">static</code>. For more information on CPU Manager, see the "Additional resources" section.
						</li><li class="listitem"><p class="simpara">
							You have configured the Topology Manager policy to <code class="literal cluster-admin">single-numa-node</code>.
						</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								When <code class="literal cluster-admin">single-numa-node</code> is unable to satisfy the request, you can configure the Topology Manager policy to <code class="literal cluster-admin">restricted</code>. For more flexible SR-IOV network resource scheduling, see <span class="emphasis"><em><span class="cluster-admin cluster-admin">Excluding SR-IOV network topology during NUMA-aware scheduling</span></em></span> in the <span class="emphasis"><em><span class="cluster-admin cluster-admin">Additional resources</span></em></span> section.
							</p></div></div></li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create the following SR-IOV pod spec, and then save the YAML in the <code class="literal cluster-admin">&lt;name&gt;-sriov-pod.yaml</code> file. Replace <code class="literal cluster-admin">&lt;name&gt;</code> with a name for this pod.
						</p><p class="cluster-admin cluster-admin">
							The following example shows an SR-IOV pod spec:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Pod
metadata:
  name: sample-pod
  annotations:
    k8s.v1.cni.cncf.io/networks: &lt;name&gt; <span id="CO104-1"><!--Empty--></span><span class="callout">1</span>
spec:
  containers:
  - name: sample-container
    image: &lt;image&gt; <span id="CO104-2"><!--Empty--></span><span class="callout">2</span>
    command: ["sleep", "infinity"]
    resources:
      limits:
        memory: "1Gi" <span id="CO104-3"><!--Empty--></span><span class="callout">3</span>
        cpu: "2" <span id="CO104-4"><!--Empty--></span><span class="callout">4</span>
      requests:
        memory: "1Gi"
        cpu: "2"</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO104-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal cluster-admin">&lt;name&gt;</code> with the name of the SR-IOV network attachment definition CR.
								</div></dd><dt><a href="#CO104-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Replace <code class="literal cluster-admin">&lt;image&gt;</code> with the name of the <code class="literal cluster-admin">sample-pod</code> image.
								</div></dd><dt><a href="#CO104-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									To create the SR-IOV pod with guaranteed QoS, set <code class="literal cluster-admin">memory limits</code> equal to <code class="literal cluster-admin">memory requests</code>.
								</div></dd><dt><a href="#CO104-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									To create the SR-IOV pod with guaranteed QoS, set <code class="literal cluster-admin">cpu limits</code> equals to <code class="literal cluster-admin">cpu requests</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the sample SR-IOV pod by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f &lt;filename&gt; <span id="CO105-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO105-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal cluster-admin">&lt;filename&gt;</code> with the name of the file you created in the previous step.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Confirm that the <code class="literal cluster-admin">sample-pod</code> is configured with guaranteed QoS.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc describe pod sample-pod</pre></li><li class="listitem"><p class="simpara">
							Confirm that the <code class="literal cluster-admin">sample-pod</code> is allocated with exclusive CPUs.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec sample-pod -- cat /sys/fs/cgroup/cpuset/cpuset.cpus</pre></li><li class="listitem"><p class="simpara">
							Confirm that the SR-IOV device and CPUs that are allocated for the <code class="literal cluster-admin">sample-pod</code> are on the same NUMA node.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec sample-pod -- cat /sys/fs/cgroup/cpuset/cpuset.cpus</pre></li></ol></div></section><section class="section cluster-admin" id="nw-openstack-ovs-sr-iov-testpmd-pod_configuring-sr-iov"><div class="titlepage"><div><div><h3 class="title">25.7.4. A test pod template for clusters that use SR-IOV on OpenStack</h3></div></div></div><p class="cluster-admin cluster-admin">
					The following <code class="literal cluster-admin">testpmd</code> pod demonstrates container creation with huge pages, reserved CPUs, and the SR-IOV port.
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>An example <code class="literal cluster-admin">testpmd</code> pod</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: testpmd-sriov
  namespace: mynamespace
  annotations:
    cpu-load-balancing.crio.io: "disable"
    cpu-quota.crio.io: "disable"
# ...
spec:
  containers:
  - name: testpmd
    command: ["sleep", "99999"]
    image: registry.redhat.io/openshift4/dpdk-base-rhel8:v4.9
    securityContext:
      capabilities:
        add: ["IPC_LOCK","SYS_ADMIN"]
      privileged: true
      runAsUser: 0
    resources:
      requests:
        memory: 1000Mi
        hugepages-1Gi: 1Gi
        cpu: '2'
        openshift.io/sriov1: 1
      limits:
        hugepages-1Gi: 1Gi
        cpu: '2'
        memory: 1000Mi
        openshift.io/sriov1: 1
    volumeMounts:
      - mountPath: /dev/hugepages
        name: hugepage
        readOnly: False
  runtimeClassName: performance-cnf-performanceprofile <span id="CO106-1"><!--Empty--></span><span class="callout">1</span>
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages</pre>

					</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO106-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							This example assumes that the name of the performance profile is <code class="literal cluster-admin">cnf-performance profile</code>.
						</div></dd></dl></div></section><section class="section _additional-resources" id="add-pod-additional-resources"><div class="titlepage"><div><div><h3 class="title">25.7.5. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-sriov-device">Configuring an SR-IOV Ethernet network attachment</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-sriov-ib-attach">Configuring an SR-IOV InfiniBand network attachment</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#using-cpu-manager">Using CPU Manager</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-sriov-exclude-topology-manager_configuring-sriov-device">Exclude SR-IOV network topology for NUMA-aware scheduling</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="configuring-interface-level-sysctl-settings-sriov-device"><div class="titlepage"><div><div><h2 class="title">25.8. Configuring interface-level network sysctl settings for SR-IOV networks</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can modify interface-level network sysctls using the tuning Container Network Interface (CNI) meta plugin for a pod connected to a SR-IOV network device.
			</p><section class="section cluster-admin" id="nw-labeling-sriov-enabled-nodes_configuring-sysctl-interface-sriov-device"><div class="titlepage"><div><div><h3 class="title">25.8.1. Labeling nodes with an SR-IOV enabled NIC</h3></div></div></div><p class="cluster-admin cluster-admin">
					If you want to enable SR-IOV on only SR-IOV capable nodes there are a couple of ways to do this:
				</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Install the Node Feature Discovery (NFD) Operator. NFD detects the presence of SR-IOV enabled NICs and labels the nodes with <code class="literal cluster-admin">node.alpha.kubernetes-incubator.io/nfd-network-sriov.capable = true</code>.
						</li><li class="listitem"><p class="simpara">
							Examine the <code class="literal cluster-admin">SriovNetworkNodeState</code> CR for each node. The <code class="literal cluster-admin">interfaces</code> stanza includes a list of all of the SR-IOV devices discovered by the SR-IOV Network Operator on the worker node. Label each node with <code class="literal cluster-admin">feature.node.kubernetes.io/network-sriov.capable: "true"</code> by using the following command:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">$ oc label node &lt;node_name&gt; feature.node.kubernetes.io/network-sriov.capable="true"</pre><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								You can label the nodes with whatever name you want.
							</p></div></div></li></ol></div></section><section class="section cluster-admin" id="nw-setting-one-sysctl-flag_configuring-sysctl-interface-sriov-device"><div class="titlepage"><div><div><h3 class="title">25.8.2. Setting one sysctl flag</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can set interface-level network <code class="literal cluster-admin">sysctl</code> settings for a pod connected to a SR-IOV network device.
				</p><p class="cluster-admin cluster-admin">
					In this example, <code class="literal cluster-admin">net.ipv4.conf.IFNAME.accept_redirects</code> is set to <code class="literal cluster-admin">1</code> on the created virtual interfaces.
				</p><p class="cluster-admin cluster-admin">
					The <code class="literal cluster-admin">sysctl-tuning-test</code> is a namespace used in this example.
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Use the following command to create the <code class="literal cluster-admin">sysctl-tuning-test</code> namespace:
						</p><pre class="screen cluster-admin cluster-admin">$ oc create namespace sysctl-tuning-test</pre></li></ul></div><section class="section cluster-admin" id="nw-basic-example-setting-one-sysctl-flag-node-policy_configuring-sysctl-interface-sriov-device"><div class="titlepage"><div><div><h4 class="title">25.8.2.1. Setting one sysctl flag on nodes with SR-IOV network devices</h4></div></div></div><p class="cluster-admin cluster-admin">
						The SR-IOV Network Operator adds the <code class="literal cluster-admin">SriovNetworkNodePolicy.sriovnetwork.openshift.io</code> custom resource definition (CRD) to OpenShift Container Platform. You can configure an SR-IOV network device by creating a <code class="literal cluster-admin">SriovNetworkNodePolicy</code> custom resource (CR).
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							When applying the configuration specified in a <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object, the SR-IOV Operator might drain and reboot the nodes.
						</p><p class="cluster-admin cluster-admin">
							It can take several minutes for a configuration change to apply.
						</p></div></div><p class="cluster-admin cluster-admin">
						Follow this procedure to create a <code class="literal cluster-admin">SriovNetworkNodePolicy</code> custom resource (CR).
					</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create an <code class="literal cluster-admin">SriovNetworkNodePolicy</code> custom resource (CR). For example, save the following YAML as the file <code class="literal cluster-admin">policyoneflag-sriov-node-network.yaml</code>:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policyoneflag <span id="CO107-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-sriov-network-operator <span id="CO107-2"><!--Empty--></span><span class="callout">2</span>
spec:
  resourceName: policyoneflag <span id="CO107-3"><!--Empty--></span><span class="callout">3</span>
  nodeSelector: <span id="CO107-4"><!--Empty--></span><span class="callout">4</span>
    feature.node.kubernetes.io/network-sriov.capable="true"
  priority: 10 <span id="CO107-5"><!--Empty--></span><span class="callout">5</span>
  numVfs: 5 <span id="CO107-6"><!--Empty--></span><span class="callout">6</span>
  nicSelector: <span id="CO107-7"><!--Empty--></span><span class="callout">7</span>
    pfNames: ["ens5"] <span id="CO107-8"><!--Empty--></span><span class="callout">8</span>
  deviceType: "netdevice" <span id="CO107-9"><!--Empty--></span><span class="callout">9</span>
  isRdma: false <span id="CO107-10"><!--Empty--></span><span class="callout">10</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO107-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The name for the custom resource object.
									</div></dd><dt><a href="#CO107-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The namespace where the SR-IOV Network Operator is installed.
									</div></dd><dt><a href="#CO107-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The resource name of the SR-IOV network device plugin. You can create multiple SR-IOV network node policies for a resource name.
									</div></dd><dt><a href="#CO107-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										The node selector specifies the nodes to configure. Only SR-IOV network devices on the selected nodes are configured. The SR-IOV Container Network Interface (CNI) plugin and device plugin are deployed on selected nodes only.
									</div></dd><dt><a href="#CO107-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										Optional: The priority is an integer value between <code class="literal cluster-admin">0</code> and <code class="literal cluster-admin">99</code>. A smaller value receives higher priority. For example, a priority of <code class="literal cluster-admin">10</code> is a higher priority than <code class="literal cluster-admin">99</code>. The default value is <code class="literal cluster-admin">99</code>.
									</div></dd><dt><a href="#CO107-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										The number of the virtual functions (VFs) to create for the SR-IOV physical network device. For an Intel network interface controller (NIC), the number of VFs cannot be larger than the total VFs supported by the device. For a Mellanox NIC, the number of VFs cannot be larger than <code class="literal cluster-admin">128</code>.
									</div></dd><dt><a href="#CO107-7"><span class="callout">7</span></a> </dt><dd><div class="para">
										The NIC selector identifies the device for the Operator to configure. You do not have to specify values for all the parameters. It is recommended to identify the network device with enough precision to avoid selecting a device unintentionally. If you specify <code class="literal cluster-admin">rootDevices</code>, you must also specify a value for <code class="literal cluster-admin">vendor</code>, <code class="literal cluster-admin">deviceID</code>, or <code class="literal cluster-admin">pfNames</code>. If you specify both <code class="literal cluster-admin">pfNames</code> and <code class="literal cluster-admin">rootDevices</code> at the same time, ensure that they refer to the same device. If you specify a value for <code class="literal cluster-admin">netFilter</code>, then you do not need to specify any other parameter because a network ID is unique.
									</div></dd><dt><a href="#CO107-8"><span class="callout">8</span></a> </dt><dd><div class="para">
										Optional: An array of one or more physical function (PF) names for the device.
									</div></dd><dt><a href="#CO107-9"><span class="callout">9</span></a> </dt><dd><div class="para">
										Optional: The driver type for the virtual functions. The only allowed value is <code class="literal cluster-admin">netdevice</code>. For a Mellanox NIC to work in DPDK mode on bare metal nodes, set <code class="literal cluster-admin">isRdma</code> to <code class="literal cluster-admin">true</code>.
									</div></dd><dt><a href="#CO107-10"><span class="callout">10</span></a> </dt><dd><div class="para">
										Optional: Configures whether to enable remote direct memory access (RDMA) mode. The default value is <code class="literal cluster-admin">false</code>. If the <code class="literal cluster-admin">isRdma</code> parameter is set to <code class="literal cluster-admin">true</code>, you can continue to use the RDMA-enabled VF as a normal network device. A device can be used in either mode. Set <code class="literal cluster-admin">isRdma</code> to <code class="literal cluster-admin">true</code> and additionally set <code class="literal cluster-admin">needVhostNet</code> to <code class="literal cluster-admin">true</code> to configure a Mellanox NIC for use with Fast Datapath DPDK applications.
									</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
									The <code class="literal cluster-admin">vfio-pci</code> driver type is not supported.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Create the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f policyoneflag-sriov-node-network.yaml</pre><p class="cluster-admin cluster-admin">
								After applying the configuration update, all the pods in <code class="literal cluster-admin">sriov-network-operator</code> namespace change to the <code class="literal cluster-admin">Running</code> status.
							</p></li><li class="listitem"><p class="simpara">
								To verify that the SR-IOV network device is configured, enter the following command. Replace <code class="literal cluster-admin">&lt;node_name&gt;</code> with the name of a node with the SR-IOV network device that you just configured.
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get sriovnetworknodestates -n openshift-sriov-network-operator &lt;node_name&gt; -o jsonpath='{.status.syncStatus}'</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Succeeded</pre>

								</p></div></li></ol></div></section><section class="section cluster-admin" id="configuring-sysctl-on-sriov-network_configuring-sysctl-interface-sriov-device"><div class="titlepage"><div><div><h4 class="title">25.8.2.2. Configuring sysctl on a SR-IOV network</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can set interface specific <code class="literal cluster-admin">sysctl</code> settings on virtual interfaces created by SR-IOV by adding the tuning configuration to the optional <code class="literal cluster-admin">metaPlugins</code> parameter of the <code class="literal cluster-admin">SriovNetwork</code> resource.
					</p><p class="cluster-admin cluster-admin">
						The SR-IOV Network Operator manages additional network definitions. When you specify an additional SR-IOV network to create, the SR-IOV Network Operator creates the <code class="literal cluster-admin">NetworkAttachmentDefinition</code> custom resource (CR) automatically.
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							Do not edit <code class="literal cluster-admin">NetworkAttachmentDefinition</code> custom resources that the SR-IOV Network Operator manages. Doing so might disrupt network traffic on your additional network.
						</p></div></div><p class="cluster-admin cluster-admin">
						To change the interface-level network <code class="literal cluster-admin">net.ipv4.conf.IFNAME.accept_redirects</code> <code class="literal cluster-admin">sysctl</code> settings, create an additional SR-IOV network with the Container Network Interface (CNI) tuning plugin.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Install the OpenShift Container Platform CLI (oc).
							</li><li class="listitem">
								Log in to the OpenShift Container Platform cluster as a user with cluster-admin privileges.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create the <code class="literal cluster-admin">SriovNetwork</code> custom resource (CR) for the additional SR-IOV network attachment and insert the <code class="literal cluster-admin">metaPlugins</code> configuration, as in the following example CR. Save the YAML as the file <code class="literal cluster-admin">sriov-network-interface-sysctl.yaml</code>.
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: onevalidflag <span id="CO108-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-sriov-network-operator <span id="CO108-2"><!--Empty--></span><span class="callout">2</span>
spec:
  resourceName: policyoneflag <span id="CO108-3"><!--Empty--></span><span class="callout">3</span>
  networkNamespace: sysctl-tuning-test <span id="CO108-4"><!--Empty--></span><span class="callout">4</span>
  ipam: '{ "type": "static" }' <span id="CO108-5"><!--Empty--></span><span class="callout">5</span>
  capabilities: '{ "mac": true, "ips": true }' <span id="CO108-6"><!--Empty--></span><span class="callout">6</span>
  metaPlugins : | <span id="CO108-7"><!--Empty--></span><span class="callout">7</span>
    {
      "type": "tuning",
      "capabilities":{
        "mac":true
      },
      "sysctl":{
         "net.ipv4.conf.IFNAME.accept_redirects": "1"
      }
    }</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO108-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										A name for the object. The SR-IOV Network Operator creates a NetworkAttachmentDefinition object with same name.
									</div></dd><dt><a href="#CO108-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The namespace where the SR-IOV Network Operator is installed.
									</div></dd><dt><a href="#CO108-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The value for the <code class="literal cluster-admin">spec.resourceName</code> parameter from the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object that defines the SR-IOV hardware for this additional network.
									</div></dd><dt><a href="#CO108-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										The target namespace for the <code class="literal cluster-admin">SriovNetwork</code> object. Only pods in the target namespace can attach to the additional network.
									</div></dd><dt><a href="#CO108-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										A configuration object for the IPAM CNI plugin as a YAML block scalar. The plugin manages IP address assignment for the attachment definition.
									</div></dd><dt><a href="#CO108-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										Optional: Set capabilities for the additional network. You can specify <code class="literal cluster-admin">"{ "ips": true }"</code> to enable IP address support or <code class="literal cluster-admin">"{ "mac": true }"</code> to enable MAC address support.
									</div></dd><dt><a href="#CO108-7"><span class="callout">7</span></a> </dt><dd><div class="para">
										Optional: The metaPlugins parameter is used to add additional capabilities to the device. In this use case set the <code class="literal cluster-admin">type</code> field to <code class="literal cluster-admin">tuning</code>. Specify the interface-level network <code class="literal cluster-admin">sysctl</code> you want to set in the <code class="literal cluster-admin">sysctl</code> field.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create the <code class="literal cluster-admin">SriovNetwork</code> resource:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f sriov-network-interface-sysctl.yaml</pre></li></ol></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Verifying that the <code class="literal cluster-admin">NetworkAttachmentDefinition</code> CR is successfully created</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								Confirm that the SR-IOV Network Operator created the <code class="literal cluster-admin">NetworkAttachmentDefinition</code> CR by running the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get network-attachment-definitions -n &lt;namespace&gt; <span id="CO109-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO109-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal cluster-admin">&lt;namespace&gt;</code> with the value for <code class="literal cluster-admin">networkNamespace</code> that you specified in the <code class="literal cluster-admin">SriovNetwork</code> object. For example, <code class="literal cluster-admin">sysctl-tuning-test</code>.
									</div></dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                                  AGE
onevalidflag                          14m</pre>

								</p></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
									There might be a delay before the SR-IOV Network Operator creates the CR.
								</p></div></div></li></ul></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Verifying that the additional SR-IOV network attachment is successful</strong></p><p>
							To verify that the tuning CNI is correctly configured and the additional SR-IOV network attachment is attached, do the following:
						</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create a <code class="literal cluster-admin">Pod</code> CR. Save the following YAML as the file <code class="literal cluster-admin">examplepod.yaml</code>:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Pod
metadata:
  name: tunepod
  namespace: sysctl-tuning-test
  annotations:
    k8s.v1.cni.cncf.io/networks: |-
      [
        {
          "name": "onevalidflag",  <span id="CO110-1"><!--Empty--></span><span class="callout">1</span>
          "mac": "0a:56:0a:83:04:0c", <span id="CO110-2"><!--Empty--></span><span class="callout">2</span>
          "ips": ["10.100.100.200/24"] <span id="CO110-3"><!--Empty--></span><span class="callout">3</span>
       }
      ]
spec:
  containers:
  - name: podexample
    image: centos
    command: ["/bin/bash", "-c", "sleep INF"]
    securityContext:
      runAsUser: 2000
      runAsGroup: 3000
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
  securityContext:
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO110-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The name of the SR-IOV network attachment definition CR.
									</div></dd><dt><a href="#CO110-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Optional: The MAC address for the SR-IOV device that is allocated from the resource type defined in the SR-IOV network attachment definition CR. To use this feature, you also must specify <code class="literal cluster-admin">{ "mac": true }</code> in the SriovNetwork object.
									</div></dd><dt><a href="#CO110-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Optional: IP addresses for the SR-IOV device that are allocated from the resource type defined in the SR-IOV network attachment definition CR. Both IPv4 and IPv6 addresses are supported. To use this feature, you also must specify <code class="literal cluster-admin">{ "ips": true }</code> in the <code class="literal cluster-admin">SriovNetwork</code> object.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create the <code class="literal cluster-admin">Pod</code> CR:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f examplepod.yaml</pre></li><li class="listitem"><p class="simpara">
								Verify that the pod is created by running the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pod -n sysctl-tuning-test</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME      READY   STATUS    RESTARTS   AGE
tunepod   1/1     Running   0          47s</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Log in to the pod by running the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc rsh -n sysctl-tuning-test tunepod</pre></li><li class="listitem"><p class="simpara">
								Verify the values of the configured sysctl flag. Find the value <code class="literal cluster-admin">net.ipv4.conf.IFNAME.accept_redirects</code> by running the following command::
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ sysctl net.ipv4.conf.net1.accept_redirects</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">net.ipv4.conf.net1.accept_redirects = 1</pre>

								</p></div></li></ol></div></section></section><section class="section cluster-admin" id="nw-configure-sysctl-settings-flag-bonded_configuring-sysctl-interface-sriov-device"><div class="titlepage"><div><div><h3 class="title">25.8.3. Configuring sysctl settings for pods associated with bonded SR-IOV interface flag</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can set interface-level network <code class="literal cluster-admin">sysctl</code> settings for a pod connected to a bonded SR-IOV network device.
				</p><p class="cluster-admin cluster-admin">
					In this example, the specific network interface-level <code class="literal cluster-admin">sysctl</code> settings that can be configured are set on the bonded interface.
				</p><p class="cluster-admin cluster-admin">
					The <code class="literal cluster-admin">sysctl-tuning-test</code> is a namespace used in this example.
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Use the following command to create the <code class="literal cluster-admin">sysctl-tuning-test</code> namespace:
						</p><pre class="screen cluster-admin cluster-admin">$ oc create namespace sysctl-tuning-test</pre></li></ul></div><section class="section cluster-admin" id="nw-setting-all-sysctls-flag-node-policy-bonded_configuring-sysctl-interface-sriov-device"><div class="titlepage"><div><div><h4 class="title">25.8.3.1. Setting all sysctl flag on nodes with bonded SR-IOV network devices</h4></div></div></div><p class="cluster-admin cluster-admin">
						The SR-IOV Network Operator adds the <code class="literal cluster-admin">SriovNetworkNodePolicy.sriovnetwork.openshift.io</code> custom resource definition (CRD) to OpenShift Container Platform. You can configure an SR-IOV network device by creating a <code class="literal cluster-admin">SriovNetworkNodePolicy</code> custom resource (CR).
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							When applying the configuration specified in a SriovNetworkNodePolicy object, the SR-IOV Operator might drain the nodes, and in some cases, reboot nodes.
						</p><p class="cluster-admin cluster-admin">
							It might take several minutes for a configuration change to apply.
						</p></div></div><p class="cluster-admin cluster-admin">
						Follow this procedure to create a <code class="literal cluster-admin">SriovNetworkNodePolicy</code> custom resource (CR).
					</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create an <code class="literal cluster-admin">SriovNetworkNodePolicy</code> custom resource (CR). Save the following YAML as the file <code class="literal cluster-admin">policyallflags-sriov-node-network.yaml</code>. Replace <code class="literal cluster-admin">policyallflags</code> with the name for the configuration.
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policyallflags <span id="CO111-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-sriov-network-operator <span id="CO111-2"><!--Empty--></span><span class="callout">2</span>
spec:
  resourceName: policyallflags <span id="CO111-3"><!--Empty--></span><span class="callout">3</span>
  nodeSelector: <span id="CO111-4"><!--Empty--></span><span class="callout">4</span>
    node.alpha.kubernetes-incubator.io/nfd-network-sriov.capable = `true`
  priority: 10 <span id="CO111-5"><!--Empty--></span><span class="callout">5</span>
  numVfs: 5 <span id="CO111-6"><!--Empty--></span><span class="callout">6</span>
  nicSelector: <span id="CO111-7"><!--Empty--></span><span class="callout">7</span>
    pfNames: ["ens1f0"]  <span id="CO111-8"><!--Empty--></span><span class="callout">8</span>
  deviceType: "netdevice" <span id="CO111-9"><!--Empty--></span><span class="callout">9</span>
  isRdma: false <span id="CO111-10"><!--Empty--></span><span class="callout">10</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO111-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The name for the custom resource object.
									</div></dd><dt><a href="#CO111-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The namespace where the SR-IOV Network Operator is installed.
									</div></dd><dt><a href="#CO111-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The resource name of the SR-IOV network device plugin. You can create multiple SR-IOV network node policies for a resource name.
									</div></dd><dt><a href="#CO111-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										The node selector specifies the nodes to configure. Only SR-IOV network devices on the selected nodes are configured. The SR-IOV Container Network Interface (CNI) plugin and device plugin are deployed on selected nodes only.
									</div></dd><dt><a href="#CO111-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										Optional: The priority is an integer value between <code class="literal cluster-admin">0</code> and <code class="literal cluster-admin">99</code>. A smaller value receives higher priority. For example, a priority of <code class="literal cluster-admin">10</code> is a higher priority than <code class="literal cluster-admin">99</code>. The default value is <code class="literal cluster-admin">99</code>.
									</div></dd><dt><a href="#CO111-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										The number of virtual functions (VFs) to create for the SR-IOV physical network device. For an Intel network interface controller (NIC), the number of VFs cannot be larger than the total VFs supported by the device. For a Mellanox NIC, the number of VFs cannot be larger than <code class="literal cluster-admin">128</code>.
									</div></dd><dt><a href="#CO111-7"><span class="callout">7</span></a> </dt><dd><div class="para">
										The NIC selector identifies the device for the Operator to configure. You do not have to specify values for all the parameters. It is recommended to identify the network device with enough precision to avoid selecting a device unintentionally. If you specify <code class="literal cluster-admin">rootDevices</code>, you must also specify a value for <code class="literal cluster-admin">vendor</code>, <code class="literal cluster-admin">deviceID</code>, or <code class="literal cluster-admin">pfNames</code>. If you specify both <code class="literal cluster-admin">pfNames</code> and <code class="literal cluster-admin">rootDevices</code> at the same time, ensure that they refer to the same device. If you specify a value for <code class="literal cluster-admin">netFilter</code>, then you do not need to specify any other parameter because a network ID is unique.
									</div></dd><dt><a href="#CO111-8"><span class="callout">8</span></a> </dt><dd><div class="para">
										Optional: An array of one or more physical function (PF) names for the device.
									</div></dd><dt><a href="#CO111-9"><span class="callout">9</span></a> </dt><dd><div class="para">
										Optional: The driver type for the virtual functions. The only allowed value is <code class="literal cluster-admin">netdevice</code>. For a Mellanox NIC to work in DPDK mode on bare metal nodes, set <code class="literal cluster-admin">isRdma</code> to <code class="literal cluster-admin">true</code>.
									</div></dd><dt><a href="#CO111-10"><span class="callout">10</span></a> </dt><dd><div class="para">
										Optional: Configures whether to enable remote direct memory access (RDMA) mode. The default value is <code class="literal cluster-admin">false</code>. If the <code class="literal cluster-admin">isRdma</code> parameter is set to <code class="literal cluster-admin">true</code>, you can continue to use the RDMA-enabled VF as a normal network device. A device can be used in either mode. Set <code class="literal cluster-admin">isRdma</code> to <code class="literal cluster-admin">true</code> and additionally set <code class="literal cluster-admin">needVhostNet</code> to <code class="literal cluster-admin">true</code> to configure a Mellanox NIC for use with Fast Datapath DPDK applications.
									</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
									The <code class="literal cluster-admin">vfio-pci</code> driver type is not supported.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Create the SriovNetworkNodePolicy object:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f policyallflags-sriov-node-network.yaml</pre><p class="cluster-admin cluster-admin">
								After applying the configuration update, all the pods in sriov-network-operator namespace change to the <code class="literal cluster-admin">Running</code> status.
							</p></li><li class="listitem"><p class="simpara">
								To verify that the SR-IOV network device is configured, enter the following command. Replace <code class="literal cluster-admin">&lt;node_name&gt;</code> with the name of a node with the SR-IOV network device that you just configured.
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get sriovnetworknodestates -n openshift-sriov-network-operator &lt;node_name&gt; -o jsonpath='{.status.syncStatus}'</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Succeeded</pre>

								</p></div></li></ol></div></section><section class="section cluster-admin" id="configuring-sysctl-on-bonded-sriov-network_configuring-sysctl-interface-sriov-device"><div class="titlepage"><div><div><h4 class="title">25.8.3.2. Configuring sysctl on a bonded SR-IOV network</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can set interface specific <code class="literal cluster-admin">sysctl</code> settings on a bonded interface created from two SR-IOV interfaces. Do this by adding the tuning configuration to the optional <code class="literal cluster-admin">Plugins</code> parameter of the bond network attachment definition.
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							Do not edit <code class="literal cluster-admin">NetworkAttachmentDefinition</code> custom resources that the SR-IOV Network Operator manages. Doing so might disrupt network traffic on your additional network.
						</p></div></div><p class="cluster-admin cluster-admin">
						To change specific interface-level network <code class="literal cluster-admin">sysctl</code> settings create the <code class="literal cluster-admin">SriovNetwork</code> custom resource (CR) with the Container Network Interface (CNI) tuning plugin by using the following procedure.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Install the OpenShift Container Platform CLI (oc).
							</li><li class="listitem">
								Log in to the OpenShift Container Platform cluster as a user with cluster-admin privileges.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create the <code class="literal cluster-admin">SriovNetwork</code> custom resource (CR) for the bonded interface as in the following example CR. Save the YAML as the file <code class="literal cluster-admin">sriov-network-attachment.yaml</code>.
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: allvalidflags <span id="CO112-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-sriov-network-operator <span id="CO112-2"><!--Empty--></span><span class="callout">2</span>
spec:
  resourceName: policyallflags <span id="CO112-3"><!--Empty--></span><span class="callout">3</span>
  networkNamespace: sysctl-tuning-test <span id="CO112-4"><!--Empty--></span><span class="callout">4</span>
  capabilities: '{ "mac": true, "ips": true }' <span id="CO112-5"><!--Empty--></span><span class="callout">5</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO112-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										A name for the object. The SR-IOV Network Operator creates a NetworkAttachmentDefinition object with same name.
									</div></dd><dt><a href="#CO112-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The namespace where the SR-IOV Network Operator is installed.
									</div></dd><dt><a href="#CO112-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The value for the <code class="literal cluster-admin">spec.resourceName</code> parameter from the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object that defines the SR-IOV hardware for this additional network.
									</div></dd><dt><a href="#CO112-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										The target namespace for the <code class="literal cluster-admin">SriovNetwork</code> object. Only pods in the target namespace can attach to the additional network.
									</div></dd><dt><a href="#CO112-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										Optional: The capabilities to configure for this additional network. You can specify <code class="literal cluster-admin">"{ "ips": true }"</code> to enable IP address support or <code class="literal cluster-admin">"{ "mac": true }"</code> to enable MAC address support.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create the <code class="literal cluster-admin">SriovNetwork</code> resource:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f sriov-network-attachment.yaml</pre></li><li class="listitem"><p class="simpara">
								Create a bond network attachment definition as in the following example CR. Save the YAML as the file <code class="literal cluster-admin">sriov-bond-network-interface.yaml</code>.
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: bond-sysctl-network
  namespace: sysctl-tuning-test
spec:
  config: '{
  "cniVersion":"0.4.0",
  "name":"bound-net",
  "plugins":[
    {
      "type":"bond", <span id="CO113-1"><!--Empty--></span><span class="callout">1</span>
      "mode": "active-backup", <span id="CO113-2"><!--Empty--></span><span class="callout">2</span>
      "failOverMac": 1, <span id="CO113-3"><!--Empty--></span><span class="callout">3</span>
      "linksInContainer": true, <span id="CO113-4"><!--Empty--></span><span class="callout">4</span>
      "miimon": "100",
      "links": [ <span id="CO113-5"><!--Empty--></span><span class="callout">5</span>
        {"name": "net1"},
        {"name": "net2"}
      ],
      "ipam":{ <span id="CO113-6"><!--Empty--></span><span class="callout">6</span>
        "type":"static"
      }
    },
    {
      "type":"tuning", <span id="CO113-7"><!--Empty--></span><span class="callout">7</span>
      "capabilities":{
        "mac":true
      },
      "sysctl":{
        "net.ipv4.conf.IFNAME.accept_redirects": "0",
        "net.ipv4.conf.IFNAME.accept_source_route": "0",
        "net.ipv4.conf.IFNAME.disable_policy": "1",
        "net.ipv4.conf.IFNAME.secure_redirects": "0",
        "net.ipv4.conf.IFNAME.send_redirects": "0",
        "net.ipv6.conf.IFNAME.accept_redirects": "0",
        "net.ipv6.conf.IFNAME.accept_source_route": "1",
        "net.ipv6.neigh.IFNAME.base_reachable_time_ms": "20000",
        "net.ipv6.neigh.IFNAME.retrans_time_ms": "2000"
      }
    }
  ]
}'</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO113-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The type is <code class="literal cluster-admin">bond</code>.
									</div></dd><dt><a href="#CO113-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The <code class="literal cluster-admin">mode</code> attribute specifies the bonding mode. The bonding modes supported are:
									</div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
												<code class="literal cluster-admin">balance-rr</code> - 0
											</li><li class="listitem">
												<code class="literal cluster-admin">active-backup</code> - 1
											</li><li class="listitem"><p class="simpara">
												<code class="literal cluster-admin">balance-xor</code> - 2
											</p><p class="cluster-admin cluster-admin">
												For <code class="literal cluster-admin">balance-rr</code> or <code class="literal cluster-admin">balance-xor</code> modes, you must set the <code class="literal cluster-admin">trust</code> mode to <code class="literal cluster-admin">on</code> for the SR-IOV virtual function.
											</p></li></ul></div></dd><dt><a href="#CO113-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The <code class="literal cluster-admin">failover</code> attribute is mandatory for active-backup mode.
									</div></dd><dt><a href="#CO113-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										The <code class="literal cluster-admin">linksInContainer=true</code> flag informs the Bond CNI that the required interfaces are to be found inside the container. By default, Bond CNI looks for these interfaces on the host which does not work for integration with SRIOV and Multus.
									</div></dd><dt><a href="#CO113-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										The <code class="literal cluster-admin">links</code> section defines which interfaces will be used to create the bond. By default, Multus names the attached interfaces as: "net", plus a consecutive number, starting with one.
									</div></dd><dt><a href="#CO113-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										A configuration object for the IPAM CNI plugin as a YAML block scalar. The plugin manages IP address assignment for the attachment definition. In this pod example IP addresses are configured manually, so in this case,<code class="literal cluster-admin">ipam</code> is set to static.
									</div></dd><dt><a href="#CO113-7"><span class="callout">7</span></a> </dt><dd><div class="para">
										Add additional capabilities to the device. For example, set the <code class="literal cluster-admin">type</code> field to <code class="literal cluster-admin">tuning</code>. Specify the interface-level network <code class="literal cluster-admin">sysctl</code> you want to set in the sysctl field. This example sets all interface-level network <code class="literal cluster-admin">sysctl</code> settings that can be set.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create the bond network attachment resource:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f sriov-bond-network-interface.yaml</pre></li></ol></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Verifying that the <code class="literal cluster-admin">NetworkAttachmentDefinition</code> CR is successfully created</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								Confirm that the SR-IOV Network Operator created the <code class="literal cluster-admin">NetworkAttachmentDefinition</code> CR by running the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get network-attachment-definitions -n &lt;namespace&gt; <span id="CO114-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO114-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal cluster-admin">&lt;namespace&gt;</code> with the networkNamespace that you specified when configuring the network attachment, for example, <code class="literal cluster-admin">sysctl-tuning-test</code>.
									</div></dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                          AGE
bond-sysctl-network           22m
allvalidflags                 47m</pre>

								</p></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
									There might be a delay before the SR-IOV Network Operator creates the CR.
								</p></div></div></li></ul></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Verifying that the additional SR-IOV network resource is successful</strong></p><p>
							To verify that the tuning CNI is correctly configured and the additional SR-IOV network attachment is attached, do the following:
						</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create a <code class="literal cluster-admin">Pod</code> CR. For example, save the following YAML as the file <code class="literal cluster-admin">examplepod.yaml</code>:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Pod
metadata:
  name: tunepod
  namespace: sysctl-tuning-test
  annotations:
    k8s.v1.cni.cncf.io/networks: |-
      [
        {"name": "allvalidflags"}, <span id="CO115-1"><!--Empty--></span><span class="callout">1</span>
        {"name": "allvalidflags"},
        {
          "name": "bond-sysctl-network",
          "interface": "bond0",
          "mac": "0a:56:0a:83:04:0c", <span id="CO115-2"><!--Empty--></span><span class="callout">2</span>
          "ips": ["10.100.100.200/24"] <span id="CO115-3"><!--Empty--></span><span class="callout">3</span>
       }
      ]
spec:
  containers:
  - name: podexample
    image: centos
    command: ["/bin/bash", "-c", "sleep INF"]
    securityContext:
      runAsUser: 2000
      runAsGroup: 3000
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
  securityContext:
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO115-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The name of the SR-IOV network attachment definition CR.
									</div></dd><dt><a href="#CO115-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Optional: The MAC address for the SR-IOV device that is allocated from the resource type defined in the SR-IOV network attachment definition CR. To use this feature, you also must specify <code class="literal cluster-admin">{ "mac": true }</code> in the SriovNetwork object.
									</div></dd><dt><a href="#CO115-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Optional: IP addresses for the SR-IOV device that are allocated from the resource type defined in the SR-IOV network attachment definition CR. Both IPv4 and IPv6 addresses are supported. To use this feature, you also must specify <code class="literal cluster-admin">{ "ips": true }</code> in the <code class="literal cluster-admin">SriovNetwork</code> object.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Apply the YAML:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f examplepod.yaml</pre></li><li class="listitem"><p class="simpara">
								Verify that the pod is created by running the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pod -n sysctl-tuning-test</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME      READY   STATUS    RESTARTS   AGE
tunepod   1/1     Running   0          47s</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Log in to the pod by running the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc rsh -n sysctl-tuning-test tunepod</pre></li><li class="listitem"><p class="simpara">
								Verify the values of the configured <code class="literal cluster-admin">sysctl</code> flag. Find the value <code class="literal cluster-admin">net.ipv6.neigh.IFNAME.base_reachable_time_ms</code> by running the following command::
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ sysctl net.ipv6.neigh.bond0.base_reachable_time_ms</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">net.ipv6.neigh.bond0.base_reachable_time_ms = 20000</pre>

								</p></div></li></ol></div></section></section></section><section class="section cluster-admin" id="using-sriov-multicast"><div class="titlepage"><div><div><h2 class="title">25.9. Using high performance multicast</h2></div></div></div><p class="cluster-admin cluster-admin">
				You can use multicast on your Single Root I/O Virtualization (SR-IOV) hardware network.
			</p><section class="section cluster-admin" id="nw-high-performance-multicast_using-sriov-multicast"><div class="titlepage"><div><div><h3 class="title">25.9.1. High performance multicast</h3></div></div></div><p class="cluster-admin cluster-admin">
					The OpenShift SDN network plugin supports multicast between pods on the default network. This is best used for low-bandwidth coordination or service discovery, and not high-bandwidth applications. For applications such as streaming media, like Internet Protocol television (IPTV) and multipoint videoconferencing, you can utilize Single Root I/O Virtualization (SR-IOV) hardware to provide near-native performance.
				</p><p class="cluster-admin cluster-admin">
					When using additional SR-IOV interfaces for multicast:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Multicast packages must be sent or received by a pod through the additional SR-IOV interface.
						</li><li class="listitem">
							The physical network which connects the SR-IOV interfaces decides the multicast routing and topology, which is not controlled by OpenShift Container Platform.
						</li></ul></div></section><section class="section cluster-admin" id="nw-using-an-sriov-interface-for-multicast_using-sriov-multicast"><div class="titlepage"><div><div><h3 class="title">25.9.2. Configuring an SR-IOV interface for multicast</h3></div></div></div><p class="cluster-admin cluster-admin">
					The follow procedure creates an example SR-IOV interface for multicast.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You must log in to the cluster with a user that has the <code class="literal cluster-admin">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-example
  namespace: openshift-sriov-network-operator
spec:
  resourceName: example
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  numVfs: 4
  nicSelector:
    vendor: "8086"
    pfNames: ['ens803f0']
    rootDevices: ['0000:86:00.0']</pre></li><li class="listitem"><p class="simpara">
							Create a <code class="literal cluster-admin">SriovNetwork</code> object:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: net-example
  namespace: openshift-sriov-network-operator
spec:
  networkNamespace: default
  ipam: | <span id="CO116-1"><!--Empty--></span><span class="callout">1</span>
    {
      "type": "host-local", <span id="CO116-2"><!--Empty--></span><span class="callout">2</span>
      "subnet": "10.56.217.0/24",
      "rangeStart": "10.56.217.171",
      "rangeEnd": "10.56.217.181",
      "routes": [
        {"dst": "224.0.0.0/5"},
        {"dst": "232.0.0.0/5"}
      ],
      "gateway": "10.56.217.1"
    }
  resourceName: example</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO116-1"><span class="callout">1</span></a> <a href="#CO116-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									If you choose to configure DHCP as IPAM, ensure that you provision the following default routes through your DHCP server: <code class="literal cluster-admin">224.0.0.0/5</code> and <code class="literal cluster-admin">232.0.0.0/5</code>. This is to override the static multicast route set by the default network provider.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create a pod with multicast application:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Pod
metadata:
  name: testpmd
  namespace: default
  annotations:
    k8s.v1.cni.cncf.io/networks: nic1
spec:
  containers:
  - name: example
    image: rhel7:latest
    securityContext:
      capabilities:
        add: ["NET_ADMIN"] <span id="CO117-1"><!--Empty--></span><span class="callout">1</span>
    command: [ "sleep", "infinity"]</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO117-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal cluster-admin">NET_ADMIN</code> capability is required only if your application needs to assign the multicast IP address to the SR-IOV interface. Otherwise, it can be omitted.
								</div></dd></dl></div></li></ol></div></section></section><section class="section cluster-admin" id="using-dpdk-and-rdma"><div class="titlepage"><div><div><h2 class="title">25.10. Using DPDK and RDMA</h2></div></div></div><p class="cluster-admin cluster-admin">
				The containerized Data Plane Development Kit (DPDK) application is supported on OpenShift Container Platform. You can use Single Root I/O Virtualization (SR-IOV) network hardware with the Data Plane Development Kit (DPDK) and with remote direct memory access (RDMA).
			</p><p class="cluster-admin cluster-admin">
				For information on supported devices, refer to <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#supported-devices_about-sriov">Supported devices</a>.
			</p><section class="section cluster-admin" id="example-vf-use-in-dpdk-mode-intel_using-dpdk-and-rdma"><div class="titlepage"><div><div><h3 class="title">25.10.1. Using a virtual function in DPDK mode with an Intel NIC</h3></div></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Install the SR-IOV Network Operator.
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create the following <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object, and then save the YAML in the <code class="literal cluster-admin">intel-dpdk-node-policy.yaml</code> file.
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: intel-dpdk-node-policy
  namespace: openshift-sriov-network-operator
spec:
  resourceName: intelnics
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  priority: &lt;priority&gt;
  numVfs: &lt;num&gt;
  nicSelector:
    vendor: "8086"
    deviceID: "158b"
    pfNames: ["&lt;pf_name&gt;", ...]
    rootDevices: ["&lt;pci_bus_id&gt;", "..."]
  deviceType: vfio-pci <span id="CO118-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO118-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the driver type for the virtual functions to <code class="literal cluster-admin">vfio-pci</code>.
								</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								See the <code class="literal cluster-admin">Configuring SR-IOV network devices</code> section for a detailed explanation on each option in <code class="literal cluster-admin">SriovNetworkNodePolicy</code>.
							</p><p class="cluster-admin cluster-admin">
								When applying the configuration specified in a <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object, the SR-IOV Operator may drain the nodes, and in some cases, reboot nodes. It may take several minutes for a configuration change to apply. Ensure that there are enough available nodes in your cluster to handle the evicted workload beforehand.
							</p><p class="cluster-admin cluster-admin">
								After the configuration update is applied, all the pods in <code class="literal cluster-admin">openshift-sriov-network-operator</code> namespace will change to a <code class="literal cluster-admin">Running</code> status.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f intel-dpdk-node-policy.yaml</pre></li><li class="listitem"><p class="simpara">
							Create the following <code class="literal cluster-admin">SriovNetwork</code> object, and then save the YAML in the <code class="literal cluster-admin">intel-dpdk-network.yaml</code> file.
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: intel-dpdk-network
  namespace: openshift-sriov-network-operator
spec:
  networkNamespace: &lt;target_namespace&gt;
  ipam: |-
# ... <span id="CO119-1"><!--Empty--></span><span class="callout">1</span>
  vlan: &lt;vlan&gt;
  resourceName: intelnics</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO119-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify a configuration object for the ipam CNI plugin as a YAML block scalar. The plugin manages IP address assignment for the attachment definition.
								</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								See the "Configuring SR-IOV additional network" section for a detailed explanation on each option in <code class="literal cluster-admin">SriovNetwork</code>.
							</p></div></div><p class="cluster-admin cluster-admin">
							An optional library, app-netutil, provides several API methods for gathering network information about a container’s parent pod.
						</p></li><li class="listitem"><p class="simpara">
							Create the <code class="literal cluster-admin">SriovNetwork</code> object by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f intel-dpdk-network.yaml</pre></li><li class="listitem"><p class="simpara">
							Create the following <code class="literal cluster-admin">Pod</code> spec, and then save the YAML in the <code class="literal cluster-admin">intel-dpdk-pod.yaml</code> file.
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Pod
metadata:
  name: dpdk-app
  namespace: &lt;target_namespace&gt; <span id="CO120-1"><!--Empty--></span><span class="callout">1</span>
  annotations:
    k8s.v1.cni.cncf.io/networks: intel-dpdk-network
spec:
  containers:
  - name: testpmd
    image: &lt;DPDK_image&gt; <span id="CO120-2"><!--Empty--></span><span class="callout">2</span>
    securityContext:
      runAsUser: 0
      capabilities:
        add: ["IPC_LOCK","SYS_RESOURCE","NET_RAW"] <span id="CO120-3"><!--Empty--></span><span class="callout">3</span>
    volumeMounts:
    - mountPath: /dev/hugepages <span id="CO120-4"><!--Empty--></span><span class="callout">4</span>
      name: hugepage
    resources:
      limits:
        openshift.io/intelnics: "1" <span id="CO120-5"><!--Empty--></span><span class="callout">5</span>
        memory: "1Gi"
        cpu: "4" <span id="CO120-6"><!--Empty--></span><span class="callout">6</span>
        hugepages-1Gi: "4Gi" <span id="CO120-7"><!--Empty--></span><span class="callout">7</span>
      requests:
        openshift.io/intelnics: "1"
        memory: "1Gi"
        cpu: "4"
        hugepages-1Gi: "4Gi"
    command: ["sleep", "infinity"]
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO120-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the same <code class="literal cluster-admin">target_namespace</code> where the <code class="literal cluster-admin">SriovNetwork</code> object <code class="literal cluster-admin">intel-dpdk-network</code> is created. If you would like to create the pod in a different namespace, change <code class="literal cluster-admin">target_namespace</code> in both the <code class="literal cluster-admin">Pod</code> spec and the <code class="literal cluster-admin">SriovNetwork</code> object.
								</div></dd><dt><a href="#CO120-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the DPDK image which includes your application and the DPDK library used by application.
								</div></dd><dt><a href="#CO120-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify additional capabilities required by the application inside the container for hugepage allocation, system resource allocation, and network interface access.
								</div></dd><dt><a href="#CO120-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Mount a hugepage volume to the DPDK pod under <code class="literal cluster-admin">/dev/hugepages</code>. The hugepage volume is backed by the emptyDir volume type with the medium being <code class="literal cluster-admin">Hugepages</code>.
								</div></dd><dt><a href="#CO120-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Optional: Specify the number of DPDK devices allocated to DPDK pod. This resource request and limit, if not explicitly specified, will be automatically added by the SR-IOV network resource injector. The SR-IOV network resource injector is an admission controller component managed by the SR-IOV Operator. It is enabled by default and can be disabled by setting <code class="literal cluster-admin">enableInjector</code> option to <code class="literal cluster-admin">false</code> in the default <code class="literal cluster-admin">SriovOperatorConfig</code> CR.
								</div></dd><dt><a href="#CO120-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Specify the number of CPUs. The DPDK pod usually requires exclusive CPUs to be allocated from the kubelet. This is achieved by setting CPU Manager policy to <code class="literal cluster-admin">static</code> and creating a pod with <code class="literal cluster-admin">Guaranteed</code> QoS.
								</div></dd><dt><a href="#CO120-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									Specify hugepage size <code class="literal cluster-admin">hugepages-1Gi</code> or <code class="literal cluster-admin">hugepages-2Mi</code> and the quantity of hugepages that will be allocated to the DPDK pod. Configure <code class="literal cluster-admin">2Mi</code> and <code class="literal cluster-admin">1Gi</code> hugepages separately. Configuring <code class="literal cluster-admin">1Gi</code> hugepage requires adding kernel arguments to Nodes. For example, adding kernel arguments <code class="literal cluster-admin">default_hugepagesz=1GB</code>, <code class="literal cluster-admin">hugepagesz=1G</code> and <code class="literal cluster-admin">hugepages=16</code> will result in <code class="literal cluster-admin">16*1Gi</code> hugepages be allocated during system boot.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the DPDK pod by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f intel-dpdk-pod.yaml</pre></li></ol></div></section><section class="section cluster-admin" id="example-vf-use-in-dpdk-mode-mellanox_using-dpdk-and-rdma"><div class="titlepage"><div><div><h3 class="title">25.10.2. Using a virtual function in DPDK mode with a Mellanox NIC</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can create a network node policy and create a Data Plane Development Kit (DPDK) pod using a virtual function in DPDK mode with a Mellanox NIC.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You have installed the Single Root I/O Virtualization (SR-IOV) Network Operator.
						</li><li class="listitem">
							You have logged in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Save the following <code class="literal cluster-admin">SriovNetworkNodePolicy</code> YAML configuration to an <code class="literal cluster-admin">mlx-dpdk-node-policy.yaml</code> file:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: mlx-dpdk-node-policy
  namespace: openshift-sriov-network-operator
spec:
  resourceName: mlxnics
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  priority: &lt;priority&gt;
  numVfs: &lt;num&gt;
  nicSelector:
    vendor: "15b3"
    deviceID: "1015" <span id="CO121-1"><!--Empty--></span><span class="callout">1</span>
    pfNames: ["&lt;pf_name&gt;", ...]
    rootDevices: ["&lt;pci_bus_id&gt;", "..."]
  deviceType: netdevice <span id="CO121-2"><!--Empty--></span><span class="callout">2</span>
  isRdma: true <span id="CO121-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO121-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the device hex code of the SR-IOV network device.
								</div></dd><dt><a href="#CO121-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the driver type for the virtual functions to <code class="literal cluster-admin">netdevice</code>. A Mellanox SR-IOV Virtual Function (VF) can work in DPDK mode without using the <code class="literal cluster-admin">vfio-pci</code> device type. The VF device appears as a kernel network interface inside a container.
								</div></dd><dt><a href="#CO121-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Enable Remote Direct Memory Access (RDMA) mode. This is required for Mellanox cards to work in DPDK mode.
								</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								See <span class="emphasis"><em><span class="cluster-admin cluster-admin">Configuring an SR-IOV network device</span></em></span> for a detailed explanation of each option in the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object.
							</p><p class="cluster-admin cluster-admin">
								When applying the configuration specified in an <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object, the SR-IOV Operator might drain the nodes, and in some cases, reboot nodes. It might take several minutes for a configuration change to apply. Ensure that there are enough available nodes in your cluster to handle the evicted workload beforehand.
							</p><p class="cluster-admin cluster-admin">
								After the configuration update is applied, all the pods in the <code class="literal cluster-admin">openshift-sriov-network-operator</code> namespace will change to a <code class="literal cluster-admin">Running</code> status.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f mlx-dpdk-node-policy.yaml</pre></li><li class="listitem"><p class="simpara">
							Save the following <code class="literal cluster-admin">SriovNetwork</code> YAML configuration to an <code class="literal cluster-admin">mlx-dpdk-network.yaml</code> file:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: mlx-dpdk-network
  namespace: openshift-sriov-network-operator
spec:
  networkNamespace: &lt;target_namespace&gt;
  ipam: |- <span id="CO122-1"><!--Empty--></span><span class="callout">1</span>
...
  vlan: &lt;vlan&gt;
  resourceName: mlxnics</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO122-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify a configuration object for the IP Address Management (IPAM) Container Network Interface (CNI) plugin as a YAML block scalar. The plugin manages IP address assignment for the attachment definition.
								</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								See <span class="emphasis"><em><span class="cluster-admin cluster-admin">Configuring an SR-IOV network device</span></em></span> for a detailed explanation on each option in the <code class="literal cluster-admin">SriovNetwork</code> object.
							</p></div></div><p class="cluster-admin cluster-admin">
							The <code class="literal cluster-admin">app-netutil</code> option library provides several API methods for gathering network information about the parent pod of a container.
						</p></li><li class="listitem"><p class="simpara">
							Create the <code class="literal cluster-admin">SriovNetwork</code> object by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f mlx-dpdk-network.yaml</pre></li><li class="listitem"><p class="simpara">
							Save the following <code class="literal cluster-admin">Pod</code> YAML configuration to an <code class="literal cluster-admin">mlx-dpdk-pod.yaml</code> file:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Pod
metadata:
  name: dpdk-app
  namespace: &lt;target_namespace&gt; <span id="CO123-1"><!--Empty--></span><span class="callout">1</span>
  annotations:
    k8s.v1.cni.cncf.io/networks: mlx-dpdk-network
spec:
  containers:
  - name: testpmd
    image: &lt;DPDK_image&gt; <span id="CO123-2"><!--Empty--></span><span class="callout">2</span>
    securityContext:
      runAsUser: 0
      capabilities:
        add: ["IPC_LOCK","SYS_RESOURCE","NET_RAW"] <span id="CO123-3"><!--Empty--></span><span class="callout">3</span>
    volumeMounts:
    - mountPath: /dev/hugepages <span id="CO123-4"><!--Empty--></span><span class="callout">4</span>
      name: hugepage
    resources:
      limits:
        openshift.io/mlxnics: "1" <span id="CO123-5"><!--Empty--></span><span class="callout">5</span>
        memory: "1Gi"
        cpu: "4" <span id="CO123-6"><!--Empty--></span><span class="callout">6</span>
        hugepages-1Gi: "4Gi" <span id="CO123-7"><!--Empty--></span><span class="callout">7</span>
      requests:
        openshift.io/mlxnics: "1"
        memory: "1Gi"
        cpu: "4"
        hugepages-1Gi: "4Gi"
    command: ["sleep", "infinity"]
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO123-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the same <code class="literal cluster-admin">target_namespace</code> where <code class="literal cluster-admin">SriovNetwork</code> object <code class="literal cluster-admin">mlx-dpdk-network</code> is created. To create the pod in a different namespace, change <code class="literal cluster-admin">target_namespace</code> in both the <code class="literal cluster-admin">Pod</code> spec and <code class="literal cluster-admin">SriovNetwork</code> object.
								</div></dd><dt><a href="#CO123-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the DPDK image which includes your application and the DPDK library used by the application.
								</div></dd><dt><a href="#CO123-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify additional capabilities required by the application inside the container for hugepage allocation, system resource allocation, and network interface access.
								</div></dd><dt><a href="#CO123-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Mount the hugepage volume to the DPDK pod under <code class="literal cluster-admin">/dev/hugepages</code>. The hugepage volume is backed by the <code class="literal cluster-admin">emptyDir</code> volume type with the medium being <code class="literal cluster-admin">Hugepages</code>.
								</div></dd><dt><a href="#CO123-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Optional: Specify the number of DPDK devices allocated for the DPDK pod. If not explicitly specified, this resource request and limit is automatically added by the SR-IOV network resource injector. The SR-IOV network resource injector is an admission controller component managed by SR-IOV Operator. It is enabled by default and can be disabled by setting the <code class="literal cluster-admin">enableInjector</code> option to <code class="literal cluster-admin">false</code> in the default <code class="literal cluster-admin">SriovOperatorConfig</code> CR.
								</div></dd><dt><a href="#CO123-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Specify the number of CPUs. The DPDK pod usually requires that exclusive CPUs be allocated from the kubelet. To do this, set the CPU Manager policy to <code class="literal cluster-admin">static</code> and create a pod with <code class="literal cluster-admin">Guaranteed</code> Quality of Service (QoS).
								</div></dd><dt><a href="#CO123-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									Specify hugepage size <code class="literal cluster-admin">hugepages-1Gi</code> or <code class="literal cluster-admin">hugepages-2Mi</code> and the quantity of hugepages that will be allocated to the DPDK pod. Configure <code class="literal cluster-admin">2Mi</code> and <code class="literal cluster-admin">1Gi</code> hugepages separately. Configuring <code class="literal cluster-admin">1Gi</code> hugepages requires adding kernel arguments to Nodes.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the DPDK pod by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f mlx-dpdk-pod.yaml</pre></li></ol></div></section><section class="section cluster-admin" id="nw-sriov-example-dpdk-line-rate_using-dpdk-and-rdma"><div class="titlepage"><div><div><h3 class="title">25.10.3. Overview of achieving a specific DPDK line rate</h3></div></div></div><p class="cluster-admin cluster-admin">
					To achieve a specific Data Plane Development Kit (DPDK) line rate, deploy a Node Tuning Operator and configure Single Root I/O Virtualization (SR-IOV). You must also tune the DPDK settings for the following resources:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Isolated CPUs
						</li><li class="listitem">
							Hugepages
						</li><li class="listitem">
							The topology scheduler
						</li></ul></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						In previous versions of OpenShift Container Platform, the Performance Addon Operator was used to implement automatic tuning to achieve low latency performance for OpenShift Container Platform applications. In OpenShift Container Platform 4.11 and later, this functionality is part of the Node Tuning Operator.
					</p></div></div><div class="cluster-admin cluster-admin"><p class="title"><strong>DPDK test environment</strong></p><p>
						The following diagram shows the components of a traffic-testing environment:
					</p></div><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/3feade4011d56df9247e6e040ee32404/261_OpenShift_DPDK_0722.png" alt="DPDK test environment"/></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Traffic generator</span></strong></span>: An application that can generate high-volume packet traffic.
						</li><li class="listitem">
							<span class="strong strong"><strong><span class="cluster-admin cluster-admin">SR-IOV-supporting NIC</span></strong></span>: A network interface card compatible with SR-IOV. The card runs a number of virtual functions on a physical interface.
						</li><li class="listitem">
							<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Physical Function (PF)</span></strong></span>: A PCI Express (PCIe) function of a network adapter that supports the SR-IOV interface.
						</li><li class="listitem">
							<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Virtual Function (VF)</span></strong></span>: A lightweight PCIe function on a network adapter that supports SR-IOV. The VF is associated with the PCIe PF on the network adapter. The VF represents a virtualized instance of the network adapter.
						</li><li class="listitem">
							<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Switch</span></strong></span>: A network switch. Nodes can also be connected back-to-back.
						</li><li class="listitem">
							<span class="strong strong"><strong><span class="cluster-admin cluster-admin"><code class="literal cluster-admin">testpmd</code></span></strong></span>: An example application included with DPDK. The <code class="literal cluster-admin">testpmd</code> application can be used to test the DPDK in a packet-forwarding mode. The <code class="literal cluster-admin">testpmd</code> application is also an example of how to build a fully-fledged application using the DPDK Software Development Kit (SDK).
						</li><li class="listitem">
							<span class="strong strong"><strong><span class="cluster-admin cluster-admin">worker 0</span></strong></span> and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">worker 1</span></strong></span>: OpenShift Container Platform nodes.
						</li></ul></div></section><section class="section cluster-admin" id="nw-example-dpdk-line-rate_using-dpdk-and-rdma"><div class="titlepage"><div><div><h3 class="title">25.10.4. Using SR-IOV and the Node Tuning Operator to achieve a DPDK line rate</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can use the Node Tuning Operator to configure isolated CPUs, hugepages, and a topology scheduler. You can then use the Node Tuning Operator with Single Root I/O Virtualization (SR-IOV) to achieve a specific Data Plane Development Kit (DPDK) line rate.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You have installed the SR-IOV Network Operator.
						</li><li class="listitem">
							You have logged in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li><li class="listitem"><p class="simpara">
							You have deployed a standalone Node Tuning Operator.
						</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								In previous versions of OpenShift Container Platform, the Performance Addon Operator was used to implement automatic tuning to achieve low latency performance for OpenShift applications. In OpenShift Container Platform 4.11 and later, this functionality is part of the Node Tuning Operator.
							</p></div></div></li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal cluster-admin">PerformanceProfile</code> object based on the following example:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: performance
spec:
  globallyDisableIrqLoadBalancing: true
  cpu:
    isolated: 21-51,73-103 <span id="CO124-1"><!--Empty--></span><span class="callout">1</span>
    reserved: 0-20,52-72 <span id="CO124-2"><!--Empty--></span><span class="callout">2</span>
  hugepages:
    defaultHugepagesSize: 1G <span id="CO124-3"><!--Empty--></span><span class="callout">3</span>
    pages:
      - count: 32
        size: 1G
  net:
    userLevelNetworking: true
  numa:
    topologyPolicy: "single-numa-node"
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO124-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									If hyperthreading is enabled on the system, allocate the relevant symbolic links to the <code class="literal cluster-admin">isolated</code> and <code class="literal cluster-admin">reserved</code> CPU groups. If the system contains multiple non-uniform memory access nodes (NUMAs), allocate CPUs from both NUMAs to both groups. You can also use the Performance Profile Creator for this task. For more information, see <span class="emphasis"><em><span class="cluster-admin cluster-admin">Creating a performance profile</span></em></span>.
								</div></dd><dt><a href="#CO124-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									You can also specify a list of devices that will have their queues set to the reserved CPU count. For more information, see <span class="emphasis"><em><span class="cluster-admin cluster-admin">Reducing NIC queues using the Node Tuning Operator</span></em></span>.
								</div></dd><dt><a href="#CO124-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Allocate the number and size of hugepages needed. You can specify the NUMA configuration for the hugepages. By default, the system allocates an even number to every NUMA node on the system. If needed, you can request the use of a realtime kernel for the nodes. See <span class="emphasis"><em><span class="cluster-admin cluster-admin">Provisioning a worker with real-time capabilities</span></em></span> for more information.
								</div></dd></dl></div></li><li class="listitem">
							Save the <code class="literal cluster-admin">yaml</code> file as <code class="literal cluster-admin">mlx-dpdk-perfprofile-policy.yaml</code>.
						</li><li class="listitem"><p class="simpara">
							Apply the performance profile using the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f mlx-dpdk-perfprofile-policy.yaml</pre></li></ol></div><section class="section cluster-admin" id="nw-sriov-network-operator_using-dpdk-and-rdma"><div class="titlepage"><div><div><h4 class="title">25.10.4.1. Example SR-IOV Network Operator for virtual functions</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can use the Single Root I/O Virtualization (SR-IOV) Network Operator to allocate and configure Virtual Functions (VFs) from SR-IOV-supporting Physical Function NICs on the nodes.
					</p><p class="cluster-admin cluster-admin">
						For more information on deploying the Operator, see <span class="emphasis"><em><span class="cluster-admin cluster-admin">Installing the SR-IOV Network Operator</span></em></span>. For more information on configuring an SR-IOV network device, see <span class="emphasis"><em><span class="cluster-admin cluster-admin">Configuring an SR-IOV network device</span></em></span>.
					</p><p class="cluster-admin cluster-admin">
						There are some differences between running Data Plane Development Kit (DPDK) workloads on Intel VFs and Mellanox VFs. This section provides object configuration examples for both VF types. The following is an example of an <code class="literal cluster-admin">sriovNetworkNodePolicy</code> object used to run DPDK applications on Intel NICs:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: dpdk-nic-1
  namespace: openshift-sriov-network-operator
spec:
  deviceType: vfio-pci <span id="CO125-1"><!--Empty--></span><span class="callout">1</span>
  needVhostNet: true <span id="CO125-2"><!--Empty--></span><span class="callout">2</span>
  nicSelector:
    pfNames: ["ens3f0"]
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""
  numVfs: 10
  priority: 99
  resourceName: dpdk_nic_1
---
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: dpdk-nic-1
  namespace: openshift-sriov-network-operator
spec:
  deviceType: vfio-pci
  needVhostNet: true
  nicSelector:
    pfNames: ["ens3f1"]
  nodeSelector:
  node-role.kubernetes.io/worker-cnf: ""
  numVfs: 10
  priority: 99
  resourceName: dpdk_nic_2</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO125-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								For Intel NICs, <code class="literal cluster-admin">deviceType</code> must be <code class="literal cluster-admin">vfio-pci</code>.
							</div></dd><dt><a href="#CO125-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								If kernel communication with DPDK workloads is required, add <code class="literal cluster-admin">needVhostNet: true</code>. This mounts the <code class="literal cluster-admin">/dev/net/tun</code> and <code class="literal cluster-admin">/dev/vhost-net</code> devices into the container so the application can create a tap device and connect the tap device to the DPDK workload.
							</div></dd></dl></div><p class="cluster-admin cluster-admin">
						The following is an example of an <code class="literal cluster-admin">sriovNetworkNodePolicy</code> object for Mellanox NICs:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: dpdk-nic-1
  namespace: openshift-sriov-network-operator
spec:
  deviceType: netdevice <span id="CO126-1"><!--Empty--></span><span class="callout">1</span>
  isRdma: true <span id="CO126-2"><!--Empty--></span><span class="callout">2</span>
  nicSelector:
    rootDevices:
      - "0000:5e:00.1"
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""
  numVfs: 5
  priority: 99
  resourceName: dpdk_nic_1
---
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: dpdk-nic-2
  namespace: openshift-sriov-network-operator
spec:
  deviceType: netdevice
  isRdma: true
  nicSelector:
    rootDevices:
      - "0000:5e:00.0"
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""
  numVfs: 5
  priority: 99
  resourceName: dpdk_nic_2</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO126-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								For Mellanox devices the <code class="literal cluster-admin">deviceType</code> must be <code class="literal cluster-admin">netdevice</code>.
							</div></dd><dt><a href="#CO126-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								For Mellanox devices <code class="literal cluster-admin">isRdma</code> must be <code class="literal cluster-admin">true</code>. Mellanox cards are connected to DPDK applications using Flow Bifurcation. This mechanism splits traffic between Linux user space and kernel space, and can enhance line rate processing capability.
							</div></dd></dl></div></section><section class="section cluster-admin" id="nw-sriov-create-object_using-dpdk-and-rdma"><div class="titlepage"><div><div><h4 class="title">25.10.4.2. Example SR-IOV network operator</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following is an example definition of an <code class="literal cluster-admin">sriovNetwork</code> object. In this case, Intel and Mellanox configurations are identical:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: dpdk-network-1
  namespace: openshift-sriov-network-operator
spec:
  ipam: '{"type": "host-local","ranges": [[{"subnet": "10.0.1.0/24"}]],"dataDir":
   "/run/my-orchestrator/container-ipam-state-1"}' <span id="CO127-1"><!--Empty--></span><span class="callout">1</span>
  networkNamespace: dpdk-test <span id="CO127-2"><!--Empty--></span><span class="callout">2</span>
  spoofChk: "off"
  trust: "on"
  resourceName: dpdk_nic_1 <span id="CO127-3"><!--Empty--></span><span class="callout">3</span>
---
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: dpdk-network-2
  namespace: openshift-sriov-network-operator
spec:
  ipam: '{"type": "host-local","ranges": [[{"subnet": "10.0.2.0/24"}]],"dataDir":
   "/run/my-orchestrator/container-ipam-state-1"}'
  networkNamespace: dpdk-test
  spoofChk: "off"
  trust: "on"
  resourceName: dpdk_nic_2</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO127-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								You can use a different IP Address Management (IPAM) implementation, such as Whereabouts. For more information, see <span class="emphasis"><em><span class="cluster-admin cluster-admin">Dynamic IP address assignment configuration with Whereabouts</span></em></span>.
							</div></dd><dt><a href="#CO127-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								You must request the <code class="literal cluster-admin">networkNamespace</code> where the network attachment definition will be created. You must create the <code class="literal cluster-admin">sriovNetwork</code> CR under the <code class="literal cluster-admin">openshift-sriov-network-operator</code> namespace.
							</div></dd><dt><a href="#CO127-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								The <code class="literal cluster-admin">resourceName</code> value must match that of the <code class="literal cluster-admin">resourceName</code> created under the <code class="literal cluster-admin">sriovNetworkNodePolicy</code>.
							</div></dd></dl></div></section><section class="section cluster-admin" id="nw-sriov-dpdk-base-workload_using-dpdk-and-rdma"><div class="titlepage"><div><div><h4 class="title">25.10.4.3. Example DPDK base workload</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following is an example of a Data Plane Development Kit (DPDK) container:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Namespace
metadata:
  name: dpdk-test
---
apiVersion: v1
kind: Pod
metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: '[ <span id="CO128-1"><!--Empty--></span><span class="callout">1</span>
     {
      "name": "dpdk-network-1",
      "namespace": "dpdk-test"
     },
     {
      "name": "dpdk-network-2",
      "namespace": "dpdk-test"
     }
   ]'
    irq-load-balancing.crio.io: "disable" <span id="CO128-2"><!--Empty--></span><span class="callout">2</span>
    cpu-load-balancing.crio.io: "disable"
    cpu-quota.crio.io: "disable"
  labels:
    app: dpdk
  name: testpmd
  namespace: dpdk-test
spec:
  runtimeClassName: performance-performance <span id="CO128-3"><!--Empty--></span><span class="callout">3</span>
  containers:
    - command:
        - /bin/bash
        - -c
        - sleep INF
      image: registry.redhat.io/openshift4/dpdk-base-rhel8
      imagePullPolicy: Always
      name: dpdk
      resources: <span id="CO128-4"><!--Empty--></span><span class="callout">4</span>
        limits:
          cpu: "16"
          hugepages-1Gi: 8Gi
          memory: 2Gi
        requests:
          cpu: "16"
          hugepages-1Gi: 8Gi
          memory: 2Gi
      securityContext:
        capabilities:
          add:
            - IPC_LOCK
            - SYS_RESOURCE
            - NET_RAW
            - NET_ADMIN
        runAsUser: 0
      volumeMounts:
        - mountPath: /mnt/huge
          name: hugepages
  terminationGracePeriodSeconds: 5
  volumes:
    - emptyDir:
        medium: HugePages
      name: hugepages</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO128-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Request the SR-IOV networks you need. Resources for the devices will be injected automatically.
							</div></dd><dt><a href="#CO128-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Disable the CPU and IRQ load balancing base. See <span class="emphasis"><em><span class="cluster-admin cluster-admin">Disabling interrupt processing for individual pods</span></em></span> for more information.
							</div></dd><dt><a href="#CO128-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Set the <code class="literal cluster-admin">runtimeClass</code> to <code class="literal cluster-admin">performance-performance</code>. Do not set the <code class="literal cluster-admin">runtimeClass</code> to <code class="literal cluster-admin">HostNetwork</code> or <code class="literal cluster-admin">privileged</code>.
							</div></dd><dt><a href="#CO128-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Request an equal number of resources for requests and limits to start the pod with <code class="literal cluster-admin">Guaranteed</code> Quality of Service (QoS).
							</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							Do not start the pod with <code class="literal cluster-admin">SLEEP</code> and then exec into the pod to start the testpmd or the DPDK workload. This can add additional interrupts as the <code class="literal cluster-admin">exec</code> process is not pinned to any CPU.
						</p></div></div></section><section class="section cluster-admin" id="nw-sriov-dpdk-running-testpmd_using-dpdk-and-rdma"><div class="titlepage"><div><div><h4 class="title">25.10.4.4. Example testpmd script</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following is an example script for running <code class="literal cluster-admin">testpmd</code>:
					</p><pre class="programlisting language-terminal cluster-admin cluster-admin">#!/bin/bash
set -ex
export CPU=$(cat /sys/fs/cgroup/cpuset/cpuset.cpus)
echo ${CPU}

dpdk-testpmd -l ${CPU} -a ${PCIDEVICE_OPENSHIFT_IO_DPDK_NIC_1} -a ${PCIDEVICE_OPENSHIFT_IO_DPDK_NIC_2} -n 4 -- -i --nb-cores=15 --rxd=4096 --txd=4096 --rxq=7 --txq=7 --forward-mode=mac --eth-peer=0,50:00:00:00:00:01 --eth-peer=1,50:00:00:00:00:02</pre><p class="cluster-admin cluster-admin">
						This example uses two different <code class="literal cluster-admin">sriovNetwork</code> CRs. The environment variable contains the Virtual Function (VF) PCI address that was allocated for the pod. If you use the same network in the pod definition, you must split the <code class="literal cluster-admin">pciAddress</code>. It is important to configure the correct MAC addresses of the traffic generator. This example uses custom MAC addresses.
					</p></section></section><section class="section cluster-admin" id="example-vf-use-in-rdma-mode-mellanox_using-dpdk-and-rdma"><div class="titlepage"><div><div><h3 class="title">25.10.5. Using a virtual function in RDMA mode with a Mellanox NIC</h3></div></div></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						RDMA over Converged Ethernet (RoCE) is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
					</p><p class="cluster-admin cluster-admin">
						For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
					</p></div></div><p class="cluster-admin cluster-admin">
					RDMA over Converged Ethernet (RoCE) is the only supported mode when using RDMA on OpenShift Container Platform.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Install the SR-IOV Network Operator.
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create the following <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object, and then save the YAML in the <code class="literal cluster-admin">mlx-rdma-node-policy.yaml</code> file.
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: mlx-rdma-node-policy
  namespace: openshift-sriov-network-operator
spec:
  resourceName: mlxnics
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  priority: &lt;priority&gt;
  numVfs: &lt;num&gt;
  nicSelector:
    vendor: "15b3"
    deviceID: "1015" <span id="CO129-1"><!--Empty--></span><span class="callout">1</span>
    pfNames: ["&lt;pf_name&gt;", ...]
    rootDevices: ["&lt;pci_bus_id&gt;", "..."]
  deviceType: netdevice <span id="CO129-2"><!--Empty--></span><span class="callout">2</span>
  isRdma: true <span id="CO129-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO129-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the device hex code of the SR-IOV network device.
								</div></dd><dt><a href="#CO129-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the driver type for the virtual functions to <code class="literal cluster-admin">netdevice</code>.
								</div></dd><dt><a href="#CO129-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Enable RDMA mode.
								</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								See the <code class="literal cluster-admin">Configuring SR-IOV network devices</code> section for a detailed explanation on each option in <code class="literal cluster-admin">SriovNetworkNodePolicy</code>.
							</p><p class="cluster-admin cluster-admin">
								When applying the configuration specified in a <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object, the SR-IOV Operator may drain the nodes, and in some cases, reboot nodes. It may take several minutes for a configuration change to apply. Ensure that there are enough available nodes in your cluster to handle the evicted workload beforehand.
							</p><p class="cluster-admin cluster-admin">
								After the configuration update is applied, all the pods in the <code class="literal cluster-admin">openshift-sriov-network-operator</code> namespace will change to a <code class="literal cluster-admin">Running</code> status.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f mlx-rdma-node-policy.yaml</pre></li><li class="listitem"><p class="simpara">
							Create the following <code class="literal cluster-admin">SriovNetwork</code> object, and then save the YAML in the <code class="literal cluster-admin">mlx-rdma-network.yaml</code> file.
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: mlx-rdma-network
  namespace: openshift-sriov-network-operator
spec:
  networkNamespace: &lt;target_namespace&gt;
  ipam: |- <span id="CO130-1"><!--Empty--></span><span class="callout">1</span>
# ...
  vlan: &lt;vlan&gt;
  resourceName: mlxnics</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO130-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify a configuration object for the ipam CNI plugin as a YAML block scalar. The plugin manages IP address assignment for the attachment definition.
								</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								See the "Configuring SR-IOV additional network" section for a detailed explanation on each option in <code class="literal cluster-admin">SriovNetwork</code>.
							</p></div></div><p class="cluster-admin cluster-admin">
							An optional library, app-netutil, provides several API methods for gathering network information about a container’s parent pod.
						</p></li><li class="listitem"><p class="simpara">
							Create the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f mlx-rdma-network.yaml</pre></li><li class="listitem"><p class="simpara">
							Create the following <code class="literal cluster-admin">Pod</code> spec, and then save the YAML in the <code class="literal cluster-admin">mlx-rdma-pod.yaml</code> file.
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Pod
metadata:
  name: rdma-app
  namespace: &lt;target_namespace&gt; <span id="CO131-1"><!--Empty--></span><span class="callout">1</span>
  annotations:
    k8s.v1.cni.cncf.io/networks: mlx-rdma-network
spec:
  containers:
  - name: testpmd
    image: &lt;RDMA_image&gt; <span id="CO131-2"><!--Empty--></span><span class="callout">2</span>
    securityContext:
      runAsUser: 0
      capabilities:
        add: ["IPC_LOCK","SYS_RESOURCE","NET_RAW"] <span id="CO131-3"><!--Empty--></span><span class="callout">3</span>
    volumeMounts:
    - mountPath: /dev/hugepages <span id="CO131-4"><!--Empty--></span><span class="callout">4</span>
      name: hugepage
    resources:
      limits:
        memory: "1Gi"
        cpu: "4" <span id="CO131-5"><!--Empty--></span><span class="callout">5</span>
        hugepages-1Gi: "4Gi" <span id="CO131-6"><!--Empty--></span><span class="callout">6</span>
      requests:
        memory: "1Gi"
        cpu: "4"
        hugepages-1Gi: "4Gi"
    command: ["sleep", "infinity"]
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO131-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the same <code class="literal cluster-admin">target_namespace</code> where <code class="literal cluster-admin">SriovNetwork</code> object <code class="literal cluster-admin">mlx-rdma-network</code> is created. If you would like to create the pod in a different namespace, change <code class="literal cluster-admin">target_namespace</code> in both <code class="literal cluster-admin">Pod</code> spec and <code class="literal cluster-admin">SriovNetwork</code> object.
								</div></dd><dt><a href="#CO131-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the RDMA image which includes your application and RDMA library used by application.
								</div></dd><dt><a href="#CO131-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify additional capabilities required by the application inside the container for hugepage allocation, system resource allocation, and network interface access.
								</div></dd><dt><a href="#CO131-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Mount the hugepage volume to RDMA pod under <code class="literal cluster-admin">/dev/hugepages</code>. The hugepage volume is backed by the emptyDir volume type with the medium being <code class="literal cluster-admin">Hugepages</code>.
								</div></dd><dt><a href="#CO131-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Specify number of CPUs. The RDMA pod usually requires exclusive CPUs be allocated from the kubelet. This is achieved by setting CPU Manager policy to <code class="literal cluster-admin">static</code> and create pod with <code class="literal cluster-admin">Guaranteed</code> QoS.
								</div></dd><dt><a href="#CO131-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Specify hugepage size <code class="literal cluster-admin">hugepages-1Gi</code> or <code class="literal cluster-admin">hugepages-2Mi</code> and the quantity of hugepages that will be allocated to the RDMA pod. Configure <code class="literal cluster-admin">2Mi</code> and <code class="literal cluster-admin">1Gi</code> hugepages separately. Configuring <code class="literal cluster-admin">1Gi</code> hugepage requires adding kernel arguments to Nodes.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the RDMA pod by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f mlx-rdma-pod.yaml</pre></li></ol></div></section><section class="section cluster-admin" id="nw-openstack-ovs-dpdk-testpmd-pod_using-dpdk-and-rdma"><div class="titlepage"><div><div><h3 class="title">25.10.6. A test pod template for clusters that use OVS-DPDK on OpenStack</h3></div></div></div><p class="cluster-admin cluster-admin">
					The following <code class="literal cluster-admin">testpmd</code> pod demonstrates container creation with huge pages, reserved CPUs, and the SR-IOV port.
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>An example <code class="literal cluster-admin">testpmd</code> pod</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: testpmd-dpdk
  namespace: mynamespace
  annotations:
    cpu-load-balancing.crio.io: "disable"
    cpu-quota.crio.io: "disable"
# ...
spec:
  containers:
  - name: testpmd
    command: ["sleep", "99999"]
    image: registry.redhat.io/openshift4/dpdk-base-rhel8:v4.9
    securityContext:
      capabilities:
        add: ["IPC_LOCK","SYS_ADMIN"]
      privileged: true
      runAsUser: 0
    resources:
      requests:
        memory: 1000Mi
        hugepages-1Gi: 1Gi
        cpu: '2'
        openshift.io/dpdk1: 1 <span id="CO132-1"><!--Empty--></span><span class="callout">1</span>
      limits:
        hugepages-1Gi: 1Gi
        cpu: '2'
        memory: 1000Mi
        openshift.io/dpdk1: 1
    volumeMounts:
      - mountPath: /dev/hugepages
        name: hugepage
        readOnly: False
  runtimeClassName: performance-cnf-performanceprofile <span id="CO132-2"><!--Empty--></span><span class="callout">2</span>
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages</pre>

					</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO132-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The name <code class="literal cluster-admin">dpdk1</code> in this example is a user-created <code class="literal cluster-admin">SriovNetworkNodePolicy</code> resource. You can substitute this name for that of a resource that you create.
						</div></dd><dt><a href="#CO132-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							If your performance profile is not named <code class="literal cluster-admin">cnf-performance profile</code>, replace that string with the correct performance profile name.
						</div></dd></dl></div></section><section class="section cluster-admin" id="nw-openstack-hw-offload-testpmd-pod_using-dpdk-and-rdma"><div class="titlepage"><div><div><h3 class="title">25.10.7. A test pod template for clusters that use OVS hardware offloading on OpenStack</h3></div></div></div><p class="cluster-admin cluster-admin">
					The following <code class="literal cluster-admin">testpmd</code> pod demonstrates Open vSwitch (OVS) hardware offloading on Red Hat OpenStack Platform (RHOSP).
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>An example <code class="literal cluster-admin">testpmd</code> pod</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: testpmd-sriov
  namespace: mynamespace
  annotations:
    k8s.v1.cni.cncf.io/networks: hwoffload1
spec:
  runtimeClassName: performance-cnf-performanceprofile <span id="CO133-1"><!--Empty--></span><span class="callout">1</span>
  containers:
  - name: testpmd
    command: ["sleep", "99999"]
    image: registry.redhat.io/openshift4/dpdk-base-rhel8:v4.9
    securityContext:
      capabilities:
        add: ["IPC_LOCK","SYS_ADMIN"]
      privileged: true
      runAsUser: 0
    resources:
      requests:
        memory: 1000Mi
        hugepages-1Gi: 1Gi
        cpu: '2'
      limits:
        hugepages-1Gi: 1Gi
        cpu: '2'
        memory: 1000Mi
    volumeMounts:
      - mountPath: /dev/hugepages
        name: hugepage
        readOnly: False
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages</pre>

					</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO133-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							If your performance profile is not named <code class="literal cluster-admin">cnf-performance profile</code>, replace that string with the correct performance profile name.
						</div></dd></dl></div></section><section class="section _additional-resources" id="additional-resources_using-dpdk-and-rdma"><div class="titlepage"><div><div><h3 class="title">25.10.8. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#cnf-about-the-profile-creator-tool_cnf-create-performance-profiles">Creating a performance profile</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#adjusting-nic-queues-with-the-performance-profile_cnf-master">Reducing NIC queues using the Node Tuning Operator</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#node-tuning-operator-provisioning-worker-with-real-time-capabilities_cnf-master">Provisioning a worker with real-time capabilities</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#installing-sr-iov-operator_installing-sriov-operator">Installing the SR-IOV Network Operator</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-sriov-networknodepolicy-object_configuring-sriov-device">Configuring an SR-IOV network device</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-multus-whereabouts_configuring-additional-network">Dynamic IP address assignment configuration with Whereabouts</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/#disabling_interrupt_processing_for_individual_pods_cnf-master">Disabling interrupt processing for individual pods</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-sriov-net-attach">Configuring an SR-IOV Ethernet network attachment</a>
						</li><li class="listitem">
							The <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-sriov-app-netutil_about-sriov">app-netutil library</a> provides several API methods for gathering network information about a container’s parent pod.
						</li></ul></div></section></section><section class="section cluster-admin" id="using-pod-level-bonding"><div class="titlepage"><div><div><h2 class="title">25.11. Using pod-level bonding</h2></div></div></div><p class="cluster-admin cluster-admin">
				Bonding at the pod level is vital to enable workloads inside pods that require high availability and more throughput. With pod-level bonding, you can create a bond interface from multiple single root I/O virtualization (SR-IOV) virtual function interfaces in a kernel mode interface. The SR-IOV virtual functions are passed into the pod and attached to a kernel driver.
			</p><p class="cluster-admin cluster-admin">
				One scenario where pod level bonding is required is creating a bond interface from multiple SR-IOV virtual functions on different physical functions. Creating a bond interface from two different physical functions on the host can be used to achieve high availability and throughput at pod level.
			</p><p class="cluster-admin cluster-admin">
				For guidance on tasks such as creating a SR-IOV network, network policies, network attachment definitions and pods, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-sriov-device">Configuring an SR-IOV network device</a>.
			</p><section class="section cluster-admin" id="nw-sriov-cfg-bond-interface-with-virtual-functions_using-pod-level-bonding"><div class="titlepage"><div><div><h3 class="title">25.11.1. Configuring a bond interface from two SR-IOV interfaces</h3></div></div></div><p class="cluster-admin cluster-admin">
					Bonding enables multiple network interfaces to be aggregated into a single logical "bonded" interface. Bond Container Network Interface (Bond-CNI) brings bond capability into containers.
				</p><p class="cluster-admin cluster-admin">
					Bond-CNI can be created using Single Root I/O Virtualization (SR-IOV) virtual functions and placing them in the container network namespace.
				</p><p class="cluster-admin cluster-admin">
					OpenShift Container Platform only supports Bond-CNI using SR-IOV virtual functions. The SR-IOV Network Operator provides the SR-IOV CNI plugin needed to manage the virtual functions. Other CNIs or types of interfaces are not supported.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							The SR-IOV Network Operator must be installed and configured to obtain virtual functions in a container.
						</li><li class="listitem">
							To configure SR-IOV interfaces, an SR-IOV network and policy must be created for each interface.
						</li><li class="listitem">
							The SR-IOV Network Operator creates a network attachment definition for each SR-IOV interface, based on the SR-IOV network and policy defined.
						</li><li class="listitem">
							The <code class="literal cluster-admin">linkState</code> is set to the default value <code class="literal cluster-admin">auto</code> for the SR-IOV virtual function.
						</li></ul></div><section class="section cluster-admin" id="nw-sriov-cfg-creating-bond-network-attachment-definition_using-pod-level-bonding"><div class="titlepage"><div><div><h4 class="title">25.11.1.1. Creating a bond network attachment definition</h4></div></div></div><p class="cluster-admin cluster-admin">
						Now that the SR-IOV virtual functions are available, you can create a bond network attachment definition.
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: "k8s.cni.cncf.io/v1"
    kind: NetworkAttachmentDefinition
    metadata:
      name: bond-net1
      namespace: demo
    spec:
      config: '{
      "type": "bond", <span id="CO134-1"><!--Empty--></span><span class="callout">1</span>
      "cniVersion": "0.3.1",
      "name": "bond-net1",
      "mode": "active-backup", <span id="CO134-2"><!--Empty--></span><span class="callout">2</span>
      "failOverMac": 1, <span id="CO134-3"><!--Empty--></span><span class="callout">3</span>
      "linksInContainer": true, <span id="CO134-4"><!--Empty--></span><span class="callout">4</span>
      "miimon": "100",
      "mtu": 1500,
      "links": [ <span id="CO134-5"><!--Empty--></span><span class="callout">5</span>
            {"name": "net1"},
            {"name": "net2"}
        ],
      "ipam": {
            "type": "host-local",
            "subnet": "10.56.217.0/24",
            "routes": [{
            "dst": "0.0.0.0/0"
            }],
            "gateway": "10.56.217.1"
        }
      }'</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO134-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The cni-type is always set to <code class="literal cluster-admin">bond</code>.
							</div></dd><dt><a href="#CO134-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The <code class="literal cluster-admin">mode</code> attribute specifies the bonding mode.
							</div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
									The bonding modes supported are:
								</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">balance-rr</code> - 0
										</li><li class="listitem">
											<code class="literal cluster-admin">active-backup</code> - 1
										</li><li class="listitem">
											<code class="literal cluster-admin">balance-xor</code> - 2
										</li></ul></div><p class="cluster-admin cluster-admin">
									For <code class="literal cluster-admin">balance-rr</code> or <code class="literal cluster-admin">balance-xor</code> modes, you must set the <code class="literal cluster-admin">trust</code> mode to <code class="literal cluster-admin">on</code> for the SR-IOV virtual function.
								</p></div></div></dd><dt><a href="#CO134-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								The <code class="literal cluster-admin">failover</code> attribute is mandatory for active-backup mode and must be set to 1.
							</div></dd><dt><a href="#CO134-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								The <code class="literal cluster-admin">linksInContainer=true</code> flag informs the Bond CNI that the required interfaces are to be found inside the container. By default, Bond CNI looks for these interfaces on the host which does not work for integration with SRIOV and Multus.
							</div></dd><dt><a href="#CO134-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								The <code class="literal cluster-admin">links</code> section defines which interfaces will be used to create the bond. By default, Multus names the attached interfaces as: "net", plus a consecutive number, starting with one.
							</div></dd></dl></div></section><section class="section cluster-admin" id="nw-sriov-cfg-creating-pod-using-interface_using-pod-level-bonding"><div class="titlepage"><div><div><h4 class="title">25.11.1.2. Creating a pod using a bond interface</h4></div></div></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Test the setup by creating a pod with a YAML file named for example <code class="literal cluster-admin">podbonding.yaml</code> with content similar to the following:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
    kind: Pod
    metadata:
      name: bondpod1
      namespace: demo
      annotations:
        k8s.v1.cni.cncf.io/networks: demo/sriovnet1, demo/sriovnet2, demo/bond-net1 <span id="CO135-1"><!--Empty--></span><span class="callout">1</span>
    spec:
      containers:
      - name: podexample
        image: quay.io/openshift/origin-network-interface-bond-cni:4.11.0
        command: ["/bin/bash", "-c", "sleep INF"]</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO135-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Note the network annotation: it contains two SR-IOV network attachments, and one bond network attachment. The bond attachment uses the two SR-IOV interfaces as bonded port interfaces.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Apply the yaml by running the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f podbonding.yaml</pre></li><li class="listitem"><p class="simpara">
								Inspect the pod interfaces with the following command:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">$ oc rsh -n demo bondpod1
sh-4.4#
sh-4.4# ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
valid_lft forever preferred_lft forever
3: eth0@if150: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1450 qdisc noqueue state UP
link/ether 62:b1:b5:c8:fb:7a brd ff:ff:ff:ff:ff:ff
inet 10.244.1.122/24 brd 10.244.1.255 scope global eth0
valid_lft forever preferred_lft forever
4: net3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP400&gt; mtu 1500 qdisc noqueue state UP qlen 1000
link/ether 9e:23:69:42:fb:8a brd ff:ff:ff:ff:ff:ff <span id="CO136-1"><!--Empty--></span><span class="callout">1</span>
inet 10.56.217.66/24 scope global bond0
valid_lft forever preferred_lft forever
43: net1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP800&gt; mtu 1500 qdisc mq master bond0 state UP qlen 1000
link/ether 9e:23:69:42:fb:8a brd ff:ff:ff:ff:ff:ff <span id="CO136-2"><!--Empty--></span><span class="callout">2</span>
44: net2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP800&gt; mtu 1500 qdisc mq master bond0 state UP qlen 1000
link/ether 9e:23:69:42:fb:8a brd ff:ff:ff:ff:ff:ff <span id="CO136-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO136-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The bond interface is automatically named <code class="literal cluster-admin">net3</code>. To set a specific interface name add <code class="literal cluster-admin">@name</code> suffix to the pod’s <code class="literal cluster-admin">k8s.v1.cni.cncf.io/networks</code> annotation.
									</div></dd><dt><a href="#CO136-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The <code class="literal cluster-admin">net1</code> interface is based on an SR-IOV virtual function.
									</div></dd><dt><a href="#CO136-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The <code class="literal cluster-admin">net2</code> interface is based on an SR-IOV virtual function.
									</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
									If no interface names are configured in the pod annotation, interface names are assigned automatically as <code class="literal cluster-admin">net&lt;n&gt;</code>, with <code class="literal cluster-admin">&lt;n&gt;</code> starting at <code class="literal cluster-admin">1</code>.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Optional: If you want to set a specific interface name for example <code class="literal cluster-admin">bond0</code>, edit the <code class="literal cluster-admin">k8s.v1.cni.cncf.io/networks</code> annotation and set <code class="literal cluster-admin">bond0</code> as the interface name as follows:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">annotations:
        k8s.v1.cni.cncf.io/networks: demo/sriovnet1, demo/sriovnet2, demo/bond-net1@bond0</pre></li></ol></div></section></section></section><section class="section cluster-admin" id="configuring-hardware-offloading"><div class="titlepage"><div><div><h2 class="title">25.12. Configuring hardware offloading</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can configure hardware offloading on compatible nodes to increase data processing performance and reduce load on host CPUs.
			</p><section class="section cluster-admin" id="about-hardware-offloading_configuring-hardware-offloading"><div class="titlepage"><div><div><h3 class="title">25.12.1. About hardware offloading</h3></div></div></div><p class="cluster-admin cluster-admin">
					Open vSwitch hardware offloading is a method of processing network tasks by diverting them away from the CPU and offloading them to a dedicated processor on a network interface controller. As a result, clusters can benefit from faster data transfer speeds, reduced CPU workloads, and lower computing costs.
				</p><p class="cluster-admin cluster-admin">
					The key element for this feature is a modern class of network interface controllers known as SmartNICs. A SmartNIC is a network interface controller that is able to handle computationally-heavy network processing tasks. In the same way that a dedicated graphics card can improve graphics performance, a SmartNIC can improve network performance. In each case, a dedicated processor improves performance for a specific type of processing task.
				</p><p class="cluster-admin cluster-admin">
					In OpenShift Container Platform, you can configure hardware offloading for bare metal nodes that have a compatible SmartNIC. Hardware offloading is configured and enabled by the SR-IOV Network Operator.
				</p><p class="cluster-admin cluster-admin">
					Hardware offloading is not compatible with all workloads or application types. Only the following two communication types are supported:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							pod-to-pod
						</li><li class="listitem">
							pod-to-service, where the service is a ClusterIP service backed by a regular pod
						</li></ul></div><p class="cluster-admin cluster-admin">
					In all cases, hardware offloading takes place only when those pods and services are assigned to nodes that have a compatible SmartNIC. Suppose, for example, that a pod on a node with hardware offloading tries to communicate with a service on a regular node. On the regular node, all the processing takes place in the kernel, so the overall performance of the pod-to-service communication is limited to the maximum performance of that regular node. Hardware offloading is not compatible with DPDK applications.
				</p><p class="cluster-admin cluster-admin">
					Enabling hardware offloading on a node, but not configuring pods to use, it can result in decreased throughput performance for pod traffic. You cannot configure hardware offloading for pods that are managed by OpenShift Container Platform.
				</p></section><section class="section cluster-admin" id="supported_devices_configuring-hardware-offloading"><div class="titlepage"><div><div><h3 class="title">25.12.2. Supported devices</h3></div></div></div><p class="cluster-admin cluster-admin">
					Hardware offloading is supported on the following network interface controllers:
				</p><div class="table" id="idm140587124227232"><p class="title"><strong>Table 25.15. Supported network interface controllers</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 40%; " class="col_2"><!--Empty--></col><col style="width: 20%; " class="col_3"><!--Empty--></col><col style="width: 20%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587124220224" scope="col">Manufacturer</th><th align="left" valign="top" id="idm140587104576240" scope="col">Model</th><th align="left" valign="top" id="idm140587104575152" scope="col">Vendor ID</th><th align="left" valign="top" id="idm140587104574064" scope="col">Device ID</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587124220224"> <p>
									Mellanox
								</p>
								 </td><td align="left" valign="top" headers="idm140587104576240"> <p>
									MT27800 Family [ConnectX‑5]
								</p>
								 </td><td align="left" valign="top" headers="idm140587104575152"> <p>
									15b3
								</p>
								 </td><td align="left" valign="top" headers="idm140587104574064"> <p>
									1017
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124220224"> <p>
									Mellanox
								</p>
								 </td><td align="left" valign="top" headers="idm140587104576240"> <p>
									MT28880 Family [ConnectX‑5 Ex]
								</p>
								 </td><td align="left" valign="top" headers="idm140587104575152"> <p>
									15b3
								</p>
								 </td><td align="left" valign="top" headers="idm140587104574064"> <p>
									1019
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124220224"> <p>
									Mellanox
								</p>
								 </td><td align="left" valign="top" headers="idm140587104576240"> <p>
									MT2892 Family [ConnectX‑6 Dx]
								</p>
								 </td><td align="left" valign="top" headers="idm140587104575152"> <p>
									15b3
								</p>
								 </td><td align="left" valign="top" headers="idm140587104574064"> <p>
									101d
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124220224"> <p>
									Mellanox
								</p>
								 </td><td align="left" valign="top" headers="idm140587104576240"> <p>
									MT2894 Family [ConnectX-6 Lx]
								</p>
								 </td><td align="left" valign="top" headers="idm140587104575152"> <p>
									15b3
								</p>
								 </td><td align="left" valign="top" headers="idm140587104574064"> <p>
									101f
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124220224"> <p>
									Mellanox
								</p>
								 </td><td align="left" valign="top" headers="idm140587104576240"> <p>
									MT42822 BlueField-2 in ConnectX-6 NIC mode
								</p>
								 </td><td align="left" valign="top" headers="idm140587104575152"> <p>
									15b3
								</p>
								 </td><td align="left" valign="top" headers="idm140587104574064"> <p>
									a2d6
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section cluster-admin" id="configuring-hardware-offloading-prerequisites"><div class="titlepage"><div><div><h3 class="title">25.12.3. Prerequisites</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Your cluster has at least one bare metal machine with a network interface controller that is supported for hardware offloading.
						</li><li class="listitem">
							You <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#installing-sr-iov-operator_installing-sriov-operator">installed the SR-IOV Network Operator</a>.
						</li><li class="listitem">
							Your cluster uses the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-ovn-kubernetes">OVN-Kubernetes network plugin</a>.
						</li><li class="listitem">
							In your <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#gatewayConfig-object_cluster-network-operator">OVN-Kubernetes network plugin configuration</a>, the <code class="literal cluster-admin">gatewayConfig.routingViaHost</code> field is set to <code class="literal cluster-admin">false</code>.
						</li></ul></div></section><section class="section cluster-admin" id="configuring-machine-config-pool_configuring-hardware-offloading"><div class="titlepage"><div><div><h3 class="title">25.12.4. Configuring a machine config pool for hardware offloading</h3></div></div></div><p class="cluster-admin cluster-admin">
					To enable hardware offloading, you must first create a dedicated machine config pool and configure it to work with the SR-IOV Network Operator.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a machine config pool for machines you want to use hardware offloading on.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a file, such as <code class="literal cluster-admin">mcp-offloading.yaml</code>, with content like the following example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: mcp-offloading <span id="CO137-1"><!--Empty--></span><span class="callout">1</span>
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,mcp-offloading]} <span id="CO137-2"><!--Empty--></span><span class="callout">2</span>
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/mcp-offloading: "" <span id="CO137-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO137-1"><span class="callout">1</span></a> <a href="#CO137-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											The name of your machine config pool for hardware offloading.
										</div></dd><dt><a href="#CO137-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											This node role label is used to add nodes to the machine config pool.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Apply the configuration for the machine config pool:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f mcp-offloading.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Add nodes to the machine config pool. Label each node with the node role label of your pool:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc label node worker-2 node-role.kubernetes.io/mcp-offloading=""</pre></li><li class="listitem"><p class="simpara">
							Optional: To verify that the new pool is created, run the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get nodes</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME       STATUS   ROLES                   AGE   VERSION
master-0   Ready    master                  2d    v1.26.0
master-1   Ready    master                  2d    v1.26.0
master-2   Ready    master                  2d    v1.26.0
worker-0   Ready    worker                  2d    v1.26.0
worker-1   Ready    worker                  2d    v1.26.0
worker-2   Ready    mcp-offloading,worker   47h   v1.26.0
worker-3   Ready    mcp-offloading,worker   47h   v1.26.0</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Add this machine config pool to the <code class="literal cluster-admin">SriovNetworkPoolConfig</code> custom resource:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a file, such as <code class="literal cluster-admin">sriov-pool-config.yaml</code>, with content like the following example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkPoolConfig
metadata:
  name: sriovnetworkpoolconfig-offload
  namespace: openshift-sriov-network-operator
spec:
  ovsHardwareOffloadConfig:
    name: mcp-offloading <span id="CO138-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO138-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											The name of your machine config pool for hardware offloading.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Apply the configuration:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f &lt;SriovNetworkPoolConfig_name&gt;.yaml</pre><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
										When you apply the configuration specified in a <code class="literal cluster-admin">SriovNetworkPoolConfig</code> object, the SR-IOV Operator drains and restarts the nodes in the machine config pool.
									</p><p class="cluster-admin cluster-admin">
										It might take several minutes for a configuration changes to apply.
									</p></div></div></li></ol></div></li></ol></div></section><section class="section cluster-admin" id="configure-sriov-node-policy_configuring-hardware-offloading"><div class="titlepage"><div><div><h3 class="title">25.12.5. Configuring the SR-IOV network node policy</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can create an SR-IOV network device configuration for a node by creating an SR-IOV network node policy. To enable hardware offloading, you must define the <code class="literal cluster-admin">.spec.eSwitchMode</code> field with the value <code class="literal cluster-admin">"switchdev"</code>.
				</p><p class="cluster-admin cluster-admin">
					The following procedure creates an SR-IOV interface for a network interface controller with hardware offloading.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a file, such as <code class="literal cluster-admin">sriov-node-policy.yaml</code>, with content like the following example:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: sriov-node-policy &lt;.&gt;
  namespace: openshift-sriov-network-operator
spec:
  deviceType: netdevice &lt;.&gt;
  eSwitchMode: "switchdev" &lt;.&gt;
  nicSelector:
    deviceID: "1019"
    rootDevices:
    - 0000:d8:00.0
    vendor: "15b3"
    pfNames:
    - ens8f0
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  numVfs: 6
  priority: 5
  resourceName: mlxnics</pre><p class="cluster-admin cluster-admin">
							&lt;.&gt; The name for the custom resource object. &lt;.&gt; Required. Hardware offloading is not supported with <code class="literal cluster-admin">vfio-pci</code>. &lt;.&gt; Required.
						</p></li><li class="listitem"><p class="simpara">
							Apply the configuration for the policy:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f sriov-node-policy.yaml</pre><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								When you apply the configuration specified in a <code class="literal cluster-admin">SriovNetworkPoolConfig</code> object, the SR-IOV Operator drains and restarts the nodes in the machine config pool.
							</p><p class="cluster-admin cluster-admin">
								It might take several minutes for a configuration change to apply.
							</p></div></div></li></ol></div><section class="section cluster-admin" id="nw-sriov-hwol-ref-openstack-sriov-policy_configuring-hardware-offloading"><div class="titlepage"><div><div><h4 class="title">25.12.5.1. An example SR-IOV network node policy for OpenStack</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following example describes an SR-IOV interface for a network interface controller (NIC) with hardware offloading on Red Hat OpenStack Platform (RHOSP).
					</p><div class="cluster-admin cluster-admin"><p class="title"><strong>An SR-IOV interface for a NIC with hardware offloading on RHOSP</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: ${name}
  namespace: openshift-sriov-network-operator
spec:
  deviceType: switchdev
  isRdma: true
  nicSelector:
    netFilter: openstack/NetworkID:${net_id}
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: 'true'
  numVfs: 1
  priority: 99
  resourceName: ${name}</pre>

						</p></div></section></section><section class="section cluster-admin" id="create-network-attachment-definition_configuring-hardware-offloading"><div class="titlepage"><div><div><h3 class="title">25.12.6. Creating a network attachment definition</h3></div></div></div><p class="cluster-admin cluster-admin">
					After you define the machine config pool and the SR-IOV network node policy, you can create a network attachment definition for the network interface card you specified.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a file, such as <code class="literal cluster-admin">net-attach-def.yaml</code>, with content like the following example:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: net-attach-def &lt;.&gt;
  namespace: net-attach-def &lt;.&gt;
  annotations:
    k8s.v1.cni.cncf.io/resourceName: openshift.io/mlxnics &lt;.&gt;
spec:
  config: '{"cniVersion":"0.3.1","name":"ovn-kubernetes","type":"ovn-k8s-cni-overlay","ipam":{},"dns":{}}'</pre><p class="cluster-admin cluster-admin">
							&lt;.&gt; The name for your network attachment definition. &lt;.&gt; The namespace for your network attachment definition. &lt;.&gt; This is the value of the <code class="literal cluster-admin">spec.resourceName</code> field you specified in the <code class="literal cluster-admin">SriovNetworkNodePolicy</code> object.
						</p></li><li class="listitem"><p class="simpara">
							Apply the configuration for the network attachment definition:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f net-attach-def.yaml</pre></li></ol></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Run the following command to see whether the new definition is present:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get net-attach-def -A</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAMESPACE         NAME             AGE
net-attach-def    net-attach-def   43h</pre>

							</p></div></li></ul></div></section><section class="section cluster-admin" id="adding-network-attachment-definition-to-pods_configuring-hardware-offloading"><div class="titlepage"><div><div><h3 class="title">25.12.7. Adding the network attachment definition to your pods</h3></div></div></div><p class="cluster-admin cluster-admin">
					After you create the machine config pool, the <code class="literal cluster-admin">SriovNetworkPoolConfig</code> and <code class="literal cluster-admin">SriovNetworkNodePolicy</code> custom resources, and the network attachment definition, you can apply these configurations to your pods by adding the network attachment definition to your pod specifications.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							In the pod specification, add the <code class="literal cluster-admin">.metadata.annotations.k8s.v1.cni.cncf.io/networks</code> field and specify the network attachment definition you created for hardware offloading:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">....
metadata:
  annotations:
    v1.multus-cni.io/default-network: net-attach-def/net-attach-def &lt;.&gt;</pre><p class="cluster-admin cluster-admin">
							&lt;.&gt; The value must be the name and namespace of the network attachment definition you created for hardware offloading.
						</p></li></ul></div></section></section><section class="section cluster-admin" id="switching-bf2-nic-dpu"><div class="titlepage"><div><div><h2 class="title">25.13. Switching Bluefield-2 from DPU to NIC</h2></div></div></div><p class="cluster-admin cluster-admin">
				You can switch the Bluefield-2 network device from data processing unit (DPU) mode to network interface controller (NIC) mode.
			</p><section class="section cluster-admin" id="proc-switching-bf2-nic_switching-bf2-nic-dpu"><div class="titlepage"><div><div><h3 class="title">25.13.1. Switching Bluefield-2 from DPU mode to NIC mode</h3></div></div></div><p class="cluster-admin cluster-admin">
					Use the following procedure to switch Bluefield-2 from data processing units (DPU) mode to network interface controller (NIC) mode.
				</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						Currently, only switching Bluefield-2 from DPU to NIC mode is supported. Switching from NIC mode to DPU mode is unsupported.
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have installed the SR-IOV Network Operator. For more information, see "Installing SR-IOV Network Operator".
						</li><li class="listitem">
							You have updated Bluefield-2 to the latest firmware. For more information, see <a class="link" href="https://network.nvidia.com/support/firmware/bluefield2/">Firmware for NVIDIA BlueField-2</a>.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Add the following labels to each of your worker nodes by entering the following commands:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc label node &lt;example_node_name_one&gt; node-role.kubernetes.io/sriov=</pre><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc label node &lt;example_node_name_two&gt; node-role.kubernetes.io/sriov=</pre></li><li class="listitem"><p class="simpara">
							Create a machine config pool for the SR-IOV Network Operator, for example:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: sriov
spec:
  machineConfigSelector:
    matchExpressions:
    - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,sriov]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/sriov: ""</pre></li><li class="listitem"><p class="simpara">
							Apply the following <code class="literal cluster-admin">machineconfig.yaml</code> file to the worker nodes:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: sriov
  name: 99-bf2-dpu
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
      - contents:
          source: data:text/plain;charset=utf-8;base64,ZmluZF9jb250YWluZXIoKSB7CiAgY3JpY3RsIHBzIC1vIGpzb24gfCBqcSAtciAnLmNvbnRhaW5lcnNbXSB8IHNlbGVjdCgubWV0YWRhdGEubmFtZT09InNyaW92LW5ldHdvcmstY29uZmlnLWRhZW1vbiIpIHwgLmlkJwp9CnVudGlsIG91dHB1dD0kKGZpbmRfY29udGFpbmVyKTsgW1sgLW4gIiRvdXRwdXQiIF1dOyBkbwogIGVjaG8gIndhaXRpbmcgZm9yIGNvbnRhaW5lciB0byBjb21lIHVwIgogIHNsZWVwIDE7CmRvbmUKISBzdWRvIGNyaWN0bCBleGVjICRvdXRwdXQgL2JpbmRhdGEvc2NyaXB0cy9iZjItc3dpdGNoLW1vZGUuc2ggIiRAIgo=
        mode: 0755
        overwrite: true
        path: /etc/default/switch_in_sriov_config_daemon.sh
    systemd:
      units:
      - name: dpu-switch.service
        enabled: true
        contents: |
          [Unit]
          Description=Switch BlueField2 card to NIC/DPU mode
          RequiresMountsFor=%t/containers
          Wants=network.target
          After=network-online.target kubelet.service
          [Service]
          SuccessExitStatus=0 120
          RemainAfterExit=True
          ExecStart=/bin/bash -c '/etc/default/switch_in_sriov_config_daemon.sh nic || shutdown -r now' <span id="CO139-1"><!--Empty--></span><span class="callout">1</span>
          Type=oneshot
          [Install]
          WantedBy=multi-user.target</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO139-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Optional: The PCI address of a specific card can optionally be specified, for example <code class="literal cluster-admin">ExecStart=/bin/bash -c '/etc/default/switch_in_sriov_config_daemon.sh nic 0000:5e:00.0 || echo done'</code>. By default, the first device is selected. If there is more than one device, you must specify which PCI address to be used. The PCI address must be the same on all nodes that are switching Bluefield-2 from DPU mode to NIC mode.
								</div></dd></dl></div></li><li class="listitem">
							Wait for the worker nodes to restart. After restarting, the Bluefield-2 network device on the worker nodes is switched into NIC mode.
						</li></ol></div><div class="_additional-resources _additional-resources"><p class="title"><strong>Additional resources</strong></p><p>
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#installing-sr-iov-operator_installing-sriov-operator">Installing SR-IOV Network Operator</a>
					</p></div></section></section><section class="section cluster-admin" id="uninstalling-sriov-operator"><div class="titlepage"><div><div><h2 class="title">25.14. Uninstalling the SR-IOV Network Operator</h2></div></div></div><p class="cluster-admin cluster-admin">
				To uninstall the SR-IOV Network Operator, you must delete any running SR-IOV workloads, uninstall the Operator, and delete the webhooks that the Operator used.
			</p><section class="section cluster-admin" id="nw-sriov-operator-uninstall_uninstalling-sr-iov-operator"><div class="titlepage"><div><div><h3 class="title">25.14.1. Uninstalling the SR-IOV Network Operator</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can uninstall the SR-IOV Network Operator.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have access to an OpenShift Container Platform cluster using an account with <code class="literal cluster-admin">cluster-admin</code> permissions.
						</li><li class="listitem">
							You have the SR-IOV Network Operator installed.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Delete all SR-IOV custom resources (CRs):
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete sriovnetwork -n openshift-sriov-network-operator --all</pre><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete sriovnetworknodepolicy -n openshift-sriov-network-operator --all</pre><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete sriovibnetwork -n openshift-sriov-network-operator --all</pre></li><li class="listitem">
							Follow the instructions in the "Deleting Operators from a cluster" section to remove the SR-IOV Network Operator from your cluster.
						</li><li class="listitem"><p class="simpara">
							Delete the SR-IOV custom resource definitions that remain in the cluster after the SR-IOV Network Operator is uninstalled:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete crd sriovibnetworks.sriovnetwork.openshift.io</pre><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete crd sriovnetworknodepolicies.sriovnetwork.openshift.io</pre><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete crd sriovnetworknodestates.sriovnetwork.openshift.io</pre><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete crd sriovnetworkpoolconfigs.sriovnetwork.openshift.io</pre><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete crd sriovnetworks.sriovnetwork.openshift.io</pre><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete crd sriovoperatorconfigs.sriovnetwork.openshift.io</pre></li><li class="listitem"><p class="simpara">
							Delete the SR-IOV webhooks:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete mutatingwebhookconfigurations network-resources-injector-config</pre><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete MutatingWebhookConfiguration sriov-operator-webhook-config</pre><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete ValidatingWebhookConfiguration sriov-operator-webhook-config</pre></li><li class="listitem"><p class="simpara">
							Delete the SR-IOV Network Operator namespace:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete namespace openshift-sriov-network-operator</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-deleting-operators-from-a-cluster">Deleting Operators from a cluster</a>
						</li></ul></div></section></section></section><section class="chapter cluster-admin" id="ovn-kubernetes-network-plugin"><div class="titlepage"><div><div><h1 class="title">Chapter 26. OVN-Kubernetes network plugin</h1></div></div></div><section class="section cluster-admin" id="about-ovn-kubernetes"><div class="titlepage"><div><div><h2 class="title">26.1. About the OVN-Kubernetes network plugin</h2></div></div></div><p class="cluster-admin cluster-admin">
				The OpenShift Container Platform cluster uses a virtualized network for pod and service networks.
			</p><p class="cluster-admin cluster-admin">
				Part of Red Hat OpenShift Networking, the OVN-Kubernetes network plugin is the default network provider for OpenShift Container Platform. OVN-Kubernetes is based on Open Virtual Network (OVN) and provides an overlay-based networking implementation. A cluster that uses the OVN-Kubernetes plugin also runs Open vSwitch (OVS) on each node. OVN configures OVS on each node to implement the declared network configuration.
			</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
					OVN-Kubernetes is the default networking solution for OpenShift Container Platform and single-node OpenShift deployments.
				</p></div></div><p class="cluster-admin cluster-admin">
				OVN-Kubernetes, which arose from the OVS project, uses many of the same constructs, such as open flow rules, to determine how packets travel through the network. For more information, see the <a class="link" href="https://www.ovn.org/en/">Open Virtual Network website</a>.
			</p><p class="cluster-admin cluster-admin">
				OVN-Kubernetes is a series of daemons for OVS that translate virtual network configurations into <code class="literal cluster-admin">OpenFlow</code> rules. <code class="literal cluster-admin">OpenFlow</code> is a protocol for communicating with network switches and routers, providing a means for remotely controlling the flow of network traffic on a network device, allowing network administrators to configure, manage, and monitor the flow of network traffic.
			</p><p class="cluster-admin cluster-admin">
				OVN-Kubernetes provides more of the advanced functionality not available with <code class="literal cluster-admin">OpenFlow</code>. OVN supports distributed virtual routing, distributed logical switches, access control, DHCP and DNS. OVN implements distributed virtual routing within logic flows which equate to open flows. So for example if you have a pod that sends out a DHCP request on the network, it sends out that broadcast looking for DHCP address there will be a logic flow rule that matches that packet, and it responds giving it a gateway, a DNS server an IP address and so on.
			</p><p class="cluster-admin cluster-admin">
				OVN-Kubernetes runs a daemon on each node. There are daemon sets for the databases and for the OVN controller that run on every node. The OVN controller programs the Open vSwitch daemon on the nodes to support the network provider features; egress IPs, firewalls, routers, hybrid networking, IPSEC encryption, IPv6, network policy, network policy logs, hardware offloading and multicast.
			</p><section class="section cluster-admin" id="nw-ovn-kubernetes-purpose_about-ovn-kubernetes"><div class="titlepage"><div><div><h3 class="title">26.1.1. OVN-Kubernetes purpose</h3></div></div></div><p class="cluster-admin cluster-admin">
					The OVN-Kubernetes network plugin is an open-source, fully-featured Kubernetes CNI plugin that uses Open Virtual Network (OVN) to manage network traffic flows. OVN is a community developed, vendor-agnostic network virtualization solution. The OVN-Kubernetes network plugin:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Uses OVN (Open Virtual Network) to manage network traffic flows. OVN is a community developed, vendor-agnostic network virtualization solution.
						</li><li class="listitem">
							Implements Kubernetes network policy support, including ingress and egress rules.
						</li><li class="listitem">
							Uses the Geneve (Generic Network Virtualization Encapsulation) protocol rather than VXLAN to create an overlay network between nodes.
						</li></ul></div><p class="cluster-admin cluster-admin">
					The OVN-Kubernetes network plugin provides the following advantages over OpenShift SDN.
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Full support for IPv6 single-stack and IPv4/IPv6 dual-stack networking on supported platforms
						</li><li class="listitem">
							Support for hybrid clusters with both Linux and Microsoft Windows workloads
						</li><li class="listitem">
							Optional IPsec encryption of intra-cluster communications
						</li><li class="listitem">
							Offload of network data processing from host CPU to compatible network cards and data processing units (DPUs)
						</li></ul></div></section><section class="section cluster-admin" id="nw-ovn-kubernetes-matrix_about-ovn-kubernetes"><div class="titlepage"><div><div><h3 class="title">26.1.2. Supported network plugin feature matrix</h3></div></div></div><p class="cluster-admin cluster-admin">
					Red Hat OpenShift Networking offers two options for the network plugin, OpenShift SDN and OVN-Kubernetes, for the network plugin. The following table summarizes the current feature support for both network plugins:
				</p><div class="table" id="idm140587108827552"><p class="title"><strong>Table 26.1. Default CNI network plugin feature comparison</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587124279136" scope="col">Feature</th><th align="left" valign="top" id="idm140587124278048" scope="col">OVN-Kubernetes</th><th align="left" valign="top" id="idm140587124276960" scope="col">OpenShift SDN</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587124279136"> <p>
									Egress IPs
								</p>
								 </td><td align="left" valign="top" headers="idm140587124278048"> <p>
									Supported
								</p>
								 </td><td align="left" valign="top" headers="idm140587124276960"> <p>
									Supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124279136"> <p>
									Egress firewall <sup>[1]</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124278048"> <p>
									Supported
								</p>
								 </td><td align="left" valign="top" headers="idm140587124276960"> <p>
									Supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124279136"> <p>
									Egress router
								</p>
								 </td><td align="left" valign="top" headers="idm140587124278048"> <p>
									Supported <sup>[2]</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124276960"> <p>
									Supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124279136"> <p>
									Hybrid networking
								</p>
								 </td><td align="left" valign="top" headers="idm140587124278048"> <p>
									Supported
								</p>
								 </td><td align="left" valign="top" headers="idm140587124276960"> <p>
									Not supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124279136"> <p>
									IPsec encryption for intra-cluster communication
								</p>
								 </td><td align="left" valign="top" headers="idm140587124278048"> <p>
									Supported
								</p>
								 </td><td align="left" valign="top" headers="idm140587124276960"> <p>
									Not supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124279136"> <p>
									IPv6
								</p>
								 </td><td align="left" valign="top" headers="idm140587124278048"> <p>
									Supported <sup>[3]</sup> <sup>[4]</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124276960"> <p>
									Not supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124279136"> <p>
									Kubernetes network policy
								</p>
								 </td><td align="left" valign="top" headers="idm140587124278048"> <p>
									Supported
								</p>
								 </td><td align="left" valign="top" headers="idm140587124276960"> <p>
									Supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124279136"> <p>
									Kubernetes network policy logs
								</p>
								 </td><td align="left" valign="top" headers="idm140587124278048"> <p>
									Supported
								</p>
								 </td><td align="left" valign="top" headers="idm140587124276960"> <p>
									Not supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124279136"> <p>
									Hardware offloading
								</p>
								 </td><td align="left" valign="top" headers="idm140587124278048"> <p>
									Supported
								</p>
								 </td><td align="left" valign="top" headers="idm140587124276960"> <p>
									Not supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124279136"> <p>
									Multicast
								</p>
								 </td><td align="left" valign="top" headers="idm140587124278048"> <p>
									Supported
								</p>
								 </td><td align="left" valign="top" headers="idm140587124276960"> <p>
									Supported
								</p>
								 </td></tr></tbody></table></div></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Egress firewall is also known as egress network policy in OpenShift SDN. This is not the same as network policy egress.
						</li><li class="listitem">
							Egress router for OVN-Kubernetes supports only redirect mode.
						</li><li class="listitem">
							IPv6 is supported only on bare metal, IBM Power, and IBM Z clusters.
						</li><li class="listitem">
							IPv6 single stack does not support <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#k8s-nmstate-about-the-k8s-nmstate-operator">Kubernetes NMState</a>.
						</li></ol></div></section><section class="section cluster-admin" id="nw-ovn-kubernetes-limitations_about-ovn-kubernetes"><div class="titlepage"><div><div><h3 class="title">26.1.3. OVN-Kubernetes IPv6 and dual-stack limitations</h3></div></div></div><p class="cluster-admin cluster-admin">
					The OVN-Kubernetes network plugin has the following limitations:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							For clusters configured for dual-stack networking, both IPv4 and IPv6 traffic must use the same network interface as the default gateway. If this requirement is not met, pods on the host in the <code class="literal cluster-admin">ovnkube-node</code> daemon set enter the <code class="literal cluster-admin">CrashLoopBackOff</code> state. If you display a pod with a command such as <code class="literal cluster-admin">oc get pod -n openshift-ovn-kubernetes -l app=ovnkube-node -o yaml</code>, the <code class="literal cluster-admin">status</code> field contains more than one message about the default gateway, as shown in the following output:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">I1006 16:09:50.985852   60651 helper_linux.go:73] Found default gateway interface br-ex 192.168.127.1
I1006 16:09:50.985923   60651 helper_linux.go:73] Found default gateway interface ens4 fe80::5054:ff:febe:bcd4
F1006 16:09:50.985939   60651 ovnkube.go:130] multiple gateway interfaces detected: br-ex ens4</pre><p class="cluster-admin cluster-admin">
							The only resolution is to reconfigure the host networking so that both IP families use the same network interface for the default gateway.
						</p></li><li class="listitem"><p class="simpara">
							For clusters configured for dual-stack networking, both the IPv4 and IPv6 routing tables must contain the default gateway. If this requirement is not met, pods on the host in the <code class="literal cluster-admin">ovnkube-node</code> daemon set enter the <code class="literal cluster-admin">CrashLoopBackOff</code> state. If you display a pod with a command such as <code class="literal cluster-admin">oc get pod -n openshift-ovn-kubernetes -l app=ovnkube-node -o yaml</code>, the <code class="literal cluster-admin">status</code> field contains more than one message about the default gateway, as shown in the following output:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">I0512 19:07:17.589083  108432 helper_linux.go:74] Found default gateway interface br-ex 192.168.123.1
F0512 19:07:17.589141  108432 ovnkube.go:133] failed to get default gateway interface</pre><p class="cluster-admin cluster-admin">
							The only resolution is to reconfigure the host networking so that both IP families contain the default gateway.
						</p></li></ul></div></section><section class="section cluster-admin" id="nw-ovn-kubernetes-session-affinity_about-ovn-kubernetes"><div class="titlepage"><div><div><h3 class="title">26.1.4. Session affinity</h3></div></div></div><p class="cluster-admin cluster-admin">
					Session affinity is a feature that applies to Kubernetes <code class="literal cluster-admin">Service</code> objects. You can use <span class="emphasis"><em><span class="cluster-admin cluster-admin">session affinity</span></em></span> if you want to ensure that each time you connect to a &lt;service_VIP&gt;:&lt;Port&gt;, the traffic is always load balanced to the same back end. For more information, including how to set session affinity based on a client’s IP address, see <a class="link" href="https://kubernetes.io/docs/reference/networking/virtual-ips/#session-affinity">Session affinity</a>.
				</p><h5 id="nw-ovn-kubernetes-session-affinity-stickyness-timeout_about-ovn-kubernetes">Stickiness timeout for session affinity</h5><p class="cluster-admin cluster-admin">
					The OVN-Kubernetes network plugin for OpenShift Container Platform calculates the stickiness timeout for a session from a client based on the last packet. For example, if you run a <code class="literal cluster-admin">curl</code> command 10 times, the sticky session timer starts from the tenth packet not the first. As a result, if the client is continuously contacting the service, then the session never times out. The timeout starts when the service has not received a packet for the amount of time set by the <a class="link" href="https://kubernetes.io/docs/reference/networking/virtual-ips/#session-stickiness-timeout"><code class="literal cluster-admin">timeoutSeconds</code></a> parameter.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-egress-firewall-ovn">Configuring an egress firewall for a project</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-network-policy">About network policy</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#logging-network-policy">Logging network policy events</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-ovn-kubernetes-enabling-multicast">Enabling multicast for a project</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-ipsec-ovn">Configuring IPsec encryption</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/api_reference/#network-operator-openshift-io-v1">Network [operator.openshift.io/v1]</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="ovn-kubernetes-architecture-assembly"><div class="titlepage"><div><div><h2 class="title">26.2. OVN-Kubernetes architecture</h2></div></div></div><section class="section cluster-admin" id="ovn-kubernetes-architecture-con"><div class="titlepage"><div><div><h3 class="title">26.2.1. Introduction to OVN-Kubernetes architecture</h3></div></div></div><p class="cluster-admin cluster-admin">
					The following diagram shows the OVN-Kubernetes architecture.
				</p><div class="figure" id="idm140587105845824"><p class="title"><strong>Figure 26.1. OVK-Kubernetes architecture</strong></p><div class="figure-contents"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/1c97fb68c22c51f69b962e6d3a64a64f/299_OpenShift_OVN-Kubernetes_arch_0223_1.png" alt="OVN-Kubernetes architecture"/></div></div></div><p class="cluster-admin cluster-admin">
					The key components are:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Cloud Management System (CMS)</span></strong></span> - A platform specific client for OVN that provides a CMS specific plugin for OVN integration. The plugin translates the cloud management system’s concept of the logical network configuration, stored in the CMS configuration database in a CMS-specific format, into an intermediate representation understood by OVN.
						</li><li class="listitem">
							<span class="strong strong"><strong><span class="cluster-admin cluster-admin">OVN Northbound database (<code class="literal cluster-admin">nbdb</code>)</span></strong></span> - Stores the logical network configuration passed by the CMS plugin.
						</li><li class="listitem">
							<span class="strong strong"><strong><span class="cluster-admin cluster-admin">OVN Southbound database (<code class="literal cluster-admin">sbdb</code>)</span></strong></span> - Stores the physical and logical network configuration state for OpenVswitch (OVS) system on each node, including tables that bind them.
						</li><li class="listitem">
							<span class="strong strong"><strong><span class="cluster-admin cluster-admin">ovn-northd</span></strong></span> - This is the intermediary client between <code class="literal cluster-admin">nbdb</code> and <code class="literal cluster-admin">sbdb</code>. It translates the logical network configuration in terms of conventional network concepts, taken from the <code class="literal cluster-admin">nbdb</code>, into logical data path flows in the <code class="literal cluster-admin">sbdb</code> below it. The container name is <code class="literal cluster-admin">northd</code> and it runs in the <code class="literal cluster-admin">ovnkube-master</code> pods.
						</li><li class="listitem">
							<span class="strong strong"><strong><span class="cluster-admin cluster-admin">ovn-controller</span></strong></span> - This is the OVN agent that interacts with OVS and hypervisors, for any information or update that is needed for <code class="literal cluster-admin">sbdb</code>. The <code class="literal cluster-admin">ovn-controller</code> reads logical flows from the <code class="literal cluster-admin">sbdb</code>, translates them into <code class="literal cluster-admin">OpenFlow</code> flows and sends them to the node’s OVS daemon. The container name is <code class="literal cluster-admin">ovn-controller</code> and it runs in the <code class="literal cluster-admin">ovnkube-node</code> pods.
						</li></ul></div><p class="cluster-admin cluster-admin">
					The OVN northbound database has the logical network configuration passed down to it by the cloud management system (CMS). The OVN northbound Database contains the current desired state of the network, presented as a collection of logical ports, logical switches, logical routers, and more. The <code class="literal cluster-admin">ovn-northd</code> (<code class="literal cluster-admin">northd</code> container) connects to the OVN northbound database and the OVN southbound database. It translates the logical network configuration in terms of conventional network concepts, taken from the OVN northbound Database, into logical data path flows in the OVN southbound database.
				</p><p class="cluster-admin cluster-admin">
					The OVN southbound database has physical and logical representations of the network and binding tables that link them together. Every node in the cluster is represented in the southbound database, and you can see the ports that are connected to it. It also contains all the logic flows, the logic flows are shared with the <code class="literal cluster-admin">ovn-controller</code> process that runs on each node and the <code class="literal cluster-admin">ovn-controller</code> turns those into <code class="literal cluster-admin">OpenFlow</code> rules to program <code class="literal cluster-admin">Open vSwitch</code>.
				</p><p class="cluster-admin cluster-admin">
					The Kubernetes control plane nodes each contain an <code class="literal cluster-admin">ovnkube-master</code> pod which hosts containers for the OVN northbound and southbound databases. All OVN northbound databases form a <code class="literal cluster-admin">Raft</code> cluster and all southbound databases form a separate <code class="literal cluster-admin">Raft</code> cluster. At any given time a single <code class="literal cluster-admin">ovnkube-master</code> is the leader and the other <code class="literal cluster-admin">ovnkube-master</code> pods are followers.
				</p></section><section class="section cluster-admin" id="nw-ovn-kubernetes-list-resources_ovn-kubernetes-architecture"><div class="titlepage"><div><div><h3 class="title">26.2.2. Listing all resources in the OVN-Kubernetes project</h3></div></div></div><p class="cluster-admin cluster-admin">
					Finding the resources and containers that run in the OVN-Kubernetes project is important to help you understand the OVN-Kubernetes networking implementation.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li><li class="listitem">
							The OpenShift CLI (<code class="literal cluster-admin">oc</code>) installed.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Run the following command to get all resources, endpoints, and <code class="literal cluster-admin">ConfigMaps</code> in the OVN-Kubernetes project:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get all,ep,cm -n openshift-ovn-kubernetes</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                       READY   STATUS    RESTARTS      AGE
pod/ovnkube-master-9g7zt   6/6     Running   1 (48m ago)   57m
pod/ovnkube-master-lqs4v   6/6     Running   0             57m
pod/ovnkube-master-vxhtq   6/6     Running   0             57m
pod/ovnkube-node-9k9kc     5/5     Running   0             57m
pod/ovnkube-node-jg52r     5/5     Running   0             51m
pod/ovnkube-node-k8wf7     5/5     Running   0             57m
pod/ovnkube-node-tlwk6     5/5     Running   0             47m
pod/ovnkube-node-xsvnk     5/5     Running   0             57m

NAME                            TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)             AGE
service/ovn-kubernetes-master   ClusterIP   None         &lt;none&gt;        9102/TCP            57m
service/ovn-kubernetes-node     ClusterIP   None         &lt;none&gt;        9103/TCP,9105/TCP   57m
service/ovnkube-db              ClusterIP   None         &lt;none&gt;        9641/TCP,9642/TCP   57m

NAME                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                                 AGE
daemonset.apps/ovnkube-master   3         3         3       3            3           beta.kubernetes.io/os=linux,node-role.kubernetes.io/master=   57m
daemonset.apps/ovnkube-node     5         5         5       5            5           beta.kubernetes.io/os=linux                                   57m

NAME                              ENDPOINTS                                                        AGE
endpoints/ovn-kubernetes-master   10.0.132.11:9102,10.0.151.18:9102,10.0.192.45:9102               57m
endpoints/ovn-kubernetes-node     10.0.132.11:9105,10.0.143.72:9105,10.0.151.18:9105 + 7 more...   57m
endpoints/ovnkube-db              10.0.132.11:9642,10.0.151.18:9642,10.0.192.45:9642 + 3 more...   57m

NAME                                 DATA   AGE
configmap/control-plane-status       1      55m
configmap/kube-root-ca.crt           1      57m
configmap/openshift-service-ca.crt   1      57m
configmap/ovn-ca                     1      57m
configmap/ovnkube-config             1      57m
configmap/signer-ca                  1      57m</pre>

							</p></div><p class="cluster-admin cluster-admin">
							There are three <code class="literal cluster-admin">ovnkube-masters</code> that run on the control plane nodes, and two daemon sets used to deploy the <code class="literal cluster-admin">ovnkube-master</code> and <code class="literal cluster-admin">ovnkube-node</code> pods. There is one <code class="literal cluster-admin">ovnkube-node</code> pod for each node in the cluster. The <code class="literal cluster-admin">ovnkube-config</code> <code class="literal cluster-admin">ConfigMap</code> has the OpenShift Container Platform OVN-Kubernetes configurations started by online-master and <code class="literal cluster-admin">ovnkube-node</code>.
						</p></li><li class="listitem"><p class="simpara">
							List all the containers in the <code class="literal cluster-admin">ovnkube-master</code> pods by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods ovnkube-master-9g7zt \
-o jsonpath='{.spec.containers[*].name}' -n openshift-ovn-kubernetes</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">northd nbdb kube-rbac-proxy sbdb ovnkube-master ovn-dbchecker</pre>

							</p></div><p class="cluster-admin cluster-admin">
							The <code class="literal cluster-admin">ovnkube-master</code> pod is made up of several containers. It is responsible for hosting the northbound database (<code class="literal cluster-admin">nbdb</code> container), the southbound database (<code class="literal cluster-admin">sbdb</code> container), watching for cluster events for pods, egressIP, namespaces, services, endpoints, egress firewall, and network policy and writing them to the northbound database (<code class="literal cluster-admin">ovnkube-master</code> pod), as well as managing pod subnet allocation to nodes.
						</p></li><li class="listitem"><p class="simpara">
							List all the containers in the <code class="literal cluster-admin">ovnkube-node</code> pods by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods ovnkube-node-jg52r \
-o jsonpath='{.spec.containers[*].name}' -n openshift-ovn-kubernetes</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">ovn-controller ovn-acl-logging kube-rbac-proxy kube-rbac-proxy-ovn-metrics ovnkube-node</pre>

							</p></div><p class="cluster-admin cluster-admin">
							The <code class="literal cluster-admin">ovnkube-node</code> pod has a container (<code class="literal cluster-admin">ovn-controller</code>) that resides on each OpenShift Container Platform node. Each node’s <code class="literal cluster-admin">ovn-controller</code> connects the OVN northbound to the OVN southbound database to learn about the OVN configuration. The <code class="literal cluster-admin">ovn-controller</code> connects southbound to <code class="literal cluster-admin">ovs-vswitchd</code> as an OpenFlow controller, for control over network traffic, and to the local <code class="literal cluster-admin">ovsdb-server</code> to allow it to monitor and control Open vSwitch configuration.
						</p></li><li class="listitem"><p class="simpara">
							List the currently elected OVN-Kubernetes master leader by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get lease -n openshift-ovn-kubernetes</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                    HOLDER                               AGE
ovn-kubernetes-master   ci-ln-gz990pb-72292-rthz2-master-2   50m</pre>

							</p></div></li></ol></div></section><section class="section cluster-admin" id="nw-ovn-kubernetes-list-database-contents_ovn-kubernetes-architecture"><div class="titlepage"><div><div><h3 class="title">26.2.3. Listing the OVN-Kubernetes northbound database contents</h3></div></div></div><p class="cluster-admin cluster-admin">
					To understand logic flow rules you need to examine the northbound database and understand what objects are there to see how they are translated into logic flow rules. The up to date information is present on the OVN Raft leader and this procedure describes how to find the Raft leader and subsequently query it to list the OVN northbound database contents.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li><li class="listitem">
							The OpenShift CLI (<code class="literal cluster-admin">oc</code>) installed.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Find the OVN Raft leader for the northbound database.
						</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								The Raft leader stores the most up to date information.
							</p></div></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									List the pods by running the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get po -n openshift-ovn-kubernetes</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                   READY   STATUS    RESTARTS       AGE
ovnkube-master-7j97q   6/6     Running   2 (148m ago)   149m
ovnkube-master-gt4ms   6/6     Running   1 (140m ago)   147m
ovnkube-master-mk6p6   6/6     Running   0              148m
ovnkube-node-8qvtr     5/5     Running   0              149m
ovnkube-node-fqdc9     5/5     Running   0              149m
ovnkube-node-tlfwv     5/5     Running   0              149m
ovnkube-node-wlwkn     5/5     Running   0              142m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Choose one of the master pods at random and run the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec -n openshift-ovn-kubernetes ovnkube-master-7j97q \
-- /usr/bin/ovn-appctl -t /var/run/ovn/ovnnb_db.ctl \
--timeout=3 cluster/status OVN_Northbound</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">Defaulted container "northd" out of: northd, nbdb, kube-rbac-proxy, sbdb, ovnkube-master, ovn-dbchecker
1c57
Name: OVN_Northbound
Cluster ID: c48a (c48aa5c0-a704-4c77-a066-24fe99d9b338)
Server ID: 1c57 (1c57b6fc-2849-49b7-8679-fbf18bafe339)
Address: ssl:10.0.147.219:9643
Status: cluster member
Role: follower <span id="CO140-1"><!--Empty--></span><span class="callout">1</span>
Term: 5
Leader: 2b4f <span id="CO140-2"><!--Empty--></span><span class="callout">2</span>
Vote: unknown

Election timer: 10000
Log: [2, 3018]
Entries not yet committed: 0
Entries not yet applied: 0
Connections: -&gt;0000 -&gt;0000 &lt;-8844 &lt;-2b4f
Disconnections: 0
Servers:
    1c57 (1c57 at ssl:10.0.147.219:9643) (self)
    8844 (8844 at ssl:10.0.163.212:9643) last msg 8928047 ms ago
    2b4f (2b4f at ssl:10.0.242.240:9643) last msg 620 ms ago <span id="CO140-3"><!--Empty--></span><span class="callout">3</span></pre>

									</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO140-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											This pod is identified as a follower
										</div></dd><dt><a href="#CO140-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											The leader is identified as <code class="literal cluster-admin">2b4f</code>
										</div></dd><dt><a href="#CO140-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											The <code class="literal cluster-admin">2b4f</code> is on IP address <code class="literal cluster-admin">10.0.242.240</code>
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Find the <code class="literal cluster-admin">ovnkube-master</code> pod running on IP Address <code class="literal cluster-admin">10.0.242.240</code> using the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get po -o wide -n openshift-ovn-kubernetes | grep 10.0.242.240 | grep -v ovnkube-node</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">ovnkube-master-gt4ms   6/6     Running             1 (143m ago)   150m   10.0.242.240   ip-10-0-242-240.ec2.internal   &lt;none&gt;           &lt;none&gt;</pre>

									</p></div><p class="cluster-admin cluster-admin">
									The <code class="literal cluster-admin">ovnkube-master-gt4ms</code> pod runs on IP Address 10.0.242.240.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Run the following command to show all the objects in the northbound database:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec -n openshift-ovn-kubernetes -it ovnkube-master-gt4ms \
-c northd -- ovn-nbctl show</pre><p class="cluster-admin cluster-admin">
							The output is too long to list here. The list includes the NAT rules, logical switches, load balancers and so on.
						</p><p class="cluster-admin cluster-admin">
							Run the following command to display the options available with the command <code class="literal cluster-admin">ovn-nbctl</code>:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec -n openshift-ovn-kubernetes -it ovnkube-master-mk6p6 \
-c northd ovn-nbctl --help</pre><p class="cluster-admin cluster-admin">
							You can narrow down and focus on specific components by using some of the following commands:
						</p></li><li class="listitem"><p class="simpara">
							Run the following command to show the list of logical routers:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec -n openshift-ovn-kubernetes -it ovnkube-master-gt4ms \
-c northd -- ovn-nbctl lr-list</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">f971f1f3-5112-402f-9d1e-48f1d091ff04 (GR_ip-10-0-145-205.ec2.internal)
69c992d8-a4cf-429e-81a3-5361209ffe44 (GR_ip-10-0-147-219.ec2.internal)
7d164271-af9e-4283-b84a-48f2a44851cd (GR_ip-10-0-163-212.ec2.internal)
111052e3-c395-408b-97b2-8dd0a20a29a5 (GR_ip-10-0-165-9.ec2.internal)
ed50ce33-df5d-48e8-8862-2df6a59169a0 (GR_ip-10-0-209-170.ec2.internal)
f44e2a96-8d1e-4a4d-abae-ed8728ac6851 (GR_ip-10-0-242-240.ec2.internal)
ef3d0057-e557-4b1a-b3c6-fcc3463790b0 (ovn_cluster_router)</pre>

							</p></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								From this output you can see there is router on each node plus an <code class="literal cluster-admin">ovn_cluster_router</code>.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Run the following command to show the list of logical switches:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec -n openshift-ovn-kubernetes -it ovnkube-master-gt4ms \
-c northd -- ovn-nbctl ls-list</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">82808c5c-b3bc-414a-bb59-8fec4b07eb14 (ext_ip-10-0-145-205.ec2.internal)
3d22444f-0272-4c51-afc6-de9e03db3291 (ext_ip-10-0-147-219.ec2.internal)
bf73b9df-59ab-4c58-a456-ce8205b34ac5 (ext_ip-10-0-163-212.ec2.internal)
bee1e8d0-ec87-45eb-b98b-63f9ec213e5e (ext_ip-10-0-165-9.ec2.internal)
812f08f2-6476-4abf-9a78-635f8516f95e (ext_ip-10-0-209-170.ec2.internal)
f65e710b-32f9-482b-8eab-8d96a44799c1 (ext_ip-10-0-242-240.ec2.internal)
84dad700-afb8-4129-86f9-923a1ddeace9 (ip-10-0-145-205.ec2.internal)
1b7b448b-e36c-4ca3-9f38-4a2cf6814bfd (ip-10-0-147-219.ec2.internal)
d92d1f56-2606-4f23-8b6a-4396a78951de (ip-10-0-163-212.ec2.internal)
6864a6b2-de15-4de3-92d8-f95014b6f28f (ip-10-0-165-9.ec2.internal)
c26bf618-4d7e-4afd-804f-1a2cbc96ec6d (ip-10-0-209-170.ec2.internal)
ab9a4526-44ed-4f82-ae1c-e20da04947d9 (ip-10-0-242-240.ec2.internal)
a8588aba-21da-4276-ba0f-9d68e88911f0 (join)</pre>

							</p></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								From this output you can see there is an ext switch for each node plus switches with the node name itself and a join switch.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Run the following command to show the list of load balancers:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec -n openshift-ovn-kubernetes -it ovnkube-master-gt4ms \
-c northd -- ovn-nbctl lb-list</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">UUID                                    LB                  PROTO      VIP                     IPs
f0fb50f9-4968-4b55-908c-616bae4db0a2    Service_default/    tcp        172.30.0.1:443          10.0.147.219:6443,10.0.163.212:6443,169.254.169.2:6443
0dc42012-4f5b-432e-ae01-2cc4bfe81b00    Service_default/    tcp        172.30.0.1:443          10.0.147.219:6443,169.254.169.2:6443,10.0.242.240:6443
f7fff5d5-5eff-4a40-98b1-3a4ba8f7f69c    Service_default/    tcp        172.30.0.1:443          169.254.169.2:6443,10.0.163.212:6443,10.0.242.240:6443
12fe57a0-50a4-4a1b-ac10-5f288badee07    Service_default/    tcp        172.30.0.1:443          10.0.147.219:6443,10.0.163.212:6443,10.0.242.240:6443
3f137fbf-0b78-4875-ba44-fbf89f254cf7    Service_openshif    tcp        172.30.23.153:443       10.130.0.14:8443
174199fe-0562-4141-b410-12094db922a7    Service_openshif    tcp        172.30.69.51:50051      10.130.0.84:50051
5ee2d4bd-c9e2-4d16-a6df-f54cd17c9ac3    Service_openshif    tcp        172.30.143.87:9001      10.0.145.205:9001,10.0.147.219:9001,10.0.163.212:9001,10.0.165.9:9001,10.0.209.170:9001,10.0.242.240:9001
a056ae3d-83f8-45bc-9c80-ef89bce7b162    Service_openshif    tcp        172.30.164.74:443       10.0.147.219:6443,10.0.163.212:6443,10.0.242.240:6443
bac51f3d-9a6f-4f5e-ac02-28fd343a332a    Service_openshif    tcp        172.30.0.10:53          10.131.0.6:5353
                                                            tcp        172.30.0.10:9154        10.131.0.6:9154
48105bbc-51d7-4178-b975-417433f9c20a    Service_openshif    tcp        172.30.26.159:2379      10.0.147.219:2379,169.254.169.2:2379,10.0.242.240:2379
                                                            tcp        172.30.26.159:9979      10.0.147.219:9979,169.254.169.2:9979,10.0.242.240:9979
7de2b8fc-342a-415f-ac13-1a493f4e39c0    Service_openshif    tcp        172.30.53.219:443       10.128.0.7:8443
                                                            tcp        172.30.53.219:9192      10.128.0.7:9192
2cef36bc-d720-4afb-8d95-9350eff1d27a    Service_openshif    tcp        172.30.81.66:443        10.128.0.23:8443
365cb6fb-e15e-45a4-a55b-21868b3cf513    Service_openshif    tcp        172.30.96.51:50051      10.130.0.19:50051
41691cbb-ec55-4cdb-8431-afce679c5e8d    Service_openshif    tcp        172.30.98.218:9099      169.254.169.2:9099
82df10ba-8143-400b-977a-8f5f416a4541    Service_openshif    tcp        172.30.26.159:2379      10.0.147.219:2379,10.0.163.212:2379,169.254.169.2:2379
                                                            tcp        172.30.26.159:9979      10.0.147.219:9979,10.0.163.212:9979,169.254.169.2:9979
debe7f3a-39a8-490e-bc0a-ebbfafdffb16    Service_openshif    tcp        172.30.23.244:443       10.128.0.48:8443,10.129.0.27:8443,10.130.0.45:8443
8a749239-02d9-4dc2-8737-716528e0da7b    Service_openshif    tcp        172.30.124.255:8443     10.128.0.14:8443
880c7c78-c790-403d-a3cb-9f06592717a3    Service_openshif    tcp        172.30.0.10:53          10.130.0.20:5353
                                                            tcp        172.30.0.10:9154        10.130.0.20:9154
d2f39078-6751-4311-a161-815bbaf7f9c7    Service_openshif    tcp        172.30.26.159:2379      169.254.169.2:2379,10.0.163.212:2379,10.0.242.240:2379
                                                            tcp        172.30.26.159:9979      169.254.169.2:9979,10.0.163.212:9979,10.0.242.240:9979
30948278-602b-455c-934a-28e64c46de12    Service_openshif    tcp        172.30.157.35:9443      10.130.0.43:9443
2cc7e376-7c02-4a82-89e8-dfa1e23fb003    Service_openshif    tcp        172.30.159.212:17698    10.128.0.48:17698,10.129.0.27:17698,10.130.0.45:17698
e7d22d35-61c2-40c2-bc30-265cff8ed18d    Service_openshif    tcp        172.30.143.87:9001      10.0.145.205:9001,10.0.147.219:9001,10.0.163.212:9001,10.0.165.9:9001,10.0.209.170:9001,169.254.169.2:9001
75164e75-e0c5-40fb-9636-bfdbf4223a02    Service_openshif    tcp        172.30.150.68:1936      10.129.4.8:1936,10.131.0.10:1936
                                                            tcp        172.30.150.68:443       10.129.4.8:443,10.131.0.10:443
                                                            tcp        172.30.150.68:80        10.129.4.8:80,10.131.0.10:80
7bc4ee74-dccf-47e9-9149-b011f09aff39    Service_openshif    tcp        172.30.164.74:443       10.0.147.219:6443,10.0.163.212:6443,169.254.169.2:6443
0db59e74-1cc6-470c-bf44-57c520e0aa8f    Service_openshif    tcp        10.0.163.212:31460
                                                            tcp        10.0.163.212:32361
c300e134-018c-49af-9f84-9deb1d0715f8    Service_openshif    tcp        172.30.42.244:50051     10.130.0.47:50051
5e352773-429b-4881-afb3-a13b7ba8b081    Service_openshif    tcp        172.30.244.66:443       10.129.0.8:8443,10.130.0.8:8443
54b82d32-1939-4465-a87d-f26321442a7a    Service_openshif    tcp        172.30.12.9:8443        10.128.0.35:8443</pre>

							</p></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								From this truncated output you can see there are many OVN-Kubernetes load balancers. Load balancers in OVN-Kubernetes are representations of services.
							</p></div></div></li></ol></div></section><section class="section cluster-admin" id="nw-ovn-kubernetes-examine-nb-database-contents-ref_ovn-kubernetes-architecture"><div class="titlepage"><div><div><h3 class="title">26.2.4. Command line arguments for ovn-nbctl to examine northbound database contents</h3></div></div></div><p class="cluster-admin cluster-admin">
					The following table describes the command line arguments that can be used with <code class="literal cluster-admin">ovn-nbctl</code> to examine the contents of the northbound database.
				</p><div class="table" id="idm140587105726864"><p class="title"><strong>Table 26.2. Command line arguments to examine northbound database contents</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 30%; " class="col_1"><!--Empty--></col><col style="width: 70%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587105721680" scope="col">Argument</th><th align="left" valign="top" id="idm140587105720592" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587105721680"> <p>
									<code class="literal cluster-admin">ovn-nbctl show</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587105720592"> <p>
									An overview of the northbound database contents.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587105721680"> <p>
									<code class="literal cluster-admin">ovn-nbctl show &lt;switch_or_router&gt;</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587105720592"> <p>
									Show the details associated with the specified switch or router.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587105721680"> <p>
									<code class="literal cluster-admin">ovn-nbctl lr-list</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587105720592"> <p>
									Show the logical routers.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587105721680"> <p>
									<code class="literal cluster-admin">ovn-nbctl lrp-list &lt;router&gt;</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587105720592"> <p>
									Using the router information from <code class="literal cluster-admin">ovn-nbctl lr-list</code> to show the router ports.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587105721680"> <p>
									<code class="literal cluster-admin">ovn-nbctl lr-nat-list &lt;router&gt;</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587105720592"> <p>
									Show network address translation details for the specified router.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587105721680"> <p>
									<code class="literal cluster-admin">ovn-nbctl ls-list</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587105720592"> <p>
									Show the logical switches
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587105721680"> <p>
									<code class="literal cluster-admin">ovn-nbctl lsp-list &lt;switch&gt;</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587105720592"> <p>
									Using the switch information from <code class="literal cluster-admin">ovn-nbctl ls-list</code> to show the switch port.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587105721680"> <p>
									<code class="literal cluster-admin">ovn-nbctl lsp-get-type &lt;port&gt;</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587105720592"> <p>
									Get the type for the logical port.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587105721680"> <p>
									<code class="literal cluster-admin">ovn-nbctl lb-list</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587105720592"> <p>
									Show the load balancers.
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section cluster-admin" id="nw-ovn-kubernetes-list-southbound-database-contents_ovn-kubernetes-architecture"><div class="titlepage"><div><div><h3 class="title">26.2.5. Listing the OVN-Kubernetes southbound database contents</h3></div></div></div><p class="cluster-admin cluster-admin">
					Logic flow rules are stored in the southbound database that is a representation of your infrastructure. The up to date information is present on the OVN Raft leader and this procedure describes how to find the Raft leader and query it to list the OVN southbound database contents.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li><li class="listitem">
							The OpenShift CLI (<code class="literal cluster-admin">oc</code>) installed.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Find the OVN Raft leader for the southbound database.
						</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								The Raft leader stores the most up to date information.
							</p></div></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									List the pods by running the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get po -n openshift-ovn-kubernetes</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                   READY   STATUS    RESTARTS       AGE
ovnkube-master-7j97q   6/6     Running   2 (134m ago)   135m
ovnkube-master-gt4ms   6/6     Running   1 (126m ago)   133m
ovnkube-master-mk6p6   6/6     Running   0              134m
ovnkube-node-8qvtr     5/5     Running   0              135m
ovnkube-node-bqztb     5/5     Running   0              117m
ovnkube-node-fqdc9     5/5     Running   0              135m
ovnkube-node-tlfwv     5/5     Running   0              135m
ovnkube-node-wlwkn     5/5     Running   0              128m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Choose one of the master pods at random and run the following command to find the OVN southbound Raft leader:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec -n openshift-ovn-kubernetes ovnkube-master-7j97q \
-- /usr/bin/ovn-appctl -t /var/run/ovn/ovnsb_db.ctl \
--timeout=3 cluster/status OVN_Southbound</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">Defaulted container "northd" out of: northd, nbdb, kube-rbac-proxy, sbdb, ovnkube-master, ovn-dbchecker
1930
Name: OVN_Southbound
Cluster ID: f772 (f77273c0-7986-42dd-bd3c-a9f18e25701f)
Server ID: 1930 (1930f4b7-314b-406f-9dcb-b81fe2729ae1)
Address: ssl:10.0.147.219:9644
Status: cluster member
Role: follower <span id="CO141-1"><!--Empty--></span><span class="callout">1</span>
Term: 3
Leader: 7081 <span id="CO141-2"><!--Empty--></span><span class="callout">2</span>
Vote: unknown

Election timer: 16000
Log: [2, 2423]
Entries not yet committed: 0
Entries not yet applied: 0
Connections: -&gt;0000 -&gt;7145 &lt;-7081 &lt;-7145
Disconnections: 0
Servers:
    7081 (7081 at ssl:10.0.163.212:9644) last msg 59 ms ago <span id="CO141-3"><!--Empty--></span><span class="callout">3</span>
    1930 (1930 at ssl:10.0.147.219:9644) (self)
    7145 (7145 at ssl:10.0.242.240:9644) last msg 7871735 ms ago</pre>

									</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO141-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											This pod is identified as a follower
										</div></dd><dt><a href="#CO141-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											The leader is identified as <code class="literal cluster-admin">7081</code>
										</div></dd><dt><a href="#CO141-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											The <code class="literal cluster-admin">7081</code> is on IP address <code class="literal cluster-admin">10.0.163.212</code>
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Find the <code class="literal cluster-admin">ovnkube-master</code> pod running on IP Address <code class="literal cluster-admin">10.0.163.212</code> using the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get po -o wide -n openshift-ovn-kubernetes | grep 10.0.163.212 | grep -v ovnkube-node</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">ovnkube-master-mk6p6   6/6     Running   0              136m   10.0.163.212   ip-10-0-163-212.ec2.internal   &lt;none&gt;           &lt;none&gt;</pre>

									</p></div><p class="cluster-admin cluster-admin">
									The <code class="literal cluster-admin">ovnkube-master-mk6p6</code> pod runs on IP Address 10.0.163.212.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Run the following command to show all the information stored in the southbound database:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec -n openshift-ovn-kubernetes -it ovnkube-master-mk6p6 \
-c northd -- ovn-sbctl show</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Chassis "8ca57b28-9834-45f0-99b0-96486c22e1be"
    hostname: ip-10-0-156-16.ec2.internal
    Encap geneve
        ip: "10.0.156.16"
        options: {csum="true"}
    Port_Binding k8s-ip-10-0-156-16.ec2.internal
    Port_Binding etor-GR_ip-10-0-156-16.ec2.internal
    Port_Binding jtor-GR_ip-10-0-156-16.ec2.internal
    Port_Binding openshift-ingress-canary_ingress-canary-hsblx
    Port_Binding rtoj-GR_ip-10-0-156-16.ec2.internal
    Port_Binding openshift-monitoring_prometheus-adapter-658fc5967-9l46x
    Port_Binding rtoe-GR_ip-10-0-156-16.ec2.internal
    Port_Binding openshift-multus_network-metrics-daemon-77nvz
    Port_Binding openshift-ingress_router-default-64fd8c67c7-df598
    Port_Binding openshift-dns_dns-default-ttpcq
    Port_Binding openshift-monitoring_alertmanager-main-0
    Port_Binding openshift-e2e-loki_loki-promtail-g2pbh
    Port_Binding openshift-network-diagnostics_network-check-target-m6tn4
    Port_Binding openshift-monitoring_thanos-querier-75b5cf8dcb-qf8qj
    Port_Binding cr-rtos-ip-10-0-156-16.ec2.internal
    Port_Binding openshift-image-registry_image-registry-7b7bc44566-mp9b8</pre>

							</p></div><p class="cluster-admin cluster-admin">
							This detailed output shows the chassis and the ports that are attached to the chassis which in this case are all of the router ports and anything that runs like host networking. Any pods communicate out to the wider network using source network address translation (SNAT). Their IP address is translated into the IP address of the node that the pod is running on and then sent out into the network.
						</p><p class="cluster-admin cluster-admin">
							In addition to the chassis information the southbound database has all the logic flows and those logic flows are then sent to the <code class="literal cluster-admin">ovn-controller</code> running on each of the nodes. The <code class="literal cluster-admin">ovn-controller</code> translates the logic flows into open flow rules and ultimately programs <code class="literal cluster-admin">OpenvSwitch</code> so that your pods can then follow open flow rules and make it out of the network.
						</p><p class="cluster-admin cluster-admin">
							Run the following command to display the options available with the command <code class="literal cluster-admin">ovn-sbctl</code>:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec -n openshift-ovn-kubernetes -it ovnkube-master-mk6p6 \
-c northd -- ovn-sbctl --help</pre></li></ol></div></section><section class="section cluster-admin" id="nw-ovn-kubernetes-examine-sb-database-contents-ref_ovn-kubernetes-architecture"><div class="titlepage"><div><div><h3 class="title">26.2.6. Command line arguments for ovn-sbctl to examine southbound database contents</h3></div></div></div><p class="cluster-admin cluster-admin">
					The following table describes the command line arguments that can be used with <code class="literal cluster-admin">ovn-sbctl</code> to examine the contents of the southbound database.
				</p><div class="table" id="idm140587106819664"><p class="title"><strong>Table 26.3. Command line arguments to examine southbound database contents</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 30%; " class="col_1"><!--Empty--></col><col style="width: 70%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587106814480" scope="col">Argument</th><th align="left" valign="top" id="idm140587106813392" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587106814480"> <p>
									<code class="literal cluster-admin">ovn-sbctl show</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587106813392"> <p>
									Overview of the southbound database contents.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587106814480"> <p>
									<code class="literal cluster-admin">ovn-sbctl list Port_Binding &lt;port&gt;</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587106813392"> <p>
									List the contents of southbound database for a the specified port .
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587106814480"> <p>
									<code class="literal cluster-admin">ovn-sbctl dump-flows</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587106813392"> <p>
									List the logical flows.
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section cluster-admin" id="ovn-kubernetes-logical-architecture-con_ovn-kubernetes-architecture"><div class="titlepage"><div><div><h3 class="title">26.2.7. OVN-Kubernetes logical architecture</h3></div></div></div><p class="cluster-admin cluster-admin">
					OVN is a network virtualization solution. It creates logical switches and routers. These switches and routers are interconnected to create any network topologies. When you run <code class="literal cluster-admin">ovnkube-trace</code> with the log level set to 2 or 5 the OVN-Kubernetes logical components are exposed. The following diagram shows how the routers and switches are connected in OpenShift Container Platform.
				</p><div class="figure" id="idm140587106947520"><p class="title"><strong>Figure 26.2. OVN-Kubernetes router and switch components</strong></p><div class="figure-contents"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/886e0f662ba0c765b2271b47e7b29fa7/299_OpenShift_OVN-Kubernetes_arch_0123_2.png" alt="OVN-Kubernetes logical architecture"/></div></div></div><p class="cluster-admin cluster-admin">
					The key components involved in packet processing are:
				</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Gateway routers</span></dt><dd>
								Gateway routers sometimes called L3 gateway routers, are typically used between the distributed routers and the physical network. Gateway routers including their logical patch ports are bound to a physical location (not distributed), or chassis. The patch ports on this router are known as l3gateway ports in the ovn-southbound database (<code class="literal cluster-admin">ovn-sbdb</code>).
							</dd><dt><span class="term">Distributed logical routers</span></dt><dd>
								Distributed logical routers and the logical switches behind them, to which virtual machines and containers attach, effectively reside on each hypervisor.
							</dd><dt><span class="term">Join local switch</span></dt><dd>
								Join local switches are used to connect the distributed router and gateway routers. It reduces the number of IP addresses needed on the distributed router.
							</dd><dt><span class="term">Logical switches with patch ports</span></dt><dd>
								Logical switches with patch ports are used to virtualize the network stack. They connect remote logical ports through tunnels.
							</dd><dt><span class="term">Logical switches with localnet ports</span></dt><dd>
								Logical switches with localnet ports are used to connect OVN to the physical network. They connect remote logical ports by bridging the packets to directly connected physical L2 segments using localnet ports.
							</dd><dt><span class="term">Patch ports</span></dt><dd>
								Patch ports represent connectivity between logical switches and logical routers and between peer logical routers. A single connection has a pair of patch ports at each such point of connectivity, one on each side.
							</dd><dt><span class="term">l3gateway ports</span></dt><dd>
								l3gateway ports are the port binding entries in the <code class="literal cluster-admin">ovn-sbdb</code> for logical patch ports used in the gateway routers. They are called l3gateway ports rather than patch ports just to portray the fact that these ports are bound to a chassis just like the gateway router itself.
							</dd><dt><span class="term">localnet ports</span></dt><dd>
								localnet ports are present on the bridged logical switches that allows a connection to a locally accessible network from each <code class="literal cluster-admin">ovn-controller</code> instance. This helps model the direct connectivity to the physical network from the logical switches. A logical switch can only have a single localnet port attached to it.
							</dd></dl></div><section class="section cluster-admin" id="nw-ovn-kubernetes-installing-network-tools_ovn-kubernetes-architecture"><div class="titlepage"><div><div><h4 class="title">26.2.7.1. Installing network-tools on local host</h4></div></div></div><p class="cluster-admin cluster-admin">
						Install <code class="literal cluster-admin">network-tools</code> on your local host to make a collection of tools available for debugging OpenShift Container Platform cluster network issues.
					</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Clone the <code class="literal cluster-admin">network-tools</code> repository onto your workstation with the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ git clone git@github.com:openshift/network-tools.git</pre></li><li class="listitem"><p class="simpara">
								Change into the directory for the repository you just cloned:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cd network-tools</pre></li><li class="listitem"><p class="simpara">
								Optional: List all available commands:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ./debug-scripts/network-tools -h</pre></li></ol></div></section><section class="section cluster-admin" id="nw-ovn-kubernetes-running-network-tools_ovn-kubernetes-architecture"><div class="titlepage"><div><div><h4 class="title">26.2.7.2. Running network-tools</h4></div></div></div><p class="cluster-admin cluster-admin">
						Get information about the logical switches and routers by running <code class="literal cluster-admin">network-tools</code>.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								You are logged in to the cluster as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have installed <code class="literal cluster-admin">network-tools</code> on local host.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								List the routers by running the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ./debug-scripts/network-tools ovn-db-run-command ovn-nbctl lr-list</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Leader pod is ovnkube-master-vslqm
5351ddd1-f181-4e77-afc6-b48b0a9df953 (GR_helix13.lab.eng.tlv2.redhat.com)
ccf9349e-1948-4df8-954e-39fb0c2d4d06 (GR_helix14.lab.eng.tlv2.redhat.com)
e426b918-75a8-4220-9e76-20b7758f92b7 (GR_hlxcl7-master-0.hlxcl7.lab.eng.tlv2.redhat.com)
dded77c8-0cc3-4b99-8420-56cd2ae6a840 (GR_hlxcl7-master-1.hlxcl7.lab.eng.tlv2.redhat.com)
4f6747e6-e7ba-4e0c-8dcd-94c8efa51798 (GR_hlxcl7-master-2.hlxcl7.lab.eng.tlv2.redhat.com)
52232654-336e-4952-98b9-0b8601e370b4 (ovn_cluster_router)</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								List the localnet ports by running the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ./debug-scripts/network-tools ovn-db-run-command \
ovn-sbctl find Port_Binding type=localnet</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Leader pod is ovnkube-master-vslqm
_uuid               : 3de79191-cca8-4c28-be5a-a228f0f9ebfc
additional_chassis  : []
additional_encap    : []
chassis             : []
datapath            : 3f1a4928-7ff5-471f-9092-fe5f5c67d15c
encap               : []
external_ids        : {}
gateway_chassis     : []
ha_chassis_group    : []
logical_port        : br-ex_helix13.lab.eng.tlv2.redhat.com
mac                 : [unknown]
nat_addresses       : []
options             : {network_name=physnet}
parent_port         : []
port_security       : []
requested_additional_chassis: []
requested_chassis   : []
tag                 : []
tunnel_key          : 2
type                : localnet
up                  : false
virtual_parent      : []

_uuid               : dbe21daf-9594-4849-b8f0-5efbfa09a455
additional_chassis  : []
additional_encap    : []
chassis             : []
datapath            : db2a6067-fe7c-4d11-95a7-ff2321329e11
encap               : []
external_ids        : {}
gateway_chassis     : []
ha_chassis_group    : []
logical_port        : br-ex_hlxcl7-master-2.hlxcl7.lab.eng.tlv2.redhat.com
mac                 : [unknown]
nat_addresses       : []
options             : {network_name=physnet}
parent_port         : []
port_security       : []
requested_additional_chassis: []
requested_chassis   : []
tag                 : []
tunnel_key          : 2
type                : localnet
up                  : false
virtual_parent      : []

[...]</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								List the <code class="literal cluster-admin">l3gateway</code> ports by running the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ./debug-scripts/network-tools ovn-db-run-command \
ovn-sbctl find Port_Binding type=l3gateway</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Leader pod is ovnkube-master-vslqm
_uuid               : 9314dc80-39e1-4af7-9cc0-ae8a9708ed59
additional_chassis  : []
additional_encap    : []
chassis             : 336a923d-99e8-4e71-89a6-12564fde5760
datapath            : db2a6067-fe7c-4d11-95a7-ff2321329e11
encap               : []
external_ids        : {}
gateway_chassis     : []
ha_chassis_group    : []
logical_port        : etor-GR_hlxcl7-master-2.hlxcl7.lab.eng.tlv2.redhat.com
mac                 : ["52:54:00:3e:95:d3"]
nat_addresses       : ["52:54:00:3e:95:d3 10.46.56.77"]
options             : {l3gateway-chassis="7eb1f1c3-87c2-4f68-8e89-60f5ca810971", peer=rtoe-GR_hlxcl7-master-2.hlxcl7.lab.eng.tlv2.redhat.com}
parent_port         : []
port_security       : []
requested_additional_chassis: []
requested_chassis   : []
tag                 : []
tunnel_key          : 1
type                : l3gateway
up                  : true
virtual_parent      : []

_uuid               : ad7eb303-b411-4e9f-8d36-d07f1f268e27
additional_chassis  : []
additional_encap    : []
chassis             : f41453b8-29c5-4f39-b86b-e82cf344bce4
datapath            : 082e7a60-d9c7-464b-b6ec-117d3426645a
encap               : []
external_ids        : {}
gateway_chassis     : []
ha_chassis_group    : []
logical_port        : etor-GR_helix14.lab.eng.tlv2.redhat.com
mac                 : ["34:48:ed:f3:e2:2c"]
nat_addresses       : ["34:48:ed:f3:e2:2c 10.46.56.14"]
options             : {l3gateway-chassis="2e8abe3a-cb94-4593-9037-f5f9596325e2", peer=rtoe-GR_helix14.lab.eng.tlv2.redhat.com}
parent_port         : []
port_security       : []
requested_additional_chassis: []
requested_chassis   : []
tag                 : []
tunnel_key          : 1
type                : l3gateway
up                  : true
virtual_parent      : []

[...]</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								List the patch ports by running the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ./debug-scripts/network-tools ovn-db-run-command \
ovn-sbctl find Port_Binding type=patch</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Leader pod is ovnkube-master-vslqm
_uuid               : c48b1380-ff26-4965-a644-6bd5b5946c61
additional_chassis  : []
additional_encap    : []
chassis             : []
datapath            : 72734d65-fae1-4bd9-a1ee-1bf4e085a060
encap               : []
external_ids        : {}
gateway_chassis     : []
ha_chassis_group    : []
logical_port        : jtor-ovn_cluster_router
mac                 : [router]
nat_addresses       : []
options             : {peer=rtoj-ovn_cluster_router}
parent_port         : []
port_security       : []
requested_additional_chassis: []
requested_chassis   : []
tag                 : []
tunnel_key          : 4
type                : patch
up                  : false
virtual_parent      : []

_uuid               : 5df51302-f3cd-415b-a059-ac24389938f7
additional_chassis  : []
additional_encap    : []
chassis             : []
datapath            : 0551c90f-e891-4909-8e9e-acc7909e06d0
encap               : []
external_ids        : {}
gateway_chassis     : []
ha_chassis_group    : []
logical_port        : rtos-hlxcl7-master-1.hlxcl7.lab.eng.tlv2.redhat.com
mac                 : ["0a:58:0a:82:00:01 10.130.0.1/23"]
nat_addresses       : []
options             : {chassis-redirect-port=cr-rtos-hlxcl7-master-1.hlxcl7.lab.eng.tlv2.redhat.com, peer=stor-hlxcl7-master-1.hlxcl7.lab.eng.tlv2.redhat.com}
parent_port         : []
port_security       : []
requested_additional_chassis: []
requested_chassis   : []
tag                 : []
tunnel_key          : 4
type                : patch
up                  : false
virtual_parent      : []

[...]</pre>

								</p></div></li></ol></div></section></section><section class="section _additional-resources" id="additional-resources_ovn-kubernetes-architecture"><div class="titlepage"><div><div><h3 class="title">26.2.8. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/solutions/5660751">How to list OVN database contents with ovn-kubernetes in Red Hat OpenShift Container Platform 4.x?</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#ovn-kubernetes-tracing-using-ovntrace">Tracing Openflow with ovnkube-trace</a>
						</li><li class="listitem">
							<a class="link" href="https://www.ovn.org/support/dist-docs/ovn-architecture.7.html">OVN architecture</a>
						</li><li class="listitem">
							<a class="link" href="https://en.wikipedia.org/wiki/Raft_(algorithm)">Raft (algorithm)</a>
						</li><li class="listitem">
							<a class="link" href="https://man7.org/linux/man-pages/man8/ovn-nbctl.8.html">ovn-nbctl linux manual page</a>
						</li><li class="listitem">
							<a class="link" href="https://man7.org/linux/man-pages/man8/ovn-sbctl.8.html">ovn-sbctl linux manual page</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="ovn-kubernetes-troubleshooting-sources"><div class="titlepage"><div><div><h2 class="title">26.3. Troubleshooting OVN-Kubernetes</h2></div></div></div><p class="cluster-admin cluster-admin">
				OVN-Kubernetes has many sources of built-in health checks and logs.
			</p><section class="section cluster-admin" id="nw-ovn-kubernetes-readiness-probes_ovn-kubernetes-sources-of-troubleshooting-information"><div class="titlepage"><div><div><h3 class="title">26.3.1. Monitoring OVN-Kubernetes health by using readiness probes</h3></div></div></div><p class="cluster-admin cluster-admin">
					The <code class="literal cluster-admin">ovnkube-master</code> and <code class="literal cluster-admin">ovnkube-node</code> pods have containers configured with readiness probes.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Access to the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You have access to the cluster with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li><li class="listitem">
							You have installed <code class="literal cluster-admin">jq</code>.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Review the details of the <code class="literal cluster-admin">ovnkube-master</code> readiness probe by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods -n openshift-ovn-kubernetes -l app=ovnkube-master \
-o json | jq '.items[0].spec.containers[] | .name,.readinessProbe'</pre><p class="cluster-admin cluster-admin">
							The readiness probe for the northbound and southbound database containers in the <code class="literal cluster-admin">ovnkube-master</code> pod checks for the health of the Raft cluster hosting the databases.
						</p></li><li class="listitem"><p class="simpara">
							Review the details of the <code class="literal cluster-admin">ovnkube-node</code> readiness probe by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods -n openshift-ovn-kubernetes -l app=ovnkube-master \
-o json | jq '.items[0].spec.containers[] | .name,.readinessProbe'</pre><p class="cluster-admin cluster-admin">
							The <code class="literal cluster-admin">ovnkube-node</code> container in the <code class="literal cluster-admin">ovnkube-node</code> pod has a readiness probe to verify the presence of the ovn-kubernetes CNI configuration file, the absence of which would indicate that the pod is not running or is not ready to accept requests to configure pods.
						</p></li><li class="listitem"><p class="simpara">
							Show all events including the probe failures, for the namespace by using the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get events -n openshift-ovn-kubernetes</pre></li><li class="listitem"><p class="simpara">
							Show the events for just this pod:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc describe pod ovnkube-master-tp2z8 -n openshift-ovn-kubernetes</pre></li><li class="listitem"><p class="simpara">
							Show the messages and statuses from the cluster network operator:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get co/network -o json | jq '.status.conditions[]'</pre></li><li class="listitem"><p class="simpara">
							Show the <code class="literal cluster-admin">ready</code> status of each container in <code class="literal cluster-admin">ovnkube-master</code> pods by running the following script:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ for p in $(oc get pods --selector app=ovnkube-master -n openshift-ovn-kubernetes \
-o jsonpath='{range.items[*]}{" "}{.metadata.name}'); do echo === $p ===;  \
oc get pods -n openshift-ovn-kubernetes $p -o json | jq '.status.containerStatuses[] | .name, .ready'; \
done</pre><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								The expectation is all container statuses are reporting as <code class="literal cluster-admin">true</code>. Failure of a readiness probe sets the status to <code class="literal cluster-admin">false</code>.
							</p></div></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/building_applications/#application-health">Monitoring application health by using health checks</a>
						</li></ul></div></section><section class="section cluster-admin" id="nw-ovn-kubernetes-alerts-console_ovn-kubernetes-sources-of-troubleshooting-information"><div class="titlepage"><div><div><h3 class="title">26.3.2. Viewing OVN-Kubernetes alerts in the console</h3></div></div></div><p class="cluster-admin cluster-admin">
					The Alerting UI provides detailed information about alerts and their governing alerting rules and silences.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have access to the cluster as a developer or as a user with view permissions for the project that you are viewing metrics for.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure (UI)</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							In the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Administrator</span></strong></span> perspective, select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Observe</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Alerting</span></strong></span>. The three main pages in the Alerting UI in this perspective are the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Alerts</span></strong></span>, <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Silences</span></strong></span>, and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Alerting Rules</span></strong></span> pages.
						</li><li class="listitem">
							View the rules for OVN-Kubernetes alerts by selecting <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Observe</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Alerting</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Alerting Rules</span></strong></span>.
						</li></ol></div></section><section class="section cluster-admin" id="nw-ovn-kubernetes-alerts-cli_ovn-kubernetes-sources-of-troubleshooting-information"><div class="titlepage"><div><div><h3 class="title">26.3.3. Viewing OVN-Kubernetes alerts in the CLI</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can get information about alerts and their governing alerting rules and silences from the command line.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li><li class="listitem">
							The OpenShift CLI (<code class="literal cluster-admin">oc</code>) installed.
						</li><li class="listitem">
							You have installed <code class="literal cluster-admin">jq</code>.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							View active or firing alerts by running the following commands.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Set the alert manager route environment variable by running the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ALERT_MANAGER=$(oc get route alertmanager-main -n openshift-monitoring \
-o jsonpath='{@.spec.host}')</pre></li><li class="listitem"><p class="simpara">
									Issue a <code class="literal cluster-admin">curl</code> request to the alert manager route API with the correct authorization details requesting specific fields by running the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ curl -s -k -H "Authorization: Bearer \
$(oc create token prometheus-k8s -n openshift-monitoring)" \
https://$ALERT_MANAGER/api/v1/alerts \
| jq '.data[] | "\(.labels.severity) \(.labels.alertname) \(.labels.pod) \(.labels.container) \(.labels.endpoint) \(.labels.instance)"'</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							View alerting rules by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n openshift-monitoring exec -c prometheus prometheus-k8s-0 -- curl -s 'http://localhost:9090/api/v1/rules' | jq '.data.groups[].rules[] | select(((.name|contains("ovn")) or (.name|contains("OVN")) or (.name|contains("Ovn")) or (.name|contains("North")) or (.name|contains("South"))) and .type=="alerting")'</pre></li></ol></div></section><section class="section cluster-admin" id="nw-ovn-kubernetes-logs-cli_ovn-kubernetes-sources-of-troubleshooting-information"><div class="titlepage"><div><div><h3 class="title">26.3.4. Viewing the OVN-Kubernetes logs using the CLI</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can view the logs for each of the pods in the <code class="literal cluster-admin">ovnkube-master</code> and <code class="literal cluster-admin">ovnkube-node</code> pods using the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li><li class="listitem">
							Access to the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You have installed <code class="literal cluster-admin">jq</code>.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							View the log for a specific pod:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc logs -f &lt;pod_name&gt; -c &lt;container_name&gt; -n &lt;namespace&gt;</pre><p class="cluster-admin cluster-admin">
							where:
						</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">-f</code></span></dt><dd>
										Optional: Specifies that the output follows what is being written into the logs.
									</dd><dt><span class="term"><code class="literal cluster-admin">&lt;pod_name&gt;</code></span></dt><dd>
										Specifies the name of the pod.
									</dd><dt><span class="term"><code class="literal cluster-admin">&lt;container_name&gt;</code></span></dt><dd>
										Optional: Specifies the name of a container. When a pod has more than one container, you must specify the container name.
									</dd><dt><span class="term"><code class="literal cluster-admin">&lt;namespace&gt;</code></span></dt><dd>
										Specify the namespace the pod is running in.
									</dd></dl></div><p class="cluster-admin cluster-admin">
							For example:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc logs ovnkube-master-7h4q7 -n openshift-ovn-kubernetes</pre><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc logs -f ovnkube-master-7h4q7 -n openshift-ovn-kubernetes -c ovn-dbchecker</pre><p class="cluster-admin cluster-admin">
							The contents of log files are printed out.
						</p></li><li class="listitem"><p class="simpara">
							Examine the most recent entries in all the containers in the <code class="literal cluster-admin">ovnkube-master</code> pods:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ for p in $(oc get pods --selector app=ovnkube-master -n openshift-ovn-kubernetes \
-o jsonpath='{range.items[*]}{" "}{.metadata.name}'); \
do echo === $p ===; for container in $(oc get pods -n openshift-ovn-kubernetes $p \
-o json | jq -r '.status.containerStatuses[] | .name');do echo ---$container---; \
oc logs -c $container $p -n openshift-ovn-kubernetes --tail=5; done; done</pre></li><li class="listitem"><p class="simpara">
							View the last 5 lines of every log in every container in an <code class="literal cluster-admin">ovnkube-master</code> pod using the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc logs -l app=ovnkube-master -n openshift-ovn-kubernetes --all-containers --tail 5</pre></li></ol></div></section><section class="section cluster-admin" id="nw-ovn-kubernetes-logs-console_ovn-kubernetes-sources-of-troubleshooting-information"><div class="titlepage"><div><div><h3 class="title">26.3.5. Viewing the OVN-Kubernetes logs using the web console</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can view the logs for each of the pods in the <code class="literal cluster-admin">ovnkube-master</code> and <code class="literal cluster-admin">ovnkube-node</code> pods in the web console.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Access to the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							In the OpenShift Container Platform console, navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Workloads</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Pods</span></strong></span> or navigate to the pod through the resource you want to investigate.
						</li><li class="listitem">
							Select the <code class="literal cluster-admin">openshift-ovn-kubernetes</code> project from the drop-down menu.
						</li><li class="listitem">
							Click the name of the pod you want to investigate.
						</li><li class="listitem">
							Click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Logs</span></strong></span>. By default for the <code class="literal cluster-admin">ovnkube-master</code> the logs associated with the <code class="literal cluster-admin">northd</code> container are displayed.
						</li><li class="listitem">
							Use the down-down menu to select logs for each container in turn.
						</li></ol></div><section class="section cluster-admin" id="nw-ovn-kubernetes-change-log-levels_ovn-kubernetes-sources-of-troubleshooting-information"><div class="titlepage"><div><div><h4 class="title">26.3.5.1. Changing the OVN-Kubernetes log levels</h4></div></div></div><p class="cluster-admin cluster-admin">
						The default log level for OVN-Kubernetes is 2. To debug OVN-Kubernetes set the log level to 5. Follow this procedure to increase the log level of the OVN-Kubernetes to help you debug an issue.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								You have access to the cluster with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have access to the OpenShift Container Platform web console.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Run the following command to get detailed information for all pods in the OVN-Kubernetes project:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get po -o wide -n openshift-ovn-kubernetes</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                   READY   STATUS    RESTARTS      AGE   IP             NODE                           NOMINATED NODE   READINESS GATES
ovnkube-master-84nc9   6/6     Running   0             50m   10.0.134.156   ip-10-0-134-156.ec2.internal   &lt;none&gt;           &lt;none&gt;
ovnkube-master-gmlqv   6/6     Running   0             50m   10.0.209.180   ip-10-0-209-180.ec2.internal   &lt;none&gt;           &lt;none&gt;
ovnkube-master-nhts2   6/6     Running   1 (48m ago)   50m   10.0.147.31    ip-10-0-147-31.ec2.internal    &lt;none&gt;           &lt;none&gt;
ovnkube-node-2cbh8     5/5     Running   0             43m   10.0.217.114   ip-10-0-217-114.ec2.internal   &lt;none&gt;           &lt;none&gt;
ovnkube-node-6fvzl     5/5     Running   0             50m   10.0.147.31    ip-10-0-147-31.ec2.internal    &lt;none&gt;           &lt;none&gt;
ovnkube-node-f4lzz     5/5     Running   0             24m   10.0.146.76    ip-10-0-146-76.ec2.internal    &lt;none&gt;           &lt;none&gt;
ovnkube-node-jf67d     5/5     Running   0             50m   10.0.209.180   ip-10-0-209-180.ec2.internal   &lt;none&gt;           &lt;none&gt;
ovnkube-node-np9mf     5/5     Running   0             40m   10.0.165.191   ip-10-0-165-191.ec2.internal   &lt;none&gt;           &lt;none&gt;
ovnkube-node-qjldg     5/5     Running   0             50m   10.0.134.156   ip-10-0-134-156.ec2.internal   &lt;none&gt;           &lt;none&gt;</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Create a <code class="literal cluster-admin">ConfigMap</code> file similar to the following example and use a filename such as <code class="literal cluster-admin">env-overrides.yaml</code>:
							</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example <code class="literal cluster-admin">ConfigMap</code> file</strong></p><p>
									
<pre class="programlisting language-yaml">kind: ConfigMap
apiVersion: v1
metadata:
  name: env-overrides
  namespace: openshift-ovn-kubernetes
data:
  ip-10-0-217-114.ec2.internal: | <span id="CO142-1"><!--Empty--></span><span class="callout">1</span>
    # This sets the log level for the ovn-kubernetes node process:
    OVN_KUBE_LOG_LEVEL=5
    # You might also/instead want to enable debug logging for ovn-controller:
    OVN_LOG_LEVEL=dbg
  ip-10-0-209-180.ec2.internal: |
    # This sets the log level for the ovn-kubernetes node process:
    OVN_KUBE_LOG_LEVEL=5
    # You might also/instead want to enable debug logging for ovn-controller:
    OVN_LOG_LEVEL=dbg
  _master: | <span id="CO142-2"><!--Empty--></span><span class="callout">2</span>
    # This sets the log level for the ovn-kubernetes master process as well as the ovn-dbchecker:
    OVN_KUBE_LOG_LEVEL=5
    # You might also/instead want to enable debug logging for northd, nbdb and sbdb on all masters:
    OVN_LOG_LEVEL=dbg</pre>

								</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO142-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify the name of the node you want to set the debug log level on.
									</div></dd><dt><a href="#CO142-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specify <code class="literal cluster-admin">_master</code> to set the log levels of <code class="literal cluster-admin">ovnkube-master</code> components.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Apply the <code class="literal cluster-admin">ConfigMap</code> file by using the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create configmap env-overrides.yaml -n openshift-ovn-kubernetes</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">configmap/env-overrides.yaml created</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Restart the <code class="literal cluster-admin">ovnkube</code> pods to apply the new log level by using the following commands:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete pod -n openshift-ovn-kubernetes \
--field-selector spec.nodeName=ip-10-0-217-114.ec2.internal -l app=ovnkube-node</pre><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete pod -n openshift-ovn-kubernetes \
--field-selector spec.nodeName=ip-10-0-209-180.ec2.internal -l app=ovnkube-node</pre><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete pod -n openshift-ovn-kubernetes -l app=ovnkube-master</pre></li></ol></div></section></section><section class="section cluster-admin" id="nw-ovn-kubernetes-pod-connectivity-checks_ovn-kubernetes-sources-of-troubleshooting-information"><div class="titlepage"><div><div><h3 class="title">26.3.6. Checking the OVN-Kubernetes pod network connectivity</h3></div></div></div><p class="cluster-admin cluster-admin">
					The connectivity check controller, in OpenShift Container Platform 4.10 and later, orchestrates connection verification checks in your cluster. These include Kubernetes API, OpenShift API and individual nodes. The results for the connection tests are stored in <code class="literal cluster-admin">PodNetworkConnectivity</code> objects in the <code class="literal cluster-admin">openshift-network-diagnostics</code> namespace. Connection tests are performed every minute in parallel.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Access to the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed <code class="literal cluster-admin">jq</code>.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							To list the current <code class="literal cluster-admin">PodNetworkConnectivityCheck</code> objects, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get podnetworkconnectivitychecks -n openshift-network-diagnostics</pre></li><li class="listitem"><p class="simpara">
							View the most recent success for each connection object by using the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get podnetworkconnectivitychecks -n openshift-network-diagnostics \
-o json | jq '.items[]| .spec.targetEndpoint,.status.successes[0]'</pre></li><li class="listitem"><p class="simpara">
							View the most recent failures for each connection object by using the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get podnetworkconnectivitychecks -n openshift-network-diagnostics \
-o json | jq '.items[]| .spec.targetEndpoint,.status.failures[0]'</pre></li><li class="listitem"><p class="simpara">
							View the most recent outages for each connection object by using the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get podnetworkconnectivitychecks -n openshift-network-diagnostics \
-o json | jq '.items[]| .spec.targetEndpoint,.status.outages[0]'</pre><p class="cluster-admin cluster-admin">
							The connectivity check controller also logs metrics from these checks into Prometheus.
						</p></li><li class="listitem"><p class="simpara">
							View all the metrics by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec prometheus-k8s-0 -n openshift-monitoring -- \
promtool query instant  http://localhost:9090 \
'{component="openshift-network-diagnostics"}'</pre></li><li class="listitem"><p class="simpara">
							View the latency between the source pod and the openshift api service for the last 5 minutes:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec prometheus-k8s-0 -n openshift-monitoring -- \
promtool query instant  http://localhost:9090 \
'{component="openshift-network-diagnostics"}'</pre></li></ol></div></section><section class="section _additional-resources" id="additional-resources_ovn-kubernetes-sources-of-troubleshooting-information"><div class="titlepage"><div><div><h3 class="title">26.3.7. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/solutions/5892971">How do I change the ovn-kubernetes loglevel in OpenShift 4?</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-pod-network-connectivity-implementation_verifying-connectivity-endpoint">Implementation of connection health checks</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-pod-network-connectivity-verify_verifying-connectivity-endpoint">Verifying network connectivity for an endpoint</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="ovn-kubernetes-tracing-using-ovntrace"><div class="titlepage"><div><div><h2 class="title">26.4. Tracing Openflow with ovnkube-trace</h2></div></div></div><p class="cluster-admin cluster-admin">
				OVN and OVS traffic flows can be simulated in a single utility called <code class="literal cluster-admin">ovnkube-trace</code>. The <code class="literal cluster-admin">ovnkube-trace</code> utility runs <code class="literal cluster-admin">ovn-trace</code>, <code class="literal cluster-admin">ovs-appctl ofproto/trace</code> and <code class="literal cluster-admin">ovn-detrace</code> and correlates that information in a single output.
			</p><p class="cluster-admin cluster-admin">
				You can execute the <code class="literal cluster-admin">ovnkube-trace</code> binary from a dedicated container. For releases after OpenShift Container Platform 4.7, you can also copy the binary to a local host and execute it from that host.
			</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
					The binaries in the Quay images do not currently work for Dual IP stack or IPv6 only environments. For those environments, you must build from source.
				</p></div></div><section class="section cluster-admin" id="nw-ovn-kubernetes-install-ovnkube-trace-local_ovn-kubernetes-tracing-with-ovnkube"><div class="titlepage"><div><div><h3 class="title">26.4.1. Installing the ovnkube-trace on local host</h3></div></div></div><p class="cluster-admin cluster-admin">
					The <code class="literal cluster-admin">ovnkube-trace</code> tool traces packet simulations for arbitrary UDP or TCP traffic between points in an OVN-Kubernetes driven OpenShift Container Platform cluster. Copy the <code class="literal cluster-admin">ovnkube-trace</code> binary to your local host making it available to run against the cluster.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You are logged in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a pod variable by using the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$  POD=$(oc get pods -n openshift-ovn-kubernetes -l app=ovnkube-master -o name | head -1 | awk -F '/' '{print $NF}')</pre></li><li class="listitem"><p class="simpara">
							Run the following command on your local host to copy the binary from the <code class="literal cluster-admin">ovnkube-master</code> pods:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$  oc cp -n openshift-ovn-kubernetes $POD:/usr/bin/ovnkube-trace ovnkube-trace</pre></li><li class="listitem"><p class="simpara">
							Make <code class="literal cluster-admin">ovnkube-trace</code> executable by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$  chmod +x ovnkube-trace</pre></li><li class="listitem"><p class="simpara">
							Display the options available with <code class="literal cluster-admin">ovnkube-trace</code> by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$  ./ovnkube-trace -help</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">I0111 15:05:27.973305  204872 ovs.go:90] Maximum command line arguments set to: 191102
Usage of ./ovnkube-trace:
  -dst string
    	dest: destination pod name
  -dst-ip string
    	destination IP address (meant for tests to external targets)
  -dst-namespace string
    	k8s namespace of dest pod (default "default")
  -dst-port string
    	dst-port: destination port (default "80")
  -kubeconfig string
    	absolute path to the kubeconfig file
  -loglevel string
    	loglevel: klog level (default "0")
  -ovn-config-namespace string
    	namespace used by ovn-config itself
  -service string
    	service: destination service name
  -skip-detrace
    	skip ovn-detrace command
  -src string
    	src: source pod name
  -src-namespace string
    	k8s namespace of source pod (default "default")
  -tcp
    	use tcp transport protocol
  -udp
    	use udp transport protocol</pre>

							</p></div><p class="cluster-admin cluster-admin">
							The command-line arguments supported are familiar Kubernetes constructs, such as namespaces, pods, services so you do not need to find the MAC address, the IP address of the destination nodes, or the ICMP type.
						</p><p class="cluster-admin cluster-admin">
							The log levels are:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									0 (minimal output)
								</li><li class="listitem">
									2 (more verbose output showing results of trace commands)
								</li><li class="listitem">
									5 (debug output)
								</li></ul></div></li></ol></div></section><section class="section cluster-admin" id="nw-ovn-kubernetes-running-ovnkube-trace_ovn-kubernetes-tracing-with-ovnkube"><div class="titlepage"><div><div><h3 class="title">26.4.2. Running ovnkube-trace</h3></div></div></div><p class="cluster-admin cluster-admin">
					Run <code class="literal cluster-admin">ovn-trace</code> to simulate packet forwarding within an OVN logical network.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You are logged in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li><li class="listitem">
							You have installed <code class="literal cluster-admin">ovnkube-trace</code> on local host
						</li></ul></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example: Testing that DNS resolution works from a deployed pod</strong></p><p>
						This example illustrates how to test the DNS resolution from a deployed pod to the core DNS pod that runs in the cluster.
					</p></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Start a web service in the default namespace by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc run web --namespace=default --image=nginx --labels="app=web" --expose --port=80</pre></li><li class="listitem"><p class="simpara">
							List the pods running in the <code class="literal cluster-admin">openshift-dns</code> namespace:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">oc get pods -n openshift-dns</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                  READY   STATUS    RESTARTS   AGE
dns-default-467qw     2/2     Running   0          49m
dns-default-6prvx     2/2     Running   0          53m
dns-default-fkqr8     2/2     Running   0          53m
dns-default-qv2rg     2/2     Running   0          49m
dns-default-s29vr     2/2     Running   0          49m
dns-default-vdsbn     2/2     Running   0          53m
node-resolver-6thtt   1/1     Running   0          53m
node-resolver-7ksdn   1/1     Running   0          49m
node-resolver-8sthh   1/1     Running   0          53m
node-resolver-c5ksw   1/1     Running   0          50m
node-resolver-gbvdp   1/1     Running   0          53m
node-resolver-sxhkd   1/1     Running   0          50m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Run the following <code class="literal cluster-admin">ovn-kube-trace</code> command to verify DNS resolution is working:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ./ovnkube-trace \
  -src-namespace default \ <span id="CO143-1"><!--Empty--></span><span class="callout">1</span>
  -src web \ <span id="CO143-2"><!--Empty--></span><span class="callout">2</span>
  -dst-namespace openshift-dns \ <span id="CO143-3"><!--Empty--></span><span class="callout">3</span>
  -dst dns-default-467qw \ <span id="CO143-4"><!--Empty--></span><span class="callout">4</span>
  -udp -dst-port 53 \ <span id="CO143-5"><!--Empty--></span><span class="callout">5</span>
  -loglevel 0 <span id="CO143-6"><!--Empty--></span><span class="callout">6</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO143-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Namespace of the source pod
								</div></dd><dt><a href="#CO143-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Source pod name
								</div></dd><dt><a href="#CO143-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Namespace of destination pod
								</div></dd><dt><a href="#CO143-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Destination pod name
								</div></dd><dt><a href="#CO143-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Use the <code class="literal cluster-admin">udp</code> transport protocol. Port 53 is the port the DNS service uses.
								</div></dd><dt><a href="#CO143-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Set the log level to 1 (0 is minimal and 5 is debug)
								</div></dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">I0116 10:19:35.601303   17900 ovs.go:90] Maximum command line arguments set to: 191102
ovn-trace source pod to destination pod indicates success from web to dns-default-467qw
ovn-trace destination pod to source pod indicates success from dns-default-467qw to web
ovs-appctl ofproto/trace source pod to destination pod indicates success from web to dns-default-467qw
ovs-appctl ofproto/trace destination pod to source pod indicates success from dns-default-467qw to web
ovn-detrace source pod to destination pod indicates success from web to dns-default-467qw
ovn-detrace destination pod to source pod indicates success from dns-default-467qw to web</pre>

							</p></div><p class="cluster-admin cluster-admin">
							The ouput indicates success from the deployed pod to the DNS port and also indicates that it is successful going back in the other direction. So you know bi-directional traffic is supported on UDP port 53 if my web pod wants to do dns resolution from core DNS.
						</p></li></ol></div><p class="cluster-admin cluster-admin">
					If for example that did not work and you wanted to get the <code class="literal cluster-admin">ovn-trace</code>, the <code class="literal cluster-admin">ovs-appctl ofproto/trace</code> and <code class="literal cluster-admin">ovn-detrace</code>, and more debug type information increase the log level to 2 and run the command again as follows:
				</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ./ovnkube-trace \
  -src-namespace default \
  -src web \
  -dst-namespace openshift-dns \
  -dst dns-default-467qw \
  -udp -dst-port 53 \
  -loglevel 2</pre><p class="cluster-admin cluster-admin">
					The output from this increased log level is too much to list here. In a failure situation the output of this command shows which flow is dropping that traffic. For example an egress or ingress network policy may be configured on the cluster that does not allow that traffic.
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example: Verifying by using debug output a configured default deny</strong></p><p>
						This example illustrates how to identify by using the debug output that an ingress default deny policy blocks traffic.
					</p></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create the following YAML that defines a <code class="literal cluster-admin">deny-by-default</code> policy to deny ingress from all pods in all namespaces. Save the YAML in the <code class="literal cluster-admin">deny-by-default.yaml</code> file:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-by-default
  namespace: default
spec:
  podSelector: {}
  ingress: []</pre></li><li class="listitem"><p class="simpara">
							Apply the policy by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f deny-by-default.yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">networkpolicy.networking.k8s.io/deny-by-default created</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Start a web service in the <code class="literal cluster-admin">default</code> namespace by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc run web --namespace=default --image=nginx --labels="app=web" --expose --port=80</pre></li><li class="listitem"><p class="simpara">
							Run the following command to create the <code class="literal cluster-admin">prod</code> namespace:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create namespace prod</pre></li><li class="listitem"><p class="simpara">
							Run the following command to label the <code class="literal cluster-admin">prod</code> namespace:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc label namespace/prod purpose=production</pre></li><li class="listitem"><p class="simpara">
							Run the following command to deploy an <code class="literal cluster-admin">alpine</code> image in the <code class="literal cluster-admin">prod</code> namespace and start a shell:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc run test-6459 --namespace=prod --rm -i -t --image=alpine -- sh</pre></li><li class="listitem">
							Open another terminal session.
						</li><li class="listitem"><p class="simpara">
							In this new terminal session run <code class="literal cluster-admin">ovn-trace</code> to verify the failure in communication between the source pod <code class="literal cluster-admin">test-6459</code> running in namespace <code class="literal cluster-admin">prod</code> and destination pod running in the <code class="literal cluster-admin">default</code> namespace:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ./ovnkube-trace \
 -src-namespace prod \
 -src test-6459 \
 -dst-namespace default \
 -dst web \
 -tcp -dst-port 80 \
 -loglevel 0</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">I0116 14:20:47.380775   50822 ovs.go:90] Maximum command line arguments set to: 191102
ovn-trace source pod to destination pod indicates failure from test-6459 to web</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Increase the log level to 2 to expose the reason for the failure by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ./ovnkube-trace \
 -src-namespace prod \
 -src test-6459 \
 -dst-namespace default \
 -dst web \
 -tcp -dst-port 80 \
 -loglevel 2</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">ct_lb_mark /* default (use --ct to customize) */
------------------------------------------------
 3. ls_out_acl_hint (northd.c:6092): !ct.new &amp;&amp; ct.est &amp;&amp; !ct.rpl &amp;&amp; ct_mark.blocked == 0, priority 4, uuid 32d45ad4
    reg0[8] = 1;
    reg0[10] = 1;
    next;
 4. ls_out_acl (northd.c:6435): reg0[10] == 1 &amp;&amp; (outport == @a16982411286042166782_ingressDefaultDeny), priority 2000, uuid f730a887 <span id="CO144-1"><!--Empty--></span><span class="callout">1</span>
    ct_commit { ct_mark.blocked = 1; };</pre>

							</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO144-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Ingress traffic is blocked due to the default deny policy being in place
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create a policy that allows traffic from all pods in a particular namespaces with a label <code class="literal cluster-admin">purpose=production</code>. Save the YAML in the <code class="literal cluster-admin">web-allow-prod.yaml</code> file:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: web-allow-prod
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: web
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          purpose: production</pre></li><li class="listitem"><p class="simpara">
							Apply the policy by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f web-allow-prod.yaml</pre></li><li class="listitem"><p class="simpara">
							Run <code class="literal cluster-admin">ovnkube-trace</code> to verify that traffic is now allowed by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ./ovnkube-trace \
 -src-namespace prod \
 -src test-6459 \
 -dst-namespace default \
 -dst web \
 -tcp -dst-port 80 \
 -loglevel 0</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">I0116 14:25:44.055207   51695 ovs.go:90] Maximum command line arguments set to: 191102
ovn-trace source pod to destination pod indicates success from test-6459 to web
ovn-trace destination pod to source pod indicates success from web to test-6459
ovs-appctl ofproto/trace source pod to destination pod indicates success from test-6459 to web
ovs-appctl ofproto/trace destination pod to source pod indicates success from web to test-6459
ovn-detrace source pod to destination pod indicates success from test-6459 to web
ovn-detrace destination pod to source pod indicates success from web to test-6459</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							In the open shell run the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin"> wget -qO- --timeout=2 http://web.default</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Expected output</strong></p><p>
								
<pre class="programlisting language-terminal">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</pre>

							</p></div></li></ol></div></section><section class="section _additional-resources" id="additional-resources_ovn-kubernetes-tracing-with-ovnkube"><div class="titlepage"><div><div><h3 class="title">26.4.3. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/solutions/5887511">Tracing Openflow with ovnkube-trace utility</a>
						</li><li class="listitem">
							<a class="link" href="https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/ovnkube-trace.md">ovnkube-trace</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="migrate-from-openshift-sdn"><div class="titlepage"><div><div><h2 class="title">26.5. Migrating from the OpenShift SDN network plugin</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can migrate to the OVN-Kubernetes network plugin from the OpenShift SDN network plugin.
			</p><p class="cluster-admin cluster-admin">
				To learn more about OVN-Kubernetes, read <a class="link" href="#about-ovn-kubernetes" title="26.1. About the OVN-Kubernetes network plugin">About the OVN-Kubernetes network plugin</a>.
			</p><section class="section cluster-admin" id="nw-ovn-kubernetes-migration-about_migrate-from-openshift-sdn"><div class="titlepage"><div><div><h3 class="title">26.5.1. Migration to the OVN-Kubernetes network plugin</h3></div></div></div><p class="cluster-admin cluster-admin">
					Migrating to the OVN-Kubernetes network plugin is a manual process that includes some downtime during which your cluster is unreachable. Although a rollback procedure is provided, the migration is intended to be a one-way process.
				</p><p class="cluster-admin cluster-admin">
					A migration to the OVN-Kubernetes network plugin is supported on the following platforms:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Bare metal hardware
						</li><li class="listitem">
							Amazon Web Services (AWS)
						</li><li class="listitem">
							Google Cloud Platform (GCP)
						</li><li class="listitem">
							IBM Cloud
						</li><li class="listitem">
							Microsoft Azure
						</li><li class="listitem">
							Red Hat OpenStack Platform (RHOSP)
						</li><li class="listitem">
							Red Hat Virtualization (RHV)
						</li><li class="listitem">
							VMware vSphere
						</li></ul></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						Migrating to or from the OVN-Kubernetes network plugin is not supported for managed OpenShift cloud services such as Red Hat OpenShift Dedicated, Azure Red Hat OpenShift(ARO), and Red Hat OpenShift Service on AWS (ROSA).
					</p></div></div><section class="section cluster-admin" id="considerations-migrating-ovn-kubernetes-network-provider_migrate-from-openshift-sdn"><div class="titlepage"><div><div><h4 class="title">26.5.1.1. Considerations for migrating to the OVN-Kubernetes network plugin</h4></div></div></div><p class="cluster-admin cluster-admin">
						If you have more than 150 nodes in your OpenShift Container Platform cluster, then open a support case for consultation on your migration to the OVN-Kubernetes network plugin.
					</p><p class="cluster-admin cluster-admin">
						The subnets assigned to nodes and the IP addresses assigned to individual pods are not preserved during the migration.
					</p><p class="cluster-admin cluster-admin">
						While the OVN-Kubernetes network plugin implements many of the capabilities present in the OpenShift SDN network plugin, the configuration is not the same.
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								If your cluster uses any of the following OpenShift SDN network plugin capabilities, you must manually configure the same capability in the OVN-Kubernetes network plugin:
							</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem">
										Namespace isolation
									</li><li class="listitem">
										Egress router pods
									</li></ul></div></li><li class="listitem">
								If your cluster or surrounding network uses any part of the <code class="literal cluster-admin">100.64.0.0/16</code> address range, you must choose another unused IP range by specifying the <code class="literal cluster-admin">v4InternalSubnet</code> spec under the <code class="literal cluster-admin">spec.defaultNetwork.ovnKubernetesConfig</code> object definition. OVN-Kubernetes uses the IP range <code class="literal cluster-admin">100.64.0.0/16</code> internally by default.
							</li></ul></div><p class="cluster-admin cluster-admin">
						The following sections highlight the differences in configuration between the aforementioned capabilities in OVN-Kubernetes and OpenShift SDN network plugins.
					</p><h6 id="namespace-isolation_migrate-from-openshift-sdn">Namespace isolation</h6><p class="cluster-admin cluster-admin">
						OVN-Kubernetes supports only the network policy isolation mode.
					</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
							If your cluster uses OpenShift SDN configured in either the multitenant or subnet isolation modes, you cannot migrate to the OVN-Kubernetes network plugin.
						</p></div></div><h6 id="egress-ip-addresses_migrate-from-openshift-sdn">Egress IP addresses</h6><p class="cluster-admin cluster-admin">
						OpenShift SDN supports two different Egress IP modes:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								In the <span class="emphasis"><em><span class="cluster-admin cluster-admin">automatically assigned</span></em></span> approach, an egress IP address range is assigned to a node.
							</li><li class="listitem">
								In the <span class="emphasis"><em><span class="cluster-admin cluster-admin">manually assigned</span></em></span> approach, a list of one or more egress IP addresses is assigned to a node.
							</li></ul></div><p class="cluster-admin cluster-admin">
						The migration process supports migrating Egress IP configurations that use the automatically assigned mode.
					</p><p class="cluster-admin cluster-admin">
						The differences in configuring an egress IP address between OVN-Kubernetes and OpenShift SDN is described in the following table:
					</p><div class="table" id="idm140587106972592"><p class="title"><strong>Table 26.4. Differences in egress IP address configuration</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587106967488" scope="col">OVN-Kubernetes</th><th align="left" valign="top" id="idm140587106966400" scope="col">OpenShift SDN</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587106967488"> <div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
												Create an <code class="literal cluster-admin">EgressIPs</code> object
											</li><li class="listitem">
												Add an annotation on a <code class="literal cluster-admin">Node</code> object
											</li></ul></div>
									 </td><td align="left" valign="top" headers="idm140587106966400"> <div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
												Patch a <code class="literal cluster-admin">NetNamespace</code> object
											</li><li class="listitem">
												Patch a <code class="literal cluster-admin">HostSubnet</code> object
											</li></ul></div>
									 </td></tr></tbody></table></div></div><p class="cluster-admin cluster-admin">
						For more information on using egress IP addresses in OVN-Kubernetes, see "Configuring an egress IP address".
					</p><h6 id="egress-network-policies_migrate-from-openshift-sdn">Egress network policies</h6><p class="cluster-admin cluster-admin">
						The difference in configuring an egress network policy, also known as an egress firewall, between OVN-Kubernetes and OpenShift SDN is described in the following table:
					</p><div class="table" id="idm140587105269008"><p class="title"><strong>Table 26.5. Differences in egress network policy configuration</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587105263904" scope="col">OVN-Kubernetes</th><th align="left" valign="top" id="idm140587105262816" scope="col">OpenShift SDN</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587105263904"> <div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
												Create an <code class="literal cluster-admin">EgressFirewall</code> object in a namespace
											</li></ul></div>
									 </td><td align="left" valign="top" headers="idm140587105262816"> <div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
												Create an <code class="literal cluster-admin">EgressNetworkPolicy</code> object in a namespace
											</li></ul></div>
									 </td></tr></tbody></table></div></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							Because the name of an <code class="literal cluster-admin">EgressFirewall</code> object can only be set to <code class="literal cluster-admin">default</code>, after the migration all migrated <code class="literal cluster-admin">EgressNetworkPolicy</code> objects are named <code class="literal cluster-admin">default</code>, regardless of what the name was under OpenShift SDN.
						</p><p class="cluster-admin cluster-admin">
							If you subsequently rollback to OpenShift SDN, all <code class="literal cluster-admin">EgressNetworkPolicy</code> objects are named <code class="literal cluster-admin">default</code> as the prior name is lost.
						</p><p class="cluster-admin cluster-admin">
							For more information on using an egress firewall in OVN-Kubernetes, see "Configuring an egress firewall for a project".
						</p></div></div><h6 id="egress-router-pods_migrate-from-openshift-sdn">Egress router pods</h6><p class="cluster-admin cluster-admin">
						OVN-Kubernetes supports egress router pods in redirect mode. OVN-Kubernetes does not support egress router pods in HTTP proxy mode or DNS proxy mode.
					</p><p class="cluster-admin cluster-admin">
						When you deploy an egress router with the Cluster Network Operator, you cannot specify a node selector to control which node is used to host the egress router pod.
					</p><h6 id="multicast_migrate-from-openshift-sdn">Multicast</h6><p class="cluster-admin cluster-admin">
						The difference between enabling multicast traffic on OVN-Kubernetes and OpenShift SDN is described in the following table:
					</p><div class="table" id="idm140587105238624"><p class="title"><strong>Table 26.6. Differences in multicast configuration</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587109208592" scope="col">OVN-Kubernetes</th><th align="left" valign="top" id="idm140587109207504" scope="col">OpenShift SDN</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587109208592"> <div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
												Add an annotation on a <code class="literal cluster-admin">Namespace</code> object
											</li></ul></div>
									 </td><td align="left" valign="top" headers="idm140587109207504"> <div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
												Add an annotation on a <code class="literal cluster-admin">NetNamespace</code> object
											</li></ul></div>
									 </td></tr></tbody></table></div></div><p class="cluster-admin cluster-admin">
						For more information on using multicast in OVN-Kubernetes, see "Enabling multicast for a project".
					</p><h6 id="network-policies_migrate-from-openshift-sdn">Network policies</h6><p class="cluster-admin cluster-admin">
						OVN-Kubernetes fully supports the Kubernetes <code class="literal cluster-admin">NetworkPolicy</code> API in the <code class="literal cluster-admin">networking.k8s.io/v1</code> API group. No changes are necessary in your network policies when migrating from OpenShift SDN.
					</p></section><section class="section cluster-admin" id="how-the-migration-process-works_migrate-from-openshift-sdn"><div class="titlepage"><div><div><h4 class="title">26.5.1.2. How the migration process works</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following table summarizes the migration process by segmenting between the user-initiated steps in the process and the actions that the migration performs in response.
					</p><div class="table" id="idm140587109189568"><p class="title"><strong>Table 26.7. Migrating to OVN-Kubernetes from OpenShift SDN</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587109184400" scope="col">User-initiated steps</th><th align="left" valign="top" id="idm140587109183312" scope="col">Migration activity</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587109184400"> <p>
										Set the <code class="literal cluster-admin">migration</code> field of the <code class="literal cluster-admin">Network.operator.openshift.io</code> custom resource (CR) named <code class="literal cluster-admin">cluster</code> to <code class="literal cluster-admin">OVNKubernetes</code>. Make sure the <code class="literal cluster-admin">migration</code> field is <code class="literal cluster-admin">null</code> before setting it to a value.
									</p>
									 </td><td align="left" valign="top" headers="idm140587109183312"> <div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Cluster Network Operator (CNO)</span></dt><dd>
													Updates the status of the <code class="literal cluster-admin">Network.config.openshift.io</code> CR named <code class="literal cluster-admin">cluster</code> accordingly.
												</dd><dt><span class="term">Machine Config Operator (MCO)</span></dt><dd>
													Rolls out an update to the systemd configuration necessary for OVN-Kubernetes; the MCO updates a single machine per pool at a time by default, causing the total time the migration takes to increase with the size of the cluster.
												</dd></dl></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587109184400"> <p>
										Update the <code class="literal cluster-admin">networkType</code> field of the <code class="literal cluster-admin">Network.config.openshift.io</code> CR.
									</p>
									 </td><td align="left" valign="top" headers="idm140587109183312"> <div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">CNO</span></dt><dd><p class="simpara">
													Performs the following actions:
												</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
															Destroys the OpenShift SDN control plane pods.
														</li><li class="listitem">
															Deploys the OVN-Kubernetes control plane pods.
														</li><li class="listitem">
															Updates the Multus objects to reflect the new network plugin.
														</li></ul></div></dd></dl></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587109184400"> <p>
										Reboot each node in the cluster.
									</p>
									 </td><td align="left" valign="top" headers="idm140587109183312"> <div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Cluster</span></dt><dd>
													As nodes reboot, the cluster assigns IP addresses to pods on the OVN-Kubernetes cluster network.
												</dd></dl></div>
									 </td></tr></tbody></table></div></div><p class="cluster-admin cluster-admin">
						If a rollback to OpenShift SDN is required, the following table describes the process.
					</p><div class="table" id="idm140587108891744"><p class="title"><strong>Table 26.8. Performing a rollback to OpenShift SDN</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587108886592" scope="col">User-initiated steps</th><th align="left" valign="top" id="idm140587108885504" scope="col">Migration activity</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587108886592"> <p>
										Suspend the MCO to ensure that it does not interrupt the migration.
									</p>
									 </td><td align="left" valign="top" headers="idm140587108885504"> <p class="cluster-admin cluster-admin">
										The MCO stops.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587108886592"> <p>
										Set the <code class="literal cluster-admin">migration</code> field of the <code class="literal cluster-admin">Network.operator.openshift.io</code> custom resource (CR) named <code class="literal cluster-admin">cluster</code> to <code class="literal cluster-admin">OpenShiftSDN</code>. Make sure the <code class="literal cluster-admin">migration</code> field is <code class="literal cluster-admin">null</code> before setting it to a value.
									</p>
									 </td><td align="left" valign="top" headers="idm140587108885504"> <div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">CNO</span></dt><dd>
													Updates the status of the <code class="literal cluster-admin">Network.config.openshift.io</code> CR named <code class="literal cluster-admin">cluster</code> accordingly.
												</dd></dl></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587108886592"> <p>
										Update the <code class="literal cluster-admin">networkType</code> field.
									</p>
									 </td><td align="left" valign="top" headers="idm140587108885504"> <div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">CNO</span></dt><dd><p class="simpara">
													Performs the following actions:
												</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
															Destroys the OVN-Kubernetes control plane pods.
														</li><li class="listitem">
															Deploys the OpenShift SDN control plane pods.
														</li><li class="listitem">
															Updates the Multus objects to reflect the new network plugin.
														</li></ul></div></dd></dl></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587108886592"> <p>
										Reboot each node in the cluster.
									</p>
									 </td><td align="left" valign="top" headers="idm140587108885504"> <div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Cluster</span></dt><dd>
													As nodes reboot, the cluster assigns IP addresses to pods on the OpenShift-SDN network.
												</dd></dl></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587108886592"> <p>
										Enable the MCO after all nodes in the cluster reboot.
									</p>
									 </td><td align="left" valign="top" headers="idm140587108885504"> <div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">MCO</span></dt><dd>
													Rolls out an update to the systemd configuration necessary for OpenShift SDN; the MCO updates a single machine per pool at a time by default, so the total time the migration takes increases with the size of the cluster.
												</dd></dl></div>
									 </td></tr></tbody></table></div></div></section></section><section class="section cluster-admin" id="nw-ovn-kubernetes-migration_migrate-from-openshift-sdn"><div class="titlepage"><div><div><h3 class="title">26.5.2. Migrating to the OVN-Kubernetes network plugin</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can change the network plugin for your cluster to OVN-Kubernetes. During the migration, you must reboot every node in your cluster.
				</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						While performing the migration, your cluster is unavailable and workloads might be interrupted. Perform the migration only when an interruption in service is acceptable.
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							A cluster configured with the OpenShift SDN CNI network plugin in the network policy isolation mode.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li><li class="listitem">
							A recent backup of the etcd database is available.
						</li><li class="listitem">
							A reboot can be triggered manually for each node.
						</li><li class="listitem">
							The cluster is in a known good state, without any errors.
						</li><li class="listitem">
							On all cloud platforms after updating software, a security group rule must be in place to allow UDP packets on port <code class="literal cluster-admin">6081</code> for all nodes.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							To backup the configuration for the cluster network, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get Network.config.openshift.io cluster -o yaml &gt; cluster-openshift-sdn.yaml</pre></li><li class="listitem"><p class="simpara">
							To prepare all the nodes for the migration, set the <code class="literal cluster-admin">migration</code> field on the Cluster Network Operator configuration object by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": { "networkType": "OVNKubernetes" } } }'</pre><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								This step does not deploy OVN-Kubernetes immediately. Instead, specifying the <code class="literal cluster-admin">migration</code> field triggers the Machine Config Operator (MCO) to apply new machine configs to all the nodes in the cluster in preparation for the OVN-Kubernetes deployment.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Optional: You can disable automatic migration of several OpenShift SDN capabilities to the OVN-Kubernetes equivalents:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Egress IPs
								</li><li class="listitem">
									Egress firewall
								</li><li class="listitem">
									Multicast
								</li></ul></div><p class="cluster-admin cluster-admin">
							To disable automatic migration of the configuration for any of the previously noted OpenShift SDN features, specify the following keys:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{
    "spec": {
      "migration": {
        "networkType": "OVNKubernetes",
        "features": {
          "egressIP": &lt;bool&gt;,
          "egressFirewall": &lt;bool&gt;,
          "multicast": &lt;bool&gt;
        }
      }
    }
  }'</pre><p class="cluster-admin cluster-admin">
							where:
						</p><p class="cluster-admin cluster-admin">
							<code class="literal cluster-admin">bool</code>: Specifies whether to enable migration of the feature. The default is <code class="literal cluster-admin">true</code>.
						</p></li><li class="listitem"><p class="simpara">
							Optional: You can customize the following settings for OVN-Kubernetes to meet your network infrastructure requirements:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Maximum transmission unit (MTU)
								</li><li class="listitem">
									Geneve (Generic Network Virtualization Encapsulation) overlay network port
								</li><li class="listitem">
									OVN-Kubernetes IPv4 internal subnet
								</li><li class="listitem">
									OVN-Kubernetes IPv6 internal subnet
								</li></ul></div><p class="cluster-admin cluster-admin">
							To customize either of the previously noted settings, enter and customize the following command. If you do not need to change the default value, omit the key from the patch.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "ovnKubernetesConfig":{
          "mtu":&lt;mtu&gt;,
          "genevePort":&lt;port&gt;,
          "v4InternalSubnet":"&lt;ipv4_subnet&gt;",
          "v6InternalSubnet":"&lt;ipv6_subnet&gt;"
    }}}}'</pre><p class="cluster-admin cluster-admin">
							where:
						</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">mtu</code></span></dt><dd>
										The MTU for the Geneve overlay network. This value is normally configured automatically, but if the nodes in your cluster do not all use the same MTU, then you must set this explicitly to <code class="literal cluster-admin">100</code> less than the smallest node MTU value.
									</dd><dt><span class="term"><code class="literal cluster-admin">port</code></span></dt><dd>
										The UDP port for the Geneve overlay network. If a value is not specified, the default is <code class="literal cluster-admin">6081</code>. The port cannot be the same as the VXLAN port that is used by OpenShift SDN. The default value for the VXLAN port is <code class="literal cluster-admin">4789</code>.
									</dd><dt><span class="term"><code class="literal cluster-admin">ipv4_subnet</code></span></dt><dd>
										An IPv4 address range for internal use by OVN-Kubernetes. You must ensure that the IP address range does not overlap with any other subnet used by your OpenShift Container Platform installation. The IP address range must be larger than the maximum number of nodes that can be added to the cluster. The default value is <code class="literal cluster-admin">100.64.0.0/16</code>.
									</dd><dt><span class="term"><code class="literal cluster-admin">ipv6_subnet</code></span></dt><dd>
										An IPv6 address range for internal use by OVN-Kubernetes. You must ensure that the IP address range does not overlap with any other subnet used by your OpenShift Container Platform installation. The IP address range must be larger than the maximum number of nodes that can be added to the cluster. The default value is <code class="literal cluster-admin">fd98::/48</code>.
									</dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example patch command to update <code class="literal cluster-admin">mtu</code> field</strong></p><p>
								
<pre class="programlisting language-terminal">$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "ovnKubernetesConfig":{
          "mtu":1200
    }}}}'</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							As the MCO updates machines in each machine config pool, it reboots each node one by one. You must wait until all the nodes are updated. Check the machine config pool status by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get mcp</pre><p class="cluster-admin cluster-admin">
							A successfully updated node has the following status: <code class="literal cluster-admin">UPDATED=true</code>, <code class="literal cluster-admin">UPDATING=false</code>, <code class="literal cluster-admin">DEGRADED=false</code>.
						</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								By default, the MCO updates one machine per pool at a time, causing the total time the migration takes to increase with the size of the cluster.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Confirm the status of the new machine configuration on the hosts:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									To list the machine configuration state and the name of the applied machine configuration, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc describe node | egrep "hostname|machineconfig"</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">kubernetes.io/hostname=master-0
machineconfiguration.openshift.io/currentConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/desiredConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done</pre>

									</p></div><p class="cluster-admin cluster-admin">
									Verify that the following statements are true:
								</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											The value of <code class="literal cluster-admin">machineconfiguration.openshift.io/state</code> field is <code class="literal cluster-admin">Done</code>.
										</li><li class="listitem">
											The value of the <code class="literal cluster-admin">machineconfiguration.openshift.io/currentConfig</code> field is equal to the value of the <code class="literal cluster-admin">machineconfiguration.openshift.io/desiredConfig</code> field.
										</li></ul></div></li><li class="listitem"><p class="simpara">
									To confirm that the machine config is correct, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get machineconfig &lt;config_name&gt; -o yaml | grep ExecStart</pre><p class="cluster-admin cluster-admin">
									where <code class="literal cluster-admin">&lt;config_name&gt;</code> is the name of the machine config from the <code class="literal cluster-admin">machineconfiguration.openshift.io/currentConfig</code> field.
								</p><p class="cluster-admin cluster-admin">
									The machine config must include the following update to the systemd configuration:
								</p><pre class="programlisting language-plain cluster-admin cluster-admin">ExecStart=/usr/local/bin/configure-ovs.sh OVNKubernetes</pre></li><li class="listitem"><p class="simpara">
									If a node is stuck in the <code class="literal cluster-admin">NotReady</code> state, investigate the machine config daemon pod logs and resolve any errors.
								</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="i"><li class="listitem"><p class="simpara">
											To list the pods, enter the following command:
										</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pod -n openshift-machine-config-operator</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
												
<pre class="programlisting language-terminal">NAME                                         READY   STATUS    RESTARTS   AGE
machine-config-controller-75f756f89d-sjp8b   1/1     Running   0          37m
machine-config-daemon-5cf4b                  2/2     Running   0          43h
machine-config-daemon-7wzcd                  2/2     Running   0          43h
machine-config-daemon-fc946                  2/2     Running   0          43h
machine-config-daemon-g2v28                  2/2     Running   0          43h
machine-config-daemon-gcl4f                  2/2     Running   0          43h
machine-config-daemon-l5tnv                  2/2     Running   0          43h
machine-config-operator-79d9c55d5-hth92      1/1     Running   0          37m
machine-config-server-bsc8h                  1/1     Running   0          43h
machine-config-server-hklrm                  1/1     Running   0          43h
machine-config-server-k9rtx                  1/1     Running   0          43h</pre>

											</p></div><p class="cluster-admin cluster-admin">
											The names for the config daemon pods are in the following format: <code class="literal cluster-admin">machine-config-daemon-&lt;seq&gt;</code>. The <code class="literal cluster-admin">&lt;seq&gt;</code> value is a random five character alphanumeric sequence.
										</p></li><li class="listitem"><p class="simpara">
											Display the pod log for the first machine config daemon pod shown in the previous output by enter the following command:
										</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc logs &lt;pod&gt; -n openshift-machine-config-operator</pre><p class="cluster-admin cluster-admin">
											where <code class="literal cluster-admin">pod</code> is the name of a machine config daemon pod.
										</p></li><li class="listitem">
											Resolve any errors in the logs shown by the output from the previous command.
										</li></ol></div></li></ol></div></li><li class="listitem"><p class="simpara">
							To start the migration, configure the OVN-Kubernetes network plugin by using one of the following commands:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
									To specify the network provider without changing the cluster network IP address block, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.config.openshift.io cluster \
  --type='merge' --patch '{ "spec": { "networkType": "OVNKubernetes" } }'</pre></li><li class="listitem"><p class="simpara">
									To specify a different cluster network IP address block, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.config.openshift.io cluster \
  --type='merge' --patch '{
    "spec": {
      "clusterNetwork": [
        {
          "cidr": "&lt;cidr&gt;",
          "hostPrefix": &lt;prefix&gt;
        }
      ],
      "networkType": "OVNKubernetes"
    }
  }'</pre><p class="cluster-admin cluster-admin">
									where <code class="literal cluster-admin">cidr</code> is a CIDR block and <code class="literal cluster-admin">prefix</code> is the slice of the CIDR block apportioned to each node in your cluster. You cannot use any CIDR block that overlaps with the <code class="literal cluster-admin">100.64.0.0/16</code> CIDR block because the OVN-Kubernetes network provider uses this block internally.
								</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
										You cannot change the service network address block during the migration.
									</p></div></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Verify that the Multus daemon set rollout is complete before continuing with subsequent steps:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n openshift-multus rollout status daemonset/multus</pre><p class="cluster-admin cluster-admin">
							The name of the Multus pods is in the form of <code class="literal cluster-admin">multus-&lt;xxxxx&gt;</code> where <code class="literal cluster-admin">&lt;xxxxx&gt;</code> is a random sequence of letters. It might take several moments for the pods to restart.
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">Waiting for daemon set "multus" rollout to finish: 1 out of 6 new pods have been updated...
...
Waiting for daemon set "multus" rollout to finish: 5 of 6 updated pods are available...
daemon set "multus" successfully rolled out</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To complete changing the network plugin, reboot each node in your cluster. You can reboot the nodes in your cluster with either of the following approaches:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
									With the <code class="literal cluster-admin">oc rsh</code> command, you can use a bash script similar to the following:
								</p><pre class="programlisting language-bash cluster-admin cluster-admin">#!/bin/bash
readarray -t POD_NODES &lt;&lt;&lt; "$(oc get pod -n openshift-machine-config-operator -o wide| grep daemon|awk '{print $1" "$7}')"

for i in "${POD_NODES[@]}"
do
  read -r POD NODE &lt;&lt;&lt; "$i"
  until oc rsh -n openshift-machine-config-operator "$POD" chroot /rootfs shutdown -r +1
    do
      echo "cannot reboot node $NODE, retry" &amp;&amp; sleep 3
    done
done</pre></li><li class="listitem"><p class="simpara">
									With the <code class="literal cluster-admin">ssh</code> command, you can use a bash script similar to the following. The script assumes that you have configured sudo to not prompt for a password.
								</p><pre class="programlisting language-bash cluster-admin cluster-admin">#!/bin/bash

for ip in $(oc get nodes  -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}')
do
   echo "reboot node $ip"
   ssh -o StrictHostKeyChecking=no core@$ip sudo shutdown -r -t 3
done</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							Confirm that the migration succeeded:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									To confirm that the network plugin is OVN-Kubernetes, enter the following command. The value of <code class="literal cluster-admin">status.networkType</code> must be <code class="literal cluster-admin">OVNKubernetes</code>.
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get network.config/cluster -o jsonpath='{.status.networkType}{"\n"}'</pre></li><li class="listitem"><p class="simpara">
									To confirm that the cluster nodes are in the <code class="literal cluster-admin">Ready</code> state, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get nodes</pre></li><li class="listitem"><p class="simpara">
									To confirm that your pods are not in an error state, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods --all-namespaces -o wide --sort-by='{.spec.nodeName}'</pre><p class="cluster-admin cluster-admin">
									If pods on a node are in an error state, reboot that node.
								</p></li><li class="listitem"><p class="simpara">
									To confirm that all of the cluster Operators are not in an abnormal state, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get co</pre><p class="cluster-admin cluster-admin">
									The status of every cluster Operator must be the following: <code class="literal cluster-admin">AVAILABLE="True"</code>, <code class="literal cluster-admin">PROGRESSING="False"</code>, <code class="literal cluster-admin">DEGRADED="False"</code>. If a cluster Operator is not available or degraded, check the logs for the cluster Operator for more information.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Complete the following steps only if the migration succeeds and your cluster is in a good state:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									To remove the migration configuration from the CNO configuration object, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": null } }'</pre></li><li class="listitem"><p class="simpara">
									To remove custom configuration for the OpenShift SDN network provider, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "defaultNetwork": { "openshiftSDNConfig": null } } }'</pre></li><li class="listitem"><p class="simpara">
									To remove the OpenShift SDN network provider namespace, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete namespace openshift-sdn</pre></li></ol></div></li></ol></div></section><section class="section _additional-resources" id="migrate-from-openshift-sdn-additional-resources"><div class="titlepage"><div><div><h3 class="title">26.5.3. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-operator-configuration-parameters-for-ovn-sdn_cluster-network-operator">Configuration parameters for the OVN-Kubernetes network plugin</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/backup_and_restore/#backup-etcd">Backing up etcd</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-network-policy">About network policy</a>
						</li><li class="listitem"><p class="simpara">
							OVN-Kubernetes capabilities
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-egress-ips-ovn">Configuring an egress IP address</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-egress-firewall-ovn">Configuring an egress firewall for a project</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-ovn-kubernetes-enabling-multicast">Enabling multicast for a project</a>
								</li></ul></div></li><li class="listitem"><p class="simpara">
							OpenShift SDN capabilities
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#assigning-egress-ips">Configuring egress IPs for a project</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-egress-firewall">Configuring an egress firewall for a project</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#enabling-multicast">Enabling multicast for a project</a>
								</li></ul></div></li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/api_reference/#network-operator-openshift-io-v1">Network [operator.openshift.io/v1</a>]
						</li></ul></div></section></section><section class="section cluster-admin" id="rollback-to-openshift-sdn"><div class="titlepage"><div><div><h2 class="title">26.6. Rolling back to the OpenShift SDN network provider</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can rollback to the OpenShift SDN network plugin from the OVN-Kubernetes network plugin if the migration to OVN-Kubernetes is unsuccessful.
			</p><section class="section cluster-admin" id="nw-ovn-kubernetes-rollback_rollback-to-openshift-sdn"><div class="titlepage"><div><div><h3 class="title">26.6.1. Migrating to the OpenShift SDN network plugin</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can migrate to the OpenShift SDN Container Network Interface (CNI) network plugin. During the migration you must reboot every node in your cluster.
				</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						Rollback to OpenShift SDN if the migration to OVN-Kubernetes fails.
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li><li class="listitem">
							A cluster installed on infrastructure configured with the OVN-Kubernetes network plugin.
						</li><li class="listitem">
							A recent backup of the etcd database is available.
						</li><li class="listitem">
							A reboot can be triggered manually for each node.
						</li><li class="listitem">
							The cluster is in a known good state, without any errors.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Stop all of the machine configuration pools managed by the Machine Config Operator (MCO):
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
									Stop the master configuration pool:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch MachineConfigPool master --type='merge' --patch \
  '{ "spec": { "paused": true } }'</pre></li><li class="listitem"><p class="simpara">
									Stop the worker machine configuration pool:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch MachineConfigPool worker --type='merge' --patch \
  '{ "spec":{ "paused": true } }'</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							To prepare for the migration, set the migration field to <code class="literal cluster-admin">null</code> by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": null } }'</pre></li><li class="listitem"><p class="simpara">
							To start the migration, set the network plugin back to OpenShift SDN by entering the following commands:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": { "networkType": "OpenShiftSDN" } } }'

$ oc patch Network.config.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "networkType": "OpenShiftSDN" } }'</pre></li><li class="listitem"><p class="simpara">
							Optional: You can disable automatic migration of several OVN-Kubernetes capabilities to the OpenShift SDN equivalents:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Egress IPs
								</li><li class="listitem">
									Egress firewall
								</li><li class="listitem">
									Multicast
								</li></ul></div><p class="cluster-admin cluster-admin">
							To disable automatic migration of the configuration for any of the previously noted OpenShift SDN features, specify the following keys:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{
    "spec": {
      "migration": {
        "networkType": "OpenShiftSDN",
        "features": {
          "egressIP": &lt;bool&gt;,
          "egressFirewall": &lt;bool&gt;,
          "multicast": &lt;bool&gt;
        }
      }
    }
  }'</pre><p class="cluster-admin cluster-admin">
							where:
						</p><p class="cluster-admin cluster-admin">
							<code class="literal cluster-admin">bool</code>: Specifies whether to enable migration of the feature. The default is <code class="literal cluster-admin">true</code>.
						</p></li><li class="listitem"><p class="simpara">
							Optional: You can customize the following settings for OpenShift SDN to meet your network infrastructure requirements:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Maximum transmission unit (MTU)
								</li><li class="listitem">
									VXLAN port
								</li></ul></div><p class="cluster-admin cluster-admin">
							To customize either or both of the previously noted settings, customize and enter the following command. If you do not need to change the default value, omit the key from the patch.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "openshiftSDNConfig":{
          "mtu":&lt;mtu&gt;,
          "vxlanPort":&lt;port&gt;
    }}}}'</pre><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">mtu</code></span></dt><dd>
										The MTU for the VXLAN overlay network. This value is normally configured automatically, but if the nodes in your cluster do not all use the same MTU, then you must set this explicitly to <code class="literal cluster-admin">50</code> less than the smallest node MTU value.
									</dd><dt><span class="term"><code class="literal cluster-admin">port</code></span></dt><dd>
										The UDP port for the VXLAN overlay network. If a value is not specified, the default is <code class="literal cluster-admin">4789</code>. The port cannot be the same as the Geneve port that is used by OVN-Kubernetes. The default value for the Geneve port is <code class="literal cluster-admin">6081</code>.
									</dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example patch command</strong></p><p>
								
<pre class="programlisting language-terminal">$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "openshiftSDNConfig":{
          "mtu":1200
    }}}}'</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Wait until the Multus daemon set rollout completes.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n openshift-multus rollout status daemonset/multus</pre><p class="cluster-admin cluster-admin">
							The name of the Multus pods is in form of <code class="literal cluster-admin">multus-&lt;xxxxx&gt;</code> where <code class="literal cluster-admin">&lt;xxxxx&gt;</code> is a random sequence of letters. It might take several moments for the pods to restart.
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">Waiting for daemon set "multus" rollout to finish: 1 out of 6 new pods have been updated...
...
Waiting for daemon set "multus" rollout to finish: 5 of 6 updated pods are available...
daemon set "multus" successfully rolled out</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To complete changing the network plugin, reboot each node in your cluster. You can reboot the nodes in your cluster with either of the following approaches:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
									With the <code class="literal cluster-admin">oc rsh</code> command, you can use a bash script similar to the following:
								</p><pre class="programlisting language-bash cluster-admin cluster-admin">#!/bin/bash
readarray -t POD_NODES &lt;&lt;&lt; "$(oc get pod -n openshift-machine-config-operator -o wide| grep daemon|awk '{print $1" "$7}')"

for i in "${POD_NODES[@]}"
do
  read -r POD NODE &lt;&lt;&lt; "$i"
  until oc rsh -n openshift-machine-config-operator "$POD" chroot /rootfs shutdown -r +1
    do
      echo "cannot reboot node $NODE, retry" &amp;&amp; sleep 3
    done
done</pre></li><li class="listitem"><p class="simpara">
									With the <code class="literal cluster-admin">ssh</code> command, you can use a bash script similar to the following. The script assumes that you have configured sudo to not prompt for a password.
								</p><pre class="programlisting language-bash cluster-admin cluster-admin">#!/bin/bash

for ip in $(oc get nodes  -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}')
do
   echo "reboot node $ip"
   ssh -o StrictHostKeyChecking=no core@$ip sudo shutdown -r -t 3
done</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							After the nodes in your cluster have rebooted, start all of the machine configuration pools:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
									Start the master configuration pool:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch MachineConfigPool master --type='merge' --patch \
  '{ "spec": { "paused": false } }'</pre></li><li class="listitem"><p class="simpara">
									Start the worker configuration pool:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch MachineConfigPool worker --type='merge' --patch \
  '{ "spec": { "paused": false } }'</pre></li></ul></div><p class="cluster-admin cluster-admin">
							As the MCO updates machines in each config pool, it reboots each node.
						</p><p class="cluster-admin cluster-admin">
							By default the MCO updates a single machine per pool at a time, so the time that the migration requires to complete grows with the size of the cluster.
						</p></li><li class="listitem"><p class="simpara">
							Confirm the status of the new machine configuration on the hosts:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									To list the machine configuration state and the name of the applied machine configuration, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc describe node | egrep "hostname|machineconfig"</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">kubernetes.io/hostname=master-0
machineconfiguration.openshift.io/currentConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/desiredConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done</pre>

									</p></div><p class="cluster-admin cluster-admin">
									Verify that the following statements are true:
								</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											The value of <code class="literal cluster-admin">machineconfiguration.openshift.io/state</code> field is <code class="literal cluster-admin">Done</code>.
										</li><li class="listitem">
											The value of the <code class="literal cluster-admin">machineconfiguration.openshift.io/currentConfig</code> field is equal to the value of the <code class="literal cluster-admin">machineconfiguration.openshift.io/desiredConfig</code> field.
										</li></ul></div></li><li class="listitem"><p class="simpara">
									To confirm that the machine config is correct, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get machineconfig &lt;config_name&gt; -o yaml</pre><p class="cluster-admin cluster-admin">
									where <code class="literal cluster-admin">&lt;config_name&gt;</code> is the name of the machine config from the <code class="literal cluster-admin">machineconfiguration.openshift.io/currentConfig</code> field.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Confirm that the migration succeeded:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									To confirm that the network plugin is OpenShift SDN, enter the following command. The value of <code class="literal cluster-admin">status.networkType</code> must be <code class="literal cluster-admin">OpenShiftSDN</code>.
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get network.config/cluster -o jsonpath='{.status.networkType}{"\n"}'</pre></li><li class="listitem"><p class="simpara">
									To confirm that the cluster nodes are in the <code class="literal cluster-admin">Ready</code> state, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get nodes</pre></li><li class="listitem"><p class="simpara">
									If a node is stuck in the <code class="literal cluster-admin">NotReady</code> state, investigate the machine config daemon pod logs and resolve any errors.
								</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="i"><li class="listitem"><p class="simpara">
											To list the pods, enter the following command:
										</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pod -n openshift-machine-config-operator</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
												
<pre class="programlisting language-terminal">NAME                                         READY   STATUS    RESTARTS   AGE
machine-config-controller-75f756f89d-sjp8b   1/1     Running   0          37m
machine-config-daemon-5cf4b                  2/2     Running   0          43h
machine-config-daemon-7wzcd                  2/2     Running   0          43h
machine-config-daemon-fc946                  2/2     Running   0          43h
machine-config-daemon-g2v28                  2/2     Running   0          43h
machine-config-daemon-gcl4f                  2/2     Running   0          43h
machine-config-daemon-l5tnv                  2/2     Running   0          43h
machine-config-operator-79d9c55d5-hth92      1/1     Running   0          37m
machine-config-server-bsc8h                  1/1     Running   0          43h
machine-config-server-hklrm                  1/1     Running   0          43h
machine-config-server-k9rtx                  1/1     Running   0          43h</pre>

											</p></div><p class="cluster-admin cluster-admin">
											The names for the config daemon pods are in the following format: <code class="literal cluster-admin">machine-config-daemon-&lt;seq&gt;</code>. The <code class="literal cluster-admin">&lt;seq&gt;</code> value is a random five character alphanumeric sequence.
										</p></li><li class="listitem"><p class="simpara">
											To display the pod log for each machine config daemon pod shown in the previous output, enter the following command:
										</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc logs &lt;pod&gt; -n openshift-machine-config-operator</pre><p class="cluster-admin cluster-admin">
											where <code class="literal cluster-admin">pod</code> is the name of a machine config daemon pod.
										</p></li><li class="listitem">
											Resolve any errors in the logs shown by the output from the previous command.
										</li></ol></div></li><li class="listitem"><p class="simpara">
									To confirm that your pods are not in an error state, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods --all-namespaces -o wide --sort-by='{.spec.nodeName}'</pre><p class="cluster-admin cluster-admin">
									If pods on a node are in an error state, reboot that node.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Complete the following steps only if the migration succeeds and your cluster is in a good state:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									To remove the migration configuration from the Cluster Network Operator configuration object, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": null } }'</pre></li><li class="listitem"><p class="simpara">
									To remove the OVN-Kubernetes configuration, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "defaultNetwork": { "ovnKubernetesConfig":null } } }'</pre></li><li class="listitem"><p class="simpara">
									To remove the OVN-Kubernetes network provider namespace, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete namespace openshift-ovn-kubernetes</pre></li></ol></div></li></ol></div></section></section><section class="section cluster-admin" id="migrate-from-kuryr-sdn"><div class="titlepage"><div><div><h2 class="title">26.7. Migrating from the Kuryr network plugin to the OVN-Kubernetes network plugin</h2></div></div></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
					Migration from Kuryr to OVN-Kubernetes is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
				</p><p class="cluster-admin cluster-admin">
					For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
				</p></div></div><p class="cluster-admin cluster-admin">
				As the administrator of a cluster that runs on Red Hat OpenStack Platform (RHOSP), you can migrate to the OVN-Kubernetes network plugin from the Kuryr SDN network plugin.
			</p><p class="cluster-admin cluster-admin">
				To learn more about OVN-Kubernetes, read <a class="link" href="#about-ovn-kubernetes" title="26.1. About the OVN-Kubernetes network plugin">About the OVN-Kubernetes network plugin</a>.
			</p><section class="section cluster-admin" id="nw-kuryr-ovn-kubernetes-migration-about_migrate-from-kuryr-sdn"><div class="titlepage"><div><div><h3 class="title">26.7.1. Migration to the OVN-Kubernetes network provider</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can manually migrate a cluster that runs on Red Hat OpenStack Platform (RHOSP) to the OVN-Kubernetes network provider.
				</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						Migration to OVN-Kubernetes is a one-way process. During migration, your cluster will be unreachable for a brief time.
					</p></div></div><section class="section cluster-admin" id="considerations-kuryr-migrating-network-provider_migrate-from-kuryr-sdn"><div class="titlepage"><div><div><h4 class="title">26.7.1.1. Considerations when migrating to the OVN-Kubernetes network provider</h4></div></div></div><p class="cluster-admin cluster-admin">
						Kubernetes namespaces are kept by Kuryr in separate RHOSP networking service (Neutron) subnets. Those subnets and the IP addresses that are assigned to individual pods are not preserved during the migration.
					</p></section><section class="section cluster-admin" id="how-the-kuryr-migration-process-works_migrate-from-kuryr-sdn"><div class="titlepage"><div><div><h4 class="title">26.7.1.2. How the migration process works</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following table summarizes the migration process by relating the steps that you perform with the actions that your cluster and Operators take.
					</p><div class="table" id="idm140587108195984"><p class="title"><strong>Table 26.9. The Kuryr to OVN-Kubernetes migration process</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587106896080" scope="col">User-initiated steps</th><th align="left" valign="top" id="idm140587106894992" scope="col">Migration activity</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587106896080"> <p>
										Set the <code class="literal cluster-admin">migration</code> field of the <code class="literal cluster-admin">Network.operator.openshift.io</code> custom resource (CR) named <code class="literal cluster-admin">cluster</code> to <code class="literal cluster-admin">OVNKubernetes</code>. Verify that the value of the <code class="literal cluster-admin">migration</code> field prints the <code class="literal cluster-admin">null</code> value before setting it to another value.
									</p>
									 </td><td align="left" valign="top" headers="idm140587106894992"> <div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Cluster Network Operator (CNO)</span></dt><dd>
													Updates the status of the <code class="literal cluster-admin">Network.config.openshift.io</code> CR named <code class="literal cluster-admin">cluster</code> accordingly.
												</dd><dt><span class="term">Machine Config Operator (MCO)</span></dt><dd>
													Deploys an update to the systemd configuration that is required by OVN-Kubernetes. By default, the MCO updates a single machine per pool at a time. As a result, large clusters have longer migration times.
												</dd></dl></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587106896080"> <p>
										Update the <code class="literal cluster-admin">networkType</code> field of the <code class="literal cluster-admin">Network.config.openshift.io</code> CR.
									</p>
									 </td><td align="left" valign="top" headers="idm140587106894992"> <div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">CNO</span></dt><dd><p class="simpara">
													Performs the following actions:
												</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
															Destroys the Kuryr control plane pods: Kuryr CNIs and the Kuryr controller.
														</li><li class="listitem">
															Deploys the OVN-Kubernetes control plane pods.
														</li><li class="listitem">
															Updates the Multus objects to reflect the new network plugin.
														</li></ul></div></dd></dl></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587106896080"> <p>
										Reboot each node in the cluster.
									</p>
									 </td><td align="left" valign="top" headers="idm140587106894992"> <div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Cluster</span></dt><dd>
													As nodes reboot, the cluster assigns IP addresses to pods on the OVN-Kubernetes cluster network.
												</dd></dl></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587106896080"> <p>
										Clean up remaining resources Kuryr controlled.
									</p>
									 </td><td align="left" valign="top" headers="idm140587106894992"> <div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Cluster</span></dt><dd>
													Holds RHOSP resources that need to be freed, as well as OpenShift Container Platform resources to configure.
												</dd></dl></div>
									 </td></tr></tbody></table></div></div></section></section><section class="section cluster-admin" id="nw-kuryr-migration_migrate-from-kuryr-sdn"><div class="titlepage"><div><div><h3 class="title">26.7.2. Migrating to the OVN-Kubernetes network plugin</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can change the network plugin for your cluster to OVN-Kubernetes.
				</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						During the migration, you must reboot every node in your cluster. Your cluster is unavailable and workloads might be interrupted. Perform the migration only if an interruption in service is acceptable.
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li><li class="listitem">
							You have a recent backup of the etcd database is available.
						</li><li class="listitem">
							You can manually reboot each node.
						</li><li class="listitem">
							The cluster you plan to migrate is in a known good state, without any errors.
						</li><li class="listitem">
							You installed the Python interpreter.
						</li><li class="listitem">
							You installed the <code class="literal cluster-admin">openstacksdk</code> python package.
						</li><li class="listitem">
							You installed the <code class="literal cluster-admin">openstack</code> CLI tool.
						</li><li class="listitem">
							You have access to the underlying RHOSP cloud.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Back up the configuration for the cluster network by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get Network.config.openshift.io cluster -o yaml &gt; cluster-kuryr.yaml</pre></li><li class="listitem"><p class="simpara">
							To set the <code class="literal cluster-admin">CLUSTERID</code> variable, run the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ CLUSTERID=$(oc get infrastructure.config.openshift.io cluster -o=jsonpath='{.status.infrastructureName}')</pre></li><li class="listitem"><p class="simpara">
							To prepare all the nodes for the migration, set the <code class="literal cluster-admin">migration</code> field on the Cluster Network Operator configuration object by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": { "networkType": "OVNKubernetes" } } }'</pre><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								This step does not deploy OVN-Kubernetes immediately. Specifying the <code class="literal cluster-admin">migration</code> field triggers the Machine Config Operator (MCO) to apply new machine configs to all the nodes in the cluster. This prepares the cluster for the OVN-Kubernetes deployment.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Optional: Customize the following settings for OVN-Kubernetes for your network infrastructure requirements:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Maximum transmission unit (MTU)
								</li><li class="listitem">
									Geneve (Generic Network Virtualization Encapsulation) overlay network port
								</li><li class="listitem">
									OVN-Kubernetes IPv4 internal subnet
								</li><li class="listitem">
									OVN-Kubernetes IPv6 internal subnet
								</li></ul></div><p class="cluster-admin cluster-admin">
							To customize these settings, enter and customize the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "ovnKubernetesConfig":{
          "mtu":&lt;mtu&gt;,
          "genevePort":&lt;port&gt;,
          "v4InternalSubnet":"&lt;ipv4_subnet&gt;",
          "v6InternalSubnet":"&lt;ipv6_subnet&gt;"
    }}}}'</pre><p class="cluster-admin cluster-admin">
							where:
						</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">mtu</code></span></dt><dd>
										Specifies the MTU for the Geneve overlay network. This value is normally configured automatically, but if the nodes in your cluster do not all use the same MTU, then you must set this explicitly to <code class="literal cluster-admin">100</code> less than the smallest node MTU value.
									</dd><dt><span class="term"><code class="literal cluster-admin">port</code></span></dt><dd>
										Specifies the UDP port for the Geneve overlay network. If a value is not specified, the default is <code class="literal cluster-admin">6081</code>. The port cannot be the same as the VXLAN port that is used by Kuryr. The default value for the VXLAN port is <code class="literal cluster-admin">4789</code>.
									</dd><dt><span class="term"><code class="literal cluster-admin">ipv4_subnet</code></span></dt><dd>
										Specifies an IPv4 address range for internal use by OVN-Kubernetes. You must ensure that the IP address range does not overlap with any other subnet used by your OpenShift Container Platform installation. The IP address range must be larger than the maximum number of nodes that can be added to the cluster. The default value is <code class="literal cluster-admin">100.64.0.0/16</code>.
									</dd><dt><span class="term"><code class="literal cluster-admin">ipv6_subnet</code></span></dt><dd>
										Specifies an IPv6 address range for internal use by OVN-Kubernetes. You must ensure that the IP address range does not overlap with any other subnet used by your OpenShift Container Platform installation. The IP address range must be larger than the maximum number of nodes that can be added to the cluster. The default value is <code class="literal cluster-admin">fd98::/48</code>.
									</dd></dl></div><p class="cluster-admin cluster-admin">
							If you do not need to change the default value, omit the key from the patch.
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example patch command to update <code class="literal cluster-admin">mtu</code> field</strong></p><p>
								
<pre class="programlisting language-terminal">$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "ovnKubernetesConfig":{
          "mtu":1200
    }}}}'</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check the machine config pool status by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get mcp</pre><p class="cluster-admin cluster-admin">
							While the MCO updates machines in each machine config pool, it reboots each node one by one. You must wait until all the nodes are updated before continuing.
						</p><p class="cluster-admin cluster-admin">
							A successfully updated node has the following status: <code class="literal cluster-admin">UPDATED=true</code>, <code class="literal cluster-admin">UPDATING=false</code>, <code class="literal cluster-admin">DEGRADED=false</code>.
						</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								By default, the MCO updates one machine per pool at a time. Large clusters take more time to migrate than small clusters.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Confirm the status of the new machine configuration on the hosts:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									To list the machine configuration state and the name of the applied machine configuration, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc describe node | egrep "hostname|machineconfig"</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">kubernetes.io/hostname=master-0
machineconfiguration.openshift.io/currentConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b <span id="CO145-1"><!--Empty--></span><span class="callout">1</span>
machineconfiguration.openshift.io/desiredConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b <span id="CO145-2"><!--Empty--></span><span class="callout">2</span>
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Review the output from the previous step. The following statements must be true:
								</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											The value of <code class="literal cluster-admin">machineconfiguration.openshift.io/state</code> field is <code class="literal cluster-admin">Done</code>.
										</li><li class="listitem">
											The value of the <code class="literal cluster-admin">machineconfiguration.openshift.io/currentConfig</code> field is equal to the value of the <code class="literal cluster-admin">machineconfiguration.openshift.io/desiredConfig</code> field.
										</li></ul></div></li><li class="listitem"><p class="simpara">
									To confirm that the machine config is correct, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get machineconfig &lt;config_name&gt; -o yaml | grep ExecStart</pre><p class="cluster-admin cluster-admin">
									where:
								</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">&lt;config_name&gt;</span></dt><dd><p class="simpara">
												Specifies the name of the machine config from the <code class="literal cluster-admin">machineconfiguration.openshift.io/currentConfig</code> field.
											</p><p class="cluster-admin cluster-admin">
												The machine config must include the following update to the systemd configuration:
											</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
													
<pre class="programlisting language-plain">ExecStart=/usr/local/bin/configure-ovs.sh OVNKubernetes</pre>

												</p></div></dd></dl></div></li><li class="listitem"><p class="simpara">
									If a node is stuck in the <code class="literal cluster-admin">NotReady</code> state, investigate the machine config daemon pod logs and resolve any errors:
								</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="i"><li class="listitem"><p class="simpara">
											To list the pods, enter the following command:
										</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pod -n openshift-machine-config-operator</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
												
<pre class="programlisting language-terminal">NAME                                         READY   STATUS    RESTARTS   AGE
machine-config-controller-75f756f89d-sjp8b   1/1     Running   0          37m
machine-config-daemon-5cf4b                  2/2     Running   0          43h
machine-config-daemon-7wzcd                  2/2     Running   0          43h
machine-config-daemon-fc946                  2/2     Running   0          43h
machine-config-daemon-g2v28                  2/2     Running   0          43h
machine-config-daemon-gcl4f                  2/2     Running   0          43h
machine-config-daemon-l5tnv                  2/2     Running   0          43h
machine-config-operator-79d9c55d5-hth92      1/1     Running   0          37m
machine-config-server-bsc8h                  1/1     Running   0          43h
machine-config-server-hklrm                  1/1     Running   0          43h
machine-config-server-k9rtx                  1/1     Running   0          43h</pre>

											</p></div><p class="cluster-admin cluster-admin">
											The names for the config daemon pods are in the following format: <code class="literal cluster-admin">machine-config-daemon-&lt;seq&gt;</code>. The <code class="literal cluster-admin">&lt;seq&gt;</code> value is a random five character alphanumeric sequence.
										</p></li><li class="listitem"><p class="simpara">
											Display the pod log for the first machine config daemon pod shown in the previous output by enter the following command:
										</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc logs &lt;pod&gt; -n openshift-machine-config-operator</pre><p class="cluster-admin cluster-admin">
											where:
										</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">&lt;pod&gt;</span></dt><dd>
														Specifies the name of a machine config daemon pod.
													</dd></dl></div></li><li class="listitem">
											Resolve any errors in the logs shown by the output from the previous command.
										</li></ol></div></li></ol></div></li><li class="listitem"><p class="simpara">
							To start the migration, configure the OVN-Kubernetes network plugin by using one of the following commands:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
									To specify the network provider without changing the cluster network IP address block, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.config.openshift.io cluster \
  --type='merge' --patch '{ "spec": { "networkType": "OVNKubernetes" } }'</pre></li><li class="listitem"><p class="simpara">
									To specify a different cluster network IP address block, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.config.openshift.io cluster \
  --type='merge' --patch '{
    "spec": {
      "clusterNetwork": [
        {
          "cidr": "&lt;cidr&gt;",
          "hostPrefix": "&lt;prefix&gt;"
        }
      ]
      "networkType": "OVNKubernetes"
    }
  }'</pre><p class="cluster-admin cluster-admin">
									where:
								</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">&lt;cidr&gt;</span></dt><dd>
												Specifies a CIDR block.
											</dd><dt><span class="term">&lt;prefix&gt;</span></dt><dd><p class="simpara">
												Specifies a slice of the CIDR block that is apportioned to each node in your cluster.
											</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
													You cannot change the service network address block during the migration.
												</p><p class="cluster-admin cluster-admin">
													You cannot use any CIDR block that overlaps with the <code class="literal cluster-admin">100.64.0.0/16</code> CIDR block because the OVN-Kubernetes network provider uses this block internally.
												</p></div></div></dd></dl></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Verify that the Multus daemon set rollout is complete by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n openshift-multus rollout status daemonset/multus</pre><p class="cluster-admin cluster-admin">
							The name of the Multus pods is in the form of <code class="literal cluster-admin">multus-&lt;xxxxx&gt;</code>, where <code class="literal cluster-admin">&lt;xxxxx&gt;</code> is a random sequence of letters. It might take several moments for the pods to restart.
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">Waiting for daemon set "multus" rollout to finish: 1 out of 6 new pods have been updated...
...
Waiting for daemon set "multus" rollout to finish: 5 of 6 updated pods are available...
daemon set "multus" successfully rolled out</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To complete the migration, reboot each node in your cluster. For example, you can use a bash script similar to the following example. The script assumes that you can connect to each host by using <code class="literal cluster-admin">ssh</code> and that you have configured <code class="literal cluster-admin">sudo</code> to not prompt for a password:
						</p><pre class="programlisting language-bash cluster-admin cluster-admin">#!/bin/bash

for ip in $(oc get nodes  -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}')
do
   echo "reboot node $ip"
   ssh -o StrictHostKeyChecking=no core@$ip sudo shutdown -r -t 3
done</pre><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								If SSH access is not available, you can use the <code class="literal cluster-admin">openstack</code> command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ for name in $(openstack server list --name ${CLUSTERID}\* -f value -c Name); do openstack server reboot $name; done</pre><p class="cluster-admin cluster-admin">
								Alternatively, you might be able to reboot each node through the management portal for your infrastructure provider. Otherwise, contact the appropriate authority to either gain access to the virtual machines through SSH or the management portal and OpenStack client.
							</p></div></div></li></ol></div><div class="orderedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Confirm that the migration succeeded, and then remove the migration resources:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									To confirm that the network plugin is OVN-Kubernetes, enter the following command.
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get network.config/cluster -o jsonpath='{.status.networkType}{"\n"}'</pre><p class="cluster-admin cluster-admin">
									The value of <code class="literal cluster-admin">status.networkType</code> must be <code class="literal cluster-admin">OVNKubernetes</code>.
								</p></li><li class="listitem"><p class="simpara">
									To confirm that the cluster nodes are in the <code class="literal cluster-admin">Ready</code> state, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get nodes</pre></li><li class="listitem"><p class="simpara">
									To confirm that your pods are not in an error state, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods --all-namespaces -o wide --sort-by='{.spec.nodeName}'</pre><p class="cluster-admin cluster-admin">
									If pods on a node are in an error state, reboot that node.
								</p></li><li class="listitem"><p class="simpara">
									To confirm that all of the cluster Operators are not in an abnormal state, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get co</pre><p class="cluster-admin cluster-admin">
									The status of every cluster Operator must be the following: <code class="literal cluster-admin">AVAILABLE="True"</code>, <code class="literal cluster-admin">PROGRESSING="False"</code>, <code class="literal cluster-admin">DEGRADED="False"</code>. If a cluster Operator is not available or degraded, check the logs for the cluster Operator for more information.
								</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
										Do not proceed if any of the previous verification steps indicate errors. You might encounter pods that have a <code class="literal cluster-admin">Terminating</code> state due to finalizers that are removed during clean up. They are not an error indication.
									</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
							If the migration completed and your cluster is in a good state, remove the migration configuration from the CNO configuration object by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": null } }'</pre></li></ol></div></section><section class="section cluster-admin" id="nw-kuryr-cleanup_migrate-from-kuryr-sdn"><div class="titlepage"><div><div><h3 class="title">26.7.3. Cleaning up resources after migration</h3></div></div></div><p class="cluster-admin cluster-admin">
					After migration from the Kuryr network plugin to the OVN-Kubernetes network plugin, you must clean up the resources that Kuryr created previously.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						The clean up process relies on a Python virtual environment to ensure that the package versions that you use support tags for Octavia objects. You do not need a virtual environment if you are certain that your environment uses at minimum: * <code class="literal cluster-admin">openstacksdk</code> version 0.54.0 * <code class="literal cluster-admin">python-openstackclient</code> version 5.5.0 * <code class="literal cluster-admin">python-octaviaclient</code> version 2.3.0
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You installed the OpenShift Container Platform CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You installed a Python interpreter.
						</li><li class="listitem">
							You installed the <code class="literal cluster-admin">openstacksdk</code> Python package.
						</li><li class="listitem">
							You installed the <code class="literal cluster-admin">openstack</code> CLI.
						</li><li class="listitem">
							You have access to the underlying RHOSP cloud.
						</li><li class="listitem">
							You can access the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a clean-up Python virtual environment:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a temporary directory for your environment. For example:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ python3 -m venv /tmp/venv</pre><p class="cluster-admin cluster-admin">
									The virtual environment located in <code class="literal cluster-admin">/tmp/venv</code> directory is used in all clean up examples.
								</p></li><li class="listitem"><p class="simpara">
									Enter the virtual environment. For example:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ source /tmp/venv/bin/activate</pre></li><li class="listitem"><p class="simpara">
									Upgrade the <code class="literal cluster-admin">pip</code> command in the virtual environment by running the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ pip install pip --upgrade</pre></li><li class="listitem"><p class="simpara">
									Install the required Python packages by running the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ pip install openstacksdk==0.54.0 python-openstackclient==5.5.0 python-octaviaclient==2.3.0</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							In your terminal, set variables to cluster and Kuryr identifiers by running the following commands:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Set the cluster ID:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ CLUSTERID=$(oc get infrastructure.config.openshift.io cluster -o=jsonpath='{.status.infrastructureName}')</pre></li><li class="listitem"><p class="simpara">
									Set the cluster tag:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ CLUSTERTAG="openshiftClusterID=${CLUSTERID}"</pre></li><li class="listitem"><p class="simpara">
									Set the router ID:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ ROUTERID=$(oc get kuryrnetwork -A --no-headers -o custom-columns=":status.routerId"|head -n 1)</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a Bash function that removes finalizers from specified resources by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ function REMFIN {
    local resource=$1
    local finalizer=$2
    for res in $(oc get $resource -A --template='{{range $i,$p := .items}}{{ $p.metadata.name }}|{{ $p.metadata.namespace }}{{"\n"}}{{end}}'); do
        name=${res%%|*}
        ns=${res##*|}
        yaml=$(oc get -n $ns $resource $name -o yaml)
        if echo "${yaml}" | grep -q "${finalizer}"; then
            echo "${yaml}" | grep -v  "${finalizer}" | oc replace -n $ns $resource $name -f -
        fi
    done
}</pre><p class="cluster-admin cluster-admin">
							The function takes two parameters: the first parameter is name of the resource, and the second parameter is the finalizer to remove. The named resource is removed from the cluster and its definition is replaced with copied data, excluding the specified finalizer.
						</p></li><li class="listitem"><p class="simpara">
							To remove Kuryr finalizers from services, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ REMFIN services kuryr.openstack.org/service-finalizer</pre></li><li class="listitem"><p class="simpara">
							To remove the Kuryr <code class="literal cluster-admin">service-subnet-gateway-ip</code> service, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ if $(oc get -n openshift-kuryr service service-subnet-gateway-ip &amp;&gt;/dev/null); then
    oc -n openshift-kuryr delete service service-subnet-gateway-ip
fi</pre></li><li class="listitem"><p class="simpara">
							To remove all tagged RHOSP load balancers from Octavia, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ for lb in $(openstack loadbalancer list --tags $CLUSTERTAG -f value -c id); do
    openstack loadbalancer delete --cascade $lb
done</pre></li><li class="listitem"><p class="simpara">
							To remove Kuryr finalizers from all <code class="literal cluster-admin">KuryrLoadBalancer</code> CRs, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ REMFIN kuryrloadbalancers.openstack.org kuryr.openstack.org/kuryrloadbalancer-finalizers</pre></li><li class="listitem"><p class="simpara">
							To remove the <code class="literal cluster-admin">openshift-kuryr</code> namespace, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ oc delete namespace openshift-kuryr</pre></li><li class="listitem"><p class="simpara">
							To remove the Kuryr service subnet from the router, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ openstack router remove subnet $ROUTERID ${CLUSTERID}-kuryr-service-subnet</pre></li><li class="listitem"><p class="simpara">
							To remove the Kuryr service network, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ openstack network delete ${CLUSTERID}-kuryr-service-network</pre></li><li class="listitem"><p class="simpara">
							To remove Kuryr finalizers from all pods, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ REMFIN pods kuryr.openstack.org/pod-finalizer</pre></li><li class="listitem"><p class="simpara">
							To remove Kuryr finalizers from all <code class="literal cluster-admin">KuryrPort</code> CRs, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ REMFIN kuryrports.openstack.org kuryr.openstack.org/kuryrport-finalizer</pre><p class="cluster-admin cluster-admin">
							This command deletes the <code class="literal cluster-admin">KuryrPort</code> CRs.
						</p></li><li class="listitem"><p class="simpara">
							To remove Kuryr finalizers from network policies, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ REMFIN networkpolicy kuryr.openstack.org/networkpolicy-finalizer</pre></li><li class="listitem"><p class="simpara">
							To remove Kuryr finalizers from remaining network policies, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ REMFIN kuryrnetworkpolicies.openstack.org kuryr.openstack.org/networkpolicy-finalizer</pre></li><li class="listitem"><p class="simpara">
							To remove subports that Kuryr created from trunks, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ read -ra trunks &lt;&lt;&lt; $(python -c "import openstack; n = openstack.connect().network; print(' '.join([x.id for x in n.trunks(any_tags='$CLUSTERTAG')]))") &amp;&amp; \
i=0 &amp;&amp; \
for trunk in "${trunks[@]}"; do
    i=$((i+1))
    echo "Processing trunk $trunk, ${i}/${#trunks[@]}."
    subports=()
    for subport in $(python -c "import openstack; n = openstack.connect().network; print(' '.join([x['port_id'] for x in n.get_trunk('$trunk').sub_ports if '$CLUSTERTAG' in n.get_port(x['port_id']).tags]))"); do
        subports+=("$subport");
    done
    args=()
    for sub in "${subports[@]}" ; do
        args+=("--subport $sub")
    done
    if [ ${#args[@]} -gt 0 ]; then
        openstack network trunk unset ${args[*]} $trunk
    fi
done</pre></li><li class="listitem"><p class="simpara">
							To retrieve all networks and subnets from <code class="literal cluster-admin">KuryrNetwork</code> CRs and remove ports, router interfaces and the network itself, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ mapfile -t kuryrnetworks &lt; &lt;(oc get kuryrnetwork -A --template='{{range $i,$p := .items}}{{ $p.status.netId }}|{{ $p.status.subnetId }}{{"\n"}}{{end}}') &amp;&amp; \
i=0 &amp;&amp; \
for kn in "${kuryrnetworks[@]}"; do
    i=$((i+1))
    netID=${kn%%|*}
    subnetID=${kn##*|}
    echo "Processing network $netID, ${i}/${#kuryrnetworks[@]}"
    # Remove all ports from the network.
    for port in $(python -c "import openstack; n = openstack.connect().network; print(' '.join([x.id for x in n.ports(network_id='$netID') if x.device_owner != 'network:router_interface']))"); do
        ( openstack port delete $port ) &amp;

        # Only allow 20 jobs in parallel.
        if [[ $(jobs -r -p | wc -l) -ge 20 ]]; then
            wait -n
        fi
    done
    wait

    # Remove the subnet from the router.
    openstack router remove subnet $ROUTERID $subnetID

    # Remove the network.
    openstack network delete $netID
done</pre></li><li class="listitem"><p class="simpara">
							To remove the Kuryr security group, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ openstack security group delete ${CLUSTERID}-kuryr-pods-security-group</pre></li><li class="listitem"><p class="simpara">
							To remove all tagged subnet pools, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ for subnetpool in $(openstack subnet pool list --tags $CLUSTERTAG -f value -c ID); do
    openstack subnet pool delete $subnetpool
done</pre></li><li class="listitem"><p class="simpara">
							To check that all of the networks based on <code class="literal cluster-admin">KuryrNetwork</code> CRs were removed, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ networks=$(oc get kuryrnetwork -A --no-headers -o custom-columns=":status.netId") &amp;&amp; \
for existingNet in $(openstack network list --tags $CLUSTERTAG -f value -c ID); do
    if [[ $networks =~ $existingNet ]]; then
        echo "Network still exists: $existingNet"
    fi
done</pre><p class="cluster-admin cluster-admin">
							If the command returns any existing networks, intestigate and remove them before you continue.
						</p></li><li class="listitem"><p class="simpara">
							To remove security groups that are related to network policy, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ for sgid in $(openstack security group list -f value -c ID -c Description | grep 'Kuryr-Kubernetes Network Policy' | cut -f 1 -d ' '); do
    openstack security group delete $sgid
done</pre></li><li class="listitem"><p class="simpara">
							To remove finalizers from <code class="literal cluster-admin">KuryrNetwork</code> CRs, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ REMFIN kuryrnetworks.openstack.org kuryrnetwork.finalizers.kuryr.openstack.org</pre></li><li class="listitem"><p class="simpara">
							To remove the Kuryr router, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">(venv) $ if $(python3 -c "import sys; import openstack; n = openstack.connect().network; r = n.get_router('$ROUTERID'); sys.exit(0) if r.description != 'Created By OpenShift Installer' else sys.exit(1)"); then
    openstack router delete $ROUTERID
fi</pre></li></ol></div></section><section class="section _additional-resources" id="migrate-from-kuryr-additional-resources"><div class="titlepage"><div><div><h3 class="title">26.7.4. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-operator-configuration-parameters-for-ovn-sdn_cluster-network-operator">Configuration parameters for the OVN-Kubernetes network plugin</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/backup_and_restore/#backup-etcd">Backing up etcd</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-network-policy">About network policy</a>
						</li><li class="listitem"><p class="simpara">
							To learn more about OVN-Kubernetes capabilities, see:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-egress-ips-ovn">Configuring an egress IP address</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-egress-firewall-ovn">Configuring an egress firewall for a project</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-ovn-kubernetes-enabling-multicast">Enabling multicast for a project</a>
								</li></ul></div></li></ul></div></section></section><section class="section cluster-admin" id="converting-to-dual-stack"><div class="titlepage"><div><div><h2 class="title">26.8. Converting to IPv4/IPv6 dual-stack networking</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can convert your IPv4 single-stack cluster to a dual-network cluster network that supports IPv4 and IPv6 address families. After converting to dual-stack, all newly created pods are dual-stack enabled.
			</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
					A dual-stack network is supported on clusters provisioned on bare metal, IBM Power, IBM Z infrastructure, and single node OpenShift clusters.
				</p></div></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
					While using dual-stack networking, you cannot use IPv4-mapped IPv6 addresses, such as <code class="literal cluster-admin">::FFFF:198.51.100.1</code>, where IPv6 is required.
				</p></div></div><section class="section cluster-admin" id="nw-dual-stack-convert_converting-to-dual-stack"><div class="titlepage"><div><div><h3 class="title">26.8.1. Converting to a dual-stack cluster network</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can convert your single-stack cluster network to a dual-stack cluster network.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						After converting to dual-stack networking only newly created pods are assigned IPv6 addresses. Any pods created before the conversion must be recreated to receive an IPv6 address.
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You are logged in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li><li class="listitem">
							Your cluster uses the OVN-Kubernetes network plugin.
						</li><li class="listitem">
							The cluster nodes have IPv6 addresses.
						</li><li class="listitem">
							You have configured an IPv6-enabled router based on your infrastructure.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							To specify IPv6 address blocks for the cluster and service networks, create a file containing the following YAML:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">- op: add
  path: /spec/clusterNetwork/-
  value: <span id="CO145-3"><!--Empty--></span><span class="callout">1</span>
    cidr: fd01::/48
    hostPrefix: 64
- op: add
  path: /spec/serviceNetwork/-
  value: fd02::/112 <span id="CO145-4"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO145-3"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify an object with the <code class="literal cluster-admin">cidr</code> and <code class="literal cluster-admin">hostPrefix</code> fields. The host prefix must be <code class="literal cluster-admin">64</code> or greater. The IPv6 CIDR prefix must be large enough to accommodate the specified host prefix.
								</div></dd><dt><a href="#CO145-1"><span class="callout">1</span></a> <a href="#CO145-4"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify an IPv6 CIDR with a prefix of <code class="literal cluster-admin">112</code>. Kubernetes uses only the lowest 16 bits. For a prefix of <code class="literal cluster-admin">112</code>, IP addresses are assigned from <code class="literal cluster-admin">112</code> to <code class="literal cluster-admin">128</code> bits.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							To patch the cluster network configuration, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch network.config.openshift.io cluster \
  --type='json' --patch-file &lt;file&gt;.yaml</pre><p class="cluster-admin cluster-admin">
							where:
						</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">file</code></span></dt><dd>
										Specifies the name of the file you created in the previous step.
									</dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">network.config.openshift.io/cluster patched</pre>

							</p></div></li></ol></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Verification</strong></p><p>
						Complete the following step to verify that the cluster network recognizes the IPv6 address blocks that you specified in the previous procedure.
					</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Display the network configuration:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc describe network</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">Status:
  Cluster Network:
    Cidr:               10.128.0.0/14
    Host Prefix:        23
    Cidr:               fd01::/48
    Host Prefix:        64
  Cluster Network MTU:  1400
  Network Type:         OVNKubernetes
  Service Network:
    172.30.0.0/16
    fd02::/112</pre>

							</p></div></li></ol></div></section><section class="section cluster-admin" id="nw-dual-stack-convert-back-single-stack_converting-to-dual-stack"><div class="titlepage"><div><div><h3 class="title">26.8.2. Converting to a single-stack cluster network</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can convert your dual-stack cluster network to a single-stack cluster network.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You are logged in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li><li class="listitem">
							Your cluster uses the OVN-Kubernetes network plugin.
						</li><li class="listitem">
							The cluster nodes have IPv6 addresses.
						</li><li class="listitem">
							You have enabled dual-stack networking.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal cluster-admin">networks.config.openshift.io</code> custom resource (CR) by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit networks.config.openshift.io</pre></li><li class="listitem">
							Remove the IPv6 specific configuration that you have added to the <code class="literal cluster-admin">cidr</code> and <code class="literal cluster-admin">hostPrefix</code> fields in the previous procedure.
						</li></ol></div></section></section><section class="section cluster-admin" id="logging-network-policy"><div class="titlepage"><div><div><h2 class="title">26.9. Logging for egress firewall and network policy rules</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can configure audit logging for your cluster and enable logging for one or more namespaces. OpenShift Container Platform produces audit logs for both egress firewalls and network policies.
			</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
					Audit logging is available for only the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-ovn-kubernetes">OVN-Kubernetes network plugin</a>.
				</p></div></div><section class="section cluster-admin" id="nw-networkpolicy-audit-concept_logging-network-policy"><div class="titlepage"><div><div><h3 class="title">26.9.1. Audit logging</h3></div></div></div><p class="cluster-admin cluster-admin">
					The OVN-Kubernetes network plugin uses Open Virtual Network (OVN) ACLs to manage egress firewalls and network policies. Audit logging exposes allow and deny ACL events.
				</p><p class="cluster-admin cluster-admin">
					You can configure the destination for audit logs, such as a syslog server or a UNIX domain socket. Regardless of any additional configuration, an audit log is always saved to <code class="literal cluster-admin">/var/log/ovn/acl-audit-log.log</code> on each OVN-Kubernetes pod in the cluster.
				</p><p class="cluster-admin cluster-admin">
					Audit logging is enabled per namespace by annotating the namespace with the <code class="literal cluster-admin">k8s.ovn.org/acl-logging</code> key as in the following example:
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example namespace annotation</strong></p><p>
						
<pre class="programlisting language-yaml">kind: Namespace
apiVersion: v1
metadata:
  name: example1
  annotations:
    k8s.ovn.org/acl-logging: |-
      {
        "deny": "info",
        "allow": "info"
      }</pre>

					</p></div><p class="cluster-admin cluster-admin">
					The logging format is compatible with syslog as defined by RFC5424. The syslog facility is configurable and defaults to <code class="literal cluster-admin">local0</code>. An example log entry might resemble the following:
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example ACL deny log entry for a network policy</strong></p><p>
						
<pre class="programlisting language-text">2021-06-13T19:33:11.590Z|00005|acl_log(ovn_pinctrl0)|INFO|name="verify-audit-logging_deny-all", verdict=drop, severity=alert: icmp,vlan_tci=0x0000,dl_src=0a:58:0a:80:02:39,dl_dst=0a:58:0a:80:02:37,nw_src=10.128.2.57,nw_dst=10.128.2.55,nw_tos=0,nw_ecn=0,nw_ttl=64,icmp_type=8,icmp_code=0</pre>

					</p></div><p class="cluster-admin cluster-admin">
					The following table describes namespace annotation values:
				</p><div class="table" id="idm140587108386112"><p class="title"><strong>Table 26.10. Audit logging namespace annotation</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 40%; " class="col_1"><!--Empty--></col><col style="width: 60%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587108380960" scope="col">Annotation</th><th align="left" valign="middle" id="idm140587108379872" scope="col">Value</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587108380960"> <p>
									<code class="literal cluster-admin">k8s.ovn.org/acl-logging</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587108379872"> <p class="cluster-admin cluster-admin">
									You must specify at least one of <code class="literal cluster-admin">allow</code>, <code class="literal cluster-admin">deny</code>, or both to enable audit logging for a namespace.
								</p>
								 <div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">deny</code></span></dt><dd>
												Optional: Specify <code class="literal cluster-admin">alert</code>, <code class="literal cluster-admin">warning</code>, <code class="literal cluster-admin">notice</code>, <code class="literal cluster-admin">info</code>, or <code class="literal cluster-admin">debug</code>.
											</dd><dt><span class="term"><code class="literal cluster-admin">allow</code></span></dt><dd>
												Optional: Specify <code class="literal cluster-admin">alert</code>, <code class="literal cluster-admin">warning</code>, <code class="literal cluster-admin">notice</code>, <code class="literal cluster-admin">info</code>, or <code class="literal cluster-admin">debug</code>.
											</dd></dl></div>
								 </td></tr></tbody></table></div></div></section><section class="section cluster-admin" id="network-policy-audit-configuration-logging-network-policy"><div class="titlepage"><div><div><h3 class="title">26.9.2. Audit configuration</h3></div></div></div><p class="cluster-admin cluster-admin">
					The configuration for audit logging is specified as part of the OVN-Kubernetes cluster network provider configuration. The following YAML illustrates the default values for the audit logging:
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Audit logging configuration</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  defaultNetwork:
    ovnKubernetesConfig:
      policyAuditConfig:
        destination: "null"
        maxFileSize: 50
        rateLimit: 20
        syslogFacility: local0</pre>

					</p></div><p class="cluster-admin cluster-admin">
					The following table describes the configuration fields for audit logging.
				</p><div class="table" id="idm140587108350992"><p class="title"><strong>Table 26.11. <code class="literal cluster-admin">policyAuditConfig</code> object</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587107188880" scope="col">Field</th><th align="left" valign="middle" id="idm140587107187792" scope="col">Type</th><th align="left" valign="middle" id="idm140587107186704" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587107188880"> <p>
									<code class="literal cluster-admin">rateLimit</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587107187792"> <p>
									integer
								</p>
								 </td><td align="left" valign="middle" headers="idm140587107186704"> <p class="cluster-admin cluster-admin">
									The maximum number of messages to generate every second per node. The default value is <code class="literal cluster-admin">20</code> messages per second.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587107188880"> <p>
									<code class="literal cluster-admin">maxFileSize</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587107187792"> <p>
									integer
								</p>
								 </td><td align="left" valign="middle" headers="idm140587107186704"> <p class="cluster-admin cluster-admin">
									The maximum size for the audit log in bytes. The default value is <code class="literal cluster-admin">50000000</code> or 50 MB.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587107188880"> <p>
									<code class="literal cluster-admin">destination</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587107187792"> <p>
									string
								</p>
								 </td><td align="left" valign="middle" headers="idm140587107186704"> <p class="cluster-admin cluster-admin">
									One of the following additional audit log targets:
								</p>
								 <div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">libc</code></span></dt><dd>
												The libc <code class="literal cluster-admin">syslog()</code> function of the journald process on the host.
											</dd><dt><span class="term"><code class="literal cluster-admin">udp:&lt;host&gt;:&lt;port&gt;</code></span></dt><dd>
												A syslog server. Replace <code class="literal cluster-admin">&lt;host&gt;:&lt;port&gt;</code> with the host and port of the syslog server.
											</dd><dt><span class="term"><code class="literal cluster-admin">unix:&lt;file&gt;</code></span></dt><dd>
												A Unix Domain Socket file specified by <code class="literal cluster-admin">&lt;file&gt;</code>.
											</dd><dt><span class="term"><code class="literal cluster-admin">null</code></span></dt><dd>
												Do not send the audit logs to any additional target.
											</dd></dl></div>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587107188880"> <p>
									<code class="literal cluster-admin">syslogFacility</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587107187792"> <p>
									string
								</p>
								 </td><td align="left" valign="middle" headers="idm140587107186704"> <p class="cluster-admin cluster-admin">
									The syslog facility, such as <code class="literal cluster-admin">kern</code>, as defined by RFC5424. The default value is <code class="literal cluster-admin">local0</code>.
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section cluster-admin" id="nw-networkpolicy-audit-configure_logging-network-policy"><div class="titlepage"><div><div><h3 class="title">26.9.3. Configuring egress firewall and network policy auditing for a cluster</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can customize audit logging for your cluster.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							To customize the audit logging configuration, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit network.operator.openshift.io/cluster</pre><div class="admonition tip cluster-admin"><div class="admonition_header">Tip</div><div><p class="cluster-admin cluster-admin">
							You can alternatively customize and apply the following YAML to configure audit logging:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  defaultNetwork:
    ovnKubernetesConfig:
      policyAuditConfig:
        destination: "null"
        maxFileSize: 50
        rateLimit: 20
        syslogFacility: local0</pre></div></div></li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							To create a namespace with network policies complete the following steps:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a namespace for verification:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat &lt;&lt;EOF| oc create -f -
kind: Namespace
apiVersion: v1
metadata:
  name: verify-audit-logging
  annotations:
    k8s.ovn.org/acl-logging: '{ "deny": "alert", "allow": "alert" }'
EOF</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-text">namespace/verify-audit-logging created</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Enable audit logging:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate namespace verify-audit-logging k8s.ovn.org/acl-logging='{ "deny": "alert", "allow": "alert" }'</pre><pre class="programlisting language-text cluster-admin cluster-admin">namespace/verify-audit-logging annotated</pre></li><li class="listitem"><p class="simpara">
									Create network policies for the namespace:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat &lt;&lt;EOF| oc create -n verify-audit-logging -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
spec:
  podSelector:
    matchLabels:
  policyTypes:
  - Ingress
  - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-same-namespace
spec:
  podSelector: {}
  policyTypes:
   - Ingress
   - Egress
  ingress:
    - from:
        - podSelector: {}
  egress:
    - to:
       - namespaceSelector:
          matchLabels:
            namespace: verify-audit-logging
EOF</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-text">networkpolicy.networking.k8s.io/deny-all created
networkpolicy.networking.k8s.io/allow-from-same-namespace created</pre>

									</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a pod for source traffic in the <code class="literal cluster-admin">default</code> namespace:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat &lt;&lt;EOF| oc create -n default -f -
apiVersion: v1
kind: Pod
metadata:
  name: client
spec:
  containers:
    - name: client
      image: registry.access.redhat.com/rhel7/rhel-tools
      command: ["/bin/sh", "-c"]
      args:
        ["sleep inf"]
EOF</pre></li><li class="listitem"><p class="simpara">
							Create two pods in the <code class="literal cluster-admin">verify-audit-logging</code> namespace:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ for name in client server; do
cat &lt;&lt;EOF| oc create -n verify-audit-logging -f -
apiVersion: v1
kind: Pod
metadata:
  name: ${name}
spec:
  containers:
    - name: ${name}
      image: registry.access.redhat.com/rhel7/rhel-tools
      command: ["/bin/sh", "-c"]
      args:
        ["sleep inf"]
EOF
done</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">pod/client created
pod/server created</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To generate traffic and produce network policy audit log entries, complete the following steps:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Obtain the IP address for pod named <code class="literal cluster-admin">server</code> in the <code class="literal cluster-admin">verify-audit-logging</code> namespace:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ POD_IP=$(oc get pods server -n verify-audit-logging -o jsonpath='{.status.podIP}')</pre></li><li class="listitem"><p class="simpara">
									Ping the IP address from the previous command from the pod named <code class="literal cluster-admin">client</code> in the <code class="literal cluster-admin">default</code> namespace and confirm that all packets are dropped:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec -it client -n default -- /bin/ping -c 2 $POD_IP</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-text">PING 10.128.2.55 (10.128.2.55) 56(84) bytes of data.

--- 10.128.2.55 ping statistics ---
2 packets transmitted, 0 received, 100% packet loss, time 2041ms</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Ping the IP address saved in the <code class="literal cluster-admin">POD_IP</code> shell environment variable from the pod named <code class="literal cluster-admin">client</code> in the <code class="literal cluster-admin">verify-audit-logging</code> namespace and confirm that all packets are allowed:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec -it client -n verify-audit-logging -- /bin/ping -c 2 $POD_IP</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-text">PING 10.128.0.86 (10.128.0.86) 56(84) bytes of data.
64 bytes from 10.128.0.86: icmp_seq=1 ttl=64 time=2.21 ms
64 bytes from 10.128.0.86: icmp_seq=2 ttl=64 time=0.440 ms

--- 10.128.0.86 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 0.440/1.329/2.219/0.890 ms</pre>

									</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Display the latest entries in the network policy audit log:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ for pod in $(oc get pods -n openshift-ovn-kubernetes -l app=ovnkube-node --no-headers=true | awk '{ print $1 }') ; do
    oc exec -it $pod -n openshift-ovn-kubernetes -- tail -4 /var/log/ovn/acl-audit-log.log
  done</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">Defaulting container name to ovn-controller.
Use 'oc describe pod/ovnkube-node-hdb8v -n openshift-ovn-kubernetes' to see all of the containers in this pod.
2021-06-13T19:33:11.590Z|00005|acl_log(ovn_pinctrl0)|INFO|name="verify-audit-logging_deny-all", verdict=drop, severity=alert: icmp,vlan_tci=0x0000,dl_src=0a:58:0a:80:02:39,dl_dst=0a:58:0a:80:02:37,nw_src=10.128.2.57,nw_dst=10.128.2.55,nw_tos=0,nw_ecn=0,nw_ttl=64,icmp_type=8,icmp_code=0
2021-06-13T19:33:12.614Z|00006|acl_log(ovn_pinctrl0)|INFO|name="verify-audit-logging_deny-all", verdict=drop, severity=alert: icmp,vlan_tci=0x0000,dl_src=0a:58:0a:80:02:39,dl_dst=0a:58:0a:80:02:37,nw_src=10.128.2.57,nw_dst=10.128.2.55,nw_tos=0,nw_ecn=0,nw_ttl=64,icmp_type=8,icmp_code=0
2021-06-13T19:44:10.037Z|00007|acl_log(ovn_pinctrl0)|INFO|name="verify-audit-logging_allow-from-same-namespace_0", verdict=allow, severity=alert: icmp,vlan_tci=0x0000,dl_src=0a:58:0a:80:02:3b,dl_dst=0a:58:0a:80:02:3a,nw_src=10.128.2.59,nw_dst=10.128.2.58,nw_tos=0,nw_ecn=0,nw_ttl=64,icmp_type=8,icmp_code=0
2021-06-13T19:44:11.037Z|00008|acl_log(ovn_pinctrl0)|INFO|name="verify-audit-logging_allow-from-same-namespace_0", verdict=allow, severity=alert: icmp,vlan_tci=0x0000,dl_src=0a:58:0a:80:02:3b,dl_dst=0a:58:0a:80:02:3a,nw_src=10.128.2.59,nw_dst=10.128.2.58,nw_tos=0,nw_ecn=0,nw_ttl=64,icmp_type=8,icmp_code=0</pre>

							</p></div></li></ol></div></section><section class="section cluster-admin" id="nw-networkpolicy-audit-enable_logging-network-policy"><div class="titlepage"><div><div><h3 class="title">26.9.4. Enabling egress firewall and network policy audit logging for a namespace</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can enable audit logging for a namespace.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							To enable audit logging for a namespace, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate namespace &lt;namespace&gt; \
  k8s.ovn.org/acl-logging='{ "deny": "alert", "allow": "notice" }'</pre><p class="cluster-admin cluster-admin">
							where:
						</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;namespace&gt;</code></span></dt><dd>
										Specifies the name of the namespace.
									</dd></dl></div><div class="admonition tip cluster-admin"><div class="admonition_header">Tip</div><div><p class="cluster-admin cluster-admin">
							You can alternatively apply the following YAML to enable audit logging:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">kind: Namespace
apiVersion: v1
metadata:
  name: &lt;namespace&gt;
  annotations:
    k8s.ovn.org/acl-logging: |-
      {
        "deny": "alert",
        "allow": "notice"
      }</pre></div></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">namespace/verify-audit-logging annotated</pre>

							</p></div></li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Display the latest entries in the audit log:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ for pod in $(oc get pods -n openshift-ovn-kubernetes -l app=ovnkube-node --no-headers=true | awk '{ print $1 }') ; do
    oc exec -it $pod -n openshift-ovn-kubernetes -- tail -4 /var/log/ovn/acl-audit-log.log
  done</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">2021-06-13T19:33:11.590Z|00005|acl_log(ovn_pinctrl0)|INFO|name="verify-audit-logging_deny-all", verdict=drop, severity=alert: icmp,vlan_tci=0x0000,dl_src=0a:58:0a:80:02:39,dl_dst=0a:58:0a:80:02:37,nw_src=10.128.2.57,nw_dst=10.128.2.55,nw_tos=0,nw_ecn=0,nw_ttl=64,icmp_type=8,icmp_code=0</pre>

							</p></div></li></ul></div></section><section class="section cluster-admin" id="nw-networkpolicy-audit-disable_logging-network-policy"><div class="titlepage"><div><div><h3 class="title">26.9.5. Disabling egress firewall and network policy audit logging for a namespace</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can disable audit logging for a namespace.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							To disable audit logging for a namespace, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate --overwrite namespace &lt;namespace&gt; k8s.ovn.org/acl-logging-</pre><p class="cluster-admin cluster-admin">
							where:
						</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;namespace&gt;</code></span></dt><dd>
										Specifies the name of the namespace.
									</dd></dl></div><div class="admonition tip cluster-admin"><div class="admonition_header">Tip</div><div><p class="cluster-admin cluster-admin">
							You can alternatively apply the following YAML to disable audit logging:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">kind: Namespace
apiVersion: v1
metadata:
  name: &lt;namespace&gt;
  annotations:
    k8s.ovn.org/acl-logging: null</pre></div></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">namespace/verify-audit-logging annotated</pre>

							</p></div></li></ul></div></section><section class="section _additional-resources" id="logging-network-policy-additional-resources"><div class="titlepage"><div><div><h3 class="title">26.9.6. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-network-policy">About network policy</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-egress-firewall-ovn">Configuring an egress firewall for a project</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="configuring-ipsec-ovn"><div class="titlepage"><div><div><h2 class="title">26.10. Configuring IPsec encryption</h2></div></div></div><p class="cluster-admin cluster-admin">
				With IPsec enabled, all pod-to-pod network traffic between nodes on the OVN-Kubernetes cluster network is encrypted with IPsec <span class="emphasis"><em><span class="cluster-admin cluster-admin">Transport mode</span></em></span>.
			</p><p class="cluster-admin cluster-admin">
				IPsec is disabled by default. It can be enabled either during or after installing the cluster. For information about cluster installation, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#ocp-installation-overview">OpenShift Container Platform installation overview</a>. If you need to enable IPsec after cluster installation, you must first resize your cluster MTU to account for the overhead of the IPsec ESP IP header.
			</p><p class="cluster-admin cluster-admin">
				The following documentation describes how to enable and disable IPSec after cluster installation.
			</p><section class="section cluster-admin" id="configuring-ipsec-ovn-prerequisites"><div class="titlepage"><div><div><h3 class="title">26.10.1. Prerequisites</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have decreased the size of the cluster MTU by <code class="literal cluster-admin">46</code> bytes to allow for the additional overhead of the IPsec ESP header. For more information on resizing the MTU that your cluster uses, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#changing-cluster-network-mtu">Changing the MTU for the cluster network</a>.
						</li></ul></div></section><section class="section cluster-admin" id="nw-ovn-ipsec-traffic_configuring-ipsec-ovn"><div class="titlepage"><div><div><h3 class="title">26.10.2. Types of network traffic flows encrypted by IPsec</h3></div></div></div><p class="cluster-admin cluster-admin">
					With IPsec enabled, only the following network traffic flows between pods are encrypted:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Traffic between pods on different nodes on the cluster network
						</li><li class="listitem">
							Traffic from a pod on the host network to a pod on the cluster network
						</li></ul></div><p class="cluster-admin cluster-admin">
					The following traffic flows are not encrypted:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Traffic between pods on the same node on the cluster network
						</li><li class="listitem">
							Traffic between pods on the host network
						</li><li class="listitem">
							Traffic from a pod on the cluster network to a pod on the host network
						</li></ul></div><p class="cluster-admin cluster-admin">
					The encrypted and unencrypted flows are illustrated in the following diagram:
				</p><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/84757edea0d11c95de018b57280f88ed/nw-ipsec-encryption.png" alt="IPsec encrypted and unencrypted traffic flows"/></div></div><section class="section cluster-admin" id="network-connectivity-requirements-when-ipsec-is-enabled"><div class="titlepage"><div><div><h4 class="title">26.10.2.1. Network connectivity requirements when IPsec is enabled</h4></div></div></div><p class="cluster-admin cluster-admin">
						You must configure the network connectivity between machines to allow OpenShift Container Platform cluster components to communicate. Each machine must be able to resolve the hostnames of all other machines in the cluster.
					</p><div class="table" id="idm140587109151696"><p class="title"><strong>Table 26.12. Ports used for all-machine to all-machine communications</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 22%; " class="col_1"><!--Empty--></col><col style="width: 22%; " class="col_2"><!--Empty--></col><col style="width: 56%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587109145584" scope="col">Protocol</th><th align="left" valign="top" id="idm140587109144496" scope="col">Port</th><th align="left" valign="top" id="idm140587109143408" scope="col">Description</th></tr></thead><tbody><tr><td rowspan="2" align="left" valign="top" headers="idm140587109145584"> <p class="cluster-admin cluster-admin">
										UDP
									</p>
									 </td><td align="left" valign="top" headers="idm140587109144496"> <p class="cluster-admin cluster-admin">
										<code class="literal cluster-admin">500</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587109143408"> <p class="cluster-admin cluster-admin">
										IPsec IKE packets
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587109144496"> <p class="cluster-admin cluster-admin">
										<code class="literal cluster-admin">4500</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587109143408"> <p class="cluster-admin cluster-admin">
										IPsec NAT-T packets
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587109145584"> <p class="cluster-admin cluster-admin">
										ESP
									</p>
									 </td><td align="left" valign="top" headers="idm140587109144496"> <p class="cluster-admin cluster-admin">
										N/A
									</p>
									 </td><td align="left" valign="top" headers="idm140587109143408"> <p class="cluster-admin cluster-admin">
										IPsec Encapsulating Security Payload (ESP)
									</p>
									 </td></tr></tbody></table></div></div></section></section><section class="section cluster-admin" id="nw-ovn-ipsec-encryption_configuring-ipsec-ovn"><div class="titlepage"><div><div><h3 class="title">26.10.3. Encryption protocol and IPsec mode</h3></div></div></div><p class="cluster-admin cluster-admin">
					The encrypt cipher used is <code class="literal cluster-admin">AES-GCM-16-256</code>. The integrity check value (ICV) is <code class="literal cluster-admin">16</code> bytes. The key length is <code class="literal cluster-admin">256</code> bits.
				</p><p class="cluster-admin cluster-admin">
					The IPsec mode used is <span class="emphasis"><em><span class="cluster-admin cluster-admin">Transport mode</span></em></span>, a mode that encrypts end-to-end communication by adding an Encapsulated Security Payload (ESP) header to the IP header of the original packet and encrypts the packet data. OpenShift Container Platform does not currently use or support IPsec <span class="emphasis"><em><span class="cluster-admin cluster-admin">Tunnel mode</span></em></span> for pod-to-pod communication.
				</p></section><section class="section cluster-admin" id="nw-ovn-ipsec-certificates_configuring-ipsec-ovn"><div class="titlepage"><div><div><h3 class="title">26.10.4. Security certificate generation and rotation</h3></div></div></div><p class="cluster-admin cluster-admin">
					The Cluster Network Operator (CNO) generates a self-signed X.509 certificate authority (CA) that is used by IPsec for encryption. Certificate signing requests (CSRs) from each node are automatically fulfilled by the CNO.
				</p><p class="cluster-admin cluster-admin">
					The CA is valid for 10 years. The individual node certificates are valid for 5 years and are automatically rotated after 4 1/2 years elapse.
				</p></section><section class="section cluster-admin" id="nw-ovn-ipsec-enable_configuring-ipsec-ovn"><div class="titlepage"><div><div><h3 class="title">26.10.5. Enabling IPsec encryption</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can enable IPsec encryption after cluster installation.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li><li class="listitem">
							You have reduced the size of your cluster MTU by <code class="literal cluster-admin">46</code> bytes to allow for the overhead of the IPsec ESP header.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							To enable IPsec encryption, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch networks.operator.openshift.io cluster --type=merge \
-p '{"spec":{"defaultNetwork":{"ovnKubernetesConfig":{"ipsecConfig":{ }}}}}'</pre></li></ul></div></section><section class="section cluster-admin" id="nw-ovn-ipsec-verification_configuring-ipsec-ovn"><div class="titlepage"><div><div><h3 class="title">26.10.6. Verifying that IPsec is enabled</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can verify that IPsec is enabled.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							To find the names of the OVN-Kubernetes control plane pods, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods -n openshift-ovn-kubernetes | grep ovnkube-master</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">ovnkube-master-4496s        1/1     Running   0          6h39m
ovnkube-master-d6cht        1/1     Running   0          6h42m
ovnkube-master-skblc        1/1     Running   0          6h51m
ovnkube-master-vf8rf        1/1     Running   0          6h51m
ovnkube-master-w7hjr        1/1     Running   0          6h51m
ovnkube-master-zsk7x        1/1     Running   0          6h42m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify that IPsec is enabled on your cluster:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n openshift-ovn-kubernetes -c nbdb rsh ovnkube-master-&lt;XXXXX&gt; \
  ovn-nbctl --no-leader-only get nb_global . ipsec</pre><p class="cluster-admin cluster-admin">
							where:
						</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;XXXXX&gt;</code></span></dt><dd>
										Specifies the random sequence of letters for a pod from the previous step.
									</dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">true</pre>

							</p></div></li></ol></div></section><section class="section cluster-admin" id="nw-ovn-ipsec-disable_configuring-ipsec-ovn"><div class="titlepage"><div><div><h3 class="title">26.10.7. Disabling IPsec encryption</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can disable IPsec encryption only if you enabled IPsec after cluster installation.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						If you enabled IPsec when you installed your cluster, you cannot disable IPsec with this procedure.
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							To disable IPsec encryption, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch networks.operator.openshift.io/cluster --type=json \
  -p='[{"op":"remove", "path":"/spec/defaultNetwork/ovnKubernetesConfig/ipsecConfig"}]'</pre></li><li class="listitem">
							Optional: You can increase the size of your cluster MTU by <code class="literal cluster-admin">46</code> bytes because there is no longer any overhead from the IPsec ESP header in IP packets.
						</li></ol></div></section><section class="section cluster-admin" id="configuring-ipsec-ovn_additional-resources"><div class="titlepage"><div><div><h3 class="title">26.10.8. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-ovn-kubernetes">About the OVN-Kubernetes Container Network Interface (CNI) network plugin</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#changing-cluster-network-mtu">Changing the MTU for the cluster network</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/api_reference/#network-operator-openshift-io-v1">Network [operator.openshift.io/v1</a>] API
						</li></ul></div></section></section><section class="section cluster-admin" id="configuring-egress-firewall-ovn"><div class="titlepage"><div><div><h2 class="title">26.11. Configuring an egress firewall for a project</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can create an egress firewall for a project that restricts egress traffic leaving your OpenShift Container Platform cluster.
			</p><section class="section cluster-admin" id="nw-egressnetworkpolicy-about_configuring-egress-firewall-ovn"><div class="titlepage"><div><div><h3 class="title">26.11.1. How an egress firewall works in a project</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can use an <span class="emphasis"><em><span class="cluster-admin cluster-admin">egress firewall</span></em></span> to limit the external hosts that some or all pods can access from within the cluster. An egress firewall supports the following scenarios:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							A pod can only connect to internal hosts and cannot initiate connections to the public internet.
						</li><li class="listitem">
							A pod can only connect to the public internet and cannot initiate connections to internal hosts that are outside the OpenShift Container Platform cluster.
						</li><li class="listitem">
							A pod cannot reach specified internal subnets or hosts outside the OpenShift Container Platform cluster.
						</li><li class="listitem">
							A pod can connect to only specific external hosts.
						</li></ul></div><p class="cluster-admin cluster-admin">
					For example, you can allow one project access to a specified IP range but deny the same access to a different project. Or you can restrict application developers from updating from Python pip mirrors, and force updates to come only from approved sources.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						Egress firewall does not apply to the host network namespace. Pods with host networking enabled are unaffected by egress firewall rules.
					</p></div></div><p class="cluster-admin cluster-admin">
					You configure an egress firewall policy by creating an EgressFirewall custom resource (CR) object. The egress firewall matches network traffic that meets any of the following criteria:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							An IP address range in CIDR format
						</li><li class="listitem">
							A DNS name that resolves to an IP address
						</li><li class="listitem">
							A port number
						</li><li class="listitem">
							A protocol that is one of the following protocols: TCP, UDP, and SCTP
						</li></ul></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						If your egress firewall includes a deny rule for <code class="literal cluster-admin">0.0.0.0/0</code>, access to your OpenShift Container Platform API servers is blocked. You must either add allow rules for each IP address or use the <code class="literal cluster-admin">nodeSelector</code> type allow rule in your egress policy rules to connect to API servers.
					</p><p class="cluster-admin cluster-admin">
						The following example illustrates the order of the egress firewall rules necessary to ensure API server access:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: k8s.ovn.org/v1
kind: EgressFirewall
metadata:
  name: default
  namespace: &lt;namespace&gt; <span id="CO146-1"><!--Empty--></span><span class="callout">1</span>
spec:
  egress:
  - to:
      cidrSelector: &lt;api_server_address_range&gt; <span id="CO146-2"><!--Empty--></span><span class="callout">2</span>
    type: Allow
# ...
  - to:
      cidrSelector: 0.0.0.0/0 <span id="CO146-3"><!--Empty--></span><span class="callout">3</span>
    type: Deny</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO146-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The namespace for the egress firewall.
							</div></dd><dt><a href="#CO146-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The IP address range that includes your OpenShift Container Platform API servers.
							</div></dd><dt><a href="#CO146-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								A global deny rule prevents access to the OpenShift Container Platform API servers.
							</div></dd></dl></div><p class="cluster-admin cluster-admin">
						To find the IP address for your API servers, run <code class="literal cluster-admin">oc get ep kubernetes -n default</code>.
					</p><p class="cluster-admin cluster-admin">
						For more information, see <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1988324">BZ#1988324</a>.
					</p></div></div><div class="admonition warning cluster-admin"><div class="admonition_header">Warning</div><div><p class="cluster-admin cluster-admin">
						Egress firewall rules do not apply to traffic that goes through routers. Any user with permission to create a Route CR object can bypass egress firewall policy rules by creating a route that points to a forbidden destination.
					</p></div></div><section class="section cluster-admin" id="limitations-of-an-egress-firewall_configuring-egress-firewall-ovn"><div class="titlepage"><div><div><h4 class="title">26.11.1.1. Limitations of an egress firewall</h4></div></div></div><p class="cluster-admin cluster-admin">
						An egress firewall has the following limitations:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								No project can have more than one EgressFirewall object.
							</li><li class="listitem">
								A maximum of one EgressFirewall object with a maximum of 8,000 rules can be defined per project.
							</li><li class="listitem">
								If you are using the OVN-Kubernetes network plugin with shared gateway mode in Red Hat OpenShift Networking, return ingress replies are affected by egress firewall rules. If the egress firewall rules drop the ingress reply destination IP, the traffic is dropped.
							</li></ul></div><p class="cluster-admin cluster-admin">
						Violating any of these restrictions results in a broken egress firewall for the project. Consequently, all external network traffic is dropped, which can cause security risks for your organization.
					</p><p class="cluster-admin cluster-admin">
						An Egress Firewall resource can be created in the <code class="literal cluster-admin">kube-node-lease</code>, <code class="literal cluster-admin">kube-public</code>, <code class="literal cluster-admin">kube-system</code>, <code class="literal cluster-admin">openshift</code> and <code class="literal cluster-admin">openshift-</code> projects.
					</p></section><section class="section cluster-admin" id="policy-rule-order_configuring-egress-firewall-ovn"><div class="titlepage"><div><div><h4 class="title">26.11.1.2. Matching order for egress firewall policy rules</h4></div></div></div><p class="cluster-admin cluster-admin">
						The egress firewall policy rules are evaluated in the order that they are defined, from first to last. The first rule that matches an egress connection from a pod applies. Any subsequent rules are ignored for that connection.
					</p></section><section class="section cluster-admin" id="domain-name-server-resolution_configuring-egress-firewall-ovn"><div class="titlepage"><div><div><h4 class="title">26.11.1.3. How Domain Name Server (DNS) resolution works</h4></div></div></div><p class="cluster-admin cluster-admin">
						If you use DNS names in any of your egress firewall policy rules, proper resolution of the domain names is subject to the following restrictions:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Domain name updates are polled based on a time-to-live (TTL) duration. By default, the duration is 30 minutes. When the egress firewall controller queries the local name servers for a domain name, if the response includes a TTL and the TTL is less than 30 minutes, the controller sets the duration for that DNS name to the returned value. Each DNS name is queried after the TTL for the DNS record expires.
							</li><li class="listitem">
								The pod must resolve the domain from the same local name servers when necessary. Otherwise the IP addresses for the domain known by the egress firewall controller and the pod can be different. If the IP addresses for a hostname differ, the egress firewall might not be enforced consistently.
							</li><li class="listitem">
								Because the egress firewall controller and pods asynchronously poll the same local name server, the pod might obtain the updated IP address before the egress controller does, which causes a race condition. Due to this current limitation, domain name usage in EgressFirewall objects is only recommended for domains with infrequent IP address changes.
							</li></ul></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							The egress firewall always allows pods access to the external interface of the node that the pod is on for DNS resolution.
						</p><p class="cluster-admin cluster-admin">
							If you use domain names in your egress firewall policy and your DNS resolution is not handled by a DNS server on the local node, then you must add egress firewall rules that allow access to your DNS server’s IP addresses. if you are using domain names in your pods.
						</p></div></div></section></section><section class="section cluster-admin" id="nw-egressnetworkpolicy-object_configuring-egress-firewall-ovn"><div class="titlepage"><div><div><h3 class="title">26.11.2. EgressFirewall custom resource (CR) object</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can define one or more rules for an egress firewall. A rule is either an <code class="literal cluster-admin">Allow</code> rule or a <code class="literal cluster-admin">Deny</code> rule, with a specification for the traffic that the rule applies to.
				</p><p class="cluster-admin cluster-admin">
					The following YAML describes an EgressFirewall CR object:
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>EgressFirewall object</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: k8s.ovn.org/v1
kind: EgressFirewall
metadata:
  name: &lt;name&gt; <span id="CO147-1"><!--Empty--></span><span class="callout">1</span>
spec:
  egress: <span id="CO147-2"><!--Empty--></span><span class="callout">2</span>
    ...</pre>

					</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO147-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The name for the object must be <code class="literal cluster-admin">default</code>.
						</div></dd><dt><a href="#CO147-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							A collection of one or more egress network policy rules as described in the following section.
						</div></dd></dl></div><section class="section cluster-admin" id="egressnetworkpolicy-rules_configuring-egress-firewall-ovn"><div class="titlepage"><div><div><h4 class="title">26.11.2.1. EgressFirewall rules</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following YAML describes an egress firewall rule object. The user can select either an IP address range in CIDR format, a domain name, or use the <code class="literal cluster-admin">nodeSelector</code> to allow or deny egress traffic. The <code class="literal cluster-admin">egress</code> stanza expects an array of one or more objects.
					</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Egress policy rule stanza</strong></p><p>
							
<pre class="programlisting language-yaml">egress:
- type: &lt;type&gt; <span id="CO148-1"><!--Empty--></span><span class="callout">1</span>
  to: <span id="CO148-2"><!--Empty--></span><span class="callout">2</span>
    cidrSelector: &lt;cidr&gt; <span id="CO148-3"><!--Empty--></span><span class="callout">3</span>
    dnsName: &lt;dns_name&gt; <span id="CO148-4"><!--Empty--></span><span class="callout">4</span>
    nodeSelector: &lt;label_name&gt;: &lt;label_value&gt; <span id="CO148-5"><!--Empty--></span><span class="callout">5</span>
  ports: <span id="CO148-6"><!--Empty--></span><span class="callout">6</span>
      ...</pre>

						</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO148-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The type of rule. The value must be either <code class="literal cluster-admin">Allow</code> or <code class="literal cluster-admin">Deny</code>.
							</div></dd><dt><a href="#CO148-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								A stanza describing an egress traffic match rule that specifies the <code class="literal cluster-admin">cidrSelector</code> field or the <code class="literal cluster-admin">dnsName</code> field. You cannot use both fields in the same rule.
							</div></dd><dt><a href="#CO148-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								An IP address range in CIDR format.
							</div></dd><dt><a href="#CO148-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								A DNS domain name.
							</div></dd><dt><a href="#CO148-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Labels are key/value pairs that the user defines. Labels are attached to objects, such as pods. The <code class="literal cluster-admin">nodeSelector</code> allows for one or more node labels to be selected and attached to pods.
							</div></dd><dt><a href="#CO148-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								Optional: A stanza describing a collection of network ports and protocols for the rule.
							</div></dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Ports stanza</strong></p><p>
							
<pre class="programlisting language-yaml">ports:
- port: &lt;port&gt; <span id="CO149-1"><!--Empty--></span><span class="callout">1</span>
  protocol: &lt;protocol&gt; <span id="CO149-2"><!--Empty--></span><span class="callout">2</span></pre>

						</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO149-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								A network port, such as <code class="literal cluster-admin">80</code> or <code class="literal cluster-admin">443</code>. If you specify a value for this field, you must also specify a value for <code class="literal cluster-admin">protocol</code>.
							</div></dd><dt><a href="#CO149-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								A network protocol. The value must be either <code class="literal cluster-admin">TCP</code>, <code class="literal cluster-admin">UDP</code>, or <code class="literal cluster-admin">SCTP</code>.
							</div></dd></dl></div></section><section class="section cluster-admin" id="egressnetworkpolicy-example_configuring-egress-firewall-ovn"><div class="titlepage"><div><div><h4 class="title">26.11.2.2. Example EgressFirewall CR objects</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following example defines several egress firewall policy rules:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: k8s.ovn.org/v1
kind: EgressFirewall
metadata:
  name: default
spec:
  egress: <span id="CO150-1"><!--Empty--></span><span class="callout">1</span>
  - type: Allow
    to:
      cidrSelector: 1.2.3.0/24
  - type: Deny
    to:
      cidrSelector: 0.0.0.0/0</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO150-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								A collection of egress firewall policy rule objects.
							</div></dd></dl></div><p class="cluster-admin cluster-admin">
						The following example defines a policy rule that denies traffic to the host at the <code class="literal cluster-admin">172.16.1.1</code> IP address, if the traffic is using either the TCP protocol and destination port <code class="literal cluster-admin">80</code> or any protocol and destination port <code class="literal cluster-admin">443</code>.
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: k8s.ovn.org/v1
kind: EgressFirewall
metadata:
  name: default
spec:
  egress:
  - type: Deny
    to:
      cidrSelector: 172.16.1.1
    ports:
    - port: 80
      protocol: TCP
    - port: 443</pre></section><section class="section cluster-admin" id="configuringNodeSelector-example_configuring-egress-firewall-ovn"><div class="titlepage"><div><div><h4 class="title">26.11.2.3. Example nodeSelector for EgressFirewall</h4></div></div></div><p class="cluster-admin cluster-admin">
						As a cluster administrator, you can allow or deny egress traffic to nodes in your cluster by specifying a label using <code class="literal cluster-admin">nodeSelector</code>. Labels can be applied to one or more nodes. The following is an example with the <code class="literal cluster-admin">region=east</code> label:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: k8s.ovn.org/v1
kind: EgressFirewall
metadata:
  name: default
spec:
    egress:
    - to:
        nodeSelector:
          matchLabels:
            region: east
      type: Allow</pre><div class="admonition tip cluster-admin"><div class="admonition_header">Tip</div><div><p class="cluster-admin cluster-admin">
						Instead of adding manual rules per node IP address, use node selectors to create a label that allows pods behind an egress firewall to access host network pods.
					</p></div></div></section></section><section class="section cluster-admin" id="nw-networkpolicy-create_configuring-egress-firewall-ovn"><div class="titlepage"><div><div><h3 class="title">26.11.3. Creating an egress firewall policy object</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can create an egress firewall policy object for a project.
				</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						If the project already has an EgressFirewall object defined, you must edit the existing policy to make changes to the egress firewall rules.
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							A cluster that uses the OVN-Kubernetes network plugin.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You must log in to the cluster as a cluster administrator.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a policy rule:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem">
									Create a <code class="literal cluster-admin">&lt;policy_name&gt;.yaml</code> file where <code class="literal cluster-admin">&lt;policy_name&gt;</code> describes the egress policy rules.
								</li><li class="listitem">
									In the file you created, define an egress policy object.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Enter the following command to create the policy object. Replace <code class="literal cluster-admin">&lt;policy_name&gt;</code> with the name of the policy and <code class="literal cluster-admin">&lt;project&gt;</code> with the project that the rule applies to.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f &lt;policy_name&gt;.yaml -n &lt;project&gt;</pre><p class="cluster-admin cluster-admin">
							In the following example, a new EgressFirewall object is created in a project named <code class="literal cluster-admin">project1</code>:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f default.yaml -n project1</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">egressfirewall.k8s.ovn.org/v1 created</pre>

							</p></div></li><li class="listitem">
							Optional: Save the <code class="literal cluster-admin">&lt;policy_name&gt;.yaml</code> file so that you can make changes later.
						</li></ol></div></section></section><section class="section cluster-admin" id="viewing-egress-firewall-ovn"><div class="titlepage"><div><div><h2 class="title">26.12. Viewing an egress firewall for a project</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can list the names of any existing egress firewalls and view the traffic rules for a specific egress firewall.
			</p><section class="section cluster-admin" id="nw-egressnetworkpolicy-view_viewing-egress-firewall-ovn"><div class="titlepage"><div><div><h3 class="title">26.12.1. Viewing an EgressFirewall object</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can view an EgressFirewall object in your cluster.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							A cluster using the OVN-Kubernetes network plugin.
						</li><li class="listitem">
							Install the OpenShift Command-line Interface (CLI), commonly known as <code class="literal cluster-admin">oc</code>.
						</li><li class="listitem">
							You must log in to the cluster.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Optional: To view the names of the EgressFirewall objects defined in your cluster, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get egressfirewall --all-namespaces</pre></li><li class="listitem"><p class="simpara">
							To inspect a policy, enter the following command. Replace <code class="literal cluster-admin">&lt;policy_name&gt;</code> with the name of the policy to inspect.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc describe egressfirewall &lt;policy_name&gt;</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Name:		default
Namespace:	project1
Created:	20 minutes ago
Labels:		&lt;none&gt;
Annotations:	&lt;none&gt;
Rule:		Allow to 1.2.3.0/24
Rule:		Allow to www.example.com
Rule:		Deny to 0.0.0.0/0</pre>

							</p></div></li></ol></div></section></section><section class="section cluster-admin" id="editing-egress-firewall-ovn"><div class="titlepage"><div><div><h2 class="title">26.13. Editing an egress firewall for a project</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can modify network traffic rules for an existing egress firewall.
			</p><section class="section cluster-admin" id="nw-egressnetworkpolicy-edit_editing-egress-firewall-ovn"><div class="titlepage"><div><div><h3 class="title">26.13.1. Editing an EgressFirewall object</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can update the egress firewall for a project.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							A cluster using the OVN-Kubernetes network plugin.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You must log in to the cluster as a cluster administrator.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Find the name of the EgressFirewall object for the project. Replace <code class="literal cluster-admin">&lt;project&gt;</code> with the name of the project.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get -n &lt;project&gt; egressfirewall</pre></li><li class="listitem"><p class="simpara">
							Optional: If you did not save a copy of the EgressFirewall object when you created the egress network firewall, enter the following command to create a copy.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get -n &lt;project&gt; egressfirewall &lt;name&gt; -o yaml &gt; &lt;filename&gt;.yaml</pre><p class="cluster-admin cluster-admin">
							Replace <code class="literal cluster-admin">&lt;project&gt;</code> with the name of the project. Replace <code class="literal cluster-admin">&lt;name&gt;</code> with the name of the object. Replace <code class="literal cluster-admin">&lt;filename&gt;</code> with the name of the file to save the YAML to.
						</p></li><li class="listitem"><p class="simpara">
							After making changes to the policy rules, enter the following command to replace the EgressFirewall object. Replace <code class="literal cluster-admin">&lt;filename&gt;</code> with the name of the file containing the updated EgressFirewall object.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc replace -f &lt;filename&gt;.yaml</pre></li></ol></div></section></section><section class="section cluster-admin" id="removing-egress-firewall-ovn"><div class="titlepage"><div><div><h2 class="title">26.14. Removing an egress firewall from a project</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can remove an egress firewall from a project to remove all restrictions on network traffic from the project that leaves the OpenShift Container Platform cluster.
			</p><section class="section cluster-admin" id="nw-egressnetworkpolicy-delete_removing-egress-firewall-ovn"><div class="titlepage"><div><div><h3 class="title">26.14.1. Removing an EgressFirewall object</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can remove an egress firewall from a project.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							A cluster using the OVN-Kubernetes network plugin.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You must log in to the cluster as a cluster administrator.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Find the name of the EgressFirewall object for the project. Replace <code class="literal cluster-admin">&lt;project&gt;</code> with the name of the project.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get -n &lt;project&gt; egressfirewall</pre></li><li class="listitem"><p class="simpara">
							Enter the following command to delete the EgressFirewall object. Replace <code class="literal cluster-admin">&lt;project&gt;</code> with the name of the project and <code class="literal cluster-admin">&lt;name&gt;</code> with the name of the object.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete -n &lt;project&gt; egressfirewall &lt;name&gt;</pre></li></ol></div></section></section><section class="section cluster-admin" id="configuring-egress-ips-ovn"><div class="titlepage"><div><div><h2 class="title">26.15. Configuring an egress IP address</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can configure the OVN-Kubernetes Container Network Interface (CNI) network plugin to assign one or more egress IP addresses to a namespace, or to specific pods in a namespace.
			</p><section class="section cluster-admin" id="nw-egress-ips-about_configuring-egress-ips-ovn"><div class="titlepage"><div><div><h3 class="title">26.15.1. Egress IP address architectural design and implementation</h3></div></div></div><p class="cluster-admin cluster-admin">
					The OpenShift Container Platform egress IP address functionality allows you to ensure that the traffic from one or more pods in one or more namespaces has a consistent source IP address for services outside the cluster network.
				</p><p class="cluster-admin cluster-admin">
					For example, you might have a pod that periodically queries a database that is hosted on a server outside of your cluster. To enforce access requirements for the server, a packet filtering device is configured to allow traffic only from specific IP addresses. To ensure that you can reliably allow access to the server from only that specific pod, you can configure a specific egress IP address for the pod that makes the requests to the server.
				</p><p class="cluster-admin cluster-admin">
					An egress IP address assigned to a namespace is different from an egress router, which is used to send traffic to specific destinations.
				</p><p class="cluster-admin cluster-admin">
					In some cluster configurations, application pods and ingress router pods run on the same node. If you configure an egress IP address for an application project in this scenario, the IP address is not used when you send a request to a route from the application project.
				</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						Egress IP addresses must not be configured in any Linux network configuration files, such as <code class="literal cluster-admin">ifcfg-eth0</code>.
					</p></div></div><section class="section cluster-admin" id="nw-egress-ips-platform-support_configuring-egress-ips-ovn"><div class="titlepage"><div><div><h4 class="title">26.15.1.1. Platform support</h4></div></div></div><p class="cluster-admin cluster-admin">
						Support for the egress IP address functionality on various platforms is summarized in the following table:
					</p><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587105463712" scope="col">Platform</th><th align="left" valign="top" id="idm140587105462624" scope="col">Supported</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587105463712"> <p>
										Bare metal
									</p>
									 </td><td align="left" valign="top" headers="idm140587105462624"> <p>
										Yes
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587105463712"> <p>
										VMware vSphere
									</p>
									 </td><td align="left" valign="top" headers="idm140587105462624"> <p>
										Yes
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587105463712"> <p>
										Red Hat OpenStack Platform (RHOSP)
									</p>
									 </td><td align="left" valign="top" headers="idm140587105462624"> <p>
										Yes
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587105463712"> <p>
										Amazon Web Services (AWS)
									</p>
									 </td><td align="left" valign="top" headers="idm140587105462624"> <p>
										Yes
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587105463712"> <p>
										Google Cloud Platform (GCP)
									</p>
									 </td><td align="left" valign="top" headers="idm140587105462624"> <p>
										Yes
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587105463712"> <p>
										Microsoft Azure
									</p>
									 </td><td align="left" valign="top" headers="idm140587105462624"> <p>
										Yes
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587105463712"> <p>
										IBM Z and IBM® LinuxONE
									</p>
									 </td><td align="left" valign="top" headers="idm140587105462624"> <p>
										Yes
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587105463712"> <p>
										IBM Z and IBM® LinuxONE for Red Hat Enterprise Linux (RHEL) KVM
									</p>
									 </td><td align="left" valign="top" headers="idm140587105462624"> <p>
										Yes
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587105463712"> <p>
										IBM Power
									</p>
									 </td><td align="left" valign="top" headers="idm140587105462624"> <p>
										Yes
									</p>
									 </td></tr></tbody></table></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
							The assignment of egress IP addresses to control plane nodes with the EgressIP feature is not supported on a cluster provisioned on Amazon Web Services (AWS). (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2039656"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">BZ#2039656</span></strong></span></a>)
						</p></div></div></section><section class="section cluster-admin" id="nw-egress-ips-public-cloud-platform-considerations_configuring-egress-ips-ovn"><div class="titlepage"><div><div><h4 class="title">26.15.1.2. Public cloud platform considerations</h4></div></div></div><p class="cluster-admin cluster-admin">
						For clusters provisioned on public cloud infrastructure, there is a constraint on the absolute number of assignable IP addresses per node. The maximum number of assignable IP addresses per node, or the <span class="emphasis"><em><span class="cluster-admin cluster-admin">IP capacity</span></em></span>, can be described in the following formula:
					</p><pre class="programlisting language-text cluster-admin cluster-admin">IP capacity = public cloud default capacity - sum(current IP assignments)</pre><p class="cluster-admin cluster-admin">
						While the Egress IPs capability manages the IP address capacity per node, it is important to plan for this constraint in your deployments. For example, for a cluster installed on bare-metal infrastructure with 8 nodes you can configure 150 egress IP addresses. However, if a public cloud provider limits IP address capacity to 10 IP addresses per node, the total number of assignable IP addresses is only 80. To achieve the same IP address capacity in this example cloud provider, you would need to allocate 7 additional nodes.
					</p><p class="cluster-admin cluster-admin">
						To confirm the IP capacity and subnets for any node in your public cloud environment, you can enter the <code class="literal cluster-admin">oc get node &lt;node_name&gt; -o yaml</code> command. The <code class="literal cluster-admin">cloud.network.openshift.io/egress-ipconfig</code> annotation includes capacity and subnet information for the node.
					</p><p class="cluster-admin cluster-admin">
						The annotation value is an array with a single object with fields that provide the following information for the primary network interface:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								<code class="literal cluster-admin">interface</code>: Specifies the interface ID on AWS and Azure and the interface name on GCP.
							</li><li class="listitem">
								<code class="literal cluster-admin">ifaddr</code>: Specifies the subnet mask for one or both IP address families.
							</li><li class="listitem">
								<code class="literal cluster-admin">capacity</code>: Specifies the IP address capacity for the node. On AWS, the IP address capacity is provided per IP address family. On Azure and GCP, the IP address capacity includes both IPv4 and IPv6 addresses.
							</li></ul></div><p class="cluster-admin cluster-admin">
						Automatic attachment and detachment of egress IP addresses for traffic between nodes are available. This allows for traffic from many pods in namespaces to have a consistent source IP address to locations outside of the cluster. This also supports OpenShift SDN and OVN-Kubernetes, which is the default networking plugin in Red Hat OpenShift Networking in OpenShift Container Platform 4.13.
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							When an RHOSP cluster administrator assigns a floating IP to the reservation port, OpenShift Container Platform cannot delete the reservation port. The <code class="literal cluster-admin">CloudPrivateIPConfig</code> object cannot perform delete and move operations until an RHOSP cluster administrator unassigns the floating IP from the reservation port.
						</p></div></div><p class="cluster-admin cluster-admin">
						The following examples illustrate the annotation from nodes on several public cloud providers. The annotations are indented for readability.
					</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example <code class="literal cluster-admin">cloud.network.openshift.io/egress-ipconfig</code> annotation on AWS</strong></p><p>
							
<pre class="programlisting language-yaml">cloud.network.openshift.io/egress-ipconfig: [
  {
    "interface":"eni-078d267045138e436",
    "ifaddr":{"ipv4":"10.0.128.0/18"},
    "capacity":{"ipv4":14,"ipv6":15}
  }
]</pre>

						</p></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example <code class="literal cluster-admin">cloud.network.openshift.io/egress-ipconfig</code> annotation on GCP</strong></p><p>
							
<pre class="programlisting language-yaml">cloud.network.openshift.io/egress-ipconfig: [
  {
    "interface":"nic0",
    "ifaddr":{"ipv4":"10.0.128.0/18"},
    "capacity":{"ip":14}
  }
]</pre>

						</p></div><p class="cluster-admin cluster-admin">
						The following sections describe the IP address capacity for supported public cloud environments for use in your capacity calculation.
					</p><section class="section cluster-admin" id="nw-egress-ips-capacity-aws_configuring-egress-ips-ovn"><div class="titlepage"><div><div><h5 class="title">26.15.1.2.1. Amazon Web Services (AWS) IP address capacity limits</h5></div></div></div><p class="cluster-admin cluster-admin">
							On AWS, constraints on IP address assignments depend on the instance type configured. For more information, see <a class="link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI">IP addresses per network interface per instance type</a>
						</p></section><section class="section cluster-admin" id="nw-egress-ips-capacity-gcp_configuring-egress-ips-ovn"><div class="titlepage"><div><div><h5 class="title">26.15.1.2.2. Google Cloud Platform (GCP) IP address capacity limits</h5></div></div></div><p class="cluster-admin cluster-admin">
							On GCP, the networking model implements additional node IP addresses through IP address aliasing, rather than IP address assignments. However, IP address capacity maps directly to IP aliasing capacity.
						</p><p class="cluster-admin cluster-admin">
							The following capacity limits exist for IP aliasing assignment:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Per node, the maximum number of IP aliases, both IPv4 and IPv6, is 100.
								</li><li class="listitem">
									Per VPC, the maximum number of IP aliases is unspecified, but OpenShift Container Platform scalability testing reveals the maximum to be approximately 15,000.
								</li></ul></div><p class="cluster-admin cluster-admin">
							For more information, see <a class="link" href="https://cloud.google.com/vpc/docs/quota#per_instance">Per instance</a> quotas and <a class="link" href="https://cloud.google.com/vpc/docs/alias-ip">Alias IP ranges overview</a>.
						</p></section><section class="section cluster-admin" id="nw-egress-ips-capacity-azure_configuring-egress-ips-ovn"><div class="titlepage"><div><div><h5 class="title">26.15.1.2.3. Microsoft Azure IP address capacity limits</h5></div></div></div><p class="cluster-admin cluster-admin">
							On Azure, the following capacity limits exist for IP address assignment:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Per NIC, the maximum number of assignable IP addresses, for both IPv4 and IPv6, is 256.
								</li><li class="listitem">
									Per virtual network, the maximum number of assigned IP addresses cannot exceed 65,536.
								</li></ul></div><p class="cluster-admin cluster-admin">
							For more information, see <a class="link" href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/azure-subscription-service-limits?toc=/azure/virtual-network/toc.json#networking-limits">Networking limits</a>.
						</p></section></section><section class="section cluster-admin" id="nw-egress-ips-considerations_configuring-egress-ips-ovn"><div class="titlepage"><div><div><h4 class="title">26.15.1.3. Assignment of egress IPs to pods</h4></div></div></div><p class="cluster-admin cluster-admin">
						To assign one or more egress IPs to a namespace or specific pods in a namespace, the following conditions must be satisfied:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								At least one node in your cluster must have the <code class="literal cluster-admin">k8s.ovn.org/egress-assignable: ""</code> label.
							</li><li class="listitem">
								An <code class="literal cluster-admin">EgressIP</code> object exists that defines one or more egress IP addresses to use as the source IP address for traffic leaving the cluster from pods in a namespace.
							</li></ul></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
							If you create <code class="literal cluster-admin">EgressIP</code> objects prior to labeling any nodes in your cluster for egress IP assignment, OpenShift Container Platform might assign every egress IP address to the first node with the <code class="literal cluster-admin">k8s.ovn.org/egress-assignable: ""</code> label.
						</p><p class="cluster-admin cluster-admin">
							To ensure that egress IP addresses are widely distributed across nodes in the cluster, always apply the label to the nodes you intent to host the egress IP addresses before creating any <code class="literal cluster-admin">EgressIP</code> objects.
						</p></div></div></section><section class="section cluster-admin" id="nw-egress-ips-node-assignment_configuring-egress-ips-ovn"><div class="titlepage"><div><div><h4 class="title">26.15.1.4. Assignment of egress IPs to nodes</h4></div></div></div><p class="cluster-admin cluster-admin">
						When creating an <code class="literal cluster-admin">EgressIP</code> object, the following conditions apply to nodes that are labeled with the <code class="literal cluster-admin">k8s.ovn.org/egress-assignable: ""</code> label:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								An egress IP address is never assigned to more than one node at a time.
							</li><li class="listitem">
								An egress IP address is equally balanced between available nodes that can host the egress IP address.
							</li><li class="listitem"><p class="simpara">
								If the <code class="literal cluster-admin">spec.EgressIPs</code> array in an <code class="literal cluster-admin">EgressIP</code> object specifies more than one IP address, the following conditions apply:
							</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem">
										No node will ever host more than one of the specified IP addresses.
									</li><li class="listitem">
										Traffic is balanced roughly equally between the specified IP addresses for a given namespace.
									</li></ul></div></li><li class="listitem">
								If a node becomes unavailable, any egress IP addresses assigned to it are automatically reassigned, subject to the previously described conditions.
							</li></ul></div><p class="cluster-admin cluster-admin">
						When a pod matches the selector for multiple <code class="literal cluster-admin">EgressIP</code> objects, there is no guarantee which of the egress IP addresses that are specified in the <code class="literal cluster-admin">EgressIP</code> objects is assigned as the egress IP address for the pod.
					</p><p class="cluster-admin cluster-admin">
						Additionally, if an <code class="literal cluster-admin">EgressIP</code> object specifies multiple egress IP addresses, there is no guarantee which of the egress IP addresses might be used. For example, if a pod matches a selector for an <code class="literal cluster-admin">EgressIP</code> object with two egress IP addresses, <code class="literal cluster-admin">10.10.20.1</code> and <code class="literal cluster-admin">10.10.20.2</code>, either might be used for each TCP connection or UDP conversation.
					</p></section><section class="section cluster-admin" id="nw-egress-ips-node-architecture_configuring-egress-ips-ovn"><div class="titlepage"><div><div><h4 class="title">26.15.1.5. Architectural diagram of an egress IP address configuration</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following diagram depicts an egress IP address configuration. The diagram describes four pods in two different namespaces running on three nodes in a cluster. The nodes are assigned IP addresses from the <code class="literal cluster-admin">192.168.126.0/18</code> CIDR block on the host network.
					</p><div class="informalfigure"><div class="mediaobject"><object type="image/svg+xml" class="svg-img" data="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/8192489dd24b978f903dfe63024f15b6/nw-egress-ips-diagram.svg"><embed type="image/svg+xml" src="images/nw-egress-ips-diagram.svg"><!--Empty--></embed></object></div></div><p class="cluster-admin cluster-admin">
						Both Node 1 and Node 3 are labeled with <code class="literal cluster-admin">k8s.ovn.org/egress-assignable: ""</code> and thus available for the assignment of egress IP addresses.
					</p><p class="cluster-admin cluster-admin">
						The dashed lines in the diagram depict the traffic flow from pod1, pod2, and pod3 traveling through the pod network to egress the cluster from Node 1 and Node 3. When an external service receives traffic from any of the pods selected by the example <code class="literal cluster-admin">EgressIP</code> object, the source IP address is either <code class="literal cluster-admin">192.168.126.10</code> or <code class="literal cluster-admin">192.168.126.102</code>. The traffic is balanced roughly equally between these two nodes.
					</p><p class="cluster-admin cluster-admin">
						The following resources from the diagram are illustrated in detail:
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">Namespace</code> objects</span></dt><dd><p class="cluster-admin cluster-admin">
									The namespaces are defined in the following manifest:
								</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Namespace objects</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: namespace1
  labels:
    env: prod
---
apiVersion: v1
kind: Namespace
metadata:
  name: namespace2
  labels:
    env: prod</pre>

									</p></div></dd><dt><span class="term"><code class="literal cluster-admin">EgressIP</code> object</span></dt><dd><p class="cluster-admin cluster-admin">
									The following <code class="literal cluster-admin">EgressIP</code> object describes a configuration that selects all pods in any namespace with the <code class="literal cluster-admin">env</code> label set to <code class="literal cluster-admin">prod</code>. The egress IP addresses for the selected pods are <code class="literal cluster-admin">192.168.126.10</code> and <code class="literal cluster-admin">192.168.126.102</code>.
								</p><div class="cluster-admin cluster-admin"><p class="title"><strong><code class="literal cluster-admin">EgressIP</code> object</strong></p><p>
										
<pre class="programlisting language-yaml">apiVersion: k8s.ovn.org/v1
kind: EgressIP
metadata:
  name: egressips-prod
spec:
  egressIPs:
  - 192.168.126.10
  - 192.168.126.102
  namespaceSelector:
    matchLabels:
      env: prod
status:
  items:
  - node: node1
    egressIP: 192.168.126.10
  - node: node3
    egressIP: 192.168.126.102</pre>

									</p></div><p class="cluster-admin cluster-admin">
									For the configuration in the previous example, OpenShift Container Platform assigns both egress IP addresses to the available nodes. The <code class="literal cluster-admin">status</code> field reflects whether and where the egress IP addresses are assigned.
								</p></dd></dl></div></section></section><section class="section cluster-admin" id="nw-egress-ips-object_configuring-egress-ips-ovn"><div class="titlepage"><div><div><h3 class="title">26.15.2. EgressIP object</h3></div></div></div><p class="cluster-admin cluster-admin">
					The following YAML describes the API for the <code class="literal cluster-admin">EgressIP</code> object. The scope of the object is cluster-wide; it is not created in a namespace.
				</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: k8s.ovn.org/v1
kind: EgressIP
metadata:
  name: &lt;name&gt; <span id="CO151-1"><!--Empty--></span><span class="callout">1</span>
spec:
  egressIPs: <span id="CO151-2"><!--Empty--></span><span class="callout">2</span>
  - &lt;ip_address&gt;
  namespaceSelector: <span id="CO151-3"><!--Empty--></span><span class="callout">3</span>
    ...
  podSelector: <span id="CO151-4"><!--Empty--></span><span class="callout">4</span>
    ...</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO151-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The name for the <code class="literal cluster-admin">EgressIPs</code> object.
						</div></dd><dt><a href="#CO151-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							An array of one or more IP addresses.
						</div></dd><dt><a href="#CO151-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							One or more selectors for the namespaces to associate the egress IP addresses with.
						</div></dd><dt><a href="#CO151-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Optional: One or more selectors for pods in the specified namespaces to associate egress IP addresses with. Applying these selectors allows for the selection of a subset of pods within a namespace.
						</div></dd></dl></div><p class="cluster-admin cluster-admin">
					The following YAML describes the stanza for the namespace selector:
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Namespace selector stanza</strong></p><p>
						
<pre class="programlisting language-yaml">namespaceSelector: <span id="CO152-1"><!--Empty--></span><span class="callout">1</span>
  matchLabels:
    &lt;label_name&gt;: &lt;label_value&gt;</pre>

					</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO152-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							One or more matching rules for namespaces. If more than one match rule is provided, all matching namespaces are selected.
						</div></dd></dl></div><p class="cluster-admin cluster-admin">
					The following YAML describes the optional stanza for the pod selector:
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Pod selector stanza</strong></p><p>
						
<pre class="programlisting language-yaml">podSelector: <span id="CO153-1"><!--Empty--></span><span class="callout">1</span>
  matchLabels:
    &lt;label_name&gt;: &lt;label_value&gt;</pre>

					</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO153-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Optional: One or more matching rules for pods in the namespaces that match the specified <code class="literal cluster-admin">namespaceSelector</code> rules. If specified, only pods that match are selected. Others pods in the namespace are not selected.
						</div></dd></dl></div><p class="cluster-admin cluster-admin">
					In the following example, the <code class="literal cluster-admin">EgressIP</code> object associates the <code class="literal cluster-admin">192.168.126.11</code> and <code class="literal cluster-admin">192.168.126.102</code> egress IP addresses with pods that have the <code class="literal cluster-admin">app</code> label set to <code class="literal cluster-admin">web</code> and are in the namespaces that have the <code class="literal cluster-admin">env</code> label set to <code class="literal cluster-admin">prod</code>:
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example <code class="literal cluster-admin">EgressIP</code> object</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: k8s.ovn.org/v1
kind: EgressIP
metadata:
  name: egress-group1
spec:
  egressIPs:
  - 192.168.126.11
  - 192.168.126.102
  podSelector:
    matchLabels:
      app: web
  namespaceSelector:
    matchLabels:
      env: prod</pre>

					</p></div><p class="cluster-admin cluster-admin">
					In the following example, the <code class="literal cluster-admin">EgressIP</code> object associates the <code class="literal cluster-admin">192.168.127.30</code> and <code class="literal cluster-admin">192.168.127.40</code> egress IP addresses with any pods that do not have the <code class="literal cluster-admin">environment</code> label set to <code class="literal cluster-admin">development</code>:
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example <code class="literal cluster-admin">EgressIP</code> object</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: k8s.ovn.org/v1
kind: EgressIP
metadata:
  name: egress-group2
spec:
  egressIPs:
  - 192.168.127.30
  - 192.168.127.40
  namespaceSelector:
    matchExpressions:
    - key: environment
      operator: NotIn
      values:
      - development</pre>

					</p></div></section><section class="section cluster-admin" id="nw-egress-ips-config-object_configuring-egress-ips-ovn"><div class="titlepage"><div><div><h3 class="title">26.15.3. EgressIPconfig object</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a feature of egress IP, the <code class="literal cluster-admin">reachabilityTotalTimeoutSeconds</code> parameter configures the total timeout for checks that are sent by probes to egress IP nodes. The <code class="literal cluster-admin">egressIPConfig</code> object allows users to set the <code class="literal cluster-admin">reachabilityTotalTimeoutSeconds</code> <code class="literal cluster-admin">spec</code>. If the EgressIP node cannot be reached within this timeout, the node is declared down.
				</p><p class="cluster-admin cluster-admin">
					You can increase this value if your network is not stable enough to handle the current default value of 1 second.
				</p><p class="cluster-admin cluster-admin">
					The following YAML describes changing the <code class="literal cluster-admin">reachabilityTotalTimeoutSeconds</code> from the default 1 second probes to 5 second probes:
				</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: k8s.ovn.org/v1
kind: EgressIP
  name: networks.operator.openshift.io
  spec:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  defaultNetwork:
    ovnKubernetesConfig:
      egressIPConfig: <span id="CO154-1"><!--Empty--></span><span class="callout">1</span>
        reachabilityTotalTimeoutSeconds: 5 <span id="CO154-2"><!--Empty--></span><span class="callout">2</span>
      gatewayConfig:
        routingViaHost: false
      genevePort: 6081</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO154-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The <code class="literal cluster-admin">egressIPConfig</code> holds the configurations for the options of the <code class="literal cluster-admin">EgressIP</code> object. Changing these configurations allows you to extend the <code class="literal cluster-admin">EgressIP</code> object.
						</div></dd><dt><a href="#CO154-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							The value for <code class="literal cluster-admin">reachabilityTotalTimeoutSeconds</code> accepts integer values from <code class="literal cluster-admin">0</code> to <code class="literal cluster-admin">60</code>. A value of 0 disables the reachability check of the egressIP node. Values of <code class="literal cluster-admin">1</code> to <code class="literal cluster-admin">60</code> correspond to the duration in seconds between probes sending the reachability check for the node.
						</div></dd></dl></div></section><section class="section cluster-admin" id="nw-egress-ips-node_configuring-egress-ips-ovn"><div class="titlepage"><div><div><h3 class="title">26.15.4. Labeling a node to host egress IP addresses</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can apply the <code class="literal cluster-admin">k8s.ovn.org/egress-assignable=""</code> label to a node in your cluster so that OpenShift Container Platform can assign one or more egress IP addresses to the node.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in to the cluster as a cluster administrator.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							To label a node so that it can host one or more egress IP addresses, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc label nodes &lt;node_name&gt; k8s.ovn.org/egress-assignable="" <span id="CO155-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO155-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the node to label.
								</div></dd></dl></div><div class="admonition tip cluster-admin"><div class="admonition_header">Tip</div><div><p class="cluster-admin cluster-admin">
							You can alternatively apply the following YAML to add the label to a node:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Node
metadata:
  labels:
    k8s.ovn.org/egress-assignable: ""
  name: &lt;node_name&gt;</pre></div></div></li></ul></div></section><section class="section cluster-admin" id="configuring-egress-ips-next-steps"><div class="titlepage"><div><div><h3 class="title">26.15.5. Next steps</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#assigning-egress-ips-ovn">Assigning egress IPs</a>
						</li></ul></div></section><section class="section _additional-resources" id="configuring-egress-ips-additional-resources"><div class="titlepage"><div><div><h3 class="title">26.15.6. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/api_reference/#labelselector-meta-v1">LabelSelector meta/v1</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/api_reference/#labelselectorrequirement-meta-v1">LabelSelectorRequirement meta/v1</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="assigning-egress-ips-ovn"><div class="titlepage"><div><div><h2 class="title">26.16. Assigning an egress IP address</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can assign an egress IP address for traffic leaving the cluster from a namespace or from specific pods in a namespace.
			</p><section class="section cluster-admin" id="nw-egress-ips-assign_assigning-egress-ips-ovn"><div class="titlepage"><div><div><h3 class="title">26.16.1. Assigning an egress IP address to a namespace</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can assign one or more egress IP addresses to a namespace or to specific pods in a namespace.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in to the cluster as a cluster administrator.
						</li><li class="listitem">
							Configure at least one node to host an egress IP address.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create an <code class="literal cluster-admin">EgressIP</code> object:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem">
									Create a <code class="literal cluster-admin">&lt;egressips_name&gt;.yaml</code> file where <code class="literal cluster-admin">&lt;egressips_name&gt;</code> is the name of the object.
								</li><li class="listitem"><p class="simpara">
									In the file that you created, define an <code class="literal cluster-admin">EgressIP</code> object, as in the following example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: k8s.ovn.org/v1
kind: EgressIP
metadata:
  name: egress-project1
spec:
  egressIPs:
  - 192.168.127.10
  - 192.168.127.11
  namespaceSelector:
    matchLabels:
      env: qa</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							To create the object, enter the following command.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f &lt;egressips_name&gt;.yaml <span id="CO156-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO156-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal cluster-admin">&lt;egressips_name&gt;</code> with the name of the object.
								</div></dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">egressips.k8s.ovn.org/&lt;egressips_name&gt; created</pre>

							</p></div></li><li class="listitem">
							Optional: Save the <code class="literal cluster-admin">&lt;egressips_name&gt;.yaml</code> file so that you can make changes later.
						</li><li class="listitem"><p class="simpara">
							Add labels to the namespace that requires egress IP addresses. To add a label to the namespace of an <code class="literal cluster-admin">EgressIP</code> object defined in step 1, run the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc label ns &lt;namespace&gt; env=qa <span id="CO157-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO157-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal cluster-admin">&lt;namespace&gt;</code> with the namespace that requires egress IP addresses.
								</div></dd></dl></div></li></ol></div></section><section class="section _additional-resources" id="assigning-egress-ips-additional-resources"><div class="titlepage"><div><div><h3 class="title">26.16.2. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-egress-ips-ovn">Configuring egress IP addresses</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="using-an-egress-router-ovn"><div class="titlepage"><div><div><h2 class="title">26.17. Considerations for the use of an egress router pod</h2></div></div></div><section class="section cluster-admin" id="nw-egress-router-about_using-an-egress-router-ovn"><div class="titlepage"><div><div><h3 class="title">26.17.1. About an egress router pod</h3></div></div></div><p class="cluster-admin cluster-admin">
					The OpenShift Container Platform egress router pod redirects traffic to a specified remote server from a private source IP address that is not used for any other purpose. An egress router pod can send network traffic to servers that are set up to allow access only from specific IP addresses.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						The egress router pod is not intended for every outgoing connection. Creating large numbers of egress router pods can exceed the limits of your network hardware. For example, creating an egress router pod for every project or application could exceed the number of local MAC addresses that the network interface can handle before reverting to filtering MAC addresses in software.
					</p></div></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						The egress router image is not compatible with Amazon AWS, Azure Cloud, or any other cloud platform that does not support layer 2 manipulations due to their incompatibility with macvlan traffic.
					</p></div></div><section class="section cluster-admin" id="nw-egress-router-about-modes_using-an-egress-router-ovn"><div class="titlepage"><div><div><h4 class="title">26.17.1.1. Egress router modes</h4></div></div></div><p class="cluster-admin cluster-admin">
						In <span class="emphasis"><em><span class="cluster-admin cluster-admin">redirect mode</span></em></span>, an egress router pod configures <code class="literal cluster-admin">iptables</code> rules to redirect traffic from its own IP address to one or more destination IP addresses. Client pods that need to use the reserved source IP address must be configured to access the service for the egress router rather than connecting directly to the destination IP. You can access the destination service and port from the application pod by using the <code class="literal cluster-admin">curl</code> command. For example:
					</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ curl &lt;router_service_IP&gt; &lt;port&gt;</pre><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							The egress router CNI plugin supports redirect mode only. This is a difference with the egress router implementation that you can deploy with OpenShift SDN. Unlike the egress router for OpenShift SDN, the egress router CNI plugin does not support HTTP proxy mode or DNS proxy mode.
						</p></div></div></section><section class="section cluster-admin" id="nw-egress-router-about-router-pod-implementation_using-an-egress-router-ovn"><div class="titlepage"><div><div><h4 class="title">26.17.1.2. Egress router pod implementation</h4></div></div></div><p class="cluster-admin cluster-admin">
						The egress router implementation uses the egress router Container Network Interface (CNI) plugin. The plugin adds a secondary network interface to a pod.
					</p><p class="cluster-admin cluster-admin">
						An egress router is a pod that has two network interfaces. For example, the pod can have <code class="literal cluster-admin">eth0</code> and <code class="literal cluster-admin">net1</code> network interfaces. The <code class="literal cluster-admin">eth0</code> interface is on the cluster network and the pod continues to use the interface for ordinary cluster-related network traffic. The <code class="literal cluster-admin">net1</code> interface is on a secondary network and has an IP address and gateway for that network. Other pods in the OpenShift Container Platform cluster can access the egress router service and the service enables the pods to access external services. The egress router acts as a bridge between pods and an external system.
					</p><p class="cluster-admin cluster-admin">
						Traffic that leaves the egress router exits through a node, but the packets have the MAC address of the <code class="literal cluster-admin">net1</code> interface from the egress router pod.
					</p><p class="cluster-admin cluster-admin">
						When you add an egress router custom resource, the Cluster Network Operator creates the following objects:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								The network attachment definition for the <code class="literal cluster-admin">net1</code> secondary network interface of the pod.
							</li><li class="listitem">
								A deployment for the egress router.
							</li></ul></div><p class="cluster-admin cluster-admin">
						If you delete an egress router custom resource, the Operator deletes the two objects in the preceding list that are associated with the egress router.
					</p></section><section class="section cluster-admin" id="nw-egress-router-about-deployments_using-an-egress-router-ovn"><div class="titlepage"><div><div><h4 class="title">26.17.1.3. Deployment considerations</h4></div></div></div><p class="cluster-admin cluster-admin">
						An egress router pod adds an additional IP address and MAC address to the primary network interface of the node. As a result, you might need to configure your hypervisor or cloud provider to allow the additional address.
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Red Hat OpenStack Platform (RHOSP)</span></dt><dd><p class="simpara">
									If you deploy OpenShift Container Platform on RHOSP, you must allow traffic from the IP and MAC addresses of the egress router pod on your OpenStack environment. If you do not allow the traffic, then <a class="link" href="https://access.redhat.com/solutions/2803331">communication will fail</a>:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ openstack port set --allowed-address \
  ip_address=&lt;ip_address&gt;,mac_address=&lt;mac_address&gt; &lt;neutron_port_uuid&gt;</pre></dd><dt><span class="term">Red Hat Virtualization (RHV)</span></dt><dd>
									If you are using <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html/administration_guide/chap-logical_networks#Explanation_of_Settings_in_the_VM_Interface_Profile_Window">RHV</a>, you must select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">No Network Filter</span></strong></span> for the Virtual network interface controller (vNIC).
								</dd><dt><span class="term">VMware vSphere</span></dt><dd>
									If you are using VMware vSphere, see the <a class="link" href="https://docs.vmware.com/en/VMware-vSphere/6.0/com.vmware.vsphere.security.doc/GUID-3507432E-AFEA-4B6B-B404-17A020575358.html">VMware documentation for securing vSphere standard switches</a>. View and change VMware vSphere default settings by selecting the host virtual switch from the vSphere Web Client.
								</dd></dl></div><p class="cluster-admin cluster-admin">
						Specifically, ensure that the following are enabled:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								<a class="link" href="https://docs.vmware.com/en/VMware-vSphere/6.0/com.vmware.vsphere.security.doc/GUID-942BD3AA-731B-4A05-8196-66F2B4BF1ACB.html">MAC Address Changes</a>
							</li><li class="listitem">
								<a class="link" href="https://docs.vmware.com/en/VMware-vSphere/6.0/com.vmware.vsphere.security.doc/GUID-7DC6486F-5400-44DF-8A62-6273798A2F80.html">Forged Transits</a>
							</li><li class="listitem">
								<a class="link" href="https://docs.vmware.com/en/VMware-vSphere/6.0/com.vmware.vsphere.security.doc/GUID-92F3AB1F-B4C5-4F25-A010-8820D7250350.html">Promiscuous Mode Operation</a>
							</li></ul></div></section><section class="section cluster-admin" id="nw-egress-router-about-failover_using-an-egress-router-ovn"><div class="titlepage"><div><div><h4 class="title">26.17.1.4. Failover configuration</h4></div></div></div><p class="cluster-admin cluster-admin">
						To avoid downtime, the Cluster Network Operator deploys the egress router pod as a deployment resource. The deployment name is <code class="literal cluster-admin">egress-router-cni-deployment</code>. The pod that corresponds to the deployment has a label of <code class="literal cluster-admin">app=egress-router-cni</code>.
					</p><p class="cluster-admin cluster-admin">
						To create a new service for the deployment, use the <code class="literal cluster-admin">oc expose deployment/egress-router-cni-deployment --port &lt;port_number&gt;</code> command or create a file like the following example:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Service
metadata:
  name: app-egress
spec:
  ports:
  - name: tcp-8080
    protocol: TCP
    port: 8080
  - name: tcp-8443
    protocol: TCP
    port: 8443
  - name: udp-80
    protocol: UDP
    port: 80
  type: ClusterIP
  selector:
    app: egress-router-cni</pre></section></section><section class="section _additional-resources" id="using-an-egress-router-ovn-additional-resources"><div class="titlepage"><div><div><h3 class="title">26.17.2. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#deploying-egress-router-ovn-redirection">Deploying an egress router in redirection mode</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="deploying-egress-router-ovn-redirection"><div class="titlepage"><div><div><h2 class="title">26.18. Deploying an egress router pod in redirect mode</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can deploy an egress router pod to redirect traffic to specified destination IP addresses from a reserved source IP address.
			</p><p class="cluster-admin cluster-admin">
				The egress router implementation uses the egress router Container Network Interface (CNI) plugin.
			</p><section class="section cluster-admin" id="nw-egress-router-ovn-cr_deploying-egress-router-ovn-redirection"><div class="titlepage"><div><div><h3 class="title">26.18.1. Egress router custom resource</h3></div></div></div><p class="cluster-admin cluster-admin">
					Define the configuration for an egress router pod in an egress router custom resource. The following YAML describes the fields for the configuration of an egress router in redirect mode:
				</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: network.operator.openshift.io/v1
kind: EgressRouter
metadata:
  name: &lt;egress_router_name&gt;
  namespace: &lt;namespace&gt;  &lt;.&gt;
spec:
  addresses: [  &lt;.&gt;
    {
      ip: "&lt;egress_router&gt;",  &lt;.&gt;
      gateway: "&lt;egress_gateway&gt;"  &lt;.&gt;
    }
  ]
  mode: Redirect
  redirect: {
    redirectRules: [  &lt;.&gt;
      {
        destinationIP: "&lt;egress_destination&gt;",
        port: &lt;egress_router_port&gt;,
        targetPort: &lt;target_port&gt;,  &lt;.&gt;
        protocol: &lt;network_protocol&gt;  &lt;.&gt;
      },
      ...
    ],
    fallbackIP: "&lt;egress_destination&gt;" &lt;.&gt;
  }</pre><p class="cluster-admin cluster-admin">
					&lt;.&gt; Optional: The <code class="literal cluster-admin">namespace</code> field specifies the namespace to create the egress router in. If you do not specify a value in the file or on the command line, the <code class="literal cluster-admin">default</code> namespace is used.
				</p><p class="cluster-admin cluster-admin">
					&lt;.&gt; The <code class="literal cluster-admin">addresses</code> field specifies the IP addresses to configure on the secondary network interface.
				</p><p class="cluster-admin cluster-admin">
					&lt;.&gt; The <code class="literal cluster-admin">ip</code> field specifies the reserved source IP address and netmask from the physical network that the node is on to use with egress router pod. Use CIDR notation to specify the IP address and netmask.
				</p><p class="cluster-admin cluster-admin">
					&lt;.&gt; The <code class="literal cluster-admin">gateway</code> field specifies the IP address of the network gateway.
				</p><p class="cluster-admin cluster-admin">
					&lt;.&gt; Optional: The <code class="literal cluster-admin">redirectRules</code> field specifies a combination of egress destination IP address, egress router port, and protocol. Incoming connections to the egress router on the specified port and protocol are routed to the destination IP address.
				</p><p class="cluster-admin cluster-admin">
					&lt;.&gt; Optional: The <code class="literal cluster-admin">targetPort</code> field specifies the network port on the destination IP address. If this field is not specified, traffic is routed to the same network port that it arrived on.
				</p><p class="cluster-admin cluster-admin">
					&lt;.&gt; The <code class="literal cluster-admin">protocol</code> field supports TCP, UDP, or SCTP.
				</p><p class="cluster-admin cluster-admin">
					&lt;.&gt; Optional: The <code class="literal cluster-admin">fallbackIP</code> field specifies a destination IP address. If you do not specify any redirect rules, the egress router sends all traffic to this fallback IP address. If you specify redirect rules, any connections to network ports that are not defined in the rules are sent by the egress router to this fallback IP address. If you do not specify this field, the egress router rejects connections to network ports that are not defined in the rules.
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example egress router specification</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: network.operator.openshift.io/v1
kind: EgressRouter
metadata:
  name: egress-router-redirect
spec:
  networkInterface: {
    macvlan: {
      mode: "Bridge"
    }
  }
  addresses: [
    {
      ip: "192.168.12.99/24",
      gateway: "192.168.12.1"
    }
  ]
  mode: Redirect
  redirect: {
    redirectRules: [
      {
        destinationIP: "10.0.0.99",
        port: 80,
        protocol: UDP
      },
      {
        destinationIP: "203.0.113.26",
        port: 8080,
        targetPort: 80,
        protocol: TCP
      },
      {
        destinationIP: "203.0.113.27",
        port: 8443,
        targetPort: 443,
        protocol: TCP
      }
    ]
  }</pre>

					</p></div></section><section class="section cluster-admin" id="nw-egress-router-redirect-mode-ovn_deploying-egress-router-ovn-redirection"><div class="titlepage"><div><div><h3 class="title">26.18.2. Deploying an egress router in redirect mode</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can deploy an egress router to redirect traffic from its own reserved source IP address to one or more destination IP addresses.
				</p><p class="cluster-admin cluster-admin">
					After you add an egress router, the client pods that need to use the reserved source IP address must be modified to connect to the egress router rather than connecting directly to the destination IP.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Create an egress router definition.
						</li><li class="listitem"><p class="simpara">
							To ensure that other pods can find the IP address of the egress router pod, create a service that uses the egress router, as in the following example:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Service
metadata:
  name: egress-1
spec:
  ports:
  - name: web-app
    protocol: TCP
    port: 8080
  type: ClusterIP
  selector:
    app: egress-router-cni &lt;.&gt;</pre><p class="cluster-admin cluster-admin">
							&lt;.&gt; Specify the label for the egress router. The value shown is added by the Cluster Network Operator and is not configurable.
						</p><p class="cluster-admin cluster-admin">
							After you create the service, your pods can connect to the service. The egress router pod redirects traffic to the corresponding port on the destination IP address. The connections originate from the reserved source IP address.
						</p></li></ol></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Verification</strong></p><p>
						To verify that the Cluster Network Operator started the egress router, complete the following procedure:
					</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							View the network attachment definition that the Operator created for the egress router:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get network-attachment-definition egress-router-cni-nad</pre><p class="cluster-admin cluster-admin">
							The name of the network attachment definition is not configurable.
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                    AGE
egress-router-cni-nad   18m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							View the deployment for the egress router pod:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get deployment egress-router-cni-deployment</pre><p class="cluster-admin cluster-admin">
							The name of the deployment is not configurable.
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
egress-router-cni-deployment   1/1     1            1           18m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							View the status of the egress router pod:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods -l app=egress-router-cni</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                            READY   STATUS    RESTARTS   AGE
egress-router-cni-deployment-575465c75c-qkq6m   1/1     Running   0          18m</pre>

							</p></div></li><li class="listitem">
							View the logs and the routing table for the egress router pod.
						</li></ol></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
							Get the node name for the egress router pod:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ POD_NODENAME=$(oc get pod -l app=egress-router-cni -o jsonpath="{.items[0].spec.nodeName}")</pre></li><li class="listitem"><p class="simpara">
							Enter into a debug session on the target node. This step instantiates a debug pod called <code class="literal cluster-admin">&lt;node_name&gt;-debug</code>:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc debug node/$POD_NODENAME</pre></li><li class="listitem"><p class="simpara">
							Set <code class="literal cluster-admin">/host</code> as the root directory within the debug shell. The debug pod mounts the root file system of the host in <code class="literal cluster-admin">/host</code> within the pod. By changing the root directory to <code class="literal cluster-admin">/host</code>, you can run binaries from the executable paths of the host:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin"># chroot /host</pre></li><li class="listitem"><p class="simpara">
							From within the <code class="literal cluster-admin">chroot</code> environment console, display the egress router logs:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin"># cat /tmp/egress-router-log</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">2021-04-26T12:27:20Z [debug] Called CNI ADD
2021-04-26T12:27:20Z [debug] Gateway: 192.168.12.1
2021-04-26T12:27:20Z [debug] IP Source Addresses: [192.168.12.99/24]
2021-04-26T12:27:20Z [debug] IP Destinations: [80 UDP 10.0.0.99/30 8080 TCP 203.0.113.26/30 80 8443 TCP 203.0.113.27/30 443]
2021-04-26T12:27:20Z [debug] Created macvlan interface
2021-04-26T12:27:20Z [debug] Renamed macvlan to "net1"
2021-04-26T12:27:20Z [debug] Adding route to gateway 192.168.12.1 on macvlan interface
2021-04-26T12:27:20Z [debug] deleted default route {Ifindex: 3 Dst: &lt;nil&gt; Src: &lt;nil&gt; Gw: 10.128.10.1 Flags: [] Table: 254}
2021-04-26T12:27:20Z [debug] Added new default route with gateway 192.168.12.1
2021-04-26T12:27:20Z [debug] Added iptables rule: iptables -t nat PREROUTING -i eth0 -p UDP --dport 80 -j DNAT --to-destination 10.0.0.99
2021-04-26T12:27:20Z [debug] Added iptables rule: iptables -t nat PREROUTING -i eth0 -p TCP --dport 8080 -j DNAT --to-destination 203.0.113.26:80
2021-04-26T12:27:20Z [debug] Added iptables rule: iptables -t nat PREROUTING -i eth0 -p TCP --dport 8443 -j DNAT --to-destination 203.0.113.27:443
2021-04-26T12:27:20Z [debug] Added iptables rule: iptables -t nat -o net1 -j SNAT --to-source 192.168.12.99</pre>

							</p></div><p class="cluster-admin cluster-admin">
							The logging file location and logging level are not configurable when you start the egress router by creating an <code class="literal cluster-admin">EgressRouter</code> object as described in this procedure.
						</p></li><li class="listitem"><p class="simpara">
							From within the <code class="literal cluster-admin">chroot</code> environment console, get the container ID:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin"># crictl ps --name egress-router-cni-pod | awk '{print $1}'</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">CONTAINER
bac9fae69ddb6</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Determine the process ID of the container. In this example, the container ID is <code class="literal cluster-admin">bac9fae69ddb6</code>:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin"># crictl inspect -o yaml bac9fae69ddb6 | grep 'pid:' | awk '{print $2}'</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">68857</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Enter the network namespace of the container:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin"># nsenter -n -t 68857</pre></li><li class="listitem"><p class="simpara">
							Display the routing table:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin"># ip route</pre><p class="cluster-admin cluster-admin">
							In the following example output, the <code class="literal cluster-admin">net1</code> network interface is the default route. Traffic for the cluster network uses the <code class="literal cluster-admin">eth0</code> network interface. Traffic for the <code class="literal cluster-admin">192.168.12.0/24</code> network uses the <code class="literal cluster-admin">net1</code> network interface and originates from the reserved source IP address <code class="literal cluster-admin">192.168.12.99</code>. The pod routes all other traffic to the gateway at IP address <code class="literal cluster-admin">192.168.12.1</code>. Routing for the service network is not shown.
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">default via 192.168.12.1 dev net1
10.128.10.0/23 dev eth0 proto kernel scope link src 10.128.10.18
192.168.12.0/24 dev net1 proto kernel scope link src 192.168.12.99
192.168.12.1 dev net1</pre>

							</p></div></li></ol></div></section></section><section class="section cluster-admin" id="nw-ovn-kubernetes-enabling-multicast"><div class="titlepage"><div><div><h2 class="title">26.19. Enabling multicast for a project</h2></div></div></div><section class="section cluster-admin" id="nw-about-multicast_ovn-kubernetes-enabling-multicast"><div class="titlepage"><div><div><h3 class="title">26.19.1. About multicast</h3></div></div></div><p class="cluster-admin cluster-admin">
					With IP multicast, data is broadcast to many IP addresses simultaneously.
				</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						At this time, multicast is best used for low-bandwidth coordination or service discovery and not a high-bandwidth solution.
					</p></div></div><p class="cluster-admin cluster-admin">
					Multicast traffic between OpenShift Container Platform pods is disabled by default. If you are using the OVN-Kubernetes network plugin, you can enable multicast on a per-project basis.
				</p></section><section class="section cluster-admin" id="nw-enabling-multicast_ovn-kubernetes-enabling-multicast"><div class="titlepage"><div><div><h3 class="title">26.19.2. Enabling multicast between pods</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can enable multicast between pods for your project.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You must log in to the cluster with a user that has the <code class="literal cluster-admin">cluster-admin</code> role.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Run the following command to enable multicast for a project. Replace <code class="literal cluster-admin">&lt;namespace&gt;</code> with the namespace for the project you want to enable multicast for.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate namespace &lt;namespace&gt; \
    k8s.ovn.org/multicast-enabled=true</pre><div class="admonition tip cluster-admin"><div class="admonition_header">Tip</div><div><p class="cluster-admin cluster-admin">
							You can alternatively apply the following YAML to add the annotation:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Namespace
metadata:
  name: &lt;namespace&gt;
  annotations:
    k8s.ovn.org/multicast-enabled: "true"</pre></div></div></li></ul></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Verification</strong></p><p>
						To verify that multicast is enabled for a project, complete the following procedure:
					</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Change your current project to the project that you enabled multicast for. Replace <code class="literal cluster-admin">&lt;project&gt;</code> with the project name.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc project &lt;project&gt;</pre></li><li class="listitem"><p class="simpara">
							Create a pod to act as a multicast receiver:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat &lt;&lt;EOF| oc create -f -
apiVersion: v1
kind: Pod
metadata:
  name: mlistener
  labels:
    app: multicast-verify
spec:
  containers:
    - name: mlistener
      image: registry.access.redhat.com/ubi9
      command: ["/bin/sh", "-c"]
      args:
        ["dnf -y install socat hostname &amp;&amp; sleep inf"]
      ports:
        - containerPort: 30102
          name: mlistener
          protocol: UDP
EOF</pre></li><li class="listitem"><p class="simpara">
							Create a pod to act as a multicast sender:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat &lt;&lt;EOF| oc create -f -
apiVersion: v1
kind: Pod
metadata:
  name: msender
  labels:
    app: multicast-verify
spec:
  containers:
    - name: msender
      image: registry.access.redhat.com/ubi9
      command: ["/bin/sh", "-c"]
      args:
        ["dnf -y install socat &amp;&amp; sleep inf"]
EOF</pre></li><li class="listitem"><p class="simpara">
							In a new terminal window or tab, start the multicast listener.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Get the IP address for the Pod:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ POD_IP=$(oc get pods mlistener -o jsonpath='{.status.podIP}')</pre></li><li class="listitem"><p class="simpara">
									Start the multicast listener by entering the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec mlistener -i -t -- \
    socat UDP4-RECVFROM:30102,ip-add-membership=224.1.0.1:$POD_IP,fork EXEC:hostname</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Start the multicast transmitter.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Get the pod network IP address range:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ CIDR=$(oc get Network.config.openshift.io cluster \
    -o jsonpath='{.status.clusterNetwork[0].cidr}')</pre></li><li class="listitem"><p class="simpara">
									To send a multicast message, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec msender -i -t -- \
    /bin/bash -c "echo | socat STDIO UDP4-DATAGRAM:224.1.0.1:30102,range=$CIDR,ip-multicast-ttl=64"</pre><p class="cluster-admin cluster-admin">
									If multicast is working, the previous command returns the following output:
								</p><pre class="programlisting language-text cluster-admin cluster-admin">mlistener</pre></li></ol></div></li></ol></div></section></section><section class="section cluster-admin" id="nw-ovn-kubernetes-disabling-multicast"><div class="titlepage"><div><div><h2 class="title">26.20. Disabling multicast for a project</h2></div></div></div><section class="section cluster-admin" id="nw-disabling-multicast_ovn-kubernetes-disabling-multicast"><div class="titlepage"><div><div><h3 class="title">26.20.1. Disabling multicast between pods</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can disable multicast between pods for your project.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You must log in to the cluster with a user that has the <code class="literal cluster-admin">cluster-admin</code> role.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Disable multicast by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate namespace &lt;namespace&gt; \ <span id="CO158-1"><!--Empty--></span><span class="callout">1</span>
    k8s.ovn.org/multicast-enabled-</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO158-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal cluster-admin">namespace</code> for the project you want to disable multicast for.
								</div></dd></dl></div><div class="admonition tip cluster-admin"><div class="admonition_header">Tip</div><div><p class="cluster-admin cluster-admin">
							You can alternatively apply the following YAML to delete the annotation:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Namespace
metadata:
  name: &lt;namespace&gt;
  annotations:
    k8s.ovn.org/multicast-enabled: null</pre></div></div></li></ul></div></section></section><section class="section cluster-admin" id="tracking-network-flows"><div class="titlepage"><div><div><h2 class="title">26.21. Tracking network flows</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can collect information about pod network flows from your cluster to assist with the following areas:
			</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
						Monitor ingress and egress traffic on the pod network.
					</li><li class="listitem">
						Troubleshoot performance issues.
					</li><li class="listitem">
						Gather data for capacity planning and security audits.
					</li></ul></div><p class="cluster-admin cluster-admin">
				When you enable the collection of the network flows, only the metadata about the traffic is collected. For example, packet data is not collected, but the protocol, source address, destination address, port numbers, number of bytes, and other packet-level information is collected.
			</p><p class="cluster-admin cluster-admin">
				The data is collected in one or more of the following record formats:
			</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
						NetFlow
					</li><li class="listitem">
						sFlow
					</li><li class="listitem">
						IPFIX
					</li></ul></div><p class="cluster-admin cluster-admin">
				When you configure the Cluster Network Operator (CNO) with one or more collector IP addresses and port numbers, the Operator configures Open vSwitch (OVS) on each node to send the network flows records to each collector.
			</p><p class="cluster-admin cluster-admin">
				You can configure the Operator to send records to more than one type of network flow collector. For example, you can send records to NetFlow collectors and also send records to sFlow collectors.
			</p><p class="cluster-admin cluster-admin">
				When OVS sends data to the collectors, each type of collector receives identical records. For example, if you configure two NetFlow collectors, OVS on a node sends identical records to the two collectors. If you also configure two sFlow collectors, the two sFlow collectors receive identical records. However, each collector type has a unique record format.
			</p><p class="cluster-admin cluster-admin">
				Collecting the network flows data and sending the records to collectors affects performance. Nodes process packets at a slower rate. If the performance impact is too great, you can delete the destinations for collectors to disable collecting network flows data and restore performance.
			</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
					Enabling network flow collectors might have an impact on the overall performance of the cluster network.
				</p></div></div><section class="section cluster-admin" id="nw-network-flows-object_tracking-network-flows"><div class="titlepage"><div><div><h3 class="title">26.21.1. Network object configuration for tracking network flows</h3></div></div></div><p class="cluster-admin cluster-admin">
					The fields for configuring network flows collectors in the Cluster Network Operator (CNO) are shown in the following table:
				</p><div class="table" id="idm140587106414976"><p class="title"><strong>Table 26.13. Network flows configuration</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="middle" id="idm140587106408896" scope="col">Field</th><th align="left" valign="middle" id="idm140587106407808" scope="col">Type</th><th align="left" valign="middle" id="idm140587106406720" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="middle" headers="idm140587106408896"> <p>
									<code class="literal cluster-admin">metadata.name</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587106407808"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587106406720"> <p class="cluster-admin cluster-admin">
									The name of the CNO object. This name is always <code class="literal cluster-admin">cluster</code>.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587106408896"> <p>
									<code class="literal cluster-admin">spec.exportNetworkFlows</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587106407808"> <p>
									<code class="literal cluster-admin">object</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587106406720"> <p class="cluster-admin cluster-admin">
									One or more of <code class="literal cluster-admin">netFlow</code>, <code class="literal cluster-admin">sFlow</code>, or <code class="literal cluster-admin">ipfix</code>.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587106408896"> <p>
									<code class="literal cluster-admin">spec.exportNetworkFlows.netFlow.collectors</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587106407808"> <p>
									<code class="literal cluster-admin">array</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587106406720"> <p class="cluster-admin cluster-admin">
									A list of IP address and network port pairs for up to 10 collectors.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587106408896"> <p>
									<code class="literal cluster-admin">spec.exportNetworkFlows.sFlow.collectors</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587106407808"> <p>
									<code class="literal cluster-admin">array</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587106406720"> <p class="cluster-admin cluster-admin">
									A list of IP address and network port pairs for up to 10 collectors.
								</p>
								 </td></tr><tr><td align="left" valign="middle" headers="idm140587106408896"> <p>
									<code class="literal cluster-admin">spec.exportNetworkFlows.ipfix.collectors</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587106407808"> <p>
									<code class="literal cluster-admin">array</code>
								</p>
								 </td><td align="left" valign="middle" headers="idm140587106406720"> <p class="cluster-admin cluster-admin">
									A list of IP address and network port pairs for up to 10 collectors.
								</p>
								 </td></tr></tbody></table></div></div><p class="cluster-admin cluster-admin">
					After applying the following manifest to the CNO, the Operator configures Open vSwitch (OVS) on each node in the cluster to send network flows records to the NetFlow collector that is listening at <code class="literal cluster-admin">192.168.1.99:2056</code>.
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example configuration for tracking network flows</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  exportNetworkFlows:
    netFlow:
      collectors:
        - 192.168.1.99:2056</pre>

					</p></div></section><section class="section cluster-admin" id="nw-network-flows-create_tracking-network-flows"><div class="titlepage"><div><div><h3 class="title">26.21.2. Adding destinations for network flows collectors</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can configure the Cluster Network Operator (CNO) to send network flows metadata about the pod network to a network flows collector.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You are logged in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li><li class="listitem">
							You have a network flows collector and know the IP address and port that it listens on.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a patch file that specifies the network flows collector type and the IP address and port information of the collectors:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">spec:
  exportNetworkFlows:
    netFlow:
      collectors:
        - 192.168.1.99:2056</pre></li><li class="listitem"><p class="simpara">
							Configure the CNO with the network flows collectors:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch network.operator cluster --type merge -p "$(cat &lt;file_name&gt;.yaml)"</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">network.operator.openshift.io/cluster patched</pre>

							</p></div></li></ol></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Verification</strong></p><p>
						Verification is not typically necessary. You can run the following command to confirm that Open vSwitch (OVS) on each node is configured to send network flows records to one or more collectors.
					</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							View the Operator configuration to confirm that the <code class="literal cluster-admin">exportNetworkFlows</code> field is configured:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get network.operator cluster -o jsonpath="{.spec.exportNetworkFlows}"</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">{"netFlow":{"collectors":["192.168.1.99:2056"]}}</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							View the network flows configuration in OVS from each node:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ for pod in $(oc get pods -n openshift-ovn-kubernetes -l app=ovnkube-node -o jsonpath='{range@.items[*]}{.metadata.name}{"\n"}{end}');
  do ;
    echo;
    echo $pod;
    oc -n openshift-ovn-kubernetes exec -c ovnkube-node $pod \
      -- bash -c 'for type in ipfix sflow netflow ; do ovs-vsctl find $type ; done';
done</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">ovnkube-node-xrn4p
_uuid               : a4d2aaca-5023-4f3d-9400-7275f92611f9
active_timeout      : 60
add_id_to_interface : false
engine_id           : []
engine_type         : []
external_ids        : {}
targets             : ["192.168.1.99:2056"]

ovnkube-node-z4vq9
_uuid               : 61d02fdb-9228-4993-8ff5-b27f01a29bd6
active_timeout      : 60
add_id_to_interface : false
engine_id           : []
engine_type         : []
external_ids        : {}
targets             : ["192.168.1.99:2056"]-

...</pre>

							</p></div></li></ol></div></section><section class="section cluster-admin" id="nw-network-flows-delete_tracking-network-flows"><div class="titlepage"><div><div><h3 class="title">26.21.3. Deleting all destinations for network flows collectors</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can configure the Cluster Network Operator (CNO) to stop sending network flows metadata to a network flows collector.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You are logged in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Remove all network flows collectors:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch network.operator cluster --type='json' \
    -p='[{"op":"remove", "path":"/spec/exportNetworkFlows"}]'</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">network.operator.openshift.io/cluster patched</pre>

							</p></div></li></ol></div></section><section class="section _additional-resources" id="additional-resources_tracking-network-flows"><div class="titlepage"><div><div><h3 class="title">26.21.4. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/api_reference/#network-operator-openshift-io-v1">Network [operator.openshift.io/v1</a>]
						</li></ul></div></section></section><section class="section cluster-admin" id="configuring-hybrid-networking"><div class="titlepage"><div><div><h2 class="title">26.22. Configuring hybrid networking</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can configure the Red Hat OpenShift Networking OVN-Kubernetes network plugin to allow Linux and Windows nodes to host Linux and Windows workloads, respectively.
			</p><section class="section cluster-admin" id="configuring-hybrid-ovnkubernetes_configuring-hybrid-networking"><div class="titlepage"><div><div><h3 class="title">26.22.1. Configuring hybrid networking with OVN-Kubernetes</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can configure your cluster to use hybrid networking with the OVN-Kubernetes network plugin. This allows a hybrid cluster that supports different node networking configurations.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						This configuration is necessary to run both Linux and Windows nodes in the same cluster.
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in to the cluster with a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li><li class="listitem">
							Ensure that the cluster uses the OVN-Kubernetes network plugin.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							To configure the OVN-Kubernetes hybrid network overlay, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch networks.operator.openshift.io cluster --type=merge \
  -p '{
    "spec":{
      "defaultNetwork":{
        "ovnKubernetesConfig":{
          "hybridOverlayConfig":{
            "hybridClusterNetwork":[
              {
                "cidr": "&lt;cidr&gt;",
                "hostPrefix": &lt;prefix&gt;
              }
            ],
            "hybridOverlayVXLANPort": &lt;overlay_port&gt;
          }
        }
      }
    }
  }'</pre><p class="cluster-admin cluster-admin">
							where:
						</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">cidr</code></span></dt><dd>
										Specify the CIDR configuration used for nodes on the additional overlay network. This CIDR cannot overlap with the cluster network CIDR.
									</dd><dt><span class="term"><code class="literal cluster-admin">hostPrefix</code></span></dt><dd>
										Specifies the subnet prefix length to assign to each individual node. For example, if <code class="literal cluster-admin">hostPrefix</code> is set to <code class="literal cluster-admin">23</code>, then each node is assigned a <code class="literal cluster-admin">/23</code> subnet out of the given <code class="literal cluster-admin">cidr</code>, which allows for 510 (2^(32 - 23) - 2) pod IP addresses. If you are required to provide access to nodes from an external network, configure load balancers and routers to manage the traffic.
									</dd><dt><span class="term"><code class="literal cluster-admin">hybridOverlayVXLANPort</code></span></dt><dd>
										Specify a custom VXLAN port for the additional overlay network. This is required for running Windows nodes in a cluster installed on vSphere, and must not be configured for any other cloud provider. The custom port can be any open port excluding the default <code class="literal cluster-admin">4789</code> port. For more information on this requirement, see the Microsoft documentation on <a class="link" href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/common-problems#pod-to-pod-connectivity-between-hosts-is-broken-on-my-kubernetes-cluster-running-on-vsphere">Pod-to-pod connectivity between hosts is broken</a>.
									</dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								Windows Server Long-Term Servicing Channel (LTSC): Windows Server 2019 is not supported on clusters with a custom <code class="literal cluster-admin">hybridOverlayVXLANPort</code> value because this Windows server version does not support selecting a custom VXLAN port.
							</p></div></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">network.operator.openshift.io/cluster patched</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To confirm that the configuration is active, enter the following command. It can take several minutes for the update to apply.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get network.operator.openshift.io -o jsonpath="{.items[0].spec.defaultNetwork.ovnKubernetesConfig}"</pre></li></ol></div></section><section class="section _additional-resources" id="configuring-hybrid-networking-additional-resources"><div class="titlepage"><div><div><h3 class="title">26.22.2. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/windows_container_support_for_openshift/#understanding-windows-container-workloads">Understanding Windows container workloads</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/windows_container_support_for_openshift/#enabling-windows-container-workloads">Enabling Windows container workloads</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-aws-network-customizations">Installing a cluster on AWS with network customizations</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-azure-network-customizations">Installing a cluster on Azure with network customizations</a>
						</li></ul></div></section></section></section><section class="chapter cluster-admin" id="openshift-sdn-network-plugin"><div class="titlepage"><div><div><h1 class="title">Chapter 27. OpenShift SDN network plugin</h1></div></div></div><section class="section cluster-admin" id="about-openshift-sdn"><div class="titlepage"><div><div><h2 class="title">27.1. About the OpenShift SDN network plugin</h2></div></div></div><p class="cluster-admin cluster-admin">
				Part of Red Hat OpenShift Networking, OpenShift SDN is a network plugin that uses a software-defined networking (SDN) approach to provide a unified cluster network that enables communication between pods across the OpenShift Container Platform cluster. This pod network is established and maintained by OpenShift SDN, which configures an overlay network using Open vSwitch (OVS).
			</p><section class="section cluster-admin" id="nw-openshift-sdn-modes_about-openshift-sdn"><div class="titlepage"><div><div><h3 class="title">27.1.1. OpenShift SDN network isolation modes</h3></div></div></div><p class="cluster-admin cluster-admin">
					OpenShift SDN provides three SDN modes for configuring the pod network:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<span class="emphasis"><em><span class="cluster-admin cluster-admin">Network policy</span></em></span> mode allows project administrators to configure their own isolation policies using <code class="literal cluster-admin">NetworkPolicy</code> objects. Network policy is the default mode in OpenShift Container Platform 4.13.
						</li><li class="listitem">
							<span class="emphasis"><em><span class="cluster-admin cluster-admin">Multitenant</span></em></span> mode provides project-level isolation for pods and services. Pods from different projects cannot send packets to or receive packets from pods and services of a different project. You can disable isolation for a project, allowing it to send network traffic to all pods and services in the entire cluster and receive network traffic from those pods and services.
						</li><li class="listitem">
							<span class="emphasis"><em><span class="cluster-admin cluster-admin">Subnet</span></em></span> mode provides a flat pod network where every pod can communicate with every other pod and service. The network policy mode provides the same functionality as subnet mode.
						</li></ul></div></section><section class="section cluster-admin" id="nw-ovn-kubernetes-matrix_about-openshift-sdn"><div class="titlepage"><div><div><h3 class="title">27.1.2. Supported network plugin feature matrix</h3></div></div></div><p class="cluster-admin cluster-admin">
					Red Hat OpenShift Networking offers two options for the network plugin, OpenShift SDN and OVN-Kubernetes, for the network plugin. The following table summarizes the current feature support for both network plugins:
				</p><div class="table" id="idm140587105520576"><p class="title"><strong>Table 27.1. Default CNI network plugin feature comparison</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587105514544" scope="col">Feature</th><th align="left" valign="top" id="idm140587105513456" scope="col">OpenShift SDN</th><th align="left" valign="top" id="idm140587105512368" scope="col">OVN-Kubernetes</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587105514544"> <p>
									Egress IPs
								</p>
								 </td><td align="left" valign="top" headers="idm140587105513456"> <p>
									Supported
								</p>
								 </td><td align="left" valign="top" headers="idm140587105512368"> <p>
									Supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587105514544"> <p>
									Egress firewall <sup>[1]</sup>
								</p>
								 </td><td align="left" valign="top" headers="idm140587105513456"> <p>
									Supported
								</p>
								 </td><td align="left" valign="top" headers="idm140587105512368"> <p>
									Supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587105514544"> <p>
									Egress router
								</p>
								 </td><td align="left" valign="top" headers="idm140587105513456"> <p>
									Supported
								</p>
								 </td><td align="left" valign="top" headers="idm140587105512368"> <p>
									Supported <sup>[2]</sup>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587105514544"> <p>
									Hybrid networking
								</p>
								 </td><td align="left" valign="top" headers="idm140587105513456"> <p>
									Not supported
								</p>
								 </td><td align="left" valign="top" headers="idm140587105512368"> <p>
									Supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587105514544"> <p>
									IPsec encryption
								</p>
								 </td><td align="left" valign="top" headers="idm140587105513456"> <p>
									Not supported
								</p>
								 </td><td align="left" valign="top" headers="idm140587105512368"> <p>
									Supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587105514544"> <p>
									IPv6
								</p>
								 </td><td align="left" valign="top" headers="idm140587105513456"> <p>
									Not supported
								</p>
								 </td><td align="left" valign="top" headers="idm140587105512368"> <p>
									Supported <sup>[3]</sup> <sup>[4]</sup>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587105514544"> <p>
									Kubernetes network policy
								</p>
								 </td><td align="left" valign="top" headers="idm140587105513456"> <p>
									Supported
								</p>
								 </td><td align="left" valign="top" headers="idm140587105512368"> <p>
									Supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587105514544"> <p>
									Kubernetes network policy logs
								</p>
								 </td><td align="left" valign="top" headers="idm140587105513456"> <p>
									Not supported
								</p>
								 </td><td align="left" valign="top" headers="idm140587105512368"> <p>
									Supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587105514544"> <p>
									Multicast
								</p>
								 </td><td align="left" valign="top" headers="idm140587105513456"> <p>
									Supported
								</p>
								 </td><td align="left" valign="top" headers="idm140587105512368"> <p>
									Supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587105514544"> <p>
									Hardware offloading
								</p>
								 </td><td align="left" valign="top" headers="idm140587105513456"> <p>
									Not supported
								</p>
								 </td><td align="left" valign="top" headers="idm140587105512368"> <p>
									Supported
								</p>
								 </td></tr></tbody></table></div></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Egress firewall is also known as egress network policy in OpenShift SDN. This is not the same as network policy egress.
						</li><li class="listitem">
							Egress router for OVN-Kubernetes supports only redirect mode.
						</li><li class="listitem">
							IPv6 is supported only on bare metal, IBM Power, and IBM Z clusters.
						</li><li class="listitem">
							IPv6 single stack does not support <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#k8s-nmstate-about-the-k8s-nmstate-operator">Kubernetes NMState</a>.
						</li></ol></div></section></section><section class="section cluster-admin" id="migrate-to-openshift-sdn"><div class="titlepage"><div><div><h2 class="title">27.2. Migrating to the OpenShift SDN network plugin</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can migrate to the OpenShift SDN network plugin from the OVN-Kubernetes network plugin.
			</p><p class="cluster-admin cluster-admin">
				To learn more about OpenShift SDN, read <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-openshift-sdn">About the OpenShift SDN network plugin</a>.
			</p><section class="section cluster-admin" id="how-the-migration-process-works_migrate-to-openshift-sdn"><div class="titlepage"><div><div><h3 class="title">27.2.1. How the migration process works</h3></div></div></div><p class="cluster-admin cluster-admin">
					The following table summarizes the migration process by segmenting between the user-initiated steps in the process and the actions that the migration performs in response.
				</p><div class="table" id="idm140587106666256"><p class="title"><strong>Table 27.2. Migrating to OpenShift SDN from OVN-Kubernetes</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587106661088" scope="col">User-initiated steps</th><th align="left" valign="top" id="idm140587106660000" scope="col">Migration activity</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587106661088"> <p>
									Set the <code class="literal cluster-admin">migration</code> field of the <code class="literal cluster-admin">Network.operator.openshift.io</code> custom resource (CR) named <code class="literal cluster-admin">cluster</code> to <code class="literal cluster-admin">OpenShiftSDN</code>. Make sure the <code class="literal cluster-admin">migration</code> field is <code class="literal cluster-admin">null</code> before setting it to a value.
								</p>
								 </td><td align="left" valign="top" headers="idm140587106660000"> <div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Cluster Network Operator (CNO)</span></dt><dd>
												Updates the status of the <code class="literal cluster-admin">Network.config.openshift.io</code> CR named <code class="literal cluster-admin">cluster</code> accordingly.
											</dd><dt><span class="term">Machine Config Operator (MCO)</span></dt><dd>
												Rolls out an update to the systemd configuration necessary for OpenShift SDN; the MCO updates a single machine per pool at a time by default, causing the total time the migration takes to increase with the size of the cluster.
											</dd></dl></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587106661088"> <p>
									Update the <code class="literal cluster-admin">networkType</code> field of the <code class="literal cluster-admin">Network.config.openshift.io</code> CR.
								</p>
								 </td><td align="left" valign="top" headers="idm140587106660000"> <div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">CNO</span></dt><dd><p class="simpara">
												Performs the following actions:
											</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
														Destroys the OVN-Kubernetes control plane pods.
													</li><li class="listitem">
														Deploys the OpenShift SDN control plane pods.
													</li><li class="listitem">
														Updates the Multus objects to reflect the new network plugin.
													</li></ul></div></dd></dl></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587106661088"> <p>
									Reboot each node in the cluster.
								</p>
								 </td><td align="left" valign="top" headers="idm140587106660000"> <div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Cluster</span></dt><dd>
												As nodes reboot, the cluster assigns IP addresses to pods on the OpenShift SDN cluster network.
											</dd></dl></div>
								 </td></tr></tbody></table></div></div></section><section class="section cluster-admin" id="nw-ovn-kubernetes-rollback_migrate-to-openshift-sdn"><div class="titlepage"><div><div><h3 class="title">27.2.2. Migrating to the OpenShift SDN network plugin</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can migrate to the OpenShift SDN Container Network Interface (CNI) network plugin. During the migration you must reboot every node in your cluster.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li><li class="listitem">
							A cluster installed on infrastructure configured with the OVN-Kubernetes network plugin.
						</li><li class="listitem">
							A recent backup of the etcd database is available.
						</li><li class="listitem">
							A reboot can be triggered manually for each node.
						</li><li class="listitem">
							The cluster is in a known good state, without any errors.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Stop all of the machine configuration pools managed by the Machine Config Operator (MCO):
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
									Stop the master configuration pool:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch MachineConfigPool master --type='merge' --patch \
  '{ "spec": { "paused": true } }'</pre></li><li class="listitem"><p class="simpara">
									Stop the worker machine configuration pool:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch MachineConfigPool worker --type='merge' --patch \
  '{ "spec":{ "paused": true } }'</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							To prepare for the migration, set the migration field to <code class="literal cluster-admin">null</code> by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": null } }'</pre></li><li class="listitem"><p class="simpara">
							To start the migration, set the network plugin back to OpenShift SDN by entering the following commands:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": { "networkType": "OpenShiftSDN" } } }'

$ oc patch Network.config.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "networkType": "OpenShiftSDN" } }'</pre></li><li class="listitem"><p class="simpara">
							Optional: You can disable automatic migration of several OVN-Kubernetes capabilities to the OpenShift SDN equivalents:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Egress IPs
								</li><li class="listitem">
									Egress firewall
								</li><li class="listitem">
									Multicast
								</li></ul></div><p class="cluster-admin cluster-admin">
							To disable automatic migration of the configuration for any of the previously noted OpenShift SDN features, specify the following keys:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{
    "spec": {
      "migration": {
        "networkType": "OpenShiftSDN",
        "features": {
          "egressIP": &lt;bool&gt;,
          "egressFirewall": &lt;bool&gt;,
          "multicast": &lt;bool&gt;
        }
      }
    }
  }'</pre><p class="cluster-admin cluster-admin">
							where:
						</p><p class="cluster-admin cluster-admin">
							<code class="literal cluster-admin">bool</code>: Specifies whether to enable migration of the feature. The default is <code class="literal cluster-admin">true</code>.
						</p></li><li class="listitem"><p class="simpara">
							Optional: You can customize the following settings for OpenShift SDN to meet your network infrastructure requirements:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Maximum transmission unit (MTU)
								</li><li class="listitem">
									VXLAN port
								</li></ul></div><p class="cluster-admin cluster-admin">
							To customize either or both of the previously noted settings, customize and enter the following command. If you do not need to change the default value, omit the key from the patch.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "openshiftSDNConfig":{
          "mtu":&lt;mtu&gt;,
          "vxlanPort":&lt;port&gt;
    }}}}'</pre><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">mtu</code></span></dt><dd>
										The MTU for the VXLAN overlay network. This value is normally configured automatically, but if the nodes in your cluster do not all use the same MTU, then you must set this explicitly to <code class="literal cluster-admin">50</code> less than the smallest node MTU value.
									</dd><dt><span class="term"><code class="literal cluster-admin">port</code></span></dt><dd>
										The UDP port for the VXLAN overlay network. If a value is not specified, the default is <code class="literal cluster-admin">4789</code>. The port cannot be the same as the Geneve port that is used by OVN-Kubernetes. The default value for the Geneve port is <code class="literal cluster-admin">6081</code>.
									</dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example patch command</strong></p><p>
								
<pre class="programlisting language-terminal">$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "openshiftSDNConfig":{
          "mtu":1200
    }}}}'</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Wait until the Multus daemon set rollout completes.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n openshift-multus rollout status daemonset/multus</pre><p class="cluster-admin cluster-admin">
							The name of the Multus pods is in form of <code class="literal cluster-admin">multus-&lt;xxxxx&gt;</code> where <code class="literal cluster-admin">&lt;xxxxx&gt;</code> is a random sequence of letters. It might take several moments for the pods to restart.
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">Waiting for daemon set "multus" rollout to finish: 1 out of 6 new pods have been updated...
...
Waiting for daemon set "multus" rollout to finish: 5 of 6 updated pods are available...
daemon set "multus" successfully rolled out</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To complete changing the network plugin, reboot each node in your cluster. You can reboot the nodes in your cluster with either of the following approaches:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
									With the <code class="literal cluster-admin">oc rsh</code> command, you can use a bash script similar to the following:
								</p><pre class="programlisting language-bash cluster-admin cluster-admin">#!/bin/bash
readarray -t POD_NODES &lt;&lt;&lt; "$(oc get pod -n openshift-machine-config-operator -o wide| grep daemon|awk '{print $1" "$7}')"

for i in "${POD_NODES[@]}"
do
  read -r POD NODE &lt;&lt;&lt; "$i"
  until oc rsh -n openshift-machine-config-operator "$POD" chroot /rootfs shutdown -r +1
    do
      echo "cannot reboot node $NODE, retry" &amp;&amp; sleep 3
    done
done</pre></li><li class="listitem"><p class="simpara">
									With the <code class="literal cluster-admin">ssh</code> command, you can use a bash script similar to the following. The script assumes that you have configured sudo to not prompt for a password.
								</p><pre class="programlisting language-bash cluster-admin cluster-admin">#!/bin/bash

for ip in $(oc get nodes  -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}')
do
   echo "reboot node $ip"
   ssh -o StrictHostKeyChecking=no core@$ip sudo shutdown -r -t 3
done</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							After the nodes in your cluster have rebooted, start all of the machine configuration pools:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
									Start the master configuration pool:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch MachineConfigPool master --type='merge' --patch \
  '{ "spec": { "paused": false } }'</pre></li><li class="listitem"><p class="simpara">
									Start the worker configuration pool:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch MachineConfigPool worker --type='merge' --patch \
  '{ "spec": { "paused": false } }'</pre></li></ul></div><p class="cluster-admin cluster-admin">
							As the MCO updates machines in each config pool, it reboots each node.
						</p><p class="cluster-admin cluster-admin">
							By default the MCO updates a single machine per pool at a time, so the time that the migration requires to complete grows with the size of the cluster.
						</p></li><li class="listitem"><p class="simpara">
							Confirm the status of the new machine configuration on the hosts:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									To list the machine configuration state and the name of the applied machine configuration, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc describe node | egrep "hostname|machineconfig"</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">kubernetes.io/hostname=master-0
machineconfiguration.openshift.io/currentConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/desiredConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done</pre>

									</p></div><p class="cluster-admin cluster-admin">
									Verify that the following statements are true:
								</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											The value of <code class="literal cluster-admin">machineconfiguration.openshift.io/state</code> field is <code class="literal cluster-admin">Done</code>.
										</li><li class="listitem">
											The value of the <code class="literal cluster-admin">machineconfiguration.openshift.io/currentConfig</code> field is equal to the value of the <code class="literal cluster-admin">machineconfiguration.openshift.io/desiredConfig</code> field.
										</li></ul></div></li><li class="listitem"><p class="simpara">
									To confirm that the machine config is correct, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get machineconfig &lt;config_name&gt; -o yaml</pre><p class="cluster-admin cluster-admin">
									where <code class="literal cluster-admin">&lt;config_name&gt;</code> is the name of the machine config from the <code class="literal cluster-admin">machineconfiguration.openshift.io/currentConfig</code> field.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Confirm that the migration succeeded:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									To confirm that the network plugin is OpenShift SDN, enter the following command. The value of <code class="literal cluster-admin">status.networkType</code> must be <code class="literal cluster-admin">OpenShiftSDN</code>.
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get network.config/cluster -o jsonpath='{.status.networkType}{"\n"}'</pre></li><li class="listitem"><p class="simpara">
									To confirm that the cluster nodes are in the <code class="literal cluster-admin">Ready</code> state, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get nodes</pre></li><li class="listitem"><p class="simpara">
									If a node is stuck in the <code class="literal cluster-admin">NotReady</code> state, investigate the machine config daemon pod logs and resolve any errors.
								</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="i"><li class="listitem"><p class="simpara">
											To list the pods, enter the following command:
										</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pod -n openshift-machine-config-operator</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
												
<pre class="programlisting language-terminal">NAME                                         READY   STATUS    RESTARTS   AGE
machine-config-controller-75f756f89d-sjp8b   1/1     Running   0          37m
machine-config-daemon-5cf4b                  2/2     Running   0          43h
machine-config-daemon-7wzcd                  2/2     Running   0          43h
machine-config-daemon-fc946                  2/2     Running   0          43h
machine-config-daemon-g2v28                  2/2     Running   0          43h
machine-config-daemon-gcl4f                  2/2     Running   0          43h
machine-config-daemon-l5tnv                  2/2     Running   0          43h
machine-config-operator-79d9c55d5-hth92      1/1     Running   0          37m
machine-config-server-bsc8h                  1/1     Running   0          43h
machine-config-server-hklrm                  1/1     Running   0          43h
machine-config-server-k9rtx                  1/1     Running   0          43h</pre>

											</p></div><p class="cluster-admin cluster-admin">
											The names for the config daemon pods are in the following format: <code class="literal cluster-admin">machine-config-daemon-&lt;seq&gt;</code>. The <code class="literal cluster-admin">&lt;seq&gt;</code> value is a random five character alphanumeric sequence.
										</p></li><li class="listitem"><p class="simpara">
											To display the pod log for each machine config daemon pod shown in the previous output, enter the following command:
										</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc logs &lt;pod&gt; -n openshift-machine-config-operator</pre><p class="cluster-admin cluster-admin">
											where <code class="literal cluster-admin">pod</code> is the name of a machine config daemon pod.
										</p></li><li class="listitem">
											Resolve any errors in the logs shown by the output from the previous command.
										</li></ol></div></li><li class="listitem"><p class="simpara">
									To confirm that your pods are not in an error state, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods --all-namespaces -o wide --sort-by='{.spec.nodeName}'</pre><p class="cluster-admin cluster-admin">
									If pods on a node are in an error state, reboot that node.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Complete the following steps only if the migration succeeds and your cluster is in a good state:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									To remove the migration configuration from the Cluster Network Operator configuration object, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": null } }'</pre></li><li class="listitem"><p class="simpara">
									To remove the OVN-Kubernetes configuration, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "defaultNetwork": { "ovnKubernetesConfig":null } } }'</pre></li><li class="listitem"><p class="simpara">
									To remove the OVN-Kubernetes network provider namespace, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete namespace openshift-ovn-kubernetes</pre></li></ol></div></li></ol></div></section><section class="section _additional-resources" id="migrate-to-openshift-sdn-additional-resources"><div class="titlepage"><div><div><h3 class="title">27.2.3. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-operator-configuration-parameters-for-openshift-sdn_cluster-network-operator">Configuration parameters for the OpenShift SDN network plugin</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/backup_and_restore/#backup-etcd">Backing up etcd</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-network-policy">About network policy</a>
						</li><li class="listitem"><p class="simpara">
							OpenShift SDN capabilities
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#assigning-egress-ips">Configuring egress IPs for a project</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-egress-firewall">Configuring an egress firewall for a project</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#enabling-multicast">Enabling multicast for a project</a>
								</li></ul></div></li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/api_reference/#network-operator-openshift-io-v1">Network [operator.openshift.io/v1</a>]
						</li></ul></div></section></section><section class="section cluster-admin" id="roll-back-to-ovn-kubernetes"><div class="titlepage"><div><div><h2 class="title">27.3. Rolling back to the OVN-Kubernetes network plugin</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can rollback to the OVN-Kubernetes network plugin from the OpenShift SDN network plugin if the migration to OpenShift SDN is unsuccessful.
			</p><p class="cluster-admin cluster-admin">
				To learn more about OVN-Kubernetes, read <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-ovn-kubernetes">About the OVN-Kubernetes network plugin</a>.
			</p><section class="section cluster-admin" id="nw-ovn-kubernetes-migration_roll-back-to-ovn-kubernetes"><div class="titlepage"><div><div><h3 class="title">27.3.1. Migrating to the OVN-Kubernetes network plugin</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can change the network plugin for your cluster to OVN-Kubernetes. During the migration, you must reboot every node in your cluster.
				</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						While performing the migration, your cluster is unavailable and workloads might be interrupted. Perform the migration only when an interruption in service is acceptable.
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							A cluster configured with the OpenShift SDN CNI network plugin in the network policy isolation mode.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li><li class="listitem">
							A recent backup of the etcd database is available.
						</li><li class="listitem">
							A reboot can be triggered manually for each node.
						</li><li class="listitem">
							The cluster is in a known good state, without any errors.
						</li><li class="listitem">
							On all cloud platforms after updating software, a security group rule must be in place to allow UDP packets on port <code class="literal cluster-admin">6081</code> for all nodes.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							To backup the configuration for the cluster network, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get Network.config.openshift.io cluster -o yaml &gt; cluster-openshift-sdn.yaml</pre></li><li class="listitem"><p class="simpara">
							To prepare all the nodes for the migration, set the <code class="literal cluster-admin">migration</code> field on the Cluster Network Operator configuration object by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": { "networkType": "OVNKubernetes" } } }'</pre><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								This step does not deploy OVN-Kubernetes immediately. Instead, specifying the <code class="literal cluster-admin">migration</code> field triggers the Machine Config Operator (MCO) to apply new machine configs to all the nodes in the cluster in preparation for the OVN-Kubernetes deployment.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Optional: You can disable automatic migration of several OpenShift SDN capabilities to the OVN-Kubernetes equivalents:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Egress IPs
								</li><li class="listitem">
									Egress firewall
								</li><li class="listitem">
									Multicast
								</li></ul></div><p class="cluster-admin cluster-admin">
							To disable automatic migration of the configuration for any of the previously noted OpenShift SDN features, specify the following keys:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{
    "spec": {
      "migration": {
        "networkType": "OVNKubernetes",
        "features": {
          "egressIP": &lt;bool&gt;,
          "egressFirewall": &lt;bool&gt;,
          "multicast": &lt;bool&gt;
        }
      }
    }
  }'</pre><p class="cluster-admin cluster-admin">
							where:
						</p><p class="cluster-admin cluster-admin">
							<code class="literal cluster-admin">bool</code>: Specifies whether to enable migration of the feature. The default is <code class="literal cluster-admin">true</code>.
						</p></li><li class="listitem"><p class="simpara">
							Optional: You can customize the following settings for OVN-Kubernetes to meet your network infrastructure requirements:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Maximum transmission unit (MTU)
								</li><li class="listitem">
									Geneve (Generic Network Virtualization Encapsulation) overlay network port
								</li><li class="listitem">
									OVN-Kubernetes IPv4 internal subnet
								</li><li class="listitem">
									OVN-Kubernetes IPv6 internal subnet
								</li></ul></div><p class="cluster-admin cluster-admin">
							To customize either of the previously noted settings, enter and customize the following command. If you do not need to change the default value, omit the key from the patch.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "ovnKubernetesConfig":{
          "mtu":&lt;mtu&gt;,
          "genevePort":&lt;port&gt;,
          "v4InternalSubnet":"&lt;ipv4_subnet&gt;",
          "v6InternalSubnet":"&lt;ipv6_subnet&gt;"
    }}}}'</pre><p class="cluster-admin cluster-admin">
							where:
						</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">mtu</code></span></dt><dd>
										The MTU for the Geneve overlay network. This value is normally configured automatically, but if the nodes in your cluster do not all use the same MTU, then you must set this explicitly to <code class="literal cluster-admin">100</code> less than the smallest node MTU value.
									</dd><dt><span class="term"><code class="literal cluster-admin">port</code></span></dt><dd>
										The UDP port for the Geneve overlay network. If a value is not specified, the default is <code class="literal cluster-admin">6081</code>. The port cannot be the same as the VXLAN port that is used by OpenShift SDN. The default value for the VXLAN port is <code class="literal cluster-admin">4789</code>.
									</dd><dt><span class="term"><code class="literal cluster-admin">ipv4_subnet</code></span></dt><dd>
										An IPv4 address range for internal use by OVN-Kubernetes. You must ensure that the IP address range does not overlap with any other subnet used by your OpenShift Container Platform installation. The IP address range must be larger than the maximum number of nodes that can be added to the cluster. The default value is <code class="literal cluster-admin">100.64.0.0/16</code>.
									</dd><dt><span class="term"><code class="literal cluster-admin">ipv6_subnet</code></span></dt><dd>
										An IPv6 address range for internal use by OVN-Kubernetes. You must ensure that the IP address range does not overlap with any other subnet used by your OpenShift Container Platform installation. The IP address range must be larger than the maximum number of nodes that can be added to the cluster. The default value is <code class="literal cluster-admin">fd98::/48</code>.
									</dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example patch command to update <code class="literal cluster-admin">mtu</code> field</strong></p><p>
								
<pre class="programlisting language-terminal">$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "ovnKubernetesConfig":{
          "mtu":1200
    }}}}'</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							As the MCO updates machines in each machine config pool, it reboots each node one by one. You must wait until all the nodes are updated. Check the machine config pool status by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get mcp</pre><p class="cluster-admin cluster-admin">
							A successfully updated node has the following status: <code class="literal cluster-admin">UPDATED=true</code>, <code class="literal cluster-admin">UPDATING=false</code>, <code class="literal cluster-admin">DEGRADED=false</code>.
						</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								By default, the MCO updates one machine per pool at a time, causing the total time the migration takes to increase with the size of the cluster.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Confirm the status of the new machine configuration on the hosts:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									To list the machine configuration state and the name of the applied machine configuration, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc describe node | egrep "hostname|machineconfig"</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">kubernetes.io/hostname=master-0
machineconfiguration.openshift.io/currentConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/desiredConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done</pre>

									</p></div><p class="cluster-admin cluster-admin">
									Verify that the following statements are true:
								</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											The value of <code class="literal cluster-admin">machineconfiguration.openshift.io/state</code> field is <code class="literal cluster-admin">Done</code>.
										</li><li class="listitem">
											The value of the <code class="literal cluster-admin">machineconfiguration.openshift.io/currentConfig</code> field is equal to the value of the <code class="literal cluster-admin">machineconfiguration.openshift.io/desiredConfig</code> field.
										</li></ul></div></li><li class="listitem"><p class="simpara">
									To confirm that the machine config is correct, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get machineconfig &lt;config_name&gt; -o yaml | grep ExecStart</pre><p class="cluster-admin cluster-admin">
									where <code class="literal cluster-admin">&lt;config_name&gt;</code> is the name of the machine config from the <code class="literal cluster-admin">machineconfiguration.openshift.io/currentConfig</code> field.
								</p><p class="cluster-admin cluster-admin">
									The machine config must include the following update to the systemd configuration:
								</p><pre class="programlisting language-plain cluster-admin cluster-admin">ExecStart=/usr/local/bin/configure-ovs.sh OVNKubernetes</pre></li><li class="listitem"><p class="simpara">
									If a node is stuck in the <code class="literal cluster-admin">NotReady</code> state, investigate the machine config daemon pod logs and resolve any errors.
								</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="i"><li class="listitem"><p class="simpara">
											To list the pods, enter the following command:
										</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pod -n openshift-machine-config-operator</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
												
<pre class="programlisting language-terminal">NAME                                         READY   STATUS    RESTARTS   AGE
machine-config-controller-75f756f89d-sjp8b   1/1     Running   0          37m
machine-config-daemon-5cf4b                  2/2     Running   0          43h
machine-config-daemon-7wzcd                  2/2     Running   0          43h
machine-config-daemon-fc946                  2/2     Running   0          43h
machine-config-daemon-g2v28                  2/2     Running   0          43h
machine-config-daemon-gcl4f                  2/2     Running   0          43h
machine-config-daemon-l5tnv                  2/2     Running   0          43h
machine-config-operator-79d9c55d5-hth92      1/1     Running   0          37m
machine-config-server-bsc8h                  1/1     Running   0          43h
machine-config-server-hklrm                  1/1     Running   0          43h
machine-config-server-k9rtx                  1/1     Running   0          43h</pre>

											</p></div><p class="cluster-admin cluster-admin">
											The names for the config daemon pods are in the following format: <code class="literal cluster-admin">machine-config-daemon-&lt;seq&gt;</code>. The <code class="literal cluster-admin">&lt;seq&gt;</code> value is a random five character alphanumeric sequence.
										</p></li><li class="listitem"><p class="simpara">
											Display the pod log for the first machine config daemon pod shown in the previous output by enter the following command:
										</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc logs &lt;pod&gt; -n openshift-machine-config-operator</pre><p class="cluster-admin cluster-admin">
											where <code class="literal cluster-admin">pod</code> is the name of a machine config daemon pod.
										</p></li><li class="listitem">
											Resolve any errors in the logs shown by the output from the previous command.
										</li></ol></div></li></ol></div></li><li class="listitem"><p class="simpara">
							To start the migration, configure the OVN-Kubernetes network plugin by using one of the following commands:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
									To specify the network provider without changing the cluster network IP address block, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.config.openshift.io cluster \
  --type='merge' --patch '{ "spec": { "networkType": "OVNKubernetes" } }'</pre></li><li class="listitem"><p class="simpara">
									To specify a different cluster network IP address block, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.config.openshift.io cluster \
  --type='merge' --patch '{
    "spec": {
      "clusterNetwork": [
        {
          "cidr": "&lt;cidr&gt;",
          "hostPrefix": &lt;prefix&gt;
        }
      ],
      "networkType": "OVNKubernetes"
    }
  }'</pre><p class="cluster-admin cluster-admin">
									where <code class="literal cluster-admin">cidr</code> is a CIDR block and <code class="literal cluster-admin">prefix</code> is the slice of the CIDR block apportioned to each node in your cluster. You cannot use any CIDR block that overlaps with the <code class="literal cluster-admin">100.64.0.0/16</code> CIDR block because the OVN-Kubernetes network provider uses this block internally.
								</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
										You cannot change the service network address block during the migration.
									</p></div></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Verify that the Multus daemon set rollout is complete before continuing with subsequent steps:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n openshift-multus rollout status daemonset/multus</pre><p class="cluster-admin cluster-admin">
							The name of the Multus pods is in the form of <code class="literal cluster-admin">multus-&lt;xxxxx&gt;</code> where <code class="literal cluster-admin">&lt;xxxxx&gt;</code> is a random sequence of letters. It might take several moments for the pods to restart.
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">Waiting for daemon set "multus" rollout to finish: 1 out of 6 new pods have been updated...
...
Waiting for daemon set "multus" rollout to finish: 5 of 6 updated pods are available...
daemon set "multus" successfully rolled out</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To complete changing the network plugin, reboot each node in your cluster. You can reboot the nodes in your cluster with either of the following approaches:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
									With the <code class="literal cluster-admin">oc rsh</code> command, you can use a bash script similar to the following:
								</p><pre class="programlisting language-bash cluster-admin cluster-admin">#!/bin/bash
readarray -t POD_NODES &lt;&lt;&lt; "$(oc get pod -n openshift-machine-config-operator -o wide| grep daemon|awk '{print $1" "$7}')"

for i in "${POD_NODES[@]}"
do
  read -r POD NODE &lt;&lt;&lt; "$i"
  until oc rsh -n openshift-machine-config-operator "$POD" chroot /rootfs shutdown -r +1
    do
      echo "cannot reboot node $NODE, retry" &amp;&amp; sleep 3
    done
done</pre></li><li class="listitem"><p class="simpara">
									With the <code class="literal cluster-admin">ssh</code> command, you can use a bash script similar to the following. The script assumes that you have configured sudo to not prompt for a password.
								</p><pre class="programlisting language-bash cluster-admin cluster-admin">#!/bin/bash

for ip in $(oc get nodes  -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}')
do
   echo "reboot node $ip"
   ssh -o StrictHostKeyChecking=no core@$ip sudo shutdown -r -t 3
done</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							Confirm that the migration succeeded:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									To confirm that the network plugin is OVN-Kubernetes, enter the following command. The value of <code class="literal cluster-admin">status.networkType</code> must be <code class="literal cluster-admin">OVNKubernetes</code>.
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get network.config/cluster -o jsonpath='{.status.networkType}{"\n"}'</pre></li><li class="listitem"><p class="simpara">
									To confirm that the cluster nodes are in the <code class="literal cluster-admin">Ready</code> state, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get nodes</pre></li><li class="listitem"><p class="simpara">
									To confirm that your pods are not in an error state, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods --all-namespaces -o wide --sort-by='{.spec.nodeName}'</pre><p class="cluster-admin cluster-admin">
									If pods on a node are in an error state, reboot that node.
								</p></li><li class="listitem"><p class="simpara">
									To confirm that all of the cluster Operators are not in an abnormal state, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get co</pre><p class="cluster-admin cluster-admin">
									The status of every cluster Operator must be the following: <code class="literal cluster-admin">AVAILABLE="True"</code>, <code class="literal cluster-admin">PROGRESSING="False"</code>, <code class="literal cluster-admin">DEGRADED="False"</code>. If a cluster Operator is not available or degraded, check the logs for the cluster Operator for more information.
								</p></li></ol></div></li><li class="listitem"><p class="simpara">
							Complete the following steps only if the migration succeeds and your cluster is in a good state:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									To remove the migration configuration from the CNO configuration object, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": null } }'</pre></li><li class="listitem"><p class="simpara">
									To remove custom configuration for the OpenShift SDN network provider, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "defaultNetwork": { "openshiftSDNConfig": null } } }'</pre></li><li class="listitem"><p class="simpara">
									To remove the OpenShift SDN network provider namespace, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete namespace openshift-sdn</pre></li></ol></div></li></ol></div></section></section><section class="section cluster-admin" id="assigning-egress-ips"><div class="titlepage"><div><div><h2 class="title">27.4. Configuring egress IPs for a project</h2></div></div></div><p class="_abstract _abstract">
				As a cluster administrator, you can configure the OpenShift SDN Container Network Interface (CNI) network plugin to assign one or more egress IP addresses to a project.
			</p><section class="section cluster-admin" id="nw-egress-ips-about_egress-ips"><div class="titlepage"><div><div><h3 class="title">27.4.1. Egress IP address architectural design and implementation</h3></div></div></div><p class="cluster-admin cluster-admin">
					The OpenShift Container Platform egress IP address functionality allows you to ensure that the traffic from one or more pods in one or more namespaces has a consistent source IP address for services outside the cluster network.
				</p><p class="cluster-admin cluster-admin">
					For example, you might have a pod that periodically queries a database that is hosted on a server outside of your cluster. To enforce access requirements for the server, a packet filtering device is configured to allow traffic only from specific IP addresses. To ensure that you can reliably allow access to the server from only that specific pod, you can configure a specific egress IP address for the pod that makes the requests to the server.
				</p><p class="cluster-admin cluster-admin">
					An egress IP address assigned to a namespace is different from an egress router, which is used to send traffic to specific destinations.
				</p><p class="cluster-admin cluster-admin">
					In some cluster configurations, application pods and ingress router pods run on the same node. If you configure an egress IP address for an application project in this scenario, the IP address is not used when you send a request to a route from the application project.
				</p><p class="cluster-admin cluster-admin">
					An egress IP address is implemented as an additional IP address on the primary network interface of a node and must be in the same subnet as the primary IP address of the node. The additional IP address must not be assigned to any other node in the cluster.
				</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						Egress IP addresses must not be configured in any Linux network configuration files, such as <code class="literal cluster-admin">ifcfg-eth0</code>.
					</p></div></div><section class="section cluster-admin" id="nw-egress-ips-platform-support_egress-ips"><div class="titlepage"><div><div><h4 class="title">27.4.1.1. Platform support</h4></div></div></div><p class="cluster-admin cluster-admin">
						Support for the egress IP address functionality on various platforms is summarized in the following table:
					</p><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587127103104" scope="col">Platform</th><th align="left" valign="top" id="idm140587127102016" scope="col">Supported</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587127103104"> <p>
										Bare metal
									</p>
									 </td><td align="left" valign="top" headers="idm140587127102016"> <p>
										Yes
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587127103104"> <p>
										VMware vSphere
									</p>
									 </td><td align="left" valign="top" headers="idm140587127102016"> <p>
										Yes
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587127103104"> <p>
										Red Hat OpenStack Platform (RHOSP)
									</p>
									 </td><td align="left" valign="top" headers="idm140587127102016"> <p>
										Yes
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587127103104"> <p>
										Amazon Web Services (AWS)
									</p>
									 </td><td align="left" valign="top" headers="idm140587127102016"> <p>
										Yes
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587127103104"> <p>
										Google Cloud Platform (GCP)
									</p>
									 </td><td align="left" valign="top" headers="idm140587127102016"> <p>
										Yes
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587127103104"> <p>
										Microsoft Azure
									</p>
									 </td><td align="left" valign="top" headers="idm140587127102016"> <p>
										Yes
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587127103104"> <p>
										IBM Z and IBM® LinuxONE
									</p>
									 </td><td align="left" valign="top" headers="idm140587127102016"> <p>
										Yes
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587127103104"> <p>
										IBM Z and IBM® LinuxONE for Red Hat Enterprise Linux (RHEL) KVM
									</p>
									 </td><td align="left" valign="top" headers="idm140587127102016"> <p>
										Yes
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587127103104"> <p>
										IBM Power
									</p>
									 </td><td align="left" valign="top" headers="idm140587127102016"> <p>
										Yes
									</p>
									 </td></tr></tbody></table></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
							The assignment of egress IP addresses to control plane nodes with the EgressIP feature is not supported on a cluster provisioned on Amazon Web Services (AWS). (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2039656"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">BZ#2039656</span></strong></span></a>)
						</p></div></div></section><section class="section cluster-admin" id="nw-egress-ips-public-cloud-platform-considerations_egress-ips"><div class="titlepage"><div><div><h4 class="title">27.4.1.2. Public cloud platform considerations</h4></div></div></div><p class="cluster-admin cluster-admin">
						For clusters provisioned on public cloud infrastructure, there is a constraint on the absolute number of assignable IP addresses per node. The maximum number of assignable IP addresses per node, or the <span class="emphasis"><em><span class="cluster-admin cluster-admin">IP capacity</span></em></span>, can be described in the following formula:
					</p><pre class="programlisting language-text cluster-admin cluster-admin">IP capacity = public cloud default capacity - sum(current IP assignments)</pre><p class="cluster-admin cluster-admin">
						While the Egress IPs capability manages the IP address capacity per node, it is important to plan for this constraint in your deployments. For example, for a cluster installed on bare-metal infrastructure with 8 nodes you can configure 150 egress IP addresses. However, if a public cloud provider limits IP address capacity to 10 IP addresses per node, the total number of assignable IP addresses is only 80. To achieve the same IP address capacity in this example cloud provider, you would need to allocate 7 additional nodes.
					</p><p class="cluster-admin cluster-admin">
						To confirm the IP capacity and subnets for any node in your public cloud environment, you can enter the <code class="literal cluster-admin">oc get node &lt;node_name&gt; -o yaml</code> command. The <code class="literal cluster-admin">cloud.network.openshift.io/egress-ipconfig</code> annotation includes capacity and subnet information for the node.
					</p><p class="cluster-admin cluster-admin">
						The annotation value is an array with a single object with fields that provide the following information for the primary network interface:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								<code class="literal cluster-admin">interface</code>: Specifies the interface ID on AWS and Azure and the interface name on GCP.
							</li><li class="listitem">
								<code class="literal cluster-admin">ifaddr</code>: Specifies the subnet mask for one or both IP address families.
							</li><li class="listitem">
								<code class="literal cluster-admin">capacity</code>: Specifies the IP address capacity for the node. On AWS, the IP address capacity is provided per IP address family. On Azure and GCP, the IP address capacity includes both IPv4 and IPv6 addresses.
							</li></ul></div><p class="cluster-admin cluster-admin">
						Automatic attachment and detachment of egress IP addresses for traffic between nodes are available. This allows for traffic from many pods in namespaces to have a consistent source IP address to locations outside of the cluster. This also supports OpenShift SDN and OVN-Kubernetes, which is the default networking plugin in Red Hat OpenShift Networking in OpenShift Container Platform 4.13.
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							The RHOSP egress IP address feature creates a Neutron reservation port called <code class="literal cluster-admin">egressip-&lt;IP address&gt;</code>. Using the same RHOSP user as the one used for the OpenShift Container Platform cluster installation, you can assign a floating IP address to this reservation port to have a predictable SNAT address for egress traffic. When an egress IP address on an RHOSP network is moved from one node to another, because of a node failover, for example, the Neutron reservation port is removed and recreated. This means that the floating IP association is lost and you need to manually reassign the floating IP address to the new reservation port.
						</p></div></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							When an RHOSP cluster administrator assigns a floating IP to the reservation port, OpenShift Container Platform cannot delete the reservation port. The <code class="literal cluster-admin">CloudPrivateIPConfig</code> object cannot perform delete and move operations until an RHOSP cluster administrator unassigns the floating IP from the reservation port.
						</p></div></div><p class="cluster-admin cluster-admin">
						The following examples illustrate the annotation from nodes on several public cloud providers. The annotations are indented for readability.
					</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example <code class="literal cluster-admin">cloud.network.openshift.io/egress-ipconfig</code> annotation on AWS</strong></p><p>
							
<pre class="programlisting language-yaml">cloud.network.openshift.io/egress-ipconfig: [
  {
    "interface":"eni-078d267045138e436",
    "ifaddr":{"ipv4":"10.0.128.0/18"},
    "capacity":{"ipv4":14,"ipv6":15}
  }
]</pre>

						</p></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example <code class="literal cluster-admin">cloud.network.openshift.io/egress-ipconfig</code> annotation on GCP</strong></p><p>
							
<pre class="programlisting language-yaml">cloud.network.openshift.io/egress-ipconfig: [
  {
    "interface":"nic0",
    "ifaddr":{"ipv4":"10.0.128.0/18"},
    "capacity":{"ip":14}
  }
]</pre>

						</p></div><p class="cluster-admin cluster-admin">
						The following sections describe the IP address capacity for supported public cloud environments for use in your capacity calculation.
					</p><section class="section cluster-admin" id="nw-egress-ips-capacity-aws_egress-ips"><div class="titlepage"><div><div><h5 class="title">27.4.1.2.1. Amazon Web Services (AWS) IP address capacity limits</h5></div></div></div><p class="cluster-admin cluster-admin">
							On AWS, constraints on IP address assignments depend on the instance type configured. For more information, see <a class="link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI">IP addresses per network interface per instance type</a>
						</p></section><section class="section cluster-admin" id="nw-egress-ips-capacity-gcp_egress-ips"><div class="titlepage"><div><div><h5 class="title">27.4.1.2.2. Google Cloud Platform (GCP) IP address capacity limits</h5></div></div></div><p class="cluster-admin cluster-admin">
							On GCP, the networking model implements additional node IP addresses through IP address aliasing, rather than IP address assignments. However, IP address capacity maps directly to IP aliasing capacity.
						</p><p class="cluster-admin cluster-admin">
							The following capacity limits exist for IP aliasing assignment:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Per node, the maximum number of IP aliases, both IPv4 and IPv6, is 100.
								</li><li class="listitem">
									Per VPC, the maximum number of IP aliases is unspecified, but OpenShift Container Platform scalability testing reveals the maximum to be approximately 15,000.
								</li></ul></div><p class="cluster-admin cluster-admin">
							For more information, see <a class="link" href="https://cloud.google.com/vpc/docs/quota#per_instance">Per instance</a> quotas and <a class="link" href="https://cloud.google.com/vpc/docs/alias-ip">Alias IP ranges overview</a>.
						</p></section><section class="section cluster-admin" id="nw-egress-ips-capacity-azure_egress-ips"><div class="titlepage"><div><div><h5 class="title">27.4.1.2.3. Microsoft Azure IP address capacity limits</h5></div></div></div><p class="cluster-admin cluster-admin">
							On Azure, the following capacity limits exist for IP address assignment:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Per NIC, the maximum number of assignable IP addresses, for both IPv4 and IPv6, is 256.
								</li><li class="listitem">
									Per virtual network, the maximum number of assigned IP addresses cannot exceed 65,536.
								</li></ul></div><p class="cluster-admin cluster-admin">
							For more information, see <a class="link" href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/azure-subscription-service-limits?toc=/azure/virtual-network/toc.json#networking-limits">Networking limits</a>.
						</p></section></section><section class="section cluster-admin" id="nw-egress-ips-limitations_egress-ips"><div class="titlepage"><div><div><h4 class="title">27.4.1.3. Limitations</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following limitations apply when using egress IP addresses with the OpenShift SDN network plugin:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								You cannot use manually assigned and automatically assigned egress IP addresses on the same nodes.
							</li><li class="listitem">
								If you manually assign egress IP addresses from an IP address range, you must not make that range available for automatic IP assignment.
							</li><li class="listitem">
								You cannot share egress IP addresses across multiple namespaces using the OpenShift SDN egress IP address implementation.
							</li></ul></div><p class="cluster-admin cluster-admin">
						If you need to share IP addresses across namespaces, the OVN-Kubernetes network plugin egress IP address implementation allows you to span IP addresses across multiple namespaces.
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							If you use OpenShift SDN in multitenant mode, you cannot use egress IP addresses with any namespace that is joined to another namespace by the projects that are associated with them. For example, if <code class="literal cluster-admin">project1</code> and <code class="literal cluster-admin">project2</code> are joined by running the <code class="literal cluster-admin">oc adm pod-network join-projects --to=project1 project2</code> command, neither project can use an egress IP address. For more information, see <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1645577">BZ#1645577</a>.
						</p></div></div></section><section class="section cluster-admin" id="automatic-manual-assignment-approaches"><div class="titlepage"><div><div><h4 class="title">27.4.1.4. IP address assignment approaches</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can assign egress IP addresses to namespaces by setting the <code class="literal cluster-admin">egressIPs</code> parameter of the <code class="literal cluster-admin">NetNamespace</code> object. After an egress IP address is associated with a project, OpenShift SDN allows you to assign egress IP addresses to hosts in two ways:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								In the <span class="emphasis"><em><span class="cluster-admin cluster-admin">automatically assigned</span></em></span> approach, an egress IP address range is assigned to a node.
							</li><li class="listitem">
								In the <span class="emphasis"><em><span class="cluster-admin cluster-admin">manually assigned</span></em></span> approach, a list of one or more egress IP address is assigned to a node.
							</li></ul></div><p class="cluster-admin cluster-admin">
						Namespaces that request an egress IP address are matched with nodes that can host those egress IP addresses, and then the egress IP addresses are assigned to those nodes. If the <code class="literal cluster-admin">egressIPs</code> parameter is set on a <code class="literal cluster-admin">NetNamespace</code> object, but no node hosts that egress IP address, then egress traffic from the namespace will be dropped.
					</p><p class="cluster-admin cluster-admin">
						High availability of nodes is automatic. If a node that hosts an egress IP address is unreachable and there are nodes that are able to host that egress IP address, then the egress IP address will move to a new node. When the unreachable node comes back online, the egress IP address automatically moves to balance egress IP addresses across nodes.
					</p><section class="section cluster-admin" id="considerations-automatic-egress-ips"><div class="titlepage"><div><div><h5 class="title">27.4.1.4.1. Considerations when using automatically assigned egress IP addresses</h5></div></div></div><p class="cluster-admin cluster-admin">
							When using the automatic assignment approach for egress IP addresses the following considerations apply:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									You set the <code class="literal cluster-admin">egressCIDRs</code> parameter of each node’s <code class="literal cluster-admin">HostSubnet</code> resource to indicate the range of egress IP addresses that can be hosted by a node. OpenShift Container Platform sets the <code class="literal cluster-admin">egressIPs</code> parameter of the <code class="literal cluster-admin">HostSubnet</code> resource based on the IP address range you specify.
								</li></ul></div><p class="cluster-admin cluster-admin">
							If the node hosting the namespace’s egress IP address is unreachable, OpenShift Container Platform will reassign the egress IP address to another node with a compatible egress IP address range. The automatic assignment approach works best for clusters installed in environments with flexibility in associating additional IP addresses with nodes.
						</p></section><section class="section cluster-admin" id="considerations-manual-egress-ips"><div class="titlepage"><div><div><h5 class="title">27.4.1.4.2. Considerations when using manually assigned egress IP addresses</h5></div></div></div><p class="cluster-admin cluster-admin">
							This approach allows you to control which nodes can host an egress IP address.
						</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								If your cluster is installed on public cloud infrastructure, you must ensure that each node that you assign egress IP addresses to has sufficient spare capacity to host the IP addresses. For more information, see "Platform considerations" in a previous section.
							</p></div></div><p class="cluster-admin cluster-admin">
							When using the manual assignment approach for egress IP addresses the following considerations apply:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									You set the <code class="literal cluster-admin">egressIPs</code> parameter of each node’s <code class="literal cluster-admin">HostSubnet</code> resource to indicate the IP addresses that can be hosted by a node.
								</li><li class="listitem">
									Multiple egress IP addresses per namespace are supported.
								</li></ul></div><p class="cluster-admin cluster-admin">
							If a namespace has multiple egress IP addresses and those addresses are hosted on multiple nodes, the following additional considerations apply:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									If a pod is on a node that is hosting an egress IP address, that pod always uses the egress IP address on the node.
								</li><li class="listitem">
									If a pod is not on a node that is hosting an egress IP address, that pod uses an egress IP address at random.
								</li></ul></div></section></section></section><section class="section cluster-admin" id="nw-egress-ips-automatic_egress-ips"><div class="titlepage"><div><div><h3 class="title">27.4.2. Configuring automatically assigned egress IP addresses for a namespace</h3></div></div></div><p class="cluster-admin cluster-admin">
					In OpenShift Container Platform you can enable automatic assignment of an egress IP address for a specific namespace across one or more nodes.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Update the <code class="literal cluster-admin">NetNamespace</code> object with the egress IP address using the following JSON:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin"> $ oc patch netnamespace &lt;project_name&gt; --type=merge -p \
  '{
    "egressIPs": [
      "&lt;ip_address&gt;"
    ]
  }'</pre><p class="cluster-admin cluster-admin">
							where:
						</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;project_name&gt;</code></span></dt><dd>
										Specifies the name of the project.
									</dd><dt><span class="term"><code class="literal cluster-admin">&lt;ip_address&gt;</code></span></dt><dd>
										Specifies one or more egress IP addresses for the <code class="literal cluster-admin">egressIPs</code> array.
									</dd></dl></div><p class="cluster-admin cluster-admin">
							For example, to assign <code class="literal cluster-admin">project1</code> to an IP address of 192.168.1.100 and <code class="literal cluster-admin">project2</code> to an IP address of 192.168.1.101:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch netnamespace project1 --type=merge -p \
  '{"egressIPs": ["192.168.1.100"]}'
$ oc patch netnamespace project2 --type=merge -p \
  '{"egressIPs": ["192.168.1.101"]}'</pre><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								Because OpenShift SDN manages the <code class="literal cluster-admin">NetNamespace</code> object, you can make changes only by modifying the existing <code class="literal cluster-admin">NetNamespace</code> object. Do not create a new <code class="literal cluster-admin">NetNamespace</code> object.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Indicate which nodes can host egress IP addresses by setting the <code class="literal cluster-admin">egressCIDRs</code> parameter for each host using the following JSON:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch hostsubnet &lt;node_name&gt; --type=merge -p \
  '{
    "egressCIDRs": [
      "&lt;ip_address_range&gt;", "&lt;ip_address_range&gt;"
    ]
  }'</pre><p class="cluster-admin cluster-admin">
							where:
						</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;node_name&gt;</code></span></dt><dd>
										Specifies a node name.
									</dd><dt><span class="term"><code class="literal cluster-admin">&lt;ip_address_range&gt;</code></span></dt><dd>
										Specifies an IP address range in CIDR format. You can specify more than one address range for the <code class="literal cluster-admin">egressCIDRs</code> array.
									</dd></dl></div><p class="cluster-admin cluster-admin">
							For example, to set <code class="literal cluster-admin">node1</code> and <code class="literal cluster-admin">node2</code> to host egress IP addresses in the range 192.168.1.0 to 192.168.1.255:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch hostsubnet node1 --type=merge -p \
  '{"egressCIDRs": ["192.168.1.0/24"]}'
$ oc patch hostsubnet node2 --type=merge -p \
  '{"egressCIDRs": ["192.168.1.0/24"]}'</pre><p class="cluster-admin cluster-admin">
							OpenShift Container Platform automatically assigns specific egress IP addresses to available nodes in a balanced way. In this case, it assigns the egress IP address 192.168.1.100 to <code class="literal cluster-admin">node1</code> and the egress IP address 192.168.1.101 to <code class="literal cluster-admin">node2</code> or vice versa.
						</p></li></ol></div></section><section class="section cluster-admin" id="nw-egress-ips-static_egress-ips"><div class="titlepage"><div><div><h3 class="title">27.4.3. Configuring manually assigned egress IP addresses for a namespace</h3></div></div></div><p class="cluster-admin cluster-admin">
					In OpenShift Container Platform you can associate one or more egress IP addresses with a namespace.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Update the <code class="literal cluster-admin">NetNamespace</code> object by specifying the following JSON object with the desired IP addresses:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin"> $ oc patch netnamespace &lt;project_name&gt; --type=merge -p \
  '{
    "egressIPs": [
      "&lt;ip_address&gt;"
    ]
  }'</pre><p class="cluster-admin cluster-admin">
							where:
						</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;project_name&gt;</code></span></dt><dd>
										Specifies the name of the project.
									</dd><dt><span class="term"><code class="literal cluster-admin">&lt;ip_address&gt;</code></span></dt><dd>
										Specifies one or more egress IP addresses for the <code class="literal cluster-admin">egressIPs</code> array.
									</dd></dl></div><p class="cluster-admin cluster-admin">
							For example, to assign the <code class="literal cluster-admin">project1</code> project to the IP addresses <code class="literal cluster-admin">192.168.1.100</code> and <code class="literal cluster-admin">192.168.1.101</code>:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch netnamespace project1 --type=merge \
  -p '{"egressIPs": ["192.168.1.100","192.168.1.101"]}'</pre><p class="cluster-admin cluster-admin">
							To provide high availability, set the <code class="literal cluster-admin">egressIPs</code> value to two or more IP addresses on different nodes. If multiple egress IP addresses are set, then pods use all egress IP addresses roughly equally.
						</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								Because OpenShift SDN manages the <code class="literal cluster-admin">NetNamespace</code> object, you can make changes only by modifying the existing <code class="literal cluster-admin">NetNamespace</code> object. Do not create a new <code class="literal cluster-admin">NetNamespace</code> object.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Manually assign the egress IP address to the node hosts.
						</p><p class="cluster-admin cluster-admin">
							If your cluster is installed on public cloud infrastructure, you must confirm that the node has available IP address capacity.
						</p><p class="cluster-admin cluster-admin">
							Set the <code class="literal cluster-admin">egressIPs</code> parameter on the <code class="literal cluster-admin">HostSubnet</code> object on the node host. Using the following JSON, include as many IP addresses as you want to assign to that node host:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch hostsubnet &lt;node_name&gt; --type=merge -p \
  '{
    "egressIPs": [
      "&lt;ip_address&gt;",
      "&lt;ip_address&gt;"
      ]
  }'</pre><p class="cluster-admin cluster-admin">
							where:
						</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;node_name&gt;</code></span></dt><dd>
										Specifies a node name.
									</dd><dt><span class="term"><code class="literal cluster-admin">&lt;ip_address&gt;</code></span></dt><dd>
										Specifies an IP address. You can specify more than one IP address for the <code class="literal cluster-admin">egressIPs</code> array.
									</dd></dl></div><p class="cluster-admin cluster-admin">
							For example, to specify that <code class="literal cluster-admin">node1</code> should have the egress IPs <code class="literal cluster-admin">192.168.1.100</code>, <code class="literal cluster-admin">192.168.1.101</code>, and <code class="literal cluster-admin">192.168.1.102</code>:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch hostsubnet node1 --type=merge -p \
  '{"egressIPs": ["192.168.1.100", "192.168.1.101", "192.168.1.102"]}'</pre><p class="cluster-admin cluster-admin">
							In the previous example, all egress traffic for <code class="literal cluster-admin">project1</code> will be routed to the node hosting the specified egress IP, and then connected through Network Address Translation (NAT) to that IP address.
						</p></li></ol></div></section><section class="section cluster-admin" id="egress-ips-additional-resources"><div class="titlepage"><div><div><h3 class="title">27.4.4. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							If you are configuring manual egress IP address assignment, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-egress-ips-public-cloud-platform-considerations_egress-ips">Platform considerations</a> for information about IP capacity planning.
						</li></ul></div></section></section><section class="section cluster-admin" id="configuring-egress-firewall"><div class="titlepage"><div><div><h2 class="title">27.5. Configuring an egress firewall for a project</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can create an egress firewall for a project that restricts egress traffic leaving your OpenShift Container Platform cluster.
			</p><section class="section cluster-admin" id="nw-egressnetworkpolicy-about_openshift-sdn-egress-firewall"><div class="titlepage"><div><div><h3 class="title">27.5.1. How an egress firewall works in a project</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can use an <span class="emphasis"><em><span class="cluster-admin cluster-admin">egress firewall</span></em></span> to limit the external hosts that some or all pods can access from within the cluster. An egress firewall supports the following scenarios:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							A pod can only connect to internal hosts and cannot initiate connections to the public internet.
						</li><li class="listitem">
							A pod can only connect to the public internet and cannot initiate connections to internal hosts that are outside the OpenShift Container Platform cluster.
						</li><li class="listitem">
							A pod cannot reach specified internal subnets or hosts outside the OpenShift Container Platform cluster.
						</li><li class="listitem">
							A pod can connect to only specific external hosts.
						</li></ul></div><p class="cluster-admin cluster-admin">
					For example, you can allow one project access to a specified IP range but deny the same access to a different project. Or you can restrict application developers from updating from Python pip mirrors, and force updates to come only from approved sources.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						Egress firewall does not apply to the host network namespace. Pods with host networking enabled are unaffected by egress firewall rules.
					</p></div></div><p class="cluster-admin cluster-admin">
					You configure an egress firewall policy by creating an EgressNetworkPolicy custom resource (CR) object. The egress firewall matches network traffic that meets any of the following criteria:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							An IP address range in CIDR format
						</li><li class="listitem">
							A DNS name that resolves to an IP address
						</li></ul></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						If your egress firewall includes a deny rule for <code class="literal cluster-admin">0.0.0.0/0</code>, access to your OpenShift Container Platform API servers is blocked. You must either add allow rules for each IP address or use the <code class="literal cluster-admin">nodeSelector</code> type allow rule in your egress policy rules to connect to API servers.
					</p><p class="cluster-admin cluster-admin">
						The following example illustrates the order of the egress firewall rules necessary to ensure API server access:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: network.openshift.io/v1
kind: EgressNetworkPolicy
metadata:
  name: default
  namespace: &lt;namespace&gt; <span id="CO159-1"><!--Empty--></span><span class="callout">1</span>
spec:
  egress:
  - to:
      cidrSelector: &lt;api_server_address_range&gt; <span id="CO159-2"><!--Empty--></span><span class="callout">2</span>
    type: Allow
# ...
  - to:
      cidrSelector: 0.0.0.0/0 <span id="CO159-3"><!--Empty--></span><span class="callout">3</span>
    type: Deny</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO159-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The namespace for the egress firewall.
							</div></dd><dt><a href="#CO159-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The IP address range that includes your OpenShift Container Platform API servers.
							</div></dd><dt><a href="#CO159-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								A global deny rule prevents access to the OpenShift Container Platform API servers.
							</div></dd></dl></div><p class="cluster-admin cluster-admin">
						To find the IP address for your API servers, run <code class="literal cluster-admin">oc get ep kubernetes -n default</code>.
					</p><p class="cluster-admin cluster-admin">
						For more information, see <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1988324">BZ#1988324</a>.
					</p></div></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						You must have OpenShift SDN configured to use either the network policy or multitenant mode to configure an egress firewall.
					</p><p class="cluster-admin cluster-admin">
						If you use network policy mode, an egress firewall is compatible with only one policy per namespace and will not work with projects that share a network, such as global projects.
					</p></div></div><div class="admonition warning cluster-admin"><div class="admonition_header">Warning</div><div><p class="cluster-admin cluster-admin">
						Egress firewall rules do not apply to traffic that goes through routers. Any user with permission to create a Route CR object can bypass egress firewall policy rules by creating a route that points to a forbidden destination.
					</p></div></div><section class="section cluster-admin" id="limitations-of-an-egress-firewall_openshift-sdn-egress-firewall"><div class="titlepage"><div><div><h4 class="title">27.5.1.1. Limitations of an egress firewall</h4></div></div></div><p class="cluster-admin cluster-admin">
						An egress firewall has the following limitations:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								No project can have more than one EgressNetworkPolicy object.
							</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
									The creation of more than one EgressNetworkPolicy object is allowed, however it should not be done. When you create more than one EgressNetworkPolicy object, the following message is returned: <code class="literal cluster-admin">dropping all rules</code>. In actuality, all external traffic is dropped, which can cause security risks for your organization.
								</p></div></div></li><li class="listitem">
								A maximum of one EgressNetworkPolicy object with a maximum of 1,000 rules can be defined per project.
							</li><li class="listitem">
								The <code class="literal cluster-admin">default</code> project cannot use an egress firewall.
							</li><li class="listitem"><p class="simpara">
								When using the OpenShift SDN network plugin in multitenant mode, the following limitations apply:
							</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem">
										Global projects cannot use an egress firewall. You can make a project global by using the <code class="literal cluster-admin">oc adm pod-network make-projects-global</code> command.
									</li><li class="listitem">
										Projects merged by using the <code class="literal cluster-admin">oc adm pod-network join-projects</code> command cannot use an egress firewall in any of the joined projects.
									</li></ul></div></li></ul></div><p class="cluster-admin cluster-admin">
						Violating any of these restrictions results in a broken egress firewall for the project. Consequently, all external network traffic is dropped, which can cause security risks for your organization.
					</p><p class="cluster-admin cluster-admin">
						An Egress Firewall resource can be created in the <code class="literal cluster-admin">kube-node-lease</code>, <code class="literal cluster-admin">kube-public</code>, <code class="literal cluster-admin">kube-system</code>, <code class="literal cluster-admin">openshift</code> and <code class="literal cluster-admin">openshift-</code> projects.
					</p></section><section class="section cluster-admin" id="policy-rule-order_openshift-sdn-egress-firewall"><div class="titlepage"><div><div><h4 class="title">27.5.1.2. Matching order for egress firewall policy rules</h4></div></div></div><p class="cluster-admin cluster-admin">
						The egress firewall policy rules are evaluated in the order that they are defined, from first to last. The first rule that matches an egress connection from a pod applies. Any subsequent rules are ignored for that connection.
					</p></section><section class="section cluster-admin" id="domain-name-server-resolution_openshift-sdn-egress-firewall"><div class="titlepage"><div><div><h4 class="title">27.5.1.3. How Domain Name Server (DNS) resolution works</h4></div></div></div><p class="cluster-admin cluster-admin">
						If you use DNS names in any of your egress firewall policy rules, proper resolution of the domain names is subject to the following restrictions:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Domain name updates are polled based on a time-to-live (TTL) duration. By default, the duration is 30 seconds. When the egress firewall controller queries the local name servers for a domain name, if the response includes a TTL that is less than 30 seconds, the controller sets the duration to the returned value. If the TTL in the response is greater than 30 minutes, the controller sets the duration to 30 minutes. If the TTL is between 30 seconds and 30 minutes, the controller ignores the value and sets the duration to 30 seconds.
							</li><li class="listitem">
								The pod must resolve the domain from the same local name servers when necessary. Otherwise the IP addresses for the domain known by the egress firewall controller and the pod can be different. If the IP addresses for a hostname differ, the egress firewall might not be enforced consistently.
							</li><li class="listitem">
								Because the egress firewall controller and pods asynchronously poll the same local name server, the pod might obtain the updated IP address before the egress controller does, which causes a race condition. Due to this current limitation, domain name usage in EgressNetworkPolicy objects is only recommended for domains with infrequent IP address changes.
							</li></ul></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							The egress firewall always allows pods access to the external interface of the node that the pod is on for DNS resolution.
						</p><p class="cluster-admin cluster-admin">
							If you use domain names in your egress firewall policy and your DNS resolution is not handled by a DNS server on the local node, then you must add egress firewall rules that allow access to your DNS server’s IP addresses. if you are using domain names in your pods.
						</p></div></div></section></section><section class="section cluster-admin" id="nw-egressnetworkpolicy-object_openshift-sdn-egress-firewall"><div class="titlepage"><div><div><h3 class="title">27.5.2. EgressNetworkPolicy custom resource (CR) object</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can define one or more rules for an egress firewall. A rule is either an <code class="literal cluster-admin">Allow</code> rule or a <code class="literal cluster-admin">Deny</code> rule, with a specification for the traffic that the rule applies to.
				</p><p class="cluster-admin cluster-admin">
					The following YAML describes an EgressNetworkPolicy CR object:
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>EgressNetworkPolicy object</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: network.openshift.io/v1
kind: EgressNetworkPolicy
metadata:
  name: &lt;name&gt; <span id="CO160-1"><!--Empty--></span><span class="callout">1</span>
spec:
  egress: <span id="CO160-2"><!--Empty--></span><span class="callout">2</span>
    ...</pre>

					</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO160-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							A name for your egress firewall policy.
						</div></dd><dt><a href="#CO160-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							A collection of one or more egress network policy rules as described in the following section.
						</div></dd></dl></div><section class="section cluster-admin" id="egressnetworkpolicy-rules_openshift-sdn-egress-firewall"><div class="titlepage"><div><div><h4 class="title">27.5.2.1. EgressNetworkPolicy rules</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following YAML describes an egress firewall rule object. The user can select either an IP address range in CIDR format, a domain name, or use the <code class="literal cluster-admin">nodeSelector</code> to allow or deny egress traffic. The <code class="literal cluster-admin">egress</code> stanza expects an array of one or more objects.
					</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Egress policy rule stanza</strong></p><p>
							
<pre class="programlisting language-yaml">egress:
- type: &lt;type&gt; <span id="CO161-1"><!--Empty--></span><span class="callout">1</span>
  to: <span id="CO161-2"><!--Empty--></span><span class="callout">2</span>
    cidrSelector: &lt;cidr&gt; <span id="CO161-3"><!--Empty--></span><span class="callout">3</span>
    dnsName: &lt;dns_name&gt; <span id="CO161-4"><!--Empty--></span><span class="callout">4</span></pre>

						</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO161-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The type of rule. The value must be either <code class="literal cluster-admin">Allow</code> or <code class="literal cluster-admin">Deny</code>.
							</div></dd><dt><a href="#CO161-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								A stanza describing an egress traffic match rule. A value for either the <code class="literal cluster-admin">cidrSelector</code> field or the <code class="literal cluster-admin">dnsName</code> field for the rule. You cannot use both fields in the same rule.
							</div></dd><dt><a href="#CO161-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								An IP address range in CIDR format.
							</div></dd><dt><a href="#CO161-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								A domain name.
							</div></dd></dl></div></section><section class="section cluster-admin" id="egressnetworkpolicy-example_openshift-sdn-egress-firewall"><div class="titlepage"><div><div><h4 class="title">27.5.2.2. Example EgressNetworkPolicy CR objects</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following example defines several egress firewall policy rules:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: network.openshift.io/v1
kind: EgressNetworkPolicy
metadata:
  name: default
spec:
  egress: <span id="CO162-1"><!--Empty--></span><span class="callout">1</span>
  - type: Allow
    to:
      cidrSelector: 1.2.3.0/24
  - type: Allow
    to:
      dnsName: www.example.com
  - type: Deny
    to:
      cidrSelector: 0.0.0.0/0</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO162-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								A collection of egress firewall policy rule objects.
							</div></dd></dl></div></section></section><section class="section cluster-admin" id="nw-networkpolicy-create_openshift-sdn-egress-firewall"><div class="titlepage"><div><div><h3 class="title">27.5.3. Creating an egress firewall policy object</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can create an egress firewall policy object for a project.
				</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						If the project already has an EgressNetworkPolicy object defined, you must edit the existing policy to make changes to the egress firewall rules.
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							A cluster that uses the OpenShift SDN network plugin.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You must log in to the cluster as a cluster administrator.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a policy rule:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem">
									Create a <code class="literal cluster-admin">&lt;policy_name&gt;.yaml</code> file where <code class="literal cluster-admin">&lt;policy_name&gt;</code> describes the egress policy rules.
								</li><li class="listitem">
									In the file you created, define an egress policy object.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Enter the following command to create the policy object. Replace <code class="literal cluster-admin">&lt;policy_name&gt;</code> with the name of the policy and <code class="literal cluster-admin">&lt;project&gt;</code> with the project that the rule applies to.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f &lt;policy_name&gt;.yaml -n &lt;project&gt;</pre><p class="cluster-admin cluster-admin">
							In the following example, a new EgressNetworkPolicy object is created in a project named <code class="literal cluster-admin">project1</code>:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f default.yaml -n project1</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">egressnetworkpolicy.network.openshift.io/v1 created</pre>

							</p></div></li><li class="listitem">
							Optional: Save the <code class="literal cluster-admin">&lt;policy_name&gt;.yaml</code> file so that you can make changes later.
						</li></ol></div></section></section><section class="section cluster-admin" id="openshift-sdn-viewing-egress-firewall"><div class="titlepage"><div><div><h2 class="title">27.6. Editing an egress firewall for a project</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can modify network traffic rules for an existing egress firewall.
			</p><section class="section cluster-admin" id="nw-egressnetworkpolicy-view_openshift-sdn-viewing-egress-firewall"><div class="titlepage"><div><div><h3 class="title">27.6.1. Viewing an EgressNetworkPolicy object</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can view an EgressNetworkPolicy object in your cluster.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							A cluster using the OpenShift SDN network plugin.
						</li><li class="listitem">
							Install the OpenShift Command-line Interface (CLI), commonly known as <code class="literal cluster-admin">oc</code>.
						</li><li class="listitem">
							You must log in to the cluster.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Optional: To view the names of the EgressNetworkPolicy objects defined in your cluster, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get egressnetworkpolicy --all-namespaces</pre></li><li class="listitem"><p class="simpara">
							To inspect a policy, enter the following command. Replace <code class="literal cluster-admin">&lt;policy_name&gt;</code> with the name of the policy to inspect.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc describe egressnetworkpolicy &lt;policy_name&gt;</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Name:		default
Namespace:	project1
Created:	20 minutes ago
Labels:		&lt;none&gt;
Annotations:	&lt;none&gt;
Rule:		Allow to 1.2.3.0/24
Rule:		Allow to www.example.com
Rule:		Deny to 0.0.0.0/0</pre>

							</p></div></li></ol></div></section></section><section class="section cluster-admin" id="editing-egress-firewall"><div class="titlepage"><div><div><h2 class="title">27.7. Editing an egress firewall for a project</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can modify network traffic rules for an existing egress firewall.
			</p><section class="section cluster-admin" id="nw-egressnetworkpolicy-edit_openshift-sdn-egress-firewall"><div class="titlepage"><div><div><h3 class="title">27.7.1. Editing an EgressNetworkPolicy object</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can update the egress firewall for a project.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							A cluster using the OpenShift SDN network plugin.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You must log in to the cluster as a cluster administrator.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Find the name of the EgressNetworkPolicy object for the project. Replace <code class="literal cluster-admin">&lt;project&gt;</code> with the name of the project.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get -n &lt;project&gt; egressnetworkpolicy</pre></li><li class="listitem"><p class="simpara">
							Optional: If you did not save a copy of the EgressNetworkPolicy object when you created the egress network firewall, enter the following command to create a copy.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get -n &lt;project&gt; egressnetworkpolicy &lt;name&gt; -o yaml &gt; &lt;filename&gt;.yaml</pre><p class="cluster-admin cluster-admin">
							Replace <code class="literal cluster-admin">&lt;project&gt;</code> with the name of the project. Replace <code class="literal cluster-admin">&lt;name&gt;</code> with the name of the object. Replace <code class="literal cluster-admin">&lt;filename&gt;</code> with the name of the file to save the YAML to.
						</p></li><li class="listitem"><p class="simpara">
							After making changes to the policy rules, enter the following command to replace the EgressNetworkPolicy object. Replace <code class="literal cluster-admin">&lt;filename&gt;</code> with the name of the file containing the updated EgressNetworkPolicy object.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc replace -f &lt;filename&gt;.yaml</pre></li></ol></div></section></section><section class="section cluster-admin" id="removing-egress-firewall"><div class="titlepage"><div><div><h2 class="title">27.8. Removing an egress firewall from a project</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can remove an egress firewall from a project to remove all restrictions on network traffic from the project that leaves the OpenShift Container Platform cluster.
			</p><section class="section cluster-admin" id="nw-egressnetworkpolicy-delete_openshift-sdn-egress-firewall"><div class="titlepage"><div><div><h3 class="title">27.8.1. Removing an EgressNetworkPolicy object</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can remove an egress firewall from a project.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							A cluster using the OpenShift SDN network plugin.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You must log in to the cluster as a cluster administrator.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Find the name of the EgressNetworkPolicy object for the project. Replace <code class="literal cluster-admin">&lt;project&gt;</code> with the name of the project.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get -n &lt;project&gt; egressnetworkpolicy</pre></li><li class="listitem"><p class="simpara">
							Enter the following command to delete the EgressNetworkPolicy object. Replace <code class="literal cluster-admin">&lt;project&gt;</code> with the name of the project and <code class="literal cluster-admin">&lt;name&gt;</code> with the name of the object.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete -n &lt;project&gt; egressnetworkpolicy &lt;name&gt;</pre></li></ol></div></section></section><section class="section cluster-admin" id="using-an-egress-router"><div class="titlepage"><div><div><h2 class="title">27.9. Considerations for the use of an egress router pod</h2></div></div></div><section class="section cluster-admin" id="nw-egress-router-about_using-an-egress-router"><div class="titlepage"><div><div><h3 class="title">27.9.1. About an egress router pod</h3></div></div></div><p class="cluster-admin cluster-admin">
					The OpenShift Container Platform egress router pod redirects traffic to a specified remote server from a private source IP address that is not used for any other purpose. An egress router pod can send network traffic to servers that are set up to allow access only from specific IP addresses.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						The egress router pod is not intended for every outgoing connection. Creating large numbers of egress router pods can exceed the limits of your network hardware. For example, creating an egress router pod for every project or application could exceed the number of local MAC addresses that the network interface can handle before reverting to filtering MAC addresses in software.
					</p></div></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						The egress router image is not compatible with Amazon AWS, Azure Cloud, or any other cloud platform that does not support layer 2 manipulations due to their incompatibility with macvlan traffic.
					</p></div></div><section class="section cluster-admin" id="nw-egress-router-about-modes_using-an-egress-router"><div class="titlepage"><div><div><h4 class="title">27.9.1.1. Egress router modes</h4></div></div></div><p class="cluster-admin cluster-admin">
						In <span class="emphasis"><em><span class="cluster-admin cluster-admin">redirect mode</span></em></span>, an egress router pod configures <code class="literal cluster-admin">iptables</code> rules to redirect traffic from its own IP address to one or more destination IP addresses. Client pods that need to use the reserved source IP address must be configured to access the service for the egress router rather than connecting directly to the destination IP. You can access the destination service and port from the application pod by using the <code class="literal cluster-admin">curl</code> command. For example:
					</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ curl &lt;router_service_IP&gt; &lt;port&gt;</pre><p class="cluster-admin cluster-admin">
						In <span class="emphasis"><em><span class="cluster-admin cluster-admin">HTTP proxy mode</span></em></span>, an egress router pod runs as an HTTP proxy on port <code class="literal cluster-admin">8080</code>. This mode only works for clients that are connecting to HTTP-based or HTTPS-based services, but usually requires fewer changes to the client pods to get them to work. Many programs can be told to use an HTTP proxy by setting an environment variable.
					</p><p class="cluster-admin cluster-admin">
						In <span class="emphasis"><em><span class="cluster-admin cluster-admin">DNS proxy mode</span></em></span>, an egress router pod runs as a DNS proxy for TCP-based services from its own IP address to one or more destination IP addresses. To make use of the reserved, source IP address, client pods must be modified to connect to the egress router pod rather than connecting directly to the destination IP address. This modification ensures that external destinations treat traffic as though it were coming from a known source.
					</p><p class="cluster-admin cluster-admin">
						Redirect mode works for all services except for HTTP and HTTPS. For HTTP and HTTPS services, use HTTP proxy mode. For TCP-based services with IP addresses or domain names, use DNS proxy mode.
					</p></section><section class="section cluster-admin" id="nw-egress-router-about-router-pod-implementation_using-an-egress-router"><div class="titlepage"><div><div><h4 class="title">27.9.1.2. Egress router pod implementation</h4></div></div></div><p class="cluster-admin cluster-admin">
						The egress router pod setup is performed by an initialization container. That container runs in a privileged context so that it can configure the macvlan interface and set up <code class="literal cluster-admin">iptables</code> rules. After the initialization container finishes setting up the <code class="literal cluster-admin">iptables</code> rules, it exits. Next the egress router pod executes the container to handle the egress router traffic. The image used varies depending on the egress router mode.
					</p><p class="cluster-admin cluster-admin">
						The environment variables determine which addresses the egress-router image uses. The image configures the macvlan interface to use <code class="literal cluster-admin">EGRESS_SOURCE</code> as its IP address, with <code class="literal cluster-admin">EGRESS_GATEWAY</code> as the IP address for the gateway.
					</p><p class="cluster-admin cluster-admin">
						Network Address Translation (NAT) rules are set up so that connections to the cluster IP address of the pod on any TCP or UDP port are redirected to the same port on IP address specified by the <code class="literal cluster-admin">EGRESS_DESTINATION</code> variable.
					</p><p class="cluster-admin cluster-admin">
						If only some of the nodes in your cluster are capable of claiming the specified source IP address and using the specified gateway, you can specify a <code class="literal cluster-admin">nodeName</code> or <code class="literal cluster-admin">nodeSelector</code> to identify which nodes are acceptable.
					</p></section><section class="section cluster-admin" id="nw-egress-router-about-deployments_using-an-egress-router"><div class="titlepage"><div><div><h4 class="title">27.9.1.3. Deployment considerations</h4></div></div></div><p class="cluster-admin cluster-admin">
						An egress router pod adds an additional IP address and MAC address to the primary network interface of the node. As a result, you might need to configure your hypervisor or cloud provider to allow the additional address.
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Red Hat OpenStack Platform (RHOSP)</span></dt><dd><p class="simpara">
									If you deploy OpenShift Container Platform on RHOSP, you must allow traffic from the IP and MAC addresses of the egress router pod on your OpenStack environment. If you do not allow the traffic, then <a class="link" href="https://access.redhat.com/solutions/2803331">communication will fail</a>:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ openstack port set --allowed-address \
  ip_address=&lt;ip_address&gt;,mac_address=&lt;mac_address&gt; &lt;neutron_port_uuid&gt;</pre></dd><dt><span class="term">Red Hat Virtualization (RHV)</span></dt><dd>
									If you are using <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html/administration_guide/chap-logical_networks#Explanation_of_Settings_in_the_VM_Interface_Profile_Window">RHV</a>, you must select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">No Network Filter</span></strong></span> for the Virtual network interface controller (vNIC).
								</dd><dt><span class="term">VMware vSphere</span></dt><dd>
									If you are using VMware vSphere, see the <a class="link" href="https://docs.vmware.com/en/VMware-vSphere/6.0/com.vmware.vsphere.security.doc/GUID-3507432E-AFEA-4B6B-B404-17A020575358.html">VMware documentation for securing vSphere standard switches</a>. View and change VMware vSphere default settings by selecting the host virtual switch from the vSphere Web Client.
								</dd></dl></div><p class="cluster-admin cluster-admin">
						Specifically, ensure that the following are enabled:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								<a class="link" href="https://docs.vmware.com/en/VMware-vSphere/6.0/com.vmware.vsphere.security.doc/GUID-942BD3AA-731B-4A05-8196-66F2B4BF1ACB.html">MAC Address Changes</a>
							</li><li class="listitem">
								<a class="link" href="https://docs.vmware.com/en/VMware-vSphere/6.0/com.vmware.vsphere.security.doc/GUID-7DC6486F-5400-44DF-8A62-6273798A2F80.html">Forged Transits</a>
							</li><li class="listitem">
								<a class="link" href="https://docs.vmware.com/en/VMware-vSphere/6.0/com.vmware.vsphere.security.doc/GUID-92F3AB1F-B4C5-4F25-A010-8820D7250350.html">Promiscuous Mode Operation</a>
							</li></ul></div></section><section class="section cluster-admin" id="nw-egress-router-about-failover_using-an-egress-router"><div class="titlepage"><div><div><h4 class="title">27.9.1.4. Failover configuration</h4></div></div></div><p class="cluster-admin cluster-admin">
						To avoid downtime, you can deploy an egress router pod with a <code class="literal cluster-admin">Deployment</code> resource, as in the following example. To create a new <code class="literal cluster-admin">Service</code> object for the example deployment, use the <code class="literal cluster-admin">oc expose deployment/egress-demo-controller</code> command.
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: apps/v1
kind: Deployment
metadata:
  name: egress-demo-controller
spec:
  replicas: 1 <span id="CO163-1"><!--Empty--></span><span class="callout">1</span>
  selector:
    matchLabels:
      name: egress-router
  template:
    metadata:
      name: egress-router
      labels:
        name: egress-router
      annotations:
        pod.network.openshift.io/assign-macvlan: "true"
    spec: <span id="CO163-2"><!--Empty--></span><span class="callout">2</span>
      initContainers:
        ...
      containers:
        ...</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO163-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Ensure that replicas is set to <code class="literal cluster-admin">1</code>, because only one pod can use a given egress source IP address at any time. This means that only a single copy of the router runs on a node.
							</div></dd><dt><a href="#CO163-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specify the <code class="literal cluster-admin">Pod</code> object template for the egress router pod.
							</div></dd></dl></div></section></section><section class="section _additional-resources" id="using-an-egress-router-additional-resources"><div class="titlepage"><div><div><h3 class="title">27.9.2. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#deploying-egress-router-layer3-redirection">Deploying an egress router in redirection mode</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#deploying-egress-router-http-redirection">Deploying an egress router in HTTP proxy mode</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#deploying-egress-router-dns-redirection">Deploying an egress router in DNS proxy mode</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="deploying-egress-router-layer3-redirection"><div class="titlepage"><div><div><h2 class="title">27.10. Deploying an egress router pod in redirect mode</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can deploy an egress router pod that is configured to redirect traffic to specified destination IP addresses.
			</p><section class="section cluster-admin" id="nw-egress-router-pod_deploying-egress-router-layer3-redirection"><div class="titlepage"><div><div><h3 class="title">27.10.1. Egress router pod specification for redirect mode</h3></div></div></div><p class="cluster-admin cluster-admin">
					Define the configuration for an egress router pod in the <code class="literal cluster-admin">Pod</code> object. The following YAML describes the fields for the configuration of an egress router pod in redirect mode:
				</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Pod
metadata:
  name: egress-1
  labels:
    name: egress-1
  annotations:
    pod.network.openshift.io/assign-macvlan: "true" <span id="CO164-1"><!--Empty--></span><span class="callout">1</span>
spec:
  initContainers:
  - name: egress-router
    image: registry.redhat.io/openshift4/ose-egress-router
    securityContext:
      privileged: true
    env:
    - name: EGRESS_SOURCE <span id="CO164-2"><!--Empty--></span><span class="callout">2</span>
      value: &lt;egress_router&gt;
    - name: EGRESS_GATEWAY <span id="CO164-3"><!--Empty--></span><span class="callout">3</span>
      value: &lt;egress_gateway&gt;
    - name: EGRESS_DESTINATION <span id="CO164-4"><!--Empty--></span><span class="callout">4</span>
      value: &lt;egress_destination&gt;
    - name: EGRESS_ROUTER_MODE
      value: init
  containers:
  - name: egress-router-wait
    image: registry.redhat.io/openshift4/ose-pod</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO164-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The annotation tells OpenShift Container Platform to create a macvlan network interface on the primary network interface controller (NIC) and move that macvlan interface into the pod’s network namespace. You must include the quotation marks around the <code class="literal cluster-admin">"true"</code> value. To have OpenShift Container Platform create the macvlan interface on a different NIC interface, set the annotation value to the name of that interface. For example, <code class="literal cluster-admin">eth1</code>.
						</div></dd><dt><a href="#CO164-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							IP address from the physical network that the node is on that is reserved for use by the egress router pod. Optional: You can include the subnet length, the <code class="literal cluster-admin">/24</code> suffix, so that a proper route to the local subnet is set. If you do not specify a subnet length, then the egress router can access only the host specified with the <code class="literal cluster-admin">EGRESS_GATEWAY</code> variable and no other hosts on the subnet.
						</div></dd><dt><a href="#CO164-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Same value as the default gateway used by the node.
						</div></dd><dt><a href="#CO164-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							External server to direct traffic to. Using this example, connections to the pod are redirected to <code class="literal cluster-admin">203.0.113.25</code>, with a source IP address of <code class="literal cluster-admin">192.168.12.99</code>.
						</div></dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example egress router pod specification</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: egress-multi
  labels:
    name: egress-multi
  annotations:
    pod.network.openshift.io/assign-macvlan: "true"
spec:
  initContainers:
  - name: egress-router
    image: registry.redhat.io/openshift4/ose-egress-router
    securityContext:
      privileged: true
    env:
    - name: EGRESS_SOURCE
      value: 192.168.12.99/24
    - name: EGRESS_GATEWAY
      value: 192.168.12.1
    - name: EGRESS_DESTINATION
      value: |
        80   tcp 203.0.113.25
        8080 tcp 203.0.113.26 80
        8443 tcp 203.0.113.26 443
        203.0.113.27
    - name: EGRESS_ROUTER_MODE
      value: init
  containers:
  - name: egress-router-wait
    image: registry.redhat.io/openshift4/ose-pod</pre>

					</p></div></section><section class="section cluster-admin" id="nw-egress-router-dest-var_deploying-egress-router-layer3-redirection"><div class="titlepage"><div><div><h3 class="title">27.10.2. Egress destination configuration format</h3></div></div></div><p class="cluster-admin cluster-admin">
					When an egress router pod is deployed in redirect mode, you can specify redirection rules by using one or more of the following formats:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<code class="literal cluster-admin">&lt;port&gt; &lt;protocol&gt; &lt;ip_address&gt;</code> - Incoming connections to the given <code class="literal cluster-admin">&lt;port&gt;</code> should be redirected to the same port on the given <code class="literal cluster-admin">&lt;ip_address&gt;</code>. <code class="literal cluster-admin">&lt;protocol&gt;</code> is either <code class="literal cluster-admin">tcp</code> or <code class="literal cluster-admin">udp</code>.
						</li><li class="listitem">
							<code class="literal cluster-admin">&lt;port&gt; &lt;protocol&gt; &lt;ip_address&gt; &lt;remote_port&gt;</code> - As above, except that the connection is redirected to a different <code class="literal cluster-admin">&lt;remote_port&gt;</code> on <code class="literal cluster-admin">&lt;ip_address&gt;</code>.
						</li><li class="listitem">
							<code class="literal cluster-admin">&lt;ip_address&gt;</code> - If the last line is a single IP address, then any connections on any other port will be redirected to the corresponding port on that IP address. If there is no fallback IP address then connections on other ports are rejected.
						</li></ul></div><p class="cluster-admin cluster-admin">
					In the example that follows several rules are defined:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							The first line redirects traffic from local port <code class="literal cluster-admin">80</code> to port <code class="literal cluster-admin">80</code> on <code class="literal cluster-admin">203.0.113.25</code>.
						</li><li class="listitem">
							The second and third lines redirect local ports <code class="literal cluster-admin">8080</code> and <code class="literal cluster-admin">8443</code> to remote ports <code class="literal cluster-admin">80</code> and <code class="literal cluster-admin">443</code> on <code class="literal cluster-admin">203.0.113.26</code>.
						</li><li class="listitem">
							The last line matches traffic for any ports not specified in the previous rules.
						</li></ul></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example configuration</strong></p><p>
						
<pre class="programlisting language-text">80   tcp 203.0.113.25
8080 tcp 203.0.113.26 80
8443 tcp 203.0.113.26 443
203.0.113.27</pre>

					</p></div></section><section class="section cluster-admin" id="nw-egress-router-redirect-mode_deploying-egress-router-layer3-redirection"><div class="titlepage"><div><div><h3 class="title">27.10.3. Deploying an egress router pod in redirect mode</h3></div></div></div><p class="cluster-admin cluster-admin">
					In <span class="emphasis"><em><span class="cluster-admin cluster-admin">redirect mode</span></em></span>, an egress router pod sets up iptables rules to redirect traffic from its own IP address to one or more destination IP addresses. Client pods that need to use the reserved source IP address must be configured to access the service for the egress router rather than connecting directly to the destination IP. You can access the destination service and port from the application pod by using the <code class="literal cluster-admin">curl</code> command. For example:
				</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ curl &lt;router_service_IP&gt; &lt;port&gt;</pre><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Create an egress router pod.
						</li><li class="listitem"><p class="simpara">
							To ensure that other pods can find the IP address of the egress router pod, create a service to point to the egress router pod, as in the following example:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Service
metadata:
  name: egress-1
spec:
  ports:
  - name: http
    port: 80
  - name: https
    port: 443
  type: ClusterIP
  selector:
    name: egress-1</pre><p class="cluster-admin cluster-admin">
							Your pods can now connect to this service. Their connections are redirected to the corresponding ports on the external server, using the reserved egress IP address.
						</p></li></ol></div></section><section class="section _additional-resources" id="deploying-egress-router-layer3-redirection-additional-resources"><div class="titlepage"><div><div><h3 class="title">27.10.4. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-egress-router-configmap">Configuring an egress router destination mappings with a ConfigMap</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="deploying-egress-router-http-redirection"><div class="titlepage"><div><div><h2 class="title">27.11. Deploying an egress router pod in HTTP proxy mode</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can deploy an egress router pod configured to proxy traffic to specified HTTP and HTTPS-based services.
			</p><section class="section cluster-admin" id="nw-egress-router-pod_deploying-egress-router-http-redirection"><div class="titlepage"><div><div><h3 class="title">27.11.1. Egress router pod specification for HTTP mode</h3></div></div></div><p class="cluster-admin cluster-admin">
					Define the configuration for an egress router pod in the <code class="literal cluster-admin">Pod</code> object. The following YAML describes the fields for the configuration of an egress router pod in HTTP mode:
				</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Pod
metadata:
  name: egress-1
  labels:
    name: egress-1
  annotations:
    pod.network.openshift.io/assign-macvlan: "true" <span id="CO165-1"><!--Empty--></span><span class="callout">1</span>
spec:
  initContainers:
  - name: egress-router
    image: registry.redhat.io/openshift4/ose-egress-router
    securityContext:
      privileged: true
    env:
    - name: EGRESS_SOURCE <span id="CO165-2"><!--Empty--></span><span class="callout">2</span>
      value: &lt;egress-router&gt;
    - name: EGRESS_GATEWAY <span id="CO165-3"><!--Empty--></span><span class="callout">3</span>
      value: &lt;egress-gateway&gt;
    - name: EGRESS_ROUTER_MODE
      value: http-proxy
  containers:
  - name: egress-router-pod
    image: registry.redhat.io/openshift4/ose-egress-http-proxy
    env:
    - name: EGRESS_HTTP_PROXY_DESTINATION <span id="CO165-4"><!--Empty--></span><span class="callout">4</span>
      value: |-
        ...
    ...</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO165-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The annotation tells OpenShift Container Platform to create a macvlan network interface on the primary network interface controller (NIC) and move that macvlan interface into the pod’s network namespace. You must include the quotation marks around the <code class="literal cluster-admin">"true"</code> value. To have OpenShift Container Platform create the macvlan interface on a different NIC interface, set the annotation value to the name of that interface. For example, <code class="literal cluster-admin">eth1</code>.
						</div></dd><dt><a href="#CO165-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							IP address from the physical network that the node is on that is reserved for use by the egress router pod. Optional: You can include the subnet length, the <code class="literal cluster-admin">/24</code> suffix, so that a proper route to the local subnet is set. If you do not specify a subnet length, then the egress router can access only the host specified with the <code class="literal cluster-admin">EGRESS_GATEWAY</code> variable and no other hosts on the subnet.
						</div></dd><dt><a href="#CO165-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Same value as the default gateway used by the node.
						</div></dd><dt><a href="#CO165-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							A string or YAML multi-line string specifying how to configure the proxy. Note that this is specified as an environment variable in the HTTP proxy container, not with the other environment variables in the init container.
						</div></dd></dl></div></section><section class="section cluster-admin" id="nw-egress-router-dest-var_deploying-egress-router-http-redirection"><div class="titlepage"><div><div><h3 class="title">27.11.2. Egress destination configuration format</h3></div></div></div><p class="cluster-admin cluster-admin">
					When an egress router pod is deployed in HTTP proxy mode, you can specify redirection rules by using one or more of the following formats. Each line in the configuration specifies one group of connections to allow or deny:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							An IP address allows connections to that IP address, such as <code class="literal cluster-admin">192.168.1.1</code>.
						</li><li class="listitem">
							A CIDR range allows connections to that CIDR range, such as <code class="literal cluster-admin">192.168.1.0/24</code>.
						</li><li class="listitem">
							A hostname allows proxying to that host, such as <code class="literal cluster-admin">www.example.com</code>.
						</li><li class="listitem">
							A domain name preceded by <code class="literal cluster-admin">*.</code> allows proxying to that domain and all of its subdomains, such as <code class="literal cluster-admin">*.example.com</code>.
						</li><li class="listitem">
							A <code class="literal cluster-admin">!</code> followed by any of the previous match expressions denies the connection instead.
						</li><li class="listitem">
							If the last line is <code class="literal cluster-admin">*</code>, then anything that is not explicitly denied is allowed. Otherwise, anything that is not allowed is denied.
						</li></ul></div><p class="cluster-admin cluster-admin">
					You can also use <code class="literal cluster-admin">*</code> to allow connections to all remote destinations.
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example configuration</strong></p><p>
						
<pre class="programlisting language-text">!*.example.com
!192.168.1.0/24
192.168.2.1
*</pre>

					</p></div></section><section class="section cluster-admin" id="nw-egress-router-http-proxy-mode_deploying-egress-router-http-redirection"><div class="titlepage"><div><div><h3 class="title">27.11.3. Deploying an egress router pod in HTTP proxy mode</h3></div></div></div><p class="cluster-admin cluster-admin">
					In <span class="emphasis"><em><span class="cluster-admin cluster-admin">HTTP proxy mode</span></em></span>, an egress router pod runs as an HTTP proxy on port <code class="literal cluster-admin">8080</code>. This mode only works for clients that are connecting to HTTP-based or HTTPS-based services, but usually requires fewer changes to the client pods to get them to work. Many programs can be told to use an HTTP proxy by setting an environment variable.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Create an egress router pod.
						</li><li class="listitem"><p class="simpara">
							To ensure that other pods can find the IP address of the egress router pod, create a service to point to the egress router pod, as in the following example:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Service
metadata:
  name: egress-1
spec:
  ports:
  - name: http-proxy
    port: 8080 <span id="CO166-1"><!--Empty--></span><span class="callout">1</span>
  type: ClusterIP
  selector:
    name: egress-1</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO166-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Ensure the <code class="literal cluster-admin">http</code> port is set to <code class="literal cluster-admin">8080</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							To configure the client pod (not the egress proxy pod) to use the HTTP proxy, set the <code class="literal cluster-admin">http_proxy</code> or <code class="literal cluster-admin">https_proxy</code> variables:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Pod
metadata:
  name: app-1
  labels:
    name: app-1
spec:
  containers:
    env:
    - name: http_proxy
      value: http://egress-1:8080/ <span id="CO167-1"><!--Empty--></span><span class="callout">1</span>
    - name: https_proxy
      value: http://egress-1:8080/
    ...</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO167-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The service created in the previous step.
								</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								Using the <code class="literal cluster-admin">http_proxy</code> and <code class="literal cluster-admin">https_proxy</code> environment variables is not necessary for all setups. If the above does not create a working setup, then consult the documentation for the tool or software you are running in the pod.
							</p></div></div></li></ol></div></section><section class="section _additional-resources" id="deploying-egress-router-http-redirection-additional-resources"><div class="titlepage"><div><div><h3 class="title">27.11.4. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-egress-router-configmap">Configuring an egress router destination mappings with a ConfigMap</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="deploying-egress-router-dns-redirection"><div class="titlepage"><div><div><h2 class="title">27.12. Deploying an egress router pod in DNS proxy mode</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can deploy an egress router pod configured to proxy traffic to specified DNS names and IP addresses.
			</p><section class="section cluster-admin" id="nw-egress-router-pod_deploying-egress-router-dns-redirection"><div class="titlepage"><div><div><h3 class="title">27.12.1. Egress router pod specification for DNS mode</h3></div></div></div><p class="cluster-admin cluster-admin">
					Define the configuration for an egress router pod in the <code class="literal cluster-admin">Pod</code> object. The following YAML describes the fields for the configuration of an egress router pod in DNS mode:
				</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Pod
metadata:
  name: egress-1
  labels:
    name: egress-1
  annotations:
    pod.network.openshift.io/assign-macvlan: "true" <span id="CO168-1"><!--Empty--></span><span class="callout">1</span>
spec:
  initContainers:
  - name: egress-router
    image: registry.redhat.io/openshift4/ose-egress-router
    securityContext:
      privileged: true
    env:
    - name: EGRESS_SOURCE <span id="CO168-2"><!--Empty--></span><span class="callout">2</span>
      value: &lt;egress-router&gt;
    - name: EGRESS_GATEWAY <span id="CO168-3"><!--Empty--></span><span class="callout">3</span>
      value: &lt;egress-gateway&gt;
    - name: EGRESS_ROUTER_MODE
      value: dns-proxy
  containers:
  - name: egress-router-pod
    image: registry.redhat.io/openshift4/ose-egress-dns-proxy
    securityContext:
      privileged: true
    env:
    - name: EGRESS_DNS_PROXY_DESTINATION <span id="CO168-4"><!--Empty--></span><span class="callout">4</span>
      value: |-
        ...
    - name: EGRESS_DNS_PROXY_DEBUG <span id="CO168-5"><!--Empty--></span><span class="callout">5</span>
      value: "1"
    ...</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO168-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The annotation tells OpenShift Container Platform to create a macvlan network interface on the primary network interface controller (NIC) and move that macvlan interface into the pod’s network namespace. You must include the quotation marks around the <code class="literal cluster-admin">"true"</code> value. To have OpenShift Container Platform create the macvlan interface on a different NIC interface, set the annotation value to the name of that interface. For example, <code class="literal cluster-admin">eth1</code>.
						</div></dd><dt><a href="#CO168-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							IP address from the physical network that the node is on that is reserved for use by the egress router pod. Optional: You can include the subnet length, the <code class="literal cluster-admin">/24</code> suffix, so that a proper route to the local subnet is set. If you do not specify a subnet length, then the egress router can access only the host specified with the <code class="literal cluster-admin">EGRESS_GATEWAY</code> variable and no other hosts on the subnet.
						</div></dd><dt><a href="#CO168-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							Same value as the default gateway used by the node.
						</div></dd><dt><a href="#CO168-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							Specify a list of one or more proxy destinations.
						</div></dd><dt><a href="#CO168-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Optional: Specify to output the DNS proxy log output to <code class="literal cluster-admin">stdout</code>.
						</div></dd></dl></div></section><section class="section cluster-admin" id="nw-egress-router-dest-var_deploying-egress-router-dns-redirection"><div class="titlepage"><div><div><h3 class="title">27.12.2. Egress destination configuration format</h3></div></div></div><p class="cluster-admin cluster-admin">
					When the router is deployed in DNS proxy mode, you specify a list of port and destination mappings. A destination may be either an IP address or a DNS name.
				</p><p class="cluster-admin cluster-admin">
					An egress router pod supports the following formats for specifying port and destination mappings:
				</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Port and remote address</span></dt><dd>
								You can specify a source port and a destination host by using the two field format: <code class="literal cluster-admin">&lt;port&gt; &lt;remote_address&gt;</code>.
							</dd></dl></div><p class="cluster-admin cluster-admin">
					The host can be an IP address or a DNS name. If a DNS name is provided, DNS resolution occurs at runtime. For a given host, the proxy connects to the specified source port on the destination host when connecting to the destination host IP address.
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Port and remote address pair example</strong></p><p>
						
<pre class="programlisting language-text">80 172.16.12.11
100 example.com</pre>

					</p></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Port, remote address, and remote port</span></dt><dd>
								You can specify a source port, a destination host, and a destination port by using the three field format: <code class="literal cluster-admin">&lt;port&gt; &lt;remote_address&gt; &lt;remote_port&gt;</code>.
							</dd></dl></div><p class="cluster-admin cluster-admin">
					The three field format behaves identically to the two field version, with the exception that the destination port can be different than the source port.
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Port, remote address, and remote port example</strong></p><p>
						
<pre class="programlisting language-text">8080 192.168.60.252 80
8443 web.example.com 443</pre>

					</p></div></section><section class="section cluster-admin" id="nw-egress-router-dns-mode_deploying-egress-router-dns-redirection"><div class="titlepage"><div><div><h3 class="title">27.12.3. Deploying an egress router pod in DNS proxy mode</h3></div></div></div><p class="cluster-admin cluster-admin">
					In <span class="emphasis"><em><span class="cluster-admin cluster-admin">DNS proxy mode</span></em></span>, an egress router pod acts as a DNS proxy for TCP-based services from its own IP address to one or more destination IP addresses.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Create an egress router pod.
						</li><li class="listitem"><p class="simpara">
							Create a service for the egress router pod:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a file named <code class="literal cluster-admin">egress-router-service.yaml</code> that contains the following YAML. Set <code class="literal cluster-admin">spec.ports</code> to the list of ports that you defined previously for the <code class="literal cluster-admin">EGRESS_DNS_PROXY_DESTINATION</code> environment variable.
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Service
metadata:
  name: egress-dns-svc
spec:
  ports:
    ...
  type: ClusterIP
  selector:
    name: egress-dns-proxy</pre><p class="cluster-admin cluster-admin">
									For example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Service
metadata:
  name: egress-dns-svc
spec:
  ports:
  - name: con1
    protocol: TCP
    port: 80
    targetPort: 80
  - name: con2
    protocol: TCP
    port: 100
    targetPort: 100
  type: ClusterIP
  selector:
    name: egress-dns-proxy</pre></li><li class="listitem"><p class="simpara">
									To create the service, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f egress-router-service.yaml</pre><p class="cluster-admin cluster-admin">
									Pods can now connect to this service. The connections are proxied to the corresponding ports on the external server, using the reserved egress IP address.
								</p></li></ol></div></li></ol></div></section><section class="section _additional-resources" id="deploying-egress-router-dns-redirection-additional-resources"><div class="titlepage"><div><div><h3 class="title">27.12.4. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-egress-router-configmap">Configuring an egress router destination mappings with a ConfigMap</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="configuring-egress-router-configmap"><div class="titlepage"><div><div><h2 class="title">27.13. Configuring an egress router pod destination list from a config map</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can define a <code class="literal cluster-admin">ConfigMap</code> object that specifies destination mappings for an egress router pod. The specific format of the configuration depends on the type of egress router pod. For details on the format, refer to the documentation for the specific egress router pod.
			</p><section class="section cluster-admin" id="configuring-egress-router-configmap_configuring-egress-router-configmap"><div class="titlepage"><div><div><h3 class="title">27.13.1. Configuring an egress router destination mappings with a config map</h3></div></div></div><p class="cluster-admin cluster-admin">
					For a large or frequently-changing set of destination mappings, you can use a config map to externally maintain the list. An advantage of this approach is that permission to edit the config map can be delegated to users without <code class="literal cluster-admin">cluster-admin</code> privileges. Because the egress router pod requires a privileged container, it is not possible for users without <code class="literal cluster-admin">cluster-admin</code> privileges to edit the pod definition directly.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						The egress router pod does not automatically update when the config map changes. You must restart the egress router pod to get updates.
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a file containing the mapping data for the egress router pod, as in the following example:
						</p><pre class="screen cluster-admin cluster-admin"># Egress routes for Project "Test", version 3

80   tcp 203.0.113.25

8080 tcp 203.0.113.26 80
8443 tcp 203.0.113.26 443

# Fallback
203.0.113.27</pre><p class="cluster-admin cluster-admin">
							You can put blank lines and comments into this file.
						</p></li><li class="listitem"><p class="simpara">
							Create a <code class="literal cluster-admin">ConfigMap</code> object from the file:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete configmap egress-routes --ignore-not-found</pre><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create configmap egress-routes \
  --from-file=destination=my-egress-destination.txt</pre><p class="cluster-admin cluster-admin">
							In the previous command, the <code class="literal cluster-admin">egress-routes</code> value is the name of the <code class="literal cluster-admin">ConfigMap</code> object to create and <code class="literal cluster-admin">my-egress-destination.txt</code> is the name of the file that the data is read from.
						</p><div class="admonition tip cluster-admin"><div class="admonition_header">Tip</div><div><p class="cluster-admin cluster-admin">
							You can alternatively apply the following YAML to create the config map:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: ConfigMap
metadata:
  name: egress-routes
data:
  destination: |
    # Egress routes for Project "Test", version 3

    80   tcp 203.0.113.25

    8080 tcp 203.0.113.26 80
    8443 tcp 203.0.113.26 443

    # Fallback
    203.0.113.27</pre></div></div></li><li class="listitem"><p class="simpara">
							Create an egress router pod definition and specify the <code class="literal cluster-admin">configMapKeyRef</code> stanza for the <code class="literal cluster-admin">EGRESS_DESTINATION</code> field in the environment stanza:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">...
env:
- name: EGRESS_DESTINATION
  valueFrom:
    configMapKeyRef:
      name: egress-routes
      key: destination
...</pre></li></ol></div></section><section class="section _additional-resources" id="configuring-egress-router-configmap-additional-resources"><div class="titlepage"><div><div><h3 class="title">27.13.2. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-egress-router-dest-var_deploying-egress-router-layer3-redirection">Redirect mode</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-egress-router-dest-var_deploying-egress-router-http-redirection">HTTP proxy mode</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-egress-router-dest-var_deploying-egress-router-dns-redirection">DNS proxy mode</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="enabling-multicast"><div class="titlepage"><div><div><h2 class="title">27.14. Enabling multicast for a project</h2></div></div></div><section class="section cluster-admin" id="nw-about-multicast_openshift-sdn-enabling-multicast"><div class="titlepage"><div><div><h3 class="title">27.14.1. About multicast</h3></div></div></div><p class="cluster-admin cluster-admin">
					With IP multicast, data is broadcast to many IP addresses simultaneously.
				</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						At this time, multicast is best used for low-bandwidth coordination or service discovery and not a high-bandwidth solution.
					</p></div></div><p class="cluster-admin cluster-admin">
					Multicast traffic between OpenShift Container Platform pods is disabled by default. If you are using the OpenShift SDN network plugin, you can enable multicast on a per-project basis.
				</p><p class="cluster-admin cluster-admin">
					When using the OpenShift SDN network plugin in <code class="literal cluster-admin">networkpolicy</code> isolation mode:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Multicast packets sent by a pod will be delivered to all other pods in the project, regardless of <code class="literal cluster-admin">NetworkPolicy</code> objects. Pods might be able to communicate over multicast even when they cannot communicate over unicast.
						</li><li class="listitem">
							Multicast packets sent by a pod in one project will never be delivered to pods in any other project, even if there are <code class="literal cluster-admin">NetworkPolicy</code> objects that allow communication between the projects.
						</li></ul></div><p class="cluster-admin cluster-admin">
					When using the OpenShift SDN network plugin in <code class="literal cluster-admin">multitenant</code> isolation mode:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Multicast packets sent by a pod will be delivered to all other pods in the project.
						</li><li class="listitem">
							Multicast packets sent by a pod in one project will be delivered to pods in other projects only if each project is joined together and multicast is enabled in each joined project.
						</li></ul></div></section><section class="section cluster-admin" id="nw-enabling-multicast_openshift-sdn-enabling-multicast"><div class="titlepage"><div><div><h3 class="title">27.14.2. Enabling multicast between pods</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can enable multicast between pods for your project.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You must log in to the cluster with a user that has the <code class="literal cluster-admin">cluster-admin</code> role.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Run the following command to enable multicast for a project. Replace <code class="literal cluster-admin">&lt;namespace&gt;</code> with the namespace for the project you want to enable multicast for.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate netnamespace &lt;namespace&gt; \
    netnamespace.network.openshift.io/multicast-enabled=true</pre></li></ul></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Verification</strong></p><p>
						To verify that multicast is enabled for a project, complete the following procedure:
					</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Change your current project to the project that you enabled multicast for. Replace <code class="literal cluster-admin">&lt;project&gt;</code> with the project name.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc project &lt;project&gt;</pre></li><li class="listitem"><p class="simpara">
							Create a pod to act as a multicast receiver:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat &lt;&lt;EOF| oc create -f -
apiVersion: v1
kind: Pod
metadata:
  name: mlistener
  labels:
    app: multicast-verify
spec:
  containers:
    - name: mlistener
      image: registry.access.redhat.com/ubi9
      command: ["/bin/sh", "-c"]
      args:
        ["dnf -y install socat hostname &amp;&amp; sleep inf"]
      ports:
        - containerPort: 30102
          name: mlistener
          protocol: UDP
EOF</pre></li><li class="listitem"><p class="simpara">
							Create a pod to act as a multicast sender:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat &lt;&lt;EOF| oc create -f -
apiVersion: v1
kind: Pod
metadata:
  name: msender
  labels:
    app: multicast-verify
spec:
  containers:
    - name: msender
      image: registry.access.redhat.com/ubi9
      command: ["/bin/sh", "-c"]
      args:
        ["dnf -y install socat &amp;&amp; sleep inf"]
EOF</pre></li><li class="listitem"><p class="simpara">
							In a new terminal window or tab, start the multicast listener.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Get the IP address for the Pod:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ POD_IP=$(oc get pods mlistener -o jsonpath='{.status.podIP}')</pre></li><li class="listitem"><p class="simpara">
									Start the multicast listener by entering the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec mlistener -i -t -- \
    socat UDP4-RECVFROM:30102,ip-add-membership=224.1.0.1:$POD_IP,fork EXEC:hostname</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Start the multicast transmitter.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Get the pod network IP address range:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ CIDR=$(oc get Network.config.openshift.io cluster \
    -o jsonpath='{.status.clusterNetwork[0].cidr}')</pre></li><li class="listitem"><p class="simpara">
									To send a multicast message, enter the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec msender -i -t -- \
    /bin/bash -c "echo | socat STDIO UDP4-DATAGRAM:224.1.0.1:30102,range=$CIDR,ip-multicast-ttl=64"</pre><p class="cluster-admin cluster-admin">
									If multicast is working, the previous command returns the following output:
								</p><pre class="programlisting language-text cluster-admin cluster-admin">mlistener</pre></li></ol></div></li></ol></div></section></section><section class="section cluster-admin" id="disabling-multicast"><div class="titlepage"><div><div><h2 class="title">27.15. Disabling multicast for a project</h2></div></div></div><section class="section cluster-admin" id="nw-disabling-multicast_openshift-sdn-disabling-multicast"><div class="titlepage"><div><div><h3 class="title">27.15.1. Disabling multicast between pods</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can disable multicast between pods for your project.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You must log in to the cluster with a user that has the <code class="literal cluster-admin">cluster-admin</code> role.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Disable multicast by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate netnamespace &lt;namespace&gt; \ <span id="CO169-1"><!--Empty--></span><span class="callout">1</span>
    netnamespace.network.openshift.io/multicast-enabled-</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO169-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal cluster-admin">namespace</code> for the project you want to disable multicast for.
								</div></dd></dl></div></li></ul></div></section></section><section class="section cluster-admin" id="configuring-multitenant-isolation"><div class="titlepage"><div><div><h2 class="title">27.16. Configuring network isolation using OpenShift SDN</h2></div></div></div><p class="cluster-admin cluster-admin">
				When your cluster is configured to use the multitenant isolation mode for the OpenShift SDN network plugin, each project is isolated by default. Network traffic is not allowed between pods or services in different projects in multitenant isolation mode.
			</p><p class="cluster-admin cluster-admin">
				You can change the behavior of multitenant isolation for a project in two ways:
			</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
						You can join one or more projects, allowing network traffic between pods and services in different projects.
					</li><li class="listitem">
						You can disable network isolation for a project. It will be globally accessible, accepting network traffic from pods and services in all other projects. A globally accessible project can access pods and services in all other projects.
					</li></ul></div><section class="section cluster-admin" id="prerequisites"><div class="titlepage"><div><div><h3 class="title">27.16.1. Prerequisites</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You must have a cluster configured to use the OpenShift SDN network plugin in multitenant isolation mode.
						</li></ul></div></section><section class="section cluster-admin" id="nw-multitenant-joining_multitenant-isolation"><div class="titlepage"><div><div><h3 class="title">27.16.2. Joining projects</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can join two or more projects to allow network traffic between pods and services in different projects.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You must log in to the cluster with a user that has the <code class="literal cluster-admin">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Use the following command to join projects to an existing project network:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc adm pod-network join-projects --to=&lt;project1&gt; &lt;project2&gt; &lt;project3&gt;</pre><p class="cluster-admin cluster-admin">
							Alternatively, instead of specifying specific project names, you can use the <code class="literal cluster-admin">--selector=&lt;project_selector&gt;</code> option to specify projects based upon an associated label.
						</p></li><li class="listitem"><p class="simpara">
							Optional: Run the following command to view the pod networks that you have joined together:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get netnamespaces</pre><p class="cluster-admin cluster-admin">
							Projects in the same pod-network have the same network ID in the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETID</span></strong></span> column.
						</p></li></ol></div></section><section class="section cluster-admin" id="nw-multitenant-isolation_multitenant-isolation"><div class="titlepage"><div><div><h3 class="title">27.16.3. Isolating a project</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can isolate a project so that pods and services in other projects cannot access its pods and services.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You must log in to the cluster with a user that has the <code class="literal cluster-admin">cluster-admin</code> role.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							To isolate the projects in the cluster, run the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc adm pod-network isolate-projects &lt;project1&gt; &lt;project2&gt;</pre><p class="cluster-admin cluster-admin">
							Alternatively, instead of specifying specific project names, you can use the <code class="literal cluster-admin">--selector=&lt;project_selector&gt;</code> option to specify projects based upon an associated label.
						</p></li></ul></div></section><section class="section cluster-admin" id="nw-multitenant-global_multitenant-isolation"><div class="titlepage"><div><div><h3 class="title">27.16.4. Disabling network isolation for a project</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can disable network isolation for a project.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You must log in to the cluster with a user that has the <code class="literal cluster-admin">cluster-admin</code> role.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Run the following command for the project:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc adm pod-network make-projects-global &lt;project1&gt; &lt;project2&gt;</pre><p class="cluster-admin cluster-admin">
							Alternatively, instead of specifying specific project names, you can use the <code class="literal cluster-admin">--selector=&lt;project_selector&gt;</code> option to specify projects based upon an associated label.
						</p></li></ul></div></section></section><section class="section cluster-admin" id="configuring-kube-proxy"><div class="titlepage"><div><div><h2 class="title">27.17. Configuring kube-proxy</h2></div></div></div><p class="cluster-admin cluster-admin">
				The Kubernetes network proxy (kube-proxy) runs on each node and is managed by the Cluster Network Operator (CNO). kube-proxy maintains network rules for forwarding connections for endpoints associated with services.
			</p><section class="section cluster-admin" id="nw-kube-proxy-sync_configuring-kube-proxy"><div class="titlepage"><div><div><h3 class="title">27.17.1. About iptables rules synchronization</h3></div></div></div><p class="cluster-admin cluster-admin">
					The synchronization period determines how frequently the Kubernetes network proxy (kube-proxy) syncs the iptables rules on a node.
				</p><p class="cluster-admin cluster-admin">
					A sync begins when either of the following events occurs:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							An event occurs, such as service or endpoint is added to or removed from the cluster.
						</li><li class="listitem">
							The time since the last sync exceeds the sync period defined for kube-proxy.
						</li></ul></div></section><section class="section cluster-admin" id="nw-kube-proxy-config_configuring-kube-proxy"><div class="titlepage"><div><div><h3 class="title">27.17.2. kube-proxy configuration parameters</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can modify the following <code class="literal cluster-admin">kubeProxyConfig</code> parameters.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						Because of performance improvements introduced in OpenShift Container Platform 4.3 and greater, adjusting the <code class="literal cluster-admin">iptablesSyncPeriod</code> parameter is no longer necessary.
					</p></div></div><div class="table" id="idm140587111305072"><p class="title"><strong>Table 27.3. Parameters</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 30%; " class="col_1"><!--Empty--></col><col style="width: 30%; " class="col_2"><!--Empty--></col><col style="width: 30%; " class="col_3"><!--Empty--></col><col style="width: 10%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587111298080" scope="col">Parameter</th><th align="left" valign="top" id="idm140587111296992" scope="col">Description</th><th align="left" valign="top" id="idm140587111295904" scope="col">Values</th><th align="left" valign="top" id="idm140587111294816" scope="col">Default</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587111298080"> <p>
									<code class="literal cluster-admin">iptablesSyncPeriod</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587111296992"> <p>
									The refresh period for <code class="literal cluster-admin">iptables</code> rules.
								</p>
								 </td><td align="left" valign="top" headers="idm140587111295904"> <p>
									A time interval, such as <code class="literal cluster-admin">30s</code> or <code class="literal cluster-admin">2m</code>. Valid suffixes include <code class="literal cluster-admin">s</code>, <code class="literal cluster-admin">m</code>, and <code class="literal cluster-admin">h</code> and are described in the <a class="link" href="https://golang.org/pkg/time/#ParseDuration">Go time package</a> documentation.
								</p>
								 </td><td align="left" valign="top" headers="idm140587111294816"> <p>
									<code class="literal cluster-admin">30s</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587111298080"> <p>
									<code class="literal cluster-admin">proxyArguments.iptables-min-sync-period</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587111296992"> <p>
									The minimum duration before refreshing <code class="literal cluster-admin">iptables</code> rules. This parameter ensures that the refresh does not happen too frequently. By default, a refresh starts as soon as a change that affects <code class="literal cluster-admin">iptables</code> rules occurs.
								</p>
								 </td><td align="left" valign="top" headers="idm140587111295904"> <p>
									A time interval, such as <code class="literal cluster-admin">30s</code> or <code class="literal cluster-admin">2m</code>. Valid suffixes include <code class="literal cluster-admin">s</code>, <code class="literal cluster-admin">m</code>, and <code class="literal cluster-admin">h</code> and are described in the <a class="link" href="https://golang.org/pkg/time/#ParseDuration">Go time package</a>
								</p>
								 </td><td align="left" valign="top" headers="idm140587111294816"> <p>
									<code class="literal cluster-admin">0s</code>
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section cluster-admin" id="nw-kube-proxy-configuring_configuring-kube-proxy"><div class="titlepage"><div><div><h3 class="title">27.17.3. Modifying the kube-proxy configuration</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can modify the Kubernetes network proxy configuration for your cluster.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in to a running cluster with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal cluster-admin">Network.operator.openshift.io</code> custom resource (CR) by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit network.operator.openshift.io cluster</pre></li><li class="listitem"><p class="simpara">
							Modify the <code class="literal cluster-admin">kubeProxyConfig</code> parameter in the CR with your changes to the kube-proxy configuration, such as in the following example CR:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  kubeProxyConfig:
    iptablesSyncPeriod: 30s
    proxyArguments:
      iptables-min-sync-period: ["30s"]</pre></li><li class="listitem"><p class="simpara">
							Save the file and exit the text editor.
						</p><p class="cluster-admin cluster-admin">
							The syntax is validated by the <code class="literal cluster-admin">oc</code> command when you save the file and exit the editor. If your modifications contain a syntax error, the editor opens the file and displays an error message.
						</p></li><li class="listitem"><p class="simpara">
							Enter the following command to confirm the configuration update:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get networks.operator.openshift.io -o yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
items:
- apiVersion: operator.openshift.io/v1
  kind: Network
  metadata:
    name: cluster
  spec:
    clusterNetwork:
    - cidr: 10.128.0.0/14
      hostPrefix: 23
    defaultNetwork:
      type: OpenShiftSDN
    kubeProxyConfig:
      iptablesSyncPeriod: 30s
      proxyArguments:
        iptables-min-sync-period:
        - 30s
    serviceNetwork:
    - 172.30.0.0/16
  status: {}
kind: List</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Optional: Enter the following command to confirm that the Cluster Network Operator accepted the configuration change:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get clusteroperator network</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      VERSION     AVAILABLE   PROGRESSING   DEGRADED   SINCE
network   4.1.0-0.9   True        False         False      1m</pre>

							</p></div><p class="cluster-admin cluster-admin">
							The <code class="literal cluster-admin">AVAILABLE</code> field is <code class="literal cluster-admin">True</code> when the configuration update is applied successfully.
						</p></li></ol></div></section></section></section><section class="chapter cluster-admin" id="configuring-routes"><div class="titlepage"><div><div><h1 class="title">Chapter 28. Configuring Routes</h1></div></div></div><section class="section cluster-admin" id="route-configuration"><div class="titlepage"><div><div><h2 class="title">28.1. Route configuration</h2></div></div></div><section class="section cluster-admin" id="nw-creating-a-route_route-configuration"><div class="titlepage"><div><div><h3 class="title">28.1.1. Creating an HTTP-based route</h3></div></div></div><p class="cluster-admin cluster-admin">
					A route allows you to host your application at a public URL. It can either be secure or unsecured, depending on the network security configuration of your application. An HTTP-based route is an unsecured route that uses the basic HTTP routing protocol and exposes a service on an unsecured application port.
				</p><p class="cluster-admin cluster-admin">
					The following procedure describes how to create a simple HTTP-based route to a web application, using the <code class="literal cluster-admin">hello-openshift</code> application as an example.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You are logged in as an administrator.
						</li><li class="listitem">
							You have a web application that exposes a port and a TCP endpoint listening for traffic on the port.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a project called <code class="literal cluster-admin">hello-openshift</code> by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc new-project hello-openshift</pre></li><li class="listitem"><p class="simpara">
							Create a pod in the project by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f https://raw.githubusercontent.com/openshift/origin/master/examples/hello-openshift/hello-pod.json</pre></li><li class="listitem"><p class="simpara">
							Create a service called <code class="literal cluster-admin">hello-openshift</code> by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc expose pod/hello-openshift</pre></li><li class="listitem"><p class="simpara">
							Create an unsecured route to the <code class="literal cluster-admin">hello-openshift</code> application by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc expose svc hello-openshift</pre></li></ol></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							To verify that the <code class="literal cluster-admin">route</code> resource that you created, run the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get routes -o yaml &lt;name of resource&gt; <span id="CO170-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO170-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									In this example, the route is named <code class="literal cluster-admin">hello-openshift</code>.
								</div></dd></dl></div></li></ul></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Sample YAML definition of the created unsecured route:</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: hello-openshift
spec:
  host: hello-openshift-hello-openshift.&lt;Ingress_Domain&gt; <span id="CO171-1"><!--Empty--></span><span class="callout">1</span>
  port:
    targetPort: 8080 <span id="CO171-2"><!--Empty--></span><span class="callout">2</span>
  to:
    kind: Service
    name: hello-openshift</pre>

					</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO171-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							<code class="literal cluster-admin">&lt;Ingress_Domain&gt;</code> is the default ingress domain name. The <code class="literal cluster-admin">ingresses.config/cluster</code> object is created during the installation and cannot be changed. If you want to specify a different domain, you can specify an alternative cluster domain using the <code class="literal cluster-admin">appsDomain</code> option.
						</div></dd><dt><a href="#CO171-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							<code class="literal cluster-admin">targetPort</code> is the target port on pods that is selected by the service that this route points to.
						</div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								To display your default ingress domain, run the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get ingresses.config/cluster -o jsonpath={.spec.domain}</pre></div></div></dd></dl></div></section><section class="section cluster-admin" id="nw-ingress-sharding-route-configuration_route-configuration"><div class="titlepage"><div><div><h3 class="title">28.1.2. Creating a route for Ingress Controller sharding</h3></div></div></div><p class="cluster-admin cluster-admin">
					A route allows you to host your application at a URL. In this case, the hostname is not set and the route uses a subdomain instead. When you specify a subdomain, you automatically use the domain of the Ingress Controller that exposes the route. For situations where a route is exposed by multiple Ingress Controllers, the route is hosted at multiple URLs.
				</p><p class="cluster-admin cluster-admin">
					The following procedure describes how to create a route for Ingress Controller sharding, using the <code class="literal cluster-admin">hello-openshift</code> application as an example.
				</p><p class="cluster-admin cluster-admin">
					Ingress Controller sharding is useful when balancing incoming traffic load among a set of Ingress Controllers and when isolating traffic to a specific Ingress Controller. For example, company A goes to one Ingress Controller and company B to another.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You are logged in as a project administrator.
						</li><li class="listitem">
							You have a web application that exposes a port and an HTTP or TLS endpoint listening for traffic on the port.
						</li><li class="listitem">
							You have configured the Ingress Controller for sharding.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a project called <code class="literal cluster-admin">hello-openshift</code> by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc new-project hello-openshift</pre></li><li class="listitem"><p class="simpara">
							Create a pod in the project by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f https://raw.githubusercontent.com/openshift/origin/master/examples/hello-openshift/hello-pod.json</pre></li><li class="listitem"><p class="simpara">
							Create a service called <code class="literal cluster-admin">hello-openshift</code> by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc expose pod/hello-openshift</pre></li><li class="listitem"><p class="simpara">
							Create a route definition called <code class="literal cluster-admin">hello-openshift-route.yaml</code>:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>YAML definition of the created route for sharding:</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    type: sharded <span id="CO172-1"><!--Empty--></span><span class="callout">1</span>
  name: hello-openshift-edge
  namespace: hello-openshift
spec:
  subdomain: hello-openshift <span id="CO172-2"><!--Empty--></span><span class="callout">2</span>
  tls:
    termination: edge
  to:
    kind: Service
    name: hello-openshift</pre>

							</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO172-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Both the label key and its corresponding label value must match the ones specified in the Ingress Controller. In this example, the Ingress Controller has the label key and value <code class="literal cluster-admin">type: sharded</code>.
								</div></dd><dt><a href="#CO172-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The route will be exposed using the value of the <code class="literal cluster-admin">subdomain</code> field. When you specify the <code class="literal cluster-admin">subdomain</code> field, you must leave the hostname unset. If you specify both the <code class="literal cluster-admin">host</code> and <code class="literal cluster-admin">subdomain</code> fields, then the route will use the value of the <code class="literal cluster-admin">host</code> field, and ignore the <code class="literal cluster-admin">subdomain</code> field.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Use <code class="literal cluster-admin">hello-openshift-route.yaml</code> to create a route to the <code class="literal cluster-admin">hello-openshift</code> application by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n hello-openshift create -f hello-openshift-route.yaml</pre></li></ol></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Get the status of the route with the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n hello-openshift get routes/hello-openshift-edge -o yaml</pre><p class="cluster-admin cluster-admin">
							The resulting <code class="literal cluster-admin">Route</code> resource should look similar to the following:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    type: sharded
  name: hello-openshift-edge
  namespace: hello-openshift
spec:
  subdomain: hello-openshift
  tls:
    termination: edge
  to:
    kind: Service
    name: hello-openshift
status:
  ingress:
  - host: hello-openshift.&lt;apps-sharded.basedomain.example.net&gt; <span id="CO173-1"><!--Empty--></span><span class="callout">1</span>
    routerCanonicalHostname: router-sharded.&lt;apps-sharded.basedomain.example.net&gt; <span id="CO173-2"><!--Empty--></span><span class="callout">2</span>
    routerName: sharded <span id="CO173-3"><!--Empty--></span><span class="callout">3</span></pre>

							</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO173-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The hostname the Ingress Controller, or router, uses to expose the route. The value of the <code class="literal cluster-admin">host</code> field is automatically determined by the Ingress Controller, and uses its domain. In this example, the domain of the Ingress Controller is <code class="literal cluster-admin">&lt;apps-sharded.basedomain.example.net&gt;</code>.
								</div></dd><dt><a href="#CO173-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The hostname of the Ingress Controller.
								</div></dd><dt><a href="#CO173-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The name of the Ingress Controller. In this example, the Ingress Controller has the name <code class="literal cluster-admin">sharded</code>.
								</div></dd></dl></div></li></ul></div></section><section class="section cluster-admin" id="nw-configuring-route-timeouts_route-configuration"><div class="titlepage"><div><div><h3 class="title">28.1.3. Configuring route timeouts</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can configure the default timeouts for an existing route when you have services in need of a low timeout, which is required for Service Level Availability (SLA) purposes, or a high timeout, for cases with a slow back end.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You need a deployed Ingress Controller on a running cluster.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Using the <code class="literal cluster-admin">oc annotate</code> command, add the timeout to the route:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate route &lt;route_name&gt; \
    --overwrite haproxy.router.openshift.io/timeout=&lt;timeout&gt;&lt;time_unit&gt; <span id="CO174-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO174-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Supported time units are microseconds (us), milliseconds (ms), seconds (s), minutes (m), hours (h), or days (d).
								</div></dd></dl></div><p class="cluster-admin cluster-admin">
							The following example sets a timeout of two seconds on a route named <code class="literal cluster-admin">myroute</code>:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate route myroute --overwrite haproxy.router.openshift.io/timeout=2s</pre></li></ol></div></section><section class="section cluster-admin" id="nw-enabling-hsts_route-configuration"><div class="titlepage"><div><div><h3 class="title">28.1.4. HTTP Strict Transport Security</h3></div></div></div><p class="cluster-admin cluster-admin">
					HTTP Strict Transport Security (HSTS) policy is a security enhancement, which signals to the browser client that only HTTPS traffic is allowed on the route host. HSTS also optimizes web traffic by signaling HTTPS transport is required, without using HTTP redirects. HSTS is useful for speeding up interactions with websites.
				</p><p class="cluster-admin cluster-admin">
					When HSTS policy is enforced, HSTS adds a Strict Transport Security header to HTTP and HTTPS responses from the site. You can use the <code class="literal cluster-admin">insecureEdgeTerminationPolicy</code> value in a route to redirect HTTP to HTTPS. When HSTS is enforced, the client changes all requests from the HTTP URL to HTTPS before the request is sent, eliminating the need for a redirect.
				</p><p class="cluster-admin cluster-admin">
					Cluster administrators can configure HSTS to do the following:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Enable HSTS per-route
						</li><li class="listitem">
							Disable HSTS per-route
						</li><li class="listitem">
							Enforce HSTS per-domain, for a set of domains, or use namespace labels in combination with domains
						</li></ul></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						HSTS works only with secure routes, either edge-terminated or re-encrypt. The configuration is ineffective on HTTP or passthrough routes.
					</p></div></div><section class="section cluster-admin" id="nw-enabling-hsts-per-route_route-configuration"><div class="titlepage"><div><div><h4 class="title">28.1.4.1. Enabling HTTP Strict Transport Security per-route</h4></div></div></div><p class="cluster-admin cluster-admin">
						HTTP strict transport security (HSTS) is implemented in the HAProxy template and applied to edge and re-encrypt routes that have the <code class="literal cluster-admin">haproxy.router.openshift.io/hsts_header</code> annotation.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								You are logged in to the cluster with a user with administrator privileges for the project.
							</li><li class="listitem">
								You installed the <code class="literal cluster-admin">oc</code> CLI.
							</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								To enable HSTS on a route, add the <code class="literal cluster-admin">haproxy.router.openshift.io/hsts_header</code> value to the edge-terminated or re-encrypt route. You can use the <code class="literal cluster-admin">oc annotate</code> tool to do this by running the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate route &lt;route_name&gt; -n &lt;namespace&gt; --overwrite=true "haproxy.router.openshift.io/hsts_header"="max-age=31536000;\ <span id="CO175-1"><!--Empty--></span><span class="callout">1</span>
includeSubDomains;preload"</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO175-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										In this example, the maximum age is set to <code class="literal cluster-admin">31536000</code> ms, which is approximately eight and a half hours.
									</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
									In this example, the equal sign (<code class="literal cluster-admin">=</code>) is in quotes. This is required to properly execute the annotate command.
								</p></div></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example route configured with an annotation</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    haproxy.router.openshift.io/hsts_header: max-age=31536000;includeSubDomains;preload <span id="CO176-1"><!--Empty--></span><span class="callout">1</span> <span id="CO176-2"><!--Empty--></span><span class="callout">2</span> <span id="CO176-3"><!--Empty--></span><span class="callout">3</span>
...
spec:
  host: def.abc.com
  tls:
    termination: "reencrypt"
    ...
  wildcardPolicy: "Subdomain"</pre>

								</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO176-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Required. <code class="literal cluster-admin">max-age</code> measures the length of time, in seconds, that the HSTS policy is in effect. If set to <code class="literal cluster-admin">0</code>, it negates the policy.
									</div></dd><dt><a href="#CO176-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Optional. When included, <code class="literal cluster-admin">includeSubDomains</code> tells the client that all subdomains of the host must have the same HSTS policy as the host.
									</div></dd><dt><a href="#CO176-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Optional. When <code class="literal cluster-admin">max-age</code> is greater than 0, you can add <code class="literal cluster-admin">preload</code> in <code class="literal cluster-admin">haproxy.router.openshift.io/hsts_header</code> to allow external services to include this site in their HSTS preload lists. For example, sites such as Google can construct a list of sites that have <code class="literal cluster-admin">preload</code> set. Browsers can then use these lists to determine which sites they can communicate with over HTTPS, even before they have interacted with the site. Without <code class="literal cluster-admin">preload</code> set, browsers must have interacted with the site over HTTPS, at least once, to get the header.
									</div></dd></dl></div></li></ul></div></section><section class="section cluster-admin" id="nw-disabling-hsts_route-configuration"><div class="titlepage"><div><div><h4 class="title">28.1.4.2. Disabling HTTP Strict Transport Security per-route</h4></div></div></div><p class="cluster-admin cluster-admin">
						To disable HTTP strict transport security (HSTS) per-route, you can set the <code class="literal cluster-admin">max-age</code> value in the route annotation to <code class="literal cluster-admin">0</code>.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								You are logged in to the cluster with a user with administrator privileges for the project.
							</li><li class="listitem">
								You installed the <code class="literal cluster-admin">oc</code> CLI.
							</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								To disable HSTS, set the <code class="literal cluster-admin">max-age</code> value in the route annotation to <code class="literal cluster-admin">0</code>, by entering the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate route &lt;route_name&gt; -n &lt;namespace&gt; --overwrite=true "haproxy.router.openshift.io/hsts_header"="max-age=0"</pre><div class="admonition tip cluster-admin"><div class="admonition_header">Tip</div><div><p class="cluster-admin cluster-admin">
								You can alternatively apply the following YAML to create the config map:
							</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example of disabling HSTS per-route</strong></p><p>
									
<pre class="programlisting language-yaml">metadata:
  annotations:
    haproxy.router.openshift.io/hsts_header: max-age=0</pre>

								</p></div></div></div></li><li class="listitem"><p class="simpara">
								To disable HSTS for every route in a namespace, enter the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate route --all -n &lt;namespace&gt; --overwrite=true "haproxy.router.openshift.io/hsts_header"="max-age=0"</pre></li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								To query the annotation for all routes, enter the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get route  --all-namespaces -o go-template='{{range .items}}{{if .metadata.annotations}}{{$a := index .metadata.annotations "haproxy.router.openshift.io/hsts_header"}}{{$n := .metadata.name}}{{with $a}}Name: {{$n}} HSTS: {{$a}}{{"\n"}}{{else}}{{""}}{{end}}{{end}}{{end}}'</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Name: routename HSTS: max-age=0</pre>

								</p></div></li></ol></div></section><section class="section cluster-admin" id="nw-enforcing-hsts-per-domain_route-configuration"><div class="titlepage"><div><div><h4 class="title">28.1.4.3. Enforcing HTTP Strict Transport Security per-domain</h4></div></div></div><p class="cluster-admin cluster-admin">
						To enforce HTTP Strict Transport Security (HSTS) per-domain for secure routes, add a <code class="literal cluster-admin">requiredHSTSPolicies</code> record to the Ingress spec to capture the configuration of the HSTS policy.
					</p><p class="cluster-admin cluster-admin">
						If you configure a <code class="literal cluster-admin">requiredHSTSPolicy</code> to enforce HSTS, then any newly created route must be configured with a compliant HSTS policy annotation.
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							To handle upgraded clusters with non-compliant HSTS routes, you can update the manifests at the source and apply the updates.
						</p></div></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							You cannot use <code class="literal cluster-admin">oc expose route</code> or <code class="literal cluster-admin">oc create route</code> commands to add a route in a domain that enforces HSTS, because the API for these commands does not accept annotations.
						</p></div></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
							HSTS cannot be applied to insecure, or non-TLS routes, even if HSTS is requested for all routes globally.
						</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								You are logged in to the cluster with a user with administrator privileges for the project.
							</li><li class="listitem">
								You installed the <code class="literal cluster-admin">oc</code> CLI.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Edit the Ingress config file:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit ingresses.config.openshift.io/cluster</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example HSTS policy</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Ingress
metadata:
  name: cluster
spec:
  domain: 'hello-openshift-default.apps.username.devcluster.openshift.com'
  requiredHSTSPolicies: <span id="CO177-1"><!--Empty--></span><span class="callout">1</span>
  - domainPatterns: <span id="CO177-2"><!--Empty--></span><span class="callout">2</span>
    - '*hello-openshift-default.apps.username.devcluster.openshift.com'
    - '*hello-openshift-default2.apps.username.devcluster.openshift.com'
    namespaceSelector: <span id="CO177-3"><!--Empty--></span><span class="callout">3</span>
      matchLabels:
        myPolicy: strict
    maxAge: <span id="CO177-4"><!--Empty--></span><span class="callout">4</span>
      smallestMaxAge: 1
      largestMaxAge: 31536000
    preloadPolicy: RequirePreload <span id="CO177-5"><!--Empty--></span><span class="callout">5</span>
    includeSubDomainsPolicy: RequireIncludeSubDomains <span id="CO177-6"><!--Empty--></span><span class="callout">6</span>
  - domainPatterns: <span id="CO177-7"><!--Empty--></span><span class="callout">7</span>
    - 'abc.example.com'
    - '*xyz.example.com'
    namespaceSelector:
      matchLabels: {}
    maxAge: {}
    preloadPolicy: NoOpinion
    includeSubDomainsPolicy: RequireNoIncludeSubDomains</pre>

								</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO177-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Required. <code class="literal cluster-admin">requiredHSTSPolicies</code> are validated in order, and the first matching <code class="literal cluster-admin">domainPatterns</code> applies.
									</div></dd><dt><a href="#CO177-2"><span class="callout">2</span></a> <a href="#CO177-7"><span class="callout">7</span></a> </dt><dd><div class="para">
										Required. You must specify at least one <code class="literal cluster-admin">domainPatterns</code> hostname. Any number of domains can be listed. You can include multiple sections of enforcing options for different <code class="literal cluster-admin">domainPatterns</code>.
									</div></dd><dt><a href="#CO177-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Optional. If you include <code class="literal cluster-admin">namespaceSelector</code>, it must match the labels of the project where the routes reside, to enforce the set HSTS policy on the routes. Routes that only match the <code class="literal cluster-admin">namespaceSelector</code> and not the <code class="literal cluster-admin">domainPatterns</code> are not validated.
									</div></dd><dt><a href="#CO177-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										Required. <code class="literal cluster-admin">max-age</code> measures the length of time, in seconds, that the HSTS policy is in effect. This policy setting allows for a smallest and largest <code class="literal cluster-admin">max-age</code> to be enforced.
									</div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
												The <code class="literal cluster-admin">largestMaxAge</code> value must be between <code class="literal cluster-admin">0</code> and <code class="literal cluster-admin">2147483647</code>. It can be left unspecified, which means no upper limit is enforced.
											</li><li class="listitem">
												The <code class="literal cluster-admin">smallestMaxAge</code> value must be between <code class="literal cluster-admin">0</code> and <code class="literal cluster-admin">2147483647</code>. Enter <code class="literal cluster-admin">0</code> to disable HSTS for troubleshooting, otherwise enter <code class="literal cluster-admin">1</code> if you never want HSTS to be disabled. It can be left unspecified, which means no lower limit is enforced.
											</li></ul></div></dd><dt><a href="#CO177-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										Optional. Including <code class="literal cluster-admin">preload</code> in <code class="literal cluster-admin">haproxy.router.openshift.io/hsts_header</code> allows external services to include this site in their HSTS preload lists. Browsers can then use these lists to determine which sites they can communicate with over HTTPS, before they have interacted with the site. Without <code class="literal cluster-admin">preload</code> set, browsers need to interact at least once with the site to get the header. <code class="literal cluster-admin">preload</code> can be set with one of the following:
									</div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
												<code class="literal cluster-admin">RequirePreload</code>: <code class="literal cluster-admin">preload</code> is required by the <code class="literal cluster-admin">RequiredHSTSPolicy</code>.
											</li><li class="listitem">
												<code class="literal cluster-admin">RequireNoPreload</code>: <code class="literal cluster-admin">preload</code> is forbidden by the <code class="literal cluster-admin">RequiredHSTSPolicy</code>.
											</li><li class="listitem">
												<code class="literal cluster-admin">NoOpinion</code>: <code class="literal cluster-admin">preload</code> does not matter to the <code class="literal cluster-admin">RequiredHSTSPolicy</code>.
											</li></ul></div></dd><dt><a href="#CO177-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										Optional. <code class="literal cluster-admin">includeSubDomainsPolicy</code> can be set with one of the following:
									</div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
												<code class="literal cluster-admin">RequireIncludeSubDomains</code>: <code class="literal cluster-admin">includeSubDomains</code> is required by the <code class="literal cluster-admin">RequiredHSTSPolicy</code>.
											</li><li class="listitem">
												<code class="literal cluster-admin">RequireNoIncludeSubDomains</code>: <code class="literal cluster-admin">includeSubDomains</code> is forbidden by the <code class="literal cluster-admin">RequiredHSTSPolicy</code>.
											</li><li class="listitem">
												<code class="literal cluster-admin">NoOpinion</code>: <code class="literal cluster-admin">includeSubDomains</code> does not matter to the <code class="literal cluster-admin">RequiredHSTSPolicy</code>.
											</li></ul></div></dd></dl></div></li><li class="listitem"><p class="simpara">
								You can apply HSTS to all routes in the cluster or in a particular namespace by entering the <code class="literal cluster-admin">oc annotate command</code>.
							</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
										To apply HSTS to all routes in the cluster, enter the <code class="literal cluster-admin">oc annotate command</code>. For example:
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate route --all --all-namespaces --overwrite=true "haproxy.router.openshift.io/hsts_header"="max-age=31536000"</pre></li><li class="listitem"><p class="simpara">
										To apply HSTS to all routes in a particular namespace, enter the <code class="literal cluster-admin">oc annotate command</code>. For example:
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate route --all -n my-namespace --overwrite=true "haproxy.router.openshift.io/hsts_header"="max-age=31536000"</pre></li></ul></div></li></ol></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Verification</strong></p><p>
							You can review the HSTS policy you configured. For example:
						</p></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								To review the <code class="literal cluster-admin">maxAge</code> set for required HSTS policies, enter the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get clusteroperator/ingress -n openshift-ingress-operator -o jsonpath='{range .spec.requiredHSTSPolicies[*]}{.spec.requiredHSTSPolicies.maxAgePolicy.largestMaxAge}{"\n"}{end}'</pre></li><li class="listitem"><p class="simpara">
								To review the HSTS annotations on all routes, enter the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get route  --all-namespaces -o go-template='{{range .items}}{{if .metadata.annotations}}{{$a := index .metadata.annotations "haproxy.router.openshift.io/hsts_header"}}{{$n := .metadata.name}}{{with $a}}Name: {{$n}} HSTS: {{$a}}{{"\n"}}{{else}}{{""}}{{end}}{{end}}{{end}}'</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Name: &lt;_routename_&gt; HSTS: max-age=31536000;preload;includeSubDomains</pre>

								</p></div></li></ul></div></section></section><section class="section cluster-admin" id="nw-throughput-troubleshoot_route-configuration"><div class="titlepage"><div><div><h3 class="title">28.1.5. Throughput issue troubleshooting methods</h3></div></div></div><p class="cluster-admin cluster-admin">
					Sometimes applications deployed by using OpenShift Container Platform can cause network throughput issues, such as unusually high latency between specific services.
				</p><p class="cluster-admin cluster-admin">
					If pod logs do not reveal any cause of the problem, use the following methods to analyze performance issues:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Use a packet analyzer, such as <code class="literal cluster-admin">ping</code> or <code class="literal cluster-admin">tcpdump</code> to analyze traffic between a pod and its node.
						</p><p class="cluster-admin cluster-admin">
							For example, <a class="link" href="https://access.redhat.com/solutions/4569211">run the <code class="literal cluster-admin">tcpdump</code> tool on each pod</a> while reproducing the behavior that led to the issue. Review the captures on both sides to compare send and receive timestamps to analyze the latency of traffic to and from a pod. Latency can occur in OpenShift Container Platform if a node interface is overloaded with traffic from other pods, storage devices, or the data plane.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ tcpdump -s 0 -i any -w /tmp/dump.pcap host &lt;podip 1&gt; &amp;&amp; host &lt;podip 2&gt; <span id="CO178-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO178-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal cluster-admin">podip</code> is the IP address for the pod. Run the <code class="literal cluster-admin">oc get pod &lt;pod_name&gt; -o wide</code> command to get the IP address of a pod.
								</div></dd></dl></div><p class="cluster-admin cluster-admin">
							The <code class="literal cluster-admin">tcpdump</code> command generates a file at <code class="literal cluster-admin">/tmp/dump.pcap</code> containing all traffic between these two pods. You can run the analyzer shortly before the issue is reproduced and stop the analyzer shortly after the issue is finished reproducing to minimize the size of the file. You can also <a class="link" href="https://access.redhat.com/solutions/5074041">run a packet analyzer between the nodes</a> (eliminating the SDN from the equation) with:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ tcpdump -s 0 -i any -w /tmp/dump.pcap port 4789</pre></li><li class="listitem"><p class="simpara">
							Use a bandwidth measuring tool, such as <a class="link" href="https://access.redhat.com/solutions/6129701"><code class="literal cluster-admin">iperf</code></a>, to measure streaming throughput and UDP throughput. Locate any bottlenecks by running the tool from the pods first, and then running it from the nodes.
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem">
									For information on installing and using <code class="literal cluster-admin">iperf</code>, see this <a class="link" href="https://access.redhat.com/solutions/33103">Red Hat Solution</a>.
								</li></ul></div></li><li class="listitem">
							In some cases, the cluster may mark the node with the router pod as unhealthy due to latency issues. Use worker latency profiles to adjust the frequency that the cluster waits for a status update from the node before taking action.
						</li><li class="listitem">
							If your cluster has designated lower-latency and higher-latency nodes, configure the <code class="literal cluster-admin">spec.nodePlacement</code> field in the Ingress Controller to control the placement of the router pod.
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-edge-remote-workers-latency">Latency spikes or temporary reduction in throughput to remote workers</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-ingress-controller-configuration-parameters_configuring-ingress">Ingress Controller configuration parameters</a>
						</li></ul></div></section><section class="section cluster-admin" id="nw-using-cookies-keep-route-statefulness_route-configuration"><div class="titlepage"><div><div><h3 class="title">28.1.6. Using cookies to keep route statefulness</h3></div></div></div><p class="cluster-admin cluster-admin">
					OpenShift Container Platform provides sticky sessions, which enables stateful application traffic by ensuring all traffic hits the same endpoint. However, if the endpoint pod terminates, whether through restart, scaling, or a change in configuration, this statefulness can disappear.
				</p><p class="cluster-admin cluster-admin">
					OpenShift Container Platform can use cookies to configure session persistence. The Ingress controller selects an endpoint to handle any user requests, and creates a cookie for the session. The cookie is passed back in the response to the request and the user sends the cookie back with the next request in the session. The cookie tells the Ingress Controller which endpoint is handling the session, ensuring that client requests use the cookie so that they are routed to the same pod.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						Cookies cannot be set on passthrough routes, because the HTTP traffic cannot be seen. Instead, a number is calculated based on the source IP address, which determines the backend.
					</p><p class="cluster-admin cluster-admin">
						If backends change, the traffic can be directed to the wrong server, making it less sticky. If you are using a load balancer, which hides source IP, the same number is set for all connections and traffic is sent to the same pod.
					</p></div></div><section class="section cluster-admin" id="nw-annotating-a-route-with-a-cookie-name_route-configuration"><div class="titlepage"><div><div><h4 class="title">28.1.6.1. Annotating a route with a cookie</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can set a cookie name to overwrite the default, auto-generated one for the route. This allows the application receiving route traffic to know the cookie name. By deleting the cookie it can force the next request to re-choose an endpoint. So, if a server was overloaded it tries to remove the requests from the client and redistribute them.
					</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Annotate the route with the specified cookie name:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate route &lt;route_name&gt; router.openshift.io/cookie_name="&lt;cookie_name&gt;"</pre><p class="cluster-admin cluster-admin">
								where:
							</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;route_name&gt;</code></span></dt><dd>
											Specifies the name of the route.
										</dd><dt><span class="term"><code class="literal cluster-admin">&lt;cookie_name&gt;</code></span></dt><dd>
											Specifies the name for the cookie.
										</dd></dl></div><p class="cluster-admin cluster-admin">
								For example, to annotate the route <code class="literal cluster-admin">my_route</code> with the cookie name <code class="literal cluster-admin">my_cookie</code>:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate route my_route router.openshift.io/cookie_name="my_cookie"</pre></li><li class="listitem"><p class="simpara">
								Capture the route hostname in a variable:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ROUTE_NAME=$(oc get route &lt;route_name&gt; -o jsonpath='{.spec.host}')</pre><p class="cluster-admin cluster-admin">
								where:
							</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">&lt;route_name&gt;</code></span></dt><dd>
											Specifies the name of the route.
										</dd></dl></div></li><li class="listitem"><p class="simpara">
								Save the cookie, and then access the route:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ curl $ROUTE_NAME -k -c /tmp/cookie_jar</pre><p class="cluster-admin cluster-admin">
								Use the cookie saved by the previous command when connecting to the route:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ curl $ROUTE_NAME -k -b /tmp/cookie_jar</pre></li></ol></div></section></section><section class="section cluster-admin" id="nw-path-based-routes_route-configuration"><div class="titlepage"><div><div><h3 class="title">28.1.7. Path-based routes</h3></div></div></div><p class="cluster-admin cluster-admin">
					Path-based routes specify a path component that can be compared against a URL, which requires that the traffic for the route be HTTP based. Thus, multiple routes can be served using the same hostname, each with a different path. Routers should match routes based on the most specific path to the least. However, this depends on the router implementation.
				</p><p class="cluster-admin cluster-admin">
					The following table shows example routes and their accessibility:
				</p><div class="table" id="idm140587110864352"><p class="title"><strong>Table 28.1. Route availability</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587110858288" scope="col">Route</th><th align="left" valign="top" id="idm140587110857200" scope="col">When Compared to</th><th align="left" valign="top" id="idm140587110856112" scope="col">Accessible</th></tr></thead><tbody><tr><td rowspan="2" align="left" valign="top" headers="idm140587110858288"> <p>
									<span class="emphasis"><em><span class="cluster-admin cluster-admin">www.example.com/test</span></em></span>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110857200"> <p>
									<span class="emphasis"><em><span class="cluster-admin cluster-admin">www.example.com/test</span></em></span>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110856112"> <p>
									Yes
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110857200"> <p>
									<span class="emphasis"><em><span class="cluster-admin cluster-admin">www.example.com</span></em></span>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110856112"> <p>
									No
								</p>
								 </td></tr><tr><td rowspan="2" align="left" valign="top" headers="idm140587110858288"> <p>
									<span class="emphasis"><em><span class="cluster-admin cluster-admin">www.example.com/test</span></em></span> and <span class="emphasis"><em><span class="cluster-admin cluster-admin">www.example.com</span></em></span>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110857200"> <p>
									<span class="emphasis"><em><span class="cluster-admin cluster-admin">www.example.com/test</span></em></span>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110856112"> <p>
									Yes
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110857200"> <p>
									<span class="emphasis"><em><span class="cluster-admin cluster-admin">www.example.com</span></em></span>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110856112"> <p>
									Yes
								</p>
								 </td></tr><tr><td rowspan="2" align="left" valign="top" headers="idm140587110858288"> <p>
									<span class="emphasis"><em><span class="cluster-admin cluster-admin">www.example.com</span></em></span>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110857200"> <p>
									<span class="emphasis"><em><span class="cluster-admin cluster-admin">www.example.com/text</span></em></span>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110856112"> <p>
									Yes (Matched by the host, not the route)
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110857200"> <p>
									<span class="emphasis"><em><span class="cluster-admin cluster-admin">www.example.com</span></em></span>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110856112"> <p>
									Yes
								</p>
								 </td></tr></tbody></table></div></div><div class="cluster-admin cluster-admin"><p class="title"><strong>An unsecured route with a path</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: route-unsecured
spec:
  host: www.example.com
  path: "/test" <span id="CO179-1"><!--Empty--></span><span class="callout">1</span>
  to:
    kind: Service
    name: service-name</pre>

					</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO179-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The path is the only added attribute for a path-based route.
						</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						Path-based routing is not available when using passthrough TLS, as the router does not terminate TLS in that case and cannot read the contents of the request.
					</p></div></div></section><section class="section cluster-admin" id="nw-route-specific-annotations_route-configuration"><div class="titlepage"><div><div><h3 class="title">28.1.8. Route-specific annotations</h3></div></div></div><p class="cluster-admin cluster-admin">
					The Ingress Controller can set the default options for all the routes it exposes. An individual route can override some of these defaults by providing specific configurations in its annotations. Red Hat does not support adding a route annotation to an operator-managed route.
				</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						To create a whitelist with multiple source IPs or subnets, use a space-delimited list. Any other delimiter type causes the list to be ignored without a warning or error message.
					</p></div></div><div class="table" id="idm140587110805296"><p class="title"><strong>Table 28.2. Route annotations</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587110799232" scope="col">Variable</th><th align="left" valign="top" id="idm140587110798144" scope="col">Description</th><th align="left" valign="top" id="idm140587110797056" scope="col">Environment variable used as default</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587110799232"> <p>
									<code class="literal cluster-admin">haproxy.router.openshift.io/balance</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110798144"> <p>
									Sets the load-balancing algorithm. Available options are <code class="literal cluster-admin">random</code>, <code class="literal cluster-admin">source</code>, <code class="literal cluster-admin">roundrobin</code>, and <code class="literal cluster-admin">leastconn</code>. The default value is <code class="literal cluster-admin">random</code>.
								</p>
								 </td><td align="left" valign="top" headers="idm140587110797056"> <p>
									<code class="literal cluster-admin">ROUTER_TCP_BALANCE_SCHEME</code> for passthrough routes. Otherwise, use <code class="literal cluster-admin">ROUTER_LOAD_BALANCE_ALGORITHM</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110799232"> <p>
									<code class="literal cluster-admin">haproxy.router.openshift.io/disable_cookies</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110798144"> <p>
									Disables the use of cookies to track related connections. If set to <code class="literal cluster-admin">'true'</code> or <code class="literal cluster-admin">'TRUE'</code>, the balance algorithm is used to choose which back-end serves connections for each incoming HTTP request.
								</p>
								 </td><td align="left" valign="top" headers="idm140587110797056"> </td></tr><tr><td align="left" valign="top" headers="idm140587110799232"> <p>
									<code class="literal cluster-admin">router.openshift.io/cookie_name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110798144"> <p>
									Specifies an optional cookie to use for this route. The name must consist of any combination of upper and lower case letters, digits, "_", and "-". The default is the hashed internal key name for the route.
								</p>
								 </td><td align="left" valign="top" headers="idm140587110797056"> </td></tr><tr><td align="left" valign="top" headers="idm140587110799232"> <p>
									<code class="literal cluster-admin">haproxy.router.openshift.io/pod-concurrent-connections</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110798144"> <p>
									Sets the maximum number of connections that are allowed to a backing pod from a router.<br/> Note: If there are multiple pods, each can have this many connections. If you have multiple routers, there is no coordination among them, each may connect this many times. If not set, or set to 0, there is no limit.
								</p>
								 </td><td align="left" valign="top" headers="idm140587110797056"> </td></tr><tr><td align="left" valign="top" headers="idm140587110799232"> <p>
									<code class="literal cluster-admin">haproxy.router.openshift.io/rate-limit-connections</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110798144"> <p>
									Setting <code class="literal cluster-admin">'true'</code> or <code class="literal cluster-admin">'TRUE'</code> enables rate limiting functionality which is implemented through stick-tables on the specific backend per route.<br/> Note: Using this annotation provides basic protection against distributed denial-of-service (DDoS) attacks.
								</p>
								 </td><td align="left" valign="top" headers="idm140587110797056"> </td></tr><tr><td align="left" valign="top" headers="idm140587110799232"> <p>
									<code class="literal cluster-admin">haproxy.router.openshift.io/rate-limit-connections.concurrent-tcp</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110798144"> <p>
									Limits the number of concurrent TCP connections made through the same source IP address. It accepts a numeric value.<br/> Note: Using this annotation provides basic protection against distributed denial-of-service (DDoS) attacks.
								</p>
								 </td><td align="left" valign="top" headers="idm140587110797056"> </td></tr><tr><td align="left" valign="top" headers="idm140587110799232"> <p>
									<code class="literal cluster-admin">haproxy.router.openshift.io/rate-limit-connections.rate-http</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110798144"> <p>
									Limits the rate at which a client with the same source IP address can make HTTP requests. It accepts a numeric value. <br/> Note: Using this annotation provides basic protection against distributed denial-of-service (DDoS) attacks.
								</p>
								 </td><td align="left" valign="top" headers="idm140587110797056"> </td></tr><tr><td align="left" valign="top" headers="idm140587110799232"> <p>
									<code class="literal cluster-admin">haproxy.router.openshift.io/rate-limit-connections.rate-tcp</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110798144"> <p>
									Limits the rate at which a client with the same source IP address can make TCP connections. It accepts a numeric value. <br/> Note: Using this annotation provides basic protection against distributed denial-of-service (DDoS) attacks.
								</p>
								 </td><td align="left" valign="top" headers="idm140587110797056"> </td></tr><tr><td align="left" valign="top" headers="idm140587110799232"> <p>
									<code class="literal cluster-admin">haproxy.router.openshift.io/timeout</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110798144"> <p>
									Sets a server-side timeout for the route. (TimeUnits)
								</p>
								 </td><td align="left" valign="top" headers="idm140587110797056"> <p>
									<code class="literal cluster-admin">ROUTER_DEFAULT_SERVER_TIMEOUT</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110799232"> <p>
									<code class="literal cluster-admin">haproxy.router.openshift.io/timeout-tunnel</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110798144"> <p>
									This timeout applies to a tunnel connection, for example, WebSocket over cleartext, edge, reencrypt, or passthrough routes. With cleartext, edge, or reencrypt route types, this annotation is applied as a timeout tunnel with the existing timeout value. For the passthrough route types, the annotation takes precedence over any existing timeout value set.
								</p>
								 </td><td align="left" valign="top" headers="idm140587110797056"> <p>
									<code class="literal cluster-admin">ROUTER_DEFAULT_TUNNEL_TIMEOUT</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110799232"> <p>
									<code class="literal cluster-admin">ingresses.config/cluster ingress.operator.openshift.io/hard-stop-after</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110798144"> <p>
									You can set either an IngressController or the ingress config . This annotation redeploys the router and configures the HA proxy to emit the haproxy <code class="literal cluster-admin">hard-stop-after</code> global option, which defines the maximum time allowed to perform a clean soft-stop.
								</p>
								 </td><td align="left" valign="top" headers="idm140587110797056"> <p>
									<code class="literal cluster-admin">ROUTER_HARD_STOP_AFTER</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110799232"> <p>
									<code class="literal cluster-admin">router.openshift.io/haproxy.health.check.interval</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110798144"> <p>
									Sets the interval for the back-end health checks. (TimeUnits)
								</p>
								 </td><td align="left" valign="top" headers="idm140587110797056"> <p>
									<code class="literal cluster-admin">ROUTER_BACKEND_CHECK_INTERVAL</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110799232"> <p>
									<code class="literal cluster-admin">haproxy.router.openshift.io/ip_whitelist</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110798144"> <p>
									Sets a whitelist for the route. The whitelist is a space-separated list of IP addresses and CIDR ranges for the approved source addresses. Requests from IP addresses that are not in the whitelist are dropped.
								</p>
								 <p>
									The maximum number of IP addresses and CIDR ranges allowed in a whitelist is 61.
								</p>
								 </td><td align="left" valign="top" headers="idm140587110797056"> </td></tr><tr><td align="left" valign="top" headers="idm140587110799232"> <p>
									<code class="literal cluster-admin">haproxy.router.openshift.io/hsts_header</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110798144"> <p>
									Sets a Strict-Transport-Security header for the edge terminated or re-encrypt route.
								</p>
								 </td><td align="left" valign="top" headers="idm140587110797056"> </td></tr><tr><td align="left" valign="top" headers="idm140587110799232"> <p>
									<code class="literal cluster-admin">haproxy.router.openshift.io/log-send-hostname</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110798144"> <p>
									Sets the <code class="literal cluster-admin">hostname</code> field in the Syslog header. Uses the hostname of the system. <code class="literal cluster-admin">log-send-hostname</code> is enabled by default if any Ingress API logging method, such as sidecar or Syslog facility, is enabled for the router.
								</p>
								 </td><td align="left" valign="top" headers="idm140587110797056"> </td></tr><tr><td align="left" valign="top" headers="idm140587110799232"> <p>
									<code class="literal cluster-admin">haproxy.router.openshift.io/rewrite-target</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110798144"> <p>
									Sets the rewrite path of the request on the backend.
								</p>
								 </td><td align="left" valign="top" headers="idm140587110797056"> </td></tr><tr><td align="left" valign="top" headers="idm140587110799232"> <p>
									<code class="literal cluster-admin">router.openshift.io/cookie-same-site</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110798144"> <p>
									Sets a value to restrict cookies. The values are:
								</p>
								 <p>
									<code class="literal cluster-admin">Lax</code>: cookies are transferred between the visited site and third-party sites.
								</p>
								 <p>
									<code class="literal cluster-admin">Strict</code>: cookies are restricted to the visited site.
								</p>
								 <p>
									<code class="literal cluster-admin">None</code>: cookies are restricted to the visited site.
								</p>
								 <p>
									This value is applicable to re-encrypt and edge routes only. For more information, see the <a class="link" href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie/SameSite">SameSite cookies documentation</a>.
								</p>
								 </td><td align="left" valign="top" headers="idm140587110797056"> </td></tr><tr><td align="left" valign="top" headers="idm140587110799232"> <p>
									<code class="literal cluster-admin">haproxy.router.openshift.io/set-forwarded-headers</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110798144"> <p>
									Sets the policy for handling the <code class="literal cluster-admin">Forwarded</code> and <code class="literal cluster-admin">X-Forwarded-For</code> HTTP headers per route. The values are:
								</p>
								 <p>
									<code class="literal cluster-admin">append</code>: appends the header, preserving any existing header. This is the default value.
								</p>
								 <p>
									<code class="literal cluster-admin">replace</code>: sets the header, removing any existing header.
								</p>
								 <p>
									<code class="literal cluster-admin">never</code>: never sets the header, but preserves any existing header.
								</p>
								 <p>
									<code class="literal cluster-admin">if-none</code>: sets the header if it is not already set.
								</p>
								 </td><td align="left" valign="top" headers="idm140587110797056"> <p>
									<code class="literal cluster-admin">ROUTER_SET_FORWARDED_HEADERS</code>
								</p>
								 </td></tr></tbody></table></div></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						Environment variables cannot be edited.
					</p></div></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Router timeout variables</strong></p><p>
						<code class="literal cluster-admin">TimeUnits</code> are represented by a number followed by the unit: <code class="literal cluster-admin">us</code> *(microseconds), <code class="literal cluster-admin">ms</code> (milliseconds, default), <code class="literal cluster-admin">s</code> (seconds), <code class="literal cluster-admin">m</code> (minutes), <code class="literal cluster-admin">h</code> *(hours), <code class="literal cluster-admin">d</code> (days).
					</p></div><p class="cluster-admin cluster-admin">
					The regular expression is: [1-9][0-9]*(<code class="literal cluster-admin">us</code>\|<code class="literal cluster-admin">ms</code>\|<code class="literal cluster-admin">s</code>\|<code class="literal cluster-admin">m</code>\|<code class="literal cluster-admin">h</code>\|<code class="literal cluster-admin">d</code>).
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 40%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 40%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587110643328" scope="col">Variable</th><th align="left" valign="top" id="idm140587110642240" scope="col">Default</th><th align="left" valign="top" id="idm140587110641152" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587110643328"> <p>
									<code class="literal cluster-admin">ROUTER_BACKEND_CHECK_INTERVAL</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110642240"> <p>
									<code class="literal cluster-admin">5000ms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110641152"> <p class="cluster-admin cluster-admin">
									Length of time between subsequent liveness checks on back ends.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110643328"> <p>
									<code class="literal cluster-admin">ROUTER_CLIENT_FIN_TIMEOUT</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110642240"> <p>
									<code class="literal cluster-admin">1s</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110641152"> <p class="cluster-admin cluster-admin">
									Controls the TCP FIN timeout period for the client connecting to the route. If the FIN sent to close the connection does not answer within the given time, HAProxy closes the connection. This is harmless if set to a low value and uses fewer resources on the router.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110643328"> <p>
									<code class="literal cluster-admin">ROUTER_DEFAULT_CLIENT_TIMEOUT</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110642240"> <p>
									<code class="literal cluster-admin">30s</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110641152"> <p class="cluster-admin cluster-admin">
									Length of time that a client has to acknowledge or send data.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110643328"> <p>
									<code class="literal cluster-admin">ROUTER_DEFAULT_CONNECT_TIMEOUT</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110642240"> <p>
									<code class="literal cluster-admin">5s</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110641152"> <p class="cluster-admin cluster-admin">
									The maximum connection time.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110643328"> <p>
									<code class="literal cluster-admin">ROUTER_DEFAULT_SERVER_FIN_TIMEOUT</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110642240"> <p>
									<code class="literal cluster-admin">1s</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110641152"> <p class="cluster-admin cluster-admin">
									Controls the TCP FIN timeout from the router to the pod backing the route.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110643328"> <p>
									<code class="literal cluster-admin">ROUTER_DEFAULT_SERVER_TIMEOUT</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110642240"> <p>
									<code class="literal cluster-admin">30s</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110641152"> <p class="cluster-admin cluster-admin">
									Length of time that a server has to acknowledge or send data.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110643328"> <p>
									<code class="literal cluster-admin">ROUTER_DEFAULT_TUNNEL_TIMEOUT</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110642240"> <p>
									<code class="literal cluster-admin">1h</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110641152"> <p class="cluster-admin cluster-admin">
									Length of time for TCP or WebSocket connections to remain open. This timeout period resets whenever HAProxy reloads.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110643328"> <p>
									<code class="literal cluster-admin">ROUTER_SLOWLORIS_HTTP_KEEPALIVE</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110642240"> <p>
									<code class="literal cluster-admin">300s</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110641152"> <p class="cluster-admin cluster-admin">
									Set the maximum time to wait for a new HTTP request to appear. If this is set too low, it can cause problems with browsers and applications not expecting a small <code class="literal cluster-admin">keepalive</code> value.
								</p>
								 <p class="cluster-admin cluster-admin">
									Some effective timeout values can be the sum of certain variables, rather than the specific expected timeout. For example, <code class="literal cluster-admin">ROUTER_SLOWLORIS_HTTP_KEEPALIVE</code> adjusts <code class="literal cluster-admin">timeout http-keep-alive</code>. It is set to <code class="literal cluster-admin">300s</code> by default, but HAProxy also waits on <code class="literal cluster-admin">tcp-request inspect-delay</code>, which is set to <code class="literal cluster-admin">5s</code>. In this case, the overall timeout would be <code class="literal cluster-admin">300s</code> plus <code class="literal cluster-admin">5s</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110643328"> <p>
									<code class="literal cluster-admin">ROUTER_SLOWLORIS_TIMEOUT</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110642240"> <p>
									<code class="literal cluster-admin">10s</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110641152"> <p class="cluster-admin cluster-admin">
									Length of time the transmission of an HTTP request can take.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110643328"> <p>
									<code class="literal cluster-admin">RELOAD_INTERVAL</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110642240"> <p>
									<code class="literal cluster-admin">5s</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110641152"> <p class="cluster-admin cluster-admin">
									Allows the minimum frequency for the router to reload and accept new changes.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110643328"> <p>
									<code class="literal cluster-admin">ROUTER_METRICS_HAPROXY_TIMEOUT</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110642240"> <p>
									<code class="literal cluster-admin">5s</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587110641152"> <p class="cluster-admin cluster-admin">
									Timeout for the gathering of HAProxy metrics.
								</p>
								 </td></tr></tbody></table></div><div class="cluster-admin cluster-admin"><p class="title"><strong>A route setting custom timeout</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    haproxy.router.openshift.io/timeout: 5500ms <span id="CO180-1"><!--Empty--></span><span class="callout">1</span>
...</pre>

					</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO180-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Specifies the new timeout with HAProxy supported units (<code class="literal cluster-admin">us</code>, <code class="literal cluster-admin">ms</code>, <code class="literal cluster-admin">s</code>, <code class="literal cluster-admin">m</code>, <code class="literal cluster-admin">h</code>, <code class="literal cluster-admin">d</code>). If the unit is not provided, <code class="literal cluster-admin">ms</code> is the default.
						</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						Setting a server-side timeout value for passthrough routes too low can cause WebSocket connections to timeout frequently on that route.
					</p></div></div><div class="cluster-admin cluster-admin"><p class="title"><strong>A route that allows only one specific IP address</strong></p><p>
						
<pre class="programlisting language-yaml">metadata:
  annotations:
    haproxy.router.openshift.io/ip_whitelist: 192.168.1.10</pre>

					</p></div><div class="cluster-admin cluster-admin"><p class="title"><strong>A route that allows several IP addresses</strong></p><p>
						
<pre class="programlisting language-yaml">metadata:
  annotations:
    haproxy.router.openshift.io/ip_whitelist: 192.168.1.10 192.168.1.11 192.168.1.12</pre>

					</p></div><div class="cluster-admin cluster-admin"><p class="title"><strong>A route that allows an IP address CIDR network</strong></p><p>
						
<pre class="programlisting language-yaml">metadata:
  annotations:
    haproxy.router.openshift.io/ip_whitelist: 192.168.1.0/24</pre>

					</p></div><div class="cluster-admin cluster-admin"><p class="title"><strong>A route that allows both IP an address and IP address CIDR networks</strong></p><p>
						
<pre class="programlisting language-yaml">metadata:
  annotations:
    haproxy.router.openshift.io/ip_whitelist: 180.5.61.153 192.168.1.0/24 10.0.0.0/8</pre>

					</p></div><div class="cluster-admin cluster-admin"><p class="title"><strong>A route specifying a rewrite target</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    haproxy.router.openshift.io/rewrite-target: / <span id="CO181-1"><!--Empty--></span><span class="callout">1</span>
...</pre>

					</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO181-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Sets <code class="literal cluster-admin">/</code> as rewrite path of the request on the backend.
						</div></dd></dl></div><p class="cluster-admin cluster-admin">
					Setting the <code class="literal cluster-admin">haproxy.router.openshift.io/rewrite-target</code> annotation on a route specifies that the Ingress Controller should rewrite paths in HTTP requests using this route before forwarding the requests to the backend application. The part of the request path that matches the path specified in <code class="literal cluster-admin">spec.path</code> is replaced with the rewrite target specified in the annotation.
				</p><p class="cluster-admin cluster-admin">
					The following table provides examples of the path rewriting behavior for various combinations of <code class="literal cluster-admin">spec.path</code>, request path, and rewrite target.
				</p><div class="table" id="idm140587110517824"><p class="title"><strong>Table 28.3. rewrite-target examples:</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587110510816" scope="col">Route.spec.path</th><th align="left" valign="top" id="idm140587110509728" scope="col">Request path</th><th align="left" valign="top" id="idm140587110508640" scope="col">Rewrite target</th><th align="left" valign="top" id="idm140587110507552" scope="col">Forwarded request path</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587110510816"> <p>
									/foo
								</p>
								 </td><td align="left" valign="top" headers="idm140587110509728"> <p>
									/foo
								</p>
								 </td><td align="left" valign="top" headers="idm140587110508640"> <p>
									/
								</p>
								 </td><td align="left" valign="top" headers="idm140587110507552"> <p>
									/
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110510816"> <p>
									/foo
								</p>
								 </td><td align="left" valign="top" headers="idm140587110509728"> <p>
									/foo/
								</p>
								 </td><td align="left" valign="top" headers="idm140587110508640"> <p>
									/
								</p>
								 </td><td align="left" valign="top" headers="idm140587110507552"> <p>
									/
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110510816"> <p>
									/foo
								</p>
								 </td><td align="left" valign="top" headers="idm140587110509728"> <p>
									/foo/bar
								</p>
								 </td><td align="left" valign="top" headers="idm140587110508640"> <p>
									/
								</p>
								 </td><td align="left" valign="top" headers="idm140587110507552"> <p>
									/bar
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110510816"> <p>
									/foo
								</p>
								 </td><td align="left" valign="top" headers="idm140587110509728"> <p>
									/foo/bar/
								</p>
								 </td><td align="left" valign="top" headers="idm140587110508640"> <p>
									/
								</p>
								 </td><td align="left" valign="top" headers="idm140587110507552"> <p>
									/bar/
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110510816"> <p>
									/foo
								</p>
								 </td><td align="left" valign="top" headers="idm140587110509728"> <p>
									/foo
								</p>
								 </td><td align="left" valign="top" headers="idm140587110508640"> <p>
									/bar
								</p>
								 </td><td align="left" valign="top" headers="idm140587110507552"> <p>
									/bar
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110510816"> <p>
									/foo
								</p>
								 </td><td align="left" valign="top" headers="idm140587110509728"> <p>
									/foo/
								</p>
								 </td><td align="left" valign="top" headers="idm140587110508640"> <p>
									/bar
								</p>
								 </td><td align="left" valign="top" headers="idm140587110507552"> <p>
									/bar/
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110510816"> <p>
									/foo
								</p>
								 </td><td align="left" valign="top" headers="idm140587110509728"> <p>
									/foo/bar
								</p>
								 </td><td align="left" valign="top" headers="idm140587110508640"> <p>
									/baz
								</p>
								 </td><td align="left" valign="top" headers="idm140587110507552"> <p>
									/baz/bar
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110510816"> <p>
									/foo
								</p>
								 </td><td align="left" valign="top" headers="idm140587110509728"> <p>
									/foo/bar/
								</p>
								 </td><td align="left" valign="top" headers="idm140587110508640"> <p>
									/baz
								</p>
								 </td><td align="left" valign="top" headers="idm140587110507552"> <p>
									/baz/bar/
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110510816"> <p>
									/foo/
								</p>
								 </td><td align="left" valign="top" headers="idm140587110509728"> <p>
									/foo
								</p>
								 </td><td align="left" valign="top" headers="idm140587110508640"> <p>
									/
								</p>
								 </td><td align="left" valign="top" headers="idm140587110507552"> <p>
									N/A (request path does not match route path)
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110510816"> <p>
									/foo/
								</p>
								 </td><td align="left" valign="top" headers="idm140587110509728"> <p>
									/foo/
								</p>
								 </td><td align="left" valign="top" headers="idm140587110508640"> <p>
									/
								</p>
								 </td><td align="left" valign="top" headers="idm140587110507552"> <p>
									/
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587110510816"> <p>
									/foo/
								</p>
								 </td><td align="left" valign="top" headers="idm140587110509728"> <p>
									/foo/bar
								</p>
								 </td><td align="left" valign="top" headers="idm140587110508640"> <p>
									/
								</p>
								 </td><td align="left" valign="top" headers="idm140587110507552"> <p>
									/bar
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section cluster-admin" id="nw-route-admission-policy_route-configuration"><div class="titlepage"><div><div><h3 class="title">28.1.9. Configuring the route admission policy</h3></div></div></div><p class="cluster-admin cluster-admin">
					Administrators and application developers can run applications in multiple namespaces with the same domain name. This is for organizations where multiple teams develop microservices that are exposed on the same hostname.
				</p><div class="admonition warning cluster-admin"><div class="admonition_header">Warning</div><div><p class="cluster-admin cluster-admin">
						Allowing claims across namespaces should only be enabled for clusters with trust between namespaces, otherwise a malicious user could take over a hostname. For this reason, the default admission policy disallows hostname claims across namespaces.
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Cluster administrator privileges.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Edit the <code class="literal cluster-admin">.spec.routeAdmission</code> field of the <code class="literal cluster-admin">ingresscontroller</code> resource variable using the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n openshift-ingress-operator patch ingresscontroller/default --patch '{"spec":{"routeAdmission":{"namespaceOwnership":"InterNamespaceAllowed"}}}' --type=merge</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Sample Ingress Controller configuration</strong></p><p>
								
<pre class="programlisting language-yaml">spec:
  routeAdmission:
    namespaceOwnership: InterNamespaceAllowed
...</pre>

							</p></div><div class="admonition tip cluster-admin"><div class="admonition_header">Tip</div><div><p class="cluster-admin cluster-admin">
							You can alternatively apply the following YAML to configure the route admission policy:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  routeAdmission:
    namespaceOwnership: InterNamespaceAllowed</pre></div></div></li></ul></div></section><section class="section cluster-admin" id="nw-ingress-creating-a-route-via-an-ingress_route-configuration"><div class="titlepage"><div><div><h3 class="title">28.1.10. Creating a route through an Ingress object</h3></div></div></div><p class="cluster-admin cluster-admin">
					Some ecosystem components have an integration with Ingress resources but not with route resources. To cover this case, OpenShift Container Platform automatically creates managed route objects when an Ingress object is created. These route objects are deleted when the corresponding Ingress objects are deleted.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Define an Ingress object in the OpenShift Container Platform console or by entering the <code class="literal cluster-admin">oc create</code> command:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>YAML Definition of an Ingress</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend
  annotations:
    route.openshift.io/termination: "reencrypt" <span id="CO182-1"><!--Empty--></span><span class="callout">1</span>
    route.openshift.io/destination-ca-certificate-secret: secret-ca-cert <span id="CO182-2"><!--Empty--></span><span class="callout">2</span>
spec:
  rules:
  - host: www.example.com <span id="CO182-3"><!--Empty--></span><span class="callout">3</span>
    http:
      paths:
      - backend:
          service:
            name: frontend
            port:
              number: 443
        path: /
        pathType: Prefix
  tls:
  - hosts:
    - www.example.com
    secretName: example-com-tls-certificate</pre>

							</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO182-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal cluster-admin">route.openshift.io/termination</code> annotation can be used to configure the <code class="literal cluster-admin">spec.tls.termination</code> field of the <code class="literal cluster-admin">Route</code> as <code class="literal cluster-admin">Ingress</code> has no field for this. The accepted values are <code class="literal cluster-admin">edge</code>, <code class="literal cluster-admin">passthrough</code> and <code class="literal cluster-admin">reencrypt</code>. All other values are silently ignored. When the annotation value is unset, <code class="literal cluster-admin">edge</code> is the default route. The TLS certificate details must be defined in the template file to implement the default edge route.
								</div></dd><dt><a href="#CO182-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									When working with an <code class="literal cluster-admin">Ingress</code> object, you must specify an explicit hostname, unlike when working with routes. You can use the <code class="literal cluster-admin">&lt;host_name&gt;.&lt;cluster_ingress_domain&gt;</code> syntax, for example <code class="literal cluster-admin">apps.openshiftdemos.com</code>, to take advantage of the <code class="literal cluster-admin">*.&lt;cluster_ingress_domain&gt;</code> wildcard DNS record and serving certificate for the cluster. Otherwise, you must ensure that there is a DNS record for the chosen hostname.
								</div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
											If you specify the <code class="literal cluster-admin">passthrough</code> value in the <code class="literal cluster-admin">route.openshift.io/termination</code> annotation, set <code class="literal cluster-admin">path</code> to <code class="literal cluster-admin">''</code> and <code class="literal cluster-admin">pathType</code> to <code class="literal cluster-admin">ImplementationSpecific</code> in the spec:
										</p><pre class="programlisting language-yaml cluster-admin cluster-admin">  spec:
    rules:
    - host: www.example.com
      http:
        paths:
        - path: ''
          pathType: ImplementationSpecific
          backend:
            service:
              name: frontend
              port:
                number: 443</pre><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f ingress.yaml</pre></li></ol></div></dd><dt><a href="#CO182-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The <code class="literal cluster-admin">route.openshift.io/destination-ca-certificate-secret</code> can be used on an Ingress object to define a route with a custom destination certificate (CA). The annotation references a kubernetes secret, <code class="literal cluster-admin">secret-ca-cert</code> that will be inserted into the generated route.
								</div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem">
											To specify a route object with a destination CA from an ingress object, you must create a <code class="literal cluster-admin">kubernetes.io/tls</code> or <code class="literal cluster-admin">Opaque</code> type secret with a certificate in PEM-encoded format in the <code class="literal cluster-admin">data.tls.crt</code> specifier of the secret.
										</li></ol></div></dd></dl></div></li><li class="listitem"><p class="simpara">
							List your routes:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get routes</pre><p class="cluster-admin cluster-admin">
							The result includes an autogenerated route whose name starts with <code class="literal cluster-admin">frontend-</code>:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">NAME             HOST/PORT         PATH    SERVICES    PORT    TERMINATION          WILDCARD
frontend-gnztq   www.example.com           frontend    443     reencrypt/Redirect   None</pre><p class="cluster-admin cluster-admin">
							If you inspect this route, it looks this:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>YAML Definition of an autogenerated route</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: frontend-gnztq
  ownerReferences:
  - apiVersion: networking.k8s.io/v1
    controller: true
    kind: Ingress
    name: frontend
    uid: 4e6c59cc-704d-4f44-b390-617d879033b6
spec:
  host: www.example.com
  path: /
  port:
    targetPort: https
  tls:
    certificate: |
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
    insecureEdgeTerminationPolicy: Redirect
    key: |
      -----BEGIN RSA PRIVATE KEY-----
      [...]
      -----END RSA PRIVATE KEY-----
    termination: reencrypt
    destinationCACertificate: |
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
  to:
    kind: Service
    name: frontend</pre>

							</p></div></li></ol></div></section><section class="section cluster-admin" id="creating-edge-route-with-default-certificate_route-configuration"><div class="titlepage"><div><div><h3 class="title">28.1.11. Creating a route using the default certificate through an Ingress object</h3></div></div></div><p class="cluster-admin cluster-admin">
					If you create an Ingress object without specifying any TLS configuration, OpenShift Container Platform generates an insecure route. To create an Ingress object that generates a secure, edge-terminated route using the default ingress certificate, you can specify an empty TLS configuration as follows.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have a service that you want to expose.
						</li><li class="listitem">
							You have access to the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a YAML file for the Ingress object. In this example, the file is called <code class="literal cluster-admin">example-ingress.yaml</code>:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>YAML definition of an Ingress object</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend
  ...
spec:
  rules:
    ...
  tls:
  - {} <span id="CO183-1"><!--Empty--></span><span class="callout">1</span></pre>

							</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO183-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Use this exact syntax to specify TLS without specifying a custom certificate.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the Ingress object by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f example-ingress.yaml</pre></li></ol></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Verify that OpenShift Container Platform has created the expected route for the Ingress object by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get routes -o yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
items:
- apiVersion: route.openshift.io/v1
  kind: Route
  metadata:
    name: frontend-j9sdd <span id="CO184-1"><!--Empty--></span><span class="callout">1</span>
    ...
  spec:
  ...
    tls: <span id="CO184-2"><!--Empty--></span><span class="callout">2</span>
      insecureEdgeTerminationPolicy: Redirect
      termination: edge <span id="CO184-3"><!--Empty--></span><span class="callout">3</span>
  ...</pre>

							</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO184-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the route includes the name of the Ingress object followed by a random suffix.
								</div></dd><dt><a href="#CO184-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									In order to use the default certificate, the route should not specify <code class="literal cluster-admin">spec.certificate</code>.
								</div></dd><dt><a href="#CO184-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The route should specify the <code class="literal cluster-admin">edge</code> termination policy.
								</div></dd></dl></div></li></ul></div></section><section class="section cluster-admin" id="creating-re-encrypt-route-with-custom-certificate_route-configuration"><div class="titlepage"><div><div><h3 class="title">28.1.12. Creating a route using the destination CA certificate in the Ingress annotation</h3></div></div></div><p class="cluster-admin cluster-admin">
					The <code class="literal cluster-admin">route.openshift.io/destination-ca-certificate-secret</code> annotation can be used on an Ingress object to define a route with a custom destination CA certificate.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You may have a certificate/key pair in PEM-encoded files, where the certificate is valid for the route host.
						</li><li class="listitem">
							You may have a separate CA certificate in a PEM-encoded file that completes the certificate chain.
						</li><li class="listitem">
							You must have a separate destination CA certificate in a PEM-encoded file.
						</li><li class="listitem">
							You must have a service that you want to expose.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Add the <code class="literal cluster-admin">route.openshift.io/destination-ca-certificate-secret</code> to the Ingress annotations:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend
  annotations:
    route.openshift.io/termination: "reencrypt"
    route.openshift.io/destination-ca-certificate-secret: secret-ca-cert <span id="CO185-1"><!--Empty--></span><span class="callout">1</span>
...</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO185-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The annotation references a kubernetes secret.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							The secret referenced in this annotation will be inserted into the generated route.
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: frontend
  annotations:
    route.openshift.io/termination: reencrypt
    route.openshift.io/destination-ca-certificate-secret: secret-ca-cert
spec:
...
  tls:
    insecureEdgeTerminationPolicy: Redirect
    termination: reencrypt
    destinationCACertificate: |
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
...</pre>

							</p></div></li></ol></div></section><section class="section cluster-admin" id="nw-router-configuring-dual-stack_route-configuration"><div class="titlepage"><div><div><h3 class="title">28.1.13. Configuring the OpenShift Container Platform Ingress Controller for dual-stack networking</h3></div></div></div><p class="cluster-admin cluster-admin">
					If your OpenShift Container Platform cluster is configured for IPv4 and IPv6 dual-stack networking, your cluster is externally reachable by OpenShift Container Platform routes.
				</p><p class="cluster-admin cluster-admin">
					The Ingress Controller automatically serves services that have both IPv4 and IPv6 endpoints, but you can configure the Ingress Controller for single-stack or dual-stack services.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You deployed an OpenShift Container Platform cluster on bare metal.
						</li><li class="listitem">
							You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							To have the Ingress Controller serve traffic over IPv4/IPv6 to a workload, you can create a service YAML file or modify an existing service YAML file by setting the <code class="literal cluster-admin">ipFamilies</code> and <code class="literal cluster-admin">ipFamilyPolicy</code> fields. For example:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Sample service YAML file</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Service
metadata:
  creationTimestamp: yyyy-mm-ddT00:00:00Z
  labels:
    name: &lt;service_name&gt;
    manager: kubectl-create
    operation: Update
    time: yyyy-mm-ddT00:00:00Z
  name: &lt;service_name&gt;
  namespace: &lt;namespace_name&gt;
  resourceVersion: "&lt;resource_version_number&gt;"
  selfLink: "/api/v1/namespaces/&lt;namespace_name&gt;/services/&lt;service_name&gt;"
  uid: &lt;uid_number&gt;
spec:
  clusterIP: 172.30.0.0/16
  clusterIPs: <span id="CO186-1"><!--Empty--></span><span class="callout">1</span>
  - 172.30.0.0/16
  - &lt;second_IP_address&gt;
  ipFamilies: <span id="CO186-2"><!--Empty--></span><span class="callout">2</span>
  - IPv4
  - IPv6
  ipFamilyPolicy: RequireDualStack <span id="CO186-3"><!--Empty--></span><span class="callout">3</span>
  ports:
  - port: 8080
    protocol: TCP
    targetport: 8080
  selector:
    name: &lt;namespace_name&gt;
  sessionAffinity: None
  type: ClusterIP
status:
  loadbalancer: {}</pre>

							</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO186-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									In a dual-stack instance, there are two different <code class="literal cluster-admin">clusterIPs</code> provided.
								</div></dd><dt><a href="#CO186-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									For a single-stack instance, enter <code class="literal cluster-admin">IPv4</code> or <code class="literal cluster-admin">IPv6</code>. For a dual-stack instance, enter both <code class="literal cluster-admin">IPv4</code> and <code class="literal cluster-admin">IPv6</code>.
								</div></dd><dt><a href="#CO186-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									For a single-stack instance, enter <code class="literal cluster-admin">SingleStack</code>. For a dual-stack instance, enter <code class="literal cluster-admin">RequireDualStack</code>.
								</div></dd></dl></div><p class="cluster-admin cluster-admin">
							These resources generate corresponding <code class="literal cluster-admin">endpoints</code>. The Ingress Controller now watches <code class="literal cluster-admin">endpointslices</code>.
						</p></li><li class="listitem"><p class="simpara">
							To view <code class="literal cluster-admin">endpoints</code>, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get endpoints</pre></li><li class="listitem"><p class="simpara">
							To view <code class="literal cluster-admin">endpointslices</code>, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get endpointslices</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-ingress">Specifying an alternative cluster domain using the appsDomain option</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="configuring-default-certificate"><div class="titlepage"><div><div><h2 class="title">28.2. Secured routes</h2></div></div></div><p class="cluster-admin cluster-admin">
				Secure routes provide the ability to use several types of TLS termination to serve certificates to the client. The following sections describe how to create re-encrypt, edge, and passthrough routes with custom certificates.
			</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
					If you create routes in Microsoft Azure through public endpoints, the resource names are subject to restriction. You cannot create resources that use certain terms. For a list of terms that Azure restricts, see <a class="link" href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-reserved-resource-name">Resolve reserved resource name errors</a> in the Azure documentation.
				</p></div></div><section class="section cluster-admin" id="nw-ingress-creating-a-reencrypt-route-with-a-custom-certificate_secured-routes"><div class="titlepage"><div><div><h3 class="title">28.2.1. Creating a re-encrypt route with a custom certificate</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can configure a secure route using reencrypt TLS termination with a custom certificate by using the <code class="literal cluster-admin">oc create route</code> command.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You must have a certificate/key pair in PEM-encoded files, where the certificate is valid for the route host.
						</li><li class="listitem">
							You may have a separate CA certificate in a PEM-encoded file that completes the certificate chain.
						</li><li class="listitem">
							You must have a separate destination CA certificate in a PEM-encoded file.
						</li><li class="listitem">
							You must have a service that you want to expose.
						</li></ul></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						Password protected key files are not supported. To remove a passphrase from a key file, use the following command:
					</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ openssl rsa -in password_protected_tls.key -out tls.key</pre></div></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Procedure</strong></p><p>
						This procedure creates a <code class="literal cluster-admin">Route</code> resource with a custom certificate and reencrypt TLS termination. The following assumes that the certificate/key pair are in the <code class="literal cluster-admin">tls.crt</code> and <code class="literal cluster-admin">tls.key</code> files in the current working directory. You must also specify a destination CA certificate to enable the Ingress Controller to trust the service’s certificate. You may also specify a CA certificate if needed to complete the certificate chain. Substitute the actual path names for <code class="literal cluster-admin">tls.crt</code>, <code class="literal cluster-admin">tls.key</code>, <code class="literal cluster-admin">cacert.crt</code>, and (optionally) <code class="literal cluster-admin">ca.crt</code>. Substitute the name of the <code class="literal cluster-admin">Service</code> resource that you want to expose for <code class="literal cluster-admin">frontend</code>. Substitute the appropriate hostname for <code class="literal cluster-admin">www.example.com</code>.
					</p></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Create a secure <code class="literal cluster-admin">Route</code> resource using reencrypt TLS termination and a custom certificate:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create route reencrypt --service=frontend --cert=tls.crt --key=tls.key --dest-ca-cert=destca.crt --ca-cert=ca.crt --hostname=www.example.com</pre><p class="cluster-admin cluster-admin">
							If you examine the resulting <code class="literal cluster-admin">Route</code> resource, it should look similar to the following:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>YAML Definition of the Secure Route</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: frontend
spec:
  host: www.example.com
  to:
    kind: Service
    name: frontend
  tls:
    termination: reencrypt
    key: |-
      -----BEGIN PRIVATE KEY-----
      [...]
      -----END PRIVATE KEY-----
    certificate: |-
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
    caCertificate: |-
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
    destinationCACertificate: |-
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----</pre>

							</p></div><p class="cluster-admin cluster-admin">
							See <code class="literal cluster-admin">oc create route reencrypt --help</code> for more options.
						</p></li></ul></div></section><section class="section cluster-admin" id="nw-ingress-creating-an-edge-route-with-a-custom-certificate_secured-routes"><div class="titlepage"><div><div><h3 class="title">28.2.2. Creating an edge route with a custom certificate</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can configure a secure route using edge TLS termination with a custom certificate by using the <code class="literal cluster-admin">oc create route</code> command. With an edge route, the Ingress Controller terminates TLS encryption before forwarding traffic to the destination pod. The route specifies the TLS certificate and key that the Ingress Controller uses for the route.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You must have a certificate/key pair in PEM-encoded files, where the certificate is valid for the route host.
						</li><li class="listitem">
							You may have a separate CA certificate in a PEM-encoded file that completes the certificate chain.
						</li><li class="listitem">
							You must have a service that you want to expose.
						</li></ul></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						Password protected key files are not supported. To remove a passphrase from a key file, use the following command:
					</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ openssl rsa -in password_protected_tls.key -out tls.key</pre></div></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Procedure</strong></p><p>
						This procedure creates a <code class="literal cluster-admin">Route</code> resource with a custom certificate and edge TLS termination. The following assumes that the certificate/key pair are in the <code class="literal cluster-admin">tls.crt</code> and <code class="literal cluster-admin">tls.key</code> files in the current working directory. You may also specify a CA certificate if needed to complete the certificate chain. Substitute the actual path names for <code class="literal cluster-admin">tls.crt</code>, <code class="literal cluster-admin">tls.key</code>, and (optionally) <code class="literal cluster-admin">ca.crt</code>. Substitute the name of the service that you want to expose for <code class="literal cluster-admin">frontend</code>. Substitute the appropriate hostname for <code class="literal cluster-admin">www.example.com</code>.
					</p></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Create a secure <code class="literal cluster-admin">Route</code> resource using edge TLS termination and a custom certificate.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create route edge --service=frontend --cert=tls.crt --key=tls.key --ca-cert=ca.crt --hostname=www.example.com</pre><p class="cluster-admin cluster-admin">
							If you examine the resulting <code class="literal cluster-admin">Route</code> resource, it should look similar to the following:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>YAML Definition of the Secure Route</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: frontend
spec:
  host: www.example.com
  to:
    kind: Service
    name: frontend
  tls:
    termination: edge
    key: |-
      -----BEGIN PRIVATE KEY-----
      [...]
      -----END PRIVATE KEY-----
    certificate: |-
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
    caCertificate: |-
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----</pre>

							</p></div><p class="cluster-admin cluster-admin">
							See <code class="literal cluster-admin">oc create route edge --help</code> for more options.
						</p></li></ul></div></section><section class="section cluster-admin" id="nw-ingress-creating-a-passthrough-route_secured-routes"><div class="titlepage"><div><div><h3 class="title">28.2.3. Creating a passthrough route</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can configure a secure route using passthrough termination by using the <code class="literal cluster-admin">oc create route</code> command. With passthrough termination, encrypted traffic is sent straight to the destination without the router providing TLS termination. Therefore no key or certificate is required on the route.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You must have a service that you want to expose.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Create a <code class="literal cluster-admin">Route</code> resource:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create route passthrough route-passthrough-secured --service=frontend --port=8080</pre><p class="cluster-admin cluster-admin">
							If you examine the resulting <code class="literal cluster-admin">Route</code> resource, it should look similar to the following:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>A Secured Route Using Passthrough Termination</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: route-passthrough-secured <span id="CO187-1"><!--Empty--></span><span class="callout">1</span>
spec:
  host: www.example.com
  port:
    targetPort: 8080
  tls:
    termination: passthrough <span id="CO187-2"><!--Empty--></span><span class="callout">2</span>
    insecureEdgeTerminationPolicy: None <span id="CO187-3"><!--Empty--></span><span class="callout">3</span>
  to:
    kind: Service
    name: frontend</pre>

							</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO187-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the object, which is limited to 63 characters.
								</div></dd><dt><a href="#CO187-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The <code class="literal cluster-admin"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">termination</span></strong></span></code> field is set to <code class="literal cluster-admin">passthrough</code>. This is the only required <code class="literal cluster-admin">tls</code> field.
								</div></dd><dt><a href="#CO187-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Optional <code class="literal cluster-admin">insecureEdgeTerminationPolicy</code>. The only valid values are <code class="literal cluster-admin">None</code>, <code class="literal cluster-admin">Redirect</code>, or empty for disabled.
								</div></dd></dl></div><p class="cluster-admin cluster-admin">
							The destination pod is responsible for serving certificates for the traffic at the endpoint. This is currently the only method that can support requiring client certificates, also known as two-way authentication.
						</p></li></ul></div></section></section></section><section class="chapter cluster-admin" id="configuring-ingress-cluster-traffic"><div class="titlepage"><div><div><h1 class="title">Chapter 29. Configuring ingress cluster traffic</h1></div></div></div><section class="section cluster-admin" id="overview-traffic"><div class="titlepage"><div><div><h2 class="title">29.1. Configuring ingress cluster traffic overview</h2></div></div></div><p class="cluster-admin cluster-admin">
				OpenShift Container Platform provides the following methods for communicating from outside the cluster with services running in the cluster.
			</p><p class="cluster-admin cluster-admin">
				The methods are recommended, in order or preference:
			</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
						If you have HTTP/HTTPS, use an Ingress Controller.
					</li><li class="listitem">
						If you have a TLS-encrypted protocol other than HTTPS. For example, for TLS with the SNI header, use an Ingress Controller.
					</li><li class="listitem">
						Otherwise, use a Load Balancer, an External IP, or a <code class="literal cluster-admin">NodePort</code>.
					</li></ul></div><div class="informaltable" id="external-access-options-table"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587110154976" scope="col">Method</th><th align="left" valign="top" id="idm140587110153888" scope="col">Purpose</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587110154976"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-ingress-cluster-traffic-ingress-controller">Use an Ingress Controller</a>
							</p>
							 </td><td align="left" valign="top" headers="idm140587110153888"> <p>
								Allows access to HTTP/HTTPS traffic and TLS-encrypted protocols other than HTTPS (for example, TLS with the SNI header).
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587110154976"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-ingress-cluster-traffic-load-balancer">Automatically assign an external IP using a load balancer service</a>
							</p>
							 </td><td align="left" valign="top" headers="idm140587110153888"> <p>
								Allows traffic to non-standard ports through an IP address assigned from a pool. Most cloud platforms offer a method to start a service with a load-balancer IP address.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587110154976"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-metallb">About MetalLB and the MetalLB Operator</a>
							</p>
							 </td><td align="left" valign="top" headers="idm140587110153888"> <p>
								Allows traffic to a specific IP address or address from a pool on the machine network. For bare-metal installations or platforms that are like bare metal, MetalLB provides a way to start a service with a load-balancer IP address.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587110154976"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-ingress-cluster-traffic-service-external-ip">Manually assign an external IP to a service</a>
							</p>
							 </td><td align="left" valign="top" headers="idm140587110153888"> <p>
								Allows traffic to non-standard ports through a specific IP address.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140587110154976"> <p>
								<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-ingress-cluster-traffic-nodeport">Configure a <code class="literal cluster-admin">NodePort</code></a>
							</p>
							 </td><td align="left" valign="top" headers="idm140587110153888"> <p>
								Expose a service on all nodes in the cluster.
							</p>
							 </td></tr></tbody></table></div><section class="section cluster-admin" id="overview-traffic-comparision_overview-traffic"><div class="titlepage"><div><div><h3 class="title">29.1.1. Comparision: Fault tolerant access to external IP addresses</h3></div></div></div><p class="cluster-admin cluster-admin">
					For the communication methods that provide access to an external IP address, fault tolerant access to the IP address is another consideration. The following features provide fault tolerant access to an external IP address.
				</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">IP failover</span></dt><dd>
								IP failover manages a pool of virtual IP address for a set of nodes. It is implemented with Keepalived and Virtual Router Redundancy Protocol (VRRP). IP failover is a layer 2 mechanism only and relies on multicast. Multicast can have disadvantages for some networks.
							</dd><dt><span class="term">MetalLB</span></dt><dd>
								MetalLB has a layer 2 mode, but it does not use multicast. Layer 2 mode has a disadvantage that it transfers all traffic for an external IP address through one node.
							</dd><dt><span class="term">Manually assigning external IP addresses</span></dt><dd>
								You can configure your cluster with an IP address block that is used to assign external IP addresses to services. By default, this feature is disabled. This feature is flexible, but places the largest burden on the cluster or network administrator. The cluster is prepared to receive traffic that is destined for the external IP, but each customer has to decide how they want to route traffic to nodes.
							</dd></dl></div></section></section><section class="section cluster-admin" id="configuring-externalip"><div class="titlepage"><div><div><h2 class="title">29.2. Configuring ExternalIPs for services</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can designate an IP address block that is external to the cluster that can send traffic to services in the cluster.
			</p><p class="cluster-admin cluster-admin">
				This functionality is generally most useful for clusters installed on bare-metal hardware.
			</p><section class="section cluster-admin" id="prerequisites-2"><div class="titlepage"><div><div><h3 class="title">29.2.1. Prerequisites</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Your network infrastructure must route traffic for the external IP addresses to your cluster.
						</li></ul></div></section><section class="section cluster-admin" id="nw-externalip-about_configuring-externalip"><div class="titlepage"><div><div><h3 class="title">29.2.2. About ExternalIP</h3></div></div></div><p class="cluster-admin cluster-admin">
					For non-cloud environments, OpenShift Container Platform supports the assignment of external IP addresses to a <code class="literal cluster-admin">Service</code> object <code class="literal cluster-admin">spec.externalIPs[]</code> field through the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">ExternalIP</span></strong></span> facility. By setting this field, OpenShift Container Platform assigns an additional virtual IP address to the service. The IP address can be outside the service network defined for the cluster. A service configured with an ExternalIP functions similarly to a service with <code class="literal cluster-admin">type=NodePort</code>, allowing you to direct traffic to a local node for load balancing.
				</p><p class="cluster-admin cluster-admin">
					You must configure your networking infrastructure to ensure that the external IP address blocks that you define are routed to the cluster.
				</p><p class="cluster-admin cluster-admin">
					OpenShift Container Platform extends the ExternalIP functionality in Kubernetes by adding the following capabilities:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Restrictions on the use of external IP addresses by users through a configurable policy
						</li><li class="listitem">
							Allocation of an external IP address automatically to a service upon request
						</li></ul></div><div class="admonition warning cluster-admin"><div class="admonition_header">Warning</div><div><p class="cluster-admin cluster-admin">
						Disabled by default, use of ExternalIP functionality can be a security risk, because in-cluster traffic to an external IP address is directed to that service. This could allow cluster users to intercept sensitive traffic destined for external resources.
					</p></div></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						This feature is supported only in non-cloud deployments. For cloud deployments, use the load balancer services for automatic deployment of a cloud load balancer to target the endpoints of a service.
					</p></div></div><p class="cluster-admin cluster-admin">
					You can assign an external IP address in the following ways:
				</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Automatic assignment of an external IP</span></dt><dd>
								OpenShift Container Platform automatically assigns an IP address from the <code class="literal cluster-admin">autoAssignCIDRs</code> CIDR block to the <code class="literal cluster-admin">spec.externalIPs[]</code> array when you create a <code class="literal cluster-admin">Service</code> object with <code class="literal cluster-admin">spec.type=LoadBalancer</code> set. In this case, OpenShift Container Platform implements a non-cloud version of the load balancer service type and assigns IP addresses to the services. Automatic assignment is disabled by default and must be configured by a cluster administrator as described in the following section.
							</dd><dt><span class="term">Manual assignment of an external IP</span></dt><dd>
								OpenShift Container Platform uses the IP addresses assigned to the <code class="literal cluster-admin">spec.externalIPs[]</code> array when you create a <code class="literal cluster-admin">Service</code> object. You cannot specify an IP address that is already in use by another service.
							</dd></dl></div><section class="section cluster-admin" id="configuration-externalip_configuring-externalip"><div class="titlepage"><div><div><h4 class="title">29.2.2.1. Configuration for ExternalIP</h4></div></div></div><p class="cluster-admin cluster-admin">
						Use of an external IP address in OpenShift Container Platform is governed by the following fields in the <code class="literal cluster-admin">Network.config.openshift.io</code> CR named <code class="literal cluster-admin">cluster</code>:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								<code class="literal cluster-admin">spec.externalIP.autoAssignCIDRs</code> defines an IP address block used by the load balancer when choosing an external IP address for the service. OpenShift Container Platform supports only a single IP address block for automatic assignment. This can be simpler than having to manage the port space of a limited number of shared IP addresses when manually assigning ExternalIPs to services. If automatic assignment is enabled, a <code class="literal cluster-admin">Service</code> object with <code class="literal cluster-admin">spec.type=LoadBalancer</code> is allocated an external IP address.
							</li><li class="listitem">
								<code class="literal cluster-admin">spec.externalIP.policy</code> defines the permissible IP address blocks when manually specifying an IP address. OpenShift Container Platform does not apply policy rules to IP address blocks defined by <code class="literal cluster-admin">spec.externalIP.autoAssignCIDRs</code>.
							</li></ul></div><p class="cluster-admin cluster-admin">
						If routed correctly, external traffic from the configured external IP address block can reach service endpoints through any TCP or UDP port that the service exposes.
					</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
							As a cluster administrator, you must configure routing to externalIPs on both OpenShiftSDN and OVN-Kubernetes network types. You must also ensure that the IP address block you assign terminates at one or more nodes in your cluster. For more information, see <a class="link" href="https://kubernetes.io/docs/concepts/services-networking/service/#external-ips"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">Kubernetes External IPs</span></strong></span></a>.
						</p></div></div><p class="cluster-admin cluster-admin">
						OpenShift Container Platform supports both the automatic and manual assignment of IP addresses, and each address is guaranteed to be assigned to a maximum of one service. This ensures that each service can expose its chosen ports regardless of the ports exposed by other services.
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							To use IP address blocks defined by <code class="literal cluster-admin">autoAssignCIDRs</code> in OpenShift Container Platform, you must configure the necessary IP address assignment and routing for your host network.
						</p></div></div><p class="cluster-admin cluster-admin">
						The following YAML describes a service with an external IP address configured:
					</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example <code class="literal cluster-admin">Service</code> object with <code class="literal cluster-admin">spec.externalIPs[]</code> set</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
kind: Service
metadata:
  name: http-service
spec:
  clusterIP: 172.30.163.110
  externalIPs:
  - 192.168.132.253
  externalTrafficPolicy: Cluster
  ports:
  - name: highport
    nodePort: 31903
    port: 30102
    protocol: TCP
    targetPort: 30102
  selector:
    app: web
  sessionAffinity: None
  type: LoadBalancer
status:
  loadBalancer:
    ingress:
    - ip: 192.168.132.253</pre>

						</p></div></section><section class="section cluster-admin" id="restrictions-on-ip-assignment_configuring-externalip"><div class="titlepage"><div><div><h4 class="title">29.2.2.2. Restrictions on the assignment of an external IP address</h4></div></div></div><p class="cluster-admin cluster-admin">
						As a cluster administrator, you can specify IP address blocks to allow and to reject.
					</p><p class="cluster-admin cluster-admin">
						Restrictions apply only to users without <code class="literal cluster-admin">cluster-admin</code> privileges. A cluster administrator can always set the service <code class="literal cluster-admin">spec.externalIPs[]</code> field to any IP address.
					</p><p class="cluster-admin cluster-admin">
						You configure IP address policy with a <code class="literal cluster-admin">policy</code> object defined by specifying the <code class="literal cluster-admin">spec.ExternalIP.policy</code> field. The policy object has the following shape:
					</p><pre class="programlisting language-json cluster-admin cluster-admin">{
  "policy": {
    "allowedCIDRs": [],
    "rejectedCIDRs": []
  }
}</pre><p class="cluster-admin cluster-admin">
						When configuring policy restrictions, the following rules apply:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								If <code class="literal cluster-admin">policy={}</code> is set, then creating a <code class="literal cluster-admin">Service</code> object with <code class="literal cluster-admin">spec.ExternalIPs[]</code> set will fail. This is the default for OpenShift Container Platform. The behavior when <code class="literal cluster-admin">policy=null</code> is set is identical.
							</li><li class="listitem"><p class="simpara">
								If <code class="literal cluster-admin">policy</code> is set and either <code class="literal cluster-admin">policy.allowedCIDRs[]</code> or <code class="literal cluster-admin">policy.rejectedCIDRs[]</code> is set, the following rules apply:
							</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem">
										If <code class="literal cluster-admin">allowedCIDRs[]</code> and <code class="literal cluster-admin">rejectedCIDRs[]</code> are both set, then <code class="literal cluster-admin">rejectedCIDRs[]</code> has precedence over <code class="literal cluster-admin">allowedCIDRs[]</code>.
									</li><li class="listitem">
										If <code class="literal cluster-admin">allowedCIDRs[]</code> is set, creating a <code class="literal cluster-admin">Service</code> object with <code class="literal cluster-admin">spec.ExternalIPs[]</code> will succeed only if the specified IP addresses are allowed.
									</li><li class="listitem">
										If <code class="literal cluster-admin">rejectedCIDRs[]</code> is set, creating a <code class="literal cluster-admin">Service</code> object with <code class="literal cluster-admin">spec.ExternalIPs[]</code> will succeed only if the specified IP addresses are not rejected.
									</li></ul></div></li></ul></div></section><section class="section cluster-admin" id="example-policy-objects_configuring-externalip"><div class="titlepage"><div><div><h4 class="title">29.2.2.3. Example policy objects</h4></div></div></div><p class="cluster-admin cluster-admin">
						The examples that follow demonstrate several different policy configurations.
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								In the following example, the policy prevents OpenShift Container Platform from creating any service with an external IP address specified:
							</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example policy to reject any value specified for <code class="literal cluster-admin">Service</code> object <code class="literal cluster-admin">spec.externalIPs[]</code></strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  externalIP:
    policy: {}
  ...</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								In the following example, both the <code class="literal cluster-admin">allowedCIDRs</code> and <code class="literal cluster-admin">rejectedCIDRs</code> fields are set.
							</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example policy that includes both allowed and rejected CIDR blocks</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  externalIP:
    policy:
      allowedCIDRs:
      - 172.16.66.10/23
      rejectedCIDRs:
      - 172.16.66.10/24
  ...</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								In the following example, <code class="literal cluster-admin">policy</code> is set to <code class="literal cluster-admin">null</code>. If set to <code class="literal cluster-admin">null</code>, when inspecting the configuration object by entering <code class="literal cluster-admin">oc get networks.config.openshift.io -o yaml</code>, the <code class="literal cluster-admin">policy</code> field will not appear in the output.
							</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example policy to allow any value specified for <code class="literal cluster-admin">Service</code> object <code class="literal cluster-admin">spec.externalIPs[]</code></strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  externalIP:
    policy: null
  ...</pre>

								</p></div></li></ul></div></section></section><section class="section cluster-admin" id="nw-externalip-object_configuring-externalip"><div class="titlepage"><div><div><h3 class="title">29.2.3. ExternalIP address block configuration</h3></div></div></div><p class="cluster-admin cluster-admin">
					The configuration for ExternalIP address blocks is defined by a Network custom resource (CR) named <code class="literal cluster-admin">cluster</code>. The Network CR is part of the <code class="literal cluster-admin">config.openshift.io</code> API group.
				</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						During cluster installation, the Cluster Version Operator (CVO) automatically creates a Network CR named <code class="literal cluster-admin">cluster</code>. Creating any other CR objects of this type is not supported.
					</p></div></div><p class="cluster-admin cluster-admin">
					The following YAML describes the ExternalIP configuration:
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Network.config.openshift.io CR named <code class="literal cluster-admin">cluster</code></strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  externalIP:
    autoAssignCIDRs: [] <span id="CO188-1"><!--Empty--></span><span class="callout">1</span>
    policy: <span id="CO188-2"><!--Empty--></span><span class="callout">2</span>
      ...</pre>

					</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO188-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Defines the IP address block in CIDR format that is available for automatic assignment of external IP addresses to a service. Only a single IP address range is allowed.
						</div></dd><dt><a href="#CO188-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							Defines restrictions on manual assignment of an IP address to a service. If no restrictions are defined, specifying the <code class="literal cluster-admin">spec.externalIP</code> field in a <code class="literal cluster-admin">Service</code> object is not allowed. By default, no restrictions are defined.
						</div></dd></dl></div><p class="cluster-admin cluster-admin">
					The following YAML describes the fields for the <code class="literal cluster-admin">policy</code> stanza:
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Network.config.openshift.io <code class="literal cluster-admin">policy</code> stanza</strong></p><p>
						
<pre class="programlisting language-yaml">policy:
  allowedCIDRs: [] <span id="CO189-1"><!--Empty--></span><span class="callout">1</span>
  rejectedCIDRs: [] <span id="CO189-2"><!--Empty--></span><span class="callout">2</span></pre>

					</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO189-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							A list of allowed IP address ranges in CIDR format.
						</div></dd><dt><a href="#CO189-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							A list of rejected IP address ranges in CIDR format.
						</div></dd></dl></div><h5 id="example-external-ip-configurations">Example external IP configurations</h5><p class="cluster-admin cluster-admin">
					Several possible configurations for external IP address pools are displayed in the following examples:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							The following YAML describes a configuration that enables automatically assigned external IP addresses:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example configuration with <code class="literal cluster-admin">spec.externalIP.autoAssignCIDRs</code> set</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  ...
  externalIP:
    autoAssignCIDRs:
    - 192.168.132.254/29</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							The following YAML configures policy rules for the allowed and rejected CIDR ranges:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example configuration with <code class="literal cluster-admin">spec.externalIP.policy</code> set</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  ...
  externalIP:
    policy:
      allowedCIDRs:
      - 192.168.132.0/29
      - 192.168.132.8/29
      rejectedCIDRs:
      - 192.168.132.7/32</pre>

							</p></div></li></ul></div></section><section class="section cluster-admin" id="nw-externalip-configuring_configuring-externalip"><div class="titlepage"><div><div><h3 class="title">29.2.4. Configure external IP address blocks for your cluster</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can configure the following ExternalIP settings:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							An ExternalIP address block used by OpenShift Container Platform to automatically populate the <code class="literal cluster-admin">spec.clusterIP</code> field for a <code class="literal cluster-admin">Service</code> object.
						</li><li class="listitem">
							A policy object to restrict what IP addresses may be manually assigned to the <code class="literal cluster-admin">spec.clusterIP</code> array of a <code class="literal cluster-admin">Service</code> object.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Optional: To display the current external IP configuration, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc describe networks.config cluster</pre></li><li class="listitem"><p class="simpara">
							To edit the configuration, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit networks.config cluster</pre></li><li class="listitem"><p class="simpara">
							Modify the ExternalIP configuration, as in the following example:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  ...
  externalIP: <span id="CO190-1"><!--Empty--></span><span class="callout">1</span>
  ...</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO190-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the configuration for the <code class="literal cluster-admin">externalIP</code> stanza.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							To confirm the updated ExternalIP configuration, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get networks.config cluster -o go-template='{{.spec.externalIP}}{{"\n"}}'</pre></li></ol></div></section><section class="section cluster-admin" id="configuring-externalip-next-steps"><div class="titlepage"><div><div><h3 class="title">29.2.5. Next steps</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-ingress-cluster-traffic-service-external-ip">Configuring ingress cluster traffic for a service external IP</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="configuring-ingress-cluster-traffic-ingress-controller"><div class="titlepage"><div><div><h2 class="title">29.3. Configuring ingress cluster traffic using an Ingress Controller</h2></div></div></div><p class="cluster-admin cluster-admin">
				OpenShift Container Platform provides methods for communicating from outside the cluster with services running in the cluster. This method uses an Ingress Controller.
			</p><section class="section cluster-admin" id="nw-using-ingress-and-routes_configuring-ingress-cluster-traffic-ingress-controller"><div class="titlepage"><div><div><h3 class="title">29.3.1. Using Ingress Controllers and routes</h3></div></div></div><p class="cluster-admin cluster-admin">
					The Ingress Operator manages Ingress Controllers and wildcard DNS.
				</p><p class="cluster-admin cluster-admin">
					Using an Ingress Controller is the most common way to allow external access to an OpenShift Container Platform cluster.
				</p><p class="cluster-admin cluster-admin">
					An Ingress Controller is configured to accept external requests and proxy them based on the configured routes. This is limited to HTTP, HTTPS using SNI, and TLS using SNI, which is sufficient for web applications and services that work over TLS with SNI.
				</p><p class="cluster-admin cluster-admin">
					Work with your administrator to configure an Ingress Controller to accept external requests and proxy them based on the configured routes.
				</p><p class="cluster-admin cluster-admin">
					The administrator can create a wildcard DNS entry and then set up an Ingress Controller. Then, you can work with the edge Ingress Controller without having to contact the administrators.
				</p><p class="cluster-admin cluster-admin">
					By default, every Ingress Controller in the cluster can admit any route created in any project in the cluster.
				</p><p class="cluster-admin cluster-admin">
					The Ingress Controller:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Has two replicas by default, which means it should be running on two worker nodes.
						</li><li class="listitem">
							Can be scaled up to have more replicas on more nodes.
						</li></ul></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						The procedures in this section require prerequisites performed by the cluster administrator.
					</p></div></div></section><section class="section cluster-admin" id="prerequisites-3"><div class="titlepage"><div><div><h3 class="title">29.3.2. Prerequisites</h3></div></div></div><p class="cluster-admin cluster-admin">
					Before starting the following procedures, the administrator must:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Set up the external port to the cluster networking environment so that requests can reach the cluster.
						</li><li class="listitem"><p class="simpara">
							Make sure there is at least one user with cluster admin role. To add this role to a user, run the following command:
						</p><pre class="screen cluster-admin cluster-admin">$ oc adm policy add-cluster-role-to-user cluster-admin username</pre></li><li class="listitem">
							Have an OpenShift Container Platform cluster with at least one master and at least one node and a system outside the cluster that has network access to the cluster. This procedure assumes that the external system is on the same subnet as the cluster. The additional networking required for external systems on a different subnet is out-of-scope for this topic.
						</li></ul></div></section><section class="section cluster-admin" id="nw-creating-project-and-service_configuring-ingress-cluster-traffic-ingress-controller"><div class="titlepage"><div><div><h3 class="title">29.3.3. Creating a project and service</h3></div></div></div><p class="cluster-admin cluster-admin">
					If the project and service that you want to expose do not exist, first create the project, then the service.
				</p><p class="cluster-admin cluster-admin">
					If the project and service already exist, skip to the procedure on exposing the service to create a route.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the <code class="literal cluster-admin">oc</code> CLI and log in as a cluster administrator.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a new project for your service by running the <code class="literal cluster-admin">oc new-project</code> command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc new-project myproject</pre></li><li class="listitem"><p class="simpara">
							Use the <code class="literal cluster-admin">oc new-app</code> command to create your service:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc new-app nodejs:12~https://github.com/sclorg/nodejs-ex.git</pre></li><li class="listitem"><p class="simpara">
							To verify that the service was created, run the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get svc -n myproject</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
nodejs-ex   ClusterIP   172.30.197.157   &lt;none&gt;        8080/TCP   70s</pre>

							</p></div><p class="cluster-admin cluster-admin">
							By default, the new service does not have an external IP address.
						</p></li></ol></div></section><section class="section cluster-admin" id="nw-exposing-service_configuring-ingress-cluster-traffic-ingress-controller"><div class="titlepage"><div><div><h3 class="title">29.3.4. Exposing the service by creating a route</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can expose the service as a route by using the <code class="literal cluster-admin">oc expose</code> command.
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Procedure</strong></p><p>
						To expose the service:
					</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Log in to OpenShift Container Platform.
						</li><li class="listitem"><p class="simpara">
							Log in to the project where the service you want to expose is located:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc project myproject</pre></li><li class="listitem"><p class="simpara">
							Run the <code class="literal cluster-admin">oc expose service</code> command to expose the route:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc expose service nodejs-ex</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">route.route.openshift.io/nodejs-ex exposed</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To verify that the service is exposed, you can use a tool, such as cURL, to make sure the service is accessible from outside the cluster.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Use the <code class="literal cluster-admin">oc get route</code> command to find the route’s host name:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get route</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME        HOST/PORT                        PATH   SERVICES    PORT       TERMINATION   WILDCARD
nodejs-ex   nodejs-ex-myproject.example.com         nodejs-ex   8080-tcp                 None</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Use cURL to check that the host responds to a GET request:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ curl --head nodejs-ex-myproject.example.com</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">HTTP/1.1 200 OK
...</pre>

									</p></div></li></ol></div></li></ol></div></section><section class="section cluster-admin" id="nw-ingress-sharding-route-labels_configuring-ingress-cluster-traffic-ingress-controller"><div class="titlepage"><div><div><h3 class="title">29.3.5. Configuring Ingress Controller sharding by using route labels</h3></div></div></div><p class="cluster-admin cluster-admin">
					Ingress Controller sharding by using route labels means that the Ingress Controller serves any route in any namespace that is selected by the route selector.
				</p><div class="figure" id="idm140587109858992"><p class="title"><strong>Figure 29.1. Ingress sharding using route labels</strong></p><div class="figure-contents"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/88e02b6d127bf165fed27d7c18366e6a/nw-sharding-route-labels.png" alt="A diagram showing multiple Ingress Controllers with different route selectors serving any route containing a label that matches a given route selector regardless of the namespace a route belongs to"/></div></div></div><p class="cluster-admin cluster-admin">
					Ingress Controller sharding is useful when balancing incoming traffic load among a set of Ingress Controllers and when isolating traffic to a specific Ingress Controller. For example, company A goes to one Ingress Controller and company B to another.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal cluster-admin">router-internal.yaml</code> file:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin"># cat router-internal.yaml
apiVersion: v1
items:
- apiVersion: operator.openshift.io/v1
  kind: IngressController
  metadata:
    name: sharded
    namespace: openshift-ingress-operator
  spec:
    domain: &lt;apps-sharded.basedomain.example.net&gt; <span id="CO191-1"><!--Empty--></span><span class="callout">1</span>
    nodePlacement:
      nodeSelector:
        matchLabels:
          node-role.kubernetes.io/worker: ""
    routeSelector:
      matchLabels:
        type: sharded
  status: {}
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO191-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify a domain to be used by the Ingress Controller. This domain must be different from the default Ingress Controller domain.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Apply the Ingress Controller <code class="literal cluster-admin">router-internal.yaml</code> file:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin"># oc apply -f router-internal.yaml</pre><p class="cluster-admin cluster-admin">
							The Ingress Controller selects routes in any namespace that have the label <code class="literal cluster-admin">type: sharded</code>.
						</p></li><li class="listitem"><p class="simpara">
							Create a new route using the domain configured in the <code class="literal cluster-admin">router-internal.yaml</code>:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc expose svc &lt;service-name&gt; --hostname &lt;route-name&gt;.apps-sharded.basedomain.example.net</pre></li></ol></div></section><section class="section cluster-admin" id="nw-ingress-sharding-namespace-labels_configuring-ingress-cluster-traffic-ingress-controller"><div class="titlepage"><div><div><h3 class="title">29.3.6. Configuring Ingress Controller sharding by using namespace labels</h3></div></div></div><p class="cluster-admin cluster-admin">
					Ingress Controller sharding by using namespace labels means that the Ingress Controller serves any route in any namespace that is selected by the namespace selector.
				</p><div class="figure" id="idm140587109834624"><p class="title"><strong>Figure 29.2. Ingress sharding using namespace labels</strong></p><div class="figure-contents"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/afe594e8499d327fa587b8ca4f61a2cd/nw-sharding-namespace-labels.png" alt="A diagram showing multiple Ingress Controllers with different namespace selectors serving routes that belong to the namespace containing a label that matches a given namespace selector"/></div></div></div><p class="cluster-admin cluster-admin">
					Ingress Controller sharding is useful when balancing incoming traffic load among a set of Ingress Controllers and when isolating traffic to a specific Ingress Controller. For example, company A goes to one Ingress Controller and company B to another.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal cluster-admin">router-internal.yaml</code> file:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin"># cat router-internal.yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
items:
- apiVersion: operator.openshift.io/v1
  kind: IngressController
  metadata:
    name: sharded
    namespace: openshift-ingress-operator
  spec:
    domain: &lt;apps-sharded.basedomain.example.net&gt; <span id="CO192-1"><!--Empty--></span><span class="callout">1</span>
    nodePlacement:
      nodeSelector:
        matchLabels:
          node-role.kubernetes.io/worker: ""
    namespaceSelector:
      matchLabels:
        type: sharded
  status: {}
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""</pre>

							</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO192-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify a domain to be used by the Ingress Controller. This domain must be different from the default Ingress Controller domain.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Apply the Ingress Controller <code class="literal cluster-admin">router-internal.yaml</code> file:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin"># oc apply -f router-internal.yaml</pre><p class="cluster-admin cluster-admin">
							The Ingress Controller selects routes in any namespace that is selected by the namespace selector that have the label <code class="literal cluster-admin">type: sharded</code>.
						</p></li><li class="listitem"><p class="simpara">
							Create a new route using the domain configured in the <code class="literal cluster-admin">router-internal.yaml</code>:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc expose svc &lt;service-name&gt; --hostname &lt;route-name&gt;.apps-sharded.basedomain.example.net</pre></li></ol></div></section><section class="section cluster-admin" id="nw-ingress-sharding-route-configuration_configuring-ingress-cluster-traffic-ingress-controller"><div class="titlepage"><div><div><h3 class="title">29.3.7. Creating a route for Ingress Controller sharding</h3></div></div></div><p class="cluster-admin cluster-admin">
					A route allows you to host your application at a URL. In this case, the hostname is not set and the route uses a subdomain instead. When you specify a subdomain, you automatically use the domain of the Ingress Controller that exposes the route. For situations where a route is exposed by multiple Ingress Controllers, the route is hosted at multiple URLs.
				</p><p class="cluster-admin cluster-admin">
					The following procedure describes how to create a route for Ingress Controller sharding, using the <code class="literal cluster-admin">hello-openshift</code> application as an example.
				</p><p class="cluster-admin cluster-admin">
					Ingress Controller sharding is useful when balancing incoming traffic load among a set of Ingress Controllers and when isolating traffic to a specific Ingress Controller. For example, company A goes to one Ingress Controller and company B to another.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You are logged in as a project administrator.
						</li><li class="listitem">
							You have a web application that exposes a port and an HTTP or TLS endpoint listening for traffic on the port.
						</li><li class="listitem">
							You have configured the Ingress Controller for sharding.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a project called <code class="literal cluster-admin">hello-openshift</code> by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc new-project hello-openshift</pre></li><li class="listitem"><p class="simpara">
							Create a pod in the project by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f https://raw.githubusercontent.com/openshift/origin/master/examples/hello-openshift/hello-pod.json</pre></li><li class="listitem"><p class="simpara">
							Create a service called <code class="literal cluster-admin">hello-openshift</code> by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc expose pod/hello-openshift</pre></li><li class="listitem"><p class="simpara">
							Create a route definition called <code class="literal cluster-admin">hello-openshift-route.yaml</code>:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>YAML definition of the created route for sharding:</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    type: sharded <span id="CO193-1"><!--Empty--></span><span class="callout">1</span>
  name: hello-openshift-edge
  namespace: hello-openshift
spec:
  subdomain: hello-openshift <span id="CO193-2"><!--Empty--></span><span class="callout">2</span>
  tls:
    termination: edge
  to:
    kind: Service
    name: hello-openshift</pre>

							</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO193-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Both the label key and its corresponding label value must match the ones specified in the Ingress Controller. In this example, the Ingress Controller has the label key and value <code class="literal cluster-admin">type: sharded</code>.
								</div></dd><dt><a href="#CO193-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The route will be exposed using the value of the <code class="literal cluster-admin">subdomain</code> field. When you specify the <code class="literal cluster-admin">subdomain</code> field, you must leave the hostname unset. If you specify both the <code class="literal cluster-admin">host</code> and <code class="literal cluster-admin">subdomain</code> fields, then the route will use the value of the <code class="literal cluster-admin">host</code> field, and ignore the <code class="literal cluster-admin">subdomain</code> field.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Use <code class="literal cluster-admin">hello-openshift-route.yaml</code> to create a route to the <code class="literal cluster-admin">hello-openshift</code> application by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n hello-openshift create -f hello-openshift-route.yaml</pre></li></ol></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Get the status of the route with the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n hello-openshift get routes/hello-openshift-edge -o yaml</pre><p class="cluster-admin cluster-admin">
							The resulting <code class="literal cluster-admin">Route</code> resource should look similar to the following:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    type: sharded
  name: hello-openshift-edge
  namespace: hello-openshift
spec:
  subdomain: hello-openshift
  tls:
    termination: edge
  to:
    kind: Service
    name: hello-openshift
status:
  ingress:
  - host: hello-openshift.&lt;apps-sharded.basedomain.example.net&gt; <span id="CO194-1"><!--Empty--></span><span class="callout">1</span>
    routerCanonicalHostname: router-sharded.&lt;apps-sharded.basedomain.example.net&gt; <span id="CO194-2"><!--Empty--></span><span class="callout">2</span>
    routerName: sharded <span id="CO194-3"><!--Empty--></span><span class="callout">3</span></pre>

							</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO194-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The hostname the Ingress Controller, or router, uses to expose the route. The value of the <code class="literal cluster-admin">host</code> field is automatically determined by the Ingress Controller, and uses its domain. In this example, the domain of the Ingress Controller is <code class="literal cluster-admin">&lt;apps-sharded.basedomain.example.net&gt;</code>.
								</div></dd><dt><a href="#CO194-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The hostname of the Ingress Controller.
								</div></dd><dt><a href="#CO194-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The name of the Ingress Controller. In this example, the Ingress Controller has the name <code class="literal cluster-admin">sharded</code>.
								</div></dd></dl></div></li></ul></div></section><section class="section _additional-resources" id="additional-resources-5"><div class="titlepage"><div><div><h3 class="title">29.3.8. Additional resources</h3></div></div></div><p class="cluster-admin cluster-admin">
					The Ingress Operator manages wildcard DNS. For more information, see the following:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-ingress">Ingress Operator in OpenShift Container Platform</a>.
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-bare-metal">Installing a cluster on bare metal</a>.
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-vsphere">Installing a cluster on vSphere</a>.
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#about-network-policy">About network policy</a>.
						</li></ul></div></section></section><section class="section cluster-admin" id="configuring-ingress-cluster-traffic-load-balancer"><div class="titlepage"><div><div><h2 class="title">29.4. Configuring ingress cluster traffic using a load balancer</h2></div></div></div><p class="cluster-admin cluster-admin">
				OpenShift Container Platform provides methods for communicating from outside the cluster with services running in the cluster. This method uses a load balancer.
			</p><section class="section cluster-admin" id="nw-using-load-balancer-getting-traffic_configuring-ingress-cluster-traffic-load-balancer"><div class="titlepage"><div><div><h3 class="title">29.4.1. Using a load balancer to get traffic into the cluster</h3></div></div></div><p class="cluster-admin cluster-admin">
					If you do not need a specific external IP address, you can configure a load balancer service to allow external access to an OpenShift Container Platform cluster.
				</p><p class="cluster-admin cluster-admin">
					A load balancer service allocates a unique IP. The load balancer has a single edge router IP, which can be a virtual IP (VIP), but is still a single machine for initial load balancing.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						If a pool is configured, it is done at the infrastructure level, not by a cluster administrator.
					</p></div></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						The procedures in this section require prerequisites performed by the cluster administrator.
					</p></div></div></section><section class="section cluster-admin" id="prerequisites-4"><div class="titlepage"><div><div><h3 class="title">29.4.2. Prerequisites</h3></div></div></div><p class="cluster-admin cluster-admin">
					Before starting the following procedures, the administrator must:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Set up the external port to the cluster networking environment so that requests can reach the cluster.
						</li><li class="listitem"><p class="simpara">
							Make sure there is at least one user with cluster admin role. To add this role to a user, run the following command:
						</p><pre class="screen cluster-admin cluster-admin">$ oc adm policy add-cluster-role-to-user cluster-admin username</pre></li><li class="listitem">
							Have an OpenShift Container Platform cluster with at least one master and at least one node and a system outside the cluster that has network access to the cluster. This procedure assumes that the external system is on the same subnet as the cluster. The additional networking required for external systems on a different subnet is out-of-scope for this topic.
						</li></ul></div></section><section class="section cluster-admin" id="nw-creating-project-and-service_configuring-ingress-cluster-traffic-load-balancer"><div class="titlepage"><div><div><h3 class="title">29.4.3. Creating a project and service</h3></div></div></div><p class="cluster-admin cluster-admin">
					If the project and service that you want to expose do not exist, first create the project, then the service.
				</p><p class="cluster-admin cluster-admin">
					If the project and service already exist, skip to the procedure on exposing the service to create a route.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the <code class="literal cluster-admin">oc</code> CLI and log in as a cluster administrator.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a new project for your service by running the <code class="literal cluster-admin">oc new-project</code> command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc new-project myproject</pre></li><li class="listitem"><p class="simpara">
							Use the <code class="literal cluster-admin">oc new-app</code> command to create your service:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc new-app nodejs:12~https://github.com/sclorg/nodejs-ex.git</pre></li><li class="listitem"><p class="simpara">
							To verify that the service was created, run the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get svc -n myproject</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
nodejs-ex   ClusterIP   172.30.197.157   &lt;none&gt;        8080/TCP   70s</pre>

							</p></div><p class="cluster-admin cluster-admin">
							By default, the new service does not have an external IP address.
						</p></li></ol></div></section><section class="section cluster-admin" id="nw-exposing-service_configuring-ingress-cluster-traffic-load-balancer"><div class="titlepage"><div><div><h3 class="title">29.4.4. Exposing the service by creating a route</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can expose the service as a route by using the <code class="literal cluster-admin">oc expose</code> command.
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Procedure</strong></p><p>
						To expose the service:
					</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Log in to OpenShift Container Platform.
						</li><li class="listitem"><p class="simpara">
							Log in to the project where the service you want to expose is located:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc project myproject</pre></li><li class="listitem"><p class="simpara">
							Run the <code class="literal cluster-admin">oc expose service</code> command to expose the route:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc expose service nodejs-ex</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">route.route.openshift.io/nodejs-ex exposed</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							To verify that the service is exposed, you can use a tool, such as cURL, to make sure the service is accessible from outside the cluster.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Use the <code class="literal cluster-admin">oc get route</code> command to find the route’s host name:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get route</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME        HOST/PORT                        PATH   SERVICES    PORT       TERMINATION   WILDCARD
nodejs-ex   nodejs-ex-myproject.example.com         nodejs-ex   8080-tcp                 None</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Use cURL to check that the host responds to a GET request:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ curl --head nodejs-ex-myproject.example.com</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">HTTP/1.1 200 OK
...</pre>

									</p></div></li></ol></div></li></ol></div></section><section class="section cluster-admin" id="nw-create-load-balancer-service_configuring-ingress-cluster-traffic-load-balancer"><div class="titlepage"><div><div><h3 class="title">29.4.5. Creating a load balancer service</h3></div></div></div><p class="cluster-admin cluster-admin">
					Use the following procedure to create a load balancer service.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Make sure that the project and service you want to expose exist.
						</li><li class="listitem">
							Your cloud provider supports load balancers.
						</li></ul></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Procedure</strong></p><p>
						To create a load balancer service:
					</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Log in to OpenShift Container Platform.
						</li><li class="listitem"><p class="simpara">
							Load the project where the service you want to expose is located.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc project project1</pre></li><li class="listitem"><p class="simpara">
							Open a text file on the control plane node and paste the following text, editing the file as needed:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Sample load balancer configuration file</strong></p><p>
								
<pre class="screen">apiVersion: v1
kind: Service
metadata:
  name: egress-2 <span id="CO195-1"><!--Empty--></span><span class="callout">1</span>
spec:
  ports:
  - name: db
    port: 3306 <span id="CO195-2"><!--Empty--></span><span class="callout">2</span>
  loadBalancerIP:
  loadBalancerSourceRanges: <span id="CO195-3"><!--Empty--></span><span class="callout">3</span>
  - 10.0.0.0/8
  - 192.168.0.0/16
  type: LoadBalancer <span id="CO195-4"><!--Empty--></span><span class="callout">4</span>
  selector:
    name: mysql <span id="CO195-5"><!--Empty--></span><span class="callout">5</span></pre>

							</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO195-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Enter a descriptive name for the load balancer service.
								</div></dd><dt><a href="#CO195-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Enter the same port that the service you want to expose is listening on.
								</div></dd><dt><a href="#CO195-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Enter a list of specific IP addresses to restrict traffic through the load balancer. This field is ignored if the cloud-provider does not support the feature.
								</div></dd><dt><a href="#CO195-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Enter <code class="literal cluster-admin">Loadbalancer</code> as the type.
								</div></dd><dt><a href="#CO195-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Enter the name of the service.
								</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								To restrict the traffic through the load balancer to specific IP addresses, it is recommended to use the Ingress Controller field <code class="literal cluster-admin">spec.endpointPublishingStrategy.loadBalancer.allowedSourceRanges</code>. Do not set the <code class="literal cluster-admin">loadBalancerSourceRanges</code> field.
							</p></div></div></li><li class="listitem">
							Save and exit the file.
						</li><li class="listitem"><p class="simpara">
							Run the following command to create the service:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f &lt;file-name&gt;</pre><p class="cluster-admin cluster-admin">
							For example:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f mysql-lb.yaml</pre></li><li class="listitem"><p class="simpara">
							Execute the following command to view the new service:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get svc</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME       TYPE           CLUSTER-IP      EXTERNAL-IP                             PORT(S)          AGE
egress-2   LoadBalancer   172.30.22.226   ad42f5d8b303045-487804948.example.com   3306:30357/TCP   15m</pre>

							</p></div><p class="cluster-admin cluster-admin">
							The service has an external IP address automatically assigned if there is a cloud provider enabled.
						</p></li><li class="listitem"><p class="simpara">
							On the master, use a tool, such as cURL, to make sure you can reach the service using the public IP address:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ curl &lt;public-ip&gt;:&lt;port&gt;</pre><p class="cluster-admin cluster-admin">
							For example:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ curl 172.29.121.74:3306</pre><p class="cluster-admin cluster-admin">
							The examples in this section use a MySQL service, which requires a client application. If you get a string of characters with the <code class="literal cluster-admin">Got packets out of order</code> message, you are connecting with the service:
						</p><p class="cluster-admin cluster-admin">
							If you have a MySQL client, log in with the standard CLI command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ mysql -h 172.30.131.89 -u admin -p</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Enter password:
Welcome to the MariaDB monitor.  Commands end with ; or \g.

MySQL [(none)]&gt;</pre>

							</p></div></li></ol></div></section></section><section class="section cluster-admin" id="configuring-ingress-cluster-traffic-aws"><div class="titlepage"><div><div><h2 class="title">29.5. Configuring ingress cluster traffic on AWS</h2></div></div></div><p class="cluster-admin cluster-admin">
				OpenShift Container Platform provides methods for communicating from outside the cluster with services running in the cluster. This method uses load balancers on AWS, specifically a Network Load Balancer (NLB) or a Classic Load Balancer (CLB). Both types of load balancers can forward the client’s IP address to the node, but a CLB requires proxy protocol support, which OpenShift Container Platform automatically enables.
			</p><p class="cluster-admin cluster-admin">
				There are two ways to configure an Ingress Controller to use an NLB:
			</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
						By force replacing the Ingress Controller that is currently using a CLB. This deletes the <code class="literal cluster-admin">IngressController</code> object and an outage will occur while the new DNS records propagate and the NLB is being provisioned.
					</li><li class="listitem">
						By editing an existing Ingress Controller that uses a CLB to use an NLB. This changes the load balancer without having to delete and recreate the <code class="literal cluster-admin">IngressController</code> object.
					</li></ol></div><p class="cluster-admin cluster-admin">
				Both methods can be used to switch from an NLB to a CLB.
			</p><p class="cluster-admin cluster-admin">
				You can configure these load balancers on a new or existing AWS cluster.
			</p><section class="section cluster-admin" id="nw-configuring-elb-timeouts-aws-classic_configuring-ingress-cluster-traffic-aws"><div class="titlepage"><div><div><h3 class="title">29.5.1. Configuring Classic Load Balancer timeouts on AWS</h3></div></div></div><p class="cluster-admin cluster-admin">
					OpenShift Container Platform provides a method for setting a custom timeout period for a specific route or Ingress Controller. Additionally, an AWS Classic Load Balancer (CLB) has its own timeout period with a default time of 60 seconds.
				</p><p class="cluster-admin cluster-admin">
					If the timeout period of the CLB is shorter than the route timeout or Ingress Controller timeout, the load balancer can prematurely terminate the connection. You can prevent this problem by increasing both the timeout period of the route and CLB.
				</p><section class="section cluster-admin" id="nw-configuring-route-timeouts_configuring-ingress-cluster-traffic-aws"><div class="titlepage"><div><div><h4 class="title">29.5.1.1. Configuring route timeouts</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can configure the default timeouts for an existing route when you have services in need of a low timeout, which is required for Service Level Availability (SLA) purposes, or a high timeout, for cases with a slow back end.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								You need a deployed Ingress Controller on a running cluster.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Using the <code class="literal cluster-admin">oc annotate</code> command, add the timeout to the route:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate route &lt;route_name&gt; \
    --overwrite haproxy.router.openshift.io/timeout=&lt;timeout&gt;&lt;time_unit&gt; <span id="CO196-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO196-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Supported time units are microseconds (us), milliseconds (ms), seconds (s), minutes (m), hours (h), or days (d).
									</div></dd></dl></div><p class="cluster-admin cluster-admin">
								The following example sets a timeout of two seconds on a route named <code class="literal cluster-admin">myroute</code>:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc annotate route myroute --overwrite haproxy.router.openshift.io/timeout=2s</pre></li></ol></div></section><section class="section cluster-admin" id="nw-configuring-clb-timeouts_configuring-ingress-cluster-traffic-aws"><div class="titlepage"><div><div><h4 class="title">29.5.1.2. Configuring Classic Load Balancer timeouts</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can configure the default timeouts for a Classic Load Balancer (CLB) to extend idle connections.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								You must have a deployed Ingress Controller on a running cluster.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Set an AWS connection idle timeout of five minutes for the default <code class="literal cluster-admin">ingresscontroller</code> by running the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n openshift-ingress-operator patch ingresscontroller/default \
    --type=merge --patch='{"spec":{"endpointPublishingStrategy": \
    {"type":"LoadBalancerService", "loadBalancer": \
    {"scope":"External", "providerParameters":{"type":"AWS", "aws": \
    {"type":"Classic", "classicLoadBalancer": \
    {"connectionIdleTimeout":"5m"}}}}}}}'</pre></li><li class="listitem"><p class="simpara">
								Optional: Restore the default value of the timeout by running the following command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n openshift-ingress-operator patch ingresscontroller/default \
    --type=merge --patch='{"spec":{"endpointPublishingStrategy": \
    {"loadBalancer":{"providerParameters":{"aws":{"classicLoadBalancer": \
    {"connectionIdleTimeout":null}}}}}}}'</pre></li></ol></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							You must specify the <code class="literal cluster-admin">scope</code> field when you change the connection timeout value unless the current scope is already set. When you set the <code class="literal cluster-admin">scope</code> field, you do not need to do so again if you restore the default timeout value.
						</p></div></div></section></section><section class="section cluster-admin" id="nw-configuring-ingress-cluster-traffic-aws-network-load-balancer_configuring-ingress-cluster-traffic-aws"><div class="titlepage"><div><div><h3 class="title">29.5.2. Configuring ingress cluster traffic on AWS using a Network Load Balancer</h3></div></div></div><p class="cluster-admin cluster-admin">
					OpenShift Container Platform provides methods for communicating from outside the cluster with services that run in the cluster. One such method uses a Network Load Balancer (NLB). You can configure an NLB on a new or existing AWS cluster.
				</p><section class="section cluster-admin" id="nw-aws-switching-clb-with-nlb_configuring-ingress-cluster-traffic-aws"><div class="titlepage"><div><div><h4 class="title">29.5.2.1. Switching the Ingress Controller from using a Classic Load Balancer to a Network Load Balancer</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can switch the Ingress Controller that is using a Classic Load Balancer (CLB) to one that uses a Network Load Balancer (NLB) on AWS.
					</p><p class="cluster-admin cluster-admin">
						Switching between these load balancers will not delete the <code class="literal cluster-admin">IngressController</code> object.
					</p><div class="admonition warning cluster-admin"><div class="admonition_header">Warning</div><div><p class="cluster-admin cluster-admin">
							This procedure might cause the following issues:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									An outage that can last several minutes due to new DNS records propagation, new load balancers provisioning, and other factors. IP addresses and canonical names of the Ingress Controller load balancer might change after applying this procedure.
								</li><li class="listitem">
									Leaked load balancer resources due to a change in the annotation of the service.
								</li></ul></div></div></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Modify the existing Ingress Controller that you want to switch to using an NLB. This example assumes that your default Ingress Controller has an <code class="literal cluster-admin">External</code> scope and no other customizations:
							</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example <code class="literal cluster-admin">ingresscontroller.yaml</code> file</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: null
  name: default
  namespace: openshift-ingress-operator
spec:
  endpointPublishingStrategy:
    loadBalancer:
      scope: External
      providerParameters:
        type: AWS
        aws:
          type: NLB
    type: LoadBalancerService</pre>

								</p></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
									If you do not specify a value for the <code class="literal cluster-admin">spec.endpointPublishingStrategy.loadBalancer.providerParameters.aws.type</code> field, the Ingress Controller uses the <code class="literal cluster-admin">spec.loadBalancer.platform.aws.type</code> value from the cluster <code class="literal cluster-admin">Ingress</code> configuration that was set during installation.
								</p></div></div><div class="admonition tip cluster-admin"><div class="admonition_header">Tip</div><div><p class="cluster-admin cluster-admin">
								If your Ingress Controller has other customizations that you want to update, such as changing the domain, consider force replacing the Ingress Controller definition file instead.
							</p></div></div></li><li class="listitem"><p class="simpara">
								Apply the changes to the Ingress Controller YAML file by running the command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f ingresscontroller.yaml</pre><p class="cluster-admin cluster-admin">
								Expect several minutes of outages while the Ingress Controller updates.
							</p></li></ol></div></section><section class="section cluster-admin" id="nw-aws-switching-nlb-with-clb_configuring-ingress-cluster-traffic-aws"><div class="titlepage"><div><div><h4 class="title">29.5.2.2. Switching the Ingress Controller from using a Network Load Balancer to a Classic Load Balancer</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can switch the Ingress Controller that is using a Network Load Balancer (NLB) to one that uses a Classic Load Balancer (CLB) on AWS.
					</p><p class="cluster-admin cluster-admin">
						Switching between these load balancers will not delete the <code class="literal cluster-admin">IngressController</code> object.
					</p><div class="admonition warning cluster-admin"><div class="admonition_header">Warning</div><div><p class="cluster-admin cluster-admin">
							This procedure might cause an outage that can last several minutes due to new DNS records propagation, new load balancers provisioning, and other factors. IP addresses and canonical names of the Ingress Controller load balancer might change after applying this procedure.
						</p></div></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Modify the existing Ingress Controller that you want to switch to using a CLB. This example assumes that your default Ingress Controller has an <code class="literal cluster-admin">External</code> scope and no other customizations:
							</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example <code class="literal cluster-admin">ingresscontroller.yaml</code> file</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: null
  name: default
  namespace: openshift-ingress-operator
spec:
  endpointPublishingStrategy:
    loadBalancer:
      scope: External
      providerParameters:
        type: AWS
        aws:
          type: Classic
    type: LoadBalancerService</pre>

								</p></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
									If you do not specify a value for the <code class="literal cluster-admin">spec.endpointPublishingStrategy.loadBalancer.providerParameters.aws.type</code> field, the Ingress Controller uses the <code class="literal cluster-admin">spec.loadBalancer.platform.aws.type</code> value from the cluster <code class="literal cluster-admin">Ingress</code> configuration that was set during installation.
								</p></div></div><div class="admonition tip cluster-admin"><div class="admonition_header">Tip</div><div><p class="cluster-admin cluster-admin">
								If your Ingress Controller has other customizations that you want to update, such as changing the domain, consider force replacing the Ingress Controller definition file instead.
							</p></div></div></li><li class="listitem"><p class="simpara">
								Apply the changes to the Ingress Controller YAML file by running the command:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f ingresscontroller.yaml</pre><p class="cluster-admin cluster-admin">
								Expect several minutes of outages while the Ingress Controller updates.
							</p></li></ol></div></section><section class="section cluster-admin" id="nw-aws-replacing-clb-with-nlb_configuring-ingress-cluster-traffic-aws"><div class="titlepage"><div><div><h4 class="title">29.5.2.3. Replacing Ingress Controller Classic Load Balancer with Network Load Balancer</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can replace an Ingress Controller that is using a Classic Load Balancer (CLB) with one that uses a Network Load Balancer (NLB) on AWS.
					</p><div class="admonition warning cluster-admin"><div class="admonition_header">Warning</div><div><p class="cluster-admin cluster-admin">
							This procedure might cause the following issues:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									An outage that can last several minutes due to new DNS records propagation, new load balancers provisioning, and other factors. IP addresses and canonical names of the Ingress Controller load balancer might change after applying this procedure.
								</li><li class="listitem">
									Leaked load balancer resources due to a change in the annotation of the service.
								</li></ul></div></div></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create a file with a new default Ingress Controller. The following example assumes that your default Ingress Controller has an <code class="literal cluster-admin">External</code> scope and no other customizations:
							</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example <code class="literal cluster-admin">ingresscontroller.yml</code> file</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: null
  name: default
  namespace: openshift-ingress-operator
spec:
  endpointPublishingStrategy:
    loadBalancer:
      scope: External
      providerParameters:
        type: AWS
        aws:
          type: NLB
    type: LoadBalancerService</pre>

								</p></div><p class="cluster-admin cluster-admin">
								If your default Ingress Controller has other customizations, ensure that you modify the file accordingly.
							</p><div class="admonition tip cluster-admin"><div class="admonition_header">Tip</div><div><p class="cluster-admin cluster-admin">
								If your Ingress Controller has no other customizations and you are only updating the load balancer type, consider following the procedure detailed in "Switching the Ingress Controller from using a Classic Load Balancer to a Network Load Balancer".
							</p></div></div></li><li class="listitem"><p class="simpara">
								Force replace the Ingress Controller YAML file:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc replace --force --wait -f ingresscontroller.yml</pre><p class="cluster-admin cluster-admin">
								Wait until the Ingress Controller is replaced. Expect several of minutes of outages.
							</p></li></ol></div></section><section class="section cluster-admin" id="nw-aws-nlb-existing-cluster_configuring-ingress-cluster-traffic-aws"><div class="titlepage"><div><div><h4 class="title">29.5.2.4. Configuring an Ingress Controller Network Load Balancer on an existing AWS cluster</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can create an Ingress Controller backed by an AWS Network Load Balancer (NLB) on an existing cluster.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								You must have an installed AWS cluster.
							</li><li class="listitem"><p class="simpara">
								<code class="literal cluster-admin">PlatformStatus</code> of the infrastructure resource must be AWS.
							</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem"><p class="simpara">
										To verify that the <code class="literal cluster-admin">PlatformStatus</code> is AWS, run:
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.type}'
AWS</pre></li></ul></div></li></ul></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Procedure</strong></p><p>
							Create an Ingress Controller backed by an AWS NLB on an existing cluster.
						</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create the Ingress Controller manifest:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin"> $ cat ingresscontroller-aws-nlb.yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: $my_ingress_controller<span id="CO197-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-ingress-operator
spec:
  domain: $my_unique_ingress_domain<span id="CO197-2"><!--Empty--></span><span class="callout">2</span>
  endpointPublishingStrategy:
    type: LoadBalancerService
    loadBalancer:
      scope: External<span id="CO197-3"><!--Empty--></span><span class="callout">3</span>
      providerParameters:
        type: AWS
        aws:
          type: NLB</pre>

								</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO197-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal cluster-admin">$my_ingress_controller</code> with a unique name for the Ingress Controller.
									</div></dd><dt><a href="#CO197-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Replace <code class="literal cluster-admin">$my_unique_ingress_domain</code> with a domain name that is unique among all Ingress Controllers in the cluster. This variable must be a subdomain of the DNS name <code class="literal cluster-admin">&lt;clustername&gt;.&lt;domain&gt;</code>.
									</div></dd><dt><a href="#CO197-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										You can replace <code class="literal cluster-admin">External</code> with <code class="literal cluster-admin">Internal</code> to use an internal NLB.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create the resource in the cluster:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f ingresscontroller-aws-nlb.yaml</pre></li></ol></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
							Before you can configure an Ingress Controller NLB on a new AWS cluster, you must complete the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installation-initializing_installing-aws-network-customizations">Creating the installation configuration file</a> procedure.
						</p></div></div></section><section class="section cluster-admin" id="nw-aws-nlb-new-cluster_configuring-ingress-cluster-traffic-aws"><div class="titlepage"><div><div><h4 class="title">29.5.2.5. Configuring an Ingress Controller Network Load Balancer on a new AWS cluster</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can create an Ingress Controller backed by an AWS Network Load Balancer (NLB) on a new cluster.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Create the <code class="literal cluster-admin">install-config.yaml</code> file and complete any modifications to it.
							</li></ul></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Procedure</strong></p><p>
							Create an Ingress Controller backed by an AWS NLB on a new cluster.
						</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Change to the directory that contains the installation program and create the manifests:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ./openshift-install create manifests --dir &lt;installation_directory&gt; <span id="CO198-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO198-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										For <code class="literal cluster-admin">&lt;installation_directory&gt;</code>, specify the name of the directory that contains the <code class="literal cluster-admin">install-config.yaml</code> file for your cluster.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create a file that is named <code class="literal cluster-admin">cluster-ingress-default-ingresscontroller.yaml</code> in the <code class="literal cluster-admin">&lt;installation_directory&gt;/manifests/</code> directory:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ touch &lt;installation_directory&gt;/manifests/cluster-ingress-default-ingresscontroller.yaml <span id="CO199-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO199-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										For <code class="literal cluster-admin">&lt;installation_directory&gt;</code>, specify the directory name that contains the <code class="literal cluster-admin">manifests/</code> directory for your cluster.
									</div></dd></dl></div><p class="cluster-admin cluster-admin">
								After creating the file, several network configuration files are in the <code class="literal cluster-admin">manifests/</code> directory, as shown:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ls &lt;installation_directory&gt;/manifests/cluster-ingress-default-ingresscontroller.yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">cluster-ingress-default-ingresscontroller.yaml</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Open the <code class="literal cluster-admin">cluster-ingress-default-ingresscontroller.yaml</code> file in an editor and enter a custom resource (CR) that describes the Operator configuration you want:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: null
  name: default
  namespace: openshift-ingress-operator
spec:
  endpointPublishingStrategy:
    loadBalancer:
      scope: External
      providerParameters:
        type: AWS
        aws:
          type: NLB
    type: LoadBalancerService</pre></li><li class="listitem">
								Save the <code class="literal cluster-admin">cluster-ingress-default-ingresscontroller.yaml</code> file and quit the text editor.
							</li><li class="listitem">
								Optional: Back up the <code class="literal cluster-admin">manifests/cluster-ingress-default-ingresscontroller.yaml</code> file. The installation program deletes the <code class="literal cluster-admin">manifests/</code> directory when creating the cluster.
							</li></ol></div></section></section><section class="section _additional-resources" id="additional-resources_configuring-ingress-cluster-traffic-aws"><div class="titlepage"><div><div><h3 class="title">29.5.3. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#installing-aws-network-customizations">Installing a cluster on AWS with network customizations</a>.
						</li><li class="listitem">
							For more information on support for NLBs, see <a class="link" href="https://kubernetes.io/docs/concepts/services-networking/service/#aws-nlb-support">Network Load Balancer support on AWS</a>.
						</li><li class="listitem">
							For more information on proxy protocol support for CLBs, see <a class="link" href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-proxy-protocol.html">Configure proxy protocol support for your Classic Load Balancer</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="configuring-ingress-cluster-traffic-service-external-ip"><div class="titlepage"><div><div><h2 class="title">29.6. Configuring ingress cluster traffic for a service external IP</h2></div></div></div><p class="cluster-admin cluster-admin">
				You can attach an external IP address to a service so that it is available to traffic outside the cluster. This is generally useful only for a cluster installed on bare metal hardware. The external network infrastructure must be configured correctly to route traffic to the service.
			</p><section class="section cluster-admin" id="configuring-ingress-cluster-traffic-service-external-ip-prerequisites"><div class="titlepage"><div><div><h3 class="title">29.6.1. Prerequisites</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Your cluster is configured with ExternalIPs enabled. For more information, read <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-externalip">Configuring ExternalIPs for services</a>.
						</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								Do not use the same ExternalIP for the egress IP.
							</p></div></div></li></ul></div></section><section class="section cluster-admin" id="nw-service-externalip-create_configuring-ingress-cluster-traffic-service-external-ip"><div class="titlepage"><div><div><h3 class="title">29.6.2. Attaching an ExternalIP to a service</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can attach an ExternalIP to a service. If your cluster is configured to allocate an ExternalIP automatically, you might not need to manually attach an ExternalIP to the service.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Optional: To confirm what IP address ranges are configured for use with ExternalIP, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get networks.config cluster -o jsonpath='{.spec.externalIP}{"\n"}'</pre><p class="cluster-admin cluster-admin">
							If <code class="literal cluster-admin">autoAssignCIDRs</code> is set, OpenShift Container Platform automatically assigns an ExternalIP to a new <code class="literal cluster-admin">Service</code> object if the <code class="literal cluster-admin">spec.externalIPs</code> field is not specified.
						</p></li><li class="listitem"><p class="simpara">
							Attach an ExternalIP to the service.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									If you are creating a new service, specify the <code class="literal cluster-admin">spec.externalIPs</code> field and provide an array of one or more valid IP addresses. For example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Service
metadata:
  name: svc-with-externalip
spec:
  ...
  externalIPs:
  - 192.174.120.10</pre></li><li class="listitem"><p class="simpara">
									If you are attaching an ExternalIP to an existing service, enter the following command. Replace <code class="literal cluster-admin">&lt;name&gt;</code> with the service name. Replace <code class="literal cluster-admin">&lt;ip_address&gt;</code> with a valid ExternalIP address. You can provide multiple IP addresses separated by commas.
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch svc &lt;name&gt; -p \
  '{
    "spec": {
      "externalIPs": [ "&lt;ip_address&gt;" ]
    }
  }'</pre><p class="cluster-admin cluster-admin">
									For example:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch svc mysql-55-rhel7 -p '{"spec":{"externalIPs":["192.174.120.10"]}}'</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
										
<pre class="programlisting language-terminal">"mysql-55-rhel7" patched</pre>

									</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
							To confirm that an ExternalIP address is attached to the service, enter the following command. If you specified an ExternalIP for a new service, you must create the service first.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get svc</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME               CLUSTER-IP      EXTERNAL-IP     PORT(S)    AGE
mysql-55-rhel7     172.30.131.89   192.174.120.10  3306/TCP   13m</pre>

							</p></div></li></ol></div></section><section class="section _additional-resources" id="configuring-ingress-cluster-traffic-service-external-ip-additional-resources"><div class="titlepage"><div><div><h3 class="title">29.6.3. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-externalip">Configuring ExternalIPs for services</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="configuring-ingress-cluster-traffic-nodeport"><div class="titlepage"><div><div><h2 class="title">29.7. Configuring ingress cluster traffic using a NodePort</h2></div></div></div><p class="cluster-admin cluster-admin">
				OpenShift Container Platform provides methods for communicating from outside the cluster with services running in the cluster. This method uses a <code class="literal cluster-admin">NodePort</code>.
			</p><section class="section cluster-admin" id="nw-using-nodeport_configuring-ingress-cluster-traffic-nodeport"><div class="titlepage"><div><div><h3 class="title">29.7.1. Using a NodePort to get traffic into the cluster</h3></div></div></div><p class="cluster-admin cluster-admin">
					Use a <code class="literal cluster-admin">NodePort</code>-type <code class="literal cluster-admin">Service</code> resource to expose a service on a specific port on all nodes in the cluster. The port is specified in the <code class="literal cluster-admin">Service</code> resource’s <code class="literal cluster-admin">.spec.ports[*].nodePort</code> field.
				</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						Using a node port requires additional port resources.
					</p></div></div><p class="cluster-admin cluster-admin">
					A <code class="literal cluster-admin">NodePort</code> exposes the service on a static port on the node’s IP address. <code class="literal cluster-admin">NodePort</code>s are in the <code class="literal cluster-admin">30000</code> to <code class="literal cluster-admin">32767</code> range by default, which means a <code class="literal cluster-admin">NodePort</code> is unlikely to match a service’s intended port. For example, port <code class="literal cluster-admin">8080</code> may be exposed as port <code class="literal cluster-admin">31020</code> on the node.
				</p><p class="cluster-admin cluster-admin">
					The administrator must ensure the external IP addresses are routed to the nodes.
				</p><p class="cluster-admin cluster-admin">
					<code class="literal cluster-admin">NodePort</code>s and external IPs are independent and both can be used concurrently.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						The procedures in this section require prerequisites performed by the cluster administrator.
					</p></div></div></section><section class="section cluster-admin" id="prerequisites-5"><div class="titlepage"><div><div><h3 class="title">29.7.2. Prerequisites</h3></div></div></div><p class="cluster-admin cluster-admin">
					Before starting the following procedures, the administrator must:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Set up the external port to the cluster networking environment so that requests can reach the cluster.
						</li><li class="listitem"><p class="simpara">
							Make sure there is at least one user with cluster admin role. To add this role to a user, run the following command:
						</p><pre class="screen cluster-admin cluster-admin">$ oc adm policy add-cluster-role-to-user cluster-admin &lt;user_name&gt;</pre></li><li class="listitem">
							Have an OpenShift Container Platform cluster with at least one master and at least one node and a system outside the cluster that has network access to the cluster. This procedure assumes that the external system is on the same subnet as the cluster. The additional networking required for external systems on a different subnet is out-of-scope for this topic.
						</li></ul></div></section><section class="section cluster-admin" id="nw-creating-project-and-service_configuring-ingress-cluster-traffic-nodeport"><div class="titlepage"><div><div><h3 class="title">29.7.3. Creating a project and service</h3></div></div></div><p class="cluster-admin cluster-admin">
					If the project and service that you want to expose do not exist, first create the project, then the service.
				</p><p class="cluster-admin cluster-admin">
					If the project and service already exist, skip to the procedure on exposing the service to create a route.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the <code class="literal cluster-admin">oc</code> CLI and log in as a cluster administrator.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a new project for your service by running the <code class="literal cluster-admin">oc new-project</code> command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc new-project myproject</pre></li><li class="listitem"><p class="simpara">
							Use the <code class="literal cluster-admin">oc new-app</code> command to create your service:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc new-app nodejs:12~https://github.com/sclorg/nodejs-ex.git</pre></li><li class="listitem"><p class="simpara">
							To verify that the service was created, run the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get svc -n myproject</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
nodejs-ex   ClusterIP   172.30.197.157   &lt;none&gt;        8080/TCP   70s</pre>

							</p></div><p class="cluster-admin cluster-admin">
							By default, the new service does not have an external IP address.
						</p></li></ol></div></section><section class="section cluster-admin" id="nw-exposing-service_configuring-ingress-cluster-traffic-nodeport"><div class="titlepage"><div><div><h3 class="title">29.7.4. Exposing the service by creating a route</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can expose the service as a route by using the <code class="literal cluster-admin">oc expose</code> command.
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Procedure</strong></p><p>
						To expose the service:
					</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Log in to OpenShift Container Platform.
						</li><li class="listitem"><p class="simpara">
							Log in to the project where the service you want to expose is located:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc project myproject</pre></li><li class="listitem"><p class="simpara">
							To expose a node port for the application, modify the custom resource definition (CRD) of a service by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit svc &lt;service_name&gt;</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">spec:
  ports:
  - name: 8443-tcp
    nodePort: 30327 <span id="CO200-1"><!--Empty--></span><span class="callout">1</span>
    port: 8443
    protocol: TCP
    targetPort: 8443
  sessionAffinity: None
  type: NodePort <span id="CO200-2"><!--Empty--></span><span class="callout">2</span></pre>

							</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO200-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Optional: Specify the node port range for the application. By default, OpenShift Container Platform selects an available port in the <code class="literal cluster-admin">30000-32767</code> range.
								</div></dd><dt><a href="#CO200-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Define the service type.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Optional: To confirm the service is available with a node port exposed, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get svc -n myproject</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
nodejs-ex           ClusterIP   172.30.217.127   &lt;none&gt;        3306/TCP         9m44s
nodejs-ex-ingress   NodePort    172.30.107.72    &lt;none&gt;        3306:31345/TCP   39s</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Optional: To remove the service created automatically by the <code class="literal cluster-admin">oc new-app</code> command, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete svc nodejs-ex</pre></li></ol></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							To check that the service node port is updated with a port in the <code class="literal cluster-admin">30000-32767</code> range, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get svc</pre><p class="cluster-admin cluster-admin">
							In the following example output, the updated port is <code class="literal cluster-admin">30327</code>:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME    TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
httpd   NodePort   172.xx.xx.xx    &lt;none&gt;        8443:30327/TCP   109s</pre>

							</p></div></li></ul></div></section><section class="section _additional-resources" id="configuring-ingress-cluster-traffic-nodeport-additional-resources"><div class="titlepage"><div><div><h3 class="title">29.7.5. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#configuring-node-port-service-range">Configuring the node port service range</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="configuring-ingress-cluster-traffic-lb-allowed-source-ranges"><div class="titlepage"><div><div><h2 class="title">29.8. Configuring ingress cluster traffic using load balancer allowed source ranges</h2></div></div></div><p class="cluster-admin cluster-admin">
				You can specify a list of IP address ranges for the <code class="literal cluster-admin">IngressController</code>. This restricts access to the load balancer service when the <code class="literal cluster-admin">endpointPublishingStrategy</code> is <code class="literal cluster-admin">LoadBalancerService</code>.
			</p><section class="section cluster-admin" id="nw-configuring-lb-allowed-source-ranges_configuring-ingress-cluster-traffic-lb-allowed-source-ranges"><div class="titlepage"><div><div><h3 class="title">29.8.1. Configuring load balancer allowed source ranges</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can enable and configure the <code class="literal cluster-admin">spec.endpointPublishingStrategy.loadBalancer.allowedSourceRanges</code> field. By configuring load balancer allowed source ranges, you can limit the access to the load balancer for the Ingress Controller to a specified list of IP address ranges. The Ingress Operator reconciles the load balancer Service and sets the <code class="literal cluster-admin">spec.loadBalancerSourceRanges</code> field based on <code class="literal cluster-admin">AllowedSourceRanges</code>.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						If you have already set the <code class="literal cluster-admin">spec.loadBalancerSourceRanges</code> field or the load balancer service anotation <code class="literal cluster-admin">service.beta.kubernetes.io/load-balancer-source-ranges</code> in a previous version of OpenShift Container Platform, Ingress Controller starts reporting <code class="literal cluster-admin">Progressing=True</code> after an upgrade. To fix this, set <code class="literal cluster-admin">AllowedSourceRanges</code> that overwrites the <code class="literal cluster-admin">spec.loadBalancerSourceRanges</code> field and clears the <code class="literal cluster-admin">service.beta.kubernetes.io/load-balancer-source-ranges</code> annotation. Ingress Controller starts reporting <code class="literal cluster-admin">Progressing=False</code> again.
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have a deployed Ingress Controller on a running cluster.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Set the allowed source ranges API for the Ingress Controller by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n openshift-ingress-operator patch ingresscontroller/default \
    --type=merge --patch='{"spec":{"endpointPublishingStrategy": \
    {"loadBalancer":{"allowedSourceRanges":["0.0.0.0/0"]}}}}' <span id="CO201-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO201-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The example value <code class="literal cluster-admin">0.0.0.0/0</code> specifies the allowed source range.
								</div></dd></dl></div></li></ul></div></section><section class="section cluster-admin" id="nw-configuring-lb-allowed-source-ranges-migration_configuring-ingress-cluster-traffic-lb-allowed-source-ranges"><div class="titlepage"><div><div><h3 class="title">29.8.2. Migrating to load balancer allowed source ranges</h3></div></div></div><p class="cluster-admin cluster-admin">
					If you have already set the annotation <code class="literal cluster-admin">service.beta.kubernetes.io/load-balancer-source-ranges</code>, you can migrate to load balancer allowed source ranges. When you set the <code class="literal cluster-admin">AllowedSourceRanges</code>, the Ingress Controller sets the <code class="literal cluster-admin">spec.loadBalancerSourceRanges</code> field based on the <code class="literal cluster-admin">AllowedSourceRanges</code> value and unsets the <code class="literal cluster-admin">service.beta.kubernetes.io/load-balancer-source-ranges</code> annotation.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						If you have already set the <code class="literal cluster-admin">spec.loadBalancerSourceRanges</code> field or the load balancer service anotation <code class="literal cluster-admin">service.beta.kubernetes.io/load-balancer-source-ranges</code> in a previous version of OpenShift Container Platform, the Ingress Controller starts reporting <code class="literal cluster-admin">Progressing=True</code> after an upgrade. To fix this, set <code class="literal cluster-admin">AllowedSourceRanges</code> that overwrites the <code class="literal cluster-admin">spec.loadBalancerSourceRanges</code> field and clears the <code class="literal cluster-admin">service.beta.kubernetes.io/load-balancer-source-ranges</code> annotation. The Ingress Controller starts reporting <code class="literal cluster-admin">Progressing=False</code> again.
					</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have set the <code class="literal cluster-admin">service.beta.kubernetes.io/load-balancer-source-ranges</code> annotation.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Ensure that the <code class="literal cluster-admin">service.beta.kubernetes.io/load-balancer-source-ranges</code> is set:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get svc router-default -n openshift-ingress -o yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: v1
kind: Service
metadata:
  annotations:
    service.beta.kubernetes.io/load-balancer-source-ranges: 192.168.0.1/32</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Ensure that the <code class="literal cluster-admin">spec.loadBalancerSourceRanges</code> field is unset:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get svc router-default -n openshift-ingress -o yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">...
spec:
  loadBalancerSourceRanges:
  - 0.0.0.0/0
...</pre>

							</p></div></li><li class="listitem">
							Update your cluster to OpenShift Container Platform 4.13.
						</li><li class="listitem"><p class="simpara">
							Set the allowed source ranges API for the <code class="literal cluster-admin">ingresscontroller</code> by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n openshift-ingress-operator patch ingresscontroller/default \
    --type=merge --patch='{"spec":{"endpointPublishingStrategy": \
    {"loadBalancer":{"allowedSourceRanges":["0.0.0.0/0"]}}}}' <span id="CO202-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO202-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The example value <code class="literal cluster-admin">0.0.0.0/0</code> specifies the allowed source range.
								</div></dd></dl></div></li></ol></div></section><section class="section _additional-resources" id="additional-resources-6"><div class="titlepage"><div><div><h3 class="title">29.8.3. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/updating_clusters/#index">Updating your cluster</a>
						</li></ul></div></section></section></section><section class="chapter cluster-admin" id="kubernetes-nmstate"><div class="titlepage"><div><div><h1 class="title">Chapter 30. Kubernetes NMState</h1></div></div></div><section class="section cluster-admin" id="k8s-nmstate-about-the-k8s-nmstate-operator"><div class="titlepage"><div><div><h2 class="title">30.1. About the Kubernetes NMState Operator</h2></div></div></div><p class="cluster-admin cluster-admin">
				The Kubernetes NMState Operator provides a Kubernetes API for performing state-driven network configuration across the OpenShift Container Platform cluster’s nodes with NMState. The Kubernetes NMState Operator provides users with functionality to configure various network interface types, DNS, and routing on cluster nodes. Additionally, the daemons on the cluster nodes periodically report on the state of each node’s network interfaces to the API server.
			</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
					Red Hat supports the Kubernetes NMState Operator in production environments on bare-metal, IBM Power, IBM Z, IBM® LinuxONE, VMware vSphere, and OpenStack installations.
				</p></div></div><p class="cluster-admin cluster-admin">
				Before you can use NMState with OpenShift Container Platform, you must install the Kubernetes NMState Operator.
			</p><section class="section cluster-admin" id="installing-the-kubernetes-nmstate-operator-cli"><div class="titlepage"><div><div><h3 class="title">30.1.1. Installing the Kubernetes NMState Operator</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can install the Kubernetes NMState Operator by using the web console or the CLI.
				</p><section class="section cluster-admin" id="installing-the-kubernetes-nmstate-operator-web-console_k8s-nmstate-operator"><div class="titlepage"><div><div><h4 class="title">30.1.1.1. Installing the Kubernetes NMState Operator using the web console</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can install the Kubernetes NMState Operator by using the web console. After it is installed, the Operator can deploy the NMState State Controller as a daemon set across all of the cluster nodes.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								You are logged in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
								Select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">OperatorHub</span></strong></span>.
							</li><li class="listitem">
								In the search field below <span class="strong strong"><strong><span class="cluster-admin cluster-admin">All Items</span></strong></span>, enter <code class="literal cluster-admin">nmstate</code> and click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Enter</span></strong></span> to search for the Kubernetes NMState Operator.
							</li><li class="listitem">
								Click on the Kubernetes NMState Operator search result.
							</li><li class="listitem">
								Click on <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Install</span></strong></span> to open the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Install Operator</span></strong></span> window.
							</li><li class="listitem">
								Click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Install</span></strong></span> to install the Operator.
							</li><li class="listitem">
								After the Operator finishes installing, click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">View Operator</span></strong></span>.
							</li><li class="listitem">
								Under <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Provided APIs</span></strong></span>, click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Create Instance</span></strong></span> to open the dialog box for creating an instance of <code class="literal cluster-admin">kubernetes-nmstate</code>.
							</li><li class="listitem"><p class="simpara">
								In the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Name</span></strong></span> field of the dialog box, ensure the name of the instance is <code class="literal cluster-admin">nmstate.</code>
							</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
									The name restriction is a known issue. The instance is a singleton for the entire cluster.
								</p></div></div></li><li class="listitem">
								Accept the default settings and click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Create</span></strong></span> to create the instance.
							</li></ol></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Summary</strong></p><p>
							Once complete, the Operator has deployed the NMState State Controller as a daemon set across all of the cluster nodes.
						</p></div></section><section class="section cluster-admin" id="installing-the-kubernetes-nmstate-operator-CLI_k8s-nmstate-operator"><div class="titlepage"><div><div><h4 class="title">30.1.1.2. Installing the Kubernetes NMState Operator using the CLI</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can install the Kubernetes NMState Operator by using the OpenShift CLI (<code class="literal cluster-admin">oc)</code>. After it is installed, the Operator can deploy the NMState State Controller as a daemon set across all of the cluster nodes.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								You are logged in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create the <code class="literal cluster-admin">nmstate</code> Operator namespace:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: v1
kind: Namespace
metadata:
  labels:
    kubernetes.io/metadata.name: openshift-nmstate
    name: openshift-nmstate
  name: openshift-nmstate
spec:
  finalizers:
  - kubernetes
EOF</pre></li><li class="listitem"><p class="simpara">
								Create the <code class="literal cluster-admin">OperatorGroup</code>:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  annotations:
    olm.providedAPIs: NMState.v1.nmstate.io
  name: openshift-nmstate
  namespace: openshift-nmstate
spec:
  targetNamespaces:
  - openshift-nmstate
EOF</pre></li><li class="listitem"><p class="simpara">
								Subscribe to the <code class="literal cluster-admin">nmstate</code> Operator:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat &lt;&lt; EOF| oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  labels:
    operators.coreos.com/kubernetes-nmstate-operator.openshift-nmstate: ""
  name: kubernetes-nmstate-operator
  namespace: openshift-nmstate
spec:
  channel: stable
  installPlanApproval: Automatic
  name: kubernetes-nmstate-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF</pre></li><li class="listitem"><p class="simpara">
								Create instance of the <code class="literal cluster-admin">nmstate</code> operator:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: nmstate.io/v1
kind: NMState
metadata:
  name: nmstate
EOF</pre></li></ol></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								Confirm that the deployment for the <code class="literal cluster-admin">nmstate</code> operator is running:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">oc get clusterserviceversion -n openshift-nmstate \
 -o custom-columns=Name:.metadata.name,Phase:.status.phase</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">Name                                             Phase
kubernetes-nmstate-operator.4.13.0-202210210157   Succeeded</pre>

								</p></div></li></ul></div></section></section></section><section class="section cluster-admin" id="k8s-nmstate-observing-node-network-state"><div class="titlepage"><div><div><h2 class="title">30.2. Observing node network state</h2></div></div></div><p class="cluster-admin cluster-admin">
				Node network state is the network configuration for all nodes in the cluster.
			</p><section class="section cluster-admin" id="virt-about-nmstate_k8s-nmstate-observing-node-network-state"><div class="titlepage"><div><div><h3 class="title">30.2.1. About nmstate</h3></div></div></div><p class="cluster-admin cluster-admin">
					OpenShift Container Platform uses <a class="link" href="https://nmstate.github.io/"><code class="literal cluster-admin">nmstate</code></a> to report on and configure the state of the node network. This makes it possible to modify network policy configuration, such as by creating a Linux bridge on all nodes, by applying a single configuration manifest to the cluster.
				</p><p class="cluster-admin cluster-admin">
					Node networking is monitored and updated by the following objects:
				</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">NodeNetworkState</code></span></dt><dd>
								Reports the state of the network on that node.
							</dd><dt><span class="term"><code class="literal cluster-admin">NodeNetworkConfigurationPolicy</code></span></dt><dd>
								Describes the requested network configuration on nodes. You update the node network configuration, including adding and removing interfaces, by applying a <code class="literal cluster-admin">NodeNetworkConfigurationPolicy</code> manifest to the cluster.
							</dd><dt><span class="term"><code class="literal cluster-admin">NodeNetworkConfigurationEnactment</code></span></dt><dd>
								Reports the network policies enacted upon each node.
							</dd></dl></div><p class="cluster-admin cluster-admin">
					OpenShift Container Platform supports the use of the following nmstate interface types:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Linux Bridge
						</li><li class="listitem">
							VLAN
						</li><li class="listitem">
							Bond
						</li><li class="listitem">
							Ethernet
						</li></ul></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						If your OpenShift Container Platform cluster uses OVN-Kubernetes as the network plugin, you cannot attach a Linux bridge or bonding to the default interface of a host because of a change in the host network topology of OVN-Kubernetes. As a workaround, use a secondary network interface connected to your host or switch to the OpenShift SDN network plugin.
					</p></div></div></section><section class="section cluster-admin" id="virt-viewing-network-state-of-node_k8s-nmstate-observing-node-network-state"><div class="titlepage"><div><div><h3 class="title">30.2.2. Viewing the network state of a node</h3></div></div></div><p class="cluster-admin cluster-admin">
					A <code class="literal cluster-admin">NodeNetworkState</code> object exists on every node in the cluster. This object is periodically updated and captures the state of the network for that node.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							List all the <code class="literal cluster-admin">NodeNetworkState</code> objects in the cluster:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get nns</pre></li><li class="listitem"><p class="simpara">
							Inspect a <code class="literal cluster-admin">NodeNetworkState</code> object to view the network on that node. The output in this example has been redacted for clarity:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get nns node01 -o yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: nmstate.io/v1
kind: NodeNetworkState
metadata:
  name: node01 <span id="CO203-1"><!--Empty--></span><span class="callout">1</span>
status:
  currentState: <span id="CO203-2"><!--Empty--></span><span class="callout">2</span>
    dns-resolver:
# ...
    interfaces:
# ...
    route-rules:
# ...
    routes:
# ...
  lastSuccessfulUpdateTime: "2020-01-31T12:14:00Z" <span id="CO203-3"><!--Empty--></span><span class="callout">3</span></pre>

							</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO203-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the <code class="literal cluster-admin">NodeNetworkState</code> object is taken from the node.
								</div></dd><dt><a href="#CO203-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The <code class="literal cluster-admin">currentState</code> contains the complete network configuration for the node, including DNS, interfaces, and routes.
								</div></dd><dt><a href="#CO203-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Timestamp of the last successful update. This is updated periodically as long as the node is reachable and can be used to evalute the freshness of the report.
								</div></dd></dl></div></li></ol></div></section></section><section class="section cluster-admin" id="k8s-nmstate-updating-node-network-config"><div class="titlepage"><div><div><h2 class="title">30.3. Updating node network configuration</h2></div></div></div><p class="cluster-admin cluster-admin">
				You can update the node network configuration, such as adding or removing interfaces from nodes, by applying <code class="literal cluster-admin">NodeNetworkConfigurationPolicy</code> manifests to the cluster.
			</p><section class="section cluster-admin" id="virt-about-nmstate_k8s_nmstate-updating-node-network-config"><div class="titlepage"><div><div><h3 class="title">30.3.1. About nmstate</h3></div></div></div><p class="cluster-admin cluster-admin">
					OpenShift Container Platform uses <a class="link" href="https://nmstate.github.io/"><code class="literal cluster-admin">nmstate</code></a> to report on and configure the state of the node network. This makes it possible to modify network policy configuration, such as by creating a Linux bridge on all nodes, by applying a single configuration manifest to the cluster.
				</p><p class="cluster-admin cluster-admin">
					Node networking is monitored and updated by the following objects:
				</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">NodeNetworkState</code></span></dt><dd>
								Reports the state of the network on that node.
							</dd><dt><span class="term"><code class="literal cluster-admin">NodeNetworkConfigurationPolicy</code></span></dt><dd>
								Describes the requested network configuration on nodes. You update the node network configuration, including adding and removing interfaces, by applying a <code class="literal cluster-admin">NodeNetworkConfigurationPolicy</code> manifest to the cluster.
							</dd><dt><span class="term"><code class="literal cluster-admin">NodeNetworkConfigurationEnactment</code></span></dt><dd>
								Reports the network policies enacted upon each node.
							</dd></dl></div><p class="cluster-admin cluster-admin">
					OpenShift Container Platform supports the use of the following nmstate interface types:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Linux Bridge
						</li><li class="listitem">
							VLAN
						</li><li class="listitem">
							Bond
						</li><li class="listitem">
							Ethernet
						</li></ul></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						If your OpenShift Container Platform cluster uses OVN-Kubernetes as the network plugin, you cannot attach a Linux bridge or bonding to the default interface of a host because of a change in the host network topology of OVN-Kubernetes. As a workaround, use a secondary network interface connected to your host or switch to the OpenShift SDN network plugin.
					</p></div></div></section><section class="section cluster-admin" id="virt-creating-interface-on-nodes_k8s_nmstate-updating-node-network-config"><div class="titlepage"><div><div><h3 class="title">30.3.2. Creating an interface on nodes</h3></div></div></div><p class="cluster-admin cluster-admin">
					Create an interface on nodes in the cluster by applying a <code class="literal cluster-admin">NodeNetworkConfigurationPolicy</code> manifest to the cluster. The manifest details the requested configuration for the interface.
				</p><p class="cluster-admin cluster-admin">
					By default, the manifest applies to all nodes in the cluster. To add the interface to specific nodes, add the <code class="literal cluster-admin">spec: nodeSelector</code> parameter and the appropriate <code class="literal cluster-admin">&lt;key&gt;:&lt;value&gt;</code> for your node selector.
				</p><p class="cluster-admin cluster-admin">
					You can configure multiple nmstate-enabled nodes concurrently. The configuration applies to 50% of the nodes in parallel. This strategy prevents the entire cluster from being unavailable if the network connection fails. To apply the policy configuration in parallel to a specific portion of the cluster, use the <code class="literal cluster-admin">maxUnavailable</code> field.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create the <code class="literal cluster-admin">NodeNetworkConfigurationPolicy</code> manifest. The following example configures a Linux bridge on all worker nodes and configures the DNS resolver:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: br1-eth1-policy <span id="CO204-1"><!--Empty--></span><span class="callout">1</span>
spec:
  nodeSelector: <span id="CO204-2"><!--Empty--></span><span class="callout">2</span>
    node-role.kubernetes.io/worker: "" <span id="CO204-3"><!--Empty--></span><span class="callout">3</span>
  maxUnavailable: 3 <span id="CO204-4"><!--Empty--></span><span class="callout">4</span>
  desiredState:
    interfaces:
      - name: br1
        description: Linux bridge with eth1 as a port <span id="CO204-5"><!--Empty--></span><span class="callout">5</span>
        type: linux-bridge
        state: up
        ipv4:
          dhcp: true
          enabled: true
          auto-dns: false
        bridge:
          options:
            stp:
              enabled: false
          port:
            - name: eth1
    dns-resolver: <span id="CO204-6"><!--Empty--></span><span class="callout">6</span>
      config:
        search:
        - example.com
        - example.org
        server:
        - 8.8.8.8</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO204-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Name of the policy.
								</div></dd><dt><a href="#CO204-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Optional: If you do not include the <code class="literal cluster-admin">nodeSelector</code> parameter, the policy applies to all nodes in the cluster.
								</div></dd><dt><a href="#CO204-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									This example uses the <code class="literal cluster-admin">node-role.kubernetes.io/worker: ""</code> node selector to select all worker nodes in the cluster.
								</div></dd><dt><a href="#CO204-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Optional: Specifies the maximum number of nmstate-enabled nodes that the policy configuration can be applied to concurrently. This parameter can be set to either a percentage value (string), for example, <code class="literal cluster-admin">"10%"</code>, or an absolute value (number), such as <code class="literal cluster-admin">3</code>.
								</div></dd><dt><a href="#CO204-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Optional: Human-readable description for the interface.
								</div></dd><dt><a href="#CO204-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Optional: Specifies the search and server settings for the DNS server.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the node network policy:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f br1-eth1-policy.yaml <span id="CO205-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO205-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									File name of the node network configuration policy manifest.
								</div></dd></dl></div></li></ol></div><h4 id="additional-resources-7">Additional resources</h4><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="#virt-example-nmstate-multiple-interfaces_k8s_nmstate-updating-node-network-config" title="30.3.5.5. Example: Multiple interfaces in the same node network configuration policy">Example for creating multiple interfaces in the same policy</a>
						</li><li class="listitem">
							<a class="link" href="#virt-example-nmstate-IP-management_k8s_nmstate-updating-node-network-config" title="30.3.7. Examples: IP management">Examples of different IP management methods in policies</a>
						</li></ul></div></section><section class="section cluster-admin" id="virt-confirming-policy-updates-on-nodes_k8s_nmstate-updating-node-network-config"><div class="titlepage"><div><div><h3 class="title">30.3.3. Confirming node network policy updates on nodes</h3></div></div></div><p class="cluster-admin cluster-admin">
					A <code class="literal cluster-admin">NodeNetworkConfigurationPolicy</code> manifest describes your requested network configuration for nodes in the cluster. The node network policy includes your requested network configuration and the status of execution of the policy on the cluster as a whole.
				</p><p class="cluster-admin cluster-admin">
					When you apply a node network policy, a <code class="literal cluster-admin">NodeNetworkConfigurationEnactment</code> object is created for every node in the cluster. The node network configuration enactment is a read-only object that represents the status of execution of the policy on that node. If the policy fails to be applied on the node, the enactment for that node includes a traceback for troubleshooting.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							To confirm that a policy has been applied to the cluster, list the policies and their status:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get nncp</pre></li><li class="listitem"><p class="simpara">
							Optional: If a policy is taking longer than expected to successfully configure, you can inspect the requested state and status conditions of a particular policy:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get nncp &lt;policy&gt; -o yaml</pre></li><li class="listitem"><p class="simpara">
							Optional: If a policy is taking longer than expected to successfully configure on all nodes, you can list the status of the enactments on the cluster:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get nnce</pre></li><li class="listitem"><p class="simpara">
							Optional: To view the configuration of a particular enactment, including any error reporting for a failed configuration:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get nnce &lt;node&gt;.&lt;policy&gt; -o yaml</pre></li></ol></div></section><section class="section cluster-admin" id="virt-removing-interface-from-nodes_k8s_nmstate-updating-node-network-config"><div class="titlepage"><div><div><h3 class="title">30.3.4. Removing an interface from nodes</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can remove an interface from one or more nodes in the cluster by editing the <code class="literal cluster-admin">NodeNetworkConfigurationPolicy</code> object and setting the <code class="literal cluster-admin">state</code> of the interface to <code class="literal cluster-admin">absent</code>.
				</p><p class="cluster-admin cluster-admin">
					Removing an interface from a node does not automatically restore the node network configuration to a previous state. If you want to restore the previous state, you will need to define that node network configuration in the policy.
				</p><p class="cluster-admin cluster-admin">
					If you remove a bridge or bonding interface, any node NICs in the cluster that were previously attached or subordinate to that bridge or bonding interface are placed in a <code class="literal cluster-admin">down</code> state and become unreachable. To avoid losing connectivity, configure the node NIC in the same policy so that it has a status of <code class="literal cluster-admin">up</code> and either DHCP or a static IP address.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						Deleting the node network policy that added an interface does not change the configuration of the policy on the node. Although a <code class="literal cluster-admin">NodeNetworkConfigurationPolicy</code> is an object in the cluster, it only represents the requested configuration.<br/> Similarly, removing an interface does not delete the policy.
					</p></div></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Update the <code class="literal cluster-admin">NodeNetworkConfigurationPolicy</code> manifest used to create the interface. The following example removes a Linux bridge and configures the <code class="literal cluster-admin">eth1</code> NIC with DHCP to avoid losing connectivity:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: &lt;br1-eth1-policy&gt; <span id="CO206-1"><!--Empty--></span><span class="callout">1</span>
spec:
  nodeSelector: <span id="CO206-2"><!--Empty--></span><span class="callout">2</span>
    node-role.kubernetes.io/worker: "" <span id="CO206-3"><!--Empty--></span><span class="callout">3</span>
  desiredState:
    interfaces:
    - name: br1
      type: linux-bridge
      state: absent <span id="CO206-4"><!--Empty--></span><span class="callout">4</span>
    - name: eth1 <span id="CO206-5"><!--Empty--></span><span class="callout">5</span>
      type: ethernet <span id="CO206-6"><!--Empty--></span><span class="callout">6</span>
      state: up <span id="CO206-7"><!--Empty--></span><span class="callout">7</span>
      ipv4:
        dhcp: true <span id="CO206-8"><!--Empty--></span><span class="callout">8</span>
        enabled: true <span id="CO206-9"><!--Empty--></span><span class="callout">9</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO206-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Name of the policy.
								</div></dd><dt><a href="#CO206-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Optional: If you do not include the <code class="literal cluster-admin">nodeSelector</code> parameter, the policy applies to all nodes in the cluster.
								</div></dd><dt><a href="#CO206-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									This example uses the <code class="literal cluster-admin">node-role.kubernetes.io/worker: ""</code> node selector to select all worker nodes in the cluster.
								</div></dd><dt><a href="#CO206-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Changing the state to <code class="literal cluster-admin">absent</code> removes the interface.
								</div></dd><dt><a href="#CO206-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									The name of the interface that is to be unattached from the bridge interface.
								</div></dd><dt><a href="#CO206-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									The type of interface. This example creates an Ethernet networking interface.
								</div></dd><dt><a href="#CO206-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									The requested state for the interface.
								</div></dd><dt><a href="#CO206-8"><span class="callout">8</span></a> </dt><dd><div class="para">
									Optional: If you do not use <code class="literal cluster-admin">dhcp</code>, you can either set a static IP or leave the interface without an IP address.
								</div></dd><dt><a href="#CO206-9"><span class="callout">9</span></a> </dt><dd><div class="para">
									Enables <code class="literal cluster-admin">ipv4</code> in this example.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Update the policy on the node and remove the interface:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f &lt;br1-eth1-policy.yaml&gt; <span id="CO207-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO207-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									File name of the policy manifest.
								</div></dd></dl></div></li></ol></div></section><section class="section cluster-admin" id="virt-nmstate-example-policy-configurations"><div class="titlepage"><div><div><h3 class="title">30.3.5. Example policy configurations for different interfaces</h3></div></div></div><section class="section cluster-admin" id="virt-example-bridge-nncp_k8s_nmstate-updating-node-network-config"><div class="titlepage"><div><div><h4 class="title">30.3.5.1. Example: Linux bridge interface node network configuration policy</h4></div></div></div><p class="cluster-admin cluster-admin">
						Create a Linux bridge interface on nodes in the cluster by applying a <code class="literal cluster-admin">NodeNetworkConfigurationPolicy</code> manifest to the cluster.
					</p><p class="cluster-admin cluster-admin">
						The following YAML file is an example of a manifest for a Linux bridge interface. It includes samples values that you must replace with your own information.
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: br1-eth1-policy <span id="CO208-1"><!--Empty--></span><span class="callout">1</span>
spec:
  nodeSelector: <span id="CO208-2"><!--Empty--></span><span class="callout">2</span>
    kubernetes.io/hostname: &lt;node01&gt; <span id="CO208-3"><!--Empty--></span><span class="callout">3</span>
  desiredState:
    interfaces:
      - name: br1 <span id="CO208-4"><!--Empty--></span><span class="callout">4</span>
        description: Linux bridge with eth1 as a port <span id="CO208-5"><!--Empty--></span><span class="callout">5</span>
        type: linux-bridge <span id="CO208-6"><!--Empty--></span><span class="callout">6</span>
        state: up <span id="CO208-7"><!--Empty--></span><span class="callout">7</span>
        ipv4:
          dhcp: true <span id="CO208-8"><!--Empty--></span><span class="callout">8</span>
          enabled: true <span id="CO208-9"><!--Empty--></span><span class="callout">9</span>
        bridge:
          options:
            stp:
              enabled: false <span id="CO208-10"><!--Empty--></span><span class="callout">10</span>
          port:
            - name: eth1 <span id="CO208-11"><!--Empty--></span><span class="callout">11</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO208-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Name of the policy.
							</div></dd><dt><a href="#CO208-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Optional: If you do not include the <code class="literal cluster-admin">nodeSelector</code> parameter, the policy applies to all nodes in the cluster.
							</div></dd><dt><a href="#CO208-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								This example uses a <code class="literal cluster-admin">hostname</code> node selector.
							</div></dd><dt><a href="#CO208-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Name of the interface.
							</div></dd><dt><a href="#CO208-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Optional: Human-readable description of the interface.
							</div></dd><dt><a href="#CO208-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								The type of interface. This example creates a bridge.
							</div></dd><dt><a href="#CO208-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								The requested state for the interface after creation.
							</div></dd><dt><a href="#CO208-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								Optional: If you do not use <code class="literal cluster-admin">dhcp</code>, you can either set a static IP or leave the interface without an IP address.
							</div></dd><dt><a href="#CO208-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								Enables <code class="literal cluster-admin">ipv4</code> in this example.
							</div></dd><dt><a href="#CO208-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								Disables <code class="literal cluster-admin">stp</code> in this example.
							</div></dd><dt><a href="#CO208-11"><span class="callout">11</span></a> </dt><dd><div class="para">
								The node NIC to which the bridge attaches.
							</div></dd></dl></div></section><section class="section cluster-admin" id="virt-example-vlan-nncp_k8s_nmstate-updating-node-network-config"><div class="titlepage"><div><div><h4 class="title">30.3.5.2. Example: VLAN interface node network configuration policy</h4></div></div></div><p class="cluster-admin cluster-admin">
						Create a VLAN interface on nodes in the cluster by applying a <code class="literal cluster-admin">NodeNetworkConfigurationPolicy</code> manifest to the cluster.
					</p><p class="cluster-admin cluster-admin">
						The following YAML file is an example of a manifest for a VLAN interface. It includes samples values that you must replace with your own information.
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: vlan-eth1-policy <span id="CO209-1"><!--Empty--></span><span class="callout">1</span>
spec:
  nodeSelector: <span id="CO209-2"><!--Empty--></span><span class="callout">2</span>
    kubernetes.io/hostname: &lt;node01&gt; <span id="CO209-3"><!--Empty--></span><span class="callout">3</span>
  desiredState:
    interfaces:
    - name: eth1.102 <span id="CO209-4"><!--Empty--></span><span class="callout">4</span>
      description: VLAN using eth1 <span id="CO209-5"><!--Empty--></span><span class="callout">5</span>
      type: vlan <span id="CO209-6"><!--Empty--></span><span class="callout">6</span>
      state: up <span id="CO209-7"><!--Empty--></span><span class="callout">7</span>
      vlan:
        base-iface: eth1 <span id="CO209-8"><!--Empty--></span><span class="callout">8</span>
        id: 102 <span id="CO209-9"><!--Empty--></span><span class="callout">9</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO209-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Name of the policy.
							</div></dd><dt><a href="#CO209-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Optional: If you do not include the <code class="literal cluster-admin">nodeSelector</code> parameter, the policy applies to all nodes in the cluster.
							</div></dd><dt><a href="#CO209-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								This example uses a <code class="literal cluster-admin">hostname</code> node selector.
							</div></dd><dt><a href="#CO209-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Name of the interface.
							</div></dd><dt><a href="#CO209-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Optional: Human-readable description of the interface.
							</div></dd><dt><a href="#CO209-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								The type of interface. This example creates a VLAN.
							</div></dd><dt><a href="#CO209-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								The requested state for the interface after creation.
							</div></dd><dt><a href="#CO209-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								The node NIC to which the VLAN is attached.
							</div></dd><dt><a href="#CO209-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								The VLAN tag.
							</div></dd></dl></div></section><section class="section cluster-admin" id="virt-example-bond-nncp_k8s_nmstate-updating-node-network-config"><div class="titlepage"><div><div><h4 class="title">30.3.5.3. Example: Bond interface node network configuration policy</h4></div></div></div><p class="cluster-admin cluster-admin">
						Create a bond interface on nodes in the cluster by applying a <code class="literal cluster-admin">NodeNetworkConfigurationPolicy</code> manifest to the cluster.
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							OpenShift Container Platform only supports the following bond modes:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									mode=1 active-backup<br/>
								</li><li class="listitem">
									mode=2 balance-xor<br/>
								</li><li class="listitem">
									mode=4 802.3ad<br/>
								</li><li class="listitem">
									mode=5 balance-tlb<br/>
								</li><li class="listitem">
									mode=6 balance-alb
								</li></ul></div></div></div><p class="cluster-admin cluster-admin">
						The following YAML file is an example of a manifest for a bond interface. It includes samples values that you must replace with your own information.
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: bond0-eth1-eth2-policy <span id="CO210-1"><!--Empty--></span><span class="callout">1</span>
spec:
  nodeSelector: <span id="CO210-2"><!--Empty--></span><span class="callout">2</span>
    kubernetes.io/hostname: &lt;node01&gt; <span id="CO210-3"><!--Empty--></span><span class="callout">3</span>
  desiredState:
    interfaces:
    - name: bond0 <span id="CO210-4"><!--Empty--></span><span class="callout">4</span>
      description: Bond with ports eth1 and eth2 <span id="CO210-5"><!--Empty--></span><span class="callout">5</span>
      type: bond <span id="CO210-6"><!--Empty--></span><span class="callout">6</span>
      state: up <span id="CO210-7"><!--Empty--></span><span class="callout">7</span>
      ipv4:
        dhcp: true <span id="CO210-8"><!--Empty--></span><span class="callout">8</span>
        enabled: true <span id="CO210-9"><!--Empty--></span><span class="callout">9</span>
      link-aggregation:
        mode: active-backup <span id="CO210-10"><!--Empty--></span><span class="callout">10</span>
        options:
          miimon: '140' <span id="CO210-11"><!--Empty--></span><span class="callout">11</span>
        port: <span id="CO210-12"><!--Empty--></span><span class="callout">12</span>
        - eth1
        - eth2
      mtu: 1450 <span id="CO210-13"><!--Empty--></span><span class="callout">13</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO210-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Name of the policy.
							</div></dd><dt><a href="#CO210-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Optional: If you do not include the <code class="literal cluster-admin">nodeSelector</code> parameter, the policy applies to all nodes in the cluster.
							</div></dd><dt><a href="#CO210-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								This example uses a <code class="literal cluster-admin">hostname</code> node selector.
							</div></dd><dt><a href="#CO210-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Name of the interface.
							</div></dd><dt><a href="#CO210-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Optional: Human-readable description of the interface.
							</div></dd><dt><a href="#CO210-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								The type of interface. This example creates a bond.
							</div></dd><dt><a href="#CO210-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								The requested state for the interface after creation.
							</div></dd><dt><a href="#CO210-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								Optional: If you do not use <code class="literal cluster-admin">dhcp</code>, you can either set a static IP or leave the interface without an IP address.
							</div></dd><dt><a href="#CO210-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								Enables <code class="literal cluster-admin">ipv4</code> in this example.
							</div></dd><dt><a href="#CO210-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								The driver mode for the bond. This example uses an active backup mode.
							</div></dd><dt><a href="#CO210-11"><span class="callout">11</span></a> </dt><dd><div class="para">
								Optional: This example uses miimon to inspect the bond link every 140ms.
							</div></dd><dt><a href="#CO210-12"><span class="callout">12</span></a> </dt><dd><div class="para">
								The subordinate node NICs in the bond.
							</div></dd><dt><a href="#CO210-13"><span class="callout">13</span></a> </dt><dd><div class="para">
								Optional: The maximum transmission unit (MTU) for the bond. If not specified, this value is set to <code class="literal cluster-admin">1500</code> by default.
							</div></dd></dl></div></section><section class="section cluster-admin" id="virt-example-ethernet-nncp_k8s_nmstate-updating-node-network-config"><div class="titlepage"><div><div><h4 class="title">30.3.5.4. Example: Ethernet interface node network configuration policy</h4></div></div></div><p class="cluster-admin cluster-admin">
						Configure an Ethernet interface on nodes in the cluster by applying a <code class="literal cluster-admin">NodeNetworkConfigurationPolicy</code> manifest to the cluster.
					</p><p class="cluster-admin cluster-admin">
						The following YAML file is an example of a manifest for an Ethernet interface. It includes sample values that you must replace with your own information.
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: eth1-policy <span id="CO211-1"><!--Empty--></span><span class="callout">1</span>
spec:
  nodeSelector: <span id="CO211-2"><!--Empty--></span><span class="callout">2</span>
    kubernetes.io/hostname: &lt;node01&gt; <span id="CO211-3"><!--Empty--></span><span class="callout">3</span>
  desiredState:
    interfaces:
    - name: eth1 <span id="CO211-4"><!--Empty--></span><span class="callout">4</span>
      description: Configuring eth1 on node01 <span id="CO211-5"><!--Empty--></span><span class="callout">5</span>
      type: ethernet <span id="CO211-6"><!--Empty--></span><span class="callout">6</span>
      state: up <span id="CO211-7"><!--Empty--></span><span class="callout">7</span>
      ipv4:
        dhcp: true <span id="CO211-8"><!--Empty--></span><span class="callout">8</span>
        enabled: true <span id="CO211-9"><!--Empty--></span><span class="callout">9</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO211-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Name of the policy.
							</div></dd><dt><a href="#CO211-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Optional: If you do not include the <code class="literal cluster-admin">nodeSelector</code> parameter, the policy applies to all nodes in the cluster.
							</div></dd><dt><a href="#CO211-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								This example uses a <code class="literal cluster-admin">hostname</code> node selector.
							</div></dd><dt><a href="#CO211-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Name of the interface.
							</div></dd><dt><a href="#CO211-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Optional: Human-readable description of the interface.
							</div></dd><dt><a href="#CO211-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								The type of interface. This example creates an Ethernet networking interface.
							</div></dd><dt><a href="#CO211-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								The requested state for the interface after creation.
							</div></dd><dt><a href="#CO211-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								Optional: If you do not use <code class="literal cluster-admin">dhcp</code>, you can either set a static IP or leave the interface without an IP address.
							</div></dd><dt><a href="#CO211-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								Enables <code class="literal cluster-admin">ipv4</code> in this example.
							</div></dd></dl></div></section><section class="section cluster-admin" id="virt-example-nmstate-multiple-interfaces_k8s_nmstate-updating-node-network-config"><div class="titlepage"><div><div><h4 class="title">30.3.5.5. Example: Multiple interfaces in the same node network configuration policy</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can create multiple interfaces in the same node network configuration policy. These interfaces can reference each other, allowing you to build and deploy a network configuration by using a single policy manifest.
					</p><p class="cluster-admin cluster-admin">
						The following example YAML file creates a bond that is named <code class="literal cluster-admin">bond10</code> across two NICs and VLAN that is named <code class="literal cluster-admin">bond10.103</code> that connects to the bond.
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: bond-vlan <span id="CO212-1"><!--Empty--></span><span class="callout">1</span>
spec:
  nodeSelector: <span id="CO212-2"><!--Empty--></span><span class="callout">2</span>
    kubernetes.io/hostname: &lt;node01&gt; <span id="CO212-3"><!--Empty--></span><span class="callout">3</span>
  desiredState:
    interfaces:
    - name: bond10 <span id="CO212-4"><!--Empty--></span><span class="callout">4</span>
      description: Bonding eth2 and eth3 <span id="CO212-5"><!--Empty--></span><span class="callout">5</span>
      type: bond <span id="CO212-6"><!--Empty--></span><span class="callout">6</span>
      state: up <span id="CO212-7"><!--Empty--></span><span class="callout">7</span>
      link-aggregation:
        mode: balance-rr <span id="CO212-8"><!--Empty--></span><span class="callout">8</span>
        options:
          miimon: '140' <span id="CO212-9"><!--Empty--></span><span class="callout">9</span>
        port: <span id="CO212-10"><!--Empty--></span><span class="callout">10</span>
        - eth2
        - eth3
    - name: bond10.103 <span id="CO212-11"><!--Empty--></span><span class="callout">11</span>
      description: vlan using bond10 <span id="CO212-12"><!--Empty--></span><span class="callout">12</span>
      type: vlan <span id="CO212-13"><!--Empty--></span><span class="callout">13</span>
      state: up <span id="CO212-14"><!--Empty--></span><span class="callout">14</span>
      vlan:
         base-iface: bond10 <span id="CO212-15"><!--Empty--></span><span class="callout">15</span>
         id: 103 <span id="CO212-16"><!--Empty--></span><span class="callout">16</span>
      ipv4:
        dhcp: true <span id="CO212-17"><!--Empty--></span><span class="callout">17</span>
        enabled: true <span id="CO212-18"><!--Empty--></span><span class="callout">18</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO212-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Name of the policy.
							</div></dd><dt><a href="#CO212-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Optional: If you do not include the <code class="literal cluster-admin">nodeSelector</code> parameter, the policy applies to all nodes in the cluster.
							</div></dd><dt><a href="#CO212-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								This example uses <code class="literal cluster-admin">hostname</code> node selector.
							</div></dd><dt><a href="#CO212-4"><span class="callout">4</span></a> <a href="#CO212-11"><span class="callout">11</span></a> </dt><dd><div class="para">
								Name of the interface.
							</div></dd><dt><a href="#CO212-5"><span class="callout">5</span></a> <a href="#CO212-12"><span class="callout">12</span></a> </dt><dd><div class="para">
								Optional: Human-readable description of the interface.
							</div></dd><dt><a href="#CO212-6"><span class="callout">6</span></a> <a href="#CO212-13"><span class="callout">13</span></a> </dt><dd><div class="para">
								The type of interface.
							</div></dd><dt><a href="#CO212-7"><span class="callout">7</span></a> <a href="#CO212-14"><span class="callout">14</span></a> </dt><dd><div class="para">
								The requested state for the interface after creation.
							</div></dd><dt><a href="#CO212-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								The driver mode for the bond.
							</div></dd><dt><a href="#CO212-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								Optional: This example uses miimon to inspect the bond link every 140ms.
							</div></dd><dt><a href="#CO212-10"><span class="callout">10</span></a> </dt><dd><div class="para">
								The subordinate node NICs in the bond.
							</div></dd><dt><a href="#CO212-15"><span class="callout">15</span></a> </dt><dd><div class="para">
								The node NIC to which the VLAN is attached.
							</div></dd><dt><a href="#CO212-16"><span class="callout">16</span></a> </dt><dd><div class="para">
								The VLAN tag.
							</div></dd><dt><a href="#CO212-17"><span class="callout">17</span></a> </dt><dd><div class="para">
								Optional: If you do not use dhcp, you can either set a static IP or leave the interface without an IP address.
							</div></dd><dt><a href="#CO212-18"><span class="callout">18</span></a> </dt><dd><div class="para">
								Enables ipv4 in this example.
							</div></dd></dl></div></section></section><section class="section cluster-admin" id="capturing-nic-static-ip_k8s-nmstate-updating-node-network-config"><div class="titlepage"><div><div><h3 class="title">30.3.6. Capturing the static IP of a NIC attached to a bridge</h3></div></div></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						Capturing the static IP of a NIC is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
					</p><p class="cluster-admin cluster-admin">
						For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
					</p></div></div><section class="section cluster-admin" id="virt-example-inherit-static-ip-from-nic_k8s_nmstate-updating-node-network-config"><div class="titlepage"><div><div><h4 class="title">30.3.6.1. Example: Linux bridge interface node network configuration policy to inherit static IP address from the NIC attached to the bridge</h4></div></div></div><p class="cluster-admin cluster-admin">
						Create a Linux bridge interface on nodes in the cluster and transfer the static IP configuration of the NIC to the bridge by applying a single <code class="literal cluster-admin">NodeNetworkConfigurationPolicy</code> manifest to the cluster.
					</p><p class="cluster-admin cluster-admin">
						The following YAML file is an example of a manifest for a Linux bridge interface. It includes sample values that you must replace with your own information.
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: br1-eth1-copy-ipv4-policy <span id="CO213-1"><!--Empty--></span><span class="callout">1</span>
spec:
  nodeSelector: <span id="CO213-2"><!--Empty--></span><span class="callout">2</span>
    node-role.kubernetes.io/worker: ""
  capture:
    eth1-nic: interfaces.name=="eth1" <span id="CO213-3"><!--Empty--></span><span class="callout">3</span>
    eth1-routes: routes.running.next-hop-interface=="eth1"
    br1-routes: capture.eth1-routes | routes.running.next-hop-interface := "br1"
  desiredState:
    interfaces:
      - name: br1
        description: Linux bridge with eth1 as a port
        type: linux-bridge <span id="CO213-4"><!--Empty--></span><span class="callout">4</span>
        state: up
        ipv4: "{{ capture.eth1-nic.interfaces.0.ipv4 }}" <span id="CO213-5"><!--Empty--></span><span class="callout">5</span>
        bridge:
          options:
            stp:
              enabled: false
          port:
            - name: eth1 <span id="CO213-6"><!--Empty--></span><span class="callout">6</span>
     routes:
        config: "{{ capture.br1-routes.routes.running }}"</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO213-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The name of the policy.
							</div></dd><dt><a href="#CO213-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Optional: If you do not include the <code class="literal cluster-admin">nodeSelector</code> parameter, the policy applies to all nodes in the cluster. This example uses the <code class="literal cluster-admin">node-role.kubernetes.io/worker: ""</code> node selector to select all worker nodes in the cluster.
							</div></dd><dt><a href="#CO213-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								The reference to the node NIC to which the bridge attaches.
							</div></dd><dt><a href="#CO213-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								The type of interface. This example creates a bridge.
							</div></dd><dt><a href="#CO213-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								The IP address of the bridge interface. This value matches the IP address of the NIC which is referenced by the <code class="literal cluster-admin">spec.capture.eth1-nic</code> entry.
							</div></dd><dt><a href="#CO213-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								The node NIC to which the bridge attaches.
							</div></dd></dl></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								<a class="link" href="https://nmstate.io/nmpolicy/user-guide/102-policy-syntax.html">The NMPolicy project - Policy syntax</a>
							</li></ul></div></section></section><section class="section cluster-admin" id="virt-example-nmstate-IP-management_k8s_nmstate-updating-node-network-config"><div class="titlepage"><div><div><h3 class="title">30.3.7. Examples: IP management</h3></div></div></div><p class="cluster-admin cluster-admin">
					The following example configuration snippets demonstrate different methods of IP management.
				</p><p class="cluster-admin cluster-admin">
					These examples use the <code class="literal cluster-admin">ethernet</code> interface type to simplify the example while showing the related context in the policy configuration. These IP management examples can be used with the other interface types.
				</p><section class="section cluster-admin" id="virt-example-nmstate-IP-management-static_k8s_nmstate-updating-node-network-config"><div class="titlepage"><div><div><h4 class="title">30.3.7.1. Static</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following snippet statically configures an IP address on the Ethernet interface:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin"># ...
    interfaces:
    - name: eth1
      description: static IP on eth1
      type: ethernet
      state: up
      ipv4:
        dhcp: false
        address:
        - ip: 192.168.122.250 <span id="CO214-1"><!--Empty--></span><span class="callout">1</span>
          prefix-length: 24
        enabled: true
# ...</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO214-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Replace this value with the static IP address for the interface.
							</div></dd></dl></div></section><section class="section cluster-admin" id="virt-example-nmstate-IP-management-no-ip_k8s_nmstate-updating-node-network-config"><div class="titlepage"><div><div><h4 class="title">30.3.7.2. No IP address</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following snippet ensures that the interface has no IP address:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin"># ...
    interfaces:
    - name: eth1
      description: No IP on eth1
      type: ethernet
      state: up
      ipv4:
        enabled: false
# ...</pre></section><section class="section cluster-admin" id="virt-example-nmstate-IP-management-dhcp_k8s_nmstate-updating-node-network-config"><div class="titlepage"><div><div><h4 class="title">30.3.7.3. Dynamic host configuration</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following snippet configures an Ethernet interface that uses a dynamic IP address, gateway address, and DNS:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin"># ...
    interfaces:
    - name: eth1
      description: DHCP on eth1
      type: ethernet
      state: up
      ipv4:
        dhcp: true
        enabled: true
# ...</pre><p class="cluster-admin cluster-admin">
						The following snippet configures an Ethernet interface that uses a dynamic IP address but does not use a dynamic gateway address or DNS:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin"># ...
    interfaces:
    - name: eth1
      description: DHCP without gateway or DNS on eth1
      type: ethernet
      state: up
      ipv4:
        dhcp: true
        auto-gateway: false
        auto-dns: false
        enabled: true
# ...</pre></section><section class="section cluster-admin" id="virt-example-nmstate-IP-management-dns_k8s_nmstate-updating-node-network-config"><div class="titlepage"><div><div><h4 class="title">30.3.7.4. DNS</h4></div></div></div><p class="cluster-admin cluster-admin">
						Setting the DNS configuration is analagous to modifying the <code class="literal cluster-admin">/etc/resolv.conf</code> file. The following snippet sets the DNS configuration on the host.
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin"># ...
    interfaces: <span id="CO215-1"><!--Empty--></span><span class="callout">1</span>
       ...
       ipv4:
         ...
         auto-dns: false
         ...
    dns-resolver:
      config:
        search:
        - example.com
        - example.org
        server:
        - 8.8.8.8
# ...</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO215-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								You must configure an interface with <code class="literal cluster-admin">auto-dns: false</code> or you must use static IP configuration on an interface in order for Kubernetes NMState to store custom DNS settings.
							</div></dd></dl></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
							You cannot use <code class="literal cluster-admin">br-ex</code>, an OVNKubernetes-managed Open vSwitch bridge, as the interface when configuring DNS resolvers.
						</p></div></div></section><section class="section cluster-admin" id="virt-example-nmstate-IP-management-static-routing_k8s_nmstate-updating-node-network-config"><div class="titlepage"><div><div><h4 class="title">30.3.7.5. Static routing</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following snippet configures a static route and a static IP on interface <code class="literal cluster-admin">eth1</code>.
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin"># ...
    interfaces:
    - name: eth1
      description: Static routing on eth1
      type: ethernet
      state: up
      ipv4:
        dhcp: false
        address:
        - ip: 192.0.2.251 <span id="CO216-1"><!--Empty--></span><span class="callout">1</span>
          prefix-length: 24
        enabled: true
    routes:
      config:
      - destination: 198.51.100.0/24
        metric: 150
        next-hop-address: 192.0.2.1 <span id="CO216-2"><!--Empty--></span><span class="callout">2</span>
        next-hop-interface: eth1
        table-id: 254
# ...</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO216-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The static IP address for the Ethernet interface.
							</div></dd><dt><a href="#CO216-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Next hop address for the node traffic. This must be in the same subnet as the IP address set for the Ethernet interface.
							</div></dd></dl></div></section></section></section><section class="section cluster-admin" id="k8s-nmstate-troubleshooting-node-network"><div class="titlepage"><div><div><h2 class="title">30.4. Troubleshooting node network configuration</h2></div></div></div><p class="cluster-admin cluster-admin">
				If the node network configuration encounters an issue, the policy is automatically rolled back and the enactments report failure. This includes issues such as:
			</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
						The configuration fails to be applied on the host.
					</li><li class="listitem">
						The host loses connection to the default gateway.
					</li><li class="listitem">
						The host loses connection to the API server.
					</li></ul></div><section class="section cluster-admin" id="virt-troubleshooting-incorrect-policy-config_k8s-nmstate-troubleshooting-node-network"><div class="titlepage"><div><div><h3 class="title">30.4.1. Troubleshooting an incorrect node network configuration policy configuration</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can apply changes to the node network configuration across your entire cluster by applying a node network configuration policy. If you apply an incorrect configuration, you can use the following example to troubleshoot and correct the failed node network policy.
				</p><p class="cluster-admin cluster-admin">
					In this example, a Linux bridge policy is applied to an example cluster that has three control plane nodes and three compute nodes. The policy fails to be applied because it references an incorrect interface. To find the error, investigate the available NMState resources. You can then update the policy with the correct configuration.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a policy and apply it to your cluster. The following example creates a simple bridge on the <code class="literal cluster-admin">ens01</code> interface:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: ens01-bridge-testfail
spec:
  desiredState:
    interfaces:
      - name: br1
        description: Linux bridge with the wrong port
        type: linux-bridge
        state: up
        ipv4:
          dhcp: true
          enabled: true
        bridge:
          options:
            stp:
              enabled: false
          port:
            - name: ens01</pre><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f ens01-bridge-testfail.yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">nodenetworkconfigurationpolicy.nmstate.io/ens01-bridge-testfail created</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify the status of the policy by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get nncp</pre><p class="cluster-admin cluster-admin">
							The output shows that the policy failed:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                    STATUS
ens01-bridge-testfail   FailedToConfigure</pre>

							</p></div><p class="cluster-admin cluster-admin">
							However, the policy status alone does not indicate if it failed on all nodes or a subset of nodes.
						</p></li><li class="listitem"><p class="simpara">
							List the node network configuration enactments to see if the policy was successful on any of the nodes. If the policy failed for only a subset of nodes, it suggests that the problem is with a specific node configuration. If the policy failed on all nodes, it suggests that the problem is with the policy.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get nnce</pre><p class="cluster-admin cluster-admin">
							The output shows that the policy failed on all nodes:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                         STATUS
control-plane-1.ens01-bridge-testfail        FailedToConfigure
control-plane-2.ens01-bridge-testfail        FailedToConfigure
control-plane-3.ens01-bridge-testfail        FailedToConfigure
compute-1.ens01-bridge-testfail              FailedToConfigure
compute-2.ens01-bridge-testfail              FailedToConfigure
compute-3.ens01-bridge-testfail              FailedToConfigure</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							View one of the failed enactments and look at the traceback. The following command uses the output tool <code class="literal cluster-admin">jsonpath</code> to filter the output:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get nnce compute-1.ens01-bridge-testfail -o jsonpath='{.status.conditions[?(@.type=="Failing")].message}'</pre><p class="cluster-admin cluster-admin">
							This command returns a large traceback that has been edited for brevity:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">error reconciling NodeNetworkConfigurationPolicy at desired state apply: , failed to execute nmstatectl set --no-commit --timeout 480: 'exit status 1' ''
...
libnmstate.error.NmstateVerificationError:
desired
=======
---
name: br1
type: linux-bridge
state: up
bridge:
  options:
    group-forward-mask: 0
    mac-ageing-time: 300
    multicast-snooping: true
    stp:
      enabled: false
      forward-delay: 15
      hello-time: 2
      max-age: 20
      priority: 32768
  port:
  - name: ens01
description: Linux bridge with the wrong port
ipv4:
  address: []
  auto-dns: true
  auto-gateway: true
  auto-routes: true
  dhcp: true
  enabled: true
ipv6:
  enabled: false
mac-address: 01-23-45-67-89-AB
mtu: 1500

current
=======
---
name: br1
type: linux-bridge
state: up
bridge:
  options:
    group-forward-mask: 0
    mac-ageing-time: 300
    multicast-snooping: true
    stp:
      enabled: false
      forward-delay: 15
      hello-time: 2
      max-age: 20
      priority: 32768
  port: []
description: Linux bridge with the wrong port
ipv4:
  address: []
  auto-dns: true
  auto-gateway: true
  auto-routes: true
  dhcp: true
  enabled: true
ipv6:
  enabled: false
mac-address: 01-23-45-67-89-AB
mtu: 1500

difference
==========
--- desired
+++ current
@@ -13,8 +13,7 @@
       hello-time: 2
       max-age: 20
       priority: 32768
-  port:
-  - name: ens01
+  port: []
 description: Linux bridge with the wrong port
 ipv4:
   address: []
  line 651, in _assert_interfaces_equal\n    current_state.interfaces[ifname],\nlibnmstate.error.NmstateVerificationError:</pre>

							</p></div><p class="cluster-admin cluster-admin">
							The <code class="literal cluster-admin">NmstateVerificationError</code> lists the <code class="literal cluster-admin">desired</code> policy configuration, the <code class="literal cluster-admin">current</code> configuration of the policy on the node, and the <code class="literal cluster-admin">difference</code> highlighting the parameters that do not match. In this example, the <code class="literal cluster-admin">port</code> is included in the <code class="literal cluster-admin">difference</code>, which suggests that the problem is the port configuration in the policy.
						</p></li><li class="listitem"><p class="simpara">
							To ensure that the policy is configured properly, view the network configuration for one or all of the nodes by requesting the <code class="literal cluster-admin">NodeNetworkState</code> object. The following command returns the network configuration for the <code class="literal cluster-admin">control-plane-1</code> node:
						</p><pre class="screen cluster-admin cluster-admin">$ oc get nns control-plane-1 -o yaml</pre><p class="cluster-admin cluster-admin">
							The output shows that the interface name on the nodes is <code class="literal cluster-admin">ens1</code> but the failed policy incorrectly uses <code class="literal cluster-admin">ens01</code>:
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-yaml">   - ipv4:
# ...
      name: ens1
      state: up
      type: ethernet</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Correct the error by editing the existing policy:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit nncp ens01-bridge-testfail</pre><pre class="programlisting language-yaml cluster-admin cluster-admin"># ...
          port:
            - name: ens1</pre><p class="cluster-admin cluster-admin">
							Save the policy to apply the correction.
						</p></li><li class="listitem"><p class="simpara">
							Check the status of the policy to ensure it updated successfully:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get nncp</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                    STATUS
ens01-bridge-testfail   SuccessfullyConfigured</pre>

							</p></div></li></ol></div><p class="cluster-admin cluster-admin">
					The updated policy is successfully configured on all nodes in the cluster.
				</p></section></section></section><section class="chapter cluster-admin" id="enable-cluster-wide-proxy"><div class="titlepage"><div><div><h1 class="title">Chapter 31. Configuring the cluster-wide proxy</h1></div></div></div><p class="cluster-admin cluster-admin">
			Production environments can deny direct access to the internet and instead have an HTTP or HTTPS proxy available. You can configure OpenShift Container Platform to use a proxy by <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-proxy-configure-object_config-cluster-wide-proxy">modifying the Proxy object for existing clusters</a> or by configuring the proxy settings in the <code class="literal cluster-admin">install-config.yaml</code> file for new clusters.
		</p><section class="section cluster-admin" id="prerequisites-6"><div class="titlepage"><div><div><h2 class="title">31.1. Prerequisites</h2></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
						Review the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/#configuring-firewall">sites that your cluster requires access to</a> and determine whether any of them must bypass the proxy. By default, all cluster system egress traffic is proxied, including calls to the cloud provider API for the cloud that hosts your cluster. System-wide proxy affects system components only, not user workloads. Add sites to the Proxy object’s <code class="literal cluster-admin">spec.noProxy</code> field to bypass the proxy if necessary.
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							The Proxy object <code class="literal cluster-admin">status.noProxy</code> field is populated with the values of the <code class="literal cluster-admin">networking.machineNetwork[].cidr</code>, <code class="literal cluster-admin">networking.clusterNetwork[].cidr</code>, and <code class="literal cluster-admin">networking.serviceNetwork[]</code> fields from your installation configuration.
						</p><p class="cluster-admin cluster-admin">
							For installations on Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, and Red Hat OpenStack Platform (RHOSP), the <code class="literal cluster-admin">Proxy</code> object <code class="literal cluster-admin">status.noProxy</code> field is also populated with the instance metadata endpoint (<code class="literal cluster-admin">169.254.169.254</code>).
						</p></div></div></li></ul></div></section><section class="section cluster-admin" id="nw-proxy-configure-object_config-cluster-wide-proxy"><div class="titlepage"><div><div><h2 class="title">31.2. Enabling the cluster-wide proxy</h2></div></div></div><p class="cluster-admin cluster-admin">
				The <code class="literal cluster-admin">Proxy</code> object is used to manage the cluster-wide egress proxy. When a cluster is installed or upgraded without the proxy configured, a <code class="literal cluster-admin">Proxy</code> object is still generated but it will have a nil <code class="literal cluster-admin">spec</code>. For example:
			</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec:
  trustedCA:
    name: ""
status:</pre><p class="cluster-admin cluster-admin">
				A cluster administrator can configure the proxy for OpenShift Container Platform by modifying this <code class="literal cluster-admin">cluster</code> <code class="literal cluster-admin">Proxy</code> object.
			</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
					Only the <code class="literal cluster-admin">Proxy</code> object named <code class="literal cluster-admin">cluster</code> is supported, and no additional proxies can be created.
				</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
						Cluster administrator permissions
					</li><li class="listitem">
						OpenShift Container Platform <code class="literal cluster-admin">oc</code> CLI tool installed
					</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
						Create a config map that contains any additional CA certificates required for proxying HTTPS connections.
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							You can skip this step if the proxy’s identity certificate is signed by an authority from the RHCOS trust bundle.
						</p></div></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
								Create a file called <code class="literal cluster-admin">user-ca-bundle.yaml</code> with the following contents, and provide the values of your PEM-encoded certificates:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
data:
  ca-bundle.crt: | <span id="CO217-1"><!--Empty--></span><span class="callout">1</span>
    &lt;MY_PEM_ENCODED_CERTS&gt; <span id="CO217-2"><!--Empty--></span><span class="callout">2</span>
kind: ConfigMap
metadata:
  name: user-ca-bundle <span id="CO217-3"><!--Empty--></span><span class="callout">3</span>
  namespace: openshift-config <span id="CO217-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO217-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										This data key must be named <code class="literal cluster-admin">ca-bundle.crt</code>.
									</div></dd><dt><a href="#CO217-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										One or more PEM-encoded X.509 certificates used to sign the proxy’s identity certificate.
									</div></dd><dt><a href="#CO217-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The config map name that will be referenced from the <code class="literal cluster-admin">Proxy</code> object.
									</div></dd><dt><a href="#CO217-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										The config map must be in the <code class="literal cluster-admin">openshift-config</code> namespace.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create the config map from this file:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f user-ca-bundle.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Use the <code class="literal cluster-admin">oc edit</code> command to modify the <code class="literal cluster-admin">Proxy</code> object:
					</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit proxy/cluster</pre></li><li class="listitem"><p class="simpara">
						Configure the necessary fields for the proxy:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec:
  httpProxy: http://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt; <span id="CO218-1"><!--Empty--></span><span class="callout">1</span>
  httpsProxy: https://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt; <span id="CO218-2"><!--Empty--></span><span class="callout">2</span>
  noProxy: example.com <span id="CO218-3"><!--Empty--></span><span class="callout">3</span>
  readinessEndpoints:
  - http://www.google.com <span id="CO218-4"><!--Empty--></span><span class="callout">4</span>
  - https://www.google.com
  trustedCA:
    name: user-ca-bundle <span id="CO218-5"><!--Empty--></span><span class="callout">5</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO218-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								A proxy URL to use for creating HTTP connections outside the cluster. The URL scheme must be <code class="literal cluster-admin">http</code>.
							</div></dd><dt><a href="#CO218-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								A proxy URL to use for creating HTTPS connections outside the cluster. The URL scheme must be either <code class="literal cluster-admin">http</code> or <code class="literal cluster-admin">https</code>. Specify a URL for the proxy that supports the URL scheme. For example, most proxies will report an error if they are configured to use <code class="literal cluster-admin">https</code> but they only support <code class="literal cluster-admin">http</code>. This failure message may not propagate to the logs and can appear to be a network connection failure instead. If using a proxy that listens for <code class="literal cluster-admin">https</code> connections from the cluster, you may need to configure the cluster to accept the CAs and certificates that the proxy uses.
							</div></dd><dt><a href="#CO218-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								A comma-separated list of destination domain names, domains, IP addresses or other network CIDRs to exclude proxying.
							</div><p class="cluster-admin cluster-admin">
								Preface a domain with <code class="literal cluster-admin">.</code> to match subdomains only. For example, <code class="literal cluster-admin">.y.com</code> matches <code class="literal cluster-admin">x.y.com</code>, but not <code class="literal cluster-admin">y.com</code>. Use <code class="literal cluster-admin">*</code> to bypass proxy for all destinations. If you scale up workers that are not included in the network defined by the <code class="literal cluster-admin">networking.machineNetwork[].cidr</code> field from the installation configuration, you must add them to this list to prevent connection issues.
							</p><p class="cluster-admin cluster-admin">
								This field is ignored if neither the <code class="literal cluster-admin">httpProxy</code> or <code class="literal cluster-admin">httpsProxy</code> fields are set.
							</p></dd><dt><a href="#CO218-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								One or more URLs external to the cluster to use to perform a readiness check before writing the <code class="literal cluster-admin">httpProxy</code> and <code class="literal cluster-admin">httpsProxy</code> values to status.
							</div></dd><dt><a href="#CO218-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								A reference to the config map in the <code class="literal cluster-admin">openshift-config</code> namespace that contains additional CA certificates required for proxying HTTPS connections. Note that the config map must already exist before referencing it here. This field is required unless the proxy’s identity certificate is signed by an authority from the RHCOS trust bundle.
							</div></dd></dl></div></li><li class="listitem">
						Save the file to apply the changes.
					</li></ol></div></section><section class="section cluster-admin" id="nw-proxy-remove_config-cluster-wide-proxy"><div class="titlepage"><div><div><h2 class="title">31.3. Removing the cluster-wide proxy</h2></div></div></div><p class="cluster-admin cluster-admin">
				The <code class="literal cluster-admin">cluster</code> Proxy object cannot be deleted. To remove the proxy from a cluster, remove all <code class="literal cluster-admin">spec</code> fields from the Proxy object.
			</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
						Cluster administrator permissions
					</li><li class="listitem">
						OpenShift Container Platform <code class="literal cluster-admin">oc</code> CLI tool installed
					</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
						Use the <code class="literal cluster-admin">oc edit</code> command to modify the proxy:
					</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit proxy/cluster</pre></li><li class="listitem"><p class="simpara">
						Remove all <code class="literal cluster-admin">spec</code> fields from the Proxy object. For example:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec: {}</pre></li><li class="listitem">
						Save the file to apply the changes.
					</li></ol></div><h3 id="additional-resources-8">Additional resources</h3><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/security_and_compliance/#ca-bundle-understanding_updating-ca-bundle">Replacing the CA Bundle certificate</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/security_and_compliance/#customization">Proxy certificate customization</a>
					</li></ul></div></section></section><section class="chapter cluster-admin" id="configuring-a-custom-pki"><div class="titlepage"><div><div><h1 class="title">Chapter 32. Configuring a custom PKI</h1></div></div></div><p class="cluster-admin cluster-admin">
			Some platform components, such as the web console, use Routes for communication and must trust other components' certificates to interact with them. If you are using a custom public key infrastructure (PKI), you must configure it so its privately signed CA certificates are recognized across the cluster.
		</p><p class="cluster-admin cluster-admin">
			You can leverage the Proxy API to add cluster-wide trusted CA certificates. You must do this either during installation or at runtime.
		</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
					During <span class="emphasis"><em><span class="cluster-admin cluster-admin">installation</span></em></span>, <a class="link" href="#installation-configure-proxy_configuring-a-custom-pki" title="32.1. Configuring the cluster-wide proxy during installation">configure the cluster-wide proxy</a>. You must define your privately signed CA certificates in the <code class="literal cluster-admin">install-config.yaml</code> file’s <code class="literal cluster-admin">additionalTrustBundle</code> setting.
				</p><p class="cluster-admin cluster-admin">
					The installation program generates a ConfigMap that is named <code class="literal cluster-admin">user-ca-bundle</code> that contains the additional CA certificates you defined. The Cluster Network Operator then creates a <code class="literal cluster-admin">trusted-ca-bundle</code> ConfigMap that merges these CA certificates with the Red Hat Enterprise Linux CoreOS (RHCOS) trust bundle; this ConfigMap is referenced in the Proxy object’s <code class="literal cluster-admin">trustedCA</code> field.
				</p></li><li class="listitem">
					At <span class="emphasis"><em><span class="cluster-admin cluster-admin">runtime</span></em></span>, <a class="link" href="#nw-proxy-configure-object_configuring-a-custom-pki" title="32.2. Enabling the cluster-wide proxy">modify the default Proxy object to include your privately signed CA certificates</a> (part of cluster’s proxy enablement workflow). This involves creating a ConfigMap that contains the privately signed CA certificates that should be trusted by the cluster, and then modifying the proxy resource with the <code class="literal cluster-admin">trustedCA</code> referencing the privately signed certificates' ConfigMap.
				</li></ul></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
				The installer configuration’s <code class="literal cluster-admin">additionalTrustBundle</code> field and the proxy resource’s <code class="literal cluster-admin">trustedCA</code> field are used to manage the cluster-wide trust bundle; <code class="literal cluster-admin">additionalTrustBundle</code> is used at install time and the proxy’s <code class="literal cluster-admin">trustedCA</code> is used at runtime.
			</p><p class="cluster-admin cluster-admin">
				The <code class="literal cluster-admin">trustedCA</code> field is a reference to a <code class="literal cluster-admin">ConfigMap</code> containing the custom certificate and key pair used by the cluster component.
			</p></div></div><section class="section cluster-admin" id="installation-configure-proxy_configuring-a-custom-pki"><div class="titlepage"><div><div><h2 class="title">32.1. Configuring the cluster-wide proxy during installation</h2></div></div></div><p class="cluster-admin cluster-admin">
				Production environments can deny direct access to the internet and instead have an HTTP or HTTPS proxy available. You can configure a new OpenShift Container Platform cluster to use a proxy by configuring the proxy settings in the <code class="literal cluster-admin">install-config.yaml</code> file.
			</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
						You have an existing <code class="literal cluster-admin">install-config.yaml</code> file.
					</li><li class="listitem"><p class="simpara">
						You reviewed the sites that your cluster requires access to and determined whether any of them need to bypass the proxy. By default, all cluster egress traffic is proxied, including calls to hosting cloud provider APIs. You added sites to the <code class="literal cluster-admin">Proxy</code> object’s <code class="literal cluster-admin">spec.noProxy</code> field to bypass the proxy if necessary.
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							The <code class="literal cluster-admin">Proxy</code> object <code class="literal cluster-admin">status.noProxy</code> field is populated with the values of the <code class="literal cluster-admin">networking.machineNetwork[].cidr</code>, <code class="literal cluster-admin">networking.clusterNetwork[].cidr</code>, and <code class="literal cluster-admin">networking.serviceNetwork[]</code> fields from your installation configuration.
						</p><p class="cluster-admin cluster-admin">
							For installations on Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, and Red Hat OpenStack Platform (RHOSP), the <code class="literal cluster-admin">Proxy</code> object <code class="literal cluster-admin">status.noProxy</code> field is also populated with the instance metadata endpoint (<code class="literal cluster-admin">169.254.169.254</code>).
						</p></div></div></li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
						Edit your <code class="literal cluster-admin">install-config.yaml</code> file and add the proxy settings. For example:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
baseDomain: my.domain.com
proxy:
  httpProxy: http://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt; <span id="CO219-1"><!--Empty--></span><span class="callout">1</span>
  httpsProxy: https://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt; <span id="CO219-2"><!--Empty--></span><span class="callout">2</span>
  noProxy: ec2.&lt;aws_region&gt;.amazonaws.com,elasticloadbalancing.&lt;aws_region&gt;.amazonaws.com,s3.&lt;aws_region&gt;.amazonaws.com <span id="CO219-3"><!--Empty--></span><span class="callout">3</span>
additionalTrustBundle: | <span id="CO219-4"><!--Empty--></span><span class="callout">4</span>
    -----BEGIN CERTIFICATE-----
    &lt;MY_TRUSTED_CA_CERT&gt;
    -----END CERTIFICATE-----
additionalTrustBundlePolicy: &lt;policy_to_add_additionalTrustBundle&gt; <span id="CO219-5"><!--Empty--></span><span class="callout">5</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO219-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								A proxy URL to use for creating HTTP connections outside the cluster. The URL scheme must be <code class="literal cluster-admin">http</code>.
							</div></dd><dt><a href="#CO219-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								A proxy URL to use for creating HTTPS connections outside the cluster.
							</div></dd><dt><a href="#CO219-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								A comma-separated list of destination domain names, IP addresses, or other network CIDRs to exclude from proxying. Preface a domain with <code class="literal cluster-admin">.</code> to match subdomains only. For example, <code class="literal cluster-admin">.y.com</code> matches <code class="literal cluster-admin">x.y.com</code>, but not <code class="literal cluster-admin">y.com</code>. Use <code class="literal cluster-admin">*</code> to bypass the proxy for all destinations. If you have added the Amazon <code class="literal cluster-admin">EC2</code>,<code class="literal cluster-admin">Elastic Load Balancing</code>, and <code class="literal cluster-admin">S3</code> VPC endpoints to your VPC, you must add these endpoints to the <code class="literal cluster-admin">noProxy</code> field.
							</div></dd><dt><a href="#CO219-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								If provided, the installation program generates a config map that is named <code class="literal cluster-admin">user-ca-bundle</code> in the <code class="literal cluster-admin">openshift-config</code> namespace that contains one or more additional CA certificates that are required for proxying HTTPS connections. The Cluster Network Operator then creates a <code class="literal cluster-admin">trusted-ca-bundle</code> config map that merges these contents with the Red Hat Enterprise Linux CoreOS (RHCOS) trust bundle, and this config map is referenced in the <code class="literal cluster-admin">trustedCA</code> field of the <code class="literal cluster-admin">Proxy</code> object. The <code class="literal cluster-admin">additionalTrustBundle</code> field is required unless the proxy’s identity certificate is signed by an authority from the RHCOS trust bundle.
							</div></dd><dt><a href="#CO219-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Optional: The policy to determine the configuration of the <code class="literal cluster-admin">Proxy</code> object to reference the <code class="literal cluster-admin">user-ca-bundle</code> config map in the <code class="literal cluster-admin">trustedCA</code> field. The allowed values are <code class="literal cluster-admin">Proxyonly</code> and <code class="literal cluster-admin">Always</code>. Use <code class="literal cluster-admin">Proxyonly</code> to reference the <code class="literal cluster-admin">user-ca-bundle</code> config map only when <code class="literal cluster-admin">http/https</code> proxy is configured. Use <code class="literal cluster-admin">Always</code> to always reference the <code class="literal cluster-admin">user-ca-bundle</code> config map. The default value is <code class="literal cluster-admin">Proxyonly</code>.
							</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							The installation program does not support the proxy <code class="literal cluster-admin">readinessEndpoints</code> field.
						</p></div></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							If the installer times out, restart and then complete the deployment by using the <code class="literal cluster-admin">wait-for</code> command of the installer. For example:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ ./openshift-install wait-for install-complete --log-level debug</pre></div></div></li><li class="listitem">
						Save the file and reference it when installing OpenShift Container Platform.
					</li></ol></div><p class="cluster-admin cluster-admin">
				The installation program creates a cluster-wide proxy that is named <code class="literal cluster-admin">cluster</code> that uses the proxy settings in the provided <code class="literal cluster-admin">install-config.yaml</code> file. If no proxy settings are provided, a <code class="literal cluster-admin">cluster</code> <code class="literal cluster-admin">Proxy</code> object is still created, but it will have a nil <code class="literal cluster-admin">spec</code>.
			</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
					Only the <code class="literal cluster-admin">Proxy</code> object named <code class="literal cluster-admin">cluster</code> is supported, and no additional proxies can be created.
				</p></div></div></section><section class="section cluster-admin" id="nw-proxy-configure-object_configuring-a-custom-pki"><div class="titlepage"><div><div><h2 class="title">32.2. Enabling the cluster-wide proxy</h2></div></div></div><p class="cluster-admin cluster-admin">
				The <code class="literal cluster-admin">Proxy</code> object is used to manage the cluster-wide egress proxy. When a cluster is installed or upgraded without the proxy configured, a <code class="literal cluster-admin">Proxy</code> object is still generated but it will have a nil <code class="literal cluster-admin">spec</code>. For example:
			</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec:
  trustedCA:
    name: ""
status:</pre><p class="cluster-admin cluster-admin">
				A cluster administrator can configure the proxy for OpenShift Container Platform by modifying this <code class="literal cluster-admin">cluster</code> <code class="literal cluster-admin">Proxy</code> object.
			</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
					Only the <code class="literal cluster-admin">Proxy</code> object named <code class="literal cluster-admin">cluster</code> is supported, and no additional proxies can be created.
				</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
						Cluster administrator permissions
					</li><li class="listitem">
						OpenShift Container Platform <code class="literal cluster-admin">oc</code> CLI tool installed
					</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
						Create a config map that contains any additional CA certificates required for proxying HTTPS connections.
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							You can skip this step if the proxy’s identity certificate is signed by an authority from the RHCOS trust bundle.
						</p></div></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
								Create a file called <code class="literal cluster-admin">user-ca-bundle.yaml</code> with the following contents, and provide the values of your PEM-encoded certificates:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
data:
  ca-bundle.crt: | <span id="CO220-1"><!--Empty--></span><span class="callout">1</span>
    &lt;MY_PEM_ENCODED_CERTS&gt; <span id="CO220-2"><!--Empty--></span><span class="callout">2</span>
kind: ConfigMap
metadata:
  name: user-ca-bundle <span id="CO220-3"><!--Empty--></span><span class="callout">3</span>
  namespace: openshift-config <span id="CO220-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO220-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										This data key must be named <code class="literal cluster-admin">ca-bundle.crt</code>.
									</div></dd><dt><a href="#CO220-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										One or more PEM-encoded X.509 certificates used to sign the proxy’s identity certificate.
									</div></dd><dt><a href="#CO220-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The config map name that will be referenced from the <code class="literal cluster-admin">Proxy</code> object.
									</div></dd><dt><a href="#CO220-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										The config map must be in the <code class="literal cluster-admin">openshift-config</code> namespace.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create the config map from this file:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f user-ca-bundle.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Use the <code class="literal cluster-admin">oc edit</code> command to modify the <code class="literal cluster-admin">Proxy</code> object:
					</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit proxy/cluster</pre></li><li class="listitem"><p class="simpara">
						Configure the necessary fields for the proxy:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec:
  httpProxy: http://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt; <span id="CO221-1"><!--Empty--></span><span class="callout">1</span>
  httpsProxy: https://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt; <span id="CO221-2"><!--Empty--></span><span class="callout">2</span>
  noProxy: example.com <span id="CO221-3"><!--Empty--></span><span class="callout">3</span>
  readinessEndpoints:
  - http://www.google.com <span id="CO221-4"><!--Empty--></span><span class="callout">4</span>
  - https://www.google.com
  trustedCA:
    name: user-ca-bundle <span id="CO221-5"><!--Empty--></span><span class="callout">5</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO221-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								A proxy URL to use for creating HTTP connections outside the cluster. The URL scheme must be <code class="literal cluster-admin">http</code>.
							</div></dd><dt><a href="#CO221-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								A proxy URL to use for creating HTTPS connections outside the cluster. The URL scheme must be either <code class="literal cluster-admin">http</code> or <code class="literal cluster-admin">https</code>. Specify a URL for the proxy that supports the URL scheme. For example, most proxies will report an error if they are configured to use <code class="literal cluster-admin">https</code> but they only support <code class="literal cluster-admin">http</code>. This failure message may not propagate to the logs and can appear to be a network connection failure instead. If using a proxy that listens for <code class="literal cluster-admin">https</code> connections from the cluster, you may need to configure the cluster to accept the CAs and certificates that the proxy uses.
							</div></dd><dt><a href="#CO221-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								A comma-separated list of destination domain names, domains, IP addresses or other network CIDRs to exclude proxying.
							</div><p class="cluster-admin cluster-admin">
								Preface a domain with <code class="literal cluster-admin">.</code> to match subdomains only. For example, <code class="literal cluster-admin">.y.com</code> matches <code class="literal cluster-admin">x.y.com</code>, but not <code class="literal cluster-admin">y.com</code>. Use <code class="literal cluster-admin">*</code> to bypass proxy for all destinations. If you scale up workers that are not included in the network defined by the <code class="literal cluster-admin">networking.machineNetwork[].cidr</code> field from the installation configuration, you must add them to this list to prevent connection issues.
							</p><p class="cluster-admin cluster-admin">
								This field is ignored if neither the <code class="literal cluster-admin">httpProxy</code> or <code class="literal cluster-admin">httpsProxy</code> fields are set.
							</p></dd><dt><a href="#CO221-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								One or more URLs external to the cluster to use to perform a readiness check before writing the <code class="literal cluster-admin">httpProxy</code> and <code class="literal cluster-admin">httpsProxy</code> values to status.
							</div></dd><dt><a href="#CO221-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								A reference to the config map in the <code class="literal cluster-admin">openshift-config</code> namespace that contains additional CA certificates required for proxying HTTPS connections. Note that the config map must already exist before referencing it here. This field is required unless the proxy’s identity certificate is signed by an authority from the RHCOS trust bundle.
							</div></dd></dl></div></li><li class="listitem">
						Save the file to apply the changes.
					</li></ol></div></section><section class="section cluster-admin" id="certificate-injection-using-operators_configuring-a-custom-pki"><div class="titlepage"><div><div><h2 class="title">32.3. Certificate injection using Operators</h2></div></div></div><p class="cluster-admin cluster-admin">
				Once your custom CA certificate is added to the cluster via ConfigMap, the Cluster Network Operator merges the user-provided and system CA certificates into a single bundle and injects the merged bundle into the Operator requesting the trust bundle injection.
			</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
					After adding a <code class="literal cluster-admin">config.openshift.io/inject-trusted-cabundle="true"</code> label to the config map, existing data in it is deleted. The Cluster Network Operator takes ownership of a config map and only accepts <code class="literal cluster-admin">ca-bundle</code> as data. You must use a separate config map to store <code class="literal cluster-admin">service-ca.crt</code> by using the <code class="literal cluster-admin">service.beta.openshift.io/inject-cabundle=true</code> annotation or a similar configuration. Adding a <code class="literal cluster-admin">config.openshift.io/inject-trusted-cabundle="true"</code> label and <code class="literal cluster-admin">service.beta.openshift.io/inject-cabundle=true</code> annotation on the same config map can cause issues.
				</p></div></div><p class="cluster-admin cluster-admin">
				Operators request this injection by creating an empty ConfigMap with the following label:
			</p><pre class="programlisting language-yaml cluster-admin cluster-admin">config.openshift.io/inject-trusted-cabundle="true"</pre><p class="cluster-admin cluster-admin">
				An example of the empty ConfigMap:
			</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
data: {}
kind: ConfigMap
metadata:
  labels:
    config.openshift.io/inject-trusted-cabundle: "true"
  name: ca-inject <span id="CO222-1"><!--Empty--></span><span class="callout">1</span>
  namespace: apache</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO222-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						Specifies the empty ConfigMap name.
					</div></dd></dl></div><p class="cluster-admin cluster-admin">
				The Operator mounts this ConfigMap into the container’s local trust store.
			</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
					Adding a trusted CA certificate is only needed if the certificate is not included in the Red Hat Enterprise Linux CoreOS (RHCOS) trust bundle.
				</p></div></div><p class="cluster-admin cluster-admin">
				Certificate injection is not limited to Operators. The Cluster Network Operator injects certificates across any namespace when an empty ConfigMap is created with the <code class="literal cluster-admin">config.openshift.io/inject-trusted-cabundle=true</code> label.
			</p><p class="cluster-admin cluster-admin">
				The ConfigMap can reside in any namespace, but the ConfigMap must be mounted as a volume to each container within a pod that requires a custom CA. For example:
			</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-example-custom-ca-deployment
  namespace: my-example-custom-ca-ns
spec:
  ...
    spec:
      ...
      containers:
        - name: my-container-that-needs-custom-ca
          volumeMounts:
          - name: trusted-ca
            mountPath: /etc/pki/ca-trust/extracted/pem
            readOnly: true
      volumes:
      - name: trusted-ca
        configMap:
          name: trusted-ca
          items:
            - key: ca-bundle.crt <span id="CO223-1"><!--Empty--></span><span class="callout">1</span>
              path: tls-ca-bundle.pem <span id="CO223-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO223-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						<code class="literal cluster-admin">ca-bundle.crt</code> is required as the ConfigMap key.
					</div></dd><dt><a href="#CO223-2"><span class="callout">2</span></a> </dt><dd><div class="para">
						<code class="literal cluster-admin">tls-ca-bundle.pem</code> is required as the ConfigMap path.
					</div></dd></dl></div></section></section><section class="chapter cluster-admin" id="load-balancing-openstack"><div class="titlepage"><div><div><h1 class="title">Chapter 33. Load balancing on RHOSP</h1></div></div></div><section class="section cluster-admin" id="nw-osp-loadbalancer-limitations_load-balancing-openstack"><div class="titlepage"><div><div><h2 class="title">33.1. Limitations of load balancer services</h2></div></div></div><p class="cluster-admin cluster-admin">
				OpenShift Container Platform clusters on Red Hat OpenStack Platform (RHOSP) use Octavia to handle load balancer services. As a result of this choice, such clusters have a number of functional limitations.
			</p><p class="cluster-admin cluster-admin">
				RHOSP Octavia has two supported providers: Amphora and OVN. These providers differ in terms of available features as well as implementation details. These distinctions affect load balancer services that are created on your cluster.
			</p><section class="section cluster-admin" id="nw-osp-loadbalancer-etp-local_load-balancing-openstack"><div class="titlepage"><div><div><h3 class="title">33.1.1. Local external traffic policies</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can set the external traffic policy (ETP) parameter, <code class="literal cluster-admin">.spec.externalTrafficPolicy</code>, on a load balancer service to preserve the source IP address of incoming traffic when it reaches service endpoint pods. However, if your cluster uses the Amphora Octavia provider, the source IP of the traffic is replaced with the IP address of the Amphora VM. This behavior does not occur if your cluster uses the OVN Octavia provider.
				</p><p class="cluster-admin cluster-admin">
					Having the <code class="literal cluster-admin">ETP</code> option set to <code class="literal cluster-admin">Local</code> requires that health monitors be created for the load balancer. Without health monitors, traffic can be routed to a node that doesn’t have a functional endpoint, which causes the connection to drop. To force Cloud Provider OpenStack to create health monitors, you must set the value of the <code class="literal cluster-admin">create-monitor</code> option in the cloud provider configuration to <code class="literal cluster-admin">true</code>.
				</p><p class="cluster-admin cluster-admin">
					In RHOSP 16.2, the OVN Octavia provider does not support health monitors. Therefore, setting the ETP to local is unsupported.
				</p><p class="cluster-admin cluster-admin">
					In RHOSP 16.2, the Amphora Octavia provider does not support HTTP monitors on UDP pools. As a result, UDP load balancer services have <code class="literal cluster-admin">UDP-CONNECT</code> monitors created instead. Due to implementation details, this configuration only functions properly with the OVN-Kubernetes CNI plugin. When the OpenShift SDN CNI plugin is used, the UDP services alive nodes are detected unreliably.
				</p></section><section class="section cluster-admin" id="nw-osp-loadbalancer-source-ranges_load-balancing-openstack"><div class="titlepage"><div><div><h3 class="title">33.1.2. Load balancer source ranges</h3></div></div></div><p class="cluster-admin cluster-admin">
					Use the <code class="literal cluster-admin">.spec.loadBalancerSourceRanges</code> property to restrict the traffic that can pass through the load balancer according to source IP. This property is supported for use with the Amphora Octavia provider only. If your cluster uses the OVN Octavia provider, the option is ignored and traffic is unrestricted.
				</p></section></section><section class="section cluster-admin" id="installation-osp-kuryr-octavia-configure_load-balancing-openstack"><div class="titlepage"><div><div><h2 class="title">33.2. Using the Octavia OVN load balancer provider driver with Kuryr SDN</h2></div></div></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
					Kuryr is a deprecated feature. Deprecated functionality is still included in OpenShift Container Platform and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments.
				</p><p class="cluster-admin cluster-admin">
					For the most recent list of major functionality that has been deprecated or removed within OpenShift Container Platform, refer to the <span class="emphasis"><em><span class="cluster-admin cluster-admin">Deprecated and removed features</span></em></span> section of the OpenShift Container Platform release notes.
				</p></div></div><p class="cluster-admin cluster-admin">
				If your OpenShift Container Platform cluster uses Kuryr and was installed on a Red Hat OpenStack Platform (RHOSP) 13 cloud that was later upgraded to RHOSP 16, you can configure it to use the Octavia OVN provider driver.
			</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
					Kuryr replaces existing load balancers after you change provider drivers. This process results in some downtime.
				</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
						Install the RHOSP CLI, <code class="literal cluster-admin">openstack</code>.
					</li><li class="listitem">
						Install the OpenShift Container Platform CLI, <code class="literal cluster-admin">oc</code>.
					</li><li class="listitem"><p class="simpara">
						Verify that the Octavia OVN driver on RHOSP is enabled.
					</p><div class="admonition tip cluster-admin"><div class="admonition_header">Tip</div><div><p class="cluster-admin cluster-admin">
						To view a list of available Octavia drivers, on a command line, enter <code class="literal cluster-admin">openstack loadbalancer provider list</code>.
					</p><p class="cluster-admin cluster-admin">
						The <code class="literal cluster-admin">ovn</code> driver is displayed in the command’s output.
					</p></div></div></li></ul></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Procedure</strong></p><p>
					To change from the Octavia Amphora provider driver to Octavia OVN:
				</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
						Open the <code class="literal cluster-admin">kuryr-config</code> ConfigMap. On a command line, enter:
					</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n openshift-kuryr edit cm kuryr-config</pre></li><li class="listitem"><p class="simpara">
						In the ConfigMap, delete the line that contains <code class="literal cluster-admin">kuryr-octavia-provider: default</code>. For example:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">...
kind: ConfigMap
metadata:
  annotations:
    networkoperator.openshift.io/kuryr-octavia-provider: default <span id="CO224-1"><!--Empty--></span><span class="callout">1</span>
...</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO224-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Delete this line. The cluster will regenerate it with <code class="literal cluster-admin">ovn</code> as the value.
							</div></dd></dl></div><p class="cluster-admin cluster-admin">
						Wait for the Cluster Network Operator to detect the modification and to redeploy the <code class="literal cluster-admin">kuryr-controller</code> and <code class="literal cluster-admin">kuryr-cni</code> pods. This process might take several minutes.
					</p></li><li class="listitem"><p class="simpara">
						Verify that the <code class="literal cluster-admin">kuryr-config</code> ConfigMap annotation is present with <code class="literal cluster-admin">ovn</code> as its value. On a command line, enter:
					</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n openshift-kuryr edit cm kuryr-config</pre><p class="cluster-admin cluster-admin">
						The <code class="literal cluster-admin">ovn</code> provider value is displayed in the output:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">...
kind: ConfigMap
metadata:
  annotations:
    networkoperator.openshift.io/kuryr-octavia-provider: ovn
...</pre></li><li class="listitem"><p class="simpara">
						Verify that RHOSP recreated its load balancers.
					</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
								On a command line, enter:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ openstack loadbalancer list | grep amphora</pre><p class="cluster-admin cluster-admin">
								A single Amphora load balancer is displayed. For example:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">a4db683b-2b7b-4988-a582-c39daaad7981 | ostest-7mbj6-kuryr-api-loadbalancer  | 84c99c906edd475ba19478a9a6690efd | 172.30.0.1     | ACTIVE              | amphora</pre></li><li class="listitem"><p class="simpara">
								Search for <code class="literal cluster-admin">ovn</code> load balancers by entering:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ openstack loadbalancer list | grep ovn</pre><p class="cluster-admin cluster-admin">
								The remaining load balancers of the <code class="literal cluster-admin">ovn</code> type are displayed. For example:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">2dffe783-98ae-4048-98d0-32aa684664cc | openshift-apiserver-operator/metrics | 84c99c906edd475ba19478a9a6690efd | 172.30.167.119 | ACTIVE              | ovn
0b1b2193-251f-4243-af39-2f99b29d18c5 | openshift-etcd/etcd                  | 84c99c906edd475ba19478a9a6690efd | 172.30.143.226 | ACTIVE              | ovn
f05b07fc-01b7-4673-bd4d-adaa4391458e | openshift-dns-operator/metrics       | 84c99c906edd475ba19478a9a6690efd | 172.30.152.27  | ACTIVE              | ovn</pre></li></ol></div></li></ol></div></section><section class="section cluster-admin" id="installation-osp-api-octavia_load-balancing-openstack"><div class="titlepage"><div><div><h2 class="title">33.3. Scaling clusters for application traffic by using Octavia</h2></div></div></div><p class="cluster-admin cluster-admin">
				OpenShift Container Platform clusters that run on Red Hat OpenStack Platform (RHOSP) can use the Octavia load balancing service to distribute traffic across multiple virtual machines (VMs) or floating IP addresses. This feature mitigates the bottleneck that single machines or addresses create.
			</p><p class="cluster-admin cluster-admin">
				If your cluster uses Kuryr, the Cluster Network Operator created an internal Octavia load balancer at deployment. You can use this load balancer for application network scaling.
			</p><p class="cluster-admin cluster-admin">
				If your cluster does not use Kuryr, you must create your own Octavia load balancer to use it for application network scaling.
			</p><section class="section cluster-admin" id="installation-osp-api-scaling_load-balancing-openstack"><div class="titlepage"><div><div><h3 class="title">33.3.1. Scaling clusters by using Octavia</h3></div></div></div><p class="cluster-admin cluster-admin">
					If you want to use multiple API load balancers, or if your cluster does not use Kuryr, create an Octavia load balancer and then configure your cluster to use it.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Octavia is available on your Red Hat OpenStack Platform (RHOSP) deployment.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							From a command line, create an Octavia load balancer that uses the Amphora driver:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ openstack loadbalancer create --name API_OCP_CLUSTER --vip-subnet-id &lt;id_of_worker_vms_subnet&gt;</pre><p class="cluster-admin cluster-admin">
							You can use a name of your choice instead of <code class="literal cluster-admin">API_OCP_CLUSTER</code>.
						</p></li><li class="listitem"><p class="simpara">
							After the load balancer becomes active, create listeners:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ openstack loadbalancer listener create --name API_OCP_CLUSTER_6443 --protocol HTTPS--protocol-port 6443 API_OCP_CLUSTER</pre><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								To view the status of the load balancer, enter <code class="literal cluster-admin">openstack loadbalancer list</code>.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Create a pool that uses the round robin algorithm and has session persistence enabled:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ openstack loadbalancer pool create --name API_OCP_CLUSTER_pool_6443 --lb-algorithm ROUND_ROBIN --session-persistence type=&lt;source_IP_address&gt; --listener API_OCP_CLUSTER_6443 --protocol HTTPS</pre></li><li class="listitem"><p class="simpara">
							To ensure that control plane machines are available, create a health monitor:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ openstack loadbalancer healthmonitor create --delay 5 --max-retries 4 --timeout 10 --type TCP API_OCP_CLUSTER_pool_6443</pre></li><li class="listitem"><p class="simpara">
							Add the control plane machines as members of the load balancer pool:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ for SERVER in $(MASTER-0-IP MASTER-1-IP MASTER-2-IP)
do
  openstack loadbalancer member create --address $SERVER  --protocol-port 6443 API_OCP_CLUSTER_pool_6443
done</pre></li><li class="listitem"><p class="simpara">
							Optional: To reuse the cluster API floating IP address, unset it:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ openstack floating ip unset $API_FIP</pre></li><li class="listitem"><p class="simpara">
							Add either the unset <code class="literal cluster-admin">API_FIP</code> or a new address to the created load balancer VIP:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ openstack floating ip set  --port $(openstack loadbalancer show -c &lt;vip_port_id&gt; -f value API_OCP_CLUSTER) $API_FIP</pre></li></ol></div><p class="cluster-admin cluster-admin">
					Your cluster now uses Octavia for load balancing.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						If Kuryr uses the Octavia Amphora driver, all traffic is routed through a single Amphora virtual machine (VM).
					</p><p class="cluster-admin cluster-admin">
						You can repeat this procedure to create additional load balancers, which can alleviate the bottleneck.
					</p></div></div></section><section class="section cluster-admin" id="installation-osp-kuryr-api-scaling_load-balancing-openstack"><div class="titlepage"><div><div><h3 class="title">33.3.2. Scaling clusters that use Kuryr by using Octavia</h3></div></div></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						Kuryr is a deprecated feature. Deprecated functionality is still included in OpenShift Container Platform and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments.
					</p><p class="cluster-admin cluster-admin">
						For the most recent list of major functionality that has been deprecated or removed within OpenShift Container Platform, refer to the <span class="emphasis"><em><span class="cluster-admin cluster-admin">Deprecated and removed features</span></em></span> section of the OpenShift Container Platform release notes.
					</p></div></div><p class="cluster-admin cluster-admin">
					If your cluster uses Kuryr, associate the API floating IP address of your cluster with the pre-existing Octavia load balancer.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Your OpenShift Container Platform cluster uses Kuryr.
						</li><li class="listitem">
							Octavia is available on your Red Hat OpenStack Platform (RHOSP) deployment.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Optional: From a command line, to reuse the cluster API floating IP address, unset it:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ openstack floating ip unset $API_FIP</pre></li><li class="listitem"><p class="simpara">
							Add either the unset <code class="literal cluster-admin">API_FIP</code> or a new address to the created load balancer VIP:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ openstack floating ip set --port $(openstack loadbalancer show -c &lt;vip_port_id&gt; -f value ${OCP_CLUSTER}-kuryr-api-loadbalancer) $API_FIP</pre></li></ol></div><p class="cluster-admin cluster-admin">
					Your cluster now uses Octavia for load balancing.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						If Kuryr uses the Octavia Amphora driver, all traffic is routed through a single Amphora virtual machine (VM).
					</p><p class="cluster-admin cluster-admin">
						You can repeat this procedure to create additional load balancers, which can alleviate the bottleneck.
					</p></div></div></section></section><section class="section cluster-admin" id="installation-osp-kuryr-octavia-scale_load-balancing-openstack"><div class="titlepage"><div><div><h2 class="title">33.4. Scaling for ingress traffic by using RHOSP Octavia</h2></div></div></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
					Kuryr is a deprecated feature. Deprecated functionality is still included in OpenShift Container Platform and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments.
				</p><p class="cluster-admin cluster-admin">
					For the most recent list of major functionality that has been deprecated or removed within OpenShift Container Platform, refer to the <span class="emphasis"><em><span class="cluster-admin cluster-admin">Deprecated and removed features</span></em></span> section of the OpenShift Container Platform release notes.
				</p></div></div><p class="cluster-admin cluster-admin">
				You can use Octavia load balancers to scale Ingress controllers on clusters that use Kuryr.
			</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
						Your OpenShift Container Platform cluster uses Kuryr.
					</li><li class="listitem">
						Octavia is available on your RHOSP deployment.
					</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
						To copy the current internal router service, on a command line, enter:
					</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n openshift-ingress get svc router-internal-default -o yaml &gt; external_router.yaml</pre></li><li class="listitem"><p class="simpara">
						In the file <code class="literal cluster-admin">external_router.yaml</code>, change the values of <code class="literal cluster-admin">metadata.name</code> and <code class="literal cluster-admin">spec.type</code> to <code class="literal cluster-admin">LoadBalancer</code>.
					</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example router file</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: v1
kind: Service
metadata:
  labels:
    ingresscontroller.operator.openshift.io/owning-ingresscontroller: default
  name: router-external-default <span id="CO225-1"><!--Empty--></span><span class="callout">1</span>
  namespace: openshift-ingress
spec:
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: http
  - name: https
    port: 443
    protocol: TCP
    targetPort: https
  - name: metrics
    port: 1936
    protocol: TCP
    targetPort: 1936
  selector:
    ingresscontroller.operator.openshift.io/deployment-ingresscontroller: default
  sessionAffinity: None
  type: LoadBalancer <span id="CO225-2"><!--Empty--></span><span class="callout">2</span></pre>

						</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO225-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Ensure that this value is descriptive, like <code class="literal cluster-admin">router-external-default</code>.
							</div></dd><dt><a href="#CO225-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Ensure that this value is <code class="literal cluster-admin">LoadBalancer</code>.
							</div></dd></dl></div></li></ol></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
					You can delete timestamps and other information that is irrelevant to load balancing.
				</p></div></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
						From a command line, create a service from the <code class="literal cluster-admin">external_router.yaml</code> file:
					</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f external_router.yaml</pre></li><li class="listitem"><p class="simpara">
						Verify that the external IP address of the service is the same as the one that is associated with the load balancer:
					</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
								On a command line, retrieve the external IP address of the service:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n openshift-ingress get svc</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                      TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)                                     AGE
router-external-default   LoadBalancer   172.30.235.33    10.46.22.161   80:30112/TCP,443:32359/TCP,1936:30317/TCP   3m38s
router-internal-default   ClusterIP      172.30.115.123   &lt;none&gt;         80/TCP,443/TCP,1936/TCP                     22h</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Retrieve the IP address of the load balancer:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ openstack loadbalancer list | grep router-external</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">| 21bf6afe-b498-4a16-a958-3229e83c002c | openshift-ingress/router-external-default | 66f3816acf1b431691b8d132cc9d793c | 172.30.235.33  | ACTIVE | octavia |</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Verify that the addresses you retrieved in the previous steps are associated with each other in the floating IP list:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ openstack floating ip list | grep 172.30.235.33</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">| e2f80e97-8266-4b69-8636-e58bacf1879e | 10.46.22.161 | 172.30.235.33 | 655e7122-806a-4e0a-a104-220c6e17bda6 | a565e55a-99e7-4d15-b4df-f9d7ee8c9deb | 66f3816acf1b431691b8d132cc9d793c |</pre>

								</p></div></li></ol></div></li></ol></div><p class="cluster-admin cluster-admin">
				You can now use the value of <code class="literal cluster-admin">EXTERNAL-IP</code> as the new Ingress address.
			</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
					If Kuryr uses the Octavia Amphora driver, all traffic is routed through a single Amphora virtual machine (VM).
				</p><p class="cluster-admin cluster-admin">
					You can repeat this procedure to create additional load balancers, which can alleviate the bottleneck.
				</p></div></div></section><section class="section cluster-admin" id="nw-osp-configuring-external-load-balancer_load-balancing-openstack"><div class="titlepage"><div><div><h2 class="title">33.5. Configuring an external load balancer</h2></div></div></div><p class="cluster-admin cluster-admin">
				You can configure an OpenShift Container Platform cluster on Red Hat OpenStack Platform (RHOSP) to use an external load balancer in place of the default load balancer.
			</p><p class="cluster-admin cluster-admin">
				You can also configure an OpenShift Container Platform cluster to use an external load balancer that supports multiple subnets. If you use multiple subnets, you can explicitly list all the IP addresses in any networks that are used by your load balancer targets. This configuration can reduce maintenance overhead because you can create and destroy nodes within those networks without reconfiguring the load balancer targets.
			</p><p class="cluster-admin cluster-admin">
				If you deploy your ingress pods by using a machine set on a smaller network, such as a <code class="literal cluster-admin">/27</code> or <code class="literal cluster-admin">/28</code>, you can simplify your load balancer targets.
			</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
					You do not need to specify API and Ingress static addresses for your installation program. If you choose this configuration, you must take additional actions to define network targets that accept an IP address from each referenced vSphere subnet.
				</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
						On your load balancer, TCP over ports 6443, 443, and 80 must be reachable by all users of your system that are located outside the cluster.
					</li><li class="listitem">
						Load balance the application ports, 443 and 80, between all the compute nodes.
					</li><li class="listitem">
						Load balance the API port, 6443, between each of the control plane nodes.
					</li><li class="listitem">
						On your load balancer, port 22623, which is used to serve ignition startup configurations to nodes, is not exposed outside of the cluster.
					</li><li class="listitem"><p class="simpara">
						Your load balancer can access the required ports on each node in your cluster. You can ensure this level of access by completing the following actions:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem">
								The API load balancer can access ports 22623 and 6443 on the control plane nodes.
							</li><li class="listitem">
								The ingress load balancer can access ports 443 and 80 on the nodes where the ingress pods are located.
							</li></ul></div></li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
						Enable access to the cluster from your load balancer on ports 6443, 443, and 80.
					</p><p class="cluster-admin cluster-admin">
						As an example, note this HAProxy configuration:
					</p><div class="cluster-admin cluster-admin"><p class="title"><strong>A section of a sample HAProxy configuration</strong></p><p>
							
<pre class="programlisting language-text">...
listen my-cluster-api-6443
    bind 0.0.0.0:6443
    mode tcp
    balance roundrobin
    server my-cluster-master-2 192.0.2.2:6443 check
    server my-cluster-master-0 192.0.2.3:6443 check
    server my-cluster-master-1 192.0.2.1:6443 check
listen my-cluster-apps-443
        bind 0.0.0.0:443
        mode tcp
        balance roundrobin
        server my-cluster-worker-0 192.0.2.6:443 check
        server my-cluster-worker-1 192.0.2.5:443 check
        server my-cluster-worker-2 192.0.2.4:443 check
listen my-cluster-apps-80
        bind 0.0.0.0:80
        mode tcp
        balance roundrobin
        server my-cluster-worker-0 192.0.2.7:80 check
        server my-cluster-worker-1 192.0.2.9:80 check
        server my-cluster-worker-2 192.0.2.8:80 check</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Add records to your DNS server for the cluster API and apps over the load balancer. For example:
					</p><pre class="programlisting language-dns cluster-admin cluster-admin">&lt;load_balancer_ip_address&gt; api.&lt;cluster_name&gt;.&lt;base_domain&gt;
&lt;load_balancer_ip_address&gt; apps.&lt;cluster_name&gt;.&lt;base_domain&gt;</pre></li><li class="listitem"><p class="simpara">
						From a command line, use <code class="literal cluster-admin">curl</code> to verify that the external load balancer and DNS configuration are operational.
					</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
								Verify that the cluster API is accessible:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ curl https://&lt;loadbalancer_ip_address&gt;:6443/version --insecure</pre><p class="cluster-admin cluster-admin">
								If the configuration is correct, you receive a JSON object in response:
							</p><pre class="programlisting language-json cluster-admin cluster-admin">{
  "major": "1",
  "minor": "11+",
  "gitVersion": "v1.11.0+ad103ed",
  "gitCommit": "ad103ed",
  "gitTreeState": "clean",
  "buildDate": "2019-01-09T06:44:10Z",
  "goVersion": "go1.10.3",
  "compiler": "gc",
  "platform": "linux/amd64"
}</pre></li><li class="listitem"><p class="simpara">
								Verify that cluster applications are accessible:
							</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
									You can also verify application accessibility by opening the OpenShift Container Platform console in a web browser.
								</p></div></div><pre class="programlisting language-terminal cluster-admin cluster-admin">$ curl http://console-openshift-console.apps.&lt;cluster_name&gt;.&lt;base_domain&gt; -I -L --insecure</pre><p class="cluster-admin cluster-admin">
								If the configuration is correct, you receive an HTTP response:
							</p><pre class="programlisting language-terminal cluster-admin cluster-admin">HTTP/1.1 302 Found
content-length: 0
location: https://console-openshift-console.apps.&lt;cluster-name&gt;.&lt;base domain&gt;/
cache-control: no-cacheHTTP/1.1 200 OK
referrer-policy: strict-origin-when-cross-origin
set-cookie: csrf-token=39HoZgztDnzjJkq/JuLJMeoKNXlfiVv2YgZc09c3TBOBU4NI6kDXaJH1LdicNhN1UsQWzon4Dor9GWGfopaTEQ==; Path=/; Secure
x-content-type-options: nosniff
x-dns-prefetch-control: off
x-frame-options: DENY
x-xss-protection: 1; mode=block
date: Tue, 17 Nov 2020 08:42:10 GMT
content-type: text/html; charset=utf-8
set-cookie: 1e2670d92730b515ce3a1bb65da45062=9b714eb87e93cf34853e87a92d6894be; path=/; HttpOnly; Secure; SameSite=None
cache-control: private</pre></li></ol></div></li></ol></div></section></section><section class="chapter cluster-admin" id="load-balancing-with-metallb"><div class="titlepage"><div><div><h1 class="title">Chapter 34. Load balancing with MetalLB</h1></div></div></div><section class="section cluster-admin" id="about-metallb"><div class="titlepage"><div><div><h2 class="title">34.1. About MetalLB and the MetalLB Operator</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can add the MetalLB Operator to your cluster so that when a service of type <code class="literal cluster-admin">LoadBalancer</code> is added to the cluster, MetalLB can add an external IP address for the service. The external IP address is added to the host network for your cluster.
			</p><section class="section cluster-admin" id="nw-metallb-when-metallb_about-metallb-and-metallb-operator"><div class="titlepage"><div><div><h3 class="title">34.1.1. When to use MetalLB</h3></div></div></div><p class="cluster-admin cluster-admin">
					Using MetalLB is valuable when you have a bare-metal cluster, or an infrastructure that is like bare metal, and you want fault-tolerant access to an application through an external IP address.
				</p><p class="cluster-admin cluster-admin">
					You must configure your networking infrastructure to ensure that network traffic for the external IP address is routed from clients to the host network for the cluster.
				</p><p class="cluster-admin cluster-admin">
					After deploying MetalLB with the MetalLB Operator, when you add a service of type <code class="literal cluster-admin">LoadBalancer</code>, MetalLB provides a platform-native load balancer.
				</p><p class="cluster-admin cluster-admin">
					MetalLB operating in layer2 mode provides support for failover by utilizing a mechanism similar to IP failover. However, instead of relying on the virtual router redundancy protocol (VRRP) and keepalived, MetalLB leverages a gossip-based protocol to identify instances of node failure. When a failover is detected, another node assumes the role of the leader node, and a gratuitous ARP message is dispatched to broadcast this change.
				</p><p class="cluster-admin cluster-admin">
					MetalLB operating in layer3 or border gateway protocol (BGP) mode delegates failure detection to the network. The BGP router or routers that the OpenShift Container Platform nodes have established a connection with will identify any node failure and terminate the routes to that node.
				</p><p class="cluster-admin cluster-admin">
					Using MetalLB instead of IP failover is preferable for ensuring high availability of pods and services.
				</p></section><section class="section cluster-admin" id="nw-metallb-operator-custom-resources_about-metallb-and-metallb-operator"><div class="titlepage"><div><div><h3 class="title">34.1.2. MetalLB Operator custom resources</h3></div></div></div><p class="cluster-admin cluster-admin">
					The MetalLB Operator monitors its own namespace for the following custom resources:
				</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">MetalLB</code></span></dt><dd>
								When you add a <code class="literal cluster-admin">MetalLB</code> custom resource to the cluster, the MetalLB Operator deploys MetalLB on the cluster. The Operator only supports a single instance of the custom resource. If the instance is deleted, the Operator removes MetalLB from the cluster.
							</dd><dt><span class="term"><code class="literal cluster-admin">IPAddressPool</code></span></dt><dd><p class="simpara">
								MetalLB requires one or more pools of IP addresses that it can assign to a service when you add a service of type <code class="literal cluster-admin">LoadBalancer</code>. An <code class="literal cluster-admin">IPAddressPool</code> includes a list of IP addresses. The list can be a single IP address that is set using a range, such as 1.1.1.1-1.1.1.1, a range specified in CIDR notation, a range specified as a starting and ending address separated by a hyphen, or a combination of the three. An <code class="literal cluster-admin">IPAddressPool</code> requires a name. The documentation uses names like <code class="literal cluster-admin">doc-example</code>, <code class="literal cluster-admin">doc-example-reserved</code>, and <code class="literal cluster-admin">doc-example-ipv6</code>. The MetalLB <code class="literal cluster-admin">controller</code> assigns IP addresses from a pool of addresses in an <code class="literal cluster-admin">IPAddressPool</code>. <code class="literal cluster-admin">L2Advertisement</code> and <code class="literal cluster-admin">BGPAdvertisement</code> custom resources enable the advertisement of a given IP from a given pool. You can assign IP addresses from an <code class="literal cluster-admin">IPAddressPool</code> to services and namespaces by using the <code class="literal cluster-admin">spec.serviceAllocation</code> specification in the <code class="literal cluster-admin">IPAddressPool</code> custom resource.
							</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
									A single <code class="literal cluster-admin">IPAddressPool</code> can be referenced by a L2 advertisement and a BGP advertisement.
								</p></div></div></dd><dt><span class="term"><code class="literal cluster-admin">BGPPeer</code></span></dt><dd>
								The BGP peer custom resource identifies the BGP router for MetalLB to communicate with, the AS number of the router, the AS number for MetalLB, and customizations for route advertisement. MetalLB advertises the routes for service load-balancer IP addresses to one or more BGP peers.
							</dd><dt><span class="term"><code class="literal cluster-admin">BFDProfile</code></span></dt><dd>
								The BFD profile custom resource configures Bidirectional Forwarding Detection (BFD) for a BGP peer. BFD provides faster path failure detection than BGP alone provides.
							</dd><dt><span class="term"><code class="literal cluster-admin">L2Advertisement</code></span></dt><dd>
								The L2Advertisement custom resource advertises an IP coming from an <code class="literal cluster-admin">IPAddressPool</code> using the L2 protocol.
							</dd><dt><span class="term"><code class="literal cluster-admin">BGPAdvertisement</code></span></dt><dd>
								The BGPAdvertisement custom resource advertises an IP coming from an <code class="literal cluster-admin">IPAddressPool</code> using the BGP protocol.
							</dd></dl></div><p class="cluster-admin cluster-admin">
					After you add the <code class="literal cluster-admin">MetalLB</code> custom resource to the cluster and the Operator deploys MetalLB, the <code class="literal cluster-admin">controller</code> and <code class="literal cluster-admin">speaker</code> MetalLB software components begin running.
				</p><p class="cluster-admin cluster-admin">
					MetalLB validates all relevant custom resources.
				</p></section><section class="section cluster-admin" id="nw-metallb-software-components_about-metallb-and-metallb-operator"><div class="titlepage"><div><div><h3 class="title">34.1.3. MetalLB software components</h3></div></div></div><p class="cluster-admin cluster-admin">
					When you install the MetalLB Operator, the <code class="literal cluster-admin">metallb-operator-controller-manager</code> deployment starts a pod. The pod is the implementation of the Operator. The pod monitors for changes to all the relevant resources.
				</p><p class="cluster-admin cluster-admin">
					When the Operator starts an instance of MetalLB, it starts a <code class="literal cluster-admin">controller</code> deployment and a <code class="literal cluster-admin">speaker</code> daemon set.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						You can configure deployment specifications in the MetalLB custom resource to manage how <code class="literal cluster-admin">controller</code> and <code class="literal cluster-admin">speaker</code> pods deploy and run in your cluster. For more information about these deployment specifications, see the <span class="emphasis"><em><span class="cluster-admin cluster-admin">Additional Resources</span></em></span> section.
					</p></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">controller</code></span></dt><dd><p class="simpara">
								The Operator starts the deployment and a single pod. When you add a service of type <code class="literal cluster-admin">LoadBalancer</code>, Kubernetes uses the <code class="literal cluster-admin">controller</code> to allocate an IP address from an address pool. In case of a service failure, verify you have the following entry in your <code class="literal cluster-admin">controller</code> pod logs:
							</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">"event":"ipAllocated","ip":"172.22.0.201","msg":"IP address assigned by controller</pre>

								</p></div></dd><dt><span class="term"><code class="literal cluster-admin">speaker</code></span></dt><dd><p class="simpara">
								The Operator starts a daemon set for <code class="literal cluster-admin">speaker</code> pods. By default, a pod is started on each node in your cluster. You can limit the pods to specific nodes by specifying a node selector in the <code class="literal cluster-admin">MetalLB</code> custom resource when you start MetalLB. If the <code class="literal cluster-admin">controller</code> allocated the IP address to the service and service is still unavailable, read the <code class="literal cluster-admin">speaker</code> pod logs. If the <code class="literal cluster-admin">speaker</code> pod is unavailable, run the <code class="literal cluster-admin">oc describe pod -n</code> command.
							</p><p class="cluster-admin cluster-admin">
								For layer 2 mode, after the <code class="literal cluster-admin">controller</code> allocates an IP address for the service, the <code class="literal cluster-admin">speaker</code> pods use an algorithm to determine which <code class="literal cluster-admin">speaker</code> pod on which node will announce the load balancer IP address. The algorithm involves hashing the node name and the load balancer IP address. For more information, see "MetalLB and external traffic policy". The <code class="literal cluster-admin">speaker</code> uses Address Resolution Protocol (ARP) to announce IPv4 addresses and Neighbor Discovery Protocol (NDP) to announce IPv6 addresses.
							</p></dd></dl></div><p class="cluster-admin cluster-admin">
					For Border Gateway Protocol (BGP) mode, after the <code class="literal cluster-admin">controller</code> allocates an IP address for the service, each <code class="literal cluster-admin">speaker</code> pod advertises the load balancer IP address with its BGP peers. You can configure which nodes start BGP sessions with BGP peers.
				</p><p class="cluster-admin cluster-admin">
					Requests for the load balancer IP address are routed to the node with the <code class="literal cluster-admin">speaker</code> that announces the IP address. After the node receives the packets, the service proxy routes the packets to an endpoint for the service. The endpoint can be on the same node in the optimal case, or it can be on another node. The service proxy chooses an endpoint each time a connection is established.
				</p></section><section class="section cluster-admin" id="nw-metallb-extern-traffic-pol_about-metallb-and-metallb-operator"><div class="titlepage"><div><div><h3 class="title">34.1.4. MetalLB and external traffic policy</h3></div></div></div><p class="cluster-admin cluster-admin">
					With layer 2 mode, one node in your cluster receives all the traffic for the service IP address. With BGP mode, a router on the host network opens a connection to one of the nodes in the cluster for a new client connection. How your cluster handles the traffic after it enters the node is affected by the external traffic policy.
				</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term"><code class="literal cluster-admin">cluster</code></span></dt><dd><p class="simpara">
								This is the default value for <code class="literal cluster-admin">spec.externalTrafficPolicy</code>.
							</p><p class="cluster-admin cluster-admin">
								With the <code class="literal cluster-admin">cluster</code> traffic policy, after the node receives the traffic, the service proxy distributes the traffic to all the pods in your service. This policy provides uniform traffic distribution across the pods, but it obscures the client IP address and it can appear to the application in your pods that the traffic originates from the node rather than the client.
							</p></dd><dt><span class="term"><code class="literal cluster-admin">local</code></span></dt><dd><p class="simpara">
								With the <code class="literal cluster-admin">local</code> traffic policy, after the node receives the traffic, the service proxy only sends traffic to the pods on the same node. For example, if the <code class="literal cluster-admin">speaker</code> pod on node A announces the external service IP, then all traffic is sent to node A. After the traffic enters node A, the service proxy only sends traffic to pods for the service that are also on node A. Pods for the service that are on additional nodes do not receive any traffic from node A. Pods for the service on additional nodes act as replicas in case failover is needed.
							</p><p class="cluster-admin cluster-admin">
								This policy does not affect the client IP address. Application pods can determine the client IP address from the incoming connections.
							</p></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						The following information is important when configuring the external traffic policy in BGP mode.
					</p><p class="cluster-admin cluster-admin">
						Although MetalLB advertises the load balancer IP address from all the eligible nodes, the number of nodes loadbalancing the service can be limited by the capacity of the router to establish equal-cost multipath (ECMP) routes. If the number of nodes advertising the IP is greater than the ECMP group limit of the router, the router will use less nodes than the ones advertising the IP.
					</p><p class="cluster-admin cluster-admin">
						For example, if the external traffic policy is set to <code class="literal cluster-admin">local</code> and the router has an ECMP group limit set to 16 and the pods implementing a LoadBalancer service are deployed on 30 nodes, this would result in pods deployed on 14 nodes not receiving any traffic. In this situation, it would be preferable to set the external traffic policy for the service to <code class="literal cluster-admin">cluster</code>.
					</p></div></div></section><section class="section cluster-admin" id="nw-metallb-layer2_about-metallb-and-metallb-operator"><div class="titlepage"><div><div><h3 class="title">34.1.5. MetalLB concepts for layer 2 mode</h3></div></div></div><p class="cluster-admin cluster-admin">
					In layer 2 mode, the <code class="literal cluster-admin">speaker</code> pod on one node announces the external IP address for a service to the host network. From a network perspective, the node appears to have multiple IP addresses assigned to a network interface.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						In layer 2 mode, MetalLB relies on ARP and NDP. These protocols implement local address resolution within a specific subnet. In this context, the client must be able to reach the VIP assigned by MetalLB that exists on the same subnet as the nodes announcing the service in order for MetalLB to work.
					</p></div></div><p class="cluster-admin cluster-admin">
					The <code class="literal cluster-admin">speaker</code> pod responds to ARP requests for IPv4 services and NDP requests for IPv6.
				</p><p class="cluster-admin cluster-admin">
					In layer 2 mode, all traffic for a service IP address is routed through one node. After traffic enters the node, the service proxy for the CNI network provider distributes the traffic to all the pods for the service.
				</p><p class="cluster-admin cluster-admin">
					Because all traffic for a service enters through a single node in layer 2 mode, in a strict sense, MetalLB does not implement a load balancer for layer 2. Rather, MetalLB implements a failover mechanism for layer 2 so that when a <code class="literal cluster-admin">speaker</code> pod becomes unavailable, a <code class="literal cluster-admin">speaker</code> pod on a different node can announce the service IP address.
				</p><p class="cluster-admin cluster-admin">
					When a node becomes unavailable, failover is automatic. The <code class="literal cluster-admin">speaker</code> pods on the other nodes detect that a node is unavailable and a new <code class="literal cluster-admin">speaker</code> pod and node take ownership of the service IP address from the failed node.
				</p><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/41d8213d50dd7449fd60c44c47ffd01f/nw-metallb-layer2.png" alt="Conceptual diagram for MetalLB and layer 2 mode"/></div></div><p class="cluster-admin cluster-admin">
					The preceding graphic shows the following concepts related to MetalLB:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							An application is available through a service that has a cluster IP on the <code class="literal cluster-admin">172.130.0.0/16</code> subnet. That IP address is accessible from inside the cluster. The service also has an external IP address that MetalLB assigned to the service, <code class="literal cluster-admin">192.168.100.200</code>.
						</li><li class="listitem">
							Nodes 1 and 3 have a pod for the application.
						</li><li class="listitem">
							The <code class="literal cluster-admin">speaker</code> daemon set runs a pod on each node. The MetalLB Operator starts these pods.
						</li><li class="listitem">
							Each <code class="literal cluster-admin">speaker</code> pod is a host-networked pod. The IP address for the pod is identical to the IP address for the node on the host network.
						</li><li class="listitem">
							The <code class="literal cluster-admin">speaker</code> pod on node 1 uses ARP to announce the external IP address for the service, <code class="literal cluster-admin">192.168.100.200</code>. The <code class="literal cluster-admin">speaker</code> pod that announces the external IP address must be on the same node as an endpoint for the service and the endpoint must be in the <code class="literal cluster-admin">Ready</code> condition.
						</li><li class="listitem"><p class="simpara">
							Client traffic is routed to the host network and connects to the <code class="literal cluster-admin">192.168.100.200</code> IP address. After traffic enters the node, the service proxy sends the traffic to the application pod on the same node or another node according to the external traffic policy that you set for the service.
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem">
									If the external traffic policy for the service is set to <code class="literal cluster-admin">cluster</code>, the node that advertises the <code class="literal cluster-admin">192.168.100.200</code> load balancer IP address is selected from the nodes where a <code class="literal cluster-admin">speaker</code> pod is running. Only that node can receive traffic for the service.
								</li><li class="listitem">
									If the external traffic policy for the service is set to <code class="literal cluster-admin">local</code>, the node that advertises the <code class="literal cluster-admin">192.168.100.200</code> load balancer IP address is selected from the nodes where a <code class="literal cluster-admin">speaker</code> pod is running and at least an endpoint of the service. Only that node can receive traffic for the service. In the preceding graphic, either node 1 or 3 would advertise <code class="literal cluster-admin">192.168.100.200</code>.
								</li></ul></div></li><li class="listitem">
							If node 1 becomes unavailable, the external IP address fails over to another node. On another node that has an instance of the application pod and service endpoint, the <code class="literal cluster-admin">speaker</code> pod begins to announce the external IP address, <code class="literal cluster-admin">192.168.100.200</code> and the new node receives the client traffic. In the diagram, the only candidate is node 3.
						</li></ul></div></section><section class="section cluster-admin" id="nw-metallb-bgp_about-metallb-and-metallb-operator"><div class="titlepage"><div><div><h3 class="title">34.1.6. MetalLB concepts for BGP mode</h3></div></div></div><p class="cluster-admin cluster-admin">
					In BGP mode, by default each <code class="literal cluster-admin">speaker</code> pod advertises the load balancer IP address for a service to each BGP peer. It is also possible to advertise the IPs coming from a given pool to a specific set of peers by adding an optional list of BGP peers. BGP peers are commonly network routers that are configured to use the BGP protocol. When a router receives traffic for the load balancer IP address, the router picks one of the nodes with a <code class="literal cluster-admin">speaker</code> pod that advertised the IP address. The router sends the traffic to that node. After traffic enters the node, the service proxy for the CNI network plugin distributes the traffic to all the pods for the service.
				</p><p class="cluster-admin cluster-admin">
					The directly-connected router on the same layer 2 network segment as the cluster nodes can be configured as a BGP peer. If the directly-connected router is not configured as a BGP peer, you need to configure your network so that packets for load balancer IP addresses are routed between the BGP peers and the cluster nodes that run the <code class="literal cluster-admin">speaker</code> pods.
				</p><p class="cluster-admin cluster-admin">
					Each time a router receives new traffic for the load balancer IP address, it creates a new connection to a node. Each router manufacturer has an implementation-specific algorithm for choosing which node to initiate the connection with. However, the algorithms commonly are designed to distribute traffic across the available nodes for the purpose of balancing the network load.
				</p><p class="cluster-admin cluster-admin">
					If a node becomes unavailable, the router initiates a new connection with another node that has a <code class="literal cluster-admin">speaker</code> pod that advertises the load balancer IP address.
				</p><div class="figure" id="idm140587125362112"><p class="title"><strong>Figure 34.1. MetalLB topology diagram for BGP mode</strong></p><div class="figure-contents"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/afe4598d665b24b7a193bfd5b4440d48/209_OpenShift_BGP_0122.png" alt="Speaker pods on host network 10.0.1.0/24 use BGP to advertise the load balancer IP address, 203.0.113.200, to a router."/></div></div></div><p class="cluster-admin cluster-admin">
					The preceding graphic shows the following concepts related to MetalLB:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							An application is available through a service that has an IPv4 cluster IP on the <code class="literal cluster-admin">172.130.0.0/16</code> subnet. That IP address is accessible from inside the cluster. The service also has an external IP address that MetalLB assigned to the service, <code class="literal cluster-admin">203.0.113.200</code>.
						</li><li class="listitem">
							Nodes 2 and 3 have a pod for the application.
						</li><li class="listitem">
							The <code class="literal cluster-admin">speaker</code> daemon set runs a pod on each node. The MetalLB Operator starts these pods. You can configure MetalLB to specify which nodes run the <code class="literal cluster-admin">speaker</code> pods.
						</li><li class="listitem">
							Each <code class="literal cluster-admin">speaker</code> pod is a host-networked pod. The IP address for the pod is identical to the IP address for the node on the host network.
						</li><li class="listitem">
							Each <code class="literal cluster-admin">speaker</code> pod starts a BGP session with all BGP peers and advertises the load balancer IP addresses or aggregated routes to the BGP peers. The <code class="literal cluster-admin">speaker</code> pods advertise that they are part of Autonomous System 65010. The diagram shows a router, R1, as a BGP peer within the same Autonomous System. However, you can configure MetalLB to start BGP sessions with peers that belong to other Autonomous Systems.
						</li><li class="listitem"><p class="simpara">
							All the nodes with a <code class="literal cluster-admin">speaker</code> pod that advertises the load balancer IP address can receive traffic for the service.
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem">
									If the external traffic policy for the service is set to <code class="literal cluster-admin">cluster</code>, all the nodes where a speaker pod is running advertise the <code class="literal cluster-admin">203.0.113.200</code> load balancer IP address and all the nodes with a <code class="literal cluster-admin">speaker</code> pod can receive traffic for the service. The host prefix is advertised to the router peer only if the external traffic policy is set to cluster.
								</li><li class="listitem">
									If the external traffic policy for the service is set to <code class="literal cluster-admin">local</code>, then all the nodes where a <code class="literal cluster-admin">speaker</code> pod is running and at least an endpoint of the service is running can advertise the <code class="literal cluster-admin">203.0.113.200</code> load balancer IP address. Only those nodes can receive traffic for the service. In the preceding graphic, nodes 2 and 3 would advertise <code class="literal cluster-admin">203.0.113.200</code>.
								</li></ul></div></li><li class="listitem">
							You can configure MetalLB to control which <code class="literal cluster-admin">speaker</code> pods start BGP sessions with specific BGP peers by specifying a node selector when you add a BGP peer custom resource.
						</li><li class="listitem">
							Any routers, such as R1, that are configured to use BGP can be set as BGP peers.
						</li><li class="listitem">
							Client traffic is routed to one of the nodes on the host network. After traffic enters the node, the service proxy sends the traffic to the application pod on the same node or another node according to the external traffic policy that you set for the service.
						</li><li class="listitem">
							If a node becomes unavailable, the router detects the failure and initiates a new connection with another node. You can configure MetalLB to use a Bidirectional Forwarding Detection (BFD) profile for BGP peers. BFD provides faster link failure detection so that routers can initiate new connections earlier than without BFD.
						</li></ul></div></section><section class="section cluster-admin" id="limitations-and-restrictions_about-metallb-and-metallb-operator"><div class="titlepage"><div><div><h3 class="title">34.1.7. Limitations and restrictions</h3></div></div></div><section class="section cluster-admin" id="nw-metallb-infra-considerations_about-metallb-and-metallb-operator"><div class="titlepage"><div><div><h4 class="title">34.1.7.1. Infrastructure considerations for MetalLB</h4></div></div></div><p class="cluster-admin cluster-admin">
						MetalLB is primarily useful for on-premise, bare metal installations because these installations do not include a native load-balancer capability. In addition to bare metal installations, installations of OpenShift Container Platform on some infrastructures might not include a native load-balancer capability. For example, the following infrastructures can benefit from adding the MetalLB Operator:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Bare metal
							</li><li class="listitem">
								VMware vSphere
							</li><li class="listitem">
								IBM Z and IBM® LinuxONE
							</li><li class="listitem">
								IBM Z and IBM® LinuxONE for Red Hat Enterprise Linux (RHEL) KVM
							</li><li class="listitem">
								IBM Power
							</li></ul></div><p class="cluster-admin cluster-admin">
						MetalLB Operator and MetalLB are supported with the OpenShift SDN and OVN-Kubernetes network providers.
					</p></section><section class="section cluster-admin" id="nw-metallb-layer2-limitations_about-metallb-and-metallb-operator"><div class="titlepage"><div><div><h4 class="title">34.1.7.2. Limitations for layer 2 mode</h4></div></div></div><section class="section cluster-admin" id="nw-metallb-layer2-limitations-bottleneck_about-metallb-and-metallb-operator"><div class="titlepage"><div><div><h5 class="title">34.1.7.2.1. Single-node bottleneck</h5></div></div></div><p class="cluster-admin cluster-admin">
							MetalLB routes all traffic for a service through a single node, the node can become a bottleneck and limit performance.
						</p><p class="cluster-admin cluster-admin">
							Layer 2 mode limits the ingress bandwidth for your service to the bandwidth of a single node. This is a fundamental limitation of using ARP and NDP to direct traffic.
						</p></section><section class="section cluster-admin" id="nw-metallb-layer2-limitations-failover_about-metallb-and-metallb-operator"><div class="titlepage"><div><div><h5 class="title">34.1.7.2.2. Slow failover performance</h5></div></div></div><p class="cluster-admin cluster-admin">
							Failover between nodes depends on cooperation from the clients. When a failover occurs, MetalLB sends gratuitous ARP packets to notify clients that the MAC address associated with the service IP has changed.
						</p><p class="cluster-admin cluster-admin">
							Most client operating systems handle gratuitous ARP packets correctly and update their neighbor caches promptly. When clients update their caches quickly, failover completes within a few seconds. Clients typically fail over to a new node within 10 seconds. However, some client operating systems either do not handle gratuitous ARP packets at all or have outdated implementations that delay the cache update.
						</p><p class="cluster-admin cluster-admin">
							Recent versions of common operating systems such as Windows, macOS, and Linux implement layer 2 failover correctly. Issues with slow failover are not expected except for older and less common client operating systems.
						</p><p class="cluster-admin cluster-admin">
							To minimize the impact from a planned failover on outdated clients, keep the old node running for a few minutes after flipping leadership. The old node can continue to forward traffic for outdated clients until their caches refresh.
						</p><p class="cluster-admin cluster-admin">
							During an unplanned failover, the service IPs are unreachable until the outdated clients refresh their cache entries.
						</p></section></section><section class="section cluster-admin" id="nw-metallb-bgp-limitations_about-metallb-and-metallb-operator"><div class="titlepage"><div><div><h4 class="title">34.1.7.3. Limitations for BGP mode</h4></div></div></div><section class="section cluster-admin" id="nw-metallb-bgp-limitations-break-connections_about-metallb-and-metallb-operator"><div class="titlepage"><div><div><h5 class="title">34.1.7.3.1. Node failure can break all active connections</h5></div></div></div><p class="cluster-admin cluster-admin">
							MetalLB shares a limitation that is common to BGP-based load balancing. When a BGP session terminates, such as when a node fails or when a <code class="literal cluster-admin">speaker</code> pod restarts, the session termination might result in resetting all active connections. End users can experience a <code class="literal cluster-admin">Connection reset by peer</code> message.
						</p><p class="cluster-admin cluster-admin">
							The consequence of a terminated BGP session is implementation-specific for each router manufacturer. However, you can anticipate that a change in the number of <code class="literal cluster-admin">speaker</code> pods affects the number of BGP sessions and that active connections with BGP peers will break.
						</p><p class="cluster-admin cluster-admin">
							To avoid or reduce the likelihood of a service interruption, you can specify a node selector when you add a BGP peer. By limiting the number of nodes that start BGP sessions, a fault on a node that does not have a BGP session has no affect on connections to the service.
						</p></section><section class="section cluster-admin" id="nw-metallb-bgp-limitations-single-asn_about-metallb-and-metallb-operator"><div class="titlepage"><div><div><h5 class="title">34.1.7.3.2. Support for a single ASN and a single router ID only</h5></div></div></div><p class="cluster-admin cluster-admin">
							When you add a BGP peer custom resource, you specify the <code class="literal cluster-admin">spec.myASN</code> field to identify the Autonomous System Number (ASN) that MetalLB belongs to. OpenShift Container Platform uses an implementation of BGP with MetalLB that requires MetalLB to belong to a single ASN. If you attempt to add a BGP peer and specify a different value for <code class="literal cluster-admin">spec.myASN</code> than an existing BGP peer custom resource, you receive an error.
						</p><p class="cluster-admin cluster-admin">
							Similarly, when you add a BGP peer custom resource, the <code class="literal cluster-admin">spec.routerID</code> field is optional. If you specify a value for this field, you must specify the same value for all other BGP peer custom resources that you add.
						</p><p class="cluster-admin cluster-admin">
							The limitation to support a single ASN and single router ID is a difference with the community-supported implementation of MetalLB.
						</p></section></section></section><section class="section _additional-resources" id="additional-resources_about-metallb-and-metallb-operator"><div class="titlepage"><div><div><h3 class="title">34.1.8. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#overview-traffic-comparision_overview-traffic">Comparison: Fault tolerant access to external IP addresses</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-ipfailover-remove_configuring-ipfailover">Removing IP failover</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-metallb-operator-deployment-specifications-for-metallb_metallb-operator-install">Deployment specifications for MetalLB</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="metallb-operator-install"><div class="titlepage"><div><div><h2 class="title">34.2. Installing the MetalLB Operator</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can add the MetallB Operator so that the Operator can manage the lifecycle for an instance of MetalLB on your cluster.
			</p><p class="cluster-admin cluster-admin">
				MetalLB and IP failover are incompatible. If you configured IP failover for your cluster, perform the steps to <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-ipfailover-remove_configuring-ipfailover">remove IP failover</a> before you install the Operator.
			</p><section class="section cluster-admin" id="installing-the-metallb-operator-using-web-console_metallb-operator-install"><div class="titlepage"><div><div><h3 class="title">34.2.1. Installing the MetalLB Operator from the OperatorHub using the web console</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can install the MetalLB Operator by using the OpenShift Container Platform web console.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">OperatorHub</span></strong></span>.
						</li><li class="listitem"><p class="simpara">
							Type a keyword into the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Filter by keyword</span></strong></span> box or scroll to find the Operator you want. For example, type <code class="literal cluster-admin">metallb</code> to find the MetalLB Operator.
						</p><p class="cluster-admin cluster-admin">
							You can also filter options by <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Infrastructure Features</span></strong></span>. For example, select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Disconnected</span></strong></span> if you want to see Operators that work in disconnected environments, also known as restricted network environments.
						</p></li><li class="listitem">
							On the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Install Operator</span></strong></span> page, accept the defaults and click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Install</span></strong></span>.
						</li></ol></div><div class="orderedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							To confirm that the installation is successful:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem">
									Navigate to the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Operators</span></strong></span> page.
								</li><li class="listitem">
									Check that the Operator is installed in the <code class="literal cluster-admin">openshift-operators</code> namespace and that its status is <code class="literal cluster-admin">Succeeded</code>.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							If the Operator is not installed successfully, check the status of the Operator and review the logs:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem">
									Navigate to the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Operators</span></strong></span> page and inspect the <code class="literal cluster-admin">Status</code> column for any errors or failures.
								</li><li class="listitem">
									Navigate to the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Workloads</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Pods</span></strong></span> page and check the logs in any pods in the <code class="literal cluster-admin">openshift-operators</code> project that are reporting issues.
								</li></ol></div></li></ol></div></section><section class="section cluster-admin" id="nw-metallb-installing-operator-cli_metallb-operator-install"><div class="titlepage"><div><div><h3 class="title">34.2.2. Installing from OperatorHub using the CLI</h3></div></div></div><p class="cluster-admin cluster-admin">
					Instead of using the OpenShift Container Platform web console, you can install an Operator from OperatorHub using the CLI. You can use the OpenShift CLI (<code class="literal cluster-admin">oc</code>) to install the MetalLB Operator.
				</p><p class="cluster-admin cluster-admin">
					It is recommended that when using the CLI you install the Operator in the <code class="literal cluster-admin">metallb-system</code> namespace.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							A cluster installed on bare-metal hardware.
						</li><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a namespace for the MetalLB Operator by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: v1
kind: Namespace
metadata:
  name: metallb-system
EOF</pre></li><li class="listitem"><p class="simpara">
							Create an Operator group custom resource (CR) in the namespace:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: metallb-operator
  namespace: metallb-system
EOF</pre></li><li class="listitem"><p class="simpara">
							Confirm the Operator group is installed in the namespace:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get operatorgroup -n metallb-system</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME               AGE
metallb-operator   14m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal cluster-admin">Subscription</code> CR:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Define the <code class="literal cluster-admin">Subscription</code> CR and save the YAML file, for example, <code class="literal cluster-admin">metallb-sub.yaml</code>:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: metallb-operator-sub
  namespace: metallb-system
spec:
  channel: stable
  name: metallb-operator
  source: redhat-operators <span id="CO226-1"><!--Empty--></span><span class="callout">1</span>
  sourceNamespace: openshift-marketplace</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO226-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											You must specify the <code class="literal cluster-admin">redhat-operators</code> value.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To create the <code class="literal cluster-admin">Subscription</code> CR, run the following command:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc create -f metallb-sub.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Optional: To ensure BGP and BFD metrics appear in Prometheus, you can label the namespace as in the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc label ns metallb-system "openshift.io/cluster-monitoring=true"</pre></li></ol></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Verification</strong></p><p>
						The verification steps assume the MetalLB Operator is installed in the <code class="literal cluster-admin">metallb-system</code> namespace.
					</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Confirm the install plan is in the namespace:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get installplan -n metallb-system</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME            CSV                                   APPROVAL    APPROVED
install-wzg94   metallb-operator.4.13.0-nnnnnnnnnnnn   Automatic   true</pre>

							</p></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								Installation of the Operator might take a few seconds.
							</p></div></div></li><li class="listitem"><p class="simpara">
							To verify that the Operator is installed, enter the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get clusterserviceversion -n metallb-system \
  -o custom-columns=Name:.metadata.name,Phase:.status.phase</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Name                                  Phase
metallb-operator.4.13.0-nnnnnnnnnnnn   Succeeded</pre>

							</p></div></li></ol></div></section><section class="section cluster-admin" id="nw-metallb-operator-initial-config_metallb-operator-install"><div class="titlepage"><div><div><h3 class="title">34.2.3. Starting MetalLB on your cluster</h3></div></div></div><p class="cluster-admin cluster-admin">
					After you install the Operator, you need to configure a single instance of a MetalLB custom resource. After you configure the custom resource, the Operator starts MetalLB on your cluster.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li><li class="listitem">
							Install the MetalLB Operator.
						</li></ul></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Procedure</strong></p><p>
						This procedure assumes the MetalLB Operator is installed in the <code class="literal cluster-admin">metallb-system</code> namespace. If you installed using the web console substitute <code class="literal cluster-admin">openshift-operators</code> for the namespace.
					</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a single instance of a MetalLB custom resource:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
EOF</pre></li></ol></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Verification</strong></p><p>
						Confirm that the deployment for the MetalLB controller and the daemon set for the MetalLB speaker are running.
					</p></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Verify that the deployment for the controller is running:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get deployment -n metallb-system controller</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME         READY   UP-TO-DATE   AVAILABLE   AGE
controller   1/1     1            1           11m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify that the daemon set for the speaker is running:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get daemonset -n metallb-system speaker</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
speaker   6         6         6       6            6           kubernetes.io/os=linux   18m</pre>

							</p></div><p class="cluster-admin cluster-admin">
							The example output indicates 6 speaker pods. The number of speaker pods in your cluster might differ from the example output. Make sure the output indicates one pod for each node in your cluster.
						</p></li></ol></div></section><section class="section cluster-admin" id="nw-metallb-operator-deployment-specifications-for-metallb_metallb-operator-install"><div class="titlepage"><div><div><h3 class="title">34.2.4. Deployment specifications for MetalLB</h3></div></div></div><p class="cluster-admin cluster-admin">
					When you start an instance of MetalLB using the <code class="literal cluster-admin">MetalLB</code> custom resource, you can configure deployment specifications in the <code class="literal cluster-admin">MetalLB</code> custom resource to manage how the <code class="literal cluster-admin">controller</code> or <code class="literal cluster-admin">speaker</code> pods deploy and run in your cluster. Use these deployment specifications to manage the following tasks:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Select nodes for MetalLB pod deployment.
						</li><li class="listitem">
							Manage scheduling by using pod priority and pod affinity.
						</li><li class="listitem">
							Assign CPU limits for MetalLB pods.
						</li><li class="listitem">
							Assign a container RuntimeClass for MetalLB pods.
						</li><li class="listitem">
							Assign metadata for MetalLB pods.
						</li></ul></div><section class="section cluster-admin" id="nw-metallb-operator-limit-speaker-to-nodes_metallb-operator-install"><div class="titlepage"><div><div><h4 class="title">34.2.4.1. Limit speaker pods to specific nodes</h4></div></div></div><p class="cluster-admin cluster-admin">
						By default, when you start MetalLB with the MetalLB Operator, the Operator starts an instance of a <code class="literal cluster-admin">speaker</code> pod on each node in the cluster. Only the nodes with a <code class="literal cluster-admin">speaker</code> pod can advertise a load balancer IP address. You can configure the <code class="literal cluster-admin">MetalLB</code> custom resource with a node selector to specify which nodes run the <code class="literal cluster-admin">speaker</code> pods.
					</p><p class="cluster-admin cluster-admin">
						The most common reason to limit the <code class="literal cluster-admin">speaker</code> pods to specific nodes is to ensure that only nodes with network interfaces on specific networks advertise load balancer IP addresses. Only the nodes with a running <code class="literal cluster-admin">speaker</code> pod are advertised as destinations of the load balancer IP address.
					</p><p class="cluster-admin cluster-admin">
						If you limit the <code class="literal cluster-admin">speaker</code> pods to specific nodes and specify <code class="literal cluster-admin">local</code> for the external traffic policy of a service, then you must ensure that the application pods for the service are deployed to the same nodes.
					</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example configuration to limit speaker pods to worker nodes</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
spec:
  nodeSelector:  &lt;.&gt;
    node-role.kubernetes.io/worker: ""
  speakerTolerations:   &lt;.&gt;
  - key: "Example"
    operator: "Exists"
    effect: "NoExecute"</pre>

						</p></div><p class="cluster-admin cluster-admin">
						&lt;.&gt; The example configuration specifies to assign the speaker pods to worker nodes, but you can specify labels that you assigned to nodes or any valid node selector. &lt;.&gt; In this example configuration, the pod that this toleration is attached to tolerates any taint that matches the <code class="literal cluster-admin">key</code> value and <code class="literal cluster-admin">effect</code> value using the <code class="literal cluster-admin">operator</code>.
					</p><p class="cluster-admin cluster-admin">
						After you apply a manifest with the <code class="literal cluster-admin">spec.nodeSelector</code> field, you can check the number of pods that the Operator deployed with the <code class="literal cluster-admin">oc get daemonset -n metallb-system speaker</code> command. Similarly, you can display the nodes that match your labels with a command like <code class="literal cluster-admin">oc get nodes -l node-role.kubernetes.io/worker=</code>.
					</p><p class="cluster-admin cluster-admin">
						You can optionally allow the node to control which speaker pods should, or should not, be scheduled on them by using affinity rules. You can also limit these pods by applying a list of tolerations. For more information about affinity rules, taints, and tolerations, see the additional resources.
					</p></section><section class="section cluster-admin" id="nw-metallb-operator-setting-runtimeclass_metallb-operator-install"><div class="titlepage"><div><div><h4 class="title">34.2.4.2. Configuring a container runtime class in a MetalLB deployment</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can optionally assign a container runtime class to <code class="literal cluster-admin">controller</code> and <code class="literal cluster-admin">speaker</code> pods by configuring the MetalLB custom resource. For example, for Windows workloads, you can assign a Windows runtime class to the pod, which uses this runtime class for all containers in the pod.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								You are logged in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have installed the MetalLB Operator.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create a <code class="literal cluster-admin">RuntimeClass</code> custom resource, such as <code class="literal cluster-admin">myRuntimeClass.yaml</code>, to define your runtime class:
							</p><pre class="programlisting language-yaml white-space-pre white-space-pre">apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: myclass
handler: myconfiguration</pre></li><li class="listitem"><p class="simpara">
								Apply the <code class="literal cluster-admin">RuntimeClass</code> custom resource configuration:
							</p><pre class="programlisting language-bash cluster-admin cluster-admin">$ oc apply -f myRuntimeClass.yaml</pre></li><li class="listitem"><p class="simpara">
								Create a <code class="literal cluster-admin">MetalLB</code> custom resource, such as <code class="literal cluster-admin">MetalLBRuntime.yaml</code>, to specify the <code class="literal cluster-admin">runtimeClassName</code> value:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
spec:
  logLevel: debug
  controllerConfig:
    runtimeClassName: myclass
    annotations: <span id="CO227-1"><!--Empty--></span><span class="callout">1</span>
      controller: demo
  speakerConfig:
    runtimeClassName: myclass
    annotations: <span id="CO227-2"><!--Empty--></span><span class="callout">2</span>
      speaker: demo</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO227-1"><span class="callout">1</span></a> <a href="#CO227-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										This example uses <code class="literal cluster-admin">annotations</code> to add metadata such as build release information or GitHub pull request information. You can populate annotations with characters that are not permitted in labels. However, you cannot use annotations to identify or select objects.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Apply the <code class="literal cluster-admin">MetalLB</code> custom resource configuration:
							</p><pre class="programlisting language-bash white-space-pre white-space-pre">$ oc apply -f MetalLBRuntime.yaml</pre></li></ol></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								To view the container runtime for a pod, run the following command:
							</p><pre class="programlisting language-bash white-space-pre white-space-pre">$ oc get pod -o custom-columns=NAME:metadata.name,STATUS:.status.phase,RUNTIME_CLASS:.spec.runtimeClassName</pre></li></ul></div></section><section class="section cluster-admin" id="nw-metallb-operator-setting-pod-priority-affinity_metallb-operator-install"><div class="titlepage"><div><div><h4 class="title">34.2.4.3. Configuring pod priority and pod affinity in a MetalLB deployment</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can optionally assign pod priority and pod affinity rules to <code class="literal cluster-admin">controller</code> and <code class="literal cluster-admin">speaker</code> pods by configuring the <code class="literal cluster-admin">MetalLB</code> custom resource. The pod priority indicates the relative importance of a pod on a node and schedules the pod based on this priority. Set a high priority on your <code class="literal cluster-admin">controller</code> or <code class="literal cluster-admin">speaker</code> pod to ensure scheduling priority over other pods on the node.
					</p><p class="cluster-admin cluster-admin">
						Pod affinity manages relationships among pods. Assign pod affinity to the <code class="literal cluster-admin">controller</code> or <code class="literal cluster-admin">speaker</code> pods to control on what node the scheduler places the pod in the context of pod relationships. For example, you can use pod affinity rules to ensure that certain pods are located on the same node or nodes, which can help improve network communication and reduce latency between those components.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								You are logged in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have installed the MetalLB Operator.
							</li><li class="listitem">
								You have started the MetalLB Operator on your cluster.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create a <code class="literal cluster-admin">PriorityClass</code> custom resource, such as <code class="literal cluster-admin">myPriorityClass.yaml</code>, to configure the priority level. This example defines a <code class="literal cluster-admin">PriorityClass</code> named <code class="literal cluster-admin">high-priority</code> with a value of <code class="literal cluster-admin">1000000</code>. Pods that are assigned this priority class are considered higher priority during scheduling compared to pods with lower priority classes:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000</pre></li><li class="listitem"><p class="simpara">
								Apply the <code class="literal cluster-admin">PriorityClass</code> custom resource configuration:
							</p><pre class="programlisting language-bash cluster-admin cluster-admin">$ oc apply -f myPriorityClass.yaml</pre></li><li class="listitem"><p class="simpara">
								Create a <code class="literal cluster-admin">MetalLB</code> custom resource, such as <code class="literal cluster-admin">MetalLBPodConfig.yaml</code>, to specify the <code class="literal cluster-admin">priorityClassName</code> and <code class="literal cluster-admin">podAffinity</code> values:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
spec:
  logLevel: debug
  controllerConfig:
    priorityClassName: high-priority <span id="CO228-1"><!--Empty--></span><span class="callout">1</span>
    affinity:
      podAffinity: <span id="CO228-2"><!--Empty--></span><span class="callout">2</span>
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
             app: metallb
          topologyKey: kubernetes.io/hostname
  speakerConfig:
    priorityClassName: high-priority
    affinity:
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
             app: metallb
          topologyKey: kubernetes.io/hostname</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO228-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specifies the priority class for the MetalLB controller pods. In this case, it is set to <code class="literal cluster-admin">high-priority</code>.
									</div></dd><dt><a href="#CO228-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specifies that you are configuring pod affinity rules. These rules dictate how pods are scheduled in relation to other pods or nodes. This configuration instructs the scheduler to schedule pods that have the label <code class="literal cluster-admin">app: metallb</code> onto nodes that share the same hostname. This helps to co-locate MetalLB-related pods on the same nodes, potentially optimizing network communication, latency, and resource usage between these pods.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Apply the <code class="literal cluster-admin">MetalLB</code> custom resource configuration:
							</p><pre class="programlisting language-bash cluster-admin cluster-admin">$ oc apply -f MetalLBPodConfig.yaml</pre></li></ol></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								To view the priority class that you assigned to pods in the <code class="literal cluster-admin">metallb-system</code> namespace, run the following command:
							</p><pre class="programlisting language-bash cluster-admin cluster-admin">$ oc get pods -n metallb-system -o custom-columns=NAME:.metadata.name,PRIORITY:.spec.priorityClassName</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                                                 PRIORITY
controller-584f5c8cd8-5zbvg                          high-priority
metallb-operator-controller-manager-9c8d9985-szkqg   &lt;none&gt;
metallb-operator-webhook-server-c895594d4-shjgx      &lt;none&gt;
speaker-dddf7                                        high-priority</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								To verify that the scheduler placed pods according to pod affinity rules, view the metadata for the pod’s node or nodes by running the following command:
							</p><pre class="programlisting language-bash cluster-admin cluster-admin">$ oc get pod -o=custom-columns=NODE:.spec.nodeName,NAME:.metadata.name -n metallb-system</pre></li></ul></div></section><section class="section cluster-admin" id="nw-metallb-operator-setting-pod-CPU-limits_metallb-operator-install"><div class="titlepage"><div><div><h4 class="title">34.2.4.4. Configuring pod CPU limits in a MetalLB deployment</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can optionally assign pod CPU limits to <code class="literal cluster-admin">controller</code> and <code class="literal cluster-admin">speaker</code> pods by configuring the <code class="literal cluster-admin">MetalLB</code> custom resource. Defining CPU limits for the <code class="literal cluster-admin">controller</code> or <code class="literal cluster-admin">speaker</code> pods helps you to manage compute resources on the node. This ensures all pods on the node have the necessary compute resources to manage workloads and cluster housekeeping.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								You are logged in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li><li class="listitem">
								You have installed the MetalLB Operator.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create a <code class="literal cluster-admin">MetalLB</code> custom resource file, such as <code class="literal cluster-admin">CPULimits.yaml</code>, to specify the <code class="literal cluster-admin">cpu</code> value for the <code class="literal cluster-admin">controller</code> and <code class="literal cluster-admin">speaker</code> pods:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
spec:
  logLevel: debug
  controllerConfig:
    resources:
      limits:
        cpu: "200m"
  speakerConfig:
    resources:
      limits:
        cpu: "300m"</pre></li><li class="listitem"><p class="simpara">
								Apply the <code class="literal cluster-admin">MetalLB</code> custom resource configuration:
							</p><pre class="programlisting language-bash cluster-admin cluster-admin">$ oc apply -f CPULimits.yaml</pre></li></ol></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								To view compute resources for a pod, run the following command, replacing <code class="literal cluster-admin">&lt;pod_name&gt;</code> with your target pod:
							</p><pre class="programlisting language-bash cluster-admin cluster-admin">$ oc describe pod &lt;pod_name&gt;</pre></li></ul></div></section></section><section class="section _additional-resources" id="additional-resources_metallb-operator-install"><div class="titlepage"><div><div><h3 class="title">34.2.5. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-node-selectors">Placing pods on specific nodes using node selectors</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-taints-tolerations-about">Understanding taints and tolerations</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-pods-priority-about_nodes-pods-priority">Understanding pod priority</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/nodes/#nodes-scheduler-pod-affinity-about_nodes-scheduler-pod-affinity">Understanding pod affinity</a>
						</li></ul></div></section><section class="section cluster-admin" id="next-steps_metallb-operator-install"><div class="titlepage"><div><div><h3 class="title">34.2.6. Next steps</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#metallb-configure-address-pools">Configuring MetalLB address pools</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="metallb-upgrading-operator"><div class="titlepage"><div><div><h2 class="title">34.3. Upgrading the MetalLB</h2></div></div></div><p class="cluster-admin cluster-admin">
				If you are currently running version 4.10 or an earlier version of the MetalLB Operator, please note that automatic updates to any version later than 4.10 do not work. Upgrading to a newer version from any version of the MetalLB Operator that is 4.11 or later is successful. For example, upgrading from version 4.12 to version 4.13 will occur smoothly.
			</p><p class="cluster-admin cluster-admin">
				A summary of the upgrade procedure for the MetalLB Operator from 4.10 and earlier is as follows:
			</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
						Delete the installed MetalLB Operator version for example 4.10. Ensure that the namespace and the <code class="literal cluster-admin">metallb</code> custom resource are not removed.
					</li><li class="listitem">
						Using the CLI, install the MetalLB Operator 4.13 in the same namespace where the previous version of the MetalLB Operator was installed.
					</li></ol></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
					This procedure does not apply to automatic z-stream updates of the MetalLB Operator, which follow the standard straightforward method.
				</p></div></div><p class="cluster-admin cluster-admin">
				For detailed steps to upgrade the MetalLB Operator from 4.10 and earlier, see the guidance that follows.
			</p><section class="section cluster-admin" id="olm-deleting-metallb-operator-from-a-cluster-using-web-console_metallb-upgrading-operator"><div class="titlepage"><div><div><h3 class="title">34.3.1. Deleting the MetalLB Operator from a cluster using the web console</h3></div></div></div><p class="cluster-admin cluster-admin">
					Cluster administrators can delete installed Operators from a selected namespace by using the web console.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Access to an OpenShift Container Platform cluster web console using an account with <code class="literal cluster-admin">cluster-admin</code> permissions.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Navigate to the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Operators</span></strong></span> page.
						</li><li class="listitem">
							Search for the MetalLB Operator. Then, click on it.
						</li><li class="listitem"><p class="simpara">
							On the right side of the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operator Details</span></strong></span> page, select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Uninstall Operator</span></strong></span> from the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Actions</span></strong></span> drop-down menu.
						</p><p class="cluster-admin cluster-admin">
							An <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Uninstall Operator?</span></strong></span> dialog box is displayed.
						</p></li><li class="listitem"><p class="simpara">
							Select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Uninstall</span></strong></span> to remove the Operator, Operator deployments, and pods. Following this action, the Operator stops running and no longer receives updates.
						</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								This action does not remove resources managed by the Operator, including custom resource definitions (CRDs) and custom resources (CRs). Dashboards and navigation items enabled by the web console and off-cluster resources that continue to run might need manual clean up. To remove these after uninstalling the Operator, you might need to manually delete the Operator CRDs.
							</p></div></div></li></ol></div></section><section class="section cluster-admin" id="olm-deleting-metallb-operator-from-a-cluster-using-cli_metallb-upgrading-operator"><div class="titlepage"><div><div><h3 class="title">34.3.2. Deleting MetalLB Operator from a cluster using the CLI</h3></div></div></div><p class="cluster-admin cluster-admin">
					Cluster administrators can delete installed Operators from a selected namespace by using the CLI.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Access to an OpenShift Container Platform cluster using an account with <code class="literal cluster-admin">cluster-admin</code> permissions.
						</li><li class="listitem">
							<code class="literal cluster-admin">oc</code> command installed on workstation.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Check the current version of the subscribed MetalLB Operator in the <code class="literal cluster-admin">currentCSV</code> field:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get subscription metallb-operator -n metallb-system -o yaml | grep currentCSV</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">  currentCSV: metallb-operator.4.10.0-202207051316</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Delete the subscription:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete subscription metallb-operator -n metallb-system</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">subscription.operators.coreos.com "metallb-operator" deleted</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Delete the CSV for the Operator in the target namespace using the <code class="literal cluster-admin">currentCSV</code> value from the previous step:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete clusterserviceversion metallb-operator.4.10.0-202207051316 -n metallb-system</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">clusterserviceversion.operators.coreos.com "metallb-operator.4.10.0-202207051316" deleted</pre>

							</p></div></li></ol></div></section><section class="section cluster-admin" id="upgrading-metallb-operator_metallb-upgrading-operator"><div class="titlepage"><div><div><h3 class="title">34.3.3. Upgrading the MetalLB Operator</h3></div></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Access the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Verify that the <code class="literal cluster-admin">metallb-system</code> namespace still exists:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get namespaces | grep metallb-system</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">metallb-system                                     Active   31m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify the <code class="literal cluster-admin">metallb</code> custom resource still exists:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get metallb -n metallb-system</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME      AGE
metallb   33m</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Follow the guidance in "Installing from OperatorHub using the CLI" to install the latest 4.13 version of the MetalLB Operator.
						</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								When installing the latest 4.13 version of the MetalLB Operator, you must install the Operator to the same namespace it was previously installed to.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Verify the upgraded version of the Operator is now the 4.13 version.
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get csv -n metallb-system</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                                   DISPLAY            VERSION               REPLACES   PHASE
metallb-operator.4.4.13.0-202207051316   MetalLB Operator   4.4.13.0-202207051316              Succeeded</pre>

							</p></div></li></ol></div></section><section class="section cluster-admin" id="additional-resources"><div class="titlepage"><div><div><h3 class="title">34.3.4. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/operators/#olm-deleting-operators-from-a-cluster">Deleting Operators from a cluster</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#metallb-operator-install">Installing the MetalLB Operator</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="metallb-configure-address-pools"><div class="titlepage"><div><div><h2 class="title">34.4. Configuring MetalLB address pools</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can add, modify, and delete address pools. The MetalLB Operator uses the address pool custom resources to set the IP addresses that MetalLB can assign to services. The namespace used in the examples assume the namespace is <code class="literal cluster-admin">metallb-system</code>.
			</p><section class="section cluster-admin" id="nw-metallb-ipaddresspool-cr_configure-metallb-address-pools"><div class="titlepage"><div><div><h3 class="title">34.4.1. About the IPAddressPool custom resource</h3></div></div></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						The address pool custom resource definition (CRD) and API documented in "Load balancing with MetalLB" in OpenShift Container Platform 4.10 can still be used in 4.13. However, the enhanced functionality associated with advertising an IP address from an <code class="literal cluster-admin">IPAddressPool</code> with layer 2 protocols, or the BGP protocol, is not supported when using the <code class="literal cluster-admin">AddressPool</code> CRD.
					</p></div></div><p class="cluster-admin cluster-admin">
					The fields for the <code class="literal cluster-admin">IPAddressPool</code> custom resource are described in the following tables.
				</p><div class="table" id="idm140587124881600"><p class="title"><strong>Table 34.1. MetalLB IPAddressPool pool custom resource</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587124875504" scope="col">Field</th><th align="left" valign="top" id="idm140587124874416" scope="col">Type</th><th align="left" valign="top" id="idm140587124873328" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587124875504"> <p>
									<code class="literal cluster-admin">metadata.name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124874416"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124873328"> <p class="cluster-admin cluster-admin">
									Specifies the name for the address pool. When you add a service, you can specify this pool name in the <code class="literal cluster-admin">metallb.universe.tf/address-pool</code> annotation to select an IP address from a specific pool. The names <code class="literal cluster-admin">doc-example</code>, <code class="literal cluster-admin">silver</code>, and <code class="literal cluster-admin">gold</code> are used throughout the documentation.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124875504"> <p>
									<code class="literal cluster-admin">metadata.namespace</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124874416"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124873328"> <p class="cluster-admin cluster-admin">
									Specifies the namespace for the address pool. Specify the same namespace that the MetalLB Operator uses.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124875504"> <p>
									<code class="literal cluster-admin">metadata.label</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124874416"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124873328"> <p class="cluster-admin cluster-admin">
									Optional: Specifies the key value pair assigned to the <code class="literal cluster-admin">IPAddressPool</code>. This can be referenced by the <code class="literal cluster-admin">ipAddressPoolSelectors</code> in the <code class="literal cluster-admin">BGPAdvertisement</code> and <code class="literal cluster-admin">L2Advertisement</code> CRD to associate the <code class="literal cluster-admin">IPAddressPool</code> with the advertisement
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124875504"> <p>
									<code class="literal cluster-admin">spec.addresses</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124874416"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124873328"> <p class="cluster-admin cluster-admin">
									Specifies a list of IP addresses for MetalLB Operator to assign to services. You can specify multiple ranges in a single pool; they will all share the same settings. Specify each range in CIDR notation or as starting and ending IP addresses separated with a hyphen.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124875504"> <p>
									<code class="literal cluster-admin">spec.autoAssign</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124874416"> <p>
									<code class="literal cluster-admin">boolean</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124873328"> <p class="cluster-admin cluster-admin">
									Optional: Specifies whether MetalLB automatically assigns IP addresses from this pool. Specify <code class="literal cluster-admin">false</code> if you want explicitly request an IP address from this pool with the <code class="literal cluster-admin">metallb.universe.tf/address-pool</code> annotation. The default value is <code class="literal cluster-admin">true</code>.
								</p>
								 </td></tr></tbody></table></div></div><p class="cluster-admin cluster-admin">
					You can assign IP addresses from an <code class="literal cluster-admin">IPAddressPool</code> to services and namespaces by configuring the <code class="literal cluster-admin">spec.serviceAllocation</code> specification.
				</p><div class="table" id="idm140587124823184"><p class="title"><strong>Table 34.2. MetalLB IPAddressPool custom resource spec.serviceAllocation subfields</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587124817072" scope="col">Field</th><th align="left" valign="top" id="idm140587124815984" scope="col">Type</th><th align="left" valign="top" id="idm140587124814896" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587124817072"> <p>
									<code class="literal cluster-admin">priority</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124815984"> <p>
									<code class="literal cluster-admin">int</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124814896"> <p class="cluster-admin cluster-admin">
									Optional: Defines the priority between IP address pools when more than one IP address pool matches a service or namespace. A lower number indicates a higher priority.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124817072"> <p>
									<code class="literal cluster-admin">namespaces</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124815984"> <p>
									<code class="literal cluster-admin">array (string)</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124814896"> <p class="cluster-admin cluster-admin">
									Optional: Specifies a list of namespaces that you can assign to IP addresses in an IP address pool.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124817072"> <p>
									<code class="literal cluster-admin">namespaceSelectors</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124815984"> <p>
									<code class="literal cluster-admin">array (LabelSelector)</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124814896"> <p class="cluster-admin cluster-admin">
									Optional: Specifies namespace labels that you can assign to IP addresses from an IP address pool by using label selectors in a list format.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124817072"> <p>
									<code class="literal cluster-admin">serviceSelectors</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124815984"> <p>
									<code class="literal cluster-admin">array (LabelSelector)</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124814896"> <p class="cluster-admin cluster-admin">
									Optional: Specifies service labels that you can assign to IP addresses from an address pool by using label selectors in a list format.
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section cluster-admin" id="nw-metallb-configure-address-pool_configure-metallb-address-pools"><div class="titlepage"><div><div><h3 class="title">34.4.2. Configuring an address pool</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can add address pools to your cluster to control the IP addresses that MetalLB can assign to load-balancer services.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a file, such as <code class="literal cluster-admin">ipaddresspool.yaml</code>, with content like the following example:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: doc-example
  labels: <span id="CO229-1"><!--Empty--></span><span class="callout">1</span>
    zone: east
spec:
  addresses:
  - 203.0.113.1-203.0.113.10
  - 203.0.113.65-203.0.113.75</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO229-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									This label assigned to the <code class="literal cluster-admin">IPAddressPool</code> can be referenced by the <code class="literal cluster-admin">ipAddressPoolSelectors</code> in the <code class="literal cluster-admin">BGPAdvertisement</code> CRD to associate the <code class="literal cluster-admin">IPAddressPool</code> with the advertisement.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Apply the configuration for the IP address pool:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f ipaddresspool.yaml</pre></li></ol></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							View the address pool:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc describe -n metallb-system IPAddressPool doc-example</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">Name:         doc-example
Namespace:    metallb-system
Labels:       zone=east
Annotations:  &lt;none&gt;
API Version:  metallb.io/v1beta1
Kind:         IPAddressPool
Metadata:
  ...
Spec:
  Addresses:
    203.0.113.1-203.0.113.10
    203.0.113.65-203.0.113.75
  Auto Assign:  true
Events:         &lt;none&gt;</pre>

							</p></div></li></ul></div><p class="cluster-admin cluster-admin">
					Confirm that the address pool name, such as <code class="literal cluster-admin">doc-example</code>, and the IP address ranges appear in the output.
				</p></section><section class="section cluster-admin" id="nw-metallb-example-addresspool_configure-metallb-address-pools"><div class="titlepage"><div><div><h3 class="title">34.4.3. Example address pool configurations</h3></div></div></div><section class="section cluster-admin" id="example-ipv4-and-cidr-ranges"><div class="titlepage"><div><div><h4 class="title">34.4.3.1. Example: IPv4 and CIDR ranges</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can specify a range of IP addresses in CIDR notation. You can combine CIDR notation with the notation that uses a hyphen to separate lower and upper bounds.
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: doc-example-cidr
  namespace: metallb-system
spec:
  addresses:
  - 192.168.100.0/24
  - 192.168.200.0/24
  - 192.168.255.1-192.168.255.5</pre></section><section class="section cluster-admin" id="example-reserve-ip-addresses"><div class="titlepage"><div><div><h4 class="title">34.4.3.2. Example: Reserve IP addresses</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can set the <code class="literal cluster-admin">autoAssign</code> field to <code class="literal cluster-admin">false</code> to prevent MetalLB from automatically assigning the IP addresses from the pool. When you add a service, you can request a specific IP address from the pool or you can specify the pool name in an annotation to request any IP address from the pool.
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: doc-example-reserved
  namespace: metallb-system
spec:
  addresses:
  - 10.0.100.0/28
  autoAssign: false</pre></section><section class="section cluster-admin" id="example-ipv4-and-ipv6-addresses"><div class="titlepage"><div><div><h4 class="title">34.4.3.3. Example: IPv4 and IPv6 addresses</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can add address pools that use IPv4 and IPv6. You can specify multiple ranges in the <code class="literal cluster-admin">addresses</code> list, just like several IPv4 examples.
					</p><p class="cluster-admin cluster-admin">
						Whether the service is assigned a single IPv4 address, a single IPv6 address, or both is determined by how you add the service. The <code class="literal cluster-admin">spec.ipFamilies</code> and <code class="literal cluster-admin">spec.ipFamilyPolicy</code> fields control how IP addresses are assigned to the service.
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: doc-example-combined
  namespace: metallb-system
spec:
  addresses:
  - 10.0.100.0/28
  - 2002:2:2::1-2002:2:2::100</pre></section><section class="section cluster-admin" id="example-assign-ip-address-pools-to-services-or-namespaces"><div class="titlepage"><div><div><h4 class="title">34.4.3.4. Example: Assign IP address pools to services or namespaces</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can assign IP addresses from an <code class="literal cluster-admin">IPAddressPool</code> to services and namespaces that you specify.
					</p><p class="cluster-admin cluster-admin">
						If you assign a service or namespace to more than one IP address pool, MetalLB uses an available IP address from the higher-priority IP address pool. If no IP addresses are available from the assigned IP address pools with a high priority, MetalLB uses available IP addresses from an IP address pool with lower priority or no priority.
					</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							You can use the <code class="literal cluster-admin">matchLabels</code> label selector, the <code class="literal cluster-admin">matchExpressions</code> label selector, or both, for the <code class="literal cluster-admin">namespaceSelectors</code> and <code class="literal cluster-admin">serviceSelectors</code> specifications. This example demonstrates one label selector for each specification.
						</p></div></div><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: doc-example-service-allocation
  namespace: metallb-system
spec:
  addresses:
    - 192.168.20.0/24
  serviceAllocation:
    priority: 50 <span id="CO230-1"><!--Empty--></span><span class="callout">1</span>
    namespaces: <span id="CO230-2"><!--Empty--></span><span class="callout">2</span>
      - namespace-a
      - namespace-b
    namespaceSelectors: <span id="CO230-3"><!--Empty--></span><span class="callout">3</span>
      - matchLabels:
          zone: east
    serviceSelectors: <span id="CO230-4"><!--Empty--></span><span class="callout">4</span>
      - matchExpressions:
        - key: security
          operator: In
          values:
          - S1</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO230-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Assign a priority to the address pool. A lower number indicates a higher priority.
							</div></dd><dt><a href="#CO230-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Assign one or more namespaces to the IP address pool in a list format.
							</div></dd><dt><a href="#CO230-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Assign one or more namespace labels to the IP address pool by using label selectors in a list format.
							</div></dd><dt><a href="#CO230-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Assign one or more service labels to the IP address pool by using label selectors in a list format.
							</div></dd></dl></div></section></section><section class="section _additional-resources" id="additional-resources_metallb-configure-address-pools"><div class="titlepage"><div><div><h3 class="title">34.4.4. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-metallb-configure-with-L2-advertisement-label_about-advertising-ip-address-pool">Configuring MetalLB with an L2 advertisement and label</a>.
						</li></ul></div></section><section class="section cluster-admin" id="next-steps_configure-metallb-address-pools"><div class="titlepage"><div><div><h3 class="title">34.4.5. Next steps</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							For BGP mode, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#metallb-configure-bgp-peers">Configuring MetalLB BGP peers</a>.
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#metallb-configure-services">Configuring services to use MetalLB</a>.
						</li></ul></div></section></section><section class="section cluster-admin" id="about-advertise-for-ipaddress-pools"><div class="titlepage"><div><div><h2 class="title">34.5. About advertising for the IP address pools</h2></div></div></div><p class="cluster-admin cluster-admin">
				You can configure MetalLB so that the IP address is advertised with layer 2 protocols, the BGP protocol, or both. With layer 2, MetalLB provides a fault-tolerant external IP address. With BGP, MetalLB provides fault-tolerance for the external IP address and load balancing.
			</p><p class="cluster-admin cluster-admin">
				MetalLB supports advertising using L2 and BGP for the same set of IP addresses.
			</p><p class="cluster-admin cluster-admin">
				MetalLB provides the flexibility to assign address pools to specific BGP peers effectively to a subset of nodes on the network. This allows for more complex configurations, for example facilitating the isolation of nodes or the segmentation of the network.
			</p><section class="section cluster-admin" id="nw-metallb-bgpadvertisement-cr_about-advertising-ip-address-pool"><div class="titlepage"><div><div><h3 class="title">34.5.1. About the BGPAdvertisement custom resource</h3></div></div></div><p class="cluster-admin cluster-admin">
					The fields for the <code class="literal cluster-admin">BGPAdvertisements</code> object are defined in the following table:
				</p><div class="table" id="idm140587124693248"><p class="title"><strong>Table 34.3. BGPAdvertisements configuration</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587124687168" scope="col">Field</th><th align="left" valign="top" id="idm140587124686080" scope="col">Type</th><th align="left" valign="top" id="idm140587124684992" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587124687168"> <p>
									<code class="literal cluster-admin">metadata.name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124686080"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124684992"> <p class="cluster-admin cluster-admin">
									Specifies the name for the BGP advertisement.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124687168"> <p>
									<code class="literal cluster-admin">metadata.namespace</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124686080"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124684992"> <p class="cluster-admin cluster-admin">
									Specifies the namespace for the BGP advertisement. Specify the same namespace that the MetalLB Operator uses.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124687168"> <p>
									<code class="literal cluster-admin">spec.aggregationLength</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124686080"> <p>
									<code class="literal cluster-admin">integer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124684992"> <p class="cluster-admin cluster-admin">
									Optional: Specifies the number of bits to include in a 32-bit CIDR mask. To aggregate the routes that the speaker advertises to BGP peers, the mask is applied to the routes for several service IP addresses and the speaker advertises the aggregated route. For example, with an aggregation length of <code class="literal cluster-admin">24</code>, the speaker can aggregate several <code class="literal cluster-admin">10.0.1.x/32</code> service IP addresses and advertise a single <code class="literal cluster-admin">10.0.1.0/24</code> route.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124687168"> <p>
									<code class="literal cluster-admin">spec.aggregationLengthV6</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124686080"> <p>
									<code class="literal cluster-admin">integer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124684992"> <p class="cluster-admin cluster-admin">
									Optional: Specifies the number of bits to include in a 128-bit CIDR mask. For example, with an aggregation length of <code class="literal cluster-admin">124</code>, the speaker can aggregate several <code class="literal cluster-admin">fc00:f853:0ccd:e799::x/128</code> service IP addresses and advertise a single <code class="literal cluster-admin">fc00:f853:0ccd:e799::0/124</code> route.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124687168"> <p>
									<code class="literal cluster-admin">spec.communities</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124686080"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124684992"> <p class="cluster-admin cluster-admin">
									Optional: Specifies one or more BGP communities. Each community is specified as two 16-bit values separated by the colon character. Well-known communities must be specified as 16-bit values:
								</p>
								 <div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">NO_EXPORT</code>: <code class="literal cluster-admin">65535:65281</code>
										</li><li class="listitem">
											<code class="literal cluster-admin">NO_ADVERTISE</code>: <code class="literal cluster-admin">65535:65282</code>
										</li><li class="listitem"><p class="simpara">
											<code class="literal cluster-admin">NO_EXPORT_SUBCONFED</code>: <code class="literal cluster-admin">65535:65283</code>
										</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
												You can also use community objects that are created along with the strings.
											</p></div></div></li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124687168"> <p>
									<code class="literal cluster-admin">spec.localPref</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124686080"> <p>
									<code class="literal cluster-admin">integer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124684992"> <p class="cluster-admin cluster-admin">
									Optional: Specifies the local preference for this advertisement. This BGP attribute applies to BGP sessions within the Autonomous System.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124687168"> <p>
									<code class="literal cluster-admin">spec.ipAddressPools</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124686080"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124684992"> <p class="cluster-admin cluster-admin">
									Optional: The list of <code class="literal cluster-admin">IPAddressPools</code> to advertise with this advertisement, selected by name.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124687168"> <p>
									<code class="literal cluster-admin">spec.ipAddressPoolSelectors</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124686080"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124684992"> <p class="cluster-admin cluster-admin">
									Optional: A selector for the <code class="literal cluster-admin">IPAddressPools</code> that gets advertised with this advertisement. This is for associating the <code class="literal cluster-admin">IPAddressPool</code> to the advertisement based on the label assigned to the <code class="literal cluster-admin">IPAddressPool</code> instead of the name itself. If no <code class="literal cluster-admin">IPAddressPool</code> is selected by this or by the list, the advertisement is applied to all the <code class="literal cluster-admin">IPAddressPools</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124687168"> <p>
									<code class="literal cluster-admin">spec.nodeSelectors</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124686080"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124684992"> <p class="cluster-admin cluster-admin">
									Optional: <code class="literal cluster-admin">NodeSelectors</code> allows to limit the nodes to announce as next hops for the load balancer IP. When empty, all the nodes are announced as next hops.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124687168"> <p>
									<code class="literal cluster-admin">spec.peers</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124686080"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124684992"> <p class="cluster-admin cluster-admin">
									Optional: Peers limits the BGP peer to advertise the IPs of the selected pools to. When empty, the load balancer IP is announced to all the BGP peers configured.
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section cluster-admin" id="nw-metallb-configure-BGP-advertisement-basic-use-case_about-advertising-ip-address-pool"><div class="titlepage"><div><div><h3 class="title">34.5.2. Configuring MetalLB with a BGP advertisement and a basic use case</h3></div></div></div><p class="cluster-admin cluster-admin">
					Configure MetalLB as follows so that the peer BGP routers receive one <code class="literal cluster-admin">203.0.113.200/32</code> route and one <code class="literal cluster-admin">fc00:f853:ccd:e799::1/128</code> route for each load-balancer IP address that MetalLB assigns to a service. Because the <code class="literal cluster-admin">localPref</code> and <code class="literal cluster-admin">communities</code> fields are not specified, the routes are advertised with <code class="literal cluster-admin">localPref</code> set to zero and no BGP communities.
				</p><section class="section cluster-admin" id="nw-metallb-advertise-a-basic-address-pool-configuration-bgp_about-advertising-ip-address-pool"><div class="titlepage"><div><div><h4 class="title">34.5.2.1. Example: Advertise a basic address pool configuration with BGP</h4></div></div></div><p class="cluster-admin cluster-admin">
						Configure MetalLB as follows so that the <code class="literal cluster-admin">IPAddressPool</code> is advertised with the BGP protocol.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create an IP address pool.
							</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
										Create a file, such as <code class="literal cluster-admin">ipaddresspool.yaml</code>, with content like the following example:
									</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: doc-example-bgp-basic
spec:
  addresses:
    - 203.0.113.200/30
    - fc00:f853:ccd:e799::/124</pre></li><li class="listitem"><p class="simpara">
										Apply the configuration for the IP address pool:
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f ipaddresspool.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Create a BGP advertisement.
							</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
										Create a file, such as <code class="literal cluster-admin">bgpadvertisement.yaml</code>, with content like the following example:
									</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: bgpadvertisement-basic
  namespace: metallb-system
spec:
  ipAddressPools:
  - doc-example-bgp-basic</pre></li><li class="listitem"><p class="simpara">
										Apply the configuration:
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f bgpadvertisement.yaml</pre></li></ol></div></li></ol></div></section></section><section class="section cluster-admin" id="nw-metallb-configure-BGP-advertisement-advanced-use-case_about-advertising-ip-address-pool"><div class="titlepage"><div><div><h3 class="title">34.5.3. Configuring MetalLB with a BGP advertisement and an advanced use case</h3></div></div></div><p class="cluster-admin cluster-admin">
					Configure MetalLB as follows so that MetalLB assigns IP addresses to load-balancer services in the ranges between <code class="literal cluster-admin">203.0.113.200</code> and <code class="literal cluster-admin">203.0.113.203</code> and between <code class="literal cluster-admin">fc00:f853:ccd:e799::0</code> and <code class="literal cluster-admin">fc00:f853:ccd:e799::f</code>.
				</p><p class="cluster-admin cluster-admin">
					To explain the two BGP advertisements, consider an instance when MetalLB assigns the IP address of <code class="literal cluster-admin">203.0.113.200</code> to a service. With that IP address as an example, the speaker advertises two routes to BGP peers:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<code class="literal cluster-admin">203.0.113.200/32</code>, with <code class="literal cluster-admin">localPref</code> set to <code class="literal cluster-admin">100</code> and the community set to the numeric value of the <code class="literal cluster-admin">NO_ADVERTISE</code> community. This specification indicates to the peer routers that they can use this route but they should not propagate information about this route to BGP peers.
						</li><li class="listitem">
							<code class="literal cluster-admin">203.0.113.200/30</code>, aggregates the load-balancer IP addresses assigned by MetalLB into a single route. MetalLB advertises the aggregated route to BGP peers with the community attribute set to <code class="literal cluster-admin">8000:800</code>. BGP peers propagate the <code class="literal cluster-admin">203.0.113.200/30</code> route to other BGP peers. When traffic is routed to a node with a speaker, the <code class="literal cluster-admin">203.0.113.200/32</code> route is used to forward the traffic into the cluster and to a pod that is associated with the service.
						</li></ul></div><p class="cluster-admin cluster-admin">
					As you add more services and MetalLB assigns more load-balancer IP addresses from the pool, peer routers receive one local route, <code class="literal cluster-admin">203.0.113.20x/32</code>, for each service, as well as the <code class="literal cluster-admin">203.0.113.200/30</code> aggregate route. Each service that you add generates the <code class="literal cluster-admin">/30</code> route, but MetalLB deduplicates the routes to one BGP advertisement before communicating with peer routers.
				</p><section class="section cluster-admin" id="nw-metallb-advertise-an-advanced-address-pool-configuration-bgp_about-advertising-ip-address-pool"><div class="titlepage"><div><div><h4 class="title">34.5.3.1. Example: Advertise an advanced address pool configuration with BGP</h4></div></div></div><p class="cluster-admin cluster-admin">
						Configure MetalLB as follows so that the <code class="literal cluster-admin">IPAddressPool</code> is advertised with the BGP protocol.
					</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
							</li><li class="listitem">
								Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
								Create an IP address pool.
							</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
										Create a file, such as <code class="literal cluster-admin">ipaddresspool.yaml</code>, with content like the following example:
									</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: doc-example-bgp-adv
  labels:
    zone: east
spec:
  addresses:
    - 203.0.113.200/30
    - fc00:f853:ccd:e799::/124
  autoAssign: false</pre></li><li class="listitem"><p class="simpara">
										Apply the configuration for the IP address pool:
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f ipaddresspool.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
								Create a BGP advertisement.
							</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
										Create a file, such as <code class="literal cluster-admin">bgpadvertisement1.yaml</code>, with content like the following example:
									</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: bgpadvertisement-adv-1
  namespace: metallb-system
spec:
  ipAddressPools:
    - doc-example-bgp-adv
  communities:
    - 65535:65282
  aggregationLength: 32
  localPref: 100</pre></li><li class="listitem"><p class="simpara">
										Apply the configuration:
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f bgpadvertisement1.yaml</pre></li><li class="listitem"><p class="simpara">
										Create a file, such as <code class="literal cluster-admin">bgpadvertisement2.yaml</code>, with content like the following example:
									</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: bgpadvertisement-adv-2
  namespace: metallb-system
spec:
  ipAddressPools:
    - doc-example-bgp-adv
  communities:
    - 8000:800
  aggregationLength: 30
  aggregationLengthV6: 124</pre></li><li class="listitem"><p class="simpara">
										Apply the configuration:
									</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f bgpadvertisement2.yaml</pre></li></ol></div></li></ol></div></section></section><section class="section cluster-admin" id="nw-metallb-advertise-ip-pools-to-node-subset_about-advertising-ip-address-pool"><div class="titlepage"><div><div><h3 class="title">34.5.4. Advertising an IP address pool from a subset of nodes</h3></div></div></div><p class="cluster-admin cluster-admin">
					To advertise an IP address from an IP addresses pool, from a specific set of nodes only, use the <code class="literal cluster-admin">.spec.nodeSelector</code> specification in the BGPAdvertisement custom resource. This specification associates a pool of IP addresses with a set of nodes in the cluster. This is useful when you have nodes on different subnets in a cluster and you want to advertise an IP addresses from an address pool from a specific subnet, for example a public-facing subnet only.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create an IP address pool by using a custom resource:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: pool1
spec:
  addresses:
    - 4.4.4.100-4.4.4.200
    - 2001:100:4::200-2001:100:4::400</pre></li><li class="listitem"><p class="simpara">
							Control which nodes in the cluster the IP address from <code class="literal cluster-admin">pool1</code> advertises from by defining the <code class="literal cluster-admin">.spec.nodeSelector</code> value in the BGPAdvertisement custom resource:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: example
spec:
  ipAddressPools:
  - pool1
  nodeSelector:
  - matchLabels:
      kubernetes.io/hostname: NodeA
  - matchLabels:
      kubernetes.io/hostname: NodeB</pre></li></ol></div><p class="cluster-admin cluster-admin">
					In this example, the IP address from <code class="literal cluster-admin">pool1</code> advertises from <code class="literal cluster-admin">NodeA</code> and <code class="literal cluster-admin">NodeB</code> only.
				</p></section><section class="section cluster-admin" id="nw-metallb-l2padvertisement-cr_about-advertising-ip-address-pool"><div class="titlepage"><div><div><h3 class="title">34.5.5. About the L2Advertisement custom resource</h3></div></div></div><p class="cluster-admin cluster-admin">
					The fields for the <code class="literal cluster-admin">l2Advertisements</code> object are defined in the following table:
				</p><div class="table" id="idm140587124481712"><p class="title"><strong>Table 34.4. L2 advertisements configuration</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587124475632" scope="col">Field</th><th align="left" valign="top" id="idm140587124474544" scope="col">Type</th><th align="left" valign="top" id="idm140587124473456" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587124475632"> <p>
									<code class="literal cluster-admin">metadata.name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124474544"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124473456"> <p class="cluster-admin cluster-admin">
									Specifies the name for the L2 advertisement.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124475632"> <p>
									<code class="literal cluster-admin">metadata.namespace</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124474544"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124473456"> <p class="cluster-admin cluster-admin">
									Specifies the namespace for the L2 advertisement. Specify the same namespace that the MetalLB Operator uses.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124475632"> <p>
									<code class="literal cluster-admin">spec.ipAddressPools</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124474544"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124473456"> <p class="cluster-admin cluster-admin">
									Optional: The list of <code class="literal cluster-admin">IPAddressPools</code> to advertise with this advertisement, selected by name.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124475632"> <p>
									<code class="literal cluster-admin">spec.ipAddressPoolSelectors</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124474544"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124473456"> <p class="cluster-admin cluster-admin">
									Optional: A selector for the <code class="literal cluster-admin">IPAddressPools</code> that gets advertised with this advertisement. This is for associating the <code class="literal cluster-admin">IPAddressPool</code> to the advertisement based on the label assigned to the <code class="literal cluster-admin">IPAddressPool</code> instead of the name itself. If no <code class="literal cluster-admin">IPAddressPool</code> is selected by this or by the list, the advertisement is applied to all the <code class="literal cluster-admin">IPAddressPools</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124475632"> <p>
									<code class="literal cluster-admin">spec.nodeSelectors</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124474544"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124473456"> <p class="cluster-admin cluster-admin">
									Optional: <code class="literal cluster-admin">NodeSelectors</code> limits the nodes to announce as next hops for the load balancer IP. When empty, all the nodes are announced as next hops.
								</p>
								 <div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
										Limiting the nodes to announce as next hops is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
									</p><p class="cluster-admin cluster-admin">
										For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
									</p></div></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587124475632"> <p>
									<code class="literal cluster-admin">spec.interfaces</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124474544"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587124473456"> <p class="cluster-admin cluster-admin">
									Optional: The list of <code class="literal cluster-admin">interfaces</code> that are used to announce the load balancer IP.
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section cluster-admin" id="nw-metallb-configure-with-L2-advertisement_about-advertising-ip-address-pool"><div class="titlepage"><div><div><h3 class="title">34.5.6. Configuring MetalLB with an L2 advertisement</h3></div></div></div><p class="cluster-admin cluster-admin">
					Configure MetalLB as follows so that the <code class="literal cluster-admin">IPAddressPool</code> is advertised with the L2 protocol.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create an IP address pool.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a file, such as <code class="literal cluster-admin">ipaddresspool.yaml</code>, with content like the following example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: doc-example-l2
spec:
  addresses:
    - 4.4.4.0/24
  autoAssign: false</pre></li><li class="listitem"><p class="simpara">
									Apply the configuration for the IP address pool:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f ipaddresspool.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a L2 advertisement.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a file, such as <code class="literal cluster-admin">l2advertisement.yaml</code>, with content like the following example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: l2advertisement
  namespace: metallb-system
spec:
  ipAddressPools:
   - doc-example-l2</pre></li><li class="listitem"><p class="simpara">
									Apply the configuration:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f l2advertisement.yaml</pre></li></ol></div></li></ol></div></section><section class="section cluster-admin" id="nw-metallb-configure-with-L2-advertisement-label_about-advertising-ip-address-pool"><div class="titlepage"><div><div><h3 class="title">34.5.7. Configuring MetalLB with a L2 advertisement and label</h3></div></div></div><p class="cluster-admin cluster-admin">
					The <code class="literal cluster-admin">ipAddressPoolSelectors</code> field in the <code class="literal cluster-admin">BGPAdvertisement</code> and <code class="literal cluster-admin">L2Advertisement</code> custom resource definitions is used to associate the <code class="literal cluster-admin">IPAddressPool</code> to the advertisement based on the label assigned to the <code class="literal cluster-admin">IPAddressPool</code> instead of the name itself.
				</p><p class="cluster-admin cluster-admin">
					This example shows how to configure MetalLB so that the <code class="literal cluster-admin">IPAddressPool</code> is advertised with the L2 protocol by configuring the <code class="literal cluster-admin">ipAddressPoolSelectors</code> field.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create an IP address pool.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a file, such as <code class="literal cluster-admin">ipaddresspool.yaml</code>, with content like the following example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: doc-example-l2-label
  labels:
    zone: east
spec:
  addresses:
    - 172.31.249.87/32</pre></li><li class="listitem"><p class="simpara">
									Apply the configuration for the IP address pool:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f ipaddresspool.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a L2 advertisement advertising the IP using <code class="literal cluster-admin">ipAddressPoolSelectors</code>.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a file, such as <code class="literal cluster-admin">l2advertisement.yaml</code>, with content like the following example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: l2advertisement-label
  namespace: metallb-system
spec:
  ipAddressPoolSelectors:
    - matchExpressions:
        - key: zone
          operator: In
          values:
            - east</pre></li><li class="listitem"><p class="simpara">
									Apply the configuration:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f l2advertisement.yaml</pre></li></ol></div></li></ol></div></section><section class="section cluster-admin" id="nw-metallb-configure-with-L2-advertisement-interface_about-advertising-ip-address-pool"><div class="titlepage"><div><div><h3 class="title">34.5.8. Configuring MetalLB with an L2 advertisement for selected interfaces</h3></div></div></div><p class="cluster-admin cluster-admin">
					By default, the IP addresses from IP address pool that has been assigned to the service, is advertised from all the network interfaces. The <code class="literal cluster-admin">interfaces</code> field in the <code class="literal cluster-admin">L2Advertisement</code> custom resource definition is used to restrict those network interfaces that advertise the IP address pool.
				</p><p class="cluster-admin cluster-admin">
					This example shows how to configure MetalLB so that the IP address pool is advertised only from the network interfaces listed in the <code class="literal cluster-admin">interfaces</code> field of all nodes.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							You are logged in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create an IP address pool.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a file, such as <code class="literal cluster-admin">ipaddresspool.yaml</code>, and enter the configuration details like the following example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: doc-example-l2
spec:
  addresses:
    - 4.4.4.0/24
  autoAssign: false</pre></li><li class="listitem"><p class="simpara">
									Apply the configuration for the IP address pool like the following example:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f ipaddresspool.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a L2 advertisement advertising the IP with <code class="literal cluster-admin">interfaces</code> selector.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a YAML file, such as <code class="literal cluster-admin">l2advertisement.yaml</code>, and enter the configuration details like the following example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: l2advertisement
  namespace: metallb-system
spec:
  ipAddressPools:
   - doc-example-l2
   interfaces:
   - interfaceA
   - interfaceB</pre></li><li class="listitem"><p class="simpara">
									Apply the configuration for the advertisement like the following example:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f l2advertisement.yaml</pre></li></ol></div></li></ol></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						The interface selector does not affect how MetalLB chooses the node to announce a given IP by using L2. The chosen node does not announce the service if the node does not have the selected interface.
					</p></div></div></section><section class="section _additional-resources" id="additional-resources_about-advertiseipaddress"><div class="titlepage"><div><div><h3 class="title">34.5.9. Additional resources</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#metallb-configure-community-alias">Configuring a community alias</a>.
						</li></ul></div></section></section><section class="section cluster-admin" id="metallb-configure-bgp-peers"><div class="titlepage"><div><div><h2 class="title">34.6. Configuring MetalLB BGP peers</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can add, modify, and delete Border Gateway Protocol (BGP) peers. The MetalLB Operator uses the BGP peer custom resources to identify which peers that MetalLB <code class="literal cluster-admin">speaker</code> pods contact to start BGP sessions. The peers receive the route advertisements for the load-balancer IP addresses that MetalLB assigns to services.
			</p><section class="section cluster-admin" id="nw-metallb-bgppeer-cr_configure-metallb-bgp-peers"><div class="titlepage"><div><div><h3 class="title">34.6.1. About the BGP peer custom resource</h3></div></div></div><p class="cluster-admin cluster-admin">
					The fields for the BGP peer custom resource are described in the following table.
				</p><div class="table" id="idm140587103864208"><p class="title"><strong>Table 34.5. MetalLB BGP peer custom resource</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587103858128" scope="col">Field</th><th align="left" valign="top" id="idm140587103857040" scope="col">Type</th><th align="left" valign="top" id="idm140587103855952" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587103858128"> <p>
									<code class="literal cluster-admin">metadata.name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103857040"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103855952"> <p>
									Specifies the name for the BGP peer custom resource.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103858128"> <p>
									<code class="literal cluster-admin">metadata.namespace</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103857040"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103855952"> <p>
									Specifies the namespace for the BGP peer custom resource.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103858128"> <p>
									<code class="literal cluster-admin">spec.myASN</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103857040"> <p>
									<code class="literal cluster-admin">integer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103855952"> <p>
									Specifies the Autonomous System number for the local end of the BGP session. Specify the same value in all BGP peer custom resources that you add. The range is <code class="literal cluster-admin">0</code> to <code class="literal cluster-admin">4294967295</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103858128"> <p>
									<code class="literal cluster-admin">spec.peerASN</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103857040"> <p>
									<code class="literal cluster-admin">integer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103855952"> <p>
									Specifies the Autonomous System number for the remote end of the BGP session. The range is <code class="literal cluster-admin">0</code> to <code class="literal cluster-admin">4294967295</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103858128"> <p>
									<code class="literal cluster-admin">spec.peerAddress</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103857040"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103855952"> <p>
									Specifies the IP address of the peer to contact for establishing the BGP session.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103858128"> <p>
									<code class="literal cluster-admin">spec.sourceAddress</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103857040"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103855952"> <p>
									Optional: Specifies the IP address to use when establishing the BGP session. The value must be an IPv4 address.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103858128"> <p>
									<code class="literal cluster-admin">spec.peerPort</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103857040"> <p>
									<code class="literal cluster-admin">integer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103855952"> <p>
									Optional: Specifies the network port of the peer to contact for establishing the BGP session. The range is <code class="literal cluster-admin">0</code> to <code class="literal cluster-admin">16384</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103858128"> <p>
									<code class="literal cluster-admin">spec.holdTime</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103857040"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103855952"> <p>
									Optional: Specifies the duration for the hold time to propose to the BGP peer. The minimum value is 3 seconds (<code class="literal cluster-admin">3s</code>). The common units are seconds and minutes, such as <code class="literal cluster-admin">3s</code>, <code class="literal cluster-admin">1m</code>, and <code class="literal cluster-admin">5m30s</code>. To detect path failures more quickly, also configure BFD.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103858128"> <p>
									<code class="literal cluster-admin">spec.keepaliveTime</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103857040"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103855952"> <p>
									Optional: Specifies the maximum interval between sending keep-alive messages to the BGP peer. If you specify this field, you must also specify a value for the <code class="literal cluster-admin">holdTime</code> field. The specified value must be less than the value for the <code class="literal cluster-admin">holdTime</code> field.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103858128"> <p>
									<code class="literal cluster-admin">spec.routerID</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103857040"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103855952"> <p>
									Optional: Specifies the router ID to advertise to the BGP peer. If you specify this field, you must specify the same value in every BGP peer custom resource that you add.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103858128"> <p>
									<code class="literal cluster-admin">spec.password</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103857040"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103855952"> <p>
									Optional: Specifies the MD5 password to send to the peer for routers that enforce TCP MD5 authenticated BGP sessions.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103858128"> <p>
									<code class="literal cluster-admin">spec.passwordSecret</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103857040"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103855952"> <p>
									Optional: Specifies name of the authentication secret for the BGP Peer. The secret must live in the <code class="literal cluster-admin">metallb</code> namespace and be of type basic-auth.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103858128"> <p>
									<code class="literal cluster-admin">spec.bfdProfile</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103857040"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103855952"> <p>
									Optional: Specifies the name of a BFD profile.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103858128"> <p>
									<code class="literal cluster-admin">spec.nodeSelectors</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103857040"> <p>
									<code class="literal cluster-admin">object[]</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103855952"> <p>
									Optional: Specifies a selector, using match expressions and match labels, to control which nodes can connect to the BGP peer.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103858128"> <p>
									<code class="literal cluster-admin">spec.ebgpMultiHop</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103857040"> <p>
									<code class="literal cluster-admin">boolean</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103855952"> <p>
									Optional: Specifies that the BGP peer is multiple network hops away. If the BGP peer is not directly connected to the same network, the speaker cannot establish a BGP session unless this field is set to <code class="literal cluster-admin">true</code>. This field applies to <span class="emphasis"><em><span class="cluster-admin cluster-admin">external BGP</span></em></span>. External BGP is the term that is used to describe when a BGP peer belongs to a different Autonomous System.
								</p>
								 </td></tr></tbody></table></div></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						The <code class="literal cluster-admin">passwordSecret</code> field is mutually exclusive with the <code class="literal cluster-admin">password</code> field, and contains a reference to a secret containing the password to use. Setting both fields results in a failure of the parsing.
					</p></div></div></section><section class="section cluster-admin" id="nw-metallb-configure-bgppeer_configure-metallb-bgp-peers"><div class="titlepage"><div><div><h3 class="title">34.6.2. Configuring a BGP peer</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can add a BGP peer custom resource to exchange routing information with network routers and advertise the IP addresses for services.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li><li class="listitem">
							Configure MetalLB with a BGP advertisement.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a file, such as <code class="literal cluster-admin">bgppeer.yaml</code>, with content like the following example:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  namespace: metallb-system
  name: doc-example-peer
spec:
  peerAddress: 10.0.0.1
  peerASN: 64501
  myASN: 64500
  routerID: 10.10.10.10</pre></li><li class="listitem"><p class="simpara">
							Apply the configuration for the BGP peer:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f bgppeer.yaml</pre></li></ol></div></section><section class="section cluster-admin" id="nw-metallb-example-assign-specific-address-pools-specific-bgp-peers_configure-metallb-bgp-peers"><div class="titlepage"><div><div><h3 class="title">34.6.3. Configure a specific set of BGP peers for a given address pool</h3></div></div></div><p class="cluster-admin cluster-admin">
					This procedure illustrates how to:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Configure a set of address pools (<code class="literal cluster-admin">pool1</code> and <code class="literal cluster-admin">pool2</code>).
						</li><li class="listitem">
							Configure a set of BGP peers (<code class="literal cluster-admin">peer1</code> and <code class="literal cluster-admin">peer2</code>).
						</li><li class="listitem">
							Configure BGP advertisement to assign <code class="literal cluster-admin">pool1</code> to <code class="literal cluster-admin">peer1</code> and <code class="literal cluster-admin">pool2</code> to <code class="literal cluster-admin">peer2</code>.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create address pool <code class="literal cluster-admin">pool1</code>.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a file, such as <code class="literal cluster-admin">ipaddresspool1.yaml</code>, with content like the following example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: pool1
spec:
  addresses:
    - 4.4.4.100-4.4.4.200
    - 2001:100:4::200-2001:100:4::400</pre></li><li class="listitem"><p class="simpara">
									Apply the configuration for the IP address pool <code class="literal cluster-admin">pool1</code>:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f ipaddresspool1.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create address pool <code class="literal cluster-admin">pool2</code>.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a file, such as <code class="literal cluster-admin">ipaddresspool2.yaml</code>, with content like the following example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: pool2
spec:
  addresses:
    - 5.5.5.100-5.5.5.200
    - 2001:100:5::200-2001:100:5::400</pre></li><li class="listitem"><p class="simpara">
									Apply the configuration for the IP address pool <code class="literal cluster-admin">pool2</code>:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f ipaddresspool2.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create BGP <code class="literal cluster-admin">peer1</code>.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a file, such as <code class="literal cluster-admin">bgppeer1.yaml</code>, with content like the following example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  namespace: metallb-system
  name: peer1
spec:
  peerAddress: 10.0.0.1
  peerASN: 64501
  myASN: 64500
  routerID: 10.10.10.10</pre></li><li class="listitem"><p class="simpara">
									Apply the configuration for the BGP peer:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f bgppeer1.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create BGP <code class="literal cluster-admin">peer2</code>.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a file, such as <code class="literal cluster-admin">bgppeer2.yaml</code>, with content like the following example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  namespace: metallb-system
  name: peer2
spec:
  peerAddress: 10.0.0.2
  peerASN: 64501
  myASN: 64500
  routerID: 10.10.10.10</pre></li><li class="listitem"><p class="simpara">
									Apply the configuration for the BGP peer2:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f bgppeer2.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create BGP advertisement 1.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a file, such as <code class="literal cluster-admin">bgpadvertisement1.yaml</code>, with content like the following example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: bgpadvertisement-1
  namespace: metallb-system
spec:
  ipAddressPools:
    - pool1
  peers:
    - peer1
  communities:
    - 65535:65282
  aggregationLength: 32
  aggregationLengthV6: 128
  localPref: 100</pre></li><li class="listitem"><p class="simpara">
									Apply the configuration:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f bgpadvertisement1.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create BGP advertisement 2.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a file, such as <code class="literal cluster-admin">bgpadvertisement2.yaml</code>, with content like the following example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: bgpadvertisement-2
  namespace: metallb-system
spec:
  ipAddressPools:
    - pool2
  peers:
    - peer2
  communities:
    - 65535:65282
  aggregationLength: 32
  aggregationLengthV6: 128
  localPref: 100</pre></li><li class="listitem"><p class="simpara">
									Apply the configuration:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f bgpadvertisement2.yaml</pre></li></ol></div></li></ol></div></section><section class="section cluster-admin" id="nw-metallb-example-bgppeer_configure-metallb-bgp-peers"><div class="titlepage"><div><div><h3 class="title">34.6.4. Example BGP peer configurations</h3></div></div></div><section class="section cluster-admin" id="nw-metallb-example-limit-nodes-bgppeer_configure-metallb-bgp-peers"><div class="titlepage"><div><div><h4 class="title">34.6.4.1. Example: Limit which nodes connect to a BGP peer</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can specify the node selectors field to control which nodes can connect to a BGP peer.
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  name: doc-example-nodesel
  namespace: metallb-system
spec:
  peerAddress: 10.0.20.1
  peerASN: 64501
  myASN: 64500
  nodeSelectors:
  - matchExpressions:
    - key: kubernetes.io/hostname
      operator: In
      values: [compute-1.example.com, compute-2.example.com]</pre></section><section class="section cluster-admin" id="nw-metallb-example-specify-bfd-profile_configure-metallb-bgp-peers"><div class="titlepage"><div><div><h4 class="title">34.6.4.2. Example: Specify a BFD profile for a BGP peer</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can specify a BFD profile to associate with BGP peers. BFD compliments BGP by providing more rapid detection of communication failures between peers than BGP alone.
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  name: doc-example-peer-bfd
  namespace: metallb-system
spec:
  peerAddress: 10.0.20.1
  peerASN: 64501
  myASN: 64500
  holdTime: "10s"
  bfdProfile: doc-example-bfd-profile-full</pre><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
							Deleting the bidirectional forwarding detection (BFD) profile and removing the <code class="literal cluster-admin">bfdProfile</code> added to the border gateway protocol (BGP) peer resource does not disable the BFD. Instead, the BGP peer starts using the default BFD profile. To disable BFD from a BGP peer resource, delete the BGP peer configuration and recreate it without a BFD profile. For more information, see <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2050824"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">BZ#2050824</span></strong></span></a>.
						</p></div></div></section><section class="section cluster-admin" id="nw-metallb-example-dual-stack_configure-metallb-bgp-peers"><div class="titlepage"><div><div><h4 class="title">34.6.4.3. Example: Specify BGP peers for dual-stack networking</h4></div></div></div><p class="cluster-admin cluster-admin">
						To support dual-stack networking, add one BGP peer custom resource for IPv4 and one BGP peer custom resource for IPv6.
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  name: doc-example-dual-stack-ipv4
  namespace: metallb-system
spec:
  peerAddress: 10.0.20.1
  peerASN: 64500
  myASN: 64500
---
apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  name: doc-example-dual-stack-ipv6
  namespace: metallb-system
spec:
  peerAddress: 2620:52:0:88::104
  peerASN: 64500
  myASN: 64500</pre></section></section><section class="section cluster-admin" id="next-steps_configure-metallb-bgp-peers"><div class="titlepage"><div><div><h3 class="title">34.6.5. Next steps</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#metallb-configure-services">Configuring services to use MetalLB</a>
						</li></ul></div></section></section><section class="section cluster-admin" id="metallb-configure-community-alias"><div class="titlepage"><div><div><h2 class="title">34.7. Configuring community alias</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can configure a community alias and use it across different advertisements.
			</p><section class="section cluster-admin" id="nw-metallb-community-cr_configure-community-alias"><div class="titlepage"><div><div><h3 class="title">34.7.1. About the community custom resource</h3></div></div></div><p class="cluster-admin cluster-admin">
					The <code class="literal cluster-admin">community</code> custom resource is a collection of aliases for communities. Users can define named aliases to be used when advertising <code class="literal cluster-admin">ipAddressPools</code> using the <code class="literal cluster-admin">BGPAdvertisement</code>. The fields for the <code class="literal cluster-admin">community</code> custom resource are described in the following table.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						The <code class="literal cluster-admin">community</code> CRD applies only to BGPAdvertisement.
					</p></div></div><div class="table" id="idm140587103613184"><p class="title"><strong>Table 34.6. MetalLB community custom resource</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587103607104" scope="col">Field</th><th align="left" valign="top" id="idm140587103606016" scope="col">Type</th><th align="left" valign="top" id="idm140587103604928" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587103607104"> <p>
									<code class="literal cluster-admin">metadata.name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103606016"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103604928"> <p class="cluster-admin cluster-admin">
									Specifies the name for the <code class="literal cluster-admin">community</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103607104"> <p>
									<code class="literal cluster-admin">metadata.namespace</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103606016"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103604928"> <p class="cluster-admin cluster-admin">
									Specifies the namespace for the <code class="literal cluster-admin">community</code>. Specify the same namespace that the MetalLB Operator uses.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103607104"> <p>
									<code class="literal cluster-admin">spec.communities</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103606016"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103604928"> <p class="cluster-admin cluster-admin">
									Specifies a list of BGP community aliases that can be used in BGPAdvertisements. A community alias consists of a pair of name (alias) and value (number:number). Link the BGPAdvertisement to a community alias by referring to the alias name in its <code class="literal cluster-admin">spec.communities</code> field.
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140587103578864"><p class="title"><strong>Table 34.7. CommunityAlias</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587103572800" scope="col">Field</th><th align="left" valign="top" id="idm140587103571712" scope="col">Type</th><th align="left" valign="top" id="idm140587103570624" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587103572800"> <p>
									<code class="literal cluster-admin">name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103571712"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103570624"> <p class="cluster-admin cluster-admin">
									The name of the alias for the <code class="literal cluster-admin">community</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103572800"> <p>
									<code class="literal cluster-admin">value</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103571712"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103570624"> <p class="cluster-admin cluster-admin">
									The BGP <code class="literal cluster-admin">community</code> value corresponding to the given name.
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section cluster-admin" id="nw-metallb-configure-BGP-advertisement-community-alias_configure-community-alias"><div class="titlepage"><div><div><h3 class="title">34.7.2. Configuring MetalLB with a BGP advertisement and community alias</h3></div></div></div><p class="cluster-admin cluster-admin">
					Configure MetalLB as follows so that the <code class="literal cluster-admin">IPAddressPool</code> is advertised with the BGP protocol and the community alias set to the numeric value of the NO_ADVERTISE community.
				</p><p class="cluster-admin cluster-admin">
					In the following example, the peer BGP router <code class="literal cluster-admin">doc-example-peer-community</code> receives one <code class="literal cluster-admin">203.0.113.200/32</code> route and one <code class="literal cluster-admin">fc00:f853:ccd:e799::1/128</code> route for each load-balancer IP address that MetalLB assigns to a service. A community alias is configured with the <code class="literal cluster-admin">NO_ADVERTISE</code> community.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create an IP address pool.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a file, such as <code class="literal cluster-admin">ipaddresspool.yaml</code>, with content like the following example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: doc-example-bgp-community
spec:
  addresses:
    - 203.0.113.200/30
    - fc00:f853:ccd:e799::/124</pre></li><li class="listitem"><p class="simpara">
									Apply the configuration for the IP address pool:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f ipaddresspool.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a community alias named <code class="literal cluster-admin">community1</code>.
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: Community
metadata:
  name: community1
  namespace: metallb-system
spec:
  communities:
    - name: NO_ADVERTISE
      value: '65535:65282'</pre></li><li class="listitem"><p class="simpara">
							Create a BGP peer named <code class="literal cluster-admin">doc-example-bgp-peer</code>.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a file, such as <code class="literal cluster-admin">bgppeer.yaml</code>, with content like the following example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  namespace: metallb-system
  name: doc-example-bgp-peer
spec:
  peerAddress: 10.0.0.1
  peerASN: 64501
  myASN: 64500
  routerID: 10.10.10.10</pre></li><li class="listitem"><p class="simpara">
									Apply the configuration for the BGP peer:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f bgppeer.yaml</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a BGP advertisement with the community alias.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Create a file, such as <code class="literal cluster-admin">bgpadvertisement.yaml</code>, with content like the following example:
								</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: bgp-community-sample
  namespace: metallb-system
spec:
  aggregationLength: 32
  aggregationLengthV6: 128
  communities:
    - NO_ADVERTISE <span id="CO231-1"><!--Empty--></span><span class="callout">1</span>
  ipAddressPools:
    - doc-example-bgp-community
  peers:
    - doc-example-peer</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO231-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the <code class="literal cluster-admin">CommunityAlias.name</code> here and not the community custom resource (CR) name.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									Apply the configuration:
								</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f bgpadvertisement.yaml</pre></li></ol></div></li></ol></div></section></section><section class="section cluster-admin" id="metallb-configure-bfd-profiles"><div class="titlepage"><div><div><h2 class="title">34.8. Configuring MetalLB BFD profiles</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, you can add, modify, and delete Bidirectional Forwarding Detection (BFD) profiles. The MetalLB Operator uses the BFD profile custom resources to identify which BGP sessions use BFD to provide faster path failure detection than BGP alone provides.
			</p><section class="section cluster-admin" id="nw-metallb-bfdprofile-cr_configure-metallb-bfd-profiles"><div class="titlepage"><div><div><h3 class="title">34.8.1. About the BFD profile custom resource</h3></div></div></div><p class="cluster-admin cluster-admin">
					The fields for the BFD profile custom resource are described in the following table.
				</p><div class="table" id="idm140587103501536"><p class="title"><strong>Table 34.8. BFD profile custom resource</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 60%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587103495456" scope="col">Field</th><th align="left" valign="top" id="idm140587103494368" scope="col">Type</th><th align="left" valign="top" id="idm140587103493280" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587103495456"> <p>
									<code class="literal cluster-admin">metadata.name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103494368"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103493280"> <p class="cluster-admin cluster-admin">
									Specifies the name for the BFD profile custom resource.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103495456"> <p>
									<code class="literal cluster-admin">metadata.namespace</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103494368"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103493280"> <p class="cluster-admin cluster-admin">
									Specifies the namespace for the BFD profile custom resource.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103495456"> <p>
									<code class="literal cluster-admin">spec.detectMultiplier</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103494368"> <p>
									<code class="literal cluster-admin">integer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103493280"> <p class="cluster-admin cluster-admin">
									Specifies the detection multiplier to determine packet loss. The remote transmission interval is multiplied by this value to determine the connection loss detection timer.
								</p>
								 <p class="cluster-admin cluster-admin">
									For example, when the local system has the detect multiplier set to <code class="literal cluster-admin">3</code> and the remote system has the transmission interval set to <code class="literal cluster-admin">300</code>, the local system detects failures only after <code class="literal cluster-admin">900</code> ms without receiving packets.
								</p>
								 <p class="cluster-admin cluster-admin">
									The range is <code class="literal cluster-admin">2</code> to <code class="literal cluster-admin">255</code>. The default value is <code class="literal cluster-admin">3</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103495456"> <p>
									<code class="literal cluster-admin">spec.echoMode</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103494368"> <p>
									<code class="literal cluster-admin">boolean</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103493280"> <p class="cluster-admin cluster-admin">
									Specifies the echo transmission mode. If you are not using distributed BFD, echo transmission mode works only when the peer is also FRR. The default value is <code class="literal cluster-admin">false</code> and echo transmission mode is disabled.
								</p>
								 <p class="cluster-admin cluster-admin">
									When echo transmission mode is enabled, consider increasing the transmission interval of control packets to reduce bandwidth usage. For example, consider increasing the transmit interval to <code class="literal cluster-admin">2000</code> ms.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103495456"> <p>
									<code class="literal cluster-admin">spec.echoInterval</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103494368"> <p>
									<code class="literal cluster-admin">integer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103493280"> <p class="cluster-admin cluster-admin">
									Specifies the minimum transmission interval, less jitter, that this system uses to send and receive echo packets. The range is <code class="literal cluster-admin">10</code> to <code class="literal cluster-admin">60000</code>. The default value is <code class="literal cluster-admin">50</code> ms.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103495456"> <p>
									<code class="literal cluster-admin">spec.minimumTtl</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103494368"> <p>
									<code class="literal cluster-admin">integer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103493280"> <p class="cluster-admin cluster-admin">
									Specifies the minimum expected TTL for an incoming control packet. This field applies to multi-hop sessions only.
								</p>
								 <p class="cluster-admin cluster-admin">
									The purpose of setting a minimum TTL is to make the packet validation requirements more stringent and avoid receiving control packets from other sessions.
								</p>
								 <p class="cluster-admin cluster-admin">
									The default value is <code class="literal cluster-admin">254</code> and indicates that the system expects only one hop between this system and the peer.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103495456"> <p>
									<code class="literal cluster-admin">spec.passiveMode</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103494368"> <p>
									<code class="literal cluster-admin">boolean</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103493280"> <p class="cluster-admin cluster-admin">
									Specifies whether a session is marked as active or passive. A passive session does not attempt to start the connection. Instead, a passive session waits for control packets from a peer before it begins to reply.
								</p>
								 <p class="cluster-admin cluster-admin">
									Marking a session as passive is useful when you have a router that acts as the central node of a star network and you want to avoid sending control packets that you do not need the system to send.
								</p>
								 <p class="cluster-admin cluster-admin">
									The default value is <code class="literal cluster-admin">false</code> and marks the session as active.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103495456"> <p>
									<code class="literal cluster-admin">spec.receiveInterval</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103494368"> <p>
									<code class="literal cluster-admin">integer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103493280"> <p class="cluster-admin cluster-admin">
									Specifies the minimum interval that this system is capable of receiving control packets. The range is <code class="literal cluster-admin">10</code> to <code class="literal cluster-admin">60000</code>. The default value is <code class="literal cluster-admin">300</code> ms.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587103495456"> <p>
									<code class="literal cluster-admin">spec.transmitInterval</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103494368"> <p>
									<code class="literal cluster-admin">integer</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587103493280"> <p class="cluster-admin cluster-admin">
									Specifies the minimum transmission interval, less jitter, that this system uses to send control packets. The range is <code class="literal cluster-admin">10</code> to <code class="literal cluster-admin">60000</code>. The default value is <code class="literal cluster-admin">300</code> ms.
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section cluster-admin" id="nw-metallb-configure-bfdprofile_configure-metallb-bfd-profiles"><div class="titlepage"><div><div><h3 class="title">34.8.2. Configuring a BFD profile</h3></div></div></div><p class="cluster-admin cluster-admin">
					As a cluster administrator, you can add a BFD profile and configure a BGP peer to use the profile. BFD provides faster path failure detection than BGP alone.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Log in as a user with <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a file, such as <code class="literal cluster-admin">bfdprofile.yaml</code>, with content like the following example:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: BFDProfile
metadata:
  name: doc-example-bfd-profile-full
  namespace: metallb-system
spec:
  receiveInterval: 300
  transmitInterval: 300
  detectMultiplier: 3
  echoMode: false
  passiveMode: true
  minimumTtl: 254</pre></li><li class="listitem"><p class="simpara">
							Apply the configuration for the BFD profile:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f bfdprofile.yaml</pre></li></ol></div></section><section class="section cluster-admin" id="next-steps_configure-metallb-bfd-profiles"><div class="titlepage"><div><div><h3 class="title">34.8.3. Next steps</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#metallb-configure-bgp-peers">Configure a BGP peer</a> to use the BFD profile.
						</li></ul></div></section></section><section class="section cluster-admin" id="metallb-configure-services"><div class="titlepage"><div><div><h2 class="title">34.9. Configuring services to use MetalLB</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a cluster administrator, when you add a service of type <code class="literal cluster-admin">LoadBalancer</code>, you can control how MetalLB assigns an IP address.
			</p><section class="section cluster-admin" id="request-specific-ip-address_configure-services-metallb"><div class="titlepage"><div><div><h3 class="title">34.9.1. Request a specific IP address</h3></div></div></div><p class="cluster-admin cluster-admin">
					Like some other load-balancer implementations, MetalLB accepts the <code class="literal cluster-admin">spec.loadBalancerIP</code> field in the service specification.
				</p><p class="cluster-admin cluster-admin">
					If the requested IP address is within a range from any address pool, MetalLB assigns the requested IP address. If the requested IP address is not within any range, MetalLB reports a warning.
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example service YAML for a specific IP address</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: Service
metadata:
  name: &lt;service_name&gt;
  annotations:
    metallb.universe.tf/address-pool: &lt;address_pool_name&gt;
spec:
  selector:
    &lt;label_key&gt;: &lt;label_value&gt;
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
  type: LoadBalancer
  loadBalancerIP: &lt;ip_address&gt;</pre>

					</p></div><p class="cluster-admin cluster-admin">
					If MetalLB cannot assign the requested IP address, the <code class="literal cluster-admin">EXTERNAL-IP</code> for the service reports <code class="literal cluster-admin">&lt;pending&gt;</code> and running <code class="literal cluster-admin">oc describe service &lt;service_name&gt;</code> includes an event like the following example.
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example event when MetalLB cannot assign a requested IP address</strong></p><p>
						
<pre class="programlisting language-terminal">  ...
Events:
  Type     Reason            Age    From                Message
  ----     ------            ----   ----                -------
  Warning  AllocationFailed  3m16s  metallb-controller  Failed to allocate IP for "default/invalid-request": "4.3.2.1" is not allowed in config</pre>

					</p></div></section><section class="section cluster-admin" id="request-ip-address-from-pool_configure-services-metallb"><div class="titlepage"><div><div><h3 class="title">34.9.2. Request an IP address from a specific pool</h3></div></div></div><p class="cluster-admin cluster-admin">
					To assign an IP address from a specific range, but you are not concerned with the specific IP address, then you can use the <code class="literal cluster-admin">metallb.universe.tf/address-pool</code> annotation to request an IP address from the specified address pool.
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example service YAML for an IP address from a specific pool</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: Service
metadata:
  name: &lt;service_name&gt;
  annotations:
    metallb.universe.tf/address-pool: &lt;address_pool_name&gt;
spec:
  selector:
    &lt;label_key&gt;: &lt;label_value&gt;
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
  type: LoadBalancer</pre>

					</p></div><p class="cluster-admin cluster-admin">
					If the address pool that you specify for <code class="literal cluster-admin">&lt;address_pool_name&gt;</code> does not exist, MetalLB attempts to assign an IP address from any pool that permits automatic assignment.
				</p></section><section class="section cluster-admin" id="accept-any-ip-address_configure-services-metallb"><div class="titlepage"><div><div><h3 class="title">34.9.3. Accept any IP address</h3></div></div></div><p class="cluster-admin cluster-admin">
					By default, address pools are configured to permit automatic assignment. MetalLB assigns an IP address from these address pools.
				</p><p class="cluster-admin cluster-admin">
					To accept any IP address from any pool that is configured for automatic assignment, no special annotation or configuration is required.
				</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example service YAML for accepting any IP address</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: v1
kind: Service
metadata:
  name: &lt;service_name&gt;
spec:
  selector:
    &lt;label_key&gt;: &lt;label_value&gt;
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
  type: LoadBalancer</pre>

					</p></div></section><section class="section cluster-admin" id="share-specific-ip-address_configure-services-metallb"><div class="titlepage"><div><div><h3 class="title">34.9.4. Share a specific IP address</h3></div></div></div><p class="cluster-admin cluster-admin">
					By default, services do not share IP addresses. However, if you need to colocate services on a single IP address, you can enable selective IP sharing by adding the <code class="literal cluster-admin">metallb.universe.tf/allow-shared-ip</code> annotation to the services.
				</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Service
metadata:
  name: service-http
  annotations:
    metallb.universe.tf/address-pool: doc-example
    metallb.universe.tf/allow-shared-ip: "web-server-svc"  <span id="CO232-1"><!--Empty--></span><span class="callout">1</span>
spec:
  ports:
    - name: http
      port: 80  <span id="CO232-2"><!--Empty--></span><span class="callout">2</span>
      protocol: TCP
      targetPort: 8080
  selector:
    &lt;label_key&gt;: &lt;label_value&gt;  <span id="CO232-3"><!--Empty--></span><span class="callout">3</span>
  type: LoadBalancer
  loadBalancerIP: 172.31.249.7  <span id="CO232-4"><!--Empty--></span><span class="callout">4</span>
---
apiVersion: v1
kind: Service
metadata:
  name: service-https
  annotations:
    metallb.universe.tf/address-pool: doc-example
    metallb.universe.tf/allow-shared-ip: "web-server-svc"  <span id="CO232-5"><!--Empty--></span><span class="callout">5</span>
spec:
  ports:
    - name: https
      port: 443  <span id="CO232-6"><!--Empty--></span><span class="callout">6</span>
      protocol: TCP
      targetPort: 8080
  selector:
    &lt;label_key&gt;: &lt;label_value&gt;  <span id="CO232-7"><!--Empty--></span><span class="callout">7</span>
  type: LoadBalancer
  loadBalancerIP: 172.31.249.7  <span id="CO232-8"><!--Empty--></span><span class="callout">8</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO232-1"><span class="callout">1</span></a> <a href="#CO232-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							Specify the same value for the <code class="literal cluster-admin">metallb.universe.tf/allow-shared-ip</code> annotation. This value is referred to as the <span class="emphasis"><em><span class="cluster-admin cluster-admin">sharing key</span></em></span>.
						</div></dd><dt><a href="#CO232-2"><span class="callout">2</span></a> <a href="#CO232-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							Specify different port numbers for the services.
						</div></dd><dt><a href="#CO232-3"><span class="callout">3</span></a> <a href="#CO232-7"><span class="callout">7</span></a> </dt><dd><div class="para">
							Specify identical pod selectors if you must specify <code class="literal cluster-admin">externalTrafficPolicy: local</code> so the services send traffic to the same set of pods. If you use the <code class="literal cluster-admin">cluster</code> external traffic policy, then the pod selectors do not need to be identical.
						</div></dd><dt><a href="#CO232-4"><span class="callout">4</span></a> <a href="#CO232-8"><span class="callout">8</span></a> </dt><dd><div class="para">
							Optional: If you specify the three preceding items, MetalLB might colocate the services on the same IP address. To ensure that services share an IP address, specify the IP address to share.
						</div></dd></dl></div><p class="cluster-admin cluster-admin">
					By default, Kubernetes does not allow multiprotocol load balancer services. This limitation would normally make it impossible to run a service like DNS that needs to listen on both TCP and UDP. To work around this limitation of Kubernetes with MetalLB, create two services:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							For one service, specify TCP and for the second service, specify UDP.
						</li><li class="listitem">
							In both services, specify the same pod selector.
						</li><li class="listitem">
							Specify the same sharing key and <code class="literal cluster-admin">spec.loadBalancerIP</code> value to colocate the TCP and UDP services on the same IP address.
						</li></ul></div></section><section class="section cluster-admin" id="nw-metallb-configure-svc_configure-services-metallb"><div class="titlepage"><div><div><h3 class="title">34.9.5. Configuring a service with MetalLB</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can configure a load-balancing service to use an external IP address from an address pool.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Install the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li><li class="listitem">
							Install the MetalLB Operator and start MetalLB.
						</li><li class="listitem">
							Configure at least one address pool.
						</li><li class="listitem">
							Configure your network to route traffic from the clients to the host network for the cluster.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal cluster-admin">&lt;service_name&gt;.yaml</code> file. In the file, ensure that the <code class="literal cluster-admin">spec.type</code> field is set to <code class="literal cluster-admin">LoadBalancer</code>.
						</p><p class="cluster-admin cluster-admin">
							Refer to the examples for information about how to request the external IP address that MetalLB assigns to the service.
						</p></li><li class="listitem"><p class="simpara">
							Create the service:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc apply -f &lt;service_name&gt;.yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">service/&lt;service_name&gt; created</pre>

							</p></div></li></ol></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
							Describe the service:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc describe service &lt;service_name&gt;</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">Name:                     &lt;service_name&gt;
Namespace:                default
Labels:                   &lt;none&gt;
Annotations:              metallb.universe.tf/address-pool: doc-example  &lt;.&gt;
Selector:                 app=service_name
Type:                     LoadBalancer  &lt;.&gt;
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.105.237.254
IPs:                      10.105.237.254
LoadBalancer Ingress:     192.168.100.5  &lt;.&gt;
Port:                     &lt;unset&gt;  80/TCP
TargetPort:               8080/TCP
NodePort:                 &lt;unset&gt;  30550/TCP
Endpoints:                10.244.0.50:8080
Session Affinity:         None
External Traffic Policy:  Cluster
Events:  &lt;.&gt;
  Type    Reason        Age                From             Message
  ----    ------        ----               ----             -------
  Normal  nodeAssigned  32m (x2 over 32m)  metallb-speaker  announcing from node "&lt;node_name&gt;"</pre>

							</p></div><p class="cluster-admin cluster-admin">
							&lt;.&gt; The annotation is present if you request an IP address from a specific pool. &lt;.&gt; The service type must indicate <code class="literal cluster-admin">LoadBalancer</code>. &lt;.&gt; The load-balancer ingress field indicates the external IP address if the service is assigned correctly. &lt;.&gt; The events field indicates the node name that is assigned to announce the external IP address. If you experience an error, the events field indicates the reason for the error.
						</p></li></ul></div></section></section><section class="section cluster-admin" id="metallb-logging-troubleshooting-support"><div class="titlepage"><div><div><h2 class="title">34.10. MetalLB logging, troubleshooting, and support</h2></div></div></div><p class="cluster-admin cluster-admin">
				If you need to troubleshoot MetalLB configuration, see the following sections for commonly used commands.
			</p><section class="section cluster-admin" id="nw-metallb-setting-metalb-logging-levels_metallb-troubleshoot-support"><div class="titlepage"><div><div><h3 class="title">34.10.1. Setting the MetalLB logging levels</h3></div></div></div><p class="cluster-admin cluster-admin">
					MetalLB uses FRRouting (FRR) in a container with the default setting of <code class="literal cluster-admin">info</code> generates a lot of logging. You can control the verbosity of the logs generated by setting the <code class="literal cluster-admin">logLevel</code> as illustrated in this example.
				</p><p class="cluster-admin cluster-admin">
					Gain a deeper insight into MetalLB by setting the <code class="literal cluster-admin">logLevel</code> to <code class="literal cluster-admin">debug</code> as follows:
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Create a file, such as <code class="literal cluster-admin">setdebugloglevel.yaml</code>, with content like the following example:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
spec:
  logLevel: debug
  nodeSelector:
    node-role.kubernetes.io/worker: ""</pre></li><li class="listitem"><p class="simpara">
							Apply the configuration:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc replace -f setdebugloglevel.yaml</pre><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								Use <code class="literal cluster-admin">oc replace</code> as the understanding is the <code class="literal cluster-admin">metallb</code> CR is already created and here you are changing the log level.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Display the names of the <code class="literal cluster-admin">speaker</code> pods:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get -n metallb-system pods -l component=speaker</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">NAME                    READY   STATUS    RESTARTS   AGE
speaker-2m9pm           4/4     Running   0          9m19s
speaker-7m4qw           3/4     Running   0          19s
speaker-szlmx           4/4     Running   0          9m19s</pre>

							</p></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								Speaker and controller pods are recreated to ensure the updated logging level is applied. The logging level is modified for all the components of MetalLB.
							</p></div></div></li><li class="listitem"><p class="simpara">
							View the <code class="literal cluster-admin">speaker</code> logs:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc logs -n metallb-system speaker-7m4qw -c speaker</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">{"branch":"main","caller":"main.go:92","commit":"3d052535","goversion":"gc / go1.17.1 / amd64","level":"info","msg":"MetalLB speaker starting (commit 3d052535, branch main)","ts":"2022-05-17T09:55:05Z","version":""}
{"caller":"announcer.go:110","event":"createARPResponder","interface":"ens4","level":"info","msg":"created ARP responder for interface","ts":"2022-05-17T09:55:05Z"}
{"caller":"announcer.go:119","event":"createNDPResponder","interface":"ens4","level":"info","msg":"created NDP responder for interface","ts":"2022-05-17T09:55:05Z"}
{"caller":"announcer.go:110","event":"createARPResponder","interface":"tun0","level":"info","msg":"created ARP responder for interface","ts":"2022-05-17T09:55:05Z"}
{"caller":"announcer.go:119","event":"createNDPResponder","interface":"tun0","level":"info","msg":"created NDP responder for interface","ts":"2022-05-17T09:55:05Z"}
I0517 09:55:06.515686      95 request.go:665] Waited for 1.026500832s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1?timeout=32s
{"Starting Manager":"(MISSING)","caller":"k8s.go:389","level":"info","ts":"2022-05-17T09:55:08Z"}
{"caller":"speakerlist.go:310","level":"info","msg":"node event - forcing sync","node addr":"10.0.128.4","node event":"NodeJoin","node name":"ci-ln-qb8t3mb-72292-7s7rh-worker-a-vvznj","ts":"2022-05-17T09:55:08Z"}
{"caller":"service_controller.go:113","controller":"ServiceReconciler","enqueueing":"openshift-kube-controller-manager-operator/metrics","epslice":"{\"metadata\":{\"name\":\"metrics-xtsxr\",\"generateName\":\"metrics-\",\"namespace\":\"openshift-kube-controller-manager-operator\",\"uid\":\"ac6766d7-8504-492c-9d1e-4ae8897990ad\",\"resourceVersion\":\"9041\",\"generation\":4,\"creationTimestamp\":\"2022-05-17T07:16:53Z\",\"labels\":{\"app\":\"kube-controller-manager-operator\",\"endpointslice.kubernetes.io/managed-by\":\"endpointslice-controller.k8s.io\",\"kubernetes.io/service-name\":\"metrics\"},\"annotations\":{\"endpoints.kubernetes.io/last-change-trigger-time\":\"2022-05-17T07:21:34Z\"},\"ownerReferences\":[{\"apiVersion\":\"v1\",\"kind\":\"Service\",\"name\":\"metrics\",\"uid\":\"0518eed3-6152-42be-b566-0bd00a60faf8\",\"controller\":true,\"blockOwnerDeletion\":true}],\"managedFields\":[{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"discovery.k8s.io/v1\",\"time\":\"2022-05-17T07:20:02Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:addressType\":{},\"f:endpoints\":{},\"f:metadata\":{\"f:annotations\":{\".\":{},\"f:endpoints.kubernetes.io/last-change-trigger-time\":{}},\"f:generateName\":{},\"f:labels\":{\".\":{},\"f:app\":{},\"f:endpointslice.kubernetes.io/managed-by\":{},\"f:kubernetes.io/service-name\":{}},\"f:ownerReferences\":{\".\":{},\"k:{\\\"uid\\\":\\\"0518eed3-6152-42be-b566-0bd00a60faf8\\\"}\":{}}},\"f:ports\":{}}}]},\"addressType\":\"IPv4\",\"endpoints\":[{\"addresses\":[\"10.129.0.7\"],\"conditions\":{\"ready\":true,\"serving\":true,\"terminating\":false},\"targetRef\":{\"kind\":\"Pod\",\"namespace\":\"openshift-kube-controller-manager-operator\",\"name\":\"kube-controller-manager-operator-6b98b89ddd-8d4nf\",\"uid\":\"dd5139b8-e41c-4946-a31b-1a629314e844\",\"resourceVersion\":\"9038\"},\"nodeName\":\"ci-ln-qb8t3mb-72292-7s7rh-master-0\",\"zone\":\"us-central1-a\"}],\"ports\":[{\"name\":\"https\",\"protocol\":\"TCP\",\"port\":8443}]}","level":"debug","ts":"2022-05-17T09:55:08Z"}</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							View the FRR logs:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc logs -n metallb-system speaker-7m4qw -c frr</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">Started watchfrr
2022/05/17 09:55:05 ZEBRA: client 16 says hello and bids fair to announce only bgp routes vrf=0
2022/05/17 09:55:05 ZEBRA: client 31 says hello and bids fair to announce only vnc routes vrf=0
2022/05/17 09:55:05 ZEBRA: client 38 says hello and bids fair to announce only static routes vrf=0
2022/05/17 09:55:05 ZEBRA: client 43 says hello and bids fair to announce only bfd routes vrf=0
2022/05/17 09:57:25.089 BGP: Creating Default VRF, AS 64500
2022/05/17 09:57:25.090 BGP: dup addr detect enable max_moves 5 time 180 freeze disable freeze_time 0
2022/05/17 09:57:25.090 BGP: bgp_get: Registering BGP instance (null) to zebra
2022/05/17 09:57:25.090 BGP: Registering VRF 0
2022/05/17 09:57:25.091 BGP: Rx Router Id update VRF 0 Id 10.131.0.1/32
2022/05/17 09:57:25.091 BGP: RID change : vrf VRF default(0), RTR ID 10.131.0.1
2022/05/17 09:57:25.091 BGP: Rx Intf add VRF 0 IF br0
2022/05/17 09:57:25.091 BGP: Rx Intf add VRF 0 IF ens4
2022/05/17 09:57:25.091 BGP: Rx Intf address add VRF 0 IF ens4 addr 10.0.128.4/32
2022/05/17 09:57:25.091 BGP: Rx Intf address add VRF 0 IF ens4 addr fe80::c9d:84da:4d86:5618/64
2022/05/17 09:57:25.091 BGP: Rx Intf add VRF 0 IF lo
2022/05/17 09:57:25.091 BGP: Rx Intf add VRF 0 IF ovs-system
2022/05/17 09:57:25.091 BGP: Rx Intf add VRF 0 IF tun0
2022/05/17 09:57:25.091 BGP: Rx Intf address add VRF 0 IF tun0 addr 10.131.0.1/23
2022/05/17 09:57:25.091 BGP: Rx Intf address add VRF 0 IF tun0 addr fe80::40f1:d1ff:feb6:5322/64
2022/05/17 09:57:25.091 BGP: Rx Intf add VRF 0 IF veth2da49fed
2022/05/17 09:57:25.091 BGP: Rx Intf address add VRF 0 IF veth2da49fed addr fe80::24bd:d1ff:fec1:d88/64
2022/05/17 09:57:25.091 BGP: Rx Intf add VRF 0 IF veth2fa08c8c
2022/05/17 09:57:25.091 BGP: Rx Intf address add VRF 0 IF veth2fa08c8c addr fe80::6870:ff:fe96:efc8/64
2022/05/17 09:57:25.091 BGP: Rx Intf add VRF 0 IF veth41e356b7
2022/05/17 09:57:25.091 BGP: Rx Intf address add VRF 0 IF veth41e356b7 addr fe80::48ff:37ff:fede:eb4b/64
2022/05/17 09:57:25.092 BGP: Rx Intf add VRF 0 IF veth1295c6e2
2022/05/17 09:57:25.092 BGP: Rx Intf address add VRF 0 IF veth1295c6e2 addr fe80::b827:a2ff:feed:637/64
2022/05/17 09:57:25.092 BGP: Rx Intf add VRF 0 IF veth9733c6dc
2022/05/17 09:57:25.092 BGP: Rx Intf address add VRF 0 IF veth9733c6dc addr fe80::3cf4:15ff:fe11:e541/64
2022/05/17 09:57:25.092 BGP: Rx Intf add VRF 0 IF veth336680ea
2022/05/17 09:57:25.092 BGP: Rx Intf address add VRF 0 IF veth336680ea addr fe80::94b1:8bff:fe7e:488c/64
2022/05/17 09:57:25.092 BGP: Rx Intf add VRF 0 IF vetha0a907b7
2022/05/17 09:57:25.092 BGP: Rx Intf address add VRF 0 IF vetha0a907b7 addr fe80::3855:a6ff:fe73:46c3/64
2022/05/17 09:57:25.092 BGP: Rx Intf add VRF 0 IF vethf35a4398
2022/05/17 09:57:25.092 BGP: Rx Intf address add VRF 0 IF vethf35a4398 addr fe80::40ef:2fff:fe57:4c4d/64
2022/05/17 09:57:25.092 BGP: Rx Intf add VRF 0 IF vethf831b7f4
2022/05/17 09:57:25.092 BGP: Rx Intf address add VRF 0 IF vethf831b7f4 addr fe80::f0d9:89ff:fe7c:1d32/64
2022/05/17 09:57:25.092 BGP: Rx Intf add VRF 0 IF vxlan_sys_4789
2022/05/17 09:57:25.092 BGP: Rx Intf address add VRF 0 IF vxlan_sys_4789 addr fe80::80c1:82ff:fe4b:f078/64
2022/05/17 09:57:26.094 BGP: 10.0.0.1 [FSM] Timer (start timer expire).
2022/05/17 09:57:26.094 BGP: 10.0.0.1 [FSM] BGP_Start (Idle-&gt;Connect), fd -1
2022/05/17 09:57:26.094 BGP: Allocated bnc 10.0.0.1/32(0)(VRF default) peer 0x7f807f7631a0
2022/05/17 09:57:26.094 BGP: sendmsg_zebra_rnh: sending cmd ZEBRA_NEXTHOP_REGISTER for 10.0.0.1/32 (vrf VRF default)
2022/05/17 09:57:26.094 BGP: 10.0.0.1 [FSM] Waiting for NHT
2022/05/17 09:57:26.094 BGP: bgp_fsm_change_status : vrf default(0), Status: Connect established_peers 0
2022/05/17 09:57:26.094 BGP: 10.0.0.1 went from Idle to Connect
2022/05/17 09:57:26.094 BGP: 10.0.0.1 [FSM] TCP_connection_open_failed (Connect-&gt;Active), fd -1
2022/05/17 09:57:26.094 BGP: bgp_fsm_change_status : vrf default(0), Status: Active established_peers 0
2022/05/17 09:57:26.094 BGP: 10.0.0.1 went from Connect to Active
2022/05/17 09:57:26.094 ZEBRA: rnh_register msg from client bgp: hdr-&gt;length=8, type=nexthop vrf=0
2022/05/17 09:57:26.094 ZEBRA: 0: Add RNH 10.0.0.1/32 type Nexthop
2022/05/17 09:57:26.094 ZEBRA: 0:10.0.0.1/32: Evaluate RNH, type Nexthop (force)
2022/05/17 09:57:26.094 ZEBRA: 0:10.0.0.1/32: NH has become unresolved
2022/05/17 09:57:26.094 ZEBRA: 0: Client bgp registers for RNH 10.0.0.1/32 type Nexthop
2022/05/17 09:57:26.094 BGP: VRF default(0): Rcvd NH update 10.0.0.1/32(0) - metric 0/0 #nhops 0/0 flags 0x6
2022/05/17 09:57:26.094 BGP: NH update for 10.0.0.1/32(0)(VRF default) - flags 0x6 chgflags 0x0 - evaluate paths
2022/05/17 09:57:26.094 BGP: evaluate_paths: Updating peer (10.0.0.1(VRF default)) status with NHT
2022/05/17 09:57:30.081 ZEBRA: Event driven route-map update triggered
2022/05/17 09:57:30.081 ZEBRA: Event handler for route-map: 10.0.0.1-out
2022/05/17 09:57:30.081 ZEBRA: Event handler for route-map: 10.0.0.1-in
2022/05/17 09:57:31.104 ZEBRA: netlink_parse_info: netlink-listen (NS 0) type RTM_NEWNEIGH(28), len=76, seq=0, pid=0
2022/05/17 09:57:31.104 ZEBRA: 	Neighbor Entry received is not on a VLAN or a BRIDGE, ignoring
2022/05/17 09:57:31.105 ZEBRA: netlink_parse_info: netlink-listen (NS 0) type RTM_NEWNEIGH(28), len=76, seq=0, pid=0
2022/05/17 09:57:31.105 ZEBRA: 	Neighbor Entry received is not on a VLAN or a BRIDGE, ignoring</pre>

							</p></div></li></ol></div><section class="section cluster-admin" id="frr-log-levels_metallb-troubleshoot-support"><div class="titlepage"><div><div><h4 class="title">34.10.1.1. FRRouting (FRR) log levels</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following table describes the FRR logging levels.
					</p><div class="table" id="idm140587103236672"><p class="title"><strong>Table 34.9. Log levels</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 30%; " class="col_1"><!--Empty--></col><col style="width: 70%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587103231536" scope="col">Log level</th><th align="left" valign="top" id="idm140587103230448" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587103231536"> <p>
										<code class="literal cluster-admin">all</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587103230448"> <p class="cluster-admin cluster-admin">
										Supplies all logging information for all logging levels.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587103231536"> <p>
										<code class="literal cluster-admin">debug</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587103230448"> <p class="cluster-admin cluster-admin">
										Information that is diagnostically helpful to people. Set to <code class="literal cluster-admin">debug</code> to give detailed troubleshooting information.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587103231536"> <p>
										<code class="literal cluster-admin">info</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587103230448"> <p>
										Provides information that always should be logged but under normal circumstances does not require user intervention. This is the default logging level.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587103231536"> <p>
										<code class="literal cluster-admin">warn</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587103230448"> <p>
										Anything that can potentially cause inconsistent <code class="literal cluster-admin">MetalLB</code> behaviour. Usually <code class="literal cluster-admin">MetalLB</code> automatically recovers from this type of error.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587103231536"> <p>
										<code class="literal cluster-admin">error</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587103230448"> <p class="cluster-admin cluster-admin">
										Any error that is fatal to the functioning of <code class="literal cluster-admin">MetalLB</code>. These errors usually require administrator intervention to fix.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587103231536"> <p>
										<code class="literal cluster-admin">none</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587103230448"> <p>
										Turn off all logging.
									</p>
									 </td></tr></tbody></table></div></div></section></section><section class="section cluster-admin" id="nw-metallb-troubleshoot-bgp_metallb-troubleshoot-support"><div class="titlepage"><div><div><h3 class="title">34.10.2. Troubleshooting BGP issues</h3></div></div></div><p class="cluster-admin cluster-admin">
					The BGP implementation that Red Hat supports uses FRRouting (FRR) in a container in the <code class="literal cluster-admin">speaker</code> pods. As a cluster administrator, if you need to troubleshoot BGP configuration issues, you need to run commands in the FRR container.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Display the names of the <code class="literal cluster-admin">speaker</code> pods:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get -n metallb-system pods -l component=speaker</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">NAME            READY   STATUS    RESTARTS   AGE
speaker-66bth   4/4     Running   0          56m
speaker-gvfnf   4/4     Running   0          56m
...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Display the running configuration for FRR:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec -n metallb-system speaker-66bth -c frr -- vtysh -c "show running-config"</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">Building configuration...

Current configuration:
!
frr version 7.5.1_git
frr defaults traditional
hostname some-hostname
log file /etc/frr/frr.log informational
log timestamp precision 3
service integrated-vtysh-config
!
router bgp 64500  <span id="CO233-1"><!--Empty--></span><span class="callout">1</span>
 bgp router-id 10.0.1.2
 no bgp ebgp-requires-policy
 no bgp default ipv4-unicast
 no bgp network import-check
 neighbor 10.0.2.3 remote-as 64500  <span id="CO233-2"><!--Empty--></span><span class="callout">2</span>
 neighbor 10.0.2.3 bfd profile doc-example-bfd-profile-full  <span id="CO233-3"><!--Empty--></span><span class="callout">3</span>
 neighbor 10.0.2.3 timers 5 15
 neighbor 10.0.2.4 remote-as 64500  <span id="CO233-4"><!--Empty--></span><span class="callout">4</span>
 neighbor 10.0.2.4 bfd profile doc-example-bfd-profile-full  <span id="CO233-5"><!--Empty--></span><span class="callout">5</span>
 neighbor 10.0.2.4 timers 5 15
 !
 address-family ipv4 unicast
  network 203.0.113.200/30   <span id="CO233-6"><!--Empty--></span><span class="callout">6</span>
  neighbor 10.0.2.3 activate
  neighbor 10.0.2.3 route-map 10.0.2.3-in in
  neighbor 10.0.2.4 activate
  neighbor 10.0.2.4 route-map 10.0.2.4-in in
 exit-address-family
 !
 address-family ipv6 unicast
  network fc00:f853:ccd:e799::/124  <span id="CO233-7"><!--Empty--></span><span class="callout">7</span>
  neighbor 10.0.2.3 activate
  neighbor 10.0.2.3 route-map 10.0.2.3-in in
  neighbor 10.0.2.4 activate
  neighbor 10.0.2.4 route-map 10.0.2.4-in in
 exit-address-family
!
route-map 10.0.2.3-in deny 20
!
route-map 10.0.2.4-in deny 20
!
ip nht resolve-via-default
!
ipv6 nht resolve-via-default
!
line vty
!
bfd
 profile doc-example-bfd-profile-full  <span id="CO233-8"><!--Empty--></span><span class="callout">8</span>
  transmit-interval 35
  receive-interval 35
  passive-mode
  echo-mode
  echo-interval 35
  minimum-ttl 10
 !
!
end</pre>

							</p></div><p class="cluster-admin cluster-admin">
							&lt;.&gt; The <code class="literal cluster-admin">router bgp</code> section indicates the ASN for MetalLB. &lt;.&gt; Confirm that a <code class="literal cluster-admin">neighbor &lt;ip-address&gt; remote-as &lt;peer-ASN&gt;</code> line exists for each BGP peer custom resource that you added. &lt;.&gt; If you configured BFD, confirm that the BFD profile is associated with the correct BGP peer and that the BFD profile appears in the command output. &lt;.&gt; Confirm that the <code class="literal cluster-admin">network &lt;ip-address-range&gt;</code> lines match the IP address ranges that you specified in address pool custom resources that you added.
						</p></li><li class="listitem"><p class="simpara">
							Display the BGP summary:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec -n metallb-system speaker-66bth -c frr -- vtysh -c "show bgp summary"</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">IPv4 Unicast Summary:
BGP router identifier 10.0.1.2, local AS number 64500 vrf-id 0
BGP table version 1
RIB entries 1, using 192 bytes of memory
Peers 2, using 29 KiB of memory

Neighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt
10.0.2.3        4      64500       387       389        0    0    0 00:32:02            0        1  <span id="CO233-9"><!--Empty--></span><span class="callout">1</span>
10.0.2.4        4      64500         0         0        0    0    0    never       Active        0  <span id="CO233-10"><!--Empty--></span><span class="callout">2</span>

Total number of neighbors 2

IPv6 Unicast Summary:
BGP router identifier 10.0.1.2, local AS number 64500 vrf-id 0
BGP table version 1
RIB entries 1, using 192 bytes of memory
Peers 2, using 29 KiB of memory

Neighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt
10.0.2.3        4      64500       387       389        0    0    0 00:32:02 NoNeg  <span id="CO233-11"><!--Empty--></span><span class="callout">3</span>
10.0.2.4        4      64500         0         0        0    0    0    never       Active        0  <span id="CO233-12"><!--Empty--></span><span class="callout">4</span>

Total number of neighbors 2</pre>

							</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO233-1"><span class="callout">1</span></a> <a href="#CO233-9"><span class="callout">1</span></a> <a href="#CO233-11"><span class="callout">3</span></a> </dt><dd><div class="para">
									Confirm that the output includes a line for each BGP peer custom resource that you added.
								</div></dd><dt><a href="#CO233-2"><span class="callout">2</span></a> <a href="#CO233-4"><span class="callout">4</span></a> <a href="#CO233-10"><span class="callout">2</span></a> <a href="#CO233-12"><span class="callout">4</span></a> </dt><dd><div class="para">
									Output that shows <code class="literal cluster-admin">0</code> messages received and messages sent indicates a BGP peer that does not have a BGP session. Check network connectivity and the BGP configuration of the BGP peer.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Display the BGP peers that received an address pool:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec -n metallb-system speaker-66bth -c frr -- vtysh -c "show bgp ipv4 unicast 203.0.113.200/30"</pre><p class="cluster-admin cluster-admin">
							Replace <code class="literal cluster-admin">ipv4</code> with <code class="literal cluster-admin">ipv6</code> to display the BGP peers that received an IPv6 address pool. Replace <code class="literal cluster-admin">203.0.113.200/30</code> with an IPv4 or IPv6 IP address range from an address pool.
						</p><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">BGP routing table entry for 203.0.113.200/30
Paths: (1 available, best #1, table default)
  Advertised to non peer-group peers:
  10.0.2.3  &lt;.&gt;
  Local
    0.0.0.0 from 0.0.0.0 (10.0.1.2)
      Origin IGP, metric 0, weight 32768, valid, sourced, local, best (First path received)
      Last update: Mon Jan 10 19:49:07 2022</pre>

							</p></div><p class="cluster-admin cluster-admin">
							&lt;.&gt; Confirm that the output includes an IP address for a BGP peer.
						</p></li></ol></div></section><section class="section cluster-admin" id="nw-metallb-troubleshoot-bfd_metallb-troubleshoot-support"><div class="titlepage"><div><div><h3 class="title">34.10.3. Troubleshooting BFD issues</h3></div></div></div><p class="cluster-admin cluster-admin">
					The Bidirectional Forwarding Detection (BFD) implementation that Red Hat supports uses FRRouting (FRR) in a container in the <code class="literal cluster-admin">speaker</code> pods. The BFD implementation relies on BFD peers also being configured as BGP peers with an established BGP session. As a cluster administrator, if you need to troubleshoot BFD configuration issues, you need to run commands in the FRR container.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal cluster-admin">oc</code>).
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Display the names of the <code class="literal cluster-admin">speaker</code> pods:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get -n metallb-system pods -l component=speaker</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">NAME            READY   STATUS    RESTARTS   AGE
speaker-66bth   4/4     Running   0          26m
speaker-gvfnf   4/4     Running   0          26m
...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Display the BFD peers:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc exec -n metallb-system speaker-66bth -c frr -- vtysh -c "show bfd peers brief"</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">Session count: 2
SessionId  LocalAddress              PeerAddress              Status
=========  ============              ===========              ======
3909139637 10.0.1.2                  10.0.2.3                 up  &lt;.&gt;</pre>

							</p></div><p class="cluster-admin cluster-admin">
							&lt;.&gt; Confirm that the <code class="literal cluster-admin">PeerAddress</code> column includes each BFD peer. If the output does not list a BFD peer IP address that you expected the output to include, troubleshoot BGP connectivity with the peer. If the status field indicates <code class="literal cluster-admin">down</code>, check for connectivity on the links and equipment between the node and the peer. You can determine the node name for the speaker pod with a command like <code class="literal cluster-admin">oc get pods -n metallb-system speaker-66bth -o jsonpath='{.spec.nodeName}'</code>.
						</p></li></ol></div></section><section class="section cluster-admin" id="nw-metallb-metrics_metallb-troubleshoot-support"><div class="titlepage"><div><div><h3 class="title">34.10.4. MetalLB metrics for BGP and BFD</h3></div></div></div><p class="cluster-admin cluster-admin">
					OpenShift Container Platform captures the following metrics that are related to MetalLB and BGP peers and BFD profiles:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<code class="literal cluster-admin">metallb_bfd_control_packet_input</code> counts the number of BFD control packets received from each BFD peer.
						</li><li class="listitem">
							<code class="literal cluster-admin">metallb_bfd_control_packet_output</code> counts the number of BFD control packets sent to each BFD peer.
						</li><li class="listitem">
							<code class="literal cluster-admin">metallb_bfd_echo_packet_input</code> counts the number of BFD echo packets received from each BFD peer.
						</li><li class="listitem">
							<code class="literal cluster-admin">metallb_bfd_echo_packet_output</code> counts the number of BFD echo packets sent to each BFD peer.
						</li><li class="listitem">
							<code class="literal cluster-admin">metallb_bfd_session_down_events</code> counts the number of times the BFD session with a peer entered the <code class="literal cluster-admin">down</code> state.
						</li><li class="listitem">
							<code class="literal cluster-admin">metallb_bfd_session_up</code> indicates the connection state with a BFD peer. <code class="literal cluster-admin">1</code> indicates the session is <code class="literal cluster-admin">up</code> and <code class="literal cluster-admin">0</code> indicates the session is <code class="literal cluster-admin">down</code>.
						</li><li class="listitem">
							<code class="literal cluster-admin">metallb_bfd_session_up_events</code> counts the number of times the BFD session with a peer entered the <code class="literal cluster-admin">up</code> state.
						</li><li class="listitem">
							<code class="literal cluster-admin">metallb_bfd_zebra_notifications</code> counts the number of BFD Zebra notifications for each BFD peer.
						</li><li class="listitem">
							<code class="literal cluster-admin">metallb_bgp_announced_prefixes_total</code> counts the number of load balancer IP address prefixes that are advertised to BGP peers. The terms <span class="emphasis"><em><span class="cluster-admin cluster-admin">prefix</span></em></span> and <span class="emphasis"><em><span class="cluster-admin cluster-admin">aggregated route</span></em></span> have the same meaning.
						</li><li class="listitem">
							<code class="literal cluster-admin">metallb_bgp_session_up</code> indicates the connection state with a BGP peer. <code class="literal cluster-admin">1</code> indicates the session is <code class="literal cluster-admin">up</code> and <code class="literal cluster-admin">0</code> indicates the session is <code class="literal cluster-admin">down</code>.
						</li><li class="listitem">
							<code class="literal cluster-admin">metallb_bgp_updates_total</code> counts the number of BGP <code class="literal cluster-admin">update</code> messages that were sent to a BGP peer.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							See <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/monitoring/#about-querying-metrics_managing-metrics">Querying metrics</a> for information about using the monitoring dashboard.
						</li></ul></div></section><section class="section cluster-admin" id="nw-metallb-collecting-data_metallb-troubleshoot-support"><div class="titlepage"><div><div><h3 class="title">34.10.5. About collecting MetalLB data</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can use the <code class="literal cluster-admin">oc adm must-gather</code> CLI command to collect information about your cluster, your MetalLB configuration, and the MetalLB Operator. The following features and objects are associated with MetalLB and the MetalLB Operator:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							The namespace and child objects that the MetalLB Operator is deployed in
						</li><li class="listitem">
							All MetalLB Operator custom resource definitions (CRDs)
						</li></ul></div><p class="cluster-admin cluster-admin">
					The <code class="literal cluster-admin">oc adm must-gather</code> CLI command collects the following information from FRRouting (FRR) that Red Hat uses to implement BGP and BFD:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<code class="literal cluster-admin">/etc/frr/frr.conf</code>
						</li><li class="listitem">
							<code class="literal cluster-admin">/etc/frr/frr.log</code>
						</li><li class="listitem">
							<code class="literal cluster-admin">/etc/frr/daemons</code> configuration file
						</li><li class="listitem">
							<code class="literal cluster-admin">/etc/frr/vtysh.conf</code>
						</li></ul></div><p class="cluster-admin cluster-admin">
					The log and configuration files in the preceding list are collected from the <code class="literal cluster-admin">frr</code> container in each <code class="literal cluster-admin">speaker</code> pod.
				</p><p class="cluster-admin cluster-admin">
					In addition to the log and configuration files, the <code class="literal cluster-admin">oc adm must-gather</code> CLI command collects the output from the following <code class="literal cluster-admin">vtysh</code> commands:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<code class="literal cluster-admin">show running-config</code>
						</li><li class="listitem">
							<code class="literal cluster-admin">show bgp ipv4</code>
						</li><li class="listitem">
							<code class="literal cluster-admin">show bgp ipv6</code>
						</li><li class="listitem">
							<code class="literal cluster-admin">show bgp neighbor</code>
						</li><li class="listitem">
							<code class="literal cluster-admin">show bfd peer</code>
						</li></ul></div><p class="cluster-admin cluster-admin">
					No additional configuration is required when you run the <code class="literal cluster-admin">oc adm must-gather</code> CLI command.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/support/#gathering-cluster-data">Gathering data about your cluster</a>
						</li></ul></div></section></section></section><section class="chapter cluster-admin" id="associating-secondary-interfaces-metrics-to-network-attachments"><div class="titlepage"><div><div><h1 class="title">Chapter 35. Associating secondary interfaces metrics to network attachments</h1></div></div></div><section class="section cluster-admin" id="cnf-associating-secondary-interfaces-metrics-to-network-attachments_secondary-interfaces-metrics"><div class="titlepage"><div><div><h2 class="title">35.1. Extending secondary network metrics for monitoring</h2></div></div></div><p class="cluster-admin cluster-admin">
				Secondary devices, or interfaces, are used for different purposes. It is important to have a way to classify them to be able to aggregate the metrics for secondary devices with the same classification.
			</p><p class="cluster-admin cluster-admin">
				Exposed metrics contain the interface but do not specify where the interface originates. This is workable when there are no additional interfaces. However, if secondary interfaces are added, it can be difficult to use the metrics since it is hard to identify interfaces using only interface names.
			</p><p class="cluster-admin cluster-admin">
				When adding secondary interfaces, their names depend on the order in which they are added, and different secondary interfaces might belong to different networks and can be used for different purposes.
			</p><p class="cluster-admin cluster-admin">
				With <code class="literal cluster-admin">pod_network_name_info</code> it is possible to extend the current metrics with additional information that identifies the interface type. In this way, it is possible to aggregate the metrics and to add specific alarms to specific interface types.
			</p><p class="cluster-admin cluster-admin">
				The network type is generated using the name of the related <code class="literal cluster-admin">NetworkAttachmentDefinition</code>, that in turn is used to differentiate different classes of secondary networks. For example, different interfaces belonging to different networks or using different CNIs use different network attachment definition names.
			</p><section class="section cluster-admin" id="cnf-associating-secondary-interfaces-metrics-to-network-attachments-network-metrics-daemon_secondary-interfaces-metrics"><div class="titlepage"><div><div><h3 class="title">35.1.1. Network Metrics Daemon</h3></div></div></div><p class="cluster-admin cluster-admin">
					The Network Metrics Daemon is a daemon component that collects and publishes network related metrics.
				</p><p class="cluster-admin cluster-admin">
					The kubelet is already publishing network related metrics you can observe. These metrics are:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<code class="literal cluster-admin">container_network_receive_bytes_total</code>
						</li><li class="listitem">
							<code class="literal cluster-admin">container_network_receive_errors_total</code>
						</li><li class="listitem">
							<code class="literal cluster-admin">container_network_receive_packets_total</code>
						</li><li class="listitem">
							<code class="literal cluster-admin">container_network_receive_packets_dropped_total</code>
						</li><li class="listitem">
							<code class="literal cluster-admin">container_network_transmit_bytes_total</code>
						</li><li class="listitem">
							<code class="literal cluster-admin">container_network_transmit_errors_total</code>
						</li><li class="listitem">
							<code class="literal cluster-admin">container_network_transmit_packets_total</code>
						</li><li class="listitem">
							<code class="literal cluster-admin">container_network_transmit_packets_dropped_total</code>
						</li></ul></div><p class="cluster-admin cluster-admin">
					The labels in these metrics contain, among others:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Pod name
						</li><li class="listitem">
							Pod namespace
						</li><li class="listitem">
							Interface name (such as <code class="literal cluster-admin">eth0</code>)
						</li></ul></div><p class="cluster-admin cluster-admin">
					These metrics work well until new interfaces are added to the pod, for example via <a class="link" href="https://github.com/intel/multus-cni">Multus</a>, as it is not clear what the interface names refer to.
				</p><p class="cluster-admin cluster-admin">
					The interface label refers to the interface name, but it is not clear what that interface is meant for. In case of many different interfaces, it would be impossible to understand what network the metrics you are monitoring refer to.
				</p><p class="cluster-admin cluster-admin">
					This is addressed by introducing the new <code class="literal cluster-admin">pod_network_name_info</code> described in the following section.
				</p></section><section class="section cluster-admin" id="cnf-associating-secondary-interfaces-metrics-with-network-name_secondary-interfaces-metrics"><div class="titlepage"><div><div><h3 class="title">35.1.2. Metrics with network name</h3></div></div></div><p class="cluster-admin cluster-admin">
					This daemonset publishes a <code class="literal cluster-admin">pod_network_name_info</code> gauge metric, with a fixed value of <code class="literal cluster-admin">0</code>:
				</p><pre class="programlisting language-bash cluster-admin cluster-admin">pod_network_name_info{interface="net0",namespace="namespacename",network_name="nadnamespace/firstNAD",pod="podname"} 0</pre><p class="cluster-admin cluster-admin">
					The network name label is produced using the annotation added by Multus. It is the concatenation of the namespace the network attachment definition belongs to, plus the name of the network attachment definition.
				</p><p class="cluster-admin cluster-admin">
					The new metric alone does not provide much value, but combined with the network related <code class="literal cluster-admin">container_network_*</code> metrics, it offers better support for monitoring secondary networks.
				</p><p class="cluster-admin cluster-admin">
					Using a <code class="literal cluster-admin">promql</code> query like the following ones, it is possible to get a new metric containing the value and the network name retrieved from the <code class="literal cluster-admin">k8s.v1.cni.cncf.io/network-status</code> annotation:
				</p><pre class="programlisting language-bash cluster-admin cluster-admin">(container_network_receive_bytes_total) + on(namespace,pod,interface) group_left(network_name) ( pod_network_name_info )
(container_network_receive_errors_total) + on(namespace,pod,interface) group_left(network_name) ( pod_network_name_info )
(container_network_receive_packets_total) + on(namespace,pod,interface) group_left(network_name) ( pod_network_name_info )
(container_network_receive_packets_dropped_total) + on(namespace,pod,interface) group_left(network_name) ( pod_network_name_info )
(container_network_transmit_bytes_total) + on(namespace,pod,interface) group_left(network_name) ( pod_network_name_info )
(container_network_transmit_errors_total) + on(namespace,pod,interface) group_left(network_name) ( pod_network_name_info )
(container_network_transmit_packets_total) + on(namespace,pod,interface) group_left(network_name) ( pod_network_name_info )
(container_network_transmit_packets_dropped_total) + on(namespace,pod,interface) group_left(network_name)</pre></section></section></section><section class="chapter cluster-admin" id="network-observability"><div class="titlepage"><div><div><h1 class="title">Chapter 36. Network Observability</h1></div></div></div><section class="section cluster-admin" id="network-observability-operator-release-notes"><div class="titlepage"><div><div><h2 class="title">36.1. Network Observability Operator release notes</h2></div></div></div><p class="cluster-admin cluster-admin">
				The Network Observability Operator enables administrators to observe and analyze network traffic flows for OpenShift Container Platform clusters.
			</p><p class="cluster-admin cluster-admin">
				These release notes track the development of the Network Observability Operator in the OpenShift Container Platform.
			</p><p class="cluster-admin cluster-admin">
				For an overview of the Network Observability Operator, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#dependency-network-observability">About Network Observability Operator</a>.
			</p><section class="section cluster-admin" id="network-observability-operator-release-notes-1-4"><div class="titlepage"><div><div><h3 class="title">36.1.1. Network Observability Operator 1.4.0</h3></div></div></div><p class="cluster-admin cluster-admin">
					The following advisory is available for the Network Observability Operator 1.4.0:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/errata/RHSA-2023:5379">RHSA-2023:5379 Network Observability Operator 1.4.0</a>
						</li></ul></div><section class="section cluster-admin" id="network-observability-channel-removal-1.4"><div class="titlepage"><div><div><h4 class="title">36.1.1.1. Channel removal</h4></div></div></div><p class="cluster-admin cluster-admin">
						You must switch your channel from <code class="literal cluster-admin">v1.0.x</code> to <code class="literal cluster-admin">stable</code> to receive the latest Operator updates. The <code class="literal cluster-admin">v1.0.x</code> channel is now removed.
					</p></section><section class="section cluster-admin" id="network-observability-operator-1.4.0-features-enhancements"><div class="titlepage"><div><div><h4 class="title">36.1.1.2. New features and enhancements</h4></div></div></div><section class="section cluster-admin" id="network-observability-enhanced-configuration-and-ui-1.4"><div class="titlepage"><div><div><h5 class="title">36.1.1.2.1. Notable enhancements</h5></div></div></div><p class="cluster-admin cluster-admin">
							The 1.4 release of the Network Observability Operator adds improvements and new capabilities to the OpenShift Container Platform web console plugin and the Operator configuration.
						</p><h6 id="web-console-enhancements-1.4_network-observability-operator-release-notes-v0">Web console enhancements:</h6><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									In the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Query Options</span></strong></span>, the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Duplicate flows</span></strong></span> checkbox is added to choose whether or not to show duplicated flows.
								</li><li class="listitem">
									You can now filter source and destination traffic with 
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/9dbbc83270dd7de4d8fb267e9172f7f3/arrow-up-long-solid.png" width="10" alt="arrow up long solid"/></span>
									 <span class="strong strong"><strong><span class="cluster-admin cluster-admin">One-way</span></strong></span>, 
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/9dbbc83270dd7de4d8fb267e9172f7f3/arrow-up-long-solid.png" width="10" alt="arrow up long solid"/></span>
									 <span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/4f8892ede40f2626cc2be02feb6a5954/arrow-down-long-solid.png" width="10" alt="arrow down long solid"/></span>
									 <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Back-and-forth</span></strong></span>, and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Swap</span></strong></span> filters.
								</li><li class="listitem"><p class="simpara">
									The Network Observability metrics dashboards in <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Observe</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Dashboards</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">NetObserv</span></strong></span> and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">NetObserv / Health</span></strong></span> are modified as follows:
								</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem">
											The <span class="strong strong"><strong><span class="cluster-admin cluster-admin">NetObserv</span></strong></span> dashboard shows top bytes, packets sent, packets received per nodes, namespaces, and workloads. Flow graphs are removed from this dashboard.
										</li><li class="listitem">
											The <span class="strong strong"><strong><span class="cluster-admin cluster-admin">NetObserv / Health</span></strong></span> dashboard shows flows overhead as well as top flow rates per nodes, namespaces, and workloads.
										</li><li class="listitem">
											Infrastructure and Application metrics are shown in a split-view for namespaces and workloads.
										</li></ul></div></li></ul></div><p class="cluster-admin cluster-admin">
							For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-dashboards">Network Observability metrics</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-quickfilternw-observe-network-traffic">Quick filters</a>.
						</p><h6 id="configuration-enhancements-1.4_network-observability-operator-release-notes-v0">Configuration enhancements:</h6><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									You now have the option to specify different namespaces for any configured ConfigMap or Secret reference, such as in certificates configuration.
								</li><li class="listitem">
									The <code class="literal cluster-admin">spec.processor.clusterName</code> parameter is added so that the name of the cluster appears in the flows data. This is useful in a multi-cluster context. When using OpenShift Container Platform, leave empty to make it automatically determined.
								</li></ul></div><p class="cluster-admin cluster-admin">
							For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-flowcollector-view_network_observability">Flow Collector sample resource</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-flowcollector-api-specifications_network_observability">Flow Collector API Reference</a>.
						</p></section><section class="section cluster-admin" id="network-observability-without-loki-1.4"><div class="titlepage"><div><div><h5 class="title">36.1.1.2.2. Network Observability without Loki</h5></div></div></div><p class="cluster-admin cluster-admin">
							The Network Observability Operator is now functional and usable without Loki. If Loki is not installed, it can only export flows to KAFKA or IPFIX format and provide metrics in the Network Observability metrics dashboards. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-without-loki_network_observability">Network Observability without Loki</a>.
						</p></section><section class="section cluster-admin" id="network-observability-dns-tracking-1.4"><div class="titlepage"><div><div><h5 class="title">36.1.1.2.3. DNS tracking</h5></div></div></div><p class="cluster-admin cluster-admin">
							In 1.4, the Network Observability Operator makes use of eBPF tracepoint hooks to enable DNS tracking. You can monitor your network, conduct security analysis, and troubleshoot DNS issues in the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Traffic</span></strong></span> and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Overview</span></strong></span> pages in the web console.
						</p><p class="cluster-admin cluster-admin">
							For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-dns-overview_nw-observe-network-traffic">Configuring DNS tracking</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-dns-tracking_nw-observe-network-traffic">Working with DNS tracking</a>.
						</p></section><section class="section cluster-admin" id="SR-IOV-configuration-1.4"><div class="titlepage"><div><div><h5 class="title">36.1.1.2.4. SR-IOV support</h5></div></div></div><p class="cluster-admin cluster-admin">
							You can now collect traffic from a cluster with Single Root I/O Virtualization (SR-IOV) device. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-SR-IOV-config_network_observability">Configuring the monitoring of SR-IOV interface traffic</a>.
						</p></section><section class="section cluster-admin" id="IPFIX-support-1.4"><div class="titlepage"><div><div><h5 class="title">36.1.1.2.5. IPFIX exporter support</h5></div></div></div><p class="cluster-admin cluster-admin">
							You can now export eBPF-enriched network flows to the IPFIX collector. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-enriched-flows_network_observability">Export enriched network flow data</a>.
						</p></section><section class="section cluster-admin" id="network-observability-packet-drop-1.4"><div class="titlepage"><div><div><h5 class="title">36.1.1.2.6. Packet drops</h5></div></div></div><p class="cluster-admin cluster-admin">
							In the 1.4 release of the Network Observability Operator, eBPF tracepoint hooks are used to enable packet drop tracking. You can now detect and analyze the cause for packet drops and make decisions to optimize network performance. This feature is only supported in OpenShift Container Platform versions 4.13+. For more information, see <a class="link" href="#network-observability-pktdrop-overview_nw-observe-network-traffic" title="36.7.1.2.2. Packet drop tracking">Configuring packet drop tracking</a> and <a class="link" href="#network-observability-packet-drops_nw-observe-network-traffic" title="36.7.2.4. Working with packet drops">Working with packet drops</a>.
						</p></section><section class="section cluster-admin" id="s390x-architecture-support"><div class="titlepage"><div><div><h5 class="title">36.1.1.2.7. s390x architecture support</h5></div></div></div><p class="cluster-admin cluster-admin">
							Network Observability Operator can now run on <code class="literal cluster-admin">s390x</code> architecture. Previously it ran on <code class="literal cluster-admin">amd64</code>, <code class="literal cluster-admin">ppc64le</code>, or <code class="literal cluster-admin">arm64</code>.
						</p></section></section><section class="section cluster-admin" id="network-observability-operator-1.4.0-bug-fixes"><div class="titlepage"><div><div><h4 class="title">36.1.1.3. Bug fixes</h4></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Previously, the Prometheus metrics exported by Network Observability were computed out of potentially duplicated network flows. In the related dashboards, from <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Observe</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Dashboards</span></strong></span>, this could result in potentially doubled rates. Note that dashboards from the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Traffic</span></strong></span> view were not affected. Now, network flows are filtered to eliminate duplicates prior to metrics calculation, which results in correct traffic rates displayed in the dashboards. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-1131"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-1131</span></strong></span></a>)
							</li><li class="listitem">
								Previously, the Network Observability Operator agents were not able to capture traffic on network interfaces when configured with Multus or SR-IOV, non-default network namespaces. Now, all available network namespaces are recognized and used for capturing flows, allowing capturing traffic for SR-IOV. There are <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-SR-IOV-config_network_observability">configurations needed</a> for the <code class="literal cluster-admin">FlowCollector</code> and <code class="literal cluster-admin">SRIOVnetwork</code> custom resource to collect traffic. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-1283"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-1283</span></strong></span></a>)
							</li><li class="listitem">
								Previously, in the Network Observability Operator details from <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Operators</span></strong></span>, the <code class="literal cluster-admin">FlowCollector</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Status</span></strong></span> field might have reported incorrect information about the state of the deployment. The status field now shows the proper conditions with improved messages. The history of events is kept, ordered by event date. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-1224"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-1224</span></strong></span></a>)
							</li><li class="listitem">
								Previously, during spikes of network traffic load, certain eBPF pods were OOM-killed and went into a <code class="literal cluster-admin">CrashLoopBackOff</code> state. Now, the <code class="literal cluster-admin">eBPF</code> agent memory footprint is improved, so pods are not OOM-killed and entering a <code class="literal cluster-admin">CrashLoopBackOff</code> state. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-975"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-975</span></strong></span></a>)
							</li><li class="listitem">
								Previously when <code class="literal cluster-admin">processor.metrics.tls</code> was set to <code class="literal cluster-admin">PROVIDED</code> the <code class="literal cluster-admin">insecureSkipVerify</code> option value was forced to be <code class="literal cluster-admin">true</code>. Now you can set <code class="literal cluster-admin">insecureSkipVerify</code> to <code class="literal cluster-admin">true</code> or <code class="literal cluster-admin">false</code>, and provide a CA certificate if needed. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-1087">NETOBSERV-1087</a>)
							</li></ul></div></section><section class="section cluster-admin" id="network-observability-operator-1.4.0-known-issues"><div class="titlepage"><div><div><h4 class="title">36.1.1.4. Known issues</h4></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Since the 1.2.0 release of the Network Observability Operator, using Loki Operator 5.6, a Loki certificate change periodically affects the <code class="literal cluster-admin">flowlogs-pipeline</code> pods and results in dropped flows rather than flows written to Loki. The problem self-corrects after some time, but it still causes temporary flow data loss during the Loki certificate change. This issue has only been observed in large-scale environments of 120 nodes or greater. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-980"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-980</span></strong></span></a>)
							</li><li class="listitem">
								Currently, when <code class="literal cluster-admin">spec.agent.ebpf.features</code> includes DNSTracking, larger DNS packets require the <code class="literal cluster-admin">eBPF</code> agent to look for DNS header outside of the 1st socket buffer (SKB) segment. A new <code class="literal cluster-admin">eBPF</code> agent helper function needs to be implemented to support it. Currently, there is no workaround for this issue. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-1304"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-1304</span></strong></span></a>)
							</li><li class="listitem">
								Currently, when <code class="literal cluster-admin">spec.agent.ebpf.features</code> includes DNSTracking, DNS over TCP packets requires the <code class="literal cluster-admin">eBPF</code> agent to look for DNS header outside of the 1st SKB segment. A new <code class="literal cluster-admin">eBPF</code> agent helper function needs to be implemented to support it. Currently, there is no workaround for this issue. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-1245"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-1245</span></strong></span></a>)
							</li><li class="listitem">
								Currently, when using a <code class="literal cluster-admin">KAFKA</code> deployment model, if conversation tracking is configured, conversation events might be duplicated across Kafka consumers, resulting in inconsistent tracking of conversations, and incorrect volumetric data. For that reason, it is not recommended to configure conversation tracking when <code class="literal cluster-admin">deploymentModel</code> is set to <code class="literal cluster-admin">KAFKA</code>. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-926"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-926</span></strong></span></a>)
							</li><li class="listitem">
								Currently, when the <code class="literal cluster-admin">processor.metrics.server.tls.type</code> is configured to use a <code class="literal cluster-admin">PROVIDED</code> certificate, the operator enters an unsteady state that might affect its performance and resource consumption. It is recommended to not use a <code class="literal cluster-admin">PROVIDED</code> certificate until this issue is resolved, and instead using an auto-generated certificate, setting <code class="literal cluster-admin">processor.metrics.server.tls.type</code> to <code class="literal cluster-admin">AUTO</code>. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-1293)"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-1293</span></strong></span></a>
							</li></ul></div></section></section><section class="section cluster-admin" id="network-observability-operator-release-notes-1-3"><div class="titlepage"><div><div><h3 class="title">36.1.2. Network Observability Operator 1.3.0</h3></div></div></div><p class="cluster-admin cluster-admin">
					The following advisory is available for the Network Observability Operator 1.3.0:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/errata/RHSA-2023:3905">RHSA-2023:3905 Network Observability Operator 1.3.0</a>
						</li></ul></div><section class="section cluster-admin" id="network-observability-channel-deprecation"><div class="titlepage"><div><div><h4 class="title">36.1.2.1. Channel deprecation</h4></div></div></div><p class="cluster-admin cluster-admin">
						You must switch your channel from <code class="literal cluster-admin">v1.0.x</code> to <code class="literal cluster-admin">stable</code> to receive future Operator updates. The <code class="literal cluster-admin">v1.0.x</code> channel is deprecated and planned for removal in the next release.
					</p></section><section class="section cluster-admin" id="network-observability-operator-1.3.0-features-enhancements"><div class="titlepage"><div><div><h4 class="title">36.1.2.2. New features and enhancements</h4></div></div></div><section class="section cluster-admin" id="multi-tenancy-1.3"><div class="titlepage"><div><div><h5 class="title">36.1.2.2.1. Multi-tenancy in Network Observability</h5></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									System administrators can allow and restrict individual user access, or group access, to the flows stored in Loki. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-multi-tenancynetwork_observability">Multi-tenancy in Network Observability</a>.
								</li></ul></div></section><section class="section cluster-admin" id="flow-based-dashboard-1.3"><div class="titlepage"><div><div><h5 class="title">36.1.2.2.2. Flow-based metrics dashboard</h5></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									This release adds a new dashboard, which provides an overview of the network flows in your OpenShift Container Platform cluster. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-dashboards">Network Observability metrics</a>.
								</li></ul></div></section><section class="section cluster-admin" id="must-gather-1.3"><div class="titlepage"><div><div><h5 class="title">36.1.2.2.3. Troubleshooting with the must-gather tool</h5></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Information about the Network Observability Operator can now be included in the must-gather data for troubleshooting. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-must-gather_network-observability-troubleshooting">Network Observability must-gather</a>.
								</li></ul></div></section><section class="section cluster-admin" id="multi-arch-1.3"><div class="titlepage"><div><div><h5 class="title">36.1.2.2.4. Multiple architectures now supported</h5></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Network Observability Operator can now run on an <code class="literal cluster-admin">amd64</code>, <code class="literal cluster-admin">ppc64le</code>, or <code class="literal cluster-admin">arm64</code> architectures. Previously, it only ran on <code class="literal cluster-admin">amd64</code>.
								</li></ul></div></section></section><section class="section cluster-admin" id="deprecated-features-1.3"><div class="titlepage"><div><div><h4 class="title">36.1.2.3. Deprecated features</h4></div></div></div><section class="section cluster-admin" id="authToken-host"><div class="titlepage"><div><div><h5 class="title">36.1.2.3.1. Deprecated configuration parameter setting</h5></div></div></div><p class="cluster-admin cluster-admin">
							The release of Network Observability Operator 1.3 deprecates the <code class="literal cluster-admin">spec.Loki.authToken</code> <code class="literal cluster-admin">HOST</code> setting. When using the Loki Operator, you must now only use the <code class="literal cluster-admin">FORWARD</code> setting.
						</p></section></section><section class="section cluster-admin" id="network-observability-operator-1.3.0-bug-fixes"><div class="titlepage"><div><div><h4 class="title">36.1.2.4. Bug fixes</h4></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Previously, when the Operator was installed from the CLI, the <code class="literal cluster-admin">Role</code> and <code class="literal cluster-admin">RoleBinding</code> that are necessary for the Cluster Monitoring Operator to read the metrics were not installed as expected. The issue did not occur when the operator was installed from the web console. Now, either way of installing the Operator installs the required <code class="literal cluster-admin">Role</code> and <code class="literal cluster-admin">RoleBinding</code>. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-1003"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-1003</span></strong></span></a>)
							</li><li class="listitem">
								Since version 1.2, the Network Observability Operator can raise alerts when a problem occurs with the flows collection. Previously, due to a bug, the related configuration to disable alerts, <code class="literal cluster-admin">spec.processor.metrics.disableAlerts</code> was not working as expected and sometimes ineffectual. Now, this configuration is fixed so that it is possible to disable the alerts. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-976"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-976</span></strong></span></a>)
							</li><li class="listitem">
								Previously, when Network Observability was configured with <code class="literal cluster-admin">spec.loki.authToken</code> set to <code class="literal cluster-admin">DISABLED</code>, only a <code class="literal cluster-admin">kubeadmin</code> cluster administrator was able to view network flows. Other types of cluster administrators received authorization failure. Now, any cluster administrator is able to view network flows. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-972"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-972</span></strong></span></a>)
							</li><li class="listitem">
								Previously, a bug prevented users from setting <code class="literal cluster-admin">spec.consolePlugin.portNaming.enable</code> to <code class="literal cluster-admin">false</code>. Now, this setting can be set to <code class="literal cluster-admin">false</code> to disable port-to-service name translation. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-971"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-971</span></strong></span></a>)
							</li><li class="listitem">
								Previously, the metrics exposed by the console plugin were not collected by the Cluster Monitoring Operator (Prometheus), due to an incorrect configuration. Now the configuration has been fixed so that the console plugin metrics are correctly collected and accessible from the OpenShift Container Platform web console. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-765"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-765</span></strong></span></a>)
							</li><li class="listitem">
								Previously, when <code class="literal cluster-admin">processor.metrics.tls</code> was set to <code class="literal cluster-admin">AUTO</code> in the <code class="literal cluster-admin">FlowCollector</code>, the <code class="literal cluster-admin">flowlogs-pipeline servicemonitor</code> did not adapt the appropriate TLS scheme, and metrics were not visible in the web console. Now the issue is fixed for AUTO mode. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-1070"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-1070</span></strong></span></a>)
							</li><li class="listitem">
								Previously, certificate configuration, such as used for Kafka and Loki, did not allow specifying a namespace field, implying that the certificates had to be in the same namespace where Network Observability is deployed. Moreover, when using Kafka with TLS/mTLS, the user had to manually copy the certificate(s) to the privileged namespace where the <code class="literal cluster-admin">eBPF</code> agent pods are deployed and manually manage certificate updates, such as in the case of certificate rotation. Now, Network Observability setup is simplified by adding a namespace field for certificates in the <code class="literal cluster-admin">FlowCollector</code> resource. As a result, users can now install Loki or Kafka in different namespaces without needing to manually copy their certificates in the Network Observability namespace. The original certificates are watched so that the copies are automatically updated when needed. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-773"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-773</span></strong></span></a>)
							</li><li class="listitem">
								Previously, the SCTP, ICMPv4 and ICMPv6 protocols were not covered by the Network Observability agents, resulting in a less comprehensive network flows coverage. These protocols are now recognized to improve the flows coverage. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-934"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-934</span></strong></span></a>)
							</li></ul></div></section><section class="section cluster-admin" id="network-observability-operator-1.3.0-known-issues"><div class="titlepage"><div><div><h4 class="title">36.1.2.5. Known issues</h4></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								When <code class="literal cluster-admin">processor.metrics.tls</code> is set to <code class="literal cluster-admin">PROVIDED</code> in the <code class="literal cluster-admin">FlowCollector</code>, the <code class="literal cluster-admin">flowlogs-pipeline</code> <code class="literal cluster-admin">servicemonitor</code> is not adapted to the TLS scheme. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-1087"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-1087</span></strong></span></a>)
							</li><li class="listitem">
								Since the 1.2.0 release of the Network Observability Operator, using Loki Operator 5.6, a Loki certificate change periodically affects the <code class="literal cluster-admin">flowlogs-pipeline</code> pods and results in dropped flows rather than flows written to Loki. The problem self-corrects after some time, but it still causes temporary flow data loss during the Loki certificate change. This issue has only been observed in large-scale environments of 120 nodes or greater.(<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-980"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-980</span></strong></span></a>)
							</li></ul></div></section></section><section class="section cluster-admin" id="network-observability-operator-release-notes-1-2"><div class="titlepage"><div><div><h3 class="title">36.1.3. Network Observability Operator 1.2.0</h3></div></div></div><p class="cluster-admin cluster-admin">
					The following advisory is available for the Network Observability Operator 1.2.0:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/errata/RHSA-2023:1817">RHSA-2023:1817 Network Observability Operator 1.2.0</a>
						</li></ul></div><section class="section cluster-admin" id="network-observability-operator-preparing-to-update"><div class="titlepage"><div><div><h4 class="title">36.1.3.1. Preparing for the next update</h4></div></div></div><p class="cluster-admin cluster-admin">
						The subscription of an installed Operator specifies an update channel that tracks and receives updates for the Operator. Until the 1.2 release of the Network Observability Operator, the only channel available was <code class="literal cluster-admin">v1.0.x</code>. The 1.2 release of the Network Observability Operator introduces the <code class="literal cluster-admin">stable</code> update channel for tracking and receiving updates. You must switch your channel from <code class="literal cluster-admin">v1.0.x</code> to <code class="literal cluster-admin">stable</code> to receive future Operator updates. The <code class="literal cluster-admin">v1.0.x</code> channel is deprecated and planned for removal in a following release.
					</p></section><section class="section cluster-admin" id="network-observability-operator-1.2.0-features-enhancements"><div class="titlepage"><div><div><h4 class="title">36.1.3.2. New features and enhancements</h4></div></div></div><section class="section cluster-admin" id="histogram-feature-1.2"><div class="titlepage"><div><div><h5 class="title">36.1.3.2.1. Histogram in Traffic Flows view</h5></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									You can now choose to show a histogram bar chart of flows over time. The histogram enables you to visualize the history of flows without hitting the Loki query limit. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-histogram-trafficflow_nw-observe-network-traffic">Using the histogram</a>.
								</li></ul></div></section><section class="section cluster-admin" id="conversation-tracking-feature-1.2"><div class="titlepage"><div><div><h5 class="title">36.1.3.2.2. Conversation tracking</h5></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									You can now query flows by <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Log Type</span></strong></span>, which enables grouping network flows that are part of the same conversation. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-working-with-conversations_nw-observe-network-traffic">Working with conversations</a>.
								</li></ul></div></section><section class="section cluster-admin" id="health-alerts-feature-1.2"><div class="titlepage"><div><div><h5 class="title">36.1.3.2.3. Network Observability health alerts</h5></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									The Network Observability Operator now creates automatic alerts if the <code class="literal cluster-admin">flowlogs-pipeline</code> is dropping flows because of errors at the write stage or if the Loki ingestion rate limit has been reached. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-alert-dashboard_network_observability">Viewing health information</a>.
								</li></ul></div></section></section><section class="section cluster-admin" id="network-observability-operator-1.2.0-bug-fixes"><div class="titlepage"><div><div><h4 class="title">36.1.3.3. Bug fixes</h4></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Previously, after changing the <code class="literal cluster-admin">namespace</code> value in the FlowCollector spec, <code class="literal cluster-admin">eBPF</code> agent pods running in the previous namespace were not appropriately deleted. Now, the pods running in the previous namespace are appropriately deleted. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-774"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-774</span></strong></span></a>)
							</li><li class="listitem">
								Previously, after changing the <code class="literal cluster-admin">caCert.name</code> value in the FlowCollector spec (such as in Loki section), FlowLogs-Pipeline pods and Console plug-in pods were not restarted, therefore they were unaware of the configuration change. Now, the pods are restarted, so they get the configuration change. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-772"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-772</span></strong></span></a>)
							</li><li class="listitem">
								Previously, network flows between pods running on different nodes were sometimes not correctly identified as being duplicates because they are captured by different network interfaces. This resulted in over-estimated metrics displayed in the console plug-in. Now, flows are correctly identified as duplicates, and the console plug-in displays accurate metrics. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-755"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-755</span></strong></span></a>)
							</li><li class="listitem">
								The "reporter" option in the console plug-in is used to filter flows based on the observation point of either source node or destination node. Previously, this option mixed the flows regardless of the node observation point. This was due to network flows being incorrectly reported as Ingress or Egress at the node level. Now, the network flow direction reporting is correct. The "reporter" option filters for source observation point, or destination observation point, as expected. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-696"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-696</span></strong></span></a>)
							</li><li class="listitem">
								Previously, for agents configured to send flows directly to the processor as gRPC+protobuf requests, the submitted payload could be too large and is rejected by the processors' GRPC server. This occurred under very-high-load scenarios and with only some configurations of the agent. The agent logged an error message, such as: <span class="emphasis"><em><span class="cluster-admin cluster-admin">grpc: received message larger than max</span></em></span>. As a consequence, there was information loss about those flows. Now, the gRPC payload is split into several messages when the size exceeds a threshold. As a result, the server maintains connectivity. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-617"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-617</span></strong></span></a>)
							</li></ul></div></section><section class="section cluster-admin" id="network-observability-operator-1.2.0-known-issues"><div class="titlepage"><div><div><h4 class="title">36.1.3.4. Known issue</h4></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								In the 1.2.0 release of the Network Observability Operator, using Loki Operator 5.6, a Loki certificate transition periodically affects the <code class="literal cluster-admin">flowlogs-pipeline</code> pods and results in dropped flows rather than flows written to Loki. The problem self-corrects after some time, but it still causes temporary flow data loss during the Loki certificate transition. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-980"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-980</span></strong></span></a>)
							</li></ul></div></section><section class="section cluster-admin" id="network-observability-operator-1.2.0-notable-technical-changes"><div class="titlepage"><div><div><h4 class="title">36.1.3.5. Notable technical changes</h4></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Previously, you could install the Network Observability Operator using a custom namespace. This release introduces the <code class="literal cluster-admin">conversion webhook</code> which changes the <code class="literal cluster-admin">ClusterServiceVersion</code>. Because of this change, all the available namespaces are no longer listed. Additionally, to enable Operator metrics collection, namespaces that are shared with other Operators, like the <code class="literal cluster-admin">openshift-operators</code> namespace, cannot be used. Now, the Operator must be installed in the <code class="literal cluster-admin">openshift-netobserv-operator</code> namespace. You cannot automatically upgrade to the new Operator version if you previously installed the Network Observability Operator using a custom namespace. If you previously installed the Operator using a custom namespace, you must delete the instance of the Operator that was installed and re-install your operator in the <code class="literal cluster-admin">openshift-netobserv-operator</code> namespace. It is important to note that custom namespaces, such as the commonly used <code class="literal cluster-admin">netobserv</code> namespace, are still possible for the <code class="literal cluster-admin">FlowCollector</code>, Loki, Kafka, and other plug-ins. (<a class="link" href="https://issues.redhat.com/browse/NETOBSERV-907"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-907</span></strong></span></a>)(<a class="link" href="https://https://issues.redhat.com/browse/NETOBSERV-956"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">NETOBSERV-956</span></strong></span></a>)
							</li></ul></div></section></section><section class="section cluster-admin" id="network-observability-operator-release-notes-1-1"><div class="titlepage"><div><div><h3 class="title">36.1.4. Network Observability Operator 1.1.0</h3></div></div></div><p class="cluster-admin cluster-admin">
					The following advisory is available for the Network Observability Operator 1.1.0:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/errata/RHSA-2023:0786">RHSA-2023:0786 Network Observability Operator Security Advisory Update</a>
						</li></ul></div><p class="cluster-admin cluster-admin">
					The Network Observability Operator is now stable and the release channel is upgraded to <code class="literal cluster-admin">v1.1.0</code>.
				</p><section class="section cluster-admin" id="network-observability-operator-1.1.0-bug-fixes"><div class="titlepage"><div><div><h4 class="title">36.1.4.1. Bug fix</h4></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Previously, unless the Loki <code class="literal cluster-admin">authToken</code> configuration was set to <code class="literal cluster-admin">FORWARD</code> mode, authentication was no longer enforced, allowing any user who could connect to the OpenShift Container Platform console in an OpenShift Container Platform cluster to retrieve flows without authentication. Now, regardless of the Loki <code class="literal cluster-admin">authToken</code> mode, only cluster administrators can retrieve flows. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2169468"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">BZ#2169468</span></strong></span></a>)
							</li></ul></div></section></section></section><section class="section cluster-admin" id="network-observability-overview"><div class="titlepage"><div><div><h2 class="title">36.2. About Network Observability</h2></div></div></div><p class="cluster-admin cluster-admin">
				Red Hat offers cluster administrators the Network Observability Operator to observe the network traffic for OpenShift Container Platform clusters. The Network Observability Operator uses the eBPF technology to create network flows. The network flows are then enriched with OpenShift Container Platform information and stored in Loki. You can view and analyze the stored network flows information in the OpenShift Container Platform console for further insight and troubleshooting.
			</p><section class="section cluster-admin" id="dependency-network-observability"><div class="titlepage"><div><div><h3 class="title">36.2.1. Dependency of Network Observability Operator</h3></div></div></div><p class="cluster-admin cluster-admin">
					The Network Observability Operator requires the following Operators:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Loki: You must install Loki. Loki is the backend that is used to store all collected flows. It is recommended to install Loki by installing the Red Hat Loki Operator for the installation of Network Observability Operator.
						</li></ul></div></section><section class="section cluster-admin" id="optional-dependency-network-observability"><div class="titlepage"><div><div><h3 class="title">36.2.2. Optional dependencies of the Network Observability Operator</h3></div></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Grafana: You can install Grafana for using custom dashboards and querying capabilities, by using the Grafana Operator. Red Hat does not support Grafana Operator.
						</li><li class="listitem">
							Kafka: It provides scalability, resiliency and high availability in the OpenShift Container Platform cluster. It is recommended to install Kafka using the AMQ Streams operator for large scale deployments.
						</li></ul></div></section><section class="section cluster-admin" id="network-observability-operator"><div class="titlepage"><div><div><h3 class="title">36.2.3. Network Observability Operator</h3></div></div></div><p class="cluster-admin cluster-admin">
					The Network Observability Operator provides the Flow Collector API custom resource definition. A Flow Collector instance is created during installation and enables configuration of network flow collection. The Flow Collector instance deploys pods and services that form a monitoring pipeline where network flows are then collected and enriched with the Kubernetes metadata before storing in Loki. The eBPF agent, which is deployed as a <code class="literal cluster-admin">daemonset</code> object, creates the network flows.
				</p></section><section class="section cluster-admin" id="no-console-integration"><div class="titlepage"><div><div><h3 class="title">36.2.4. OpenShift Container Platform console integration</h3></div></div></div><p class="cluster-admin cluster-admin">
					OpenShift Container Platform console integration offers overview, topology view and traffic flow tables.
				</p><section class="section cluster-admin" id="network-observability-dashboards"><div class="titlepage"><div><div><h4 class="title">36.2.4.1. Network Observability metrics dashboards</h4></div></div></div><p class="cluster-admin cluster-admin">
						On the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Overview</span></strong></span> tab in the OpenShift Container Platform console, you can view the overall aggregated metrics of the network traffic flow on the cluster. You can choose to display the information by node, namespace, owner, pod, and service. Filters and display options can further refine the metrics.
					</p><p class="cluster-admin cluster-admin">
						In <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Observe</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Dashboards</span></strong></span>, the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Netobserv</span></strong></span> dashboard provides a quick overview of the network flows in your OpenShift Container Platform cluster. You can view distillations of the network traffic metrics in the following categories:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Top byte rates received per source and destination nodes</span></strong></span>
							</li><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Top byte rates received per source and destination namespaces</span></strong></span>
							</li><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Top byte rates received per source and destination workloads</span></strong></span>
							</li></ul></div><p class="cluster-admin cluster-admin">
						<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Infrastructure</span></strong></span> and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Application</span></strong></span> metrics are shown in a split-view for namespace and workloads. You can configure the <code class="literal cluster-admin">FlowCollector</code> <code class="literal cluster-admin">spec.processor.metrics</code> to add or remove metrics by changing the <code class="literal cluster-admin">ignoreTags</code> list. For more information about available tags, see the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-flowcollector-api-specifications_network_observability">Flow Collector API Reference</a>
					</p><p class="cluster-admin cluster-admin">
						Also in <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Observe</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Dashboards</span></strong></span>, the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Netobserv/Health</span></strong></span> dashboard provides metrics about the health of the Operator in the following categories.
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flows</span></strong></span>
							</li><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flows Overhead</span></strong></span>
							</li><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Top flow rates per source and destination nodes</span></strong></span>
							</li><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Top flow rates per source and destination namespaces</span></strong></span>
							</li><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Top flow rates per source and destination workloads</span></strong></span>
							</li><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Agents</span></strong></span>
							</li><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Processor</span></strong></span>
							</li><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operator</span></strong></span>
							</li></ul></div><p class="cluster-admin cluster-admin">
						<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Infrastructure</span></strong></span> and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Application</span></strong></span> metrics are shown in a split-view for namespace and workloads.
					</p></section><section class="section cluster-admin" id="network-observability-topology-views"><div class="titlepage"><div><div><h4 class="title">36.2.4.2. Network Observability topology views</h4></div></div></div><p class="cluster-admin cluster-admin">
						The OpenShift Container Platform console offers the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Topology</span></strong></span> tab which displays a graphical representation of the network flows and the amount of traffic. The topology view represents traffic between the OpenShift Container Platform components as a network graph. You can refine the graph by using the filters and display options. You can access the information for node, namespace, owner, pod, and service.
					</p></section><section class="section cluster-admin" id="traffic-flow-tables"><div class="titlepage"><div><div><h4 class="title">36.2.4.3. Traffic flow tables</h4></div></div></div><p class="cluster-admin cluster-admin">
						The traffic flow table view provides a view for raw flows, non aggregated filtering options, and configurable columns. The OpenShift Container Platform console offers the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Traffic flows</span></strong></span> tab which displays the data of the network flows and the amount of traffic.
					</p></section></section></section><section class="section cluster-admin" id="installing-network-observability-operators"><div class="titlepage"><div><div><h2 class="title">36.3. Installing the Network Observability Operator</h2></div></div></div><p class="cluster-admin cluster-admin">
				Installing Loki is a recommended prerequisite for using the Network Observability Operator. You can choose to use <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-without-loki_network_observability">Network Observability without Loki</a>, but there are some considerations for doing this, described in the previously linked section.
			</p><p class="cluster-admin cluster-admin">
				The Loki Operator integrates a gateway that implements multi-tenancy and authentication with Loki for data flow storage. The <code class="literal cluster-admin">LokiStack</code> resource manages Loki, which is a scalable, highly-available, multi-tenant log aggregation system, and a web proxy with OpenShift Container Platform authentication. The <code class="literal cluster-admin">LokiStack</code> proxy uses OpenShift Container Platform authentication to enforce multi-tenancy and facilitate the saving and indexing of data in Loki log stores.
			</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
					The Loki Operator can also be used for <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/logging/#cluster-logging-loki">Logging with the LokiStack</a>. The Network Observability Operator requires a dedicated LokiStack separate from Logging.
				</p></div></div><section class="section cluster-admin" id="network-observability-without-loki_network_observability"><div class="titlepage"><div><div><h3 class="title">36.3.1. Network Observability without Loki</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can use Network Observability without Loki by not performing the Loki installation steps in the following section and instead using exporters, such as Kafka or IPFIX. The following table compares available features with and without Loki:
				</p><div class="table" id="idm140587102564352"><p class="title"><strong>Table 36.1. Comparison of feature availability with and without Loki</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587102558320" scope="col"> </th><th align="left" valign="top" id="idm140587102557392" scope="col"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">With Loki</span></strong></span></th><th align="left" valign="top" id="idm140587102555376" scope="col"><span class="strong strong"><strong><span class="cluster-admin cluster-admin">Without Loki</span></strong></span></th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587102558320"> <p>
									<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Exporters</span></strong></span>
								</p>
								 </td><td align="left" valign="top" headers="idm140587102557392"> <p>
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/d9f8fac8d51db230a1fc79f10fa2e90f/check-solid.png" width="10" alt="check solid"/></span>

								</p>
								 </td><td align="left" valign="top" headers="idm140587102555376"> <p>
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/d9f8fac8d51db230a1fc79f10fa2e90f/check-solid.png" width="10" alt="check solid"/></span>

								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587102558320"> <p>
									<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flow-based dashboards</span></strong></span>
								</p>
								 </td><td align="left" valign="top" headers="idm140587102557392"> <p>
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/d9f8fac8d51db230a1fc79f10fa2e90f/check-solid.png" width="10" alt="check solid"/></span>

								</p>
								 </td><td align="left" valign="top" headers="idm140587102555376"> <p>
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/d9f8fac8d51db230a1fc79f10fa2e90f/check-solid.png" width="10" alt="check solid"/></span>

								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587102558320"> <p>
									<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Traffic Flow Overview, Table and Topology views</span></strong></span>
								</p>
								 </td><td align="left" valign="top" headers="idm140587102557392"> <p>
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/d9f8fac8d51db230a1fc79f10fa2e90f/check-solid.png" width="10" alt="check solid"/></span>

								</p>
								 </td><td align="left" valign="top" headers="idm140587102555376"> <p>
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/5c1f04f90b916b43683020195887c070/x-solid.png" width="10" alt="x solid"/></span>

								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587102558320"> <p>
									<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Quick Filters</span></strong></span>
								</p>
								 </td><td align="left" valign="top" headers="idm140587102557392"> <p>
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/d9f8fac8d51db230a1fc79f10fa2e90f/check-solid.png" width="10" alt="check solid"/></span>

								</p>
								 </td><td align="left" valign="top" headers="idm140587102555376"> <p>
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/5c1f04f90b916b43683020195887c070/x-solid.png" width="10" alt="x solid"/></span>

								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587102558320"> <p>
									<span class="strong strong"><strong><span class="cluster-admin cluster-admin">OpenShift Container Platform console Network Traffic tab integration</span></strong></span>
								</p>
								 </td><td align="left" valign="top" headers="idm140587102557392"> <p>
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/d9f8fac8d51db230a1fc79f10fa2e90f/check-solid.png" width="10" alt="check solid"/></span>

								</p>
								 </td><td align="left" valign="top" headers="idm140587102555376"> <p>
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/5c1f04f90b916b43683020195887c070/x-solid.png" width="10" alt="x solid"/></span>

								</p>
								 </td></tr></tbody></table></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-enriched-flows_network_observability">Export enriched network flow data</a>.
						</li></ul></div></section><section class="section cluster-admin" id="network-observability-loki-installation_network_observability"><div class="titlepage"><div><div><h3 class="title">36.3.2. Installing the Loki Operator</h3></div></div></div><p class="cluster-admin cluster-admin">
					The <a class="link" href="https://catalog.redhat.com/software/containers/openshift-logging/loki-rhel8-operator/622b46bcae289285d6fcda39">Loki Operator versions 5.7+</a> are the supported Loki Operator versions for Network Observabilty; these versions provide the ability to create a <code class="literal cluster-admin">LokiStack</code> instance using the <code class="literal cluster-admin">openshift-network</code> tenant configuration mode and provide fully-automatic, in-cluster authentication and authorization support for Network Observability. There are several ways you can install Loki. One way is by using the OpenShift Container Platform web console Operator Hub.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Supported Log Store (AWS S3, Google Cloud Storage, Azure, Swift, Minio, OpenShift Data Foundation)
						</li><li class="listitem">
							OpenShift Container Platform 4.10+
						</li><li class="listitem">
							Linux Kernel 4.18+
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							In the OpenShift Container Platform web console, click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">OperatorHub</span></strong></span>.
						</li><li class="listitem">
							Choose <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Loki Operator</span></strong></span> from the list of available Operators, and click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Install</span></strong></span>.
						</li><li class="listitem">
							Under <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installation Mode</span></strong></span>, select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">All namespaces on the cluster</span></strong></span>.
						</li></ol></div><div class="orderedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Verify that you installed the Loki Operator. Visit the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Operators</span></strong></span> page and look for <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Loki Operator</span></strong></span>.
						</li><li class="listitem">
							Verify that <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Loki Operator</span></strong></span> is listed with <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Status</span></strong></span> as <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Succeeded</span></strong></span> in all the projects.
						</li></ol></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						To uninstall Loki, refer to the uninstallation process that corresponds with the method you used to install Loki. You might have remaining <code class="literal cluster-admin">ClusterRoles</code> and <code class="literal cluster-admin">ClusterRoleBindings</code>, data stored in object store, and persistent volume that must be removed.
					</p></div></div><section class="section cluster-admin" id="network-observability-loki-secret_network_observability"><div class="titlepage"><div><div><h4 class="title">36.3.2.1. Creating a secret for Loki storage</h4></div></div></div><p class="cluster-admin cluster-admin">
						The Loki Operator supports a few log storage options, such as AWS S3, Google Cloud Storage, Azure, Swift, Minio, OpenShift Data Foundation. The following example shows how to create a secret for AWS S3 storage. The secret created in this example, <code class="literal cluster-admin">loki-s3</code>, is referenced in "Creating a LokiStack resource". You can create this secret in the web console or CLI.
					</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
								Using the web console, navigate to the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Project</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">All Projects</span></strong></span> dropdown and select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Create Project</span></strong></span>. Name the project <code class="literal cluster-admin">netobserv</code> and click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Create</span></strong></span>.
							</li><li class="listitem"><p class="simpara">
								Navigate to the Import icon, <span class="strong strong"><strong><span class="cluster-admin cluster-admin">+</span></strong></span>, in the top right corner. Paste your YAML file into the editor.
							</p><p class="cluster-admin cluster-admin">
								The following shows an example secret YAML file for S3 storage:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: v1
kind: Secret
metadata:
  name: loki-s3
  namespace: netobserv   <span id="CO234-1"><!--Empty--></span><span class="callout">1</span>
stringData:
  access_key_id: QUtJQUlPU0ZPRE5ON0VYQU1QTEUK
  access_key_secret: d0phbHJYVXRuRkVNSS9LN01ERU5HL2JQeFJmaUNZRVhBTVBMRUtFWQo=
  bucketnames: s3-bucket-name
  endpoint: https://s3.eu-central-1.amazonaws.com
  region: eu-central-1</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO234-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The installation examples in this documentation use the same namespace, <code class="literal cluster-admin">netobserv</code>, across all components. You can optionally use a different namespace for the different components
									</div></dd></dl></div></li></ol></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Once you create the secret, you should see it listed under <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Workloads</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Secrets</span></strong></span> in the web console.
							</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
								For more information about the option to use different namespaces for the separate components, see the <code class="literal cluster-admin">spec.loki.tls.caCert.namespace</code> specification in the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/network_observability/#network-observability-flowcollector-api-specifications_network_observability">Flow Collector API Reference</a> and callout number 5 in the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/network_observability/#network-observability-flowcollector-view_network_observability">Flow Collector sample resource</a>.
							</li></ul></div></section><section class="section cluster-admin" id="network-observability-lokistack-create_network_observability"><div class="titlepage"><div><div><h4 class="title">36.3.2.2. Creating a LokiStack custom resource</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can deploy a LokiStack using the web console or CLI to create a namespace, or new project.
					</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
								Navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Operators</span></strong></span>, viewing <span class="strong strong"><strong><span class="cluster-admin cluster-admin">All projects</span></strong></span> from the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Project</span></strong></span> dropdown.
							</li><li class="listitem">
								Look for <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Loki Operator</span></strong></span>. In the details, under <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Provided APIs</span></strong></span>, select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">LokiStack</span></strong></span>.
							</li><li class="listitem">
								Click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Create LokiStack</span></strong></span>.
							</li><li class="listitem"><p class="simpara">
								Ensure the following fields are specified in either <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Form View</span></strong></span> or <span class="strong strong"><strong><span class="cluster-admin cluster-admin">YAML view</span></strong></span>:
							</p><pre class="programlisting language-yaml cluster-admin cluster-admin">  apiVersion: loki.grafana.com/v1
  kind: LokiStack
  metadata:
    name: loki
    namespace: netobserv   <span id="CO235-1"><!--Empty--></span><span class="callout">1</span>
  spec:
    size: 1x.small
    storage:
      schemas:
      - version: v12
        effectiveDate: '2022-06-01'
      secret:
        name: loki-s3
        type: s3
    storageClassName: gp3  <span id="CO235-2"><!--Empty--></span><span class="callout">2</span>
    tenants:
      mode: openshift-network</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO235-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The installation examples in this documentation use the same namespace, <code class="literal cluster-admin">netobserv</code>, across all components. You can optionally use a different namespace.
									</div></dd><dt><a href="#CO235-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Use a storage class name that is available on the cluster for <code class="literal cluster-admin">ReadWriteOnce</code> access mode. You can use <code class="literal cluster-admin">oc get storageclasses</code> to see what is available on your cluster.
									</div></dd></dl></div><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
									You must not reuse the same <code class="literal cluster-admin">LokiStack</code> that is used for cluster logging.
								</p></div></div></li><li class="listitem">
								Click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Create</span></strong></span>.
							</li></ol></div><section class="section cluster-admin" id="deployment-sizing_network_observability"><div class="titlepage"><div><div><h5 class="title">36.3.2.2.1. Deployment Sizing</h5></div></div></div><p class="cluster-admin cluster-admin">
							Sizing for Loki follows the format of <code class="literal cluster-admin">N&lt;x&gt;.<span class="emphasis"><em><span class="cluster-admin cluster-admin">&lt;size&gt;</span></em></span></code> where the value <code class="literal cluster-admin">&lt;N&gt;</code> is the number of instances and <code class="literal cluster-admin">&lt;size&gt;</code> specifies performance capabilities.
						</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
								1x.extra-small is for demo purposes only, and is not supported.
							</p></div></div><div class="table" id="idm140587102377600"><p class="title"><strong>Table 36.2. Loki Sizing</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587102370608" scope="col"> </th><th align="left" valign="top" id="idm140587102369680" scope="col">1x.extra-small</th><th align="left" valign="top" id="idm140587102368592" scope="col">1x.small</th><th align="left" valign="top" id="idm140587102367504" scope="col">1x.medium</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587102370608"> <p>
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Data transfer</span></strong></span>
										</p>
										 </td><td align="left" valign="top" headers="idm140587102369680"> <p>
											Demo use only.
										</p>
										 </td><td align="left" valign="top" headers="idm140587102368592"> <p>
											500GB/day
										</p>
										 </td><td align="left" valign="top" headers="idm140587102367504"> <p>
											2TB/day
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587102370608"> <p>
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Queries per second (QPS)</span></strong></span>
										</p>
										 </td><td align="left" valign="top" headers="idm140587102369680"> <p>
											Demo use only.
										</p>
										 </td><td align="left" valign="top" headers="idm140587102368592"> <p>
											25-50 QPS at 200ms
										</p>
										 </td><td align="left" valign="top" headers="idm140587102367504"> <p>
											25-75 QPS at 200ms
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587102370608"> <p>
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Replication factor</span></strong></span>
										</p>
										 </td><td align="left" valign="top" headers="idm140587102369680"> <p>
											None
										</p>
										 </td><td align="left" valign="top" headers="idm140587102368592"> <p>
											2
										</p>
										 </td><td align="left" valign="top" headers="idm140587102367504"> <p>
											3
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587102370608"> <p>
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Total CPU requests</span></strong></span>
										</p>
										 </td><td align="left" valign="top" headers="idm140587102369680"> <p>
											5 vCPUs
										</p>
										 </td><td align="left" valign="top" headers="idm140587102368592"> <p>
											36 vCPUs
										</p>
										 </td><td align="left" valign="top" headers="idm140587102367504"> <p>
											54 vCPUs
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587102370608"> <p>
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Total Memory requests</span></strong></span>
										</p>
										 </td><td align="left" valign="top" headers="idm140587102369680"> <p>
											7.5Gi
										</p>
										 </td><td align="left" valign="top" headers="idm140587102368592"> <p>
											63Gi
										</p>
										 </td><td align="left" valign="top" headers="idm140587102367504"> <p>
											139Gi
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140587102370608"> <p>
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Total Disk requests</span></strong></span>
										</p>
										 </td><td align="left" valign="top" headers="idm140587102369680"> <p>
											150Gi
										</p>
										 </td><td align="left" valign="top" headers="idm140587102368592"> <p>
											300Gi
										</p>
										 </td><td align="left" valign="top" headers="idm140587102367504"> <p>
											450Gi
										</p>
										 </td></tr></tbody></table></div></div></section></section><section class="section cluster-admin" id="network-observability-lokistack-configuring-ingestionnetwork_observability"><div class="titlepage"><div><div><h4 class="title">36.3.2.3. LokiStack ingestion limits and health alerts</h4></div></div></div><p class="cluster-admin cluster-admin">
						The LokiStack instance comes with default settings according to the configured size. It is possible to override some of these settings, such as the ingestion and query limits. You might want to update them if you get Loki errors showing up in the Console plugin, or in <code class="literal cluster-admin">flowlogs-pipeline</code> logs. An automatic alert in the web console notifies you when these limits are reached.
					</p><p class="cluster-admin cluster-admin">
						Here is an example of configured limits:
					</p><pre class="programlisting language-yaml cluster-admin cluster-admin">spec:
  limits:
    global:
      ingestion:
        ingestionBurstSize: 40
        ingestionRate: 20
        maxGlobalStreamsPerTenant: 25000
      queries:
        maxChunksPerQuery: 2000000
        maxEntriesLimitPerQuery: 10000
        maxQuerySeries: 3000</pre><p class="cluster-admin cluster-admin">
						For more information about these settings, see the <a class="link" href="https://loki-operator.dev/docs/api.md/#loki-grafana-com-v1-IngestionLimitSpec">LokiStack API reference</a>.
					</p></section></section><section class="section cluster-admin" id="network-observability-auth-mutli-tenancy_network_observability"><div class="titlepage"><div><div><h3 class="title">36.3.3. Configuring authorization and multi-tenancy</h3></div></div></div><p class="cluster-admin cluster-admin">
					Define <code class="literal cluster-admin">ClusterRole</code> and <code class="literal cluster-admin">ClusterRoleBinding</code>. The <code class="literal cluster-admin">netobserv-reader</code> <code class="literal cluster-admin">ClusterRole</code> enables multi-tenancy and allows individual user access, or group access, to the flows stored in Loki. You can create a YAML file to define these roles.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Using the web console, click the Import icon, <span class="strong strong"><strong><span class="cluster-admin cluster-admin">+</span></strong></span>.
						</li><li class="listitem">
							Drop your YAML file into the editor and click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Create</span></strong></span>:
						</li></ol></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example ClusterRole reader yaml</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: netobserv-reader    <span id="CO236-1"><!--Empty--></span><span class="callout">1</span>
rules:
- apiGroups:
  - 'loki.grafana.com'
  resources:
  - network
  resourceNames:
  - logs
  verbs:
  - 'get'</pre>

					</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO236-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							This role can be used for multi-tenancy.
						</div></dd></dl></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example ClusterRole writer yaml</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: netobserv-writer
rules:
- apiGroups:
  - 'loki.grafana.com'
  resources:
  - network
  resourceNames:
  - logs
  verbs:
  - 'create'</pre>

					</p></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Example ClusterRoleBinding yaml</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: netobserv-writer-flp
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: netobserv-writer
subjects:
- kind: ServiceAccount
  name: flowlogs-pipeline    <span id="CO237-1"><!--Empty--></span><span class="callout">1</span>
  namespace: netobserv
- kind: ServiceAccount
  name: flowlogs-pipeline-transformer
  namespace: netobserv</pre>

					</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO237-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The <code class="literal cluster-admin">flowlogs-pipeline</code> writes to Loki. If you are using Kafka, this value is <code class="literal cluster-admin">flowlogs-pipeline-transformer</code>.
						</div></dd></dl></div></section><section class="section cluster-admin" id="network-observability-multi-tenancynetwork_observability"><div class="titlepage"><div><div><h3 class="title">36.3.4. Enabling multi-tenancy in Network Observability</h3></div></div></div><p class="cluster-admin cluster-admin">
					Multi-tenancy in the Network Observability Operator allows and restricts individual user access, or group access, to the flows stored in Loki. Access is enabled for project admins. Project admins who have limited access to some namespaces can access flows for only those namespaces.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisite</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have installed <a class="link" href="https://catalog.redhat.com/software/containers/openshift-logging/loki-rhel8-operator/622b46bcae289285d6fcda39">Loki Operator version 5.7</a>
						</li><li class="listitem">
							The <code class="literal cluster-admin">FlowCollector</code> <code class="literal cluster-admin">spec.loki.authToken</code> configuration must be set to <code class="literal cluster-admin">FORWARD</code>.
						</li><li class="listitem">
							You must be logged in as a project administrator
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Authorize reading permission to <code class="literal cluster-admin">user1</code> by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc adm policy add-cluster-role-to-user netobserv-reader user1</pre><p class="cluster-admin cluster-admin">
							Now, the data is restricted to only allowed user namespaces. For example, a user that has access to a single namespace can see all the flows internal to this namespace, as well as flows going from and to this namespace. Project admins have access to the Administrator perspective in the OpenShift Container Platform console to access the Network Flows Traffic page.
						</p></li></ol></div></section><section class="section cluster-admin" id="network-observability-kafka-option_network_observability"><div class="titlepage"><div><div><h3 class="title">36.3.5. Installing Kafka (optional)</h3></div></div></div><p class="cluster-admin cluster-admin">
					The Kafka Operator is supported for large scale environments. You can install the Kafka Operator as <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_amq_streams/2.2">Red Hat AMQ Streams</a> from the Operator Hub, just as the Loki Operator and Network Observability Operator were installed.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						To uninstall Kafka, refer to the uninstallation process that corresponds with the method you used to install.
					</p></div></div></section><section class="section cluster-admin" id="network-observability-operator-installation_network_observability"><div class="titlepage"><div><div><h3 class="title">36.3.6. Installing the Network Observability Operator</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can install the Network Observability Operator using the OpenShift Container Platform web console Operator Hub. When you install the Operator, it provides the <code class="literal cluster-admin">FlowCollector</code> custom resource definition (CRD). You can set specifications in the web console when you create the <code class="literal cluster-admin">FlowCollector</code>.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							If you choose to use Loki, install the <a class="link" href="https://catalog.redhat.com/software/containers/openshift-logging/loki-rhel8-operator/622b46bcae289285d6fcda39">Loki Operator version 5.7+</a>.
						</li><li class="listitem">
							You must have <code class="literal cluster-admin">cluster-admin</code> privileges.
						</li><li class="listitem">
							One of the following supported architectures is required: <code class="literal cluster-admin">amd64</code>, <code class="literal cluster-admin">ppc64le</code>, <code class="literal cluster-admin">arm64</code>, or <code class="literal cluster-admin">s390x</code>.
						</li><li class="listitem">
							Any CPU supported by Red Hat Enterprise Linux (RHEL) 9.
						</li><li class="listitem">
							Must be configured with OVN-Kubernetes or OpenShift SDN as the main network plugin, and optionally using secondary interfaces, such as Multus and SR-IOV.
						</li></ul></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						This documentation assumes that your <code class="literal cluster-admin">LokiStack</code> instance name is <code class="literal cluster-admin">loki</code>. Using a different name requires additional configuration.
					</p></div></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							In the OpenShift Container Platform web console, click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">OperatorHub</span></strong></span>.
						</li><li class="listitem">
							Choose <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Observability Operator</span></strong></span> from the list of available Operators in the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">OperatorHub</span></strong></span>, and click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Install</span></strong></span>.
						</li><li class="listitem">
							Select the checkbox <code class="literal cluster-admin">Enable Operator recommended cluster monitoring on this Namespace</code>.
						</li><li class="listitem"><p class="simpara">
							Navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Operators</span></strong></span>. Under Provided APIs for Network Observability, select the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flow Collector</span></strong></span> link.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Navigate to the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flow Collector</span></strong></span> tab, and click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Create FlowCollector</span></strong></span>. Make the following selections in the form view:
								</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">spec.agent.ebpf.Sampling</span></strong></span> : Specify a sampling size for flows. Lower sampling sizes will have higher impact on resource utilization. For more information, see the <code class="literal cluster-admin">FlowCollector</code> API reference, under spec.agent.ebpf.
										</li><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">spec.deploymentModel</span></strong></span>: If you are using Kafka, verify Kafka is selected.
										</li><li class="listitem"><p class="simpara">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">spec.exporters</span></strong></span>: If you are using Kafka, you can optionally send network flows to Kafka, so that they can be consumed by any processor or storage that supports Kafka input, such as Splunk, Elasticsearch, or Fluentd. To do this, set the following specifications:
										</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem">
													Set the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">type</span></strong></span> to <code class="literal cluster-admin">KAFKA</code>.
												</li><li class="listitem">
													Set the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">address</span></strong></span> as <code class="literal cluster-admin">kafka-cluster-kafka-bootstrap.netobserv</code>.
												</li><li class="listitem">
													Set the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">topic</span></strong></span> as <code class="literal cluster-admin">netobserv-flows-export</code>. The Operator exports all flows to the configured Kafka topic.
												</li><li class="listitem"><p class="simpara">
													Set the following <span class="strong strong"><strong><span class="cluster-admin cluster-admin">tls</span></strong></span> specifications:
												</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="square"><li class="listitem"><p class="simpara">
															<span class="strong strong"><strong><span class="cluster-admin cluster-admin">certFile</span></strong></span>: <code class="literal cluster-admin">service-ca.crt</code>, <span class="strong strong"><strong><span class="cluster-admin cluster-admin">name</span></strong></span>: <code class="literal cluster-admin">kafka-gateway-ca-bundle</code>, and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">type</span></strong></span>: <code class="literal cluster-admin">configmap</code>.
														</p><p class="cluster-admin cluster-admin">
															You can also configure this option at a later time by directly editing the YAML. For more information, see <span class="emphasis"><em><span class="cluster-admin cluster-admin">Export enriched network flow data</span></em></span>.
														</p></li></ul></div></li></ul></div></li><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">loki.enable</span></strong></span>: Set to <code class="literal cluster-admin">true</code>.
										</li><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">loki.url</span></strong></span>: Since authentication is specified separately, this URL needs to be updated to <code class="literal cluster-admin"><a class="link" href="https://loki-gateway-http.netobserv.svc:8080/api/logs/v1/network">https://loki-gateway-http.netobserv.svc:8080/api/logs/v1/network</a></code>. The first part of the URL, "loki", should match the name of your LokiStack.
										</li><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">loki.statusUrl</span></strong></span>: Set this to <code class="literal cluster-admin"><a class="link" href="https://loki-query-frontend-http.netobserv.svc:3100/">https://loki-query-frontend-http.netobserv.svc:3100/</a></code>. The first part of the URL, "loki", should match the name of your LokiStack.
										</li><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">loki.authToken</span></strong></span>: Select the <code class="literal cluster-admin">FORWARD</code> value.
										</li><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">tls.enable</span></strong></span>: Verify that the box is checked so it is enabled.
										</li><li class="listitem"><p class="simpara">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">statusTls</span></strong></span>: The <code class="literal cluster-admin">enable</code> value is false by default.
										</p><p class="cluster-admin cluster-admin">
											For the first part of the certificate reference names: <code class="literal cluster-admin">loki-gateway-ca-bundle</code>, <code class="literal cluster-admin">loki-ca-bundle</code>, and <code class="literal cluster-admin">loki-query-frontend-http</code>,<code class="literal cluster-admin">loki</code>, should match the name of your <code class="literal cluster-admin">LokiStack</code>.
										</p></li></ul></div></li><li class="listitem">
									Click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Create</span></strong></span>.
								</li></ol></div></li></ol></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Verification</strong></p><p>
						To confirm this was successful, when you navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Observe</span></strong></span> you should see <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Traffic</span></strong></span> listed in the options.
					</p></div><p class="cluster-admin cluster-admin">
					In the absence of <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Application Traffic</span></strong></span> within the OpenShift Container Platform cluster, default filters might show that there are "No results", which results in no visual flow. Beside the filter selections, select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Clear all filters</span></strong></span> to see the flow.
				</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
						If you installed Loki using the Loki Operator, it is advised not to use <code class="literal cluster-admin">querierUrl</code>, as it can break the console access to Loki. If you installed Loki using another type of Loki installation, this does not apply.
					</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For more information about Flow Collector specifications, see the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-flowcollector-api-specifications_network_observability">Flow Collector API Reference</a> and the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-flowcollector-view_network_observability">Flow Collector sample resource</a>.
						</li><li class="listitem">
							For more information about exporting flow data to Kafka or IPFIX for third party processing consumption, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-enriched-flows_network_observability">Export enriched network flow data</a>.
						</li></ul></div></section><section class="section cluster-admin" id="network-observability-operator-uninstall_network_observability"><div class="titlepage"><div><div><h3 class="title">36.3.7. Uninstalling the Network Observability Operator</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can uninstall the Network Observability Operator using the OpenShift Container Platform web console Operator Hub, working in the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Operators</span></strong></span> area.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Remove the <code class="literal cluster-admin">FlowCollector</code> custom resource.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem">
									Click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flow Collector</span></strong></span>, which is next to the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Observability Operator</span></strong></span> in the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Provided APIs</span></strong></span> column.
								</li><li class="listitem">
									Click the options menu 
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
									 for the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">cluster</span></strong></span> and select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Delete FlowCollector</span></strong></span>.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Uninstall the Network Observability Operator.
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem">
									Navigate back to the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Operators</span></strong></span> area.
								</li><li class="listitem">
									Click the options menu 
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
									 next to the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Observability Operator</span></strong></span> and select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Uninstall Operator</span></strong></span>.
								</li><li class="listitem">
									<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Home</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Projects</span></strong></span> and select <code class="literal cluster-admin">openshift-netobserv-operator</code>
								</li><li class="listitem">
									Navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Actions</span></strong></span> and select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Delete Project</span></strong></span>
								</li></ol></div></li><li class="listitem"><p class="simpara">
							Remove the <code class="literal cluster-admin">FlowCollector</code> custom resource definition (CRD).
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem">
									Navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Administration</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">CustomResourceDefinitions</span></strong></span>.
								</li><li class="listitem">
									Look for <span class="strong strong"><strong><span class="cluster-admin cluster-admin">FlowCollector</span></strong></span> and click the options menu 
									<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png" alt="kebab"/></span>
									 .
								</li><li class="listitem"><p class="simpara">
									Select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Delete CustomResourceDefinition</span></strong></span>.
								</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
										The Loki Operator and Kafka remain if they were installed and must be removed separately. Additionally, you might have remaining data stored in an object store, and a persistent volume that must be removed.
									</p></div></div></li></ol></div></li></ol></div></section></section><section class="section cluster-admin" id="nw-network-observability-operator"><div class="titlepage"><div><div><h2 class="title">36.4. Network Observability Operator in OpenShift Container Platform</h2></div></div></div><p class="cluster-admin cluster-admin">
				Network Observability is an OpenShift operator that deploys a monitoring pipeline to collect and enrich network traffic flows that are produced by the Network Observability eBPF agent.
			</p><section class="section cluster-admin" id="nw-network-observability-operator_nw-network-observability-operator"><div class="titlepage"><div><div><h3 class="title">36.4.1. Viewing statuses</h3></div></div></div><p class="cluster-admin cluster-admin">
					The Network Observability Operator provides the Flow Collector API. When a Flow Collector resource is created, it deploys pods and services to create and store network flows in the Loki log store, as well as to display dashboards, metrics, and flows in the OpenShift Container Platform web console.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Run the following command to view the state of <code class="literal cluster-admin">FlowCollector</code>:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get flowcollector/cluster</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">NAME      AGENT   SAMPLING (EBPF)   DEPLOYMENT MODEL   STATUS
cluster   EBPF    50                DIRECT             Ready</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check the status of pods running in the <code class="literal cluster-admin">netobserv</code> namespace by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods -n netobserv</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">NAME                              READY   STATUS    RESTARTS   AGE
flowlogs-pipeline-56hbp           1/1     Running   0          147m
flowlogs-pipeline-9plvv           1/1     Running   0          147m
flowlogs-pipeline-h5gkb           1/1     Running   0          147m
flowlogs-pipeline-hh6kf           1/1     Running   0          147m
flowlogs-pipeline-w7vv5           1/1     Running   0          147m
netobserv-plugin-cdd7dc6c-j8ggp   1/1     Running   0          147m</pre>

							</p></div></li></ol></div><p class="cluster-admin cluster-admin">
					<code class="literal cluster-admin">flowlogs-pipeline</code> pods collect flows, enriches the collected flows, then send flows to the Loki storage. <code class="literal cluster-admin">netobserv-plugin</code> pods create a visualization plugin for the OpenShift Container Platform Console.
				</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Check the status of pods running in the namespace <code class="literal cluster-admin">netobserv-privileged</code> by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods -n netobserv-privileged</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">NAME                         READY   STATUS    RESTARTS   AGE
netobserv-ebpf-agent-4lpp6   1/1     Running   0          151m
netobserv-ebpf-agent-6gbrk   1/1     Running   0          151m
netobserv-ebpf-agent-klpl9   1/1     Running   0          151m
netobserv-ebpf-agent-vrcnf   1/1     Running   0          151m
netobserv-ebpf-agent-xf5jh   1/1     Running   0          151m</pre>

							</p></div></li></ol></div><p class="cluster-admin cluster-admin">
					<code class="literal cluster-admin">netobserv-ebpf-agent</code> pods monitor network interfaces of the nodes to get flows and send them to <code class="literal cluster-admin">flowlogs-pipeline</code> pods.
				</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							If you are using a Loki Operator, check the status of pods running in the <code class="literal cluster-admin">openshift-operators-redhat</code> namespace by entering the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods -n openshift-operators-redhat</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">NAME                                                READY   STATUS    RESTARTS   AGE
loki-operator-controller-manager-5f6cff4f9d-jq25h   2/2     Running   0          18h
lokistack-compactor-0                               1/1     Running   0          18h
lokistack-distributor-654f87c5bc-qhkhv              1/1     Running   0          18h
lokistack-distributor-654f87c5bc-skxgm              1/1     Running   0          18h
lokistack-gateway-796dc6ff7-c54gz                   2/2     Running   0          18h
lokistack-index-gateway-0                           1/1     Running   0          18h
lokistack-index-gateway-1                           1/1     Running   0          18h
lokistack-ingester-0                                1/1     Running   0          18h
lokistack-ingester-1                                1/1     Running   0          18h
lokistack-ingester-2                                1/1     Running   0          18h
lokistack-querier-66747dc666-6vh5x                  1/1     Running   0          18h
lokistack-querier-66747dc666-cjr45                  1/1     Running   0          18h
lokistack-querier-66747dc666-xh8rq                  1/1     Running   0          18h
lokistack-query-frontend-85c6db4fbd-b2xfb           1/1     Running   0          18h
lokistack-query-frontend-85c6db4fbd-jm94f           1/1     Running   0          18h</pre>

							</p></div></li></ol></div></section><section class="section cluster-admin" id="network-observability-architecture_nw-network-observability-operator"><div class="titlepage"><div><div><h3 class="title">36.4.2. Network Observablity Operator architecture</h3></div></div></div><p class="cluster-admin cluster-admin">
					The Network Observability Operator provides the <code class="literal cluster-admin">FlowCollector</code> API, which is instantiated at installation and configured to reconcile the <code class="literal cluster-admin">eBPF agent</code>, the <code class="literal cluster-admin">flowlogs-pipeline</code>, and the <code class="literal cluster-admin">netobserv-plugin</code> components. Only a single <code class="literal cluster-admin">FlowCollector</code> per cluster is supported.
				</p><p class="cluster-admin cluster-admin">
					The <code class="literal cluster-admin">eBPF agent</code> runs on each cluster node with some privileges to collect network flows. The <code class="literal cluster-admin">flowlogs-pipeline</code> receives the network flows data and enriches the data with Kubernetes identifiers. If you are using Loki, the <code class="literal cluster-admin">flowlogs-pipeline</code> sends flow logs data to Loki for storing and indexing. The <code class="literal cluster-admin">netobserv-plugin</code>, which is a dynamic OpenShift Container Platform web console plugin, queries Loki to fetch network flows data. Cluster-admins can view the the data in the web console.
				</p><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/cad59d16483b417993fdc83090b51c32/network-observability-architecture.png" alt="Network Observability eBPF export architecture"/></div></div><p class="cluster-admin cluster-admin">
					If you are using the Kafka option, the eBPF agent sends the network flow data to Kafka, and the <code class="literal cluster-admin">flowlogs-pipeline</code> reads from the Kafka topic before sending to Loki, as shown in the following diagram.
				</p><div class="informalfigure"><div class="mediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/eabcab32c6e6c32d8854e72f55b00ef5/network-observability-arch-kafka-FLP.png" alt="Network Observability using Kafka"/></div></div></section><section class="section cluster-admin" id="nw-status-configuration-network-observability-operator_nw-network-observability-operator"><div class="titlepage"><div><div><h3 class="title">36.4.3. Viewing Network Observability Operator status and configuration</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can inspect the status and view the details of the <code class="literal cluster-admin">FlowCollector</code> using the <code class="literal cluster-admin">oc describe</code> command.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Run the following command to view the status and configuration of the Network Observability Operator:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc describe flowcollector/cluster</pre></li></ol></div></section></section><section class="section cluster-admin" id="configuring-network-observability-operators"><div class="titlepage"><div><div><h2 class="title">36.5. Configuring the Network Observability Operator</h2></div></div></div><p class="cluster-admin cluster-admin">
				You can update the Flow Collector API resource to configure the Network Observability Operator and its managed components. The Flow Collector is explicitly created during installation. Since this resource operates cluster-wide, only a single <code class="literal cluster-admin">FlowCollector</code> is allowed, and it has to be named <code class="literal cluster-admin">cluster</code>.
			</p><section class="section cluster-admin" id="network-observability-flowcollector-view_network_observability"><div class="titlepage"><div><div><h3 class="title">36.5.1. View the FlowCollector resource</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can view and edit YAML directly in the OpenShift Container Platform web console.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							In the web console, navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Operators</span></strong></span>.
						</li><li class="listitem">
							Under the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Provided APIs</span></strong></span> heading for the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">NetObserv Operator</span></strong></span>, select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flow Collector</span></strong></span>.
						</li><li class="listitem">
							Select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">cluster</span></strong></span> then select the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">YAML</span></strong></span> tab. There, you can modify the <code class="literal cluster-admin">FlowCollector</code> resource to configure the Network Observability operator.
						</li></ol></div><p class="cluster-admin cluster-admin">
					The following example shows a sample <code class="literal cluster-admin">FlowCollector</code> resource for OpenShift Container Platform Network Observability operator:
				</p><div id="network-observability-flowcollector-configuring-about-sample_network_observability" class="cluster-admin cluster-admin"><p class="title"><strong>Sample <code class="literal cluster-admin">FlowCollector</code> resource</strong></p><p>
						
<pre class="programlisting language-yaml">apiVersion: flows.netobserv.io/v1beta1
kind: FlowCollector
metadata:
  name: cluster
spec:
  namespace: netobserv
  deploymentModel: DIRECT
  agent:
    type: EBPF                                <span id="CO238-1"><!--Empty--></span><span class="callout">1</span>
    ebpf:
      sampling: 50                            <span id="CO238-2"><!--Empty--></span><span class="callout">2</span>
      logLevel: info
      privileged: false
      resources:
        requests:
          memory: 50Mi
          cpu: 100m
        limits:
          memory: 800Mi
  processor:
    logLevel: info
    resources:
      requests:
        memory: 100Mi
        cpu: 100m
      limits:
        memory: 800Mi
    conversationEndTimeout: 10s
    logTypes: FLOWS                            <span id="CO238-3"><!--Empty--></span><span class="callout">3</span>
    conversationHeartbeatInterval: 30s
  loki:                                       <span id="CO238-4"><!--Empty--></span><span class="callout">4</span>
    url: 'https://loki-gateway-http.netobserv.svc:8080/api/logs/v1/network'
    statusUrl: 'https://loki-query-frontend-http.netobserv.svc:3100/'
    authToken: FORWARD
    tls:
      enable: true
      caCert:
        type: configmap
        name: loki-gateway-ca-bundle
        certFile: service-ca.crt
        namespace: loki-namespace          #  <span id="CO238-5"><!--Empty--></span><span class="callout">5</span>
  consolePlugin:
    register: true
    logLevel: info
    portNaming:
      enable: true
      portNames:
        "3100": loki
    quickFilters:                             <span id="CO238-6"><!--Empty--></span><span class="callout">6</span>
    - name: Applications
      filter:
        src_namespace!: 'openshift-,netobserv'
        dst_namespace!: 'openshift-,netobserv'
      default: true
    - name: Infrastructure
      filter:
        src_namespace: 'openshift-,netobserv'
        dst_namespace: 'openshift-,netobserv'
    - name: Pods network
      filter:
        src_kind: 'Pod'
        dst_kind: 'Pod'
      default: true
    - name: Services network
      filter:
        dst_kind: 'Service'</pre>

					</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO238-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The Agent specification, <code class="literal cluster-admin">spec.agent.type</code>, must be <code class="literal cluster-admin">EBPF</code>. eBPF is the only OpenShift Container Platform supported option.
						</div></dd><dt><a href="#CO238-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							You can set the Sampling specification, <code class="literal cluster-admin">spec.agent.ebpf.sampling</code>, to manage resources. Lower sampling values might consume a large amount of computational, memory and storage resources. You can mitigate this by specifying a sampling ratio value. A value of 100 means 1 flow every 100 is sampled. A value of 0 or 1 means all flows are captured. The lower the value, the increase in returned flows and the accuracy of derived metrics. By default, eBPF sampling is set to a value of 50, so 1 flow every 50 is sampled. Note that more sampled flows also means more storage needed. It is recommend to start with default values and refine empirically, to determine which setting your cluster can manage.
						</div></dd><dt><a href="#CO238-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							The optional specifications <code class="literal cluster-admin">spec.processor.logTypes</code>, <code class="literal cluster-admin">spec.processor.conversationHeartbeatInterval</code>, and <code class="literal cluster-admin">spec.processor.conversationEndTimeout</code> can be set to enable conversation tracking. When enabled, conversation events are queryable in the web console. The values for <code class="literal cluster-admin">spec.processor.logTypes</code> are as follows: <code class="literal cluster-admin">FLOWS</code> <code class="literal cluster-admin">CONVERSATIONS</code>, <code class="literal cluster-admin">ENDED_CONVERSATIONS</code>, or <code class="literal cluster-admin">ALL</code>. Storage requirements are highest for <code class="literal cluster-admin">ALL</code> and lowest for <code class="literal cluster-admin">ENDED_CONVERSATIONS</code>.
						</div></dd><dt><a href="#CO238-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							The Loki specification, <code class="literal cluster-admin">spec.loki</code>, specifies the Loki client. The default values match the Loki install paths mentioned in the Installing the Loki Operator section. If you used another installation method for Loki, specify the appropriate client information for your install.
						</div></dd><dt><a href="#CO238-5"><span class="callout">5</span></a> </dt><dd><div class="para">
							The original certificates are copied to the Network Observability instance namespace and watched for updates. When not provided, the namespace defaults to be the same as "spec.namespace". If you chose to install Loki in a different namespace, you must specify it in the <code class="literal cluster-admin">spec.loki.tls.caCert.namespace</code> field. Similarly, the <code class="literal cluster-admin">spec.exporters.kafka.tls.caCert.namespace</code> field is available for Kafka installed in a different namespace.
						</div></dd><dt><a href="#CO238-6"><span class="callout">6</span></a> </dt><dd><div class="para">
							The <code class="literal cluster-admin">spec.quickFilters</code> specification defines filters that show up in the web console. The <code class="literal cluster-admin">Application</code> filter keys,<code class="literal cluster-admin">src_namespace</code> and <code class="literal cluster-admin">dst_namespace</code>, are negated (<code class="literal cluster-admin">!</code>), so the <code class="literal cluster-admin">Application</code> filter shows all traffic that <span class="emphasis"><em><span class="cluster-admin cluster-admin">does not</span></em></span> originate from, or have a destination to, any <code class="literal cluster-admin">openshift-</code> or <code class="literal cluster-admin">netobserv</code> namespaces. For more information, see Configuring quick filters below.
						</div></dd></dl></div><div class="_additional-resources _additional-resources"><p class="title"><strong>Additional resources</strong></p><p>
						For more information about conversation tracking, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-working-with-conversations_nw-observe-network-traffic">Working with conversations</a>.
					</p></div></section><section class="section cluster-admin" id="network-observability-flowcollector-kafka-config_network_observability"><div class="titlepage"><div><div><h3 class="title">36.5.2. Configuring the Flow Collector resource with Kafka</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can configure the <code class="literal cluster-admin">FlowCollector</code> resource to use Kafka. A Kafka instance needs to be running, and a Kafka topic dedicated to OpenShift Container Platform Network Observability must be created in that instance. For more information, refer to your Kafka documentation, such as <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.7/html/using_amq_streams_on_openshift/using-the-topic-operator-str">Kafka documentation with AMQ Streams</a>.
				</p><p class="cluster-admin cluster-admin">
					The following example shows how to modify the <code class="literal cluster-admin">FlowCollector</code> resource for OpenShift Container Platform Network Observability operator to use Kafka:
				</p><div id="network-observability-flowcollector-configuring-kafka-sample_network_observability" class="cluster-admin cluster-admin"><p class="title"><strong>Sample Kafka configuration in <code class="literal cluster-admin">FlowCollector</code> resource</strong></p><p>
						
<pre class="programlisting language-yaml">  deploymentModel: KAFKA                                    <span id="CO239-1"><!--Empty--></span><span class="callout">1</span>
  kafka:
    address: "kafka-cluster-kafka-bootstrap.netobserv"      <span id="CO239-2"><!--Empty--></span><span class="callout">2</span>
    topic: network-flows                                    <span id="CO239-3"><!--Empty--></span><span class="callout">3</span>
    tls:
      enable: false                                         <span id="CO239-4"><!--Empty--></span><span class="callout">4</span></pre>

					</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO239-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							Set <code class="literal cluster-admin">spec.deploymentModel</code> to <code class="literal cluster-admin">KAFKA</code> instead of <code class="literal cluster-admin">DIRECT</code> to enable the Kafka deployment model.
						</div></dd><dt><a href="#CO239-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							<code class="literal cluster-admin">spec.kafka.address</code> refers to the Kafka bootstrap server address. You can specify a port if needed, for instance <code class="literal cluster-admin">kafka-cluster-kafka-bootstrap.netobserv:9093</code> for using TLS on port 9093.
						</div></dd><dt><a href="#CO239-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							<code class="literal cluster-admin">spec.kafka.topic</code> should match the name of a topic created in Kafka.
						</div></dd><dt><a href="#CO239-4"><span class="callout">4</span></a> </dt><dd><div class="para">
							<code class="literal cluster-admin">spec.kafka.tls</code> can be used to encrypt all communications to and from Kafka with TLS or mTLS. When enabled, the Kafka CA certificate must be available as a ConfigMap or a Secret, both in the namespace where the <code class="literal cluster-admin">flowlogs-pipeline</code> processor component is deployed (default: <code class="literal cluster-admin">netobserv</code>) and where the eBPF agents are deployed (default: <code class="literal cluster-admin">netobserv-privileged</code>). It must be referenced with <code class="literal cluster-admin">spec.kafka.tls.caCert</code>. When using mTLS, client secrets must be available in these namespaces as well (they can be generated for instance using the AMQ Streams User Operator) and referenced with <code class="literal cluster-admin">spec.kafka.tls.userCert</code>.
						</div></dd></dl></div></section><section class="section cluster-admin" id="network-observability-enriched-flows_network_observability"><div class="titlepage"><div><div><h3 class="title">36.5.3. Export enriched network flow data</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can send network flows to Kafka, IPFIX, or both at the same time. Any processor or storage that supports Kafka or IPFIX input, such as Splunk, Elasticsearch, or Fluentd, can consume the enriched network flow data.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Your Kafka or IPFIX collector endpoint(s) are available from Network Observability <code class="literal cluster-admin">flowlogs-pipeline</code> pods.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							In the web console, navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Operators</span></strong></span>.
						</li><li class="listitem">
							Under the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Provided APIs</span></strong></span> heading for the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">NetObserv Operator</span></strong></span>, select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flow Collector</span></strong></span>.
						</li><li class="listitem">
							Select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">cluster</span></strong></span> and then select the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">YAML</span></strong></span> tab.
						</li><li class="listitem"><p class="simpara">
							Edit the <code class="literal cluster-admin">FlowCollector</code> to configure <code class="literal cluster-admin">spec.exporters</code> as follows:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: flows.netobserv.io/v1alpha1
kind: FlowCollector
metadata:
  name: cluster
spec:
  exporters:
  - type: KAFKA                         <span id="CO240-1"><!--Empty--></span><span class="callout">1</span>
      kafka:
        address: "kafka-cluster-kafka-bootstrap.netobserv"
        topic: netobserv-flows-export   <span id="CO240-2"><!--Empty--></span><span class="callout">2</span>
        tls:
          enable: false                 <span id="CO240-3"><!--Empty--></span><span class="callout">3</span>
  - type: IPFIX                         <span id="CO240-4"><!--Empty--></span><span class="callout">4</span>
      ipfix:
        targetHost: "ipfix-collector.ipfix.svc.cluster.local"
        targetPort: 4739
        transport: tcp or udp           <span id="CO240-5"><!--Empty--></span><span class="callout">5</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO240-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The Network Observability Operator exports all flows to the configured Kafka topic.
								</div></dd><dt><a href="#CO240-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									You can encrypt all communications to and from Kafka with SSL/TLS or mTLS. When enabled, the Kafka CA certificate must be available as a ConfigMap or a Secret, both in the namespace where the <code class="literal cluster-admin">flowlogs-pipeline</code> processor component is deployed (default: netobserv). It must be referenced with <code class="literal cluster-admin">spec.exporters.tls.caCert</code>. When using mTLS, client secrets must be available in these namespaces as well (they can be generated for instance using the AMQ Streams User Operator) and referenced with <code class="literal cluster-admin">spec.exporters.tls.userCert</code>.
								</div></dd><dt><a href="#CO240-1"><span class="callout">1</span></a> <a href="#CO240-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									You can export flows to IPFIX instead of or in conjunction with exporting flows to Kafka.
								</div></dd><dt><a href="#CO240-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									You have the option to specify transport. The default value is <code class="literal cluster-admin">tcp</code> but you can also specify <code class="literal cluster-admin">udp</code>.
								</div></dd></dl></div></li><li class="listitem">
							After configuration, network flows data can be sent to an available output in a JSON format. For more information, see <span class="emphasis"><em><span class="cluster-admin cluster-admin">Network flows format reference</span></em></span>.
						</li></ol></div><div class="_additional-resources _additional-resources"><p class="title"><strong>Additional resources</strong></p><p>
						For more information about specifying flow format, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-flows-format_json_reference">Network flows format reference</a>.
					</p></div></section><section class="section cluster-admin" id="network-observability-config-FLP-sampling_network_observability"><div class="titlepage"><div><div><h3 class="title">36.5.4. Updating the Flow Collector resource</h3></div></div></div><p class="cluster-admin cluster-admin">
					As an alternative to editing YAML in the OpenShift Container Platform web console, you can configure specifications, such as eBPF sampling, by patching the <code class="literal cluster-admin">flowcollector</code> custom resource (CR):
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Run the following command to patch the <code class="literal cluster-admin">flowcollector</code> CR and update the <code class="literal cluster-admin">spec.agent.ebpf.sampling</code> value:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc patch flowcollector cluster --type=json -p "[{"op": "replace", "path": "/spec/agent/ebpf/sampling", "value": &lt;new value&gt;}] -n netobserv"</pre></li></ol></div></section><section class="section cluster-admin" id="network-observability-config-quick-filters_network_observability"><div class="titlepage"><div><div><h3 class="title">36.5.5. Configuring quick filters</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can modify the filters in the <code class="literal cluster-admin">FlowCollector</code> resource. Exact matches are possible using double-quotes around values. Otherwise, partial matches are used for textual values. The bang (!) character, placed at the end of a key, means negation. See the sample <code class="literal cluster-admin">FlowCollector</code> resource for more context about modifying the YAML.
				</p><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						The filter matching types "all of" or "any of" is a UI setting that the users can modify from the query options. It is not part of this resource configuration.
					</p></div></div><p class="cluster-admin cluster-admin">
					Here is a list of all available filter keys:
				</p><div class="table" id="idm140587101866960"><p class="title"><strong>Table 36.3. Filter keys</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 9%; " class="col_1"><!--Empty--></col><col style="width: 9%; " class="col_2"><!--Empty--></col><col style="width: 9%; " class="col_3"><!--Empty--></col><col style="width: 73%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587101859968" scope="col">Universal*</th><th align="left" valign="top" id="idm140587101858880" scope="col">Source</th><th align="left" valign="top" id="idm140587101857792" scope="col">Destination</th><th align="left" valign="top" id="idm140587101856704" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587101859968"> <p>
									namespace
								</p>
								 </td><td align="left" valign="top" headers="idm140587101858880"> <p>
									<code class="literal cluster-admin">src_namespace</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587101857792"> <p>
									<code class="literal cluster-admin">dst_namespace</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587101856704"> <p class="cluster-admin cluster-admin">
									Filter traffic related to a specific namespace.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587101859968"> <p>
									name
								</p>
								 </td><td align="left" valign="top" headers="idm140587101858880"> <p>
									<code class="literal cluster-admin">src_name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587101857792"> <p>
									<code class="literal cluster-admin">dst_name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587101856704"> <p class="cluster-admin cluster-admin">
									Filter traffic related to a given leaf resource name, such as a specific pod, service, or node (for host-network traffic).
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587101859968"> <p>
									kind
								</p>
								 </td><td align="left" valign="top" headers="idm140587101858880"> <p>
									<code class="literal cluster-admin">src_kind</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587101857792"> <p>
									<code class="literal cluster-admin">dst_kind</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587101856704"> <p class="cluster-admin cluster-admin">
									Filter traffic related to a given resource kind. The resource kinds include the leaf resource (Pod, Service or Node), or the owner resource (Deployment and StatefulSet).
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587101859968"> <p>
									owner_name
								</p>
								 </td><td align="left" valign="top" headers="idm140587101858880"> <p>
									<code class="literal cluster-admin">src_owner_name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587101857792"> <p>
									<code class="literal cluster-admin">dst_owner_name</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587101856704"> <p class="cluster-admin cluster-admin">
									Filter traffic related to a given resource owner; that is, a workload or a set of pods. For example, it can be a Deployment name, a StatefulSet name, etc.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587101859968"> <p>
									resource
								</p>
								 </td><td align="left" valign="top" headers="idm140587101858880"> <p>
									<code class="literal cluster-admin">src_resource</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587101857792"> <p>
									<code class="literal cluster-admin">dst_resource</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587101856704"> <p class="cluster-admin cluster-admin">
									Filter traffic related to a specific resource that is denoted by its canonical name, that identifies it uniquely. The canonical notation is <code class="literal cluster-admin">kind.namespace.name</code> for namespaced kinds, or <code class="literal cluster-admin">node.name</code> for nodes. For example, <code class="literal cluster-admin">Deployment.my-namespace.my-web-server</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587101859968"> <p>
									address
								</p>
								 </td><td align="left" valign="top" headers="idm140587101858880"> <p>
									<code class="literal cluster-admin">src_address</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587101857792"> <p>
									<code class="literal cluster-admin">dst_address</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587101856704"> <p class="cluster-admin cluster-admin">
									Filter traffic related to an IP address. IPv4 and IPv6 are supported. CIDR ranges are also supported.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587101859968"> <p>
									mac
								</p>
								 </td><td align="left" valign="top" headers="idm140587101858880"> <p>
									<code class="literal cluster-admin">src_mac</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587101857792"> <p>
									<code class="literal cluster-admin">dst_mac</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587101856704"> <p class="cluster-admin cluster-admin">
									Filter traffic related to a MAC address.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587101859968"> <p>
									port
								</p>
								 </td><td align="left" valign="top" headers="idm140587101858880"> <p>
									<code class="literal cluster-admin">src_port</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587101857792"> <p>
									<code class="literal cluster-admin">dst_port</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587101856704"> <p class="cluster-admin cluster-admin">
									Filter traffic related to a specific port.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587101859968"> <p>
									host_address
								</p>
								 </td><td align="left" valign="top" headers="idm140587101858880"> <p>
									<code class="literal cluster-admin">src_host_address</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587101857792"> <p>
									<code class="literal cluster-admin">dst_host_address</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587101856704"> <p class="cluster-admin cluster-admin">
									Filter traffic related to the host IP address where the pods are running.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587101859968"> <p>
									protocol
								</p>
								 </td><td align="left" valign="top" headers="idm140587101858880"> <p>
									N/A
								</p>
								 </td><td align="left" valign="top" headers="idm140587101857792"> <p>
									N/A
								</p>
								 </td><td align="left" valign="top" headers="idm140587101856704"> <p class="cluster-admin cluster-admin">
									Filter traffic related to a protocol, such as TCP or UDP.
								</p>
								 </td></tr></tbody></table></div></div><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Universal keys filter for any of source or destination. For example, filtering <code class="literal cluster-admin">name: 'my-pod'</code> means all traffic from <code class="literal cluster-admin">my-pod</code> and all traffic to <code class="literal cluster-admin">my-pod</code>, regardless of the matching type used, whether <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Match all</span></strong></span> or <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Match any</span></strong></span>.
						</li></ul></div></section><section class="section cluster-admin" id="network-observability-SR-IOV-config_network_observability"><div class="titlepage"><div><div><h3 class="title">36.5.6. Configuring monitoring for SR-IOV interface traffic</h3></div></div></div><p class="cluster-admin cluster-admin">
					In order to collect traffic from a cluster with a Single Root I/O Virtualization (SR-IOV) device, you must set the <code class="literal cluster-admin">FlowCollector</code> <code class="literal cluster-admin">spec.agent.ebpf.privileged</code> field to <code class="literal cluster-admin">true</code>. Then, the eBPF agent monitors other network namespaces in addition to the host network namespaces, which are monitored by default. When a pod with a virtual functions (VF) interface is created, a new network namespace is created. With <code class="literal cluster-admin">SRIOVNetwork</code> policy <code class="literal cluster-admin">IPAM</code> configurations specified, the VF interface is migrated from the host network namespace to the pod network namespace.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							Access to an OpenShift Container Platform cluster with a SR-IOV device.
						</li><li class="listitem">
							The <code class="literal cluster-admin">SRIOVNetwork</code> custom resource (CR) <code class="literal cluster-admin">spec.ipam</code> configuration must be set with an IP address from the range that the interface lists or from other plugins.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							In the web console, navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Operators</span></strong></span>.
						</li><li class="listitem">
							Under the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Provided APIs</span></strong></span> heading for the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">NetObserv Operator</span></strong></span>, select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flow Collector</span></strong></span>.
						</li><li class="listitem">
							Select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">cluster</span></strong></span> and then select the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">YAML</span></strong></span> tab.
						</li><li class="listitem"><p class="simpara">
							Configure the <code class="literal cluster-admin">FlowCollector</code> custom resource. A sample configuration is as follows:
						</p><div id="network-observability-flowcollector-configuring-SRIOV-monitoringnetwork_observability" class="cluster-admin cluster-admin"><p class="title"><strong>Configure <code class="literal cluster-admin">FlowCollector</code> for SR-IOV monitoring</strong></p><p>
								
<pre class="programlisting language-yaml">apiVersion: flows.netobserv.io/v1alpha1
kind: FlowCollector
metadata:
  name: cluster
spec:
  namespace: netobserv
  deploymentModel: DIRECT
  agent:
    type: EBPF
    ebpf:
      privileged: true   <span id="CO241-1"><!--Empty--></span><span class="callout">1</span></pre>

							</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO241-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The <code class="literal cluster-admin">spec.agent.ebpf.privileged</code> field value must be set to <code class="literal cluster-admin">true</code> to enable SR-IOV monitoring.
								</div></dd></dl></div></li></ol></div><div class="_additional-resources _additional-resources"><p class="title"><strong>Additional resources</strong></p><p>
						For more information about creating the <code class="literal cluster-admin">SriovNetwork</code> custom resource, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#cnf-creating-an-additional-sriov-network-with-vrf-plug-in_configuring-sriov-device">Creating an additional SR-IOV network attachment with the CNI VRF plugin</a>.
					</p></div></section><section class="section cluster-admin" id="network-observability-resource-recommendations_network_observability"><div class="titlepage"><div><div><h3 class="title">36.5.7. Resource management and performance considerations</h3></div></div></div><p class="cluster-admin cluster-admin">
					The amount of resources required by Network Observability depends on the size of your cluster and your requirements for the cluster to ingest and store observability data. To manage resources and set performance criteria for your cluster, consider configuring the following settings. Configuring these settings might meet your optimal setup and observability needs.
				</p><p class="cluster-admin cluster-admin">
					The following settings can help you manage resources and performance from the outset:
				</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">eBPF Sampling</span></dt><dd>
								You can set the Sampling specification, <code class="literal cluster-admin">spec.agent.ebpf.sampling</code>, to manage resources. Smaller sampling values might consume a large amount of computational, memory and storage resources. You can mitigate this by specifying a sampling ratio value. A value of <code class="literal cluster-admin">100</code> means 1 flow every 100 is sampled. A value of <code class="literal cluster-admin">0</code> or <code class="literal cluster-admin">1</code> means all flows are captured. Smaller values result in an increase in returned flows and the accuracy of derived metrics. By default, eBPF sampling is set to a value of 50, so 1 flow every 50 is sampled. Note that more sampled flows also means more storage needed. Consider starting with the default values and refine empirically, in order to determine which setting your cluster can manage.
							</dd><dt><span class="term">Restricting or excluding interfaces</span></dt><dd>
								Reduce the overall observed traffic by setting the values for <code class="literal cluster-admin">spec.agent.ebpf.interfaces</code> and <code class="literal cluster-admin">spec.agent.ebpf.excludeInterfaces</code>. By default, the agent fetches all the interfaces in the system, except the ones listed in <code class="literal cluster-admin">excludeInterfaces</code> and <code class="literal cluster-admin">lo</code> (local interface). Note that the interface names might vary according to the Container Network Interface (CNI) used.
							</dd></dl></div><p class="cluster-admin cluster-admin">
					The following settings can be used to fine-tune performance after the Network Observability has been running for a while:
				</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Resource requirements and limits</span></dt><dd>
								Adapt the resource requirements and limits to the load and memory usage you expect on your cluster by using the <code class="literal cluster-admin">spec.agent.ebpf.resources</code> and <code class="literal cluster-admin">spec.processor.resources</code> specifications. The default limits of 800MB might be sufficient for most medium-sized clusters.
							</dd><dt><span class="term">Cache max flows timeout</span></dt><dd>
								Control how often flows are reported by the agents by using the eBPF agent’s <code class="literal cluster-admin">spec.agent.ebpf.cacheMaxFlows</code> and <code class="literal cluster-admin">spec.agent.ebpf.cacheActiveTimeout</code> specifications. A larger value results in less traffic being generated by the agents, which correlates with a lower CPU load. However, a larger value leads to a slightly higher memory consumption, and might generate more latency in the flow collection.
							</dd></dl></div><section class="section cluster-admin" id="network-observability-resources-table_network_observability"><div class="titlepage"><div><div><h4 class="title">36.5.7.1. Resource considerations</h4></div></div></div><p class="cluster-admin cluster-admin">
						The following table outlines examples of resource considerations for clusters with certain workload sizes.
					</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
							The examples outlined in the table demonstrate scenarios that are tailored to specific workloads. Consider each example only as a baseline from which adjustments can be made to accommodate your workload needs.
						</p></div></div><div class="table" id="idm140587101690672"><p class="title"><strong>Table 36.4. Resource recommendations</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 20%; " class="col_2"><!--Empty--></col><col style="width: 20%; " class="col_3"><!--Empty--></col><col style="width: 20%; " class="col_4"><!--Empty--></col><col style="width: 20%; " class="col_5"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587101682736" scope="col"> </th><th align="left" valign="top" id="idm140587101681808" scope="col">Extra small (10 nodes)</th><th align="left" valign="top" id="idm140587101680720" scope="col">Small (25 nodes)</th><th align="left" valign="top" id="idm140587101679632" scope="col">Medium (65 nodes) <sup>[2]</sup></th><th align="left" valign="top" id="idm140587101677920" scope="col">Large (120 nodes) <sup>[2]</sup></th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587101682736"> <p>
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Worker Node vCPU and memory</span></strong></span>
									</p>
									 </td><td align="left" valign="top" headers="idm140587101681808"> <p>
										4 vCPUs| 16GiB mem <sup>[1]</sup>
									</p>
									 </td><td align="left" valign="top" headers="idm140587101680720"> <p>
										16 vCPUs| 64GiB mem <sup>[1]</sup>
									</p>
									 </td><td align="left" valign="top" headers="idm140587101679632"> <p>
										16 vCPUs| 64GiB mem <sup>[1]</sup>
									</p>
									 </td><td align="left" valign="top" headers="idm140587101677920"> <p>
										16 vCPUs| 64GiB Mem <sup>[1]</sup>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587101682736"> <p>
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">LokiStack size</span></strong></span>
									</p>
									 </td><td align="left" valign="top" headers="idm140587101681808"> <p>
										<code class="literal cluster-admin">1x.extra-small</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587101680720"> <p>
										<code class="literal cluster-admin">1x.small</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587101679632"> <p>
										<code class="literal cluster-admin">1x.small</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587101677920"> <p>
										<code class="literal cluster-admin">1x.medium</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587101682736"> <p>
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Observability controller memory limit</span></strong></span>
									</p>
									 </td><td align="left" valign="top" headers="idm140587101681808"> <p>
										400Mi (default)
									</p>
									 </td><td align="left" valign="top" headers="idm140587101680720"> <p>
										400Mi (default)
									</p>
									 </td><td align="left" valign="top" headers="idm140587101679632"> <p>
										400Mi (default)
									</p>
									 </td><td align="left" valign="top" headers="idm140587101677920"> <p>
										800Mi
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587101682736"> <p>
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">eBPF sampling rate</span></strong></span>
									</p>
									 </td><td align="left" valign="top" headers="idm140587101681808"> <p>
										50 (default)
									</p>
									 </td><td align="left" valign="top" headers="idm140587101680720"> <p>
										50 (default)
									</p>
									 </td><td align="left" valign="top" headers="idm140587101679632"> <p>
										50 (default)
									</p>
									 </td><td align="left" valign="top" headers="idm140587101677920"> <p>
										50 (default)
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587101682736"> <p>
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">eBPF memory limit</span></strong></span>
									</p>
									 </td><td align="left" valign="top" headers="idm140587101681808"> <p>
										800Mi (default)
									</p>
									 </td><td align="left" valign="top" headers="idm140587101680720"> <p>
										800Mi (default)
									</p>
									 </td><td align="left" valign="top" headers="idm140587101679632"> <p>
										2000Mi
									</p>
									 </td><td align="left" valign="top" headers="idm140587101677920"> <p>
										800Mi (default)
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587101682736"> <p>
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">FLP memory limit</span></strong></span>
									</p>
									 </td><td align="left" valign="top" headers="idm140587101681808"> <p>
										800Mi (default)
									</p>
									 </td><td align="left" valign="top" headers="idm140587101680720"> <p>
										800Mi (default)
									</p>
									 </td><td align="left" valign="top" headers="idm140587101679632"> <p>
										800Mi (default)
									</p>
									 </td><td align="left" valign="top" headers="idm140587101677920"> <p>
										800Mi (default)
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587101682736"> <p>
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">FLP Kafka partitions</span></strong></span>
									</p>
									 </td><td align="left" valign="top" headers="idm140587101681808"> <p>
										N/A
									</p>
									 </td><td align="left" valign="top" headers="idm140587101680720"> <p>
										48
									</p>
									 </td><td align="left" valign="top" headers="idm140587101679632"> <p>
										48
									</p>
									 </td><td align="left" valign="top" headers="idm140587101677920"> <p>
										48
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587101682736"> <p>
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Kafka consumer replicas</span></strong></span>
									</p>
									 </td><td align="left" valign="top" headers="idm140587101681808"> <p>
										N/A
									</p>
									 </td><td align="left" valign="top" headers="idm140587101680720"> <p>
										24
									</p>
									 </td><td align="left" valign="top" headers="idm140587101679632"> <p>
										24
									</p>
									 </td><td align="left" valign="top" headers="idm140587101677920"> <p>
										24
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587101682736"> <p>
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Kafka brokers</span></strong></span>
									</p>
									 </td><td align="left" valign="top" headers="idm140587101681808"> <p>
										N/A
									</p>
									 </td><td align="left" valign="top" headers="idm140587101680720"> <p>
										3 (default)
									</p>
									 </td><td align="left" valign="top" headers="idm140587101679632"> <p>
										3 (default)
									</p>
									 </td><td align="left" valign="top" headers="idm140587101677920"> <p>
										3 (default)
									</p>
									 </td></tr></tbody></table></div></div><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
								Tested with AWS M6i instances.
							</li><li class="listitem">
								In addition to this worker and its controller, 3 infra nodes (size <code class="literal cluster-admin">M6i.12xlarge</code>) and 1 workload node (size <code class="literal cluster-admin">M6i.8xlarge</code>) were tested.
							</li></ol></div></section></section></section><section class="section cluster-admin" id="network-observability-network-policy"><div class="titlepage"><div><div><h2 class="title">36.6. Network Policy</h2></div></div></div><p class="cluster-admin cluster-admin">
				As a user with the <code class="literal cluster-admin">admin</code> role, you can create a network policy for the <code class="literal cluster-admin">netobserv</code> namespace.
			</p><section class="section cluster-admin" id="network-observability-network-policy_network_observability"><div class="titlepage"><div><div><h3 class="title">36.6.1. Creating a network policy for Network Observability</h3></div></div></div><p class="cluster-admin cluster-admin">
					You might need to create a network policy to secure ingress traffic to the <code class="literal cluster-admin">netobserv</code> namespace. In the web console, you can create a network policy using the form view.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Networking</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">NetworkPolicies</span></strong></span>.
						</li><li class="listitem">
							Select the <code class="literal cluster-admin">netobserv</code> project from the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Project</span></strong></span> dropdown menu.
						</li><li class="listitem">
							Name the policy. For this example, the policy name is <code class="literal cluster-admin">allow-ingress</code>.
						</li><li class="listitem">
							Click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Add ingress rule</span></strong></span> three times to create three ingress rules.
						</li><li class="listitem"><p class="simpara">
							Specify the following in the form:
						</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem"><p class="simpara">
									Make the following specifications for the first <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Ingress rule</span></strong></span>:
								</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="i"><li class="listitem">
											From the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Add allowed source</span></strong></span> dropdown menu, select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Allow pods from the same namespace</span></strong></span>.
										</li></ol></div></li><li class="listitem"><p class="simpara">
									Make the following specifications for the second <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Ingress rule</span></strong></span>:
								</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="i"><li class="listitem">
											From the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Add allowed source</span></strong></span> dropdown menu, select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Allow pods from inside the cluster</span></strong></span>.
										</li><li class="listitem">
											Click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">+ Add namespace selector</span></strong></span>.
										</li><li class="listitem">
											Add the label, <code class="literal cluster-admin">kubernetes.io/metadata.name</code>, and the selector, <code class="literal cluster-admin">openshift-console</code>.
										</li></ol></div></li><li class="listitem"><p class="simpara">
									Make the following specifications for the third <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Ingress rule</span></strong></span>:
								</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="i"><li class="listitem">
											From the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Add allowed source</span></strong></span> dropdown menu, select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Allow pods from inside the cluster</span></strong></span>.
										</li><li class="listitem">
											Click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">+ Add namespace selector</span></strong></span>.
										</li><li class="listitem">
											Add the label, <code class="literal cluster-admin">kubernetes.io/metadata.name</code>, and the selector, <code class="literal cluster-admin">openshift-monitoring</code>.
										</li></ol></div></li></ol></div></li></ol></div><div class="orderedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Observe</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Traffic</span></strong></span>.
						</li><li class="listitem">
							View the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Traffic Flows</span></strong></span> tab, or any tab, to verify that the data is displayed.
						</li><li class="listitem">
							Navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Observe</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Dashboards</span></strong></span>. In the NetObserv/Health selection, verify that the flows are being ingested and sent to Loki, which is represented in the first graph.
						</li></ol></div></section><section class="section cluster-admin" id="network-observability-sample-network-policy_network_observability"><div class="titlepage"><div><div><h3 class="title">36.6.2. Example network policy</h3></div></div></div><p class="cluster-admin cluster-admin">
					The following annotates an example <code class="literal cluster-admin">NetworkPolicy</code> object for the <code class="literal cluster-admin">netobserv</code> namespace:
				</p><div id="network-observability-network-policy-sample_network_observability" class="cluster-admin cluster-admin"><p class="title"><strong>Sample network policy</strong></p><p>
						
<pre class="programlisting language-yaml">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-ingress
  namespace: netobserv
spec:
  podSelector: {}            <span id="CO242-1"><!--Empty--></span><span class="callout">1</span>
  ingress:
    - from:
        - podSelector: {}    <span id="CO242-2"><!--Empty--></span><span class="callout">2</span>
          namespaceSelector: <span id="CO242-3"><!--Empty--></span><span class="callout">3</span>
            matchLabels:
              kubernetes.io/metadata.name: openshift-console
        - podSelector: {}
          namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: openshift-monitoring
  policyTypes:
    - Ingress
status: {}</pre>

					</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO242-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							A selector that describes the pods to which the policy applies. The policy object can only select pods in the project that defines the <code class="literal cluster-admin">NetworkPolicy</code> object. In this documentation, it would be the project in which the Network Observability Operator is installed, which is the <code class="literal cluster-admin">netobserv</code> project.
						</div></dd><dt><a href="#CO242-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							A selector that matches the pods from which the policy object allows ingress traffic. The default is that the selector matches pods in the same namespace as the <code class="literal cluster-admin">NetworkPolicy</code>.
						</div></dd><dt><a href="#CO242-3"><span class="callout">3</span></a> </dt><dd><div class="para">
							When the <code class="literal cluster-admin">namespaceSelector</code> is specified, the selector matches pods in the specified namespace.
						</div></dd></dl></div><div class="_additional-resources _additional-resources"><p class="title"><strong>Additional resources</strong></p><p>
						<a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#nw-networkpolicy-object_creating-network-policy">Creating a network policy using the CLI</a>
					</p></div></section></section><section class="section cluster-admin" id="nw-observe-network-traffic"><div class="titlepage"><div><div><h2 class="title">36.7. Observing the network traffic</h2></div></div></div><p class="cluster-admin cluster-admin">
				As an administrator, you can observe the network traffic in the OpenShift Container Platform console for detailed troubleshooting and analysis. This feature helps you get insights from different graphical representations of traffic flow. There are several available views to observe the network traffic.
			</p><section class="section cluster-admin" id="network-observability-overview_nw-observe-network-traffic"><div class="titlepage"><div><div><h3 class="title">36.7.1. Observing the network traffic from the Overview view</h3></div></div></div><p class="cluster-admin cluster-admin">
					The <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Overview</span></strong></span> view displays the overall aggregated metrics of the network traffic flow on the cluster. As an administrator, you can monitor the statistics with the available display options.
				</p><section class="section cluster-admin" id="network-observability-working-with-overview_nw-observe-network-traffic"><div class="titlepage"><div><div><h4 class="title">36.7.1.1. Working with the Overview view</h4></div></div></div><p class="cluster-admin cluster-admin">
						As an administrator, you can navigate to the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Overview</span></strong></span> view to see the graphical representation of the flow rate statistics.
					</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
								Navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Observe</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Traffic</span></strong></span>.
							</li><li class="listitem">
								In the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Traffic</span></strong></span> page, click the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Overview</span></strong></span> tab.
							</li></ol></div><p class="cluster-admin cluster-admin">
						You can configure the scope of each flow rate data by clicking the menu icon.
					</p></section><section class="section cluster-admin" id="network-observability-configuring-options-overview_nw-observe-network-traffic"><div class="titlepage"><div><div><h4 class="title">36.7.1.2. Configuring advanced options for the Overview view</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can customize the graphical view by using advanced options. To access the advanced options, click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Show advanced options</span></strong></span>.You can configure the details in the graph by using the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Display options</span></strong></span> drop-down menu. The options available are:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Metric type</span></strong></span>: The metrics to be shown in <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Bytes</span></strong></span> or <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Packets</span></strong></span>. The default value is <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Bytes</span></strong></span>.
							</li><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Scope</span></strong></span>: To select the detail of components between which the network traffic flows. You can set the scope to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Node</span></strong></span>, <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Namespace</span></strong></span>, <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Owner</span></strong></span>, or <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Resource</span></strong></span>. <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Owner</span></strong></span> is an aggregation of resources. <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Resource</span></strong></span> can be a pod, service, node, in case of host-network traffic, or an unknown IP address. The default value is <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Namespace</span></strong></span>.
							</li><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Truncate labels</span></strong></span>: Select the required width of the label from the drop-down list. The default value is <span class="strong strong"><strong><span class="cluster-admin cluster-admin">M</span></strong></span>.
							</li></ul></div><section class="section cluster-admin" id="network-observability-cao-managing-panels-overview_nw-observe-network-traffic"><div class="titlepage"><div><div><h5 class="title">36.7.1.2.1. Managing panels</h5></div></div></div><p class="cluster-admin cluster-admin">
							You can select the required statistics to be displayed, and reorder them. To manage columns, click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Manage panels</span></strong></span>.
						</p></section><section class="section cluster-admin" id="network-observability-pktdrop-overview_nw-observe-network-traffic"><div class="titlepage"><div><div><h5 class="title">36.7.1.2.2. Packet drop tracking</h5></div></div></div><p class="cluster-admin cluster-admin">
							You can configure graphical representation of network flow records with packet loss in the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Overview</span></strong></span> view. By employing eBPF tracepoint hooks, you can gain valuable insights into packet drops for TCP, UDP, SCTP, ICMPv4, and ICMPv6 protocols, which can result in the following actions:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Identification: Pinpoint the exact locations and network paths where packet drops are occurring. Determine whether specific devices, interfaces, or routes are more prone to drops.
								</li><li class="listitem">
									Root cause analysis: Examine the data collected by the eBPF program to understand the causes of packet drops. For example, are they a result of congestion, buffer issues, or specific network events?
								</li><li class="listitem">
									Performance optimization: With a clearer picture of packet drops, you can take steps to optimize network performance, such as adjust buffer sizes, reconfigure routing paths, or implement Quality of Service (QoS) measures.
								</li></ul></div><p class="cluster-admin cluster-admin">
							When packet drop tracking is enabled, you can see the following metrics represented in a chart in the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Overview</span></strong></span>.
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Top X flow dropped rates stacked
								</li><li class="listitem">
									Total dropped rate
								</li><li class="listitem">
									Top X dropped state
								</li><li class="listitem">
									Top X dropped cause
								</li><li class="listitem">
									Top X flow dropped rates stacked with total
								</li></ul></div><p class="cluster-admin cluster-admin">
							See the <span class="emphasis"><em><span class="cluster-admin cluster-admin">Additional Resources</span></em></span> of this section for more information about enabling and working with packet drop tracking.
						</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
									For more information about configuring packet drops in the <code class="literal cluster-admin">FlowCollector</code>, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/network_observability/#network-observability-packet-drops_nw-observe-network-traffic">Working with packet drops</a>.
								</li></ul></div></section><section class="section cluster-admin" id="network-observability-dns-overview_nw-observe-network-traffic"><div class="titlepage"><div><div><h5 class="title">36.7.1.2.3. DNS tracking</h5></div></div></div><p class="cluster-admin cluster-admin">
							You can configure graphical representation of Domain Name System (DNS) tracking of network flows in the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Overview</span></strong></span> view. Using DNS tracking with extended Berkeley Packet Filter (eBPF) tracepoint hooks can serve various purposes:
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Network Monitoring: Gain insights into DNS queries and responses, helping network administrators identify unusual patterns, potential bottlenecks, or performance issues.
								</li><li class="listitem">
									Security Analysis: Detect suspicious DNS activities, such as domain name generation algorithms (DGA) used by malware, or identify unauthorized DNS resolutions that might indicate a security breach.
								</li><li class="listitem">
									Troubleshooting: Debug DNS-related issues by tracing DNS resolution steps, tracking latency, and identifying misconfigurations.
								</li></ul></div><p class="cluster-admin cluster-admin">
							When DNS tracking is enabled, you can see the following metrics represented in a chart in the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Overview</span></strong></span>. See the <span class="emphasis"><em><span class="cluster-admin cluster-admin">Additional Resources</span></em></span> in this section for more information about enabling and working with this view.
						</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
									Top 5 average DNS latencies
								</li><li class="listitem">
									Top 5 DNS response code
								</li><li class="listitem">
									Top 5 DNS response code stacked with total
								</li></ul></div><p class="cluster-admin cluster-admin">
							This feature is supported for IPv4 and IPv6 UDP protocol.
						</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
									For more information about configuring DNS in the <code class="literal cluster-admin">FlowCollector</code>, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/network_observability/#network-observability-dns-tracking_nw-observe-network-traffic">Working with DNS tracking</a>.
								</li></ul></div></section></section></section><section class="section cluster-admin" id="network-observability-trafficflow_nw-observe-network-traffic"><div class="titlepage"><div><div><h3 class="title">36.7.2. Observing the network traffic from the Traffic flows view</h3></div></div></div><p class="cluster-admin cluster-admin">
					The <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Traffic flows</span></strong></span> view displays the data of the network flows and the amount of traffic in a table. As an administrator, you can monitor the amount of traffic across the application by using the traffic flow table.
				</p><section class="section cluster-admin" id="network-observability-working-with-trafficflow_nw-observe-network-traffic"><div class="titlepage"><div><div><h4 class="title">36.7.2.1. Working with the Traffic flows view</h4></div></div></div><p class="cluster-admin cluster-admin">
						As an administrator, you can navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Traffic flows</span></strong></span> table to see network flow information.
					</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
								Navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Observe</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Traffic</span></strong></span>.
							</li><li class="listitem">
								In the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Traffic</span></strong></span> page, click the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Traffic flows</span></strong></span> tab.
							</li></ol></div><p class="cluster-admin cluster-admin">
						You can click on each row to get the corresponding flow information.
					</p></section><section class="section cluster-admin" id="network-observability-configuring-options-trafficflow_nw-observe-network-traffic"><div class="titlepage"><div><div><h4 class="title">36.7.2.2. Configuring advanced options for the Traffic flows view</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can customize and export the view by using <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Show advanced options</span></strong></span>. You can set the row size by using the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Display options</span></strong></span> drop-down menu. The default value is <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Normal</span></strong></span>.
					</p><section class="section cluster-admin" id="network-observability-cao-managing-columns-trafficflownw-observe-network-traffic"><div class="titlepage"><div><div><h5 class="title">36.7.2.2.1. Managing columns</h5></div></div></div><p class="cluster-admin cluster-admin">
							You can select the required columns to be displayed, and reorder them. To manage columns, click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Manage columns</span></strong></span>.
						</p></section><section class="section cluster-admin" id="network-observability-cao-export-trafficflow_nw-observe-network-traffic"><div class="titlepage"><div><div><h5 class="title">36.7.2.2.2. Exporting the traffic flow data</h5></div></div></div><p class="cluster-admin cluster-admin">
							You can export data from the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Traffic flows</span></strong></span> view.
						</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
									Click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Export data</span></strong></span>.
								</li><li class="listitem">
									In the pop-up window, you can select the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Export all data</span></strong></span> checkbox to export all the data, and clear the checkbox to select the required fields to be exported.
								</li><li class="listitem">
									Click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Export</span></strong></span>.
								</li></ol></div></section></section><section class="section cluster-admin" id="network-observability-working-with-conversations_nw-observe-network-traffic"><div class="titlepage"><div><div><h4 class="title">36.7.2.3. Working with conversation tracking</h4></div></div></div><p class="cluster-admin cluster-admin">
						As an administrator, you can you can group network flows that are part of the same conversation. A conversation is defined as a grouping of peers that are identified by their IP addresses, ports, and protocols, resulting in an unique <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Conversation Id</span></strong></span>. You can query conversation events in the web console. These events are represented in the web console as follows:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Conversation start</span></strong></span>: This event happens when a connection is starting or TCP flag intercepted
							</li><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Conversation tick</span></strong></span>: This event happens at each specified interval defined in the <code class="literal cluster-admin">FlowCollector</code> <code class="literal cluster-admin">spec.processor.conversationHeartbeatInterval</code> parameter while the connection is active.
							</li><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Conversation end</span></strong></span>: This event happens when the <code class="literal cluster-admin">FlowCollector</code> <code class="literal cluster-admin">spec.processor.conversationEndTimeout</code> parameter is reached or the TCP flag is intercepted.
							</li><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flow</span></strong></span>: This is the network traffic flow that occurs within the specified interval.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
								In the web console, navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Operators</span></strong></span>.
							</li><li class="listitem">
								Under the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Provided APIs</span></strong></span> heading for the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">NetObserv Operator</span></strong></span>, select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flow Collector</span></strong></span>.
							</li><li class="listitem">
								Select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">cluster</span></strong></span> then select the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">YAML</span></strong></span> tab.
							</li><li class="listitem"><p class="simpara">
								Configure the <code class="literal cluster-admin">FlowCollector</code> custom resource so that <code class="literal cluster-admin">spec.processor.logTypes</code>, <code class="literal cluster-admin">conversationEndTimeout</code>, and <code class="literal cluster-admin">conversationHeartbeatInterval</code> parameters are set according to your observation needs. A sample configuration is as follows:
							</p><div id="network-observability-flowcollector-configuring-conversations_nw-observe-network-traffic" class="cluster-admin cluster-admin"><p class="title"><strong>Configure <code class="literal cluster-admin">FlowCollector</code> for conversation tracking</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: flows.netobserv.io/v1alpha1
kind: FlowCollector
metadata:
  name: cluster
spec:
 processor:
  conversationEndTimeout: 10s                  <span id="CO243-1"><!--Empty--></span><span class="callout">1</span>
  logTypes: FLOWS                              <span id="CO243-2"><!--Empty--></span><span class="callout">2</span>
  conversationHeartbeatInterval: 30s           <span id="CO243-3"><!--Empty--></span><span class="callout">3</span></pre>

								</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO243-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Conversation end</span></strong></span> event represents the point when the <code class="literal cluster-admin">conversationEndTimeout</code> is reached or the TCP flag is intercepted.
									</div></dd><dt><a href="#CO243-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										When <code class="literal cluster-admin">logTypes</code> is set to <code class="literal cluster-admin">FLOWS</code>, only the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flow</span></strong></span> event is exported. If you set the value to <code class="literal cluster-admin">ALL</code>, both conversation and flow events are exported and visible in the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Traffic</span></strong></span> page. To focus only on conversation events, you can specify <code class="literal cluster-admin">CONVERSATIONS</code> which exports the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Conversation start</span></strong></span>, <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Conversation tick</span></strong></span> and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Conversation end</span></strong></span> events; or <code class="literal cluster-admin">ENDED_CONVERSATIONS</code> exports only the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Conversation end</span></strong></span> events. Storage requirements are highest for <code class="literal cluster-admin">ALL</code> and lowest for <code class="literal cluster-admin">ENDED_CONVERSATIONS</code>.
									</div></dd><dt><a href="#CO243-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Conversation tick</span></strong></span> event represents each specified interval defined in the <code class="literal cluster-admin">FlowCollector</code> <code class="literal cluster-admin">conversationHeartbeatInterval</code> parameter while the network connection is active.
									</div></dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
									If you update the <code class="literal cluster-admin">logType</code> option, the flows from the previous selection do not clear from the console plugin. For example, if you initially set <code class="literal cluster-admin">logType</code> to <code class="literal cluster-admin">CONVERSATIONS</code> for a span of time until 10 AM and then move to <code class="literal cluster-admin">ENDED_CONVERSATIONS</code>, the console plugin shows all conversation events before 10 AM and only ended conversations after 10 AM.
								</p></div></div></li><li class="listitem">
								Refresh the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Traffic</span></strong></span> page on the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Traffic flows</span></strong></span> tab. Notice there are two new columns, <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Event/Type</span></strong></span> and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Conversation Id</span></strong></span>. All the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Event/Type</span></strong></span> fields are <code class="literal cluster-admin">Flow</code> when <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flow</span></strong></span> is the selected query option.
							</li><li class="listitem">
								Select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Query Options</span></strong></span> and choose the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Log Type</span></strong></span>, <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Conversation</span></strong></span>. Now the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Event/Type</span></strong></span> shows all of the desired conversation events.
							</li><li class="listitem">
								Next you can filter on a specific conversation ID or switch between the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Conversation</span></strong></span> and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flow</span></strong></span> log type options from the side panel.
							</li></ol></div></section><section class="section cluster-admin" id="network-observability-packet-drops_nw-observe-network-traffic"><div class="titlepage"><div><div><h4 class="title">36.7.2.4. Working with packet drops</h4></div></div></div><p class="cluster-admin cluster-admin">
						Packet loss occurs when one or more packets of network flow data fail to reach their destination. You can track these drops by editing the <code class="literal cluster-admin">FlowCollector</code> to the specifications in the following YAML example.
					</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
							CPU and memory usage increases when this feature is enabled.
						</p></div></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								Access to an OpenShift Container Platform cluster with version 4.13.
							</li><li class="listitem">
								Kernel supported by Red Hat Enterprise Linux (RHEL) 9.2.
							</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
								In the web console, navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Operators</span></strong></span>.
							</li><li class="listitem">
								Under the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Provided APIs</span></strong></span> heading for the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">NetObserv Operator</span></strong></span>, select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flow Collector</span></strong></span>.
							</li><li class="listitem">
								Select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">cluster</span></strong></span>, and then select the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">YAML</span></strong></span> tab.
							</li><li class="listitem"><p class="simpara">
								Configure the <code class="literal cluster-admin">FlowCollector</code> custom resource for packet drops, for example:
							</p><div id="network-observability-flowcollector-configuring-pkt-drop_nw-observe-network-traffic" class="cluster-admin cluster-admin"><p class="title"><strong>Example <code class="literal cluster-admin">FlowCollector</code> configuration</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: flows.netobserv.io/v1alpha1
kind: FlowCollector
metadata:
  name: cluster
spec:
  namespace: netobserv
  deploymentModel: DIRECT
  agent:
    type: EBPF
    ebpf:
      features:
       - PacketDrop            <span id="CO244-1"><!--Empty--></span><span class="callout">1</span>
      privileged: true       <span id="CO244-2"><!--Empty--></span><span class="callout">2</span></pre>

								</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO244-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										You can start reporting the packet drops of each network flow by listing the <code class="literal cluster-admin">PacketDrop</code> parameter in the <code class="literal cluster-admin">spec.agent.ebpf.features</code> specification list.
									</div></dd><dt><a href="#CO244-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The <code class="literal cluster-admin">spec.agent.ebpf.privileged</code> specification value must be <code class="literal cluster-admin">true</code> for packet drop tracking.
									</div></dd></dl></div></li></ol></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem"><p class="simpara">
								When you refresh the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Traffic</span></strong></span> page, the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Overview</span></strong></span>, <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Traffic Flow</span></strong></span>, and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Topology</span></strong></span> views display new information about packet drops:
							</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem">
										Select new choices in <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Manage panels</span></strong></span> to choose which graphical visualizations of packet drops to display in the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Overview</span></strong></span>.
									</li><li class="listitem"><p class="simpara">
										Select new choices in <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Manage columns</span></strong></span> to choose which packet drop information to display in the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Traffic flows</span></strong></span> table.
									</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="i"><li class="listitem">
												In the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Traffic Flows</span></strong></span> view, you can also expand the side panel to view more information about packet drops.
											</li></ol></div></li><li class="listitem">
										In the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Topology</span></strong></span> view, red lines are displayed where drops are present.
									</li></ol></div></li></ul></div></section><section class="section cluster-admin" id="network-observability-dns-tracking_nw-observe-network-traffic"><div class="titlepage"><div><div><h4 class="title">36.7.2.5. Working with DNS tracking</h4></div></div></div><p class="cluster-admin cluster-admin">
						Using DNS tracking, you can monitor your network, conduct security analysis, and troubleshoot DNS issues. You can track DNS by editing the <code class="literal cluster-admin">FlowCollector</code> to the specifications in the following YAML example.
					</p><div class="admonition important cluster-admin"><div class="admonition_header">Important</div><div><p class="cluster-admin cluster-admin">
							CPU and memory usage increases are observed in the eBPF agent when this feature is enabled.
						</p></div></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
								In the web console, navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Operators</span></strong></span>.
							</li><li class="listitem">
								Under the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Provided APIs</span></strong></span> heading for the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">NetObserv Operator</span></strong></span>, select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flow Collector</span></strong></span>.
							</li><li class="listitem">
								Select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">cluster</span></strong></span> then select the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">YAML</span></strong></span> tab.
							</li><li class="listitem"><p class="simpara">
								Configure the <code class="literal cluster-admin">FlowCollector</code> custom resource. A sample configuration is as follows:
							</p><div id="network-observability-flowcollector-configuring-dns_nw-observe-network-traffic" class="cluster-admin cluster-admin"><p class="title"><strong>Configure <code class="literal cluster-admin">FlowCollector</code> for DNS tracking</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: flows.netobserv.io/v1alpha1
kind: FlowCollector
metadata:
  name: cluster
namespace: netobserv
  deploymentModel: DIRECT
  agent:
    type: EBPF
    ebpf:
      features:
      - DNSTracking           <span id="CO245-1"><!--Empty--></span><span class="callout">1</span>
      privileged: true        <span id="CO245-2"><!--Empty--></span><span class="callout">2</span></pre>

								</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO245-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										You can set the <code class="literal cluster-admin">spec.agent.ebpf.features</code> parameter list to enable DNS tracking of each network flow in the web console.
									</div></dd><dt><a href="#CO245-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Note that the <code class="literal cluster-admin">spec.agent.ebpf.privileged</code> specification value must be <code class="literal cluster-admin">true</code> for DNS tracking to be enabled.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								When you refresh the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Traffic</span></strong></span> page, there are new DNS representations you can choose to view in the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Overview</span></strong></span> and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Traffic Flow</span></strong></span> views and new filters you can apply.
							</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="a"><li class="listitem">
										Select new DNS choices in <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Manage panels</span></strong></span> to display graphical visualizations and DNS metrics in the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Overview</span></strong></span>.
									</li><li class="listitem">
										Select new choices in <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Manage columns</span></strong></span> to add DNS columns to the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Traffic Flows</span></strong></span> view.
									</li><li class="listitem">
										Filter on specific DNS metrics, such as <span class="strong strong"><strong><span class="cluster-admin cluster-admin">DNS Id</span></strong></span>, <span class="strong strong"><strong><span class="cluster-admin cluster-admin">DNS Latency</span></strong></span> and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">DNS Response Code</span></strong></span>, and see more information from the side panel.
									</li></ol></div></li></ol></div><section class="section cluster-admin" id="network-observability-histogram-trafficflow_nw-observe-network-traffic"><div class="titlepage"><div><div><h5 class="title">36.7.2.5.1. Using the histogram</h5></div></div></div><p class="cluster-admin cluster-admin">
							You can click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Show histogram</span></strong></span> to display a toolbar view for visualizing the history of flows as a bar chart. The histogram shows the number of logs over time. You can select a part of the histogram to filter the network flow data in the table that follows the toolbar.
						</p></section></section></section><section class="section cluster-admin" id="network-observability-topology_nw-observe-network-traffic"><div class="titlepage"><div><div><h3 class="title">36.7.3. Observing the network traffic from the Topology view</h3></div></div></div><p class="cluster-admin cluster-admin">
					The <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Topology</span></strong></span> view provides a graphical representation of the network flows and the amount of traffic. As an administrator, you can monitor the traffic data across the application by using the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Topology</span></strong></span> view.
				</p><section class="section cluster-admin" id="network-observability-working-with-topology_nw-observe-network-traffic"><div class="titlepage"><div><div><h4 class="title">36.7.3.1. Working with the Topology view</h4></div></div></div><p class="cluster-admin cluster-admin">
						As an administrator, you can navigate to the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Topology</span></strong></span> view to see the details and metrics of the component.
					</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
								Navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Observe</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Traffic</span></strong></span>.
							</li><li class="listitem">
								In the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Traffic</span></strong></span> page, click the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Topology</span></strong></span> tab.
							</li></ol></div><p class="cluster-admin cluster-admin">
						You can click each component in the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Topology</span></strong></span> to view the details and metrics of the component.
					</p></section><section class="section cluster-admin" id="network-observability-configuring-options-topology_nw-observe-network-traffic"><div class="titlepage"><div><div><h4 class="title">36.7.3.2. Configuring the advanced options for the Topology view</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can customize and export the view by using <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Show advanced options</span></strong></span>. The advanced options view has the following features:
					</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Find in view</span></strong></span>: To search the required components in the view.
							</li><li class="listitem"><p class="simpara">
								<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Display options</span></strong></span>: To configure the following options:
							</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem">
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Layout</span></strong></span>: To select the layout of the graphical representation. The default value is <span class="strong strong"><strong><span class="cluster-admin cluster-admin">ColaNoForce</span></strong></span>.
									</li><li class="listitem">
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Scope</span></strong></span>: To select the scope of components between which the network traffic flows. The default value is <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Namespace</span></strong></span>.
									</li><li class="listitem">
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Groups</span></strong></span>: To enchance the understanding of ownership by grouping the components. The default value is <span class="strong strong"><strong><span class="cluster-admin cluster-admin">None</span></strong></span>.
									</li><li class="listitem">
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Collapse groups</span></strong></span>: To expand or collapse the groups. The groups are expanded by default. This option is disabled if <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Groups</span></strong></span> has value <span class="strong strong"><strong><span class="cluster-admin cluster-admin">None</span></strong></span>.
									</li><li class="listitem">
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Show</span></strong></span>: To select the details that need to be displayed. All the options are checked by default. The options available are: <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Edges</span></strong></span>, <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Edges label</span></strong></span>, and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Badges</span></strong></span>.
									</li><li class="listitem">
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Truncate labels</span></strong></span>: To select the required width of the label from the drop-down list. The default value is <span class="strong strong"><strong><span class="cluster-admin cluster-admin">M</span></strong></span>.
									</li></ul></div></li></ul></div><section class="section cluster-admin" id="network-observability-cao-export-topology_nw-observe-network-traffic"><div class="titlepage"><div><div><h5 class="title">36.7.3.2.1. Exporting the topology view</h5></div></div></div><p class="cluster-admin cluster-admin">
							To export the view, click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Export topology view</span></strong></span>. The view is downloaded in PNG format.
						</p></section></section></section><section class="section cluster-admin" id="network-observability-quickfilternw-observe-network-traffic"><div class="titlepage"><div><div><h3 class="title">36.7.4. Filtering the network traffic</h3></div></div></div><p class="cluster-admin cluster-admin">
					By default, the Network Traffic page displays the traffic flow data in the cluster based on the default filters configured in the <code class="literal cluster-admin">FlowCollector</code> instance. You can use the filter options to observe the required data by changing the preset filter.
				</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Query Options</span></dt><dd><p class="simpara">
								You can use <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Query Options</span></strong></span> to optimize the search results, as listed below:
							</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Log Type</span></strong></span>: The available options <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Conversation</span></strong></span> and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flows</span></strong></span> provide the ability to query flows by log type, such as flow log, new conversation, completed conversation, and a heartbeat, which is a periodic record with updates for long conversations. A conversation is an aggregation of flows between the same peers.
									</li><li class="listitem">
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Duplicated flows</span></strong></span>: A flow might be reported from several interfaces, and from both source and destination nodes, making it appear in the data several times. By selecting this query option, you can choose to show duplicated flows. Duplicated flows have the same sources and destinations, including ports, and also have the same protocols, with the exception of <code class="literal cluster-admin">Interface</code> and <code class="literal cluster-admin">Direction</code> fields. Duplicates are hidden by default. Use the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Direction</span></strong></span> filter in the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Common</span></strong></span> section of the dropdown list to switch between ingress and egress traffic.
									</li><li class="listitem">
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Match filters</span></strong></span>: You can determine the relation between different filter parameters selected in the advanced filter. The available options are <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Match all</span></strong></span> and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Match any</span></strong></span>. <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Match all</span></strong></span> provides results that match all the values, and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Match any</span></strong></span> provides results that match any of the values entered. The default value is <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Match all</span></strong></span>.
									</li><li class="listitem"><p class="simpara">
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Drops filter</span></strong></span>: You can view different levels of dropped packets with the following query options:
									</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="circle"><li class="listitem">
												<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Fully dropped</span></strong></span> shows flow records with fully dropped packets.
											</li><li class="listitem">
												<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Containing drops</span></strong></span> shows flow records that contain drops but can be sent.
											</li><li class="listitem">
												<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Without drops</span></strong></span> shows records that contain sent packets.
											</li><li class="listitem">
												<span class="strong strong"><strong><span class="cluster-admin cluster-admin">All</span></strong></span> shows all the aforementioned records.
											</li></ul></div></li><li class="listitem">
										<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Limit</span></strong></span>: The data limit for internal backend queries. Depending upon the matching and the filter settings, the number of traffic flow data is displayed within the specified limit.
									</li></ul></div></dd><dt><span class="term">Quick filters</span></dt><dd>
								The default values in <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Quick filters</span></strong></span> drop-down menu are defined in the <code class="literal cluster-admin">FlowCollector</code> configuration. You can modify the options from console.
							</dd><dt><span class="term">Advanced filters</span></dt><dd>
								You can set the advanced filters, <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Common</span></strong></span>, <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Source</span></strong></span>, or <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Destination</span></strong></span>, by selecting the parameter to be filtered from the dropdown list. The flow data is filtered based on the selection. To enable or disable the applied filter, you can click on the applied filter listed below the filter options.
							</dd></dl></div><div class="admonition note cluster-admin"><div class="admonition_header">Note</div><div><p class="cluster-admin cluster-admin">
						To understand the rules of specifying the text value, click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Learn More</span></strong></span>.
					</p></div></div><p class="cluster-admin cluster-admin">
					You can toggle between 
					<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/9dbbc83270dd7de4d8fb267e9172f7f3/arrow-up-long-solid.png" width="10" alt="arrow up long solid"/></span>
					 <span class="strong strong"><strong><span class="cluster-admin cluster-admin">One way</span></strong></span> and 
					<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/9dbbc83270dd7de4d8fb267e9172f7f3/arrow-up-long-solid.png" width="10" alt="arrow up long solid"/></span>
					 <span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/4f8892ede40f2626cc2be02feb6a5954/arrow-down-long-solid.png" width="10" alt="arrow down long solid"/></span>
					 <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Back and forth</span></strong></span> filtering. The 
					<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/9dbbc83270dd7de4d8fb267e9172f7f3/arrow-up-long-solid.png" width="10" alt="arrow up long solid"/></span>
					 <span class="strong strong"><strong><span class="cluster-admin cluster-admin">One way</span></strong></span> filter shows only <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Source</span></strong></span> and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Destination</span></strong></span> traffic according to your filter selections. You can use <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Swap</span></strong></span> to change the directional view of the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Source</span></strong></span> and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Destination</span></strong></span> traffic. The 
					<span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/9dbbc83270dd7de4d8fb267e9172f7f3/arrow-up-long-solid.png" width="10" alt="arrow up long solid"/></span>
					 <span class="inlinemediaobject"><img src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.13-Networking-en-US/images/4f8892ede40f2626cc2be02feb6a5954/arrow-down-long-solid.png" width="10" alt="arrow down long solid"/></span>
					 <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Back and forth</span></strong></span> filter includes return traffic with the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Source</span></strong></span> and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Destination</span></strong></span> filters.
				</p><p class="cluster-admin cluster-admin">
					You can click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Reset defaults</span></strong></span> to remove the existing filters, and apply the filter defined in <code class="literal cluster-admin">FlowCollector</code> configuration.
				</p><p class="cluster-admin cluster-admin">
					Alternatively, you can access the traffic flow data in the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Traffic</span></strong></span> tab of the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Namespaces</span></strong></span>, <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Services</span></strong></span>, <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Routes</span></strong></span>, <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Nodes</span></strong></span>, and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Workloads</span></strong></span> pages which provide the filtered data of the corresponding aggregations.
				</p><div class="_additional-resources _additional-resources"><p class="title"><strong>Additional resources</strong></p><p>
						For more information about configuring quick filters in the <code class="literal cluster-admin">FlowCollector</code>, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-config-quick-filters_network_observability">Configuring Quick Filters</a> and the <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/networking/#network-observability-flowcollector-view_network_observability">Flow Collector sample resource</a>.
					</p></div></section></section><section class="section cluster-admin" id="network-observability-operator-monitoring"><div class="titlepage"><div><div><h2 class="title">36.8. Monitoring the Network Observability Operator</h2></div></div></div><p class="cluster-admin cluster-admin">
				You can use the web console to monitor alerts related to the health of the Network Observability Operator.
			</p><section class="section cluster-admin" id="network-observability-alert-dashboard_network_observability"><div class="titlepage"><div><div><h3 class="title">36.8.1. Viewing health information</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can access metrics about health and resource usage of the Network Observability Operator from the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Dashboards</span></strong></span> page in the web console. A health alert banner that directs you to the dashboard can appear on the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Network Traffic</span></strong></span> and <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Home</span></strong></span> pages in the event that an alert is triggered. Alerts are generated in the following cases:
				</p><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							The <code class="literal cluster-admin">NetObservLokiError</code> alert occurs if the <code class="literal cluster-admin">flowlogs-pipeline</code> workload is dropping flows because of Loki errors, such as if the Loki ingestion rate limit has been reached.
						</li><li class="listitem">
							The <code class="literal cluster-admin">NetObservNoFlows</code> alert occurs if no flows are ingested for a certain amount of time.
						</li></ul></div><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have the Network Observability Operator installed.
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal cluster-admin">cluster-admin</code> role or with view permissions for all projects.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							From the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Administrator</span></strong></span> perspective in the web console, navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Observe</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Dashboards</span></strong></span>.
						</li><li class="listitem">
							From the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Dashboards</span></strong></span> dropdown, select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Netobserv/Health</span></strong></span>. Metrics about the health of the Operator are displayed on the page.
						</li></ol></div><section class="section cluster-admin" id="network-observability-disable-alerts_network_observability"><div class="titlepage"><div><div><h4 class="title">36.8.1.1. Disabling health alerts</h4></div></div></div><p class="cluster-admin cluster-admin">
						You can opt out of health alerting by editing the <code class="literal cluster-admin">FlowCollector</code> resource:
					</p><div class="orderedlist cluster-admin"><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
								In the web console, navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Operators</span></strong></span>.
							</li><li class="listitem">
								Under the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Provided APIs</span></strong></span> heading for the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">NetObserv Operator</span></strong></span>, select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flow Collector</span></strong></span>.
							</li><li class="listitem">
								Select <span class="strong strong"><strong><span class="cluster-admin cluster-admin">cluster</span></strong></span> then select the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">YAML</span></strong></span> tab.
							</li><li class="listitem">
								Add <code class="literal cluster-admin">spec.processor.metrics.disableAlerts</code> to disable health alerts, as in the following YAML sample:
							</li></ol></div><pre class="screen cluster-admin cluster-admin">apiVersion: flows.netobserv.io/v1alpha1
kind: FlowCollector
metadata:
  name: cluster
spec:
  processor:
    metrics:
      disableAlerts: [NetObservLokiError, NetObservNoFlows] <span id="CO246-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO246-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								You can specify one or a list with both types of alerts to disable.
							</div></dd></dl></div></section></section><section class="section cluster-admin" id="network-observability-netobserv-dashboard-rate-limit-alerts_network_observability"><div class="titlepage"><div><div><h3 class="title">36.8.2. Creating Loki rate limit alerts for the NetObserv dashboard</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can create custom rules for the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Netobserv</span></strong></span> dashboard metrics to trigger alerts when Loki rate limits have been reached.
				</p><p class="cluster-admin cluster-admin">
					An example of an alerting rule configuration YAML file is as follows:
				</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: loki-alerts
  namespace: openshift-operators-redhat
spec:
  groups:
  - name: LokiRateLimitAlerts
    rules:
    - alert: LokiTenantRateLimit
      annotations:
        message: |-
          {{ $labels.job }} {{ $labels.route }} is experiencing 429 errors.
        summary: "At any number of requests are responded with the rate limit error code."
      expr: sum(irate(loki_request_duration_seconds_count{status_code="429"}[1m])) by (job, namespace, route) / sum(irate(loki_request_duration_seconds_count[1m])) by (job, namespace, route) * 100 &gt; 0
      for: 10s
      labels:
        severity: warning</pre><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							For more information about creating alerts that you can see on the dashboard, see <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/monitoring/#creating-alerting-rules-for-user-defined-projects_managing-alerts">Creating alerting rules for user-defined projects</a>.
						</li></ul></div></section></section><section class="section cluster-admin" id="flowcollector-api"><div class="titlepage"><div><div><h2 class="title">36.9. FlowCollector configuration parameters</h2></div></div></div><p class="cluster-admin cluster-admin">
				FlowCollector is the Schema for the network flows collection API, which pilots and configures the underlying deployments.
			</p><section class="section cluster-admin" id="network-observability-flowcollector-api-specifications_network_observability"><div class="titlepage"><div><div><h3 class="title">36.9.1. FlowCollector API specifications</h3></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
								<code class="literal cluster-admin">FlowCollector</code> is the schema for the network flows collection API, which pilots and configures the underlying deployments.
							</dd><dt><span class="term">Type</span></dt><dd>
								<code class="literal cluster-admin">object</code>
							</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587100918368" scope="col">Property</th><th align="left" valign="top" id="idm140587100917376" scope="col">Type</th><th align="left" valign="top" id="idm140587100916288" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587100918368"> <p>
									<code class="literal cluster-admin">apiVersion</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587100917376"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587100916288"> <p>
									APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and might reject unrecognized values. More info: <a class="link" href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources">https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources</a>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587100918368"> <p>
									<code class="literal cluster-admin">kind</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587100917376"> <p>
									<code class="literal cluster-admin">string</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587100916288"> <p>
									Kind is a string value representing the REST resource this object represents. Servers might infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: <a class="link" href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds">https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds</a>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587100918368"> <p>
									<code class="literal cluster-admin">metadata</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587100917376"> <p>
									<code class="literal cluster-admin">object</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587100916288"> <p>
									Standard object’s metadata. More info: <a class="link" href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata">https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata</a>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140587100918368"> <p>
									<code class="literal cluster-admin">spec</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587100917376"> <p>
									<code class="literal cluster-admin">object</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140587100916288"> <p>
									Defines the desired state of the FlowCollector resource. <br/><br/> *: the mention of "unsupported", or "deprecated" for a feature throughout this document means that this feature is not officially supported by Red Hat. It might have been, for example, contributed by the community and accepted without a formal agreement for maintenance. The product maintainers might provide some support for these features as a best effort only.
								</p>
								 </td></tr></tbody></table></div><section class="section cluster-admin" id="metadata"><div class="titlepage"><div><div><h4 class="title">36.9.1.1. .metadata</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									Standard object’s metadata. More info: <a class="link" href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata">https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata</a>
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div></section><section class="section cluster-admin" id="spec"><div class="titlepage"><div><div><h4 class="title">36.9.1.2. .spec</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									Defines the desired state of the FlowCollector resource. <br/><br/> *: the mention of "unsupported", or "deprecated" for a feature throughout this document means that this feature is not officially supported by Red Hat. It might have been, for example, contributed by the community and accepted without a formal agreement for maintenance. The product maintainers might provide some support for these features as a best effort only.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587100859568" scope="col">Property</th><th align="left" valign="top" id="idm140587100858480" scope="col">Type</th><th align="left" valign="top" id="idm140587100857392" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587100859568"> <p>
										<code class="literal cluster-admin">agent</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100858480"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100857392"> <p>
										Agent configuration for flows extraction.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100859568"> <p>
										<code class="literal cluster-admin">consolePlugin</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100858480"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100857392"> <p>
										<code class="literal cluster-admin">consolePlugin</code> defines the settings related to the OpenShift Container Platform Console plugin, when available.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100859568"> <p>
										<code class="literal cluster-admin">deploymentModel</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100858480"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100857392"> <p>
										<code class="literal cluster-admin">deploymentModel</code> defines the desired type of deployment for flow processing. Possible values are:<br/> - <code class="literal cluster-admin">DIRECT</code> (default) to make the flow processor listening directly from the agents.<br/> - <code class="literal cluster-admin">KAFKA</code> to make flows sent to a Kafka pipeline before consumption by the processor.<br/> Kafka can provide better scalability, resiliency, and high availability (for more details, see <a class="link" href="https://www.redhat.com/en/topics/integration/what-is-apache-kafka">https://www.redhat.com/en/topics/integration/what-is-apache-kafka</a>).
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100859568"> <p>
										<code class="literal cluster-admin">exporters</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100858480"> <p>
										<code class="literal cluster-admin">array</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100857392"> <p>
										<code class="literal cluster-admin">exporters</code> define additional optional exporters for custom consumption or storage.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100859568"> <p>
										<code class="literal cluster-admin">kafka</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100858480"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100857392"> <p>
										Kafka configuration, allowing to use Kafka as a broker as part of the flow collection pipeline. Available when the <code class="literal cluster-admin">spec.deploymentModel</code> is <code class="literal cluster-admin">KAFKA</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100859568"> <p>
										<code class="literal cluster-admin">loki</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100858480"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100857392"> <p>
										Loki, the flow store, client settings.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100859568"> <p>
										<code class="literal cluster-admin">namespace</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100858480"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100857392"> <p>
										Namespace where Network Observability pods are deployed.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100859568"> <p>
										<code class="literal cluster-admin">processor</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100858480"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100857392"> <p>
										<code class="literal cluster-admin">processor</code> defines the settings of the component that receives the flows from the agent, enriches them, generates metrics, and forwards them to the Loki persistence layer and/or any available exporter.
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-agent"><div class="titlepage"><div><div><h4 class="title">36.9.1.3. .spec.agent</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									Agent configuration for flows extraction.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587100778656" scope="col">Property</th><th align="left" valign="top" id="idm140587100777568" scope="col">Type</th><th align="left" valign="top" id="idm140587100776480" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587100778656"> <p>
										<code class="literal cluster-admin">ebpf</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100777568"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100776480"> <p>
										<code class="literal cluster-admin">ebpf</code> describes the settings related to the eBPF-based flow reporter when <code class="literal cluster-admin">spec.agent.type</code> is set to <code class="literal cluster-admin">EBPF</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100778656"> <p>
										<code class="literal cluster-admin">ipfix</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100777568"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100776480"> <p>
										<code class="literal cluster-admin">ipfix</code> [deprecated (*)] - describes the settings related to the IPFIX-based flow reporter when <code class="literal cluster-admin">spec.agent.type</code> is set to <code class="literal cluster-admin">IPFIX</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100778656"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100777568"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100776480"> <p>
										<code class="literal cluster-admin">type</code> selects the flows tracing agent. Possible values are:<br/> - <code class="literal cluster-admin">EBPF</code> (default) to use Network Observability eBPF agent.<br/> - <code class="literal cluster-admin">IPFIX</code> [deprecated (*)] - to use the legacy IPFIX collector.<br/> <code class="literal cluster-admin">EBPF</code> is recommended as it offers better performances and should work regardless of the CNI installed on the cluster. <code class="literal cluster-admin">IPFIX</code> works with OVN-Kubernetes CNI (other CNIs could work if they support exporting IPFIX, but they would require manual configuration).
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-agent-ebpf"><div class="titlepage"><div><div><h4 class="title">36.9.1.4. .spec.agent.ebpf</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">ebpf</code> describes the settings related to the eBPF-based flow reporter when <code class="literal cluster-admin">spec.agent.type</code> is set to <code class="literal cluster-admin">EBPF</code>.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587100728192" scope="col">Property</th><th align="left" valign="top" id="idm140587100727104" scope="col">Type</th><th align="left" valign="top" id="idm140587100726016" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587100728192"> <p>
										<code class="literal cluster-admin">cacheActiveTimeout</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100727104"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100726016"> <p>
										<code class="literal cluster-admin">cacheActiveTimeout</code> is the max period during which the reporter aggregates flows before sending. Increasing <code class="literal cluster-admin">cacheMaxFlows</code> and <code class="literal cluster-admin">cacheActiveTimeout</code> can decrease the network traffic overhead and the CPU load, however you can expect higher memory consumption and an increased latency in the flow collection.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100728192"> <p>
										<code class="literal cluster-admin">cacheMaxFlows</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100727104"> <p>
										<code class="literal cluster-admin">integer</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100726016"> <p>
										<code class="literal cluster-admin">cacheMaxFlows</code> is the max number of flows in an aggregate; when reached, the reporter sends the flows. Increasing <code class="literal cluster-admin">cacheMaxFlows</code> and <code class="literal cluster-admin">cacheActiveTimeout</code> can decrease the network traffic overhead and the CPU load, however you can expect higher memory consumption and an increased latency in the flow collection.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100728192"> <p>
										<code class="literal cluster-admin">debug</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100727104"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100726016"> <p>
										<code class="literal cluster-admin">debug</code> allows setting some aspects of the internal configuration of the eBPF agent. This section is aimed exclusively for debugging and fine-grained performance optimizations, such as GOGC and GOMAXPROCS env vars. Users setting its values do it at their own risk.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100728192"> <p>
										<code class="literal cluster-admin">excludeInterfaces</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100727104"> <p>
										<code class="literal cluster-admin">array (string)</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100726016"> <p>
										<code class="literal cluster-admin">excludeInterfaces</code> contains the interface names that are excluded from flow tracing. An entry is enclosed by slashes, such as <code class="literal cluster-admin">/br-/</code> and is matched as a regular expression. Otherwise it is matched as a case-sensitive string.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100728192"> <p>
										<code class="literal cluster-admin">features</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100727104"> <p>
										<code class="literal cluster-admin">array (string)</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100726016"> <p>
										List of additional features to enable. They are all disabled by default. Enabling additional features might have performance impacts. Possible values are:<br/> - <code class="literal cluster-admin">PacketDrop</code>: enable the packets drop flows logging feature. This feature requires mounting the kernel debug filesystem, so the eBPF pod has to run as privileged. If the <code class="literal cluster-admin">spec.agent.eBPF.privileged</code> parameter is not set, an error is reported.<br/> - <code class="literal cluster-admin">DNSTracking</code>: enable the DNS tracking feature. This feature requires mounting the kernel debug filesystem hence the eBPF pod has to run as privileged. If the <code class="literal cluster-admin">spec.agent.eBPF.privileged</code> parameter is not set, an error is reported.<br/> - <code class="literal cluster-admin">FlowRTT</code> [unsupported (*)]: enable flow latency (RTT) calculations in the eBPF agent during TCP handshakes. This feature better works with <code class="literal cluster-admin">sampling</code> set to 1.<br/>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100728192"> <p>
										<code class="literal cluster-admin">imagePullPolicy</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100727104"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100726016"> <p>
										<code class="literal cluster-admin">imagePullPolicy</code> is the Kubernetes pull policy for the image defined above
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100728192"> <p>
										<code class="literal cluster-admin">interfaces</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100727104"> <p>
										<code class="literal cluster-admin">array (string)</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100726016"> <p>
										<code class="literal cluster-admin">interfaces</code> contains the interface names from where flows are collected. If empty, the agent fetches all the interfaces in the system, excepting the ones listed in ExcludeInterfaces. An entry is enclosed by slashes, such as <code class="literal cluster-admin">/br-/</code>, is matched as a regular expression. Otherwise it is matched as a case-sensitive string.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100728192"> <p>
										<code class="literal cluster-admin">kafkaBatchSize</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100727104"> <p>
										<code class="literal cluster-admin">integer</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100726016"> <p>
										<code class="literal cluster-admin">kafkaBatchSize</code> limits the maximum size of a request in bytes before being sent to a partition. Ignored when not using Kafka. Default: 10MB.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100728192"> <p>
										<code class="literal cluster-admin">logLevel</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100727104"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100726016"> <p>
										<code class="literal cluster-admin">logLevel</code> defines the log level for the Network Observability eBPF Agent
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100728192"> <p>
										<code class="literal cluster-admin">privileged</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100727104"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100726016"> <p>
										Privileged mode for the eBPF Agent container. In general this setting can be ignored or set to false: in that case, the operator sets granular capabilities (BPF, PERFMON, NET_ADMIN, SYS_RESOURCE) to the container, to enable its correct operation. If for some reason these capabilities cannot be set, such as if an old kernel version not knowing CAP_BPF is in use, then you can turn on this mode for more global privileges.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100728192"> <p>
										<code class="literal cluster-admin">resources</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100727104"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100726016"> <p>
										<code class="literal cluster-admin">resources</code> are the compute resources required by this container. More info: <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100728192"> <p>
										<code class="literal cluster-admin">sampling</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100727104"> <p>
										<code class="literal cluster-admin">integer</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100726016"> <p>
										Sampling rate of the flow reporter. 100 means one flow on 100 is sent. 0 or 1 means all flows are sampled.
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-agent-ebpf-debug"><div class="titlepage"><div><div><h4 class="title">36.9.1.5. .spec.agent.ebpf.debug</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">debug</code> allows setting some aspects of the internal configuration of the eBPF agent. This section is aimed exclusively for debugging and fine-grained performance optimizations, such as GOGC and GOMAXPROCS env vars. Users setting its values do it at their own risk.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587100606976" scope="col">Property</th><th align="left" valign="top" id="idm140587100605888" scope="col">Type</th><th align="left" valign="top" id="idm140587100604800" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587100606976"> <p>
										<code class="literal cluster-admin">env</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100605888"> <p>
										<code class="literal cluster-admin">object (string)</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100604800"> <p>
										<code class="literal cluster-admin">env</code> allows passing custom environment variables to underlying components. Useful for passing some very concrete performance-tuning options, such as GOGC and GOMAXPROCS, that should not be publicly exposed as part of the FlowCollector descriptor, as they are only useful in edge debug or support scenarios.
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-agent-ebpf-resources"><div class="titlepage"><div><div><h4 class="title">36.9.1.6. .spec.agent.ebpf.resources</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">resources</code> are the compute resources required by this container. More info: <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a>
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587100579632" scope="col">Property</th><th align="left" valign="top" id="idm140587100578544" scope="col">Type</th><th align="left" valign="top" id="idm140587100577456" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587100579632"> <p>
										<code class="literal cluster-admin">limits</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100578544"> <p>
										<code class="literal cluster-admin">integer-or-string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100577456"> <p>
										Limits describes the maximum amount of compute resources allowed. More info: <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100579632"> <p>
										<code class="literal cluster-admin">requests</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100578544"> <p>
										<code class="literal cluster-admin">integer-or-string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100577456"> <p>
										Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a>
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-agent-ipfix"><div class="titlepage"><div><div><h4 class="title">36.9.1.7. .spec.agent.ipfix</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">ipfix</code> [deprecated (*)] - describes the settings related to the IPFIX-based flow reporter when <code class="literal cluster-admin">spec.agent.type</code> is set to <code class="literal cluster-admin">IPFIX</code>.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587100543872" scope="col">Property</th><th align="left" valign="top" id="idm140587100542784" scope="col">Type</th><th align="left" valign="top" id="idm140587100541696" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587100543872"> <p>
										<code class="literal cluster-admin">cacheActiveTimeout</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100542784"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100541696"> <p>
										<code class="literal cluster-admin">cacheActiveTimeout</code> is the max period during which the reporter aggregates flows before sending.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100543872"> <p>
										<code class="literal cluster-admin">cacheMaxFlows</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100542784"> <p>
										<code class="literal cluster-admin">integer</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100541696"> <p>
										<code class="literal cluster-admin">cacheMaxFlows</code> is the max number of flows in an aggregate; when reached, the reporter sends the flows.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100543872"> <p>
										<code class="literal cluster-admin">clusterNetworkOperator</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100542784"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100541696"> <p>
										<code class="literal cluster-admin">clusterNetworkOperator</code> defines the settings related to the OpenShift Container Platform Cluster Network Operator, when available.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100543872"> <p>
										<code class="literal cluster-admin">forceSampleAll</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100542784"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100541696"> <p>
										<code class="literal cluster-admin">forceSampleAll</code> allows disabling sampling in the IPFIX-based flow reporter. It is not recommended to sample all the traffic with IPFIX, as it might generate cluster instability. If you REALLY want to do that, set this flag to true. Use at your own risk. When it is set to true, the value of <code class="literal cluster-admin">sampling</code> is ignored.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100543872"> <p>
										<code class="literal cluster-admin">ovnKubernetes</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100542784"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100541696"> <p>
										<code class="literal cluster-admin">ovnKubernetes</code> defines the settings of the OVN-Kubernetes CNI, when available. This configuration is used when using OVN’s IPFIX exports, without OpenShift Container Platform. When using OpenShift Container Platform, refer to the <code class="literal cluster-admin">clusterNetworkOperator</code> property instead.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100543872"> <p>
										<code class="literal cluster-admin">sampling</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100542784"> <p>
										<code class="literal cluster-admin">integer</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100541696"> <p>
										<code class="literal cluster-admin">sampling</code> is the sampling rate on the reporter. 100 means one flow on 100 is sent. To ensure cluster stability, it is not possible to set a value below 2. If you really want to sample every packet, which might impact the cluster stability, refer to <code class="literal cluster-admin">forceSampleAll</code>. Alternatively, you can use the eBPF Agent instead of IPFIX.
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-agent-ipfix-clusternetworkoperator"><div class="titlepage"><div><div><h4 class="title">36.9.1.8. .spec.agent.ipfix.clusterNetworkOperator</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">clusterNetworkOperator</code> defines the settings related to the OpenShift Container Platform Cluster Network Operator, when available.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587100476320" scope="col">Property</th><th align="left" valign="top" id="idm140587100475232" scope="col">Type</th><th align="left" valign="top" id="idm140587100474144" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587100476320"> <p>
										<code class="literal cluster-admin">namespace</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100475232"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100474144"> <p>
										Namespace where the config map is going to be deployed.
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-agent-ipfix-ovnkubernetes"><div class="titlepage"><div><div><h4 class="title">36.9.1.9. .spec.agent.ipfix.ovnKubernetes</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">ovnKubernetes</code> defines the settings of the OVN-Kubernetes CNI, when available. This configuration is used when using OVN’s IPFIX exports, without OpenShift Container Platform. When using OpenShift Container Platform, refer to the <code class="literal cluster-admin">clusterNetworkOperator</code> property instead.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587100449792" scope="col">Property</th><th align="left" valign="top" id="idm140587100448704" scope="col">Type</th><th align="left" valign="top" id="idm140587100447616" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587100449792"> <p>
										<code class="literal cluster-admin">containerName</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100448704"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100447616"> <p>
										<code class="literal cluster-admin">containerName</code> defines the name of the container to configure for IPFIX.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100449792"> <p>
										<code class="literal cluster-admin">daemonSetName</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100448704"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100447616"> <p>
										<code class="literal cluster-admin">daemonSetName</code> defines the name of the DaemonSet controlling the OVN-Kubernetes pods.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100449792"> <p>
										<code class="literal cluster-admin">namespace</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100448704"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100447616"> <p>
										Namespace where OVN-Kubernetes pods are deployed.
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-consoleplugin"><div class="titlepage"><div><div><h4 class="title">36.9.1.10. .spec.consolePlugin</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">consolePlugin</code> defines the settings related to the OpenShift Container Platform Console plugin, when available.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587100409184" scope="col">Property</th><th align="left" valign="top" id="idm140587100408096" scope="col">Type</th><th align="left" valign="top" id="idm140587100407008" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587100409184"> <p>
										<code class="literal cluster-admin">autoscaler</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100408096"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100407008"> <p>
										<code class="literal cluster-admin">autoscaler</code> spec of a horizontal pod autoscaler to set up for the plugin Deployment. Refer to HorizontalPodAutoscaler documentation (autoscaling/v2).
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100409184"> <p>
										<code class="literal cluster-admin">enable</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100408096"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100407008"> <p>
										enable the console plugin deployment. spec.Loki.enable must also be true
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100409184"> <p>
										<code class="literal cluster-admin">imagePullPolicy</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100408096"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100407008"> <p>
										<code class="literal cluster-admin">imagePullPolicy</code> is the Kubernetes pull policy for the image defined above
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100409184"> <p>
										<code class="literal cluster-admin">logLevel</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100408096"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100407008"> <p>
										<code class="literal cluster-admin">logLevel</code> for the console plugin backend
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100409184"> <p>
										<code class="literal cluster-admin">port</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100408096"> <p>
										<code class="literal cluster-admin">integer</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100407008"> <p>
										<code class="literal cluster-admin">port</code> is the plugin service port. Do not use 9002, which is reserved for metrics.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100409184"> <p>
										<code class="literal cluster-admin">portNaming</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100408096"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100407008"> <p>
										<code class="literal cluster-admin">portNaming</code> defines the configuration of the port-to-service name translation
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100409184"> <p>
										<code class="literal cluster-admin">quickFilters</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100408096"> <p>
										<code class="literal cluster-admin">array</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100407008"> <p>
										<code class="literal cluster-admin">quickFilters</code> configures quick filter presets for the Console plugin
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100409184"> <p>
										<code class="literal cluster-admin">register</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100408096"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100407008"> <p>
										<code class="literal cluster-admin">register</code> allows, when set to true, to automatically register the provided console plugin with the OpenShift Container Platform Console operator. When set to false, you can still register it manually by editing console.operator.openshift.io/cluster with the following command: <code class="literal cluster-admin">oc patch console.operator.openshift.io cluster --type='json' -p '[{"op": "add", "path": "/spec/plugins/-", "value": "netobserv-plugin"}]'</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100409184"> <p>
										<code class="literal cluster-admin">replicas</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100408096"> <p>
										<code class="literal cluster-admin">integer</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100407008"> <p>
										<code class="literal cluster-admin">replicas</code> defines the number of replicas (pods) to start.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100409184"> <p>
										<code class="literal cluster-admin">resources</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100408096"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100407008"> <p>
										<code class="literal cluster-admin">resources</code>, in terms of compute resources, required by this container. More info: <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a>
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-consoleplugin-autoscaler"><div class="titlepage"><div><div><h4 class="title">36.9.1.11. .spec.consolePlugin.autoscaler</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">autoscaler</code> spec of a horizontal pod autoscaler to set up for the plugin Deployment. Refer to HorizontalPodAutoscaler documentation (autoscaling/v2).
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div></section><section class="section cluster-admin" id="spec-consoleplugin-portnaming"><div class="titlepage"><div><div><h4 class="title">36.9.1.12. .spec.consolePlugin.portNaming</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">portNaming</code> defines the configuration of the port-to-service name translation
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587100304512" scope="col">Property</th><th align="left" valign="top" id="idm140587100303424" scope="col">Type</th><th align="left" valign="top" id="idm140587100302336" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587100304512"> <p>
										<code class="literal cluster-admin">enable</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100303424"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100302336"> <p>
										Enable the console plugin port-to-service name translation
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100304512"> <p>
										<code class="literal cluster-admin">portNames</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100303424"> <p>
										<code class="literal cluster-admin">object (string)</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100302336"> <p>
										<code class="literal cluster-admin">portNames</code> defines additional port names to use in the console, for example, <code class="literal cluster-admin">portNames: {"3100": "loki"}</code>.
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-consoleplugin-quickfilters"><div class="titlepage"><div><div><h4 class="title">36.9.1.13. .spec.consolePlugin.quickFilters</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">quickFilters</code> configures quick filter presets for the Console plugin
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">array</code>
								</dd></dl></div></section><section class="section cluster-admin" id="spec-consoleplugin-quickfilters-2"><div class="titlepage"><div><div><h4 class="title">36.9.1.14. .spec.consolePlugin.quickFilters[]</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">QuickFilter</code> defines preset configuration for Console’s quick filters
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd><dt><span class="term">Required</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">filter</code>
										</li><li class="listitem">
											<code class="literal cluster-admin">name</code>
										</li></ul></div></dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587100256240" scope="col">Property</th><th align="left" valign="top" id="idm140587100255152" scope="col">Type</th><th align="left" valign="top" id="idm140587100254064" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587100256240"> <p>
										<code class="literal cluster-admin">default</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100255152"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100254064"> <p>
										<code class="literal cluster-admin">default</code> defines whether this filter should be active by default or not
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100256240"> <p>
										<code class="literal cluster-admin">filter</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100255152"> <p>
										<code class="literal cluster-admin">object (string)</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100254064"> <p>
										<code class="literal cluster-admin">filter</code> is a set of keys and values to be set when this filter is selected. Each key can relate to a list of values using a coma-separated string, for example, <code class="literal cluster-admin">filter: {"src_namespace": "namespace1,namespace2"}</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100256240"> <p>
										<code class="literal cluster-admin">name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100255152"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100254064"> <p>
										Name of the filter, that is displayed in the Console
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-consoleplugin-resources"><div class="titlepage"><div><div><h4 class="title">36.9.1.15. .spec.consolePlugin.resources</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">resources</code>, in terms of compute resources, required by this container. More info: <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a>
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587100213744" scope="col">Property</th><th align="left" valign="top" id="idm140587100212656" scope="col">Type</th><th align="left" valign="top" id="idm140587100211568" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587100213744"> <p>
										<code class="literal cluster-admin">limits</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100212656"> <p>
										<code class="literal cluster-admin">integer-or-string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100211568"> <p>
										Limits describes the maximum amount of compute resources allowed. More info: <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100213744"> <p>
										<code class="literal cluster-admin">requests</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100212656"> <p>
										<code class="literal cluster-admin">integer-or-string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100211568"> <p>
										Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a>
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-exporters"><div class="titlepage"><div><div><h4 class="title">36.9.1.16. .spec.exporters</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">exporters</code> define additional optional exporters for custom consumption or storage.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">array</code>
								</dd></dl></div></section><section class="section cluster-admin" id="spec-exporters-2"><div class="titlepage"><div><div><h4 class="title">36.9.1.17. .spec.exporters[]</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">FlowCollectorExporter</code> defines an additional exporter to send enriched flows to.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd><dt><span class="term">Required</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">type</code>
										</li></ul></div></dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587100166992" scope="col">Property</th><th align="left" valign="top" id="idm140587100165904" scope="col">Type</th><th align="left" valign="top" id="idm140587100164816" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587100166992"> <p>
										<code class="literal cluster-admin">ipfix</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100165904"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100164816"> <p>
										IPFIX configuration, such as the IP address and port to send enriched IPFIX flows to.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100166992"> <p>
										<code class="literal cluster-admin">kafka</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100165904"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100164816"> <p>
										Kafka configuration, such as the address and topic, to send enriched flows to.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100166992"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100165904"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100164816"> <p>
										<code class="literal cluster-admin">type</code> selects the type of exporters. The available options are <code class="literal cluster-admin">KAFKA</code> and <code class="literal cluster-admin">IPFIX</code>.
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-exporters-ipfix"><div class="titlepage"><div><div><h4 class="title">36.9.1.18. .spec.exporters[].ipfix</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									IPFIX configuration, such as the IP address and port to send enriched IPFIX flows to.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd><dt><span class="term">Required</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">targetHost</code>
										</li><li class="listitem">
											<code class="literal cluster-admin">targetPort</code>
										</li></ul></div></dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587100120672" scope="col">Property</th><th align="left" valign="top" id="idm140587100119584" scope="col">Type</th><th align="left" valign="top" id="idm140587100118496" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587100120672"> <p>
										<code class="literal cluster-admin">targetHost</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100119584"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100118496"> <p>
										Address of the IPFIX external receiver
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100120672"> <p>
										<code class="literal cluster-admin">targetPort</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100119584"> <p>
										<code class="literal cluster-admin">integer</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100118496"> <p>
										Port for the IPFIX external receiver
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100120672"> <p>
										<code class="literal cluster-admin">transport</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100119584"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100118496"> <p>
										Transport protocol (<code class="literal cluster-admin">TCP</code> or <code class="literal cluster-admin">UDP</code>) to be used for the IPFIX connection, defaults to <code class="literal cluster-admin">TCP</code>.
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-exporters-kafka"><div class="titlepage"><div><div><h4 class="title">36.9.1.19. .spec.exporters[].kafka</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									Kafka configuration, such as the address and topic, to send enriched flows to.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd><dt><span class="term">Required</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">address</code>
										</li><li class="listitem">
											<code class="literal cluster-admin">topic</code>
										</li></ul></div></dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587100074448" scope="col">Property</th><th align="left" valign="top" id="idm140587100073360" scope="col">Type</th><th align="left" valign="top" id="idm140587100072272" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587100074448"> <p>
										<code class="literal cluster-admin">address</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100073360"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100072272"> <p>
										Address of the Kafka server
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100074448"> <p>
										<code class="literal cluster-admin">sasl</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100073360"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100072272"> <p>
										SASL authentication configuration. [Unsupported (*)].
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100074448"> <p>
										<code class="literal cluster-admin">tls</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100073360"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100072272"> <p>
										TLS client configuration. When using TLS, verify that the address matches the Kafka port used for TLS, generally 9093.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100074448"> <p>
										<code class="literal cluster-admin">topic</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100073360"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100072272"> <p>
										Kafka topic to use. It must exist. Network Observability does not create it.
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-exporters-kafka-sasl"><div class="titlepage"><div><div><h4 class="title">36.9.1.20. .spec.exporters[].kafka.sasl</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									SASL authentication configuration. [Unsupported (*)].
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587100029248" scope="col">Property</th><th align="left" valign="top" id="idm140587100028160" scope="col">Type</th><th align="left" valign="top" id="idm140587100027072" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587100029248"> <p>
										<code class="literal cluster-admin">clientIDReference</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100028160"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100027072"> <p>
										Reference to the secret or config map containing the client ID
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100029248"> <p>
										<code class="literal cluster-admin">clientSecretReference</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100028160"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100027072"> <p>
										Reference to the secret or config map containing the client secret
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587100029248"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100028160"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587100027072"> <p>
										Type of SASL authentication to use, or <code class="literal cluster-admin">DISABLED</code> if SASL is not used
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-exporters-kafka-sasl-clientidreference"><div class="titlepage"><div><div><h4 class="title">36.9.1.21. .spec.exporters[].kafka.sasl.clientIDReference</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									Reference to the secret or config map containing the client ID
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587099990064" scope="col">Property</th><th align="left" valign="top" id="idm140587099988976" scope="col">Type</th><th align="left" valign="top" id="idm140587099987888" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587099990064"> <p>
										<code class="literal cluster-admin">file</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099988976"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099987888"> <p>
										File name within the config map or secret
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099990064"> <p>
										<code class="literal cluster-admin">name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099988976"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099987888"> <p>
										Name of the config map or secret containing the file
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099990064"> <p>
										<code class="literal cluster-admin">namespace</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099988976"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099987888"> <p>
										Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where Network Observability is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099990064"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099988976"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099987888"> <p>
										Type for the file reference: "configmap" or "secret"
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-exporters-kafka-sasl-clientsecretreference"><div class="titlepage"><div><div><h4 class="title">36.9.1.22. .spec.exporters[].kafka.sasl.clientSecretReference</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									Reference to the secret or config map containing the client secret
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587099944640" scope="col">Property</th><th align="left" valign="top" id="idm140587099943552" scope="col">Type</th><th align="left" valign="top" id="idm140587099942464" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587099944640"> <p>
										<code class="literal cluster-admin">file</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099943552"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099942464"> <p>
										File name within the config map or secret
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099944640"> <p>
										<code class="literal cluster-admin">name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099943552"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099942464"> <p>
										Name of the config map or secret containing the file
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099944640"> <p>
										<code class="literal cluster-admin">namespace</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099943552"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099942464"> <p>
										Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where Network Observability is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099944640"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099943552"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099942464"> <p>
										Type for the file reference: "configmap" or "secret"
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-exporters-kafka-tls"><div class="titlepage"><div><div><h4 class="title">36.9.1.23. .spec.exporters[].kafka.tls</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									TLS client configuration. When using TLS, verify that the address matches the Kafka port used for TLS, generally 9093.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587099899232" scope="col">Property</th><th align="left" valign="top" id="idm140587099898144" scope="col">Type</th><th align="left" valign="top" id="idm140587099897056" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587099899232"> <p>
										<code class="literal cluster-admin">caCert</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099898144"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099897056"> <p>
										<code class="literal cluster-admin">caCert</code> defines the reference of the certificate for the Certificate Authority
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099899232"> <p>
										<code class="literal cluster-admin">enable</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099898144"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099897056"> <p>
										Enable TLS
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099899232"> <p>
										<code class="literal cluster-admin">insecureSkipVerify</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099898144"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099897056"> <p>
										<code class="literal cluster-admin">insecureSkipVerify</code> allows skipping client-side verification of the server certificate. If set to true, the <code class="literal cluster-admin">caCert</code> field is ignored.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099899232"> <p>
										<code class="literal cluster-admin">userCert</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099898144"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099897056"> <p>
										<code class="literal cluster-admin">userCert</code> defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-exporters-kafka-tls-cacert"><div class="titlepage"><div><div><h4 class="title">36.9.1.24. .spec.exporters[].kafka.tls.caCert</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">caCert</code> defines the reference of the certificate for the Certificate Authority
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587099850208" scope="col">Property</th><th align="left" valign="top" id="idm140587099849120" scope="col">Type</th><th align="left" valign="top" id="idm140587099848032" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587099850208"> <p>
										<code class="literal cluster-admin">certFile</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099849120"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099848032"> <p>
										<code class="literal cluster-admin">certFile</code> defines the path to the certificate file name within the config map or secret
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099850208"> <p>
										<code class="literal cluster-admin">certKey</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099849120"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099848032"> <p>
										<code class="literal cluster-admin">certKey</code> defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099850208"> <p>
										<code class="literal cluster-admin">name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099849120"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099848032"> <p>
										Name of the config map or secret containing certificates
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099850208"> <p>
										<code class="literal cluster-admin">namespace</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099849120"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099848032"> <p>
										Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where Network Observability is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099850208"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099849120"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099848032"> <p>
										Type for the certificate reference: <code class="literal cluster-admin">configmap</code> or <code class="literal cluster-admin">secret</code>
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-exporters-kafka-tls-usercert"><div class="titlepage"><div><div><h4 class="title">36.9.1.25. .spec.exporters[].kafka.tls.userCert</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">userCert</code> defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587099794032" scope="col">Property</th><th align="left" valign="top" id="idm140587099792944" scope="col">Type</th><th align="left" valign="top" id="idm140587099791856" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587099794032"> <p>
										<code class="literal cluster-admin">certFile</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099792944"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099791856"> <p>
										<code class="literal cluster-admin">certFile</code> defines the path to the certificate file name within the config map or secret
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099794032"> <p>
										<code class="literal cluster-admin">certKey</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099792944"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099791856"> <p>
										<code class="literal cluster-admin">certKey</code> defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099794032"> <p>
										<code class="literal cluster-admin">name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099792944"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099791856"> <p>
										Name of the config map or secret containing certificates
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099794032"> <p>
										<code class="literal cluster-admin">namespace</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099792944"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099791856"> <p>
										Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where Network Observability is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099794032"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099792944"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099791856"> <p>
										Type for the certificate reference: <code class="literal cluster-admin">configmap</code> or <code class="literal cluster-admin">secret</code>
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-kafka"><div class="titlepage"><div><div><h4 class="title">36.9.1.26. .spec.kafka</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									Kafka configuration, allowing to use Kafka as a broker as part of the flow collection pipeline. Available when the <code class="literal cluster-admin">spec.deploymentModel</code> is <code class="literal cluster-admin">KAFKA</code>.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd><dt><span class="term">Required</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">address</code>
										</li><li class="listitem">
											<code class="literal cluster-admin">topic</code>
										</li></ul></div></dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587099731488" scope="col">Property</th><th align="left" valign="top" id="idm140587099730400" scope="col">Type</th><th align="left" valign="top" id="idm140587099729312" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587099731488"> <p>
										<code class="literal cluster-admin">address</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099730400"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099729312"> <p>
										Address of the Kafka server
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099731488"> <p>
										<code class="literal cluster-admin">sasl</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099730400"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099729312"> <p>
										SASL authentication configuration. [Unsupported (*)].
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099731488"> <p>
										<code class="literal cluster-admin">tls</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099730400"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099729312"> <p>
										TLS client configuration. When using TLS, verify that the address matches the Kafka port used for TLS, generally 9093.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099731488"> <p>
										<code class="literal cluster-admin">topic</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099730400"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099729312"> <p>
										Kafka topic to use. It must exist, Network Observability does not create it.
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-kafka-sasl"><div class="titlepage"><div><div><h4 class="title">36.9.1.27. .spec.kafka.sasl</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									SASL authentication configuration. [Unsupported (*)].
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587099686432" scope="col">Property</th><th align="left" valign="top" id="idm140587099685344" scope="col">Type</th><th align="left" valign="top" id="idm140587099684256" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587099686432"> <p>
										<code class="literal cluster-admin">clientIDReference</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099685344"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099684256"> <p>
										Reference to the secret or config map containing the client ID
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099686432"> <p>
										<code class="literal cluster-admin">clientSecretReference</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099685344"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099684256"> <p>
										Reference to the secret or config map containing the client secret
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099686432"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099685344"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099684256"> <p>
										Type of SASL authentication to use, or <code class="literal cluster-admin">DISABLED</code> if SASL is not used
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-kafka-sasl-clientidreference"><div class="titlepage"><div><div><h4 class="title">36.9.1.28. .spec.kafka.sasl.clientIDReference</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									Reference to the secret or config map containing the client ID
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587099647392" scope="col">Property</th><th align="left" valign="top" id="idm140587099646304" scope="col">Type</th><th align="left" valign="top" id="idm140587099645216" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587099647392"> <p>
										<code class="literal cluster-admin">file</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099646304"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099645216"> <p>
										File name within the config map or secret
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099647392"> <p>
										<code class="literal cluster-admin">name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099646304"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099645216"> <p>
										Name of the config map or secret containing the file
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099647392"> <p>
										<code class="literal cluster-admin">namespace</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099646304"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099645216"> <p>
										Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where Network Observability is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099647392"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099646304"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099645216"> <p>
										Type for the file reference: "configmap" or "secret"
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-kafka-sasl-clientsecretreference"><div class="titlepage"><div><div><h4 class="title">36.9.1.29. .spec.kafka.sasl.clientSecretReference</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									Reference to the secret or config map containing the client secret
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587099602048" scope="col">Property</th><th align="left" valign="top" id="idm140587099600960" scope="col">Type</th><th align="left" valign="top" id="idm140587099599872" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587099602048"> <p>
										<code class="literal cluster-admin">file</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099600960"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099599872"> <p>
										File name within the config map or secret
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099602048"> <p>
										<code class="literal cluster-admin">name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099600960"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099599872"> <p>
										Name of the config map or secret containing the file
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099602048"> <p>
										<code class="literal cluster-admin">namespace</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099600960"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099599872"> <p>
										Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where Network Observability is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099602048"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099600960"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099599872"> <p>
										Type for the file reference: "configmap" or "secret"
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-kafka-tls"><div class="titlepage"><div><div><h4 class="title">36.9.1.30. .spec.kafka.tls</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									TLS client configuration. When using TLS, verify that the address matches the Kafka port used for TLS, generally 9093.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587099556720" scope="col">Property</th><th align="left" valign="top" id="idm140587099555632" scope="col">Type</th><th align="left" valign="top" id="idm140587099554544" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587099556720"> <p>
										<code class="literal cluster-admin">caCert</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099555632"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099554544"> <p>
										<code class="literal cluster-admin">caCert</code> defines the reference of the certificate for the Certificate Authority
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099556720"> <p>
										<code class="literal cluster-admin">enable</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099555632"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099554544"> <p>
										Enable TLS
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099556720"> <p>
										<code class="literal cluster-admin">insecureSkipVerify</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099555632"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099554544"> <p>
										<code class="literal cluster-admin">insecureSkipVerify</code> allows skipping client-side verification of the server certificate. If set to true, the <code class="literal cluster-admin">caCert</code> field is ignored.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099556720"> <p>
										<code class="literal cluster-admin">userCert</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099555632"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099554544"> <p>
										<code class="literal cluster-admin">userCert</code> defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-kafka-tls-cacert"><div class="titlepage"><div><div><h4 class="title">36.9.1.31. .spec.kafka.tls.caCert</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">caCert</code> defines the reference of the certificate for the Certificate Authority
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587099507712" scope="col">Property</th><th align="left" valign="top" id="idm140587099506624" scope="col">Type</th><th align="left" valign="top" id="idm140587099505536" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587099507712"> <p>
										<code class="literal cluster-admin">certFile</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099506624"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099505536"> <p>
										<code class="literal cluster-admin">certFile</code> defines the path to the certificate file name within the config map or secret
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099507712"> <p>
										<code class="literal cluster-admin">certKey</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099506624"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099505536"> <p>
										<code class="literal cluster-admin">certKey</code> defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099507712"> <p>
										<code class="literal cluster-admin">name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099506624"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099505536"> <p>
										Name of the config map or secret containing certificates
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099507712"> <p>
										<code class="literal cluster-admin">namespace</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099506624"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099505536"> <p>
										Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where Network Observability is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099507712"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099506624"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099505536"> <p>
										Type for the certificate reference: <code class="literal cluster-admin">configmap</code> or <code class="literal cluster-admin">secret</code>
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-kafka-tls-usercert"><div class="titlepage"><div><div><h4 class="title">36.9.1.32. .spec.kafka.tls.userCert</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">userCert</code> defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587099451584" scope="col">Property</th><th align="left" valign="top" id="idm140587099450496" scope="col">Type</th><th align="left" valign="top" id="idm140587099449408" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587099451584"> <p>
										<code class="literal cluster-admin">certFile</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099450496"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099449408"> <p>
										<code class="literal cluster-admin">certFile</code> defines the path to the certificate file name within the config map or secret
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099451584"> <p>
										<code class="literal cluster-admin">certKey</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099450496"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099449408"> <p>
										<code class="literal cluster-admin">certKey</code> defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099451584"> <p>
										<code class="literal cluster-admin">name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099450496"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099449408"> <p>
										Name of the config map or secret containing certificates
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099451584"> <p>
										<code class="literal cluster-admin">namespace</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099450496"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099449408"> <p>
										Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where Network Observability is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099451584"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099450496"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099449408"> <p>
										Type for the certificate reference: <code class="literal cluster-admin">configmap</code> or <code class="literal cluster-admin">secret</code>
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-loki"><div class="titlepage"><div><div><h4 class="title">36.9.1.33. .spec.loki</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									Loki, the flow store, client settings.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587099396384" scope="col">Property</th><th align="left" valign="top" id="idm140587099395296" scope="col">Type</th><th align="left" valign="top" id="idm140587099394208" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587099396384"> <p>
										<code class="literal cluster-admin">authToken</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099395296"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099394208"> <p>
										<code class="literal cluster-admin">authToken</code> describes the way to get a token to authenticate to Loki.<br/> - <code class="literal cluster-admin">DISABLED</code> does not send any token with the request.<br/> - <code class="literal cluster-admin">FORWARD</code> forwards the user token for authorization.<br/> - <code class="literal cluster-admin">HOST</code> [deprecated (*)] - uses the local pod service account to authenticate to Loki.<br/> When using the Loki Operator, this must be set to <code class="literal cluster-admin">FORWARD</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099396384"> <p>
										<code class="literal cluster-admin">batchSize</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099395296"> <p>
										<code class="literal cluster-admin">integer</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099394208"> <p>
										<code class="literal cluster-admin">batchSize</code> is the maximum batch size (in bytes) of logs to accumulate before sending.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099396384"> <p>
										<code class="literal cluster-admin">batchWait</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099395296"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099394208"> <p>
										<code class="literal cluster-admin">batchWait</code> is the maximum time to wait before sending a batch.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099396384"> <p>
										<code class="literal cluster-admin">enable</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099395296"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099394208"> <p>
										Set to <code class="literal cluster-admin">enable</code> to store flows to Loki. It is required for the OpenShift Container Platform Console plugin installation.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099396384"> <p>
										<code class="literal cluster-admin">maxBackoff</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099395296"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099394208"> <p>
										<code class="literal cluster-admin">maxBackoff</code> is the maximum backoff time for client connection between retries.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099396384"> <p>
										<code class="literal cluster-admin">maxRetries</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099395296"> <p>
										<code class="literal cluster-admin">integer</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099394208"> <p>
										<code class="literal cluster-admin">maxRetries</code> is the maximum number of retries for client connections.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099396384"> <p>
										<code class="literal cluster-admin">minBackoff</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099395296"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099394208"> <p>
										<code class="literal cluster-admin">minBackoff</code> is the initial backoff time for client connection between retries.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099396384"> <p>
										<code class="literal cluster-admin">querierUrl</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099395296"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099394208"> <p>
										<code class="literal cluster-admin">querierURL</code> specifies the address of the Loki querier service, in case it is different from the Loki ingester URL. If empty, the URL value is used (assuming that the Loki ingester and querier are in the same server). When using the Loki Operator, do not set it, since ingestion and queries use the Loki gateway.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099396384"> <p>
										<code class="literal cluster-admin">staticLabels</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099395296"> <p>
										<code class="literal cluster-admin">object (string)</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099394208"> <p>
										<code class="literal cluster-admin">staticLabels</code> is a map of common labels to set on each flow.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099396384"> <p>
										<code class="literal cluster-admin">statusTls</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099395296"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099394208"> <p>
										TLS client configuration for Loki status URL.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099396384"> <p>
										<code class="literal cluster-admin">statusUrl</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099395296"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099394208"> <p>
										<code class="literal cluster-admin">statusURL</code> specifies the address of the Loki <code class="literal cluster-admin">/ready</code>, <code class="literal cluster-admin">/metrics</code> and <code class="literal cluster-admin">/config</code> endpoints, in case it is different from the Loki querier URL. If empty, the <code class="literal cluster-admin">querierURL</code> value is used. This is useful to show error messages and some context in the frontend. When using the Loki Operator, set it to the Loki HTTP query frontend service, for example <a class="link" href="https://loki-query-frontend-http.netobserv.svc:3100/">https://loki-query-frontend-http.netobserv.svc:3100/</a>. <code class="literal cluster-admin">statusTLS</code> configuration is used when <code class="literal cluster-admin">statusUrl</code> is set.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099396384"> <p>
										<code class="literal cluster-admin">tenantID</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099395296"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099394208"> <p>
										<code class="literal cluster-admin">tenantID</code> is the Loki <code class="literal cluster-admin">X-Scope-OrgID</code> that identifies the tenant for each request. When using the Loki Operator, set it to <code class="literal cluster-admin">network</code>, which corresponds to a special tenant mode.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099396384"> <p>
										<code class="literal cluster-admin">timeout</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099395296"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099394208"> <p>
										<code class="literal cluster-admin">timeout</code> is the maximum time connection / request limit. A timeout of zero means no timeout.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099396384"> <p>
										<code class="literal cluster-admin">tls</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099395296"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099394208"> <p>
										TLS client configuration for Loki URL.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099396384"> <p>
										<code class="literal cluster-admin">url</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099395296"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099394208"> <p>
										<code class="literal cluster-admin">url</code> is the address of an existing Loki service to push the flows to. When using the Loki Operator, set it to the Loki gateway service with the <code class="literal cluster-admin">network</code> tenant set in path, for example <a class="link" href="https://loki-gateway-http.netobserv.svc:8080/api/logs/v1/network">https://loki-gateway-http.netobserv.svc:8080/api/logs/v1/network</a>.
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-loki-statustls"><div class="titlepage"><div><div><h4 class="title">36.9.1.34. .spec.loki.statusTls</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									TLS client configuration for Loki status URL.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587099252192" scope="col">Property</th><th align="left" valign="top" id="idm140587099251104" scope="col">Type</th><th align="left" valign="top" id="idm140587099250016" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587099252192"> <p>
										<code class="literal cluster-admin">caCert</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099251104"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099250016"> <p>
										<code class="literal cluster-admin">caCert</code> defines the reference of the certificate for the Certificate Authority
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099252192"> <p>
										<code class="literal cluster-admin">enable</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099251104"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099250016"> <p>
										Enable TLS
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099252192"> <p>
										<code class="literal cluster-admin">insecureSkipVerify</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099251104"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099250016"> <p>
										<code class="literal cluster-admin">insecureSkipVerify</code> allows skipping client-side verification of the server certificate. If set to true, the <code class="literal cluster-admin">caCert</code> field is ignored.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099252192"> <p>
										<code class="literal cluster-admin">userCert</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099251104"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099250016"> <p>
										<code class="literal cluster-admin">userCert</code> defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-loki-statustls-cacert"><div class="titlepage"><div><div><h4 class="title">36.9.1.35. .spec.loki.statusTls.caCert</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">caCert</code> defines the reference of the certificate for the Certificate Authority
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587099203168" scope="col">Property</th><th align="left" valign="top" id="idm140587099202080" scope="col">Type</th><th align="left" valign="top" id="idm140587099200992" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587099203168"> <p>
										<code class="literal cluster-admin">certFile</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099202080"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099200992"> <p>
										<code class="literal cluster-admin">certFile</code> defines the path to the certificate file name within the config map or secret
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099203168"> <p>
										<code class="literal cluster-admin">certKey</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099202080"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099200992"> <p>
										<code class="literal cluster-admin">certKey</code> defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099203168"> <p>
										<code class="literal cluster-admin">name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099202080"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099200992"> <p>
										Name of the config map or secret containing certificates
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099203168"> <p>
										<code class="literal cluster-admin">namespace</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099202080"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099200992"> <p>
										Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where Network Observability is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099203168"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099202080"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099200992"> <p>
										Type for the certificate reference: <code class="literal cluster-admin">configmap</code> or <code class="literal cluster-admin">secret</code>
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-loki-statustls-usercert"><div class="titlepage"><div><div><h4 class="title">36.9.1.36. .spec.loki.statusTls.userCert</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">userCert</code> defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587099146992" scope="col">Property</th><th align="left" valign="top" id="idm140587099145904" scope="col">Type</th><th align="left" valign="top" id="idm140587099144816" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587099146992"> <p>
										<code class="literal cluster-admin">certFile</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099145904"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099144816"> <p>
										<code class="literal cluster-admin">certFile</code> defines the path to the certificate file name within the config map or secret
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099146992"> <p>
										<code class="literal cluster-admin">certKey</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099145904"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099144816"> <p>
										<code class="literal cluster-admin">certKey</code> defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099146992"> <p>
										<code class="literal cluster-admin">name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099145904"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099144816"> <p>
										Name of the config map or secret containing certificates
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099146992"> <p>
										<code class="literal cluster-admin">namespace</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099145904"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099144816"> <p>
										Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where Network Observability is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099146992"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099145904"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099144816"> <p>
										Type for the certificate reference: <code class="literal cluster-admin">configmap</code> or <code class="literal cluster-admin">secret</code>
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-loki-tls"><div class="titlepage"><div><div><h4 class="title">36.9.1.37. .spec.loki.tls</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									TLS client configuration for Loki URL.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587099091792" scope="col">Property</th><th align="left" valign="top" id="idm140587099090704" scope="col">Type</th><th align="left" valign="top" id="idm140587099089616" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587099091792"> <p>
										<code class="literal cluster-admin">caCert</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099090704"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099089616"> <p>
										<code class="literal cluster-admin">caCert</code> defines the reference of the certificate for the Certificate Authority
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099091792"> <p>
										<code class="literal cluster-admin">enable</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099090704"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099089616"> <p>
										Enable TLS
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099091792"> <p>
										<code class="literal cluster-admin">insecureSkipVerify</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099090704"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099089616"> <p>
										<code class="literal cluster-admin">insecureSkipVerify</code> allows skipping client-side verification of the server certificate. If set to true, the <code class="literal cluster-admin">caCert</code> field is ignored.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099091792"> <p>
										<code class="literal cluster-admin">userCert</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099090704"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099089616"> <p>
										<code class="literal cluster-admin">userCert</code> defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-loki-tls-cacert"><div class="titlepage"><div><div><h4 class="title">36.9.1.38. .spec.loki.tls.caCert</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">caCert</code> defines the reference of the certificate for the Certificate Authority
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587099042784" scope="col">Property</th><th align="left" valign="top" id="idm140587099041696" scope="col">Type</th><th align="left" valign="top" id="idm140587099040608" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587099042784"> <p>
										<code class="literal cluster-admin">certFile</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099041696"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099040608"> <p>
										<code class="literal cluster-admin">certFile</code> defines the path to the certificate file name within the config map or secret
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099042784"> <p>
										<code class="literal cluster-admin">certKey</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099041696"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099040608"> <p>
										<code class="literal cluster-admin">certKey</code> defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099042784"> <p>
										<code class="literal cluster-admin">name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099041696"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099040608"> <p>
										Name of the config map or secret containing certificates
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099042784"> <p>
										<code class="literal cluster-admin">namespace</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099041696"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099040608"> <p>
										Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where Network Observability is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587099042784"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099041696"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587099040608"> <p>
										Type for the certificate reference: <code class="literal cluster-admin">configmap</code> or <code class="literal cluster-admin">secret</code>
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-loki-tls-usercert"><div class="titlepage"><div><div><h4 class="title">36.9.1.39. .spec.loki.tls.userCert</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">userCert</code> defines the user certificate reference and is used for mTLS (you can ignore it when using one-way TLS)
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587098986736" scope="col">Property</th><th align="left" valign="top" id="idm140587098985648" scope="col">Type</th><th align="left" valign="top" id="idm140587098984560" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587098986736"> <p>
										<code class="literal cluster-admin">certFile</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098985648"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098984560"> <p>
										<code class="literal cluster-admin">certFile</code> defines the path to the certificate file name within the config map or secret
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098986736"> <p>
										<code class="literal cluster-admin">certKey</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098985648"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098984560"> <p>
										<code class="literal cluster-admin">certKey</code> defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098986736"> <p>
										<code class="literal cluster-admin">name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098985648"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098984560"> <p>
										Name of the config map or secret containing certificates
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098986736"> <p>
										<code class="literal cluster-admin">namespace</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098985648"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098984560"> <p>
										Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where Network Observability is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098986736"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098985648"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098984560"> <p>
										Type for the certificate reference: <code class="literal cluster-admin">configmap</code> or <code class="literal cluster-admin">secret</code>
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-processor"><div class="titlepage"><div><div><h4 class="title">36.9.1.40. .spec.processor</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">processor</code> defines the settings of the component that receives the flows from the agent, enriches them, generates metrics, and forwards them to the Loki persistence layer and/or any available exporter.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587098930544" scope="col">Property</th><th align="left" valign="top" id="idm140587098929456" scope="col">Type</th><th align="left" valign="top" id="idm140587098928368" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">clusterName</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										<code class="literal cluster-admin">clusterName</code> is the name of the cluster to appear in the flows data. This is useful in a multi-cluster context. When using OpenShift Container Platform, leave empty to make it automatically determined.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">conversationEndTimeout</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										<code class="literal cluster-admin">conversationEndTimeout</code> is the time to wait after a network flow is received, to consider the conversation ended. This delay is ignored when a FIN packet is collected for TCP flows (see <code class="literal cluster-admin">conversationTerminatingTimeout</code> instead).
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">conversationHeartbeatInterval</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										<code class="literal cluster-admin">conversationHeartbeatInterval</code> is the time to wait between "tick" events of a conversation
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">conversationTerminatingTimeout</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										<code class="literal cluster-admin">conversationTerminatingTimeout</code> is the time to wait from detected FIN flag to end a conversation. Only relevant for TCP flows.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">debug</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										<code class="literal cluster-admin">debug</code> allows setting some aspects of the internal configuration of the flow processor. This section is aimed exclusively for debugging and fine-grained performance optimizations, such as GOGC and GOMAXPROCS env vars. Users setting its values do it at their own risk.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">dropUnusedFields</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										<code class="literal cluster-admin">dropUnusedFields</code> allows, when set to true, to drop fields that are known to be unused by OVS, to save storage space.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">enableKubeProbes</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										<code class="literal cluster-admin">enableKubeProbes</code> is a flag to enable or disable Kubernetes liveness and readiness probes
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">healthPort</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">integer</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										<code class="literal cluster-admin">healthPort</code> is a collector HTTP port in the Pod that exposes the health check API
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">imagePullPolicy</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										<code class="literal cluster-admin">imagePullPolicy</code> is the Kubernetes pull policy for the image defined above
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">kafkaConsumerAutoscaler</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										<code class="literal cluster-admin">kafkaConsumerAutoscaler</code> is the spec of a horizontal pod autoscaler to set up for <code class="literal cluster-admin">flowlogs-pipeline-transformer</code>, which consumes Kafka messages. This setting is ignored when Kafka is disabled. Refer to HorizontalPodAutoscaler documentation (autoscaling/v2).
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">kafkaConsumerBatchSize</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">integer</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										<code class="literal cluster-admin">kafkaConsumerBatchSize</code> indicates to the broker the maximum batch size, in bytes, that the consumer accepts. Ignored when not using Kafka. Default: 10MB.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">kafkaConsumerQueueCapacity</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">integer</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										<code class="literal cluster-admin">kafkaConsumerQueueCapacity</code> defines the capacity of the internal message queue used in the Kafka consumer client. Ignored when not using Kafka.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">kafkaConsumerReplicas</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">integer</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										<code class="literal cluster-admin">kafkaConsumerReplicas</code> defines the number of replicas (pods) to start for <code class="literal cluster-admin">flowlogs-pipeline-transformer</code>, which consumes Kafka messages. This setting is ignored when Kafka is disabled.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">logLevel</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										<code class="literal cluster-admin">logLevel</code> of the processor runtime
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">logTypes</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										<code class="literal cluster-admin">logTypes</code> defines the desired record types to generate. Possible values are:<br/> - <code class="literal cluster-admin">FLOWS</code> (default) to export regular network flows<br/> - <code class="literal cluster-admin">CONVERSATIONS</code> to generate events for started conversations, ended conversations as well as periodic "tick" updates<br/> - <code class="literal cluster-admin">ENDED_CONVERSATIONS</code> to generate only ended conversations events<br/> - <code class="literal cluster-admin">ALL</code> to generate both network flows and all conversations events<br/>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">metrics</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										<code class="literal cluster-admin">Metrics</code> define the processor configuration regarding metrics
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">port</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">integer</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										Port of the flow collector (host port). By convention, some values are forbidden. It must be greater than 1024 and different from 4500, 4789 and 6081.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">profilePort</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">integer</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										<code class="literal cluster-admin">profilePort</code> allows setting up a Go pprof profiler listening to this port
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098930544"> <p>
										<code class="literal cluster-admin">resources</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098929456"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098928368"> <p>
										<code class="literal cluster-admin">resources</code> are the compute resources required by this container. More info: <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a>
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-processor-debug"><div class="titlepage"><div><div><h4 class="title">36.9.1.41. .spec.processor.debug</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">debug</code> allows setting some aspects of the internal configuration of the flow processor. This section is aimed exclusively for debugging and fine-grained performance optimizations, such as GOGC and GOMAXPROCS env vars. Users setting its values do it at their own risk.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587098758960" scope="col">Property</th><th align="left" valign="top" id="idm140587098757872" scope="col">Type</th><th align="left" valign="top" id="idm140587098756784" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587098758960"> <p>
										<code class="literal cluster-admin">env</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098757872"> <p>
										<code class="literal cluster-admin">object (string)</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098756784"> <p>
										<code class="literal cluster-admin">env</code> allows passing custom environment variables to underlying components. Useful for passing some very concrete performance-tuning options, such as GOGC and GOMAXPROCS, that should not be publicly exposed as part of the FlowCollector descriptor, as they are only useful in edge debug or support scenarios.
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-processor-kafkaconsumerautoscaler"><div class="titlepage"><div><div><h4 class="title">36.9.1.42. .spec.processor.kafkaConsumerAutoscaler</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">kafkaConsumerAutoscaler</code> is the spec of a horizontal pod autoscaler to set up for <code class="literal cluster-admin">flowlogs-pipeline-transformer</code>, which consumes Kafka messages. This setting is ignored when Kafka is disabled. Refer to HorizontalPodAutoscaler documentation (autoscaling/v2).
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div></section><section class="section cluster-admin" id="spec-processor-metrics"><div class="titlepage"><div><div><h4 class="title">36.9.1.43. .spec.processor.metrics</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">Metrics</code> define the processor configuration regarding metrics
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587098723008" scope="col">Property</th><th align="left" valign="top" id="idm140587098721920" scope="col">Type</th><th align="left" valign="top" id="idm140587098720832" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587098723008"> <p>
										<code class="literal cluster-admin">disableAlerts</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098721920"> <p>
										<code class="literal cluster-admin">array (string)</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098720832"> <p>
										<code class="literal cluster-admin">disableAlerts</code> is a list of alerts that should be disabled. Possible values are:<br/> <code class="literal cluster-admin">NetObservNoFlows</code>, which is triggered when no flows are being observed for a certain period.<br/> <code class="literal cluster-admin">NetObservLokiError</code>, which is triggered when flows are being dropped due to Loki errors.<br/>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098723008"> <p>
										<code class="literal cluster-admin">ignoreTags</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098721920"> <p>
										<code class="literal cluster-admin">array (string)</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098720832"> <p>
										<code class="literal cluster-admin">ignoreTags</code> is a list of tags to specify which metrics to ignore. Each metric is associated with a list of tags. More details in <a class="link" href="https://github.com/netobserv/network-observability-operator/tree/main/controllers/flowlogspipeline/metrics_definitions">https://github.com/netobserv/network-observability-operator/tree/main/controllers/flowlogspipeline/metrics_definitions</a> . Available tags are: <code class="literal cluster-admin">egress</code>, <code class="literal cluster-admin">ingress</code>, <code class="literal cluster-admin">flows</code>, <code class="literal cluster-admin">bytes</code>, <code class="literal cluster-admin">packets</code>, <code class="literal cluster-admin">namespaces</code>, <code class="literal cluster-admin">nodes</code>, <code class="literal cluster-admin">workloads</code>, <code class="literal cluster-admin">nodes-flows</code>, <code class="literal cluster-admin">namespaces-flows</code>, <code class="literal cluster-admin">workloads-flows</code>. Namespace-based metrics are covered by both <code class="literal cluster-admin">workloads</code> and <code class="literal cluster-admin">namespaces</code> tags, hence it is recommended to always ignore one of them (<code class="literal cluster-admin">workloads</code> offering a finer granularity).
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098723008"> <p>
										<code class="literal cluster-admin">server</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098721920"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098720832"> <p>
										Metrics server endpoint configuration for Prometheus scraper
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-processor-metrics-server"><div class="titlepage"><div><div><h4 class="title">36.9.1.44. .spec.processor.metrics.server</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									Metrics server endpoint configuration for Prometheus scraper
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587098668352" scope="col">Property</th><th align="left" valign="top" id="idm140587098667264" scope="col">Type</th><th align="left" valign="top" id="idm140587098666176" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587098668352"> <p>
										<code class="literal cluster-admin">port</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098667264"> <p>
										<code class="literal cluster-admin">integer</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098666176"> <p>
										The prometheus HTTP port
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098668352"> <p>
										<code class="literal cluster-admin">tls</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098667264"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098666176"> <p>
										TLS configuration.
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-processor-metrics-server-tls"><div class="titlepage"><div><div><h4 class="title">36.9.1.45. .spec.processor.metrics.server.tls</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									TLS configuration.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587098636960" scope="col">Property</th><th align="left" valign="top" id="idm140587098635872" scope="col">Type</th><th align="left" valign="top" id="idm140587098634784" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587098636960"> <p>
										<code class="literal cluster-admin">insecureSkipVerify</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098635872"> <p>
										<code class="literal cluster-admin">boolean</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098634784"> <p>
										<code class="literal cluster-admin">insecureSkipVerify</code> allows skipping client-side verification of the provided certificate. If set to true, the <code class="literal cluster-admin">providedCaFile</code> field is ignored.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098636960"> <p>
										<code class="literal cluster-admin">provided</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098635872"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098634784"> <p>
										TLS configuration when <code class="literal cluster-admin">type</code> is set to <code class="literal cluster-admin">PROVIDED</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098636960"> <p>
										<code class="literal cluster-admin">providedCaFile</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098635872"> <p>
										<code class="literal cluster-admin">object</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098634784"> <p>
										Reference to the CA file when <code class="literal cluster-admin">type</code> is set to <code class="literal cluster-admin">PROVIDED</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098636960"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098635872"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098634784"> <p>
										Select the type of TLS configuration:<br/> - <code class="literal cluster-admin">DISABLED</code> (default) to not configure TLS for the endpoint. - <code class="literal cluster-admin">PROVIDED</code> to manually provide cert file and a key file. - <code class="literal cluster-admin">AUTO</code> to use OpenShift Container Platform auto generated certificate using annotations.
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-processor-metrics-server-tls-provided"><div class="titlepage"><div><div><h4 class="title">36.9.1.46. .spec.processor.metrics.server.tls.provided</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									TLS configuration when <code class="literal cluster-admin">type</code> is set to <code class="literal cluster-admin">PROVIDED</code>.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587098582736" scope="col">Property</th><th align="left" valign="top" id="idm140587098581648" scope="col">Type</th><th align="left" valign="top" id="idm140587098580560" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587098582736"> <p>
										<code class="literal cluster-admin">certFile</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098581648"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098580560"> <p>
										<code class="literal cluster-admin">certFile</code> defines the path to the certificate file name within the config map or secret
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098582736"> <p>
										<code class="literal cluster-admin">certKey</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098581648"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098580560"> <p>
										<code class="literal cluster-admin">certKey</code> defines the path to the certificate private key file name within the config map or secret. Omit when the key is not necessary.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098582736"> <p>
										<code class="literal cluster-admin">name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098581648"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098580560"> <p>
										Name of the config map or secret containing certificates
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098582736"> <p>
										<code class="literal cluster-admin">namespace</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098581648"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098580560"> <p>
										Namespace of the config map or secret containing certificates. If omitted, the default is to use the same namespace as where Network Observability is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098582736"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098581648"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098580560"> <p>
										Type for the certificate reference: <code class="literal cluster-admin">configmap</code> or <code class="literal cluster-admin">secret</code>
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-processor-metrics-server-tls-providedcafile"><div class="titlepage"><div><div><h4 class="title">36.9.1.47. .spec.processor.metrics.server.tls.providedCaFile</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									Reference to the CA file when <code class="literal cluster-admin">type</code> is set to <code class="literal cluster-admin">PROVIDED</code>.
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587098525760" scope="col">Property</th><th align="left" valign="top" id="idm140587098524672" scope="col">Type</th><th align="left" valign="top" id="idm140587098523584" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587098525760"> <p>
										<code class="literal cluster-admin">file</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098524672"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098523584"> <p>
										File name within the config map or secret
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098525760"> <p>
										<code class="literal cluster-admin">name</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098524672"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098523584"> <p>
										Name of the config map or secret containing the file
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098525760"> <p>
										<code class="literal cluster-admin">namespace</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098524672"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098523584"> <p>
										Namespace of the config map or secret containing the file. If omitted, the default is to use the same namespace as where Network Observability is deployed. If the namespace is different, the config map or the secret is copied so that it can be mounted as required.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098525760"> <p>
										<code class="literal cluster-admin">type</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098524672"> <p>
										<code class="literal cluster-admin">string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098523584"> <p>
										Type for the file reference: "configmap" or "secret"
									</p>
									 </td></tr></tbody></table></div></section><section class="section cluster-admin" id="spec-processor-resources"><div class="titlepage"><div><div><h4 class="title">36.9.1.48. .spec.processor.resources</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Description</span></dt><dd>
									<code class="literal cluster-admin">resources</code> are the compute resources required by this container. More info: <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a>
								</dd><dt><span class="term">Type</span></dt><dd>
									<code class="literal cluster-admin">object</code>
								</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows cluster-admin"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140587098478736" scope="col">Property</th><th align="left" valign="top" id="idm140587098477648" scope="col">Type</th><th align="left" valign="top" id="idm140587098476560" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140587098478736"> <p>
										<code class="literal cluster-admin">limits</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098477648"> <p>
										<code class="literal cluster-admin">integer-or-string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098476560"> <p>
										Limits describes the maximum amount of compute resources allowed. More info: <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm140587098478736"> <p>
										<code class="literal cluster-admin">requests</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098477648"> <p>
										<code class="literal cluster-admin">integer-or-string</code>
									</p>
									 </td><td align="left" valign="top" headers="idm140587098476560"> <p>
										Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: <a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a>
									</p>
									 </td></tr></tbody></table></div></section></section></section><section class="section cluster-admin" id="json-flows-format-reference"><div class="titlepage"><div><div><h2 class="title">36.10. Network flows format reference</h2></div></div></div><p class="cluster-admin cluster-admin">
				These are the specifications for network flows format, used both internally and when exporting flows to Kafka.
			</p><section class="section cluster-admin" id="network-observability-flows-format_json_reference"><div class="titlepage"><div><div><h3 class="title">36.10.1. Network Flows format reference</h3></div></div></div><p class="cluster-admin cluster-admin">
					This is the specification of the network flows format, used both internally and when exporting flows to Kafka.
				</p><p class="cluster-admin cluster-admin">
					The document is organized in two main categories: <span class="emphasis"><em><span class="cluster-admin cluster-admin">Labels</span></em></span> and regular <span class="emphasis"><em><span class="cluster-admin cluster-admin">Fields</span></em></span>. This distinction only matters when querying Loki. This is because <span class="emphasis"><em><span class="cluster-admin cluster-admin">Labels</span></em></span>, unlike <span class="emphasis"><em><span class="cluster-admin cluster-admin">Fields</span></em></span>, must be used in <a class="link" href="https://grafana.com/docs/loki/latest/logql/log_queries/#log-stream-selector">stream selectors</a>.
				</p><p class="cluster-admin cluster-admin">
					If you are reading this specification as a reference for the Kafka export feature, you must treat all <span class="emphasis"><em><span class="cluster-admin cluster-admin">Labels</span></em></span> and <span class="emphasis"><em><span class="cluster-admin cluster-admin">Fields</span></em></span> as regular fields and ignore any distinctions between them that are specific to Loki.
				</p><section class="section cluster-admin" id="labels"><div class="titlepage"><div><div><h4 class="title">36.10.1.1. Labels</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">SrcK8S_Namespace</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">SrcK8S_Namespace</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Source namespace
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">DstK8S_Namespace</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">DstK8S_Namespace</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Destination namespace
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">SrcK8S_OwnerName</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">SrcK8S_OwnerName</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Source owner, such as Deployment, StatefulSet, etc.
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">DstK8S_OwnerName</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">DstK8S_OwnerName</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Destination owner, such as Deployment, StatefulSet, etc.
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">FlowDirection</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">FlowDirection</span></strong></span>: <code class="literal cluster-admin">FlowDirection</code> (see the following section, Enumeration: FlowDirection)
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Flow direction from the node observation point
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">_RecordType</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">_RecordType</span></strong></span>: <code class="literal cluster-admin">RecordType</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Type of record: 'flowLog' for regular flow logs, or 'allConnections', 'newConnection', 'heartbeat', 'endConnection' for conversation tracking
					</p></section><section class="section cluster-admin" id="fields"><div class="titlepage"><div><div><h4 class="title">36.10.1.2. Fields</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">SrcAddr</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">SrcAddr</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Source IP address (ipv4 or ipv6)
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">DstAddr</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">DstAddr</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Destination IP address (ipv4 or ipv6)
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">SrcMac</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">SrcMac</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Source MAC address
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">DstMac</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">DstMac</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Destination MAC address
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">SrcK8S_Name</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">SrcK8S_Name</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Name of the source matched Kubernetes object, such as Pod name, Service name, etc.
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">DstK8S_Name</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">DstK8S_Name</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Name of the destination matched Kubernetes object, such as Pod name, Service name, etc.
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">SrcK8S_Type</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">SrcK8S_Type</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Kind of the source matched Kubernetes object, such as Pod, Service, etc.
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">DstK8S_Type</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">DstK8S_Type</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Kind of the destination matched Kubernetes object, such as Pod name, Service name, etc.
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">SrcPort</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">SrcPort</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Source port
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">DstPort</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">DstPort</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Destination port
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">SrcK8S_OwnerType</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">SrcK8S_OwnerType</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Kind of the source Kubernetes owner, such as Deployment, StatefulSet, etc.
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">DstK8S_OwnerType</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">DstK8S_OwnerType</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Kind of the destination Kubernetes owner, such as Deployment, StatefulSet, etc.
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">SrcK8S_HostIP</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">SrcK8S_HostIP</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Source node IP
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">DstK8S_HostIP</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">DstK8S_HostIP</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Destination node IP
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">SrcK8S_HostName</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">SrcK8S_HostName</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Source node name
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">DstK8S_HostName</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">DstK8S_HostName</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Destination node name
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Proto</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Proto</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						L4 protocol
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Interface</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Interface</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Network interface
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">IfDirection</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">IfDirection</span></strong></span>: <code class="literal cluster-admin">InterfaceDirection</code> (see the following section, Enumeration: InterfaceDirection)
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Flow direction from the network interface observation point
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Flags</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Flags</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						TCP flags
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Packets</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Packets</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Number of packets
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Packets_AB</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Packets_AB</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						In conversation tracking, A to B packets counter per conversation
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Packets_BA</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Packets_BA</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						In conversation tracking, B to A packets counter per conversation
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Bytes</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Bytes</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Number of bytes
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Bytes_AB</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Bytes_AB</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						In conversation tracking, A to B bytes counter per conversation
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Bytes_BA</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Bytes_BA</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						In conversation tracking, B to A bytes counter per conversation
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">IcmpType</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">IcmpType</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						ICMP type
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">IcmpCode</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">IcmpCode</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						ICMP code
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">PktDropLatestState</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">PktDropLatestState</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Pkt TCP state for drops
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">PktDropLatestDropCause</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">PktDropLatestDropCause</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Pkt cause for drops
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">PktDropLatestFlags</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">PktDropLatestFlags</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Pkt TCP flags for drops
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">PktDropPackets</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">PktDropPackets</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Number of packets dropped by the kernel
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">PktDropPackets_AB</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">PktDropPackets_AB</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						In conversation tracking, A to B packets dropped counter per conversation
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">PktDropPackets_BA</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">PktDropPackets_BA</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						In conversation tracking, B to A packets dropped counter per conversation
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">PktDropBytes</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">PktDropBytes</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Number of bytes dropped by the kernel
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">PktDropBytes_AB</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">PktDropBytes_AB</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						In conversation tracking, A to B bytes dropped counter per conversation
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">PktDropBytes_BA</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">PktDropBytes_BA</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						In conversation tracking, B to A bytes dropped counter per conversation
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">DnsId</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">DnsId</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						DNS record id
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">DnsFlags</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">DnsFlags</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						DNS flags for DNS record
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">DnsFlagsResponseCode</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">DnsFlagsResponseCode</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Parsed DNS header RCODEs name
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">DnsLatencyMs</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">DnsLatencyMs</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Calculated time between response and request, in milliseconds
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">TimeFlowStartMs</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">TimeFlowStartMs</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Start timestamp of this flow, in milliseconds
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">TimeFlowEndMs</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">TimeFlowEndMs</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						End timestamp of this flow, in milliseconds
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">TimeReceived</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">TimeReceived</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Timestamp when this flow was received and processed by the flow collector, in seconds
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">TimeFlowRttNs</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">TimeFlowRttNs</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Flow Round Trip Time (RTT) in nanoseconds
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">_HashId</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">_HashId</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						In conversation tracking, the conversation identifier
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">_IsFirst</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">_IsFirst</span></strong></span>: <code class="literal cluster-admin">string</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						In conversation tracking, a flag identifying the first flow
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">numFlowLogs</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<code class="literal cluster-admin">Optional</code> <span class="strong strong"><strong><span class="cluster-admin cluster-admin">numFlowLogs</span></strong></span>: <code class="literal cluster-admin">number</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						In conversation tracking, a counter of flow logs per conversation
					</p></section><section class="section cluster-admin" id="enumeration-flowdirection"><div class="titlepage"><div><div><h4 class="title">36.10.1.3. Enumeration: FlowDirection</h4></div></div></div><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Ingress</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Ingress</span></strong></span> = <code class="literal cluster-admin">"0"</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Incoming traffic, from the node observation point
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Egress</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Egress</span></strong></span> = <code class="literal cluster-admin">"1"</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Outgoing traffic, from the node observation point
					</p><p>
						
					</p><div class="variablelist cluster-admin"><dl class="variablelist cluster-admin"><dt><span class="term">Inner</span></dt><dd><div class="itemizedlist cluster-admin"><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
											<span class="strong strong"><strong><span class="cluster-admin cluster-admin">Inner</span></strong></span> = <code class="literal cluster-admin">"2"</code>
										</li></ul></div></dd></dl></div><p class="cluster-admin cluster-admin">
						Inner traffic, with the same source and destination node
					</p></section></section></section><section class="section cluster-admin" id="installing-troubleshooting"><div class="titlepage"><div><div><h2 class="title">36.11. Troubleshooting Network Observability</h2></div></div></div><p class="cluster-admin cluster-admin">
				To assist in troubleshooting Network Observability issues, you can perform some troubleshooting actions.
			</p><section class="section cluster-admin" id="network-observability-must-gather_network-observability-troubleshooting"><div class="titlepage"><div><div><h3 class="title">36.11.1. Using the must-gather tool</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can use the must-gather tool to collect information about the Network Observability Operator resources and cluster-wide resources, such as pod logs, <code class="literal cluster-admin">FlowCollector</code>, and <code class="literal cluster-admin">webhook</code> configurations.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Navigate to the directory where you want to store the must-gather data.
						</li><li class="listitem"><p class="simpara">
							Run the following command to collect cluster-wide must-gather resources:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc adm must-gather
 --image-stream=openshift/must-gather \
 --image=quay.io/netobserv/must-gather</pre></li></ol></div></section><section class="section cluster-admin" id="configure-network-traffic-console_network-observability-troubleshooting"><div class="titlepage"><div><div><h3 class="title">36.11.2. Configuring network traffic menu entry in the OpenShift Container Platform console</h3></div></div></div><p class="cluster-admin cluster-admin">
					Manually configure the network traffic menu entry in the OpenShift Container Platform console when the network traffic menu entry is not listed in <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Observe</span></strong></span> menu in the OpenShift Container Platform console.
				</p><div class="itemizedlist cluster-admin"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist cluster-admin" type="disc"><li class="listitem">
							You have installed OpenShift Container Platform version 4.10 or newer.
						</li></ul></div><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Check if the <code class="literal cluster-admin">spec.consolePlugin.register</code> field is set to <code class="literal cluster-admin">true</code> by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n netobserv get flowcollector cluster -o yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">apiVersion: flows.netobserv.io/v1alpha1
kind: FlowCollector
metadata:
  name: cluster
spec:
  consolePlugin:
    register: false</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Optional: Add the <code class="literal cluster-admin">netobserv-plugin</code> plugin by manually editing the Console Operator config:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit console.operator.openshift.io cluster</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">...
spec:
  plugins:
  - netobserv-plugin
...</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Optional: Set the <code class="literal cluster-admin">spec.consolePlugin.register</code> field to <code class="literal cluster-admin">true</code> by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n netobserv edit flowcollector cluster -o yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">apiVersion: flows.netobserv.io/v1alpha1
kind: FlowCollector
metadata:
  name: cluster
spec:
  consolePlugin:
    register: true</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Ensure the status of console pods is <code class="literal cluster-admin">running</code> by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods -n openshift-console -l app=console</pre></li><li class="listitem"><p class="simpara">
							Restart the console pods by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete pods -n openshift-console -l app=console</pre></li><li class="listitem">
							Clear your browser cache and history.
						</li><li class="listitem"><p class="simpara">
							Check the status of Network Observability plugin pods by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc get pods -n netobserv -l app=netobserv-plugin</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">NAME                                READY   STATUS    RESTARTS   AGE
netobserv-plugin-68c7bbb9bb-b69q6   1/1     Running   0          21s</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Check the logs of the Network Observability plugin pods by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc logs -n netobserv -l app=netobserv-plugin</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">time="2022-12-13T12:06:49Z" level=info msg="Starting netobserv-console-plugin [build version: , build date: 2022-10-21 15:15] at log level info" module=main
time="2022-12-13T12:06:49Z" level=info msg="listening on https://:9001" module=server</pre>

							</p></div></li></ol></div></section><section class="section cluster-admin" id="configure-network-traffic-flowlogs-pipeline-kafka_network-observability-troubleshooting"><div class="titlepage"><div><div><h3 class="title">36.11.3. Flowlogs-Pipeline does not consume network flows after installing Kafka</h3></div></div></div><p class="cluster-admin cluster-admin">
					If you deployed the flow collector first with <code class="literal cluster-admin">deploymentModel: KAFKA</code> and then deployed Kafka, the flow collector might not connect correctly to Kafka. Manually restart the flow-pipeline pods where Flowlogs-pipeline does not consume network flows from Kafka.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Delete the flow-pipeline pods to restart them by running the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc delete pods -n netobserv -l app=flowlogs-pipeline-transformer</pre></li></ol></div></section><section class="section cluster-admin" id="configure-network-traffic-interfaces_network-observability-troubleshooting"><div class="titlepage"><div><div><h3 class="title">36.11.4. Failing to see network flows from both <code class="literal cluster-admin">br-int</code> and <code class="literal cluster-admin">br-ex</code> interfaces</h3></div></div></div><p class="cluster-admin cluster-admin">
					br-ex` and <code class="literal cluster-admin">br-int</code> are virtual bridge devices operated at OSI layer 2. The eBPF agent works at the IP and TCP levels, layers 3 and 4 respectively. You can expect that the eBPF agent captures the network traffic passing through <code class="literal cluster-admin">br-ex</code> and <code class="literal cluster-admin">br-int</code>, when the network traffic is processed by other interfaces such as physical host or virtual pod interfaces. If you restrict the eBPF agent network interfaces to attach only to <code class="literal cluster-admin">br-ex</code> and <code class="literal cluster-admin">br-int</code>, you do not see any network flow.
				</p><p class="cluster-admin cluster-admin">
					Manually remove the part in the <code class="literal cluster-admin">interfaces</code> or <code class="literal cluster-admin">excludeInterfaces</code> that restricts the network interfaces to <code class="literal cluster-admin">br-int</code> and <code class="literal cluster-admin">br-ex</code>.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Remove the <code class="literal cluster-admin">interfaces: [ 'br-int', 'br-ex' ]</code> field. This allows the agent to fetch information from all the interfaces. Alternatively, you can specify the Layer-3 interface for example, <code class="literal cluster-admin">eth0</code>. Run the following command:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc edit -n netobserv flowcollector.yaml -o yaml</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">apiVersion: flows.netobserv.io/v1alpha1
kind: FlowCollector
metadata:
  name: cluster
spec:
  agent:
    type: EBPF
    ebpf:
      interfaces: [ 'br-int', 'br-ex' ] <span id="CO247-1"><!--Empty--></span><span class="callout">1</span></pre>

							</p></div><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO247-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specifies the network interfaces.
								</div></dd></dl></div></li></ol></div></section><section class="section cluster-admin" id="controller-manager-pod-runs-out-of-memory_network-observability-troubleshooting"><div class="titlepage"><div><div><h3 class="title">36.11.5. Network Observability controller manager pod runs out of memory</h3></div></div></div><p class="cluster-admin cluster-admin">
					You can increase memory limits for the Network Observability operator by patching the Cluster Service Version (CSV), where Network Observability controller manager pod runs out of memory.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem"><p class="simpara">
							Run the following command to patch the CSV:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n netobserv patch csv network-observability-operator.v1.0.0 --type='json' -p='[{"op": "replace", "path":"/spec/install/spec/deployments/0/spec/template/spec/containers/0/resources/limits/memory", value: "1Gi"}]'</pre><div class="cluster-admin cluster-admin"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="screen">clusterserviceversion.operators.coreos.com/network-observability-operator.v1.0.0 patched</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Run the following command to view the updated CSV:
						</p><pre class="programlisting language-terminal cluster-admin cluster-admin">$ oc -n netobserv get csv network-observability-operator.v1.0.0 -o jsonpath='{.spec.install.spec.deployments[0].spec.template.spec.containers[0].resources.limits.memory}'
1Gi</pre></li></ol></div></section><section class="section cluster-admin" id="resource-troubleshooting"><div class="titlepage"><div><div><h3 class="title">36.11.6. Resource troubleshooting</h3></div></div></div></section><section class="section cluster-admin" id="network-observability-troubleshooting-loki-tenant-rate-limit_network-observability-troubleshooting"><div class="titlepage"><div><div><h3 class="title">36.11.7. LokiStack rate limit errors</h3></div></div></div><p class="cluster-admin cluster-admin">
					A rate-limit placed on the Loki tenant can result in potential temporary loss of data and a 429 error: <code class="literal cluster-admin">Per stream rate limit exceeded (limit:xMB/sec) while attempting to ingest for stream</code>. You might consider having an alert set to notify you of this error. For more information, see "Creating Loki rate limit alerts for the NetObserv dashboard" in the Additional resources of this section.
				</p><p class="cluster-admin cluster-admin">
					You can update the LokiStack CRD with the <code class="literal cluster-admin">perStreamRateLimit</code> and <code class="literal cluster-admin">perStreamRateLimitBurst</code> specifications, as shown in the following procedure.
				</p><div class="orderedlist cluster-admin"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist cluster-admin" type="1"><li class="listitem">
							Navigate to <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Operators</span></strong></span> → <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Installed Operators</span></strong></span>, viewing <span class="strong strong"><strong><span class="cluster-admin cluster-admin">All projects</span></strong></span> from the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Project</span></strong></span> dropdown.
						</li><li class="listitem">
							Look for <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Loki Operator</span></strong></span>, and select the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">LokiStack</span></strong></span> tab.
						</li><li class="listitem"><p class="simpara">
							Create or edit an existing <span class="strong strong"><strong><span class="cluster-admin cluster-admin">LokiStack</span></strong></span> instance using the <span class="strong strong"><strong><span class="cluster-admin cluster-admin">YAML view</span></strong></span> to add the <code class="literal cluster-admin">perStreamRateLimit</code> and <code class="literal cluster-admin">perStreamRateLimitBurst</code> specifications:
						</p><pre class="programlisting language-yaml cluster-admin cluster-admin">apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: loki
  namespace: netobserv
spec:
  limits:
    global:
      ingestion:
        perStreamRateLimit: 6        <span id="CO248-1"><!--Empty--></span><span class="callout">1</span>
        perStreamRateLimitBurst: 30  <span id="CO248-2"><!--Empty--></span><span class="callout">2</span>
  tenants:
    mode: openshift-network
  managementState: Managed</pre><div class="calloutlist cluster-admin"><dl class="calloutlist cluster-admin"><dt><a href="#CO248-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The default value for <code class="literal cluster-admin">perStreamRateLimit</code> is <code class="literal cluster-admin">3</code>.
								</div></dd><dt><a href="#CO248-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The default value for <code class="literal cluster-admin">perStreamRateLimitBurst</code> is <code class="literal cluster-admin">15</code>.
								</div></dd></dl></div></li><li class="listitem">
							Click <span class="strong strong"><strong><span class="cluster-admin cluster-admin">Save</span></strong></span>.
						</li></ol></div><div class="cluster-admin cluster-admin"><p class="title"><strong>Verification</strong></p><p>
						Once you update the <code class="literal cluster-admin">perStreamRateLimit</code> and <code class="literal cluster-admin">perStreamRateLimitBurst</code> specifications, the pods in your cluster restart and the 429 rate-limit error no longer occurs.
					</p></div></section></section></section><div><div xml:lang="en-US" class="legalnotice" id="idm140587105891184"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"><!--Empty--></span>© 2023 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div>


  <nav class="pvof-doc__book-nav">
  <ol class="book-nav__list">
              </ol>
</nav>


          </div>
              </div>
              <div id="comments-footer" class="book-comments">
          

  

        </div>
          </div>
  </article>
<meta itemscope="" itemref="md1">



    </div>
      <!-- CP_PRIMER_FOOTER -->            </div>
        </main>
    </div>
    <!--googleoff: all-->
    <div id="to-top"><a class="btn_slideto" href="#masthead" aria-label="Back to Top"><span class="web-icon-upload"></span></a></div>
    <footer class="footer-main">
        <div class="footer-top">
            <div class="container">

              <div class="brand">
                <a href="https://redhat.com">
                  <svg class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 613 145">
                    <defs>
                      <style>
                        .rh-logo-hat {
                          fill: #e00;
                        }
                        .rh-logo-type {
                          fill: #fff;
                        }
                      </style>
                    </defs>
                    <title>Red Hat</title>
                    <path
                      class="rh-logo-hat"
                      d="M127.47,83.49c12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42l-7.45-32.36c-1.72-7.12-3.23-10.35-15.73-16.6C124.89,8.69,103.76.5,97.51.5,91.69.5,90,8,83.06,8c-6.68,0-11.64-5.6-17.89-5.6-6,0-9.91,4.09-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33C22.27,52,.5,55,.5,74.22c0,31.48,74.59,70.28,133.65,70.28,45.28,0,56.7-20.48,56.7-36.65,0-12.72-11-27.16-30.83-35.78"/>
                      <path class="rh-logo-band"
                      d="M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45,12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/>
                      <path
                      class="rh-logo-type"
                      d="M579.74,92.8c0,11.89,7.15,17.67,20.19,17.67a52.11,52.11,0,0,0,11.89-1.68V95a24.84,24.84,0,0,1-7.68,1.16c-5.37,0-7.36-1.68-7.36-6.73V68.3h15.56V54.1H596.78v-18l-17,3.68V54.1H568.49V68.3h11.25Zm-53,.32c0-3.68,3.69-5.47,9.26-5.47a43.12,43.12,0,0,1,10.1,1.26v7.15a21.51,21.51,0,0,1-10.63,2.63c-5.46,0-8.73-2.1-8.73-5.57m5.2,17.56c6,0,10.84-1.26,15.36-4.31v3.37h16.82V74.08c0-13.56-9.14-21-24.39-21-8.52,0-16.94,2-26,6.1l6.1,12.52c6.52-2.74,12-4.42,16.83-4.42,7,0,10.62,2.73,10.62,8.31v2.73a49.53,49.53,0,0,0-12.62-1.58c-14.31,0-22.93,6-22.93,16.73,0,9.78,7.78,17.24,20.19,17.24m-92.44-.94h18.09V80.92h30.29v28.82H506V36.12H487.93V64.41H457.64V36.12H439.55ZM370.62,81.87c0-8,6.31-14.1,14.62-14.1A17.22,17.22,0,0,1,397,72.09V91.54A16.36,16.36,0,0,1,385.24,96c-8.2,0-14.62-6.1-14.62-14.09m26.61,27.87h16.83V32.44l-17,3.68V57.05a28.3,28.3,0,0,0-14.2-3.68c-16.19,0-28.92,12.51-28.92,28.5a28.25,28.25,0,0,0,28.4,28.6,25.12,25.12,0,0,0,14.93-4.83ZM320,67c5.36,0,9.88,3.47,11.67,8.83H308.47C310.15,70.3,314.36,67,320,67M291.33,82c0,16.2,13.25,28.82,30.28,28.82,9.36,0,16.2-2.53,23.25-8.42l-11.26-10c-2.63,2.74-6.52,4.21-11.14,4.21a14.39,14.39,0,0,1-13.68-8.83h39.65V83.55c0-17.67-11.88-30.39-28.08-30.39a28.57,28.57,0,0,0-29,28.81M262,51.58c6,0,9.36,3.78,9.36,8.31S268,68.2,262,68.2H244.11V51.58Zm-36,58.16h18.09V82.92h13.77l13.89,26.82H292l-16.2-29.45a22.27,22.27,0,0,0,13.88-20.72c0-13.25-10.41-23.45-26-23.45H226Z"/>
                  </svg>
                </a>
              </div>

              <div role="navigation" aria-label="quick">
                  <h3>Quick Links</h3>
                  <ul>
                      <li><a class="download-software" href="https://access.redhat.com/downloads/">Downloads</a></li>
                      <li><a class="manage-subscriptions" href="https://access.redhat.com/management">Subscriptions</a></li>
                      <li><a class="support-cases" href="https://access.redhat.com/support">Support Cases</a></li>
                      <li><a class="customer-service" href="https://access.redhat.com/support/customer-service">Customer Service</a></li>
                      <li><a class="quick-docs" href="https://access.redhat.com/documentation">Product Documentation</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="help">
                  <h3>Help</h3>
                  <ul>
                      <li><a class="contact-us" href="https://access.redhat.com/support/contact/">Contact Us</a></li>
                      <li><a class="cp-faqs" href="https://access.redhat.com/articles/33844">Customer Portal FAQ</a></li>
                      <li><a class="login-problems" href="https://access.redhat.com/help/login_assistance">Log-in Assistance</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="site">
                  <h3>Site Info</h3>
                  <ul>
                      <li><a class="trust-red-hat" href="https://www.redhat.com/en/trust">Trust Red Hat</a></li>
                      <li><a class="browser-support-policy" href="https://www.redhat.com/en/about/browser-support">Browser Support Policy</a></li>
                      <li><a class="accessibility" href="https://www.redhat.com/en/about/digital-accessibility">Accessibility</a></li>
                      <li><a class="recognition" href="https://access.redhat.com/recognition/">Awards and Recognition</a></li>
                      <li><a class="colophon" href="https://access.redhat.com/help/colophon/">Colophon</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="other">
                  <h3>Related Sites</h3>
                  <ul>
                      <li><a href="https://www.redhat.com/" class="red-hat-com">redhat.com</a></li>
                      <li><a href="http://developers.redhat.com/" class="red-hat-developers">developers.redhat.com</a></li>
                      <li><a href="https://connect.redhat.com/" class="partner-connect">connect.redhat.com</a></li>
                      <li><a href="https://cloud.redhat.com/" class="cloud-com">cloud.redhat.com</a></li>
                  </ul>
              </div>

              <div role="navigation" aria-label="about">
                  <h3>About</h3>
                  <ul>
                      <li><a href="https://access.redhat.com/subscription-value" class="subscription-value">Red Hat Subscription Value</a></li>
                      <li><a href="https://www.redhat.com/about/" class="about-red-hat">About Red Hat</a></li>
                      <li><a href="http://jobs.redhat.com" class="about-jobs">Red Hat Jobs</a></li>
                  </ul>
              </div>

            </div>
        </div>

        <div class="anchor">
            <div class="container">
                <div class="status-legal">
                    <a hidden href="https://status.redhat.com" class="status-page-widget">
                        <span class="status-description"></span>
                        <span class="status-dot shape-circle"></span>
                    </a>
                    <div class="legal-copyright">
                        <div class="copyright">Copyright © 2023 Red Hat, Inc.</div>
                        <div role="navigation" aria-label="legal" class="legal">
                            <ul>
                                <li><a href="http://www.redhat.com/en/about/privacy-policy" class="privacy-policy">Privacy Statement</a></li>
                                <li><a href="https://www.redhat.com/en/about/terms-use" class="terms-of-use">Terms of Use</a></li>
                                <li><a href="http://www.redhat.com/en/about/all-policies-guidelines" class="all-policies">All Policies and Guidelines</a></li>
                                <li><a id="teconsent"></a></li>
                            </ul>
                            <div id="privacy_policy">We've updated our <a href='http://www.redhat.com/en/about/privacy-policy' class='privacy-policy'>Privacy Statement</a> effective September 15, 2023.
                            </div>
                          </div>
                        </div>
                </div>
                <div class="social">
                    <a href="http://www.redhat.com/summit/" class="summit">
                        <img src="https://access.redhat.com/chrome_themes/nimbus/img/rh-summit-red-a.svg" alt="Red Hat Summit" /> <span class="offscreen">Red Hat Summit</span>
                    </a>

                    <div class="social-media">
                        <a href="https://twitter.com/RedHat" class="sm-icon twitter"><span class="nicon-twitter"></span><span class="offscreen">Twitter</span></a>                        
                    </div>
                </div>
            </div>
        </div>
    </footer>
    <!-- TrustArc -->
    <div id="consent_blackbar"></div> 
    <!--googleon: all-->
</div>
<!-- /CP_PRIMER_FOOTER -->


  </div>

    
  </body>
</html>
